<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-26</h1>
<h3>Title: A Survey on Generative AI and LLM for Video Generation, Understanding,  and Streaming</h3>
<ul>
<li><strong>Authors: </strong>Pengyuan Zhou, Lin Wang, Zhi Liu, Yanbin Hao, Pan Hui, Sasu Tarkoma, Jussi Kangasharju</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16038">https://arxiv.org/abs/2404.16038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16038">https://arxiv.org/pdf/2404.16038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16038]] A Survey on Generative AI and LLM for Video Generation, Understanding,  and Streaming(https://arxiv.org/abs/2404.16038)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>This paper offers an insightful examination of how currently top-trending AI technologies, i.e., generative artificial intelligence (Generative AI) and large language models (LLMs), are reshaping the field of video technology, including video generation, understanding, and streaming. It highlights the innovative use of these technologies in producing highly realistic videos, a significant leap in bridging the gap between real-world dynamics and digital creation. The study also delves into the advanced capabilities of LLMs in video understanding, demonstrating their effectiveness in extracting meaningful information from visual content, thereby enhancing our interaction with videos. In the realm of video streaming, the paper discusses how LLMs contribute to more efficient and user-centric streaming experiences, adapting content delivery to individual viewer preferences. This comprehensive review navigates through the current achievements, ongoing challenges, and future possibilities of applying Generative AI and LLMs to video-related tasks, underscoring the immense potential these technologies hold for advancing the field of video technology related to multimedia, networking, and AI communities.</li>
</ul>

<h3>Title: zkLLM: Zero Knowledge Proofs for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haochen Sun, Jason Li, Hongyang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16109">https://arxiv.org/abs/2404.16109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16109">https://arxiv.org/pdf/2404.16109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16109]] zkLLM: Zero Knowledge Proofs for Large Language Models(https://arxiv.org/abs/2404.16109)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>The recent surge in artificial intelligence (AI), characterized by the prominence of large language models (LLMs), has ushered in fundamental transformations across the globe. However, alongside these advancements, concerns surrounding the legitimacy of LLMs have grown, posing legal challenges to their extensive applications. Compounding these concerns, the parameters of LLMs are often treated as intellectual property, restricting direct investigations. In this study, we address a fundamental challenge within the realm of AI legislation: the need to establish the authenticity of outputs generated by LLMs. To tackle this issue, we present zkLLM, which stands as the inaugural specialized zero-knowledge proof tailored for LLMs to the best of our knowledge. Addressing the persistent challenge of non-arithmetic operations in deep learning, we introduce tlookup, a parallelized lookup argument designed for non-arithmetic tensor operations in deep learning, offering a solution with no asymptotic overhead. Furthermore, leveraging the foundation of tlookup, we introduce zkAttn, a specialized zero-knowledge proof crafted for the attention mechanism, carefully balancing considerations of running time, memory usage, and accuracy. Empowered by our fully parallelized CUDA implementation, zkLLM emerges as a significant stride towards achieving efficient zero-knowledge verifiable computations over LLMs. Remarkably, for LLMs boasting 13 billion parameters, our approach enables the generation of a correctness proof for the entire inference process in under 15 minutes. The resulting proof, compactly sized at less than 200 kB, is designed to uphold the privacy of the model parameters, ensuring no inadvertent information leakage.</li>
</ul>

<h3>Title: Mamba-360: Survey of State Space Models as Transformer Alternative for  Long Sequence Modelling: Methods, Applications, and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Badri Narayana Patro, Vijay Srinivas Agneeswaran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.MM, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16112">https://arxiv.org/abs/2404.16112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16112">https://arxiv.org/pdf/2404.16112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16112]] Mamba-360: Survey of State Space Models as Transformer Alternative for  Long Sequence Modelling: Methods, Applications, and Challenges(https://arxiv.org/abs/2404.16112)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Sequence modeling is a crucial area across various domains, including Natural Language Processing (NLP), speech recognition, time series forecasting, music generation, and bioinformatics. Recurrent Neural Networks (RNNs) and Long Short Term Memory Networks (LSTMs) have historically dominated sequence modeling tasks like Machine Translation, Named Entity Recognition (NER), etc. However, the advancement of transformers has led to a shift in this paradigm, given their superior performance. Yet, transformers suffer from $O(N^2)$ attention complexity and challenges in handling inductive bias. Several variations have been proposed to address these issues which use spectral networks or convolutions and have performed well on a range of tasks. However, they still have difficulty in dealing with long sequences. State Space Models(SSMs) have emerged as promising alternatives for sequence modeling paradigms in this context, especially with the advent of S4 and its variants, such as S4nd, Hippo, Hyena, Diagnol State Spaces (DSS), Gated State Spaces (GSS), Linear Recurrent Unit (LRU), Liquid-S4, Mamba, etc. In this survey, we categorize the foundational SSMs based on three paradigms namely, Gating architectures, Structural architectures, and Recurrent architectures. This survey also highlights diverse applications of SSMs across domains such as vision, video, audio, speech, language (especially long sequence modeling), medical (including genomics), chemical (like drug design), recommendation systems, and time series analysis, including tabular data. Moreover, we consolidate the performance of SSMs on benchmark datasets like Long Range Arena (LRA), WikiText, Glue, Pile, ImageNet, Kinetics-400, sstv2, as well as video datasets such as Breakfast, COIN, LVU, and various time series datasets. The project page for Mamba-360 work is available on this webpage.\url{https://github.com/badripatro/mamba360}.</li>
</ul>

<h3>Title: Classifying Human-Generated and AI-Generated Election Claims in Social  Media</h3>
<ul>
<li><strong>Authors: </strong>Alphaeus Dmonte, Marcos Zampieri, Kevin Lybarger, Massimiliano Albanese</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16116">https://arxiv.org/abs/2404.16116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16116">https://arxiv.org/pdf/2404.16116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16116]] Classifying Human-Generated and AI-Generated Election Claims in Social  Media(https://arxiv.org/abs/2404.16116)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Politics is one of the most prevalent topics discussed on social media platforms, particularly during major election cycles, where users engage in conversations about candidates and electoral processes. Malicious actors may use this opportunity to disseminate misinformation to undermine trust in the electoral process. The emergence of Large Language Models (LLMs) exacerbates this issue by enabling malicious actors to generate misinformation at an unprecedented scale. Artificial intelligence (AI)-generated content is often indistinguishable from authentic user content, raising concerns about the integrity of information on social networks. In this paper, we present a novel taxonomy for characterizing election-related claims. This taxonomy provides an instrument for analyzing election-related claims, with granular categories related to jurisdiction, equipment, processes, and the nature of claims. We introduce ElectAI, a novel benchmark dataset that consists of 9,900 tweets, each labeled as human- or AI-generated. For AI-generated tweets, the specific LLM variant that produced them is specified. We annotated a subset of 1,550 tweets using the proposed taxonomy to capture the characteristics of election-related claims. We explored the capabilities of LLMs in extracting the taxonomy attributes and trained various machine learning models using ElectAI to distinguish between human- and AI-generated posts and identify the specific LLM variant.</li>
</ul>

<h3>Title: Cybersecurity Assessment of the Polar Bluetooth Low Energy Heart-rate  Sensor</h3>
<ul>
<li><strong>Authors: </strong>Smone Soderi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16117">https://arxiv.org/abs/2404.16117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16117">https://arxiv.org/pdf/2404.16117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16117]] Cybersecurity Assessment of the Polar Bluetooth Low Energy Heart-rate  Sensor(https://arxiv.org/abs/2404.16117)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Wireless communications among wearable and implantable devices implement the information exchange around the human body. Wireless body area network (WBAN) technology enables non-invasive applications in our daily lives. Wireless connected devices improve the quality of many services, and they make procedures easier. On the other hand, they open up large attack surfaces and introduces potential security vulnerabilities. Bluetooth low energy (BLE) is a low-power protocol widely used in wireless personal area networks (WPANs). This paper analyzes the security vulnerabilities of a BLE heart-rate sensor. By observing the received signal strength indicator (RSSI) variations, it is possible to detect anomalies in the BLE connection. The case-study shows that an attacker can easily intercept and manipulate the data transmitted between the mobile app and the BLE device. With this research, the author would raise awareness about the security of the heart-rate information that we can receive from our wireless body sensors.</li>
</ul>

<h3>Title: Act as a Honeytoken Generator! An Investigation into Honeytoken  Generation with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Daniel Reti, Norman Becker, Tillmann Angeli, Anasuya Chattopadhyay, Daniel Schneider, Sebastian Vollmer, Hans D. Schotten</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16118">https://arxiv.org/abs/2404.16118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16118">https://arxiv.org/pdf/2404.16118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16118]] Act as a Honeytoken Generator! An Investigation into Honeytoken  Generation with Large Language Models(https://arxiv.org/abs/2404.16118)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, large language model</a></li>
<li><strong>Abstract: </strong>With the increasing prevalence of security incidents, the adoption of deception-based defense strategies has become pivotal in cyber security. This work addresses the challenge of scalability in designing honeytokens, a key component of such defense mechanisms. The manual creation of honeytokens is a tedious task. Although automated generators exists, they often lack versatility, being specialized for specific types of honeytokens, and heavily rely on suitable training datasets. To overcome these limitations, this work systematically investigates the approach of utilizing Large Language Models (LLMs) to create a variety of honeytokens. Out of the seven different honeytoken types created in this work, such as configuration files, databases, and log files, two were used to evaluate the optimal prompt. The generation of robots.txt files and honeywords was used to systematically test 210 different prompt structures, based on 16 prompt building blocks. Furthermore, all honeytokens were tested across different state-of-the-art LLMs to assess the varying performance of different models. Prompts performing optimally on one LLMs do not necessarily generalize well to another. Honeywords generated by GPT-3.5 were found to be less distinguishable from real passwords compared to previous methods of automated honeyword generation. Overall, the findings of this work demonstrate that generic LLMs are capable of creating a wide array of honeytokens using the presented prompt structures.</li>
</ul>

<h3>Title: Securing Hybrid Wireless Body Area Networks (HyWBAN): Advancements in  Semantic Communications and Jamming Techniques</h3>
<ul>
<li><strong>Authors: </strong>Simone Soderi, Mariella Särestöniemi, Syifaul Fuada, Matti Hämäläinen, Marcos Katz, Jari Iinatti</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16120">https://arxiv.org/abs/2404.16120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16120">https://arxiv.org/pdf/2404.16120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16120]] Securing Hybrid Wireless Body Area Networks (HyWBAN): Advancements in  Semantic Communications and Jamming Techniques(https://arxiv.org/abs/2404.16120)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack</a></li>
<li><strong>Abstract: </strong>This paper explores novel strategies to strengthen the security of Hybrid Wireless Body Area Networks (HyWBANs), essential in smart healthcare and Internet of Things (IoT) applications. Recognizing the vulnerability of HyWBAN to sophisticated cyber-attacks, we propose an innovative combination of semantic communications and jamming receivers. This dual-layered security mechanism protects against unauthorized access and data breaches, particularly in scenarios involving in-body to on-body communication channels. We conduct comprehensive laboratory measurements to understand hybrid (radio and optical) communication propagation through biological tissues and utilize these insights to refine a dataset for training a Deep Learning (DL) model. These models, in turn, generate semantic concepts linked to cryptographic keys for enhanced data confidentiality and integrity using a jamming receiver. The proposed model demonstrates a significant reduction in energy consumption compared to traditional cryptographic methods, like Elliptic Curve Diffie-Hellman (ECDH), especially when supplemented with jamming. Our approach addresses the primary security concerns and sets the baseline for future secure biomedical communication systems advancements.</li>
</ul>

<h3>Title: FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities  in Semantic Dataset Deduplication</h3>
<ul>
<li><strong>Authors: </strong>Eric Slyman, Stefan Lee, Scott Cohen, Kushal Kafle</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16123">https://arxiv.org/abs/2404.16123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16123">https://arxiv.org/pdf/2404.16123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16123]] FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities  in Semantic Dataset Deduplication(https://arxiv.org/abs/2404.16123)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset. These results have been based on pruning commonly used image-caption datasets collected from the web -- datasets that are known to harbor harmful social biases that may then be codified in trained models. In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-to-implement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe. When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks.</li>
</ul>

<h3>Title: From Local to Global: A Graph RAG Approach to Query-Focused  Summarization</h3>
<ul>
<li><strong>Authors: </strong>Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Jonathan Larson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16130">https://arxiv.org/abs/2404.16130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16130">https://arxiv.org/pdf/2404.16130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16130]] From Local to Global: A Graph RAG Approach to Query-Focused  Summarization(https://arxiv.org/abs/2404.16130)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as "What are the main themes in the dataset?", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a na\"ive RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at https://aka.ms/graphrag.</li>
</ul>

<h3>Title: Quantitative Characterization of Retinal Features in Translated OCTA</h3>
<ul>
<li><strong>Authors: </strong>Rashadul Hasan Badhon, Atalie Carina Thompson, Jennifer I. Lim, Theodore Leng, Minhaj Nur Alam</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16133">https://arxiv.org/abs/2404.16133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16133">https://arxiv.org/pdf/2404.16133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16133]] Quantitative Characterization of Retinal Features in Translated OCTA(https://arxiv.org/abs/2404.16133)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Purpose: This study explores the feasibility of using generative machine learning (ML) to translate Optical Coherence Tomography (OCT) images into Optical Coherence Tomography Angiography (OCTA) images, potentially bypassing the need for specialized OCTA hardware. Methods: The method involved implementing a generative adversarial network framework that includes a 2D vascular segmentation model and a 2D OCTA image translation model. The study utilizes a public dataset of 500 patients, divided into subsets based on resolution and disease status, to validate the quality of TR-OCTA images. The validation employs several quality and quantitative metrics to compare the translated images with ground truth OCTAs (GT-OCTA). We then quantitatively characterize vascular features generated in TR-OCTAs with GT-OCTAs to assess the feasibility of using TR-OCTA for objective disease diagnosis. Result: TR-OCTAs showed high image quality in both 3 and 6 mm datasets (high-resolution, moderate structural similarity and contrast quality compared to GT-OCTAs). There were slight discrepancies in vascular metrics, especially in diseased patients. Blood vessel features like tortuosity and vessel perimeter index showed a better trend compared to density features which are affected by local vascular distortions. Conclusion: This study presents a promising solution to the limitations of OCTA adoption in clinical practice by using vascular features from TR-OCTA for disease detection. Translation relevance: This study has the potential to significantly enhance the diagnostic process for retinal diseases by making detailed vascular imaging more widely available and reducing dependency on costly OCTA equipment.</li>
</ul>

<h3>Title: A Survey on Intermediate Fusion Methods for Collaborative Perception  Categorized by Real World Challenges</h3>
<ul>
<li><strong>Authors: </strong>Melih Yazgan, Thomas Graf, Min Liu, J. Marius Zoellner</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16139">https://arxiv.org/abs/2404.16139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16139">https://arxiv.org/pdf/2404.16139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16139]] A Survey on Intermediate Fusion Methods for Collaborative Perception  Categorized by Real World Challenges(https://arxiv.org/abs/2404.16139)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>This survey analyzes intermediate fusion methods in collaborative perception for autonomous driving, categorized by real-world challenges. We examine various methods, detailing their features and the evaluation metrics they employ. The focus is on addressing challenges like transmission efficiency, localization errors, communication disruptions, and heterogeneity. Moreover, we explore strategies to counter adversarial attacks and defenses, as well as approaches to adapt to domain shifts. The objective is to present an overview of how intermediate fusion methods effectively meet these diverse challenges, highlighting their role in advancing the field of collaborative perception in autonomous driving.</li>
</ul>

<h3>Title: A Comparative Analysis of Adversarial Robustness for Quantum and  Classical Machine Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Wendlinger, Kilian Tscharke, Pascal Debus</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16154">https://arxiv.org/abs/2404.16154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16154">https://arxiv.org/pdf/2404.16154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16154]] A Comparative Analysis of Adversarial Robustness for Quantum and  Classical Machine Learning Models(https://arxiv.org/abs/2404.16154)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Quantum machine learning (QML) continues to be an area of tremendous interest from research and industry. While QML models have been shown to be vulnerable to adversarial attacks much in the same manner as classical machine learning models, it is still largely unknown how to compare adversarial attacks on quantum versus classical models. In this paper, we show how to systematically investigate the similarities and differences in adversarial robustness of classical and quantum models using transfer attacks, perturbation patterns and Lipschitz bounds. More specifically, we focus on classification tasks on a handcrafted dataset that allows quantitative analysis for feature attribution. This enables us to get insight, both theoretically and experimentally, on the robustness of classification networks. We start by comparing typical QML model architectures such as amplitude and re-upload encoding circuits with variational parameters to a classical ConvNet architecture. Next, we introduce a classical approximation of QML circuits (originally obtained with Random Fourier Features sampling but adapted in this work to fit a trainable encoding) and evaluate this model, denoted Fourier network, in comparison to other architectures. Our findings show that this Fourier network can be seen as a "middle ground" on the quantum-classical boundary. While adversarial attacks successfully transfer across this boundary in both directions, we also show that regularization helps quantum networks to be more robust, which has direct impact on Lipschitz bounds and transfer attacks.</li>
</ul>

<h3>Title: Does SAM dream of EIG? Characterizing Interactive Segmenter Performance  using Expected Information Gain</h3>
<ul>
<li><strong>Authors: </strong>Kuan-I Chung, Daniel Moyer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IT, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16155">https://arxiv.org/abs/2404.16155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16155">https://arxiv.org/pdf/2404.16155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16155]] Does SAM dream of EIG? Characterizing Interactive Segmenter Performance  using Expected Information Gain(https://arxiv.org/abs/2404.16155)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We introduce an assessment procedure for interactive segmentation models. Based on concepts from Bayesian Experimental Design, the procedure measures a model's understanding of point prompts and their correspondence with the desired segmentation mask. We show that Oracle Dice index measurements are insensitive or even misleading in measuring this property. We demonstrate the use of the proposed procedure on three interactive segmentation models and subsets of two large image segmentation datasets.</li>
</ul>

<h3>Title: Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant</h3>
<ul>
<li><strong>Authors: </strong>Cheng Kang, Daniel Novak, Katerina Urbanova, Yuqing Cheng, Yong Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16160">https://arxiv.org/abs/2404.16160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16160">https://arxiv.org/pdf/2404.16160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16160]] Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant(https://arxiv.org/abs/2404.16160)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive generalization capabilities on specific tasks with human-written instruction data. However, the limited quantity, diversity, and professional expertise of such instruction data raise concerns about the performance of LLMs in psychotherapy tasks when provided with domain-specific instructions. To address this, we firstly propose Domain-Specific Assistant Instructions based on AlexanderStreet therapy, and secondly, we use an adaption fine-tuning method and retrieval augmented generation method to improve pre-trained LLMs. Through quantitative evaluation of linguistic quality using automatic and human evaluation, we observe that pre-trained LLMs on Psychotherapy Assistant Instructions outperform state-of-the-art LLMs response baselines. Our Assistant-Instruction approach offers a half-annotation method to align pre-trained LLMs with instructions and provide pre-trained LLMs with more psychotherapy knowledge.</li>
</ul>

<h3>Title: Towards a Holistic Evaluation of LLMs on Factual Knowledge Recall</h3>
<ul>
<li><strong>Authors: </strong>Jiaqing Yuan, Lin Pan, Chung-Wei Hang, Jiang Guo, Jiarong Jiang, Bonan Min, Patrick Ng, Zhiguo Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16164">https://arxiv.org/abs/2404.16164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16164">https://arxiv.org/pdf/2404.16164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16164]] Towards a Holistic Evaluation of LLMs on Factual Knowledge Recall(https://arxiv.org/abs/2404.16164)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable performance on a variety of NLP tasks, and are being rapidly adopted in a wide range of use cases. It is therefore of vital importance to holistically evaluate the factuality of their generated outputs, as hallucinations remain a challenging issue. In this work, we focus on assessing LLMs' ability to recall factual knowledge learned from pretraining, and the factors that affect this ability. To that end, we construct FACT-BENCH, a representative benchmark covering 20 domains, 134 property types, 3 answer types, and different knowledge popularity levels. We benchmark 31 models from 10 model families and provide a holistic assessment of their strengths and weaknesses. We observe that instruction-tuning hurts knowledge recall, as pretraining-only models consistently outperform their instruction-tuned counterparts, and positive effects of model scaling, as larger models outperform smaller ones for all model families. However, the best performance from GPT-4 still represents a large gap with the upper-bound. We additionally study the role of in-context exemplars using counterfactual demonstrations, which lead to significant degradation of factual knowledge recall for large models. By further decoupling model known and unknown knowledge, we find the degradation is attributed to exemplars that contradict a model's known knowledge, as well as the number of such exemplars. Lastly, we fine-tune LLaMA-7B in different settings of known and unknown knowledge. In particular, fine-tuning on a model's known knowledge is beneficial, and consistently outperforms fine-tuning on unknown and mixed knowledge. We will make our benchmark publicly available.</li>
</ul>

<h3>Title: S2DEVFMAP: Self-Supervised Learning Framework with Dual Ensemble Voting  Fusion for Maximizing Anomaly Prediction in Timeseries</h3>
<ul>
<li><strong>Authors: </strong>Sarala Naidu, Ning Xiong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16179">https://arxiv.org/abs/2404.16179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16179">https://arxiv.org/pdf/2404.16179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16179]] S2DEVFMAP: Self-Supervised Learning Framework with Dual Ensemble Voting  Fusion for Maximizing Anomaly Prediction in Timeseries(https://arxiv.org/abs/2404.16179)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Anomaly detection plays a crucial role in industrial settings, particularly in maintaining the reliability and optimal performance of cooling systems. Traditional anomaly detection methods often face challenges in handling diverse data characteristics and variations in noise levels, resulting in limited effectiveness. And yet traditional anomaly detection often relies on application of single models. This work proposes a novel, robust approach using five heterogeneous independent models combined with a dual ensemble fusion of voting techniques. Diverse models capture various system behaviors, while the fusion strategy maximizes detection effectiveness and minimizes false alarms. Each base autoencoder model learns a unique representation of the data, leveraging their complementary strengths to improve anomaly detection performance. To increase the effectiveness and reliability of final anomaly prediction, dual ensemble technique is applied. This approach outperforms in maximizing the coverage of identifying anomalies. Experimental results on a real-world dataset of industrial cooling system data demonstrate the effectiveness of the proposed approach. This approach can be extended to other industrial applications where anomaly detection is critical for ensuring system reliability and preventing potential malfunctions.</li>
</ul>

<h3>Title: Blind Federated Learning without initial model</h3>
<ul>
<li><strong>Authors: </strong>Jose L. Salmeron, Irina Arévalo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16180">https://arxiv.org/abs/2404.16180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16180">https://arxiv.org/pdf/2404.16180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16180]] Blind Federated Learning without initial model(https://arxiv.org/abs/2404.16180)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning is an emerging machine learning approach that allows the construction of a model between several participants who hold their own private data. This method is secure and privacy-preserving, suitable for training a machine learning model using sensitive data from different sources, such as hospitals. In this paper, the authors propose two innovative methodologies for Particle Swarm Optimisation-based federated learning of Fuzzy Cognitive Maps in a privacy-preserving way. In addition, one relevant contribution this research includes is the lack of an initial model in the federated learning process, making it effectively blind. This proposal is tested with several open datasets, improving both accuracy and precision.</li>
</ul>

<h3>Title: A Game-Theoretic Analysis of Auditing Differentially Private Algorithms  with Epistemically Disparate Herd</h3>
<ul>
<li><strong>Authors: </strong>Ya-Ting Yang, Tao Zhang, Quanyan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16195">https://arxiv.org/abs/2404.16195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16195">https://arxiv.org/pdf/2404.16195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16195]] A Game-Theoretic Analysis of Auditing Differentially Private Algorithms  with Epistemically Disparate Herd(https://arxiv.org/abs/2404.16195)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Privacy-preserving AI algorithms are widely adopted in various domains, but the lack of transparency might pose accountability issues. While auditing algorithms can address this issue, machine-based audit approaches are often costly and time-consuming. Herd audit, on the other hand, offers an alternative solution by harnessing collective intelligence. Nevertheless, the presence of epistemic disparity among auditors, resulting in varying levels of expertise and access to knowledge, may impact audit performance. An effective herd audit will establish a credible accountability threat for algorithm developers, incentivizing them to uphold their claims. In this study, our objective is to develop a systematic framework that examines the impact of herd audits on algorithm developers using the Stackelberg game approach. The optimal strategy for auditors emphasizes the importance of easy access to relevant information, as it increases the auditors' confidence in the audit process. Similarly, the optimal choice for developers suggests that herd audit is viable when auditors face lower costs in acquiring knowledge. By enhancing transparency and accountability, herd audit contributes to the responsible development of privacy-preserving algorithms.</li>
</ul>

<h3>Title: Towards Efficient Patient Recruitment for Clinical Trials: Application  of a Prompt-Based Learning Model</h3>
<ul>
<li><strong>Authors: </strong>Mojdeh Rahmanian, Seyed Mostafa Fakhrahmad, Seyedeh Zahra Mousavi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16198">https://arxiv.org/abs/2404.16198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16198">https://arxiv.org/pdf/2404.16198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16198]] Towards Efficient Patient Recruitment for Clinical Trials: Application  of a Prompt-Based Learning Model(https://arxiv.org/abs/2404.16198)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Objective: Clinical trials are essential for advancing pharmaceutical interventions, but they face a bottleneck in selecting eligible participants. Although leveraging electronic health records (EHR) for recruitment has gained popularity, the complex nature of unstructured medical texts presents challenges in efficiently identifying participants. Natural Language Processing (NLP) techniques have emerged as a solution with a recent focus on transformer models. In this study, we aimed to evaluate the performance of a prompt-based large language model for the cohort selection task from unstructured medical notes collected in the EHR. Methods: To process the medical records, we selected the most related sentences of the records to the eligibility criteria needed for the trial. The SNOMED CT concepts related to each eligibility criterion were collected. Medical records were also annotated with MedCAT based on the SNOMED CT ontology. Annotated sentences including concepts matched with the criteria-relevant terms were extracted. A prompt-based large language model (Generative Pre-trained Transformer (GPT) in this study) was then used with the extracted sentences as the training set. To assess its effectiveness, we evaluated the model's performance using the dataset from the 2018 n2c2 challenge, which aimed to classify medical records of 311 patients based on 13 eligibility criteria through NLP techniques. Results: Our proposed model showed the overall micro and macro F measures of 0.9061 and 0.8060 which were among the highest scores achieved by the experiments performed with this dataset. Conclusion: The application of a prompt-based large language model in this study to classify patients based on eligibility criteria received promising scores. Besides, we proposed a method of extractive summarization with the aid of SNOMED CT ontology that can be also applied to other medical texts.</li>
</ul>

<h3>Title: An Analysis of Recent Advances in Deepfake Image Detection in an  Evolving Threat Landscape</h3>
<ul>
<li><strong>Authors: </strong>Sifat Muhammad Abdullah, Aravind Cheruvu, Shravya Kanchi, Taejoong Chung, Peng Gao, Murtuza Jadliwala, Bimal Viswanath</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16212">https://arxiv.org/abs/2404.16212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16212">https://arxiv.org/pdf/2404.16212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16212]] An Analysis of Recent Advances in Deepfake Image Detection in an  Evolving Threat Landscape(https://arxiv.org/abs/2404.16212)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, generative</a></li>
<li><strong>Abstract: </strong>Deepfake or synthetic images produced using deep generative models pose serious risks to online platforms. This has triggered several research efforts to accurately detect deepfake images, achieving excellent performance on publicly available deepfake datasets. In this work, we study 8 state-of-the-art detectors and argue that they are far from being ready for deployment due to two recent developments. First, the emergence of lightweight methods to customize large generative models, can enable an attacker to create many customized generators (to create deepfakes), thereby substantially increasing the threat surface. We show that existing defenses fail to generalize well to such \emph{user-customized generative models} that are publicly available today. We discuss new machine learning approaches based on content-agnostic features, and ensemble modeling to improve generalization performance against user-customized models. Second, the emergence of \textit{vision foundation models} -- machine learning models trained on broad data that can be easily adapted to several downstream tasks -- can be misused by attackers to craft adversarial deepfakes that can evade existing defenses. We propose a simple adversarial attack that leverages existing foundation models to craft adversarial samples \textit{without adding any adversarial noise}, through careful semantic manipulation of the image content. We highlight the vulnerabilities of several defenses against our attack, and explore directions leveraging advanced foundation models and adversarial training to defend against this new threat.</li>
</ul>

<h3>Title: Computational analysis of the language of pain: a systematic review</h3>
<ul>
<li><strong>Authors: </strong>Diogo A.P. Nunes, Joana Ferreira-Gomes, Fani Neto, David Martins de Matos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16226">https://arxiv.org/abs/2404.16226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16226">https://arxiv.org/pdf/2404.16226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16226]] Computational analysis of the language of pain: a systematic review(https://arxiv.org/abs/2404.16226)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Objectives: This study aims to systematically review the literature on the computational processing of the language of pain, whether generated by patients or physicians, identifying current trends and challenges. Methods: Following the PRISMA guidelines, a comprehensive literature search was conducted to select relevant studies on the computational processing of the language of pain and answer pre-defined research questions. Data extraction and synthesis were performed to categorize selected studies according to their primary purpose and outcome, patient and pain population, textual data, computational methodology, and outcome targets. Results: Physician-generated language of pain, specifically from clinical notes, was the most used data. Tasks included patient diagnosis and triaging, identification of pain mentions, treatment response prediction, biomedical entity extraction, correlation of linguistic features with clinical states, and lexico-semantic analysis of pain narratives. Only one study included previous linguistic knowledge on pain utterances in their experimental setup. Most studies targeted their outcomes for physicians, either directly as clinical tools or as indirect knowledge. The least targeted stage of clinical pain care was self-management, in which patients are most involved. The least studied dimensions of pain were affective and sociocultural. Only two studies measured how physician performance on clinical tasks improved with the inclusion of the proposed algorithm. Discussion: This study found that future research should focus on analyzing patient-generated language of pain, developing patient-centered resources for self-management and patient-empowerment, exploring affective and sociocultural aspects of pain, and measuring improvements in physician performance when aided by the proposed tools.</li>
</ul>

<h3>Title: SECO: Secure Inference With Model Splitting Across Multi-Server  Hierarchy</h3>
<ul>
<li><strong>Authors: </strong>Shuangyi Chen, Ashish Khisti</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16232">https://arxiv.org/abs/2404.16232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16232">https://arxiv.org/pdf/2404.16232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16232]] SECO: Secure Inference With Model Splitting Across Multi-Server  Hierarchy(https://arxiv.org/abs/2404.16232)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect</a></li>
<li><strong>Abstract: </strong>In the context of prediction-as-a-service, concerns about the privacy of the data and the model have been brought up and tackled via secure inference protocols. These protocols are built up by using single or multiple cryptographic tools designed under a variety of different security assumptions. In this paper, we introduce SECO, a secure inference protocol that enables a user holding an input data vector and multiple server nodes deployed with a split neural network model to collaboratively compute the prediction, without compromising either party's data privacy. We extend prior work on secure inference that requires the entire neural network model to be located on a single server node, to a multi-server hierarchy, where the user communicates to a gateway server node, which in turn communicates to remote server nodes. The inference task is split across the server nodes and must be performed over an encrypted copy of the data vector. We adopt multiparty homomorphic encryption and multiparty garbled circuit schemes, making the system secure against dishonest majority of semi-honest servers as well as protecting the partial model structure from the user. We evaluate SECO on multiple models, achieving the reduction of computation and communication cost for the user, making the protocol applicable to user's devices with limited resources.</li>
</ul>

<h3>Title: AutoGluon-Multimodal (AutoMM): Supercharging Multimodal AutoML with  Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiqiang Tang, Haoyang Fang, Su Zhou, Taojiannan Yang, Zihan Zhong, Tony Hu, Katrin Kirchhoff, George Karypis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16233">https://arxiv.org/abs/2404.16233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16233">https://arxiv.org/pdf/2404.16233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16233]] AutoGluon-Multimodal (AutoMM): Supercharging Multimodal AutoML with  Foundation Models(https://arxiv.org/abs/2404.16233)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>AutoGluon-Multimodal (AutoMM) is introduced as an open-source AutoML library designed specifically for multimodal learning. Distinguished by its exceptional ease of use, AutoMM enables fine-tuning of foundational models with just three lines of code. Supporting various modalities including image, text, and tabular data, both independently and in combination, the library offers a comprehensive suite of functionalities spanning classification, regression, object detection, semantic matching, and image segmentation. Experiments across diverse datasets and tasks showcases AutoMM's superior performance in basic classification and regression tasks compared to existing AutoML tools, while also demonstrating competitive results in advanced tasks, aligning with specialized toolboxes designed for such purposes.</li>
</ul>

<h3>Title: Synergizing Privacy and Utility in Data Analytics Through Advanced  Information Theorization</h3>
<ul>
<li><strong>Authors: </strong>Zahir Alsulaimawi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16241">https://arxiv.org/abs/2404.16241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16241">https://arxiv.org/pdf/2404.16241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16241]] Synergizing Privacy and Utility in Data Analytics Through Advanced  Information Theorization(https://arxiv.org/abs/2404.16241)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, extraction</a></li>
<li><strong>Abstract: </strong>This study develops a novel framework for privacy-preserving data analytics, addressing the critical challenge of balancing data utility with privacy concerns. We introduce three sophisticated algorithms: a Noise-Infusion Technique tailored for high-dimensional image data, a Variational Autoencoder (VAE) for robust feature extraction while masking sensitive attributes and an Expectation Maximization (EM) approach optimized for structured data privacy. Applied to datasets such as Modified MNIST and CelebrityA, our methods significantly reduce mutual information between sensitive attributes and transformed data, thereby enhancing privacy. Our experimental results confirm that these approaches achieve superior privacy protection and retain high utility, making them viable for practical applications where both aspects are crucial. The research contributes to the field by providing a flexible and effective strategy for deploying privacy-preserving algorithms across various data types and establishing new benchmarks for utility and confidentiality in data analytics.</li>
</ul>

<h3>Title: Investigating the prompt leakage effect and black-box defenses for  multi-turn LLM interactions</h3>
<ul>
<li><strong>Authors: </strong>Divyansh Agarwal, Alexander R. Fabbri, Philippe Laban, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16251">https://arxiv.org/abs/2404.16251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16251">https://arxiv.org/pdf/2404.16251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16251]] Investigating the prompt leakage effect and black-box defenses for  multi-turn LLM interactions(https://arxiv.org/abs/2404.16251)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Prompt leakage in large language models (LLMs) poses a significant security and privacy threat, particularly in retrieval-augmented generation (RAG) systems. However, leakage in multi-turn LLM interactions along with mitigation strategies has not been studied in a standardized manner. This paper investigates LLM vulnerabilities against prompt leakage across 4 diverse domains and 10 closed- and open-source LLMs. Our unique multi-turn threat model leverages the LLM's sycophancy effect and our analysis dissects task instruction and knowledge leakage in the LLM response. In a multi-turn setting, our threat model elevates the average attack success rate (ASR) to 86.2%, including a 99% leakage with GPT-4 and claude-1.3. We find that some black-box LLMs like Gemini show variable susceptibility to leakage across domains - they are more likely to leak contextual knowledge in the news domain compared to the medical domain. Our experiments measure specific effects of 6 black-box defense strategies, including a query-rewriter in the RAG scenario. Our proposed multi-tier combination of defenses still has an ASR of 5.3% for black-box LLMs, indicating room for enhancement and future direction for LLM security research.</li>
</ul>

<h3>Title: Enhancing Privacy in Face Analytics Using Fully Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Bharat Yalavarthi, Arjun Ramesh Kaushik, Arun Ross, Vishnu Boddeti, Nalini Ratha</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16255">https://arxiv.org/abs/2404.16255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16255">https://arxiv.org/pdf/2404.16255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16255]] Enhancing Privacy in Face Analytics Using Fully Homomorphic Encryption(https://arxiv.org/abs/2404.16255)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, biometric</a></li>
<li><strong>Abstract: </strong>Modern face recognition systems utilize deep neural networks to extract salient features from a face. These features denote embeddings in latent space and are often stored as templates in a face recognition system. These embeddings are susceptible to data leakage and, in some cases, can even be used to reconstruct the original face image. To prevent compromising identities, template protection schemes are commonly employed. However, these schemes may still not prevent the leakage of soft biometric information such as age, gender and race. To alleviate this issue, we propose a novel technique that combines Fully Homomorphic Encryption (FHE) with an existing template protection scheme known as PolyProtect. We show that the embeddings can be compressed and encrypted using FHE and transformed into a secure PolyProtect template using polynomial transformation, for additional protection. We demonstrate the efficacy of the proposed approach through extensive experiments on multiple datasets. Our proposed approach ensures irreversibility and unlinkability, effectively preventing the leakage of soft biometric attributes from face embeddings without compromising recognition accuracy.</li>
</ul>

<h3>Title: Probabilistic Tracker Management Policies for Low-Cost and Scalable  Rowhammer Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Aamer Jaleel, Stephen W. Keckler, Gururaj Saileshwar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16256">https://arxiv.org/abs/2404.16256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16256">https://arxiv.org/pdf/2404.16256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16256]] Probabilistic Tracker Management Policies for Low-Cost and Scalable  Rowhammer Mitigation(https://arxiv.org/abs/2404.16256)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack</a></li>
<li><strong>Abstract: </strong>This paper focuses on mitigating DRAM Rowhammer attacks. In recent years, solutions like TRR have been deployed in DDR4 DRAM to track aggressor rows and then issue a mitigative action by refreshing neighboring victim rows. Unfortunately, such in-DRAM solutions are resource-constrained (only able to provision few tens of counters to track aggressor rows) and are prone to thrashing based attacks, that have been used to fool them. Secure alternatives for in-DRAM trackers require tens of thousands of counters. In this work, we demonstrate secure and scalable rowhammer mitigation using resource-constrained trackers. Our key idea is to manage such trackers with probabilistic management policies (PROTEAS). PROTEAS includes component policies like request-stream sampling and random evictions which enable thrash-resistance for resource-constrained trackers. We show that PROTEAS can secure small in-DRAM trackers (with 16 counters per DRAM bank) even when Rowhammer thresholds drop to 500 while incurring less than 3% slowdown. Moreover, we show that PROTEAS significantly outperforms a recent similar probabilistic proposal from Samsung (called DSAC) while achieving 11X - 19X the resilience against Rowhammer.</li>
</ul>

<h3>Title: Interpreting Answers to Yes-No Questions in Dialogues from Multiple  Domains</h3>
<ul>
<li><strong>Authors: </strong>Zijie Wang, Farzana Rashid, Eduardo Blanco</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16262">https://arxiv.org/abs/2404.16262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16262">https://arxiv.org/pdf/2404.16262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16262]] Interpreting Answers to Yes-No Questions in Dialogues from Multiple  Domains(https://arxiv.org/abs/2404.16262)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>People often answer yes-no questions without explicitly saying yes, no, or similar polar keywords. Figuring out the meaning of indirect answers is challenging, even for large language models. In this paper, we investigate this problem working with dialogues from multiple domains. We present new benchmarks in three diverse domains: movie scripts, tennis interviews, and airline customer service. We present an approach grounded on distant supervision and blended training to quickly adapt to a new dialogue domain. Experimental results show that our approach is never detrimental and yields F1 improvements as high as 11-34%.</li>
</ul>

<h3>Title: A Multi-objective Optimization Benchmark Test Suite for Real-time  Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yifan Zhao, Zhenyu Liang, Zhichao Lu, Ran Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16266">https://arxiv.org/abs/2404.16266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16266">https://arxiv.org/pdf/2404.16266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16266]] A Multi-objective Optimization Benchmark Test Suite for Real-time  Semantic Segmentation(https://arxiv.org/abs/2404.16266)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>As one of the emerging challenges in Automated Machine Learning, the Hardware-aware Neural Architecture Search (HW-NAS) tasks can be treated as black-box multi-objective optimization problems (MOPs). An important application of HW-NAS is real-time semantic segmentation, which plays a pivotal role in autonomous driving scenarios. The HW-NAS for real-time semantic segmentation inherently needs to balance multiple optimization objectives, including model accuracy, inference speed, and hardware-specific considerations. Despite its importance, benchmarks have yet to be developed to frame such a challenging task as multi-objective optimization. To bridge the gap, we introduce a tailored streamline to transform the task of HW-NAS for real-time semantic segmentation into standard MOPs. Building upon the streamline, we present a benchmark test suite, CitySeg/MOP, comprising fifteen MOPs derived from the Cityscapes dataset. The CitySeg/MOP test suite is integrated into the EvoXBench platform to provide seamless interfaces with various programming languages (e.g., Python and MATLAB) for instant fitness evaluations. We comprehensively assessed the CitySeg/MOP test suite on various multi-objective evolutionary algorithms, showcasing its versatility and practicality. Source codes are available at https://github.com/EMI-Group/evoxbench.</li>
</ul>

<h3>Title: Lacunarity Pooling Layers for Plant Image Classification using Texture  Analysis</h3>
<ul>
<li><strong>Authors: </strong>Akshatha Mohan, Joshua Peeples</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16268">https://arxiv.org/abs/2404.16268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16268">https://arxiv.org/pdf/2404.16268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16268]] Lacunarity Pooling Layers for Plant Image Classification using Texture  Analysis(https://arxiv.org/abs/2404.16268)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Pooling layers (e.g., max and average) may overlook important information encoded in the spatial arrangement of pixel intensity and/or feature values. We propose a novel lacunarity pooling layer that aims to capture the spatial heterogeneity of the feature maps by evaluating the variability within local windows. The layer operates at multiple scales, allowing the network to adaptively learn hierarchical features. The lacunarity pooling layer can be seamlessly integrated into any artificial neural network architecture. Experimental results demonstrate the layer's effectiveness in capturing intricate spatial patterns, leading to improved feature extraction capabilities. The proposed approach holds promise in various domains, especially in agricultural image analysis tasks. This work contributes to the evolving landscape of artificial neural network architectures by introducing a novel pooling layer that enriches the representation of spatial features. Our code is publicly available.</li>
</ul>

<h3>Title: True random number generation using metastable 1T' molybdenum  ditelluride</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Pengyu Liu, Yingyi Wen, Zihan Liang, Songwei Liu, Lekai Song, Jingfang Pei, Xiaoyue Fan, Teng Ma, Gang Wang, Shuo Gao, Kong-Pang Pun, Xiaolong Chen, Guohua Hu</a></li>
<li><strong>Subjects: </strong>cs.CR, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16271">https://arxiv.org/abs/2404.16271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16271">https://arxiv.org/pdf/2404.16271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16271]] True random number generation using metastable 1T' molybdenum  ditelluride(https://arxiv.org/abs/2404.16271)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>True random numbers play a critical role in secure cryptography. The generation relies on a stable and readily extractable entropy source. Here, from solution-processed structurally metastable 1T' MoTe2, we prove stable output of featureless, stochastic, and yet stable conductance noise at a broad temperature (down to 15 K) with minimal power consumption (down to 0.05 micro-W). Our characterizations and statistical analysis of the characteristics of the conductance noise suggest that the noise arises from the volatility of the stochastic polarization of the underlying ferroelectric dipoles in the 1T' MoTe2. Further, as proved in our experiments and indicated by our Monte Carlo simulation, the ferroelectric dipole polarization is a reliable entropy source with the stochastic polarization persistent and stable over time. Exploiting the conductance noise, we achieve the generation of true random numbers and demonstrate their use in common cryptographic applications, for example, password generation and data encryption. Besides, particularly, we show a privacy safeguarding approach to sensitive data that can be critical for the cryptography of neural networks. We believe our work will bring insights into the understanding of the metastable 1T' MoTe2 and, more importantly, underpin its great potential in secure cryptography.</li>
</ul>

<h3>Title: LLM-Based Section Identifiers Excel on Open Source but Stumble in Real  World Applications</h3>
<ul>
<li><strong>Authors: </strong>Saranya Krishnamoorthy, Ayush Singh, Shabnam Tafreshi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16294">https://arxiv.org/abs/2404.16294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16294">https://arxiv.org/pdf/2404.16294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16294]] LLM-Based Section Identifiers Excel on Open Source but Stumble in Real  World Applications(https://arxiv.org/abs/2404.16294)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Electronic health records (EHR) even though a boon for healthcare practitioners, are growing convoluted and longer every day. Sifting around these lengthy EHRs is taxing and becomes a cumbersome part of physician-patient interaction. Several approaches have been proposed to help alleviate this prevalent issue either via summarization or sectioning, however, only a few approaches have truly been helpful in the past. With the rise of automated methods, machine learning (ML) has shown promise in solving the task of identifying relevant sections in EHR. However, most ML methods rely on labeled data which is difficult to get in healthcare. Large language models (LLMs) on the other hand, have performed impressive feats in natural language processing (NLP), that too in a zero-shot manner, i.e. without any labeled data. To that end, we propose using LLMs to identify relevant section headers. We find that GPT-4 can effectively solve the task on both zero and few-shot settings as well as segment dramatically better than state-of-the-art methods. Additionally, we also annotate a much harder real world dataset and find that GPT-4 struggles to perform well, alluding to further research and harder benchmarks.</li>
</ul>

<h3>Title: Research on Splicing Image Detection Algorithms Based on Natural Image  Statistical Characteristics</h3>
<ul>
<li><strong>Authors: </strong>Ao Xiang, Jingyu Zhang, Qin Yang, Liyang Wang, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16296">https://arxiv.org/abs/2404.16296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16296">https://arxiv.org/pdf/2404.16296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16296]] Research on Splicing Image Detection Algorithms Based on Natural Image  Statistical Characteristics(https://arxiv.org/abs/2404.16296)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>With the development and widespread application of digital image processing technology, image splicing has become a common method of image manipulation, raising numerous security and legal issues. This paper introduces a new splicing image detection algorithm based on the statistical characteristics of natural images, aimed at improving the accuracy and efficiency of splicing image detection. By analyzing the limitations of traditional methods, we have developed a detection framework that integrates advanced statistical analysis techniques and machine learning methods. The algorithm has been validated using multiple public datasets, showing high accuracy in detecting spliced edges and locating tampered areas, as well as good robustness. Additionally, we explore the potential applications and challenges faced by the algorithm in real-world scenarios. This research not only provides an effective technological means for the field of image tampering detection but also offers new ideas and methods for future related research.</li>
</ul>

<h3>Title: Reinforcement Learning with Generative Models for Compact Support Sets</h3>
<ul>
<li><strong>Authors: </strong>Nico Schiavone, Xingyu Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16300">https://arxiv.org/abs/2404.16300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16300">https://arxiv.org/pdf/2404.16300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16300]] Reinforcement Learning with Generative Models for Compact Support Sets(https://arxiv.org/abs/2404.16300)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Foundation models contain a wealth of information from their vast number of training samples. However, most prior arts fail to extract this information in a precise and efficient way for small sample sizes. In this work, we propose a framework utilizing reinforcement learning as a control for foundation models, allowing for the granular generation of small, focused synthetic support sets to augment the performance of neural network models on real data classification tasks. We first allow a reinforcement learning agent access to a novel context based dictionary; the agent then uses this dictionary with a novel prompt structure to form and optimize prompts as inputs to generative models, receiving feedback based on a reward function combining the change in validation accuracy and entropy. A support set is formed this way over several exploration steps. Our framework produced excellent results, increasing classification accuracy by significant margins for no additional labelling or data cost.</li>
</ul>

<h3>Title: Style Adaptation for Domain-adaptive Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ting Li, Jianshu Chao, Deyu An</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16301">https://arxiv.org/abs/2404.16301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16301">https://arxiv.org/pdf/2404.16301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16301]] Style Adaptation for Domain-adaptive Semantic Segmentation(https://arxiv.org/abs/2404.16301)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Unsupervised Domain Adaptation (UDA) refers to the method that utilizes annotated source domain data and unlabeled target domain data to train a model capable of generalizing to the target domain data. Domain discrepancy leads to a significant decrease in the performance of general network models trained on the source domain data when applied to the target domain. We introduce a straightforward approach to mitigate the domain discrepancy, which necessitates no additional parameter calculations and seamlessly integrates with self-training-based UDA methods. Through the transfer of the target domain style to the source domain in the latent feature space, the model is trained to prioritize the target domain style during the decision-making process. We tackle the problem at both the image-level and shallow feature map level by transferring the style information from the target domain to the source domain data. As a result, we obtain a model that exhibits superior performance on the target domain. Our method yields remarkable enhancements in the state-of-the-art performance for synthetic-to-real UDA tasks. For example, our proposed method attains a noteworthy UDA performance of 76.93 mIoU on the GTA->Cityscapes dataset, representing a notable improvement of +1.03 percentage points over the previous state-of-the-art results.</li>
</ul>

<h3>Title: CFMW: Cross-modality Fusion Mamba for Multispectral Object Detection  under Adverse Weather Conditions</h3>
<ul>
<li><strong>Authors: </strong>Haoyuan Li, Qi Hu, You Yao, Kailun Yang, Peng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM, cs.RO, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16302">https://arxiv.org/abs/2404.16302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16302">https://arxiv.org/pdf/2404.16302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16302]] CFMW: Cross-modality Fusion Mamba for Multispectral Object Detection  under Adverse Weather Conditions(https://arxiv.org/abs/2404.16302)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Cross-modality images that integrate visible-infrared spectra cues can provide richer complementary information for object detection. Despite this, existing visible-infrared object detection methods severely degrade in severe weather conditions. This failure stems from the pronounced sensitivity of visible images to environmental perturbations, such as rain, haze, and snow, which frequently cause false negatives and false positives in detection. To address this issue, we introduce a novel and challenging task, termed visible-infrared object detection under adverse weather conditions. To foster this task, we have constructed a new Severe Weather Visible-Infrared Dataset (SWVID) with diverse severe weather scenes. Furthermore, we introduce the Cross-modality Fusion Mamba with Weather-removal (CFMW) to augment detection accuracy in adverse weather conditions. Thanks to the proposed Weather Removal Diffusion Model (WRDM) and Cross-modality Fusion Mamba (CFM) modules, CFMW is able to mine more essential information of pedestrian features in cross-modality fusion, thus could transfer to other rarer scenarios with high efficiency and has adequate availability on those platforms with low computing power. To the best of our knowledge, this is the first study that targeted improvement and integrated both Diffusion and Mamba modules in cross-modality object detection, successfully expanding the practical application of this type of model with its higher accuracy and more advanced architecture. Extensive experiments on both well-recognized and self-created datasets conclusively demonstrate that our CFMW achieves state-of-the-art detection performance, surpassing existing benchmarks. The dataset and source code will be made publicly available at https://github.com/lhy-zjut/CFMW.</li>
</ul>

<h3>Title: BezierFormer: A Unified Architecture for 2D and 3D Lane Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Dong, Xi Zhu, Xiya Cao, Ran Ding, Wei Li, Caifa Zhou, Yongliang Wang, Qiangbo Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16304">https://arxiv.org/abs/2404.16304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16304">https://arxiv.org/pdf/2404.16304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16304]] BezierFormer: A Unified Architecture for 2D and 3D Lane Detection(https://arxiv.org/abs/2404.16304)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Lane detection has made significant progress in recent years, but there is not a unified architecture for its two sub-tasks: 2D lane detection and 3D lane detection. To fill this gap, we introduce B\'{e}zierFormer, a unified 2D and 3D lane detection architecture based on B\'{e}zier curve lane representation. B\'{e}zierFormer formulate queries as B\'{e}zier control points and incorporate a novel B\'{e}zier curve attention mechanism. This attention mechanism enables comprehensive and accurate feature extraction for slender lane curves via sampling and fusing multiple reference points on each curve. In addition, we propose a novel Chamfer IoU-based loss which is more suitable for the B\'{e}zier control points regression. The state-of-the-art performance of B\'{e}zierFormer on widely-used 2D and 3D lane detection benchmarks verifies its effectiveness and suggests the worthiness of further exploration.</li>
</ul>

<h3>Title: TI2V-Zero: Zero-Shot Image Conditioning for Text-to-Video Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Haomiao Ni, Bernhard Egger, Suhas Lohit, Anoop Cherian, Ye Wang, Toshiaki Koike-Akino, Sharon X. Huang, Tim K. Marks</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16306">https://arxiv.org/abs/2404.16306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16306">https://arxiv.org/pdf/2404.16306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16306]] TI2V-Zero: Zero-Shot Image Conditioning for Text-to-Video Diffusion  Models(https://arxiv.org/abs/2404.16306)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-conditioned image-to-video generation (TI2V) aims to synthesize a realistic video starting from a given image (e.g., a woman's photo) and a text description (e.g., "a woman is drinking water."). Existing TI2V frameworks often require costly training on video-text datasets and specific model designs for text and image conditioning. In this paper, we propose TI2V-Zero, a zero-shot, tuning-free method that empowers a pretrained text-to-video (T2V) diffusion model to be conditioned on a provided image, enabling TI2V generation without any optimization, fine-tuning, or introducing external modules. Our approach leverages a pretrained T2V diffusion foundation model as the generative prior. To guide video generation with the additional image input, we propose a "repeat-and-slide" strategy that modulates the reverse denoising process, allowing the frozen diffusion model to synthesize a video frame-by-frame starting from the provided image. To ensure temporal continuity, we employ a DDPM inversion strategy to initialize Gaussian noise for each newly synthesized frame and a resampling technique to help preserve visual details. We conduct comprehensive experiments on both domain-specific and open-domain datasets, where TI2V-Zero consistently outperforms a recent open-domain TI2V model. Furthermore, we show that TI2V-Zero can seamlessly extend to other tasks such as video infilling and prediction when provided with more images. Its autoregressive design also supports long video generation.</li>
</ul>

<h3>Title: DIG3D: Marrying Gaussian Splatting with Deformable Transformer for  Single Image 3D Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Jiamin Wu, Kenkun Liu, Han Gao, Xiaoke Jiang, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16323">https://arxiv.org/abs/2404.16323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16323">https://arxiv.org/pdf/2404.16323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16323]] DIG3D: Marrying Gaussian Splatting with Deformable Transformer for  Single Image 3D Reconstruction(https://arxiv.org/abs/2404.16323)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we study the problem of 3D reconstruction from a single-view RGB image and propose a novel approach called DIG3D for 3D object reconstruction and novel view synthesis. Our method utilizes an encoder-decoder framework which generates 3D Gaussians in decoder with the guidance of depth-aware image features from encoder. In particular, we introduce the use of deformable transformer, allowing efficient and effective decoding through 3D reference point and multi-layer refinement adaptations. By harnessing the benefits of 3D Gaussians, our approach offers an efficient and accurate solution for 3D reconstruction from single-view images. We evaluate our method on the ShapeNet SRN dataset, getting PSNR of 24.21 and 24.98 in car and chair dataset, respectively. The result outperforming the recent method by around 2.25%, demonstrating the effectiveness of our method in achieving superior results.</li>
</ul>

<h3>Title: Semantic Segmentation Refiner for Ultrasound Applications with Zero-Shot  Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Hedda Cohen Indelman, Elay Dahan, Angeles M. Perez-Agosto, Carmit Shiran, Doron Shaked, Nati Daniel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16325">https://arxiv.org/abs/2404.16325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16325">https://arxiv.org/pdf/2404.16325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16325]] Semantic Segmentation Refiner for Ultrasound Applications with Zero-Shot  Foundation Models(https://arxiv.org/abs/2404.16325)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of deep learning in medical imaging analysis, medical image segmentation remains challenging due to the scarcity of high-quality labeled images for supervision. Further, the significant domain gap between natural and medical images in general and ultrasound images in particular hinders fine-tuning models trained on natural images to the task at hand. In this work, we address the performance degradation of segmentation models in low-data regimes and propose a prompt-less segmentation method harnessing the ability of segmentation foundation models to segment abstract shapes. We do that via our novel prompt point generation algorithm which uses coarse semantic segmentation masks as input and a zero-shot prompt-able foundation model as an optimization target. We demonstrate our method on a segmentation findings task (pathologic anomalies) in ultrasound images. Our method's advantages are brought to light in varying degrees of low-data regime experiments on a small-scale musculoskeletal ultrasound images dataset, yielding a larger performance gain as the training set size decreases.</li>
</ul>

<h3>Title: FedStyle: Style-Based Federated Learning Crowdsourcing Framework for Art  Commissions</h3>
<ul>
<li><strong>Authors: </strong>Changjuan Ran, Yeting Guo, Fang Liu, Shenglan Cui, Yunfan Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16336">https://arxiv.org/abs/2404.16336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16336">https://arxiv.org/pdf/2404.16336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16336]] FedStyle: Style-Based Federated Learning Crowdsourcing Framework for Art  Commissions(https://arxiv.org/abs/2404.16336)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, generative</a></li>
<li><strong>Abstract: </strong>The unique artistic style is crucial to artists' occupational competitiveness, yet prevailing Art Commission Platforms rarely support style-based retrieval. Meanwhile, the fast-growing generative AI techniques aggravate artists' concerns about releasing personal artworks to public platforms. To achieve artistic style-based retrieval without exposing personal artworks, we propose FedStyle, a style-based federated learning crowdsourcing framework. It allows artists to train local style models and share model parameters rather than artworks for collaboration. However, most artists possess a unique artistic style, resulting in severe model drift among them. FedStyle addresses such extreme data heterogeneity by having artists learn their abstract style representations and align with the server, rather than merely aggregating model parameters lacking semantics. Besides, we introduce contrastive learning to meticulously construct the style representation space, pulling artworks with similar styles closer and keeping different ones apart in the embedding space. Extensive experiments on the proposed datasets demonstrate the superiority of FedStyle.</li>
</ul>

<h3>Title: Evolutionary Causal Discovery with Relative Impact Stratification for  Interpretable Data Analysis</h3>
<ul>
<li><strong>Authors: </strong>Ou Deng, Shoji Nishimura, Atsushi Ogihara, Qun Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE, cs.SC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16361">https://arxiv.org/abs/2404.16361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16361">https://arxiv.org/pdf/2404.16361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16361]] Evolutionary Causal Discovery with Relative Impact Stratification for  Interpretable Data Analysis(https://arxiv.org/abs/2404.16361)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This study proposes Evolutionary Causal Discovery (ECD) for causal discovery that tailors response variables, predictor variables, and corresponding operators to research datasets. Utilizing genetic programming for variable relationship parsing, the method proceeds with the Relative Impact Stratification (RIS) algorithm to assess the relative impact of predictor variables on the response variable, facilitating expression simplification and enhancing the interpretability of variable relationships. ECD proposes an expression tree to visualize the RIS results, offering a differentiated depiction of unknown causal relationships compared to conventional causal discovery. The ECD method represents an evolution and augmentation of existing causal discovery methods, providing an interpretable approach for analyzing variable relationships in complex systems, particularly in healthcare settings with Electronic Health Record (EHR) data. Experiments on both synthetic and real-world EHR datasets demonstrate the efficacy of ECD in uncovering patterns and mechanisms among variables, maintaining high accuracy and stability across different noise levels. On the real-world EHR dataset, ECD reveals the intricate relationships between the response variable and other predictive variables, aligning with the results of structural equation modeling and shapley additive explanations analyses.</li>
</ul>

<h3>Title: Byzantine Attacks Exploiting Penalties in Ethereum PoS</h3>
<ul>
<li><strong>Authors: </strong>Ulysse Pavloff, Yackolley Amoussou-Genou, Sara Tucci-Piergiovanni</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16363">https://arxiv.org/abs/2404.16363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16363">https://arxiv.org/pdf/2404.16363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16363]] Byzantine Attacks Exploiting Penalties in Ethereum PoS(https://arxiv.org/abs/2404.16363)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>In May 2023, the Ethereum blockchain experienced its first inactivity leak, a mechanism designed to reinstate chain finalization amid persistent network disruptions. This mechanism aims to reduce the voting power of validators who are unreachable within the network, reallocating this power to active validators. This paper investigates the implications of the inactivity leak on safety within the Ethereum blockchain. Our theoretical analysis reveals scenarios where actions by Byzantine validators expedite the finalization of two conflicting branches, and instances where Byzantine validators reach a voting power exceeding the critical safety threshold of one-third. Additionally, we revisit the probabilistic bouncing attack, illustrating how the inactivity leak can result in a probabilistic breach of safety, potentially allowing Byzantine validators to exceed the one-third safety threshold. Our findings uncover how penalizing inactive nodes can compromise blockchain properties, particularly in the presence of Byzantine validators capable of coordinating actions.</li>
</ul>

<h3>Title: Learning Syntax Without Planting Trees: Understanding When and Why  Transformers Generalize Hierarchically</h3>
<ul>
<li><strong>Authors: </strong>Kabir Ahuja, Vidhisha Balachandran, Madhur Panwar, Tianxing He, Noah A. Smith, Navin Goyal, Yulia Tsvetkov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16367">https://arxiv.org/abs/2404.16367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16367">https://arxiv.org/pdf/2404.16367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16367]] Learning Syntax Without Planting Trees: Understanding When and Why  Transformers Generalize Hierarchically(https://arxiv.org/abs/2404.16367)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers trained on natural language data have been shown to learn its hierarchical structure and generalize to sentences with unseen syntactic structures without explicitly encoding any structural bias. In this work, we investigate sources of inductive bias in transformer models and their training that could cause such generalization behavior to emerge. We extensively experiment with transformer models trained on multiple synthetic datasets and with different training objectives and show that while other objectives e.g. sequence-to-sequence modeling, prefix language modeling, often failed to lead to hierarchical generalization, models trained with the language modeling objective consistently learned to generalize hierarchically. We then conduct pruning experiments to study how transformers trained with the language modeling objective encode hierarchical structure. When pruned, we find joint existence of subnetworks within the model with different generalization behaviors (subnetworks corresponding to hierarchical structure and linear order). Finally, we take a Bayesian perspective to further uncover transformers' preference for hierarchical generalization: We establish a correlation between whether transformers generalize hierarchically on a dataset and whether the simplest explanation of that dataset is provided by a hierarchical grammar compared to regular grammars exhibiting linear generalization.</li>
</ul>

<h3>Title: Don't Say No: Jailbreaking LLM by Suppressing Refusal</h3>
<ul>
<li><strong>Authors: </strong>Yukai Zhou, Wenjie Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16369">https://arxiv.org/abs/2404.16369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16369">https://arxiv.org/pdf/2404.16369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16369]] Don't Say No: Jailbreaking LLM by Suppressing Refusal(https://arxiv.org/abs/2404.16369)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Ensuring the safety alignment of Large Language Models (LLMs) is crucial to generating responses consistent with human values. Despite their ability to recognize and avoid harmful queries, LLMs are vulnerable to "jailbreaking" attacks, where carefully crafted prompts elicit them to produce toxic content. One category of jailbreak attacks is reformulating the task as adversarial attacks by eliciting the LLM to generate an affirmative response. However, the typical attack in this category GCG has very limited attack success rate. In this study, to better study the jailbreak attack, we introduce the DSN (Don't Say No) attack, which prompts LLMs to not only generate affirmative responses but also novelly enhance the objective to suppress refusals. In addition, another challenge lies in jailbreak attacks is the evaluation, as it is difficult to directly and accurately assess the harmfulness of the attack. The existing evaluation such as refusal keyword matching has its own limitation as it reveals numerous false positive and false negative instances. To overcome this challenge, we propose an ensemble evaluation pipeline incorporating Natural Language Inference (NLI) contradiction assessment and two external LLM evaluators. Extensive experiments demonstrate the potency of the DSN and the effectiveness of ensemble evaluation compared to baseline methods.</li>
</ul>

<h3>Title: Multimodal Information Interaction for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xinxin Fan, Lin Liu, Haoran Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16371">https://arxiv.org/abs/2404.16371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16371">https://arxiv.org/pdf/2404.16371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16371]] Multimodal Information Interaction for Medical Image Segmentation(https://arxiv.org/abs/2404.16371)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The use of multimodal data in assisted diagnosis and segmentation has emerged as a prominent area of interest in current research. However, one of the primary challenges is how to effectively fuse multimodal features. Most of the current approaches focus on the integration of multimodal features while ignoring the correlation and consistency between different modal features, leading to the inclusion of potentially irrelevant information. To address this issue, we introduce an innovative Multimodal Information Cross Transformer (MicFormer), which employs a dual-stream architecture to simultaneously extract features from each modality. Leveraging the Cross Transformer, it queries features from one modality and retrieves corresponding responses from another, facilitating effective communication between bimodal features. Additionally, we incorporate a deformable Transformer architecture to expand the search space. We conducted experiments on the MM-WHS dataset, and in the CT-MRI multimodal image segmentation task, we successfully improved the whole-heart segmentation DICE score to 85.57 and MIoU to 75.51. Compared to other multimodal segmentation techniques, our method outperforms by margins of 2.83 and 4.23, respectively. This demonstrates the efficacy of MicFormer in integrating relevant information between different modalities in multimodal tasks. These findings hold significant implications for multimodal image tasks, and we believe that MicFormer possesses extensive potential for broader applications across various domains. Access to our method is available at https://github.com/fxxJuses/MICFormer</li>
</ul>

<h3>Title: List Items One by One: A New Data Source and Learning Paradigm for  Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>An Yan, Zhengyuan Yang, Junda Wu, Wanrong Zhu, Jianwei Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Julian McAuley, Jianfeng Gao, Lijuan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16375">https://arxiv.org/abs/2404.16375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16375">https://arxiv.org/pdf/2404.16375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16375]] List Items One by One: A New Data Source and Learning Paradigm for  Multimodal LLMs(https://arxiv.org/abs/2404.16375)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Set-of-Mark (SoM) Prompting unleashes the visual grounding capability of GPT-4V, by enabling the model to associate visual objects with tags inserted on the image. These tags, marked with alphanumerics, can be indexed via text tokens for easy reference. Despite the extraordinary performance from GPT-4V, we observe that other Multimodal Large Language Models (MLLMs) struggle to understand these visual tags. To promote the learning of SoM prompting for open-source models, we propose a new learning paradigm: "list items one by one," which asks the model to enumerate and describe all visual tags placed on the image following the alphanumeric orders of tags. By integrating our curated dataset with other visual instruction tuning datasets, we are able to equip existing MLLMs with the SoM prompting ability. Furthermore, we evaluate our finetuned SoM models on five MLLM benchmarks. We find that this new dataset, even in a relatively small size (10k-30k images with tags), significantly enhances visual reasoning capabilities and reduces hallucinations for MLLMs. Perhaps surprisingly, these improvements persist even when the visual tags are omitted from input images during inference. This suggests the potential of "list items one by one" as a new paradigm for training MLLMs, which strengthens the object-text alignment through the use of visual tags in the training stage. Finally, we conduct analyses by probing trained models to understand the working mechanism of SoM. Our code and data are available at \url{https://github.com/zzxslp/SoM-LLaVA}.</li>
</ul>

<h3>Title: Efficient Higher-order Convolution for Small Kernels in Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Zuocheng Wen, Lingzhong Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16380">https://arxiv.org/abs/2404.16380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16380">https://arxiv.org/pdf/2404.16380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16380]] Efficient Higher-order Convolution for Small Kernels in Deep Learning(https://arxiv.org/abs/2404.16380)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Deep convolutional neural networks (DCNNs) are a class of artificial neural networks, primarily for computer vision tasks such as segmentation and classification. Many nonlinear operations, such as activation functions and pooling strategies, are used in DCNNs to enhance their ability to process different signals with different tasks. Conceptional convolution, a linear filter, is the essential component of DCNNs while nonlinear convolution is generally implemented as higher-order Volterra filters, However, for Volterra filtering, significant memory and computational costs pose a primary limitation for its widespread application in DCNN applications. In this study, we propose a novel method to perform higher-order Volterra filtering with lower memory and computation cost in forward and backward pass in DCNN training. The proposed method demonstrates computational advantages compared with conventional Volterra filter implementation. Furthermore, based on the proposed method, a new attention module called Higher-order Local Attention Block (HLA) is proposed and tested on CIFAR-100 dataset, which shows competitive improvement for classification task. Source code is available at: https://github.com/WinterWen666/Efficient-High-Order-Volterra-Convolution.git</li>
</ul>

<h3>Title: Promoting CNNs with Cross-Architecture Knowledge Distillation for  Efficient Monocular Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zhimeng Zheng, Tao Huang, Gongsheng Li, Zuyi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16386">https://arxiv.org/abs/2404.16386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16386">https://arxiv.org/pdf/2404.16386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16386]] Promoting CNNs with Cross-Architecture Knowledge Distillation for  Efficient Monocular Depth Estimation(https://arxiv.org/abs/2404.16386)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recently, the performance of monocular depth estimation (MDE) has been significantly boosted with the integration of transformer models. However, the transformer models are usually computationally-expensive, and their effectiveness in light-weight models are limited compared to convolutions. This limitation hinders their deployment on resource-limited devices. In this paper, we propose a cross-architecture knowledge distillation method for MDE, dubbed DisDepth, to enhance efficient CNN models with the supervision of state-of-the-art transformer models. Concretely, we first build a simple framework of convolution-based MDE, which is then enhanced with a novel local-global convolution module to capture both local and global information in the image. To effectively distill valuable information from the transformer teacher and bridge the gap between convolution and transformer features, we introduce a method to acclimate the teacher with a ghost decoder. The ghost decoder is a copy of the student's decoder, and adapting the teacher with the ghost decoder aligns the features to be student-friendly while preserving their original performance. Furthermore, we propose an attentive knowledge distillation loss that adaptively identifies features valuable for depth estimation. This loss guides the student to focus more on attentive regions, improving its performance. Extensive experiments on KITTI and NYU Depth V2 datasets demonstrate the effectiveness of DisDepth. Our method achieves significant improvements on various efficient backbones, showcasing its potential for efficient monocular depth estimation.</li>
</ul>

<h3>Title: Asking and Answering Questions to Extract Event-Argument Structures</h3>
<ul>
<li><strong>Authors: </strong>Md Nayem Uddin, Enfa Rose George, Eduardo Blanco, Steven Corman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16413">https://arxiv.org/abs/2404.16413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16413">https://arxiv.org/pdf/2404.16413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16413]] Asking and Answering Questions to Extract Event-Argument Structures(https://arxiv.org/abs/2404.16413)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a question-answering approach to extract document-level event-argument structures. We automatically ask and answer questions for each argument type an event may have. Questions are generated using manually defined templates and generative transformers. Template-based questions are generated using predefined role-specific wh-words and event triggers from the context document. Transformer-based questions are generated using large language models trained to formulate questions based on a passage and the expected answer. Additionally, we develop novel data augmentation strategies specialized in inter-sentential event-argument relations. We use a simple span-swapping technique, coreference resolution, and large language models to augment the training instances. Our approach enables transfer learning without any corpora-specific modifications and yields competitive results with the RAMS dataset. It outperforms previous work, and it is especially beneficial to extract arguments that appear in different sentences than the event trigger. We also present detailed quantitative and qualitative analyses shedding light on the most common errors made by our best model.</li>
</ul>

<h3>Title: SynCellFactory: Generative Data Augmentation for Cell Tracking</h3>
<ul>
<li><strong>Authors: </strong>Moritz Sturm, Lorenzo Cerrone, Fred A. Hamprecht</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16421">https://arxiv.org/abs/2404.16421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16421">https://arxiv.org/pdf/2404.16421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16421]] SynCellFactory: Generative Data Augmentation for Cell Tracking(https://arxiv.org/abs/2404.16421)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Cell tracking remains a pivotal yet challenging task in biomedical research. The full potential of deep learning for this purpose is often untapped due to the limited availability of comprehensive and varied training data sets. In this paper, we present SynCellFactory, a generative cell video augmentation. At the heart of SynCellFactory lies the ControlNet architecture, which has been fine-tuned to synthesize cell imagery with photorealistic accuracy in style and motion patterns. This technique enables the creation of synthetic yet realistic cell videos that mirror the complexity of authentic microscopy time-lapses. Our experiments demonstrate that SynCellFactory boosts the performance of well-established deep learning models for cell tracking, particularly when original training data is sparse.</li>
</ul>

<h3>Title: Robust Fine-tuning for Pre-trained 3D Point Cloud Models</h3>
<ul>
<li><strong>Authors: </strong>Zhibo Zhang, Ximing Yang, Weizhong Zhang, Cheng Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16422">https://arxiv.org/abs/2404.16422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16422">https://arxiv.org/pdf/2404.16422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16422]] Robust Fine-tuning for Pre-trained 3D Point Cloud Models(https://arxiv.org/abs/2404.16422)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a robust fine-tuning method designed for pre-trained 3D point cloud models, to enhance feature robustness in downstream fine-tuned models. We highlight the limitations of current fine-tuning methods and the challenges of learning robust models. The proposed method, named Weight-Space Ensembles for Fine-Tuning then Linear Probing (WiSE-FT-LP), integrates the original pre-training and fine-tuning models through weight space integration followed by Linear Probing. This approach significantly enhances the performance of downstream fine-tuned models under distribution shifts, improving feature robustness while maintaining high performance on the target distribution. We apply this robust fine-tuning method to mainstream 3D point cloud pre-trained models and evaluate the quality of model parameters and the degradation of downstream task performance. Experimental results demonstrate the effectiveness of WiSE-FT-LP in enhancing model robustness, effectively balancing downstream task performance and model feature robustness without altering the model structures.</li>
</ul>

<h3>Title: Contextual Categorization Enhancement through LLMs Latent-Space</h3>
<ul>
<li><strong>Authors: </strong>Zineddine Bettouche, Anas Safi, Andreas Fischer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16442">https://arxiv.org/abs/2404.16442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16442">https://arxiv.org/pdf/2404.16442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16442]] Contextual Categorization Enhancement through LLMs Latent-Space(https://arxiv.org/abs/2404.16442)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Managing the semantic quality of the categorization in large textual datasets, such as Wikipedia, presents significant challenges in terms of complexity and cost. In this paper, we propose leveraging transformer models to distill semantic information from texts in the Wikipedia dataset and its associated categories into a latent space. We then explore different approaches based on these encodings to assess and enhance the semantic identity of the categories. Our graphical approach is powered by Convex Hull, while we utilize Hierarchical Navigable Small Worlds (HNSWs) for the hierarchical approach. As a solution to the information loss caused by the dimensionality reduction, we modulate the following mathematical solution: an exponential decay function driven by the Euclidean distances between the high-dimensional encodings of the textual categories. This function represents a filter built around a contextual category and retrieves items with a certain Reconsideration Probability (RP). Retrieving high-RP items serves as a tool for database administrators to improve data groupings by providing recommendations and identifying outliers within a contextual framework.</li>
</ul>

<h3>Title: Automating the Discovery of Partial Differential Equations in Dynamical  Systems</h3>
<ul>
<li><strong>Authors: </strong>Weizhen Li, Rui Carvalho</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS, stat.AP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16444">https://arxiv.org/abs/2404.16444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16444">https://arxiv.org/pdf/2404.16444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16444]] Automating the Discovery of Partial Differential Equations in Dynamical  Systems(https://arxiv.org/abs/2404.16444)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Identifying partial differential equations (PDEs) from data is crucial for understanding the governing mechanisms of natural phenomena, yet it remains a challenging task. We present an extension to the ARGOS framework, ARGOS-RAL, which leverages sparse regression with the recurrent adaptive lasso to identify PDEs from limited prior knowledge automatically. Our method automates calculating partial derivatives, constructing a candidate library, and estimating a sparse model. We rigorously evaluate the performance of ARGOS-RAL in identifying canonical PDEs under various noise levels and sample sizes, demonstrating its robustness in handling noisy and non-uniformly distributed data. We also test the algorithm's performance on datasets consisting solely of random noise to simulate scenarios with severely compromised data quality. Our results show that ARGOS-RAL effectively and reliably identifies the underlying PDEs from data, outperforming the sequential threshold ridge regression method in most cases. We highlight the potential of combining statistical methods, machine learning, and dynamical systems theory to automatically discover governing equations from collected data, streamlining the scientific modeling process.</li>
</ul>

<h3>Title: PAD: Patch-Agnostic Defense against Adversarial Patch Attacks</h3>
<ul>
<li><strong>Authors: </strong>Lihua Jing, Rui Wang, Wenqi Ren, Xin Dong, Cong Zou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16452">https://arxiv.org/abs/2404.16452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16452">https://arxiv.org/pdf/2404.16452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16452]] PAD: Patch-Agnostic Defense against Adversarial Patch Attacks(https://arxiv.org/abs/2404.16452)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Adversarial patch attacks present a significant threat to real-world object detectors due to their practical feasibility. Existing defense methods, which rely on attack data or prior knowledge, struggle to effectively address a wide range of adversarial patches. In this paper, we show two inherent characteristics of adversarial patches, semantic independence and spatial heterogeneity, independent of their appearance, shape, size, quantity, and location. Semantic independence indicates that adversarial patches operate autonomously within their semantic context, while spatial heterogeneity manifests as distinct image quality of the patch area that differs from original clean image due to the independent generation process. Based on these observations, we propose PAD, a novel adversarial patch localization and removal method that does not require prior knowledge or additional training. PAD offers patch-agnostic defense against various adversarial patches, compatible with any pre-trained object detectors. Our comprehensive digital and physical experiments involving diverse patch types, such as localized noise, printable, and naturalistic patches, exhibit notable improvements over state-of-the-art works. Our code is available at https://github.com/Lihua-Jing/PAD.</li>
</ul>

<h3>Title: Large Language Models Perform on Par with Experts Identifying Mental  Health Factors in Adolescent Online Forums</h3>
<ul>
<li><strong>Authors: </strong>Isablle Lorge, Dam W. Joyce, Andrey Kormilitzin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16461">https://arxiv.org/abs/2404.16461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16461">https://arxiv.org/pdf/2404.16461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16461]] Large Language Models Perform on Par with Experts Identifying Mental  Health Factors in Adolescent Online Forums(https://arxiv.org/abs/2404.16461)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Mental health in children and adolescents has been steadily deteriorating over the past few years [ 1 ]. The recent advent of Large Language Models (LLMs) offers much hope for cost and time efficient scaling of monitoring and intervention, yet despite specifically prevalent issues such as school bullying and eating disorders, previous studies on have not investigated performance in this domain or for open information extraction where the set of answers is not predetermined. We create a new dataset of Reddit posts from adolescents aged 12-19 annotated by expert psychiatrists for the following categories: TRAUMA, PRECARITY, CONDITION, SYMPTOMS, SUICIDALITY and TREATMENT and compare expert labels to annotations from two top performing LLMs (GPT3.5 and GPT4). In addition, we create two synthetic datasets to assess whether LLMs perform better when annotating data as they generate it. We find GPT4 to be on par with human inter-annotator agreement and performance on synthetic data to be substantially higher, however we find the model still occasionally errs on issues of negation and factuality and higher performance on synthetic data is driven by greater complexity of real data rather than inherent advantage.</li>
</ul>

<h3>Title: COBRA -- COnfidence score Based on shape Regression Analysis for  method-independent quality assessment of object pose estimation from single  images</h3>
<ul>
<li><strong>Authors: </strong>Panagiotis Sapoutzoglou, Georgios Giapitzakis Tzintanos, George Terzakis, Maria Pateraki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16471">https://arxiv.org/abs/2404.16471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16471">https://arxiv.org/pdf/2404.16471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16471]] COBRA -- COnfidence score Based on shape Regression Analysis for  method-independent quality assessment of object pose estimation from single  images(https://arxiv.org/abs/2404.16471)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present a generic algorithm for scoring pose estimation methods that rely on single image semantic analysis. The algorithm employs a lightweight putative shape representation using a combination of multiple Gaussian Processes. Each Gaussian Process (GP) yields distance normal distributions from multiple reference points in the object's coordinate system to its surface, thus providing a geometric evaluation framework for scoring predicted poses. Our confidence measure comprises the average mixture probability of pixel back-projections onto the shape template. In the reported experiments, we compare the accuracy of our GP based representation of objects versus the actual geometric models and demonstrate the ability of our method to capture the influence of outliers as opposed to the corresponding intrinsic measures that ship with the segmentation and pose estimation methods.</li>
</ul>

<h3>Title: DiffSeg: A Segmentation Model for Skin Lesions Based on Diffusion  Difference</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Shuai, Yinan Chen, Shunqiang Mao, Yihan Zho, Xiaohong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16474">https://arxiv.org/abs/2404.16474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16474">https://arxiv.org/pdf/2404.16474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16474]] DiffSeg: A Segmentation Model for Skin Lesions Based on Diffusion  Difference(https://arxiv.org/abs/2404.16474)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Weakly supervised medical image segmentation (MIS) using generative models is crucial for clinical diagnosis. However, the accuracy of the segmentation results is often limited by insufficient supervision and the complex nature of medical imaging. Existing models also only provide a single outcome, which does not allow for the measurement of uncertainty. In this paper, we introduce DiffSeg, a segmentation model for skin lesions based on diffusion difference which exploits diffusion model principles to ex-tract noise-based features from images with diverse semantic information. By discerning difference between these noise features, the model identifies diseased areas. Moreover, its multi-output capability mimics doctors' annotation behavior, facilitating the visualization of segmentation result consistency and ambiguity. Additionally, it quantifies output uncertainty using Generalized Energy Distance (GED), aiding interpretability and decision-making for physicians. Finally, the model integrates outputs through the Dense Conditional Random Field (DenseCRF) algorithm to refine the segmentation boundaries by considering inter-pixel correlations, which improves the accuracy and optimizes the segmentation results. We demonstrate the effectiveness of DiffSeg on the ISIC 2018 Challenge dataset, outperforming state-of-the-art U-Net-based methods.</li>
</ul>

<h3>Title: Evaluating Consistency and Reasoning Capabilities of Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Yash Saxena, Sarthak Chopra, Arunendra Mani Tripathi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16478">https://arxiv.org/abs/2404.16478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16478">https://arxiv.org/pdf/2404.16478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16478]] Evaluating Consistency and Reasoning Capabilities of Large Language  Models(https://arxiv.org/abs/2404.16478)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are extensively used today across various sectors, including academia, research, business, and finance, for tasks such as text generation, summarization, and translation. Despite their widespread adoption, these models often produce incorrect and misleading information, exhibiting a tendency to hallucinate. This behavior can be attributed to several factors, with consistency and reasoning capabilities being significant contributors. LLMs frequently lack the ability to generate explanations and engage in coherent reasoning, leading to inaccurate responses. Moreover, they exhibit inconsistencies in their outputs. This paper aims to evaluate and compare the consistency and reasoning capabilities of both public and proprietary LLMs. The experiments utilize the Boolq dataset as the ground truth, comprising questions, answers, and corresponding explanations. Queries from the dataset are presented as prompts to the LLMs, and the generated responses are evaluated against the ground truth answers. Additionally, explanations are generated to assess the models' reasoning abilities. Consistency is evaluated by repeatedly presenting the same query to the models and observing for variations in their responses. For measuring reasoning capabilities, the generated explanations are compared to the ground truth explanations using metrics such as BERT, BLEU, and F-1 scores. The findings reveal that proprietary models generally outperform public models in terms of both consistency and reasoning capabilities. However, even when presented with basic general knowledge questions, none of the models achieved a score of 90\% in both consistency and reasoning. This study underscores the direct correlation between consistency and reasoning abilities in LLMs and highlights the inherent reasoning challenges present in current language models.</li>
</ul>

<h3>Title: T-Explainer: A Model-Agnostic Explainability Framework Based on  Gradients</h3>
<ul>
<li><strong>Authors: </strong>Evandro S. Ortigossa, Fábio F. Dias, Brian Barr, Claudio T. Silva, Luis Gustavo Nonato</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16495">https://arxiv.org/abs/2404.16495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16495">https://arxiv.org/pdf/2404.16495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16495]] T-Explainer: A Model-Agnostic Explainability Framework Based on  Gradients(https://arxiv.org/abs/2404.16495)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>The development of machine learning applications has increased significantly in recent years, motivated by the remarkable ability of learning-powered systems to discover and generalize intricate patterns hidden in massive datasets. Modern learning models, while powerful, often exhibit a level of complexity that renders them opaque black boxes, resulting in a notable lack of transparency that hinders our ability to decipher their decision-making processes. Opacity challenges the interpretability and practical application of machine learning, especially in critical domains where understanding the underlying reasons is essential for informed decision-making. Explainable Artificial Intelligence (XAI) rises to meet that challenge, unraveling the complexity of black boxes by providing elucidating explanations. Among the various XAI approaches, feature attribution/importance XAI stands out for its capacity to delineate the significance of input features in the prediction process. However, most existing attribution methods have limitations, such as instability, when divergent explanations may result from similar or even the same instance. In this work, we introduce T-Explainer, a novel local additive attribution explainer based on Taylor expansion endowed with desirable properties, such as local accuracy and consistency, while stable over multiple runs. We demonstrate T-Explainer's effectiveness through benchmark experiments with well-known attribution methods. In addition, T-Explainer is developed as a comprehensive XAI framework comprising quantitative metrics to assess and visualize attribution explanations.</li>
</ul>

<h3>Title: 360SFUDA++: Towards Source-free UDA for Panoramic Segmentation by  Learning Reliable Category Prototypes</h3>
<ul>
<li><strong>Authors: </strong>Xu Zheng, Pengyuan Zhou, Athanasios V. Vasilakos, Lin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16501">https://arxiv.org/abs/2404.16501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16501">https://arxiv.org/pdf/2404.16501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16501]] 360SFUDA++: Towards Source-free UDA for Panoramic Segmentation by  Learning Reliable Category Prototypes(https://arxiv.org/abs/2404.16501)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we address the challenging source-free unsupervised domain adaptation (SFUDA) for pinhole-to-panoramic semantic segmentation, given only a pinhole image pre-trained model (i.e., source) and unlabeled panoramic images (i.e., target). Tackling this problem is non-trivial due to three critical challenges: 1) semantic mismatches from the distinct Field-of-View (FoV) between domains, 2) style discrepancies inherent in the UDA problem, and 3) inevitable distortion of the panoramic images. To tackle these problems, we propose 360SFUDA++ that effectively extracts knowledge from the source pinhole model with only unlabeled panoramic images and transfers the reliable knowledge to the target panoramic domain. Specifically, we first utilize Tangent Projection (TP) as it has less distortion and meanwhile slits the equirectangular projection (ERP) to patches with fixed FoV projection (FFP) to mimic the pinhole images. Both projections are shown effective in extracting knowledge from the source model. However, as the distinct projections make it less possible to directly transfer knowledge between domains, we then propose Reliable Panoramic Prototype Adaptation Module (RP2AM) to transfer knowledge at both prediction and prototype levels. RP$^2$AM selects the confident knowledge and integrates panoramic prototypes for reliable knowledge adaptation. Moreover, we introduce Cross-projection Dual Attention Module (CDAM), which better aligns the spatial and channel characteristics across projections at the feature level between domains. Both knowledge extraction and transfer processes are synchronously updated to reach the best performance. Extensive experiments on the synthetic and real-world benchmarks, including outdoor and indoor scenarios, demonstrate that our 360SFUDA++ achieves significantly better performance than prior SFUDA methods.</li>
</ul>

<h3>Title: Building a Japanese Document-Level Relation Extraction Dataset Assisted  by Cross-Lingual Transfer</h3>
<ul>
<li><strong>Authors: </strong>Youmi Ma, An Wang, Naoaki Okazaki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16506">https://arxiv.org/abs/2404.16506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16506">https://arxiv.org/pdf/2404.16506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16506]] Building a Japanese Document-Level Relation Extraction Dataset Assisted  by Cross-Lingual Transfer(https://arxiv.org/abs/2404.16506)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Document-level Relation Extraction (DocRE) is the task of extracting all semantic relationships from a document. While studies have been conducted on English DocRE, limited attention has been given to DocRE in non-English languages. This work delves into effectively utilizing existing English resources to promote DocRE studies in non-English languages, with Japanese as the representative case. As an initial attempt, we construct a dataset by transferring an English dataset to Japanese. However, models trained on such a dataset suffer from low recalls. We investigate the error cases and attribute the failure to different surface structures and semantics of documents translated from English and those written by native speakers. We thus switch to explore if the transferred dataset can assist human annotation on Japanese documents. In our proposal, annotators edit relation predictions from a model trained on the transferred dataset. Quantitative analysis shows that relation recommendations suggested by the model help reduce approximately 50% of the human edit steps compared with the previous approach. Experiments quantify the performance of existing DocRE models on our collected dataset, portraying the challenges of Japanese and cross-lingual DocRE.</li>
</ul>

<h3>Title: Global Concept Explanations for Graphs by Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Jonas Teufel, Pascal Friederich</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16532">https://arxiv.org/abs/2404.16532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16532">https://arxiv.org/pdf/2404.16532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16532]] Global Concept Explanations for Graphs by Contrastive Learning(https://arxiv.org/abs/2404.16532)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, explainability</a></li>
<li><strong>Abstract: </strong>Beyond improving trust and validating model fairness, xAI practices also have the potential to recover valuable scientific insights in application domains where little to no prior human intuition exists. To that end, we propose a method to extract global concept explanations from the predictions of graph neural networks to develop a deeper understanding of the tasks underlying structure-property relationships. We identify concept explanations as dense clusters in the self-explaining Megan models subgraph latent space. For each concept, we optimize a representative prototype graph and optionally use GPT-4 to provide hypotheses about why each structure has a certain effect on the prediction. We conduct computational experiments on synthetic and real-world graph property prediction tasks. For the synthetic tasks we find that our method correctly reproduces the structural rules by which they were created. For real-world molecular property regression and classification tasks, we find that our method rediscovers established rules of thumb. More specifically, our results for molecular mutagenicity prediction indicate more fine-grained resolution of structural details than existing explainability methods, consistent with previous results from chemistry literature. Overall, our results show promising capability to extract the underlying structure-property relationships for complex graph property prediction tasks.</li>
</ul>

<h3>Title: 3D Face Modeling via Weakly-supervised Disentanglement Network joint  Identity-consistency Prior</h3>
<ul>
<li><strong>Authors: </strong>Guohao Li, Hongyu Yang, Di Huang, Yunhong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16536">https://arxiv.org/abs/2404.16536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16536">https://arxiv.org/pdf/2404.16536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16536]] 3D Face Modeling via Weakly-supervised Disentanglement Network joint  Identity-consistency Prior(https://arxiv.org/abs/2404.16536)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative 3D face models featuring disentangled controlling factors hold immense potential for diverse applications in computer vision and computer graphics. However, previous 3D face modeling methods face a challenge as they demand specific labels to effectively disentangle these factors. This becomes particularly problematic when integrating multiple 3D face datasets to improve the generalization of the model. Addressing this issue, this paper introduces a Weakly-Supervised Disentanglement Framework, denoted as WSDF, to facilitate the training of controllable 3D face models without an overly stringent labeling requirement. Adhering to the paradigm of Variational Autoencoders (VAEs), the proposed model achieves disentanglement of identity and expression controlling factors through a two-branch encoder equipped with dedicated identity-consistency prior. It then faithfully re-entangles these factors via a tensor-based combination mechanism. Notably, the introduction of the Neutral Bank allows precise acquisition of subject-specific information using only identity labels, thereby averting degeneration due to insufficient supervision. Additionally, the framework incorporates a label-free second-order loss function for the expression factor to regulate deformation space and eliminate extraneous information, resulting in enhanced disentanglement. Extensive experiments have been conducted to substantiate the superior performance of WSDF. Our code is available at https://github.com/liguohao96/WSDF.</li>
</ul>

<h3>Title: OpenDlign: Enhancing Open-World 3D Learning with Depth-Aligned Images</h3>
<ul>
<li><strong>Authors: </strong>Ye Mao, Junpeng Jing, Krystian Mikolajczyk</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16538">https://arxiv.org/abs/2404.16538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16538">https://arxiv.org/pdf/2404.16538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16538]] OpenDlign: Enhancing Open-World 3D Learning with Depth-Aligned Images(https://arxiv.org/abs/2404.16538)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in Vision and Language Models (VLMs) have improved open-world 3D representation, facilitating 3D zero-shot capability in unseen categories. Existing open-world methods pre-train an extra 3D encoder to align features from 3D data (e.g., depth maps or point clouds) with CAD-rendered images and corresponding texts. However, the limited color and texture variations in CAD images can compromise the alignment robustness. Furthermore, the volume discrepancy between pre-training datasets of the 3D encoder and VLM leads to sub-optimal 2D to 3D knowledge transfer. To overcome these issues, we propose OpenDlign, a novel framework for learning open-world 3D representations, that leverages depth-aligned images generated from point cloud-projected depth maps. Unlike CAD-rendered images, our generated images provide rich, realistic color and texture diversity while preserving geometric and semantic consistency with the depth maps. OpenDlign also optimizes depth map projection and integrates depth-specific text prompts, improving 2D VLM knowledge adaptation for 3D learning efficient fine-tuning. Experimental results show that OpenDlign significantly outperforms existing benchmarks in zero-shot and few-shot 3D tasks, exceeding prior scores by 8.0% on ModelNet40 and 16.4% on OmniObject3D with just 6 million tuned parameters. Moreover, integrating generated depth-aligned images into existing 3D learning pipelines consistently improves their performance.</li>
</ul>

<h3>Title: Conditional Distribution Modelling for Few-Shot Image Synthesis with  Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Parul Gupta, Munawar Hayat, Abhinav Dhall, Thanh-Toan Do</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16556">https://arxiv.org/abs/2404.16556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16556">https://arxiv.org/pdf/2404.16556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16556]] Conditional Distribution Modelling for Few-Shot Image Synthesis with  Diffusion Models(https://arxiv.org/abs/2404.16556)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Few-shot image synthesis entails generating diverse and realistic images of novel categories using only a few example images. While multiple recent efforts in this direction have achieved impressive results, the existing approaches are dependent only upon the few novel samples available at test time in order to generate new images, which restricts the diversity of the generated images. To overcome this limitation, we propose Conditional Distribution Modelling (CDM) -- a framework which effectively utilizes Diffusion models for few-shot image generation. By modelling the distribution of the latent space used to condition a Diffusion process, CDM leverages the learnt statistics of the training data to get a better approximation of the unseen class distribution, thereby removing the bias arising due to limited number of few shot samples. Simultaneously, we devise a novel inversion based optimization strategy that further improves the approximated unseen class distribution, and ensures the fidelity of the generated samples to the unseen class. The experimental results on four benchmark datasets demonstrate the effectiveness of our proposed CDM for few-shot generation.</li>
</ul>

<h3>Title: Energy-Latency Manipulation of Multi-modal Large Language Models via  Verbose Samples</h3>
<ul>
<li><strong>Authors: </strong>Kuofeng Gao, Jindong Gu, Yang Bai, Shu-Tao Xia, Philip Torr, Wei Liu, Zhifeng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16557">https://arxiv.org/abs/2404.16557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16557">https://arxiv.org/pdf/2404.16557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16557]] Energy-Latency Manipulation of Multi-modal Large Language Models via  Verbose Samples(https://arxiv.org/abs/2404.16557)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the exceptional performance of multi-modal large language models (MLLMs), their deployment requires substantial computational resources. Once malicious users induce high energy consumption and latency time (energy-latency cost), it will exhaust computational resources and harm availability of service. In this paper, we investigate this vulnerability for MLLMs, particularly image-based and video-based ones, and aim to induce high energy-latency cost during inference by crafting an imperceptible perturbation. We find that high energy-latency cost can be manipulated by maximizing the length of generated sequences, which motivates us to propose verbose samples, including verbose images and videos. Concretely, two modality non-specific losses are proposed, including a loss to delay end-of-sequence (EOS) token and an uncertainty loss to increase the uncertainty over each generated token. In addition, improving diversity is important to encourage longer responses by increasing the complexity, which inspires the following modality specific loss. For verbose images, a token diversity loss is proposed to promote diverse hidden states. For verbose videos, a frame feature diversity loss is proposed to increase the feature diversity among frames. To balance these losses, we propose a temporal weight adjustment algorithm. Experiments demonstrate that our verbose samples can largely extend the length of generated sequences.</li>
</ul>

<h3>Title: DeepKalPose: An Enhanced Deep-Learning Kalman Filter for Temporally  Consistent Monocular Vehicle Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Leandro Di Bella, Yangxintong Lyu, Adrian Munteanu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16558">https://arxiv.org/abs/2404.16558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16558">https://arxiv.org/pdf/2404.16558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16558]] DeepKalPose: An Enhanced Deep-Learning Kalman Filter for Temporally  Consistent Monocular Vehicle Pose Estimation(https://arxiv.org/abs/2404.16558)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents DeepKalPose, a novel approach for enhancing temporal consistency in monocular vehicle pose estimation applied on video through a deep-learning-based Kalman Filter. By integrating a Bi-directional Kalman filter strategy utilizing forward and backward time-series processing, combined with a learnable motion model to represent complex motion patterns, our method significantly improves pose accuracy and robustness across various conditions, particularly for occluded or distant vehicles. Experimental validation on the KITTI dataset confirms that DeepKalPose outperforms existing methods in both pose accuracy and temporal consistency.</li>
</ul>

<h3>Title: Research on geometric figure classification algorithm based on Deep  Learning</h3>
<ul>
<li><strong>Authors: </strong>Ruiyang Wang, Haonan Wang, Junfeng Sun, Mingjia Zhao, Meng Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16561">https://arxiv.org/abs/2404.16561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16561">https://arxiv.org/pdf/2404.16561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16561]] Research on geometric figure classification algorithm based on Deep  Learning(https://arxiv.org/abs/2404.16561)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In recent years, with the rapid development of computer information technology, the development of artificial intelligence has been accelerating. The traditional geometry recognition technology is relatively backward and the recognition rate is low. In the face of massive information database, the traditional algorithm model inevitably has the problems of low recognition accuracy and poor performance. Deep learning theory has gradually become a very important part of machine learning. The implementation of convolutional neural network (CNN) reduces the difficulty of graphics generation algorithm. In this paper, using the advantages of lenet-5 architecture sharing weights and feature extraction and classification, the proposed geometric pattern recognition algorithm model is faster in the training data set. By constructing the shared feature parameters of the algorithm model, the cross-entropy loss function is used in the recognition process to improve the generalization of the model and improve the average recognition accuracy of the test data set.</li>
</ul>

<h3>Title: Evaluating Large Language Models on Time Series Feature Understanding: A  Comprehensive Taxonomy and Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Elizabeth Fons, Rachneet Kaur, Soham Palande, Zhen Zeng, Svitlana Vyetrenko, Tucker Balch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16563">https://arxiv.org/abs/2404.16563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16563">https://arxiv.org/pdf/2404.16563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16563]] Evaluating Large Language Models on Time Series Feature Understanding: A  Comprehensive Taxonomy and Benchmark(https://arxiv.org/abs/2404.16563)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) offer the potential for automatic time series analysis and reporting, which is a critical task across many domains, spanning healthcare, finance, climate, energy, and many more. In this paper, we propose a framework for rigorously evaluating the capabilities of LLMs on time series understanding, encompassing both univariate and multivariate forms. We introduce a comprehensive taxonomy of time series features, a critical framework that delineates various characteristics inherent in time series data. Leveraging this taxonomy, we have systematically designed and synthesized a diverse dataset of time series, embodying the different outlined features. This dataset acts as a solid foundation for assessing the proficiency of LLMs in comprehending time series. Our experiments shed light on the strengths and limitations of state-of-the-art LLMs in time series understanding, revealing which features these models readily comprehend effectively and where they falter. In addition, we uncover the sensitivity of LLMs to factors including the formatting of the data, the position of points queried within a series and the overall time series length.</li>
</ul>

<h3>Title: MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth  Estimation of Endoscopic Images</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Wang, Ying Zhou, Shiquan He, Ting Li, Yitong Zhang, Xinxia Feng, Mei Liu, Qiang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16571">https://arxiv.org/abs/2404.16571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16571">https://arxiv.org/pdf/2404.16571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16571]] MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth  Estimation of Endoscopic Images(https://arxiv.org/abs/2404.16571)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Photometric constraint is indispensable for self-supervised monocular depth estimation. It involves warping a source image onto a target view using estimated depth&pose, and then minimizing the difference between the warped and target images. However, the endoscopic built-in light causes significant brightness fluctuations, and thus makes the photometric constraint unreliable. Previous efforts only mitigate this relying on extra models to calibrate image brightness. In this paper, we propose MonoPCC to address the brightness inconsistency radically by reshaping the photometric constraint into a cycle form. Instead of only warping the source image, MonoPCC constructs a closed loop consisting of two opposite forward-backward warping paths: from target to source and then back to target. Thus, the target image finally receives an image cycle-warped from itself, which naturally makes the constraint invariant to brightness changes. Moreover, MonoPCC transplants the source image's phase-frequency into the intermediate warped image to avoid structure lost, and also stabilizes the training via an exponential moving average (EMA) strategy to avoid frequent changes in the forward warping. The comprehensive and extensive experimental results on three datasets demonstrate that our proposed MonoPCC shows a great robustness to the brightness inconsistency, and exceeds other state-of-the-arts by reducing the absolute relative error by at least 7.27%.</li>
</ul>

<h3>Title: Multi-Scale Representations by Varying Window Attention for Semantic  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Haotian Yan, Ming Wu, Chuang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16573">https://arxiv.org/abs/2404.16573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16573">https://arxiv.org/pdf/2404.16573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16573]] Multi-Scale Representations by Varying Window Attention for Semantic  Segmentation(https://arxiv.org/abs/2404.16573)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Multi-scale learning is central to semantic segmentation. We visualize the effective receptive field (ERF) of canonical multi-scale representations and point out two risks in learning them: scale inadequacy and field inactivation. A novel multi-scale learner, varying window attention (VWA), is presented to address these issues. VWA leverages the local window attention (LWA) and disentangles LWA into the query window and context window, allowing the context's scale to vary for the query to learn representations at multiple scales. However, varying the context to large-scale windows (enlarging ratio R) can significantly increase the memory footprint and computation cost (R^2 times larger than LWA). We propose a simple but professional re-scaling strategy to zero the extra induced cost without compromising performance. Consequently, VWA uses the same cost as LWA to overcome the receptive limitation of the local window. Furthermore, depending on VWA and employing various MLPs, we introduce a multi-scale decoder (MSD), VWFormer, to improve multi-scale representations for semantic segmentation. VWFormer achieves efficiency competitive with the most compute-friendly MSDs, like FPN and MLP decoder, but performs much better than any MSDs. For instance, using nearly half of UPerNet's computation, VWFormer outperforms it by 1.0%-2.5% mIoU on ADE20K. With little extra overhead, ~10G FLOPs, Mask2Former armed with VWFormer improves by 1.0%-1.3%.</li>
</ul>

<h3>Title: Exploring Internal Numeracy in Language Models: A Case Study on ALBERT</h3>
<ul>
<li><strong>Authors: </strong>Ulme Wennberg, Gustav Eje Henter</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16574">https://arxiv.org/abs/2404.16574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16574">https://arxiv.org/pdf/2404.16574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16574]] Exploring Internal Numeracy in Language Models: A Case Study on ALBERT(https://arxiv.org/abs/2404.16574)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>It has been found that Transformer-based language models have the ability to perform basic quantitative reasoning. In this paper, we propose a method for studying how these models internally represent numerical data, and use our proposal to analyze the ALBERT family of language models. Specifically, we extract the learned embeddings these models use to represent tokens that correspond to numbers and ordinals, and subject these embeddings to Principal Component Analysis (PCA). PCA results reveal that ALBERT models of different sizes, trained and initialized separately, consistently learn to use the axes of greatest variation to represent the approximate ordering of various numerical concepts. Numerals and their textual counterparts are represented in separate clusters, but increase along the same direction in 2D space. Our findings illustrate that language models, trained purely to model text, can intuit basic mathematical concepts, opening avenues for NLP applications that intersect with quantitative reasoning.</li>
</ul>

<h3>Title: Road Surface Friction Estimation for Winter Conditions Utilising General  Visual Features</h3>
<ul>
<li><strong>Authors: </strong>Risto Ojala, Eerik Alamikkotervo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16578">https://arxiv.org/abs/2404.16578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16578">https://arxiv.org/pdf/2404.16578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16578]] Road Surface Friction Estimation for Winter Conditions Utilising General  Visual Features(https://arxiv.org/abs/2404.16578)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>In below freezing winter conditions, road surface friction can greatly vary based on the mixture of snow, ice, and water on the road. Friction between the road and vehicle tyres is a critical parameter defining vehicle dynamics, and therefore road surface friction information is essential to acquire for several intelligent transportation applications, such as safe control of automated vehicles or alerting drivers of slippery road conditions. This paper explores computer vision-based evaluation of road surface friction from roadside cameras. Previous studies have extensively investigated the application of convolutional neural networks for the task of evaluating the road surface condition from images. Here, we propose a hybrid deep learning architecture, WCamNet, consisting of a pretrained visual transformer model and convolutional blocks. The motivation of the architecture is to combine general visual features provided by the transformer model, as well as finetuned feature extraction properties of the convolutional blocks. To benchmark the approach, an extensive dataset was gathered from national Finnish road infrastructure network of roadside cameras and optical road surface friction sensors. Acquired results highlight that the proposed WCamNet outperforms previous approaches in the task of predicting the road surface friction from the roadside camera images.</li>
</ul>

<h3>Title: Understanding Privacy Risks of Embeddings Induced by Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Zhu, Ninglu Shao, Defu Lian, Chenwang Wu, Zheng Liu, Yi Yang, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16587">https://arxiv.org/abs/2404.16587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16587">https://arxiv.org/pdf/2404.16587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16587]] Understanding Privacy Risks of Embeddings Induced by Large Language  Models(https://arxiv.org/abs/2404.16587)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) show early signs of artificial general intelligence but struggle with hallucinations. One promising solution to mitigate these hallucinations is to store external knowledge as embeddings, aiding LLMs in retrieval-augmented generation. However, such a solution risks compromising privacy, as recent studies experimentally showed that the original text can be partially reconstructed from text embeddings by pre-trained language models. The significant advantage of LLMs over traditional pre-trained models may exacerbate these concerns. To this end, we investigate the effectiveness of reconstructing original knowledge and predicting entity attributes from these embeddings when LLMs are employed. Empirical findings indicate that LLMs significantly improve the accuracy of two evaluated tasks over those from pre-trained models, regardless of whether the texts are in-distribution or out-of-distribution. This underscores a heightened potential for LLMs to jeopardize user privacy, highlighting the negative consequences of their widespread use. We further discuss preliminary strategies to mitigate this risk.</li>
</ul>

<h3>Title: SFMViT: SlowFast Meet ViT in Chaotic World</h3>
<ul>
<li><strong>Authors: </strong>Jiaying Lin, Jiajun Wen, Mengyuan Liu, Jinfu Liu, Baiqiao Yin, Yue Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16609">https://arxiv.org/abs/2404.16609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16609">https://arxiv.org/pdf/2404.16609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16609]] SFMViT: SlowFast Meet ViT in Chaotic World(https://arxiv.org/abs/2404.16609)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The task of spatiotemporal action localization in chaotic scenes is a challenging task toward advanced video understanding. Paving the way with high-quality video feature extraction and enhancing the precision of detector-predicted anchors can effectively improve model performance. To this end, we propose a high-performance dual-stream spatiotemporal feature extraction network SFMViT with an anchor pruning strategy. The backbone of our SFMViT is composed of ViT and SlowFast with prior knowledge of spatiotemporal action localization, which fully utilizes ViT's excellent global feature extraction capabilities and SlowFast's spatiotemporal sequence modeling capabilities. Secondly, we introduce the confidence maximum heap to prune the anchors detected in each frame of the picture to filter out the effective anchors. These designs enable our SFMViT to achieve a mAP of 26.62% in the Chaotic World dataset, far exceeding existing models. Code is available at https://github.com/jfightyr/SlowFast-Meet-ViT.</li>
</ul>

<h3>Title: MuseumMaker: Continual Style Customization without Catastrophic  Forgetting</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Liu, Gan Sun, Wenqi Liang, Jiahua Dong, Can Qin, Yang Cong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16612">https://arxiv.org/abs/2404.16612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16612">https://arxiv.org/pdf/2404.16612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16612]] MuseumMaker: Continual Style Customization without Catastrophic  Forgetting(https://arxiv.org/abs/2404.16612)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Pre-trained large text-to-image (T2I) models with an appropriate text prompt has attracted growing interests in customized images generation field. However, catastrophic forgetting issue make it hard to continually synthesize new user-provided styles while retaining the satisfying results amongst learned styles. In this paper, we propose MuseumMaker, a method that enables the synthesis of images by following a set of customized styles in a never-end manner, and gradually accumulate these creative artistic works as a Museum. When facing with a new customization style, we develop a style distillation loss module to transfer the style of the whole dataset into generation of images. It can minimize the learning biases caused by content of images, and address the catastrophic overfitting issue induced by few-shot images. To deal with catastrophic forgetting amongst past learned styles, we devise a dual regularization for shared-LoRA module to optimize the direction of model update, which could regularize the diffusion model from both weight and feature aspects, respectively. Meanwhile, a unique token embedding corresponding to this new style is learned by a task-wise token learning module, which could preserve historical knowledge from past styles with the limitation of LoRA parameter quantity. As any new user-provided style come, our MuseumMaker can capture the nuances of the new styles while maintaining the details of learned styles. Experimental results on diverse style datasets validate the effectiveness of our proposed MuseumMaker method, showcasing its robustness and versatility across various scenarios.</li>
</ul>

<h3>Title: Robust Capped lp-Norm Support Vector Ordinal Regression</h3>
<ul>
<li><strong>Authors: </strong>Haorui Xiang, Zhichang Wu, Guoxu Li, Rong Wang, Feiping Nie, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16616">https://arxiv.org/abs/2404.16616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16616">https://arxiv.org/pdf/2404.16616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16616]] Robust Capped lp-Norm Support Vector Ordinal Regression(https://arxiv.org/abs/2404.16616)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Ordinal regression is a specialized supervised problem where the labels show an inherent order. The order distinguishes it from normal multi-class problem. Support Vector Ordinal Regression, as an outstanding ordinal regression model, is widely used in many ordinal regression tasks. However, like most supervised learning algorithms, the design of SVOR is based on the assumption that the training data are real and reliable, which is difficult to satisfy in real-world data. In many practical applications, outliers are frequently present in the training set, potentially leading to misguide the learning process, such that the performance is non-optimal. In this paper, we propose a novel capped $\ell_{p}$-norm loss function that is theoretically robust to both light and heavy outliers. The capped $\ell_{p}$-norm loss can help the model detect and eliminate outliers during training process. Adhering to this concept, we introduce a new model, Capped $\ell_{p}$-Norm Support Vector Ordinal Regression(CSVOR), that is robust to outliers. CSVOR uses a weight matrix to detect and eliminate outliers during the training process to improve the robustness to outliers. Moreover, a Re-Weighted algorithm algorithm which is illustrated convergence by our theoretical results is proposed to effectively minimize the corresponding problem. Extensive experimental results demonstrate that our model outperforms state-of-the-art(SOTA) methods, particularly in the presence of outliers.</li>
</ul>

<h3>Title: Denoising: from classical methods to deep CNNs</h3>
<ul>
<li><strong>Authors: </strong>Jean-Eric Campagne</a></li>
<li><strong>Subjects: </strong>cs.CV, math.HO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16617">https://arxiv.org/abs/2404.16617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16617">https://arxiv.org/pdf/2404.16617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16617]] Denoising: from classical methods to deep CNNs(https://arxiv.org/abs/2404.16617)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper aims to explore the evolution of image denoising in a pedagological way. We briefly review classical methods such as Fourier analysis and wavelet bases, highlighting the challenges they faced until the emergence of neural networks, notably the U-Net, in the 2010s. The remarkable performance of these networks has been demonstrated in studies such as Kadkhodaie et al. (2024). They exhibit adaptability to various image types, including those with fixed regularity, facial images, and bedroom scenes, achieving optimal results and biased towards geometry-adaptive harmonic basis. The introduction of score diffusion has played a crucial role in image generation. In this context, denoising becomes essential as it facilitates the estimation of probability density scores. We discuss the prerequisites for genuine learning of probability densities, offering insights that extend from mathematical research to the implications of universal structures.</li>
</ul>

<h3>Title: Hippocrates: An Open-Source Framework for Advancing Large Language  Models in Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Emre Can Acikgoz, Osman Batur İnce, Rayene Bench, Arda Anıl Boz, İlker Kesen, Aykut Erdem, Erkut Erdem</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16621">https://arxiv.org/abs/2404.16621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16621">https://arxiv.org/pdf/2404.16621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16621]] Hippocrates: An Open-Source Framework for Advancing Large Language  Models in Healthcare(https://arxiv.org/abs/2404.16621)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The integration of Large Language Models (LLMs) into healthcare promises to transform medical diagnostics, research, and patient care. Yet, the progression of medical LLMs faces obstacles such as complex training requirements, rigorous evaluation demands, and the dominance of proprietary models that restrict academic exploration. Transparent, comprehensive access to LLM resources is essential for advancing the field, fostering reproducibility, and encouraging innovation in healthcare AI. We present Hippocrates, an open-source LLM framework specifically developed for the medical domain. In stark contrast to previous efforts, it offers unrestricted access to its training datasets, codebase, checkpoints, and evaluation protocols. This open approach is designed to stimulate collaborative research, allowing the community to build upon, refine, and rigorously evaluate medical LLMs within a transparent ecosystem. Also, we introduce Hippo, a family of 7B models tailored for the medical domain, fine-tuned from Mistral and LLaMA2 through continual pre-training, instruction tuning, and reinforcement learning from human and AI feedback. Our models outperform existing open medical LLMs models by a large-margin, even surpassing models with 70B parameters. Through Hippocrates, we aspire to unlock the full potential of LLMs not just to advance medical knowledge and patient care but also to democratize the benefits of AI research in healthcare, making them available across the globe.</li>
</ul>

<h3>Title: Introducing Systems Thinking as a Framework for Teaching and Assessing  Threat Modeling Competency</h3>
<ul>
<li><strong>Authors: </strong>Siddhant S. Joshi, Preeti Mukherjee, Kirsten A. Davis, James C. Davis</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16632">https://arxiv.org/abs/2404.16632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16632">https://arxiv.org/pdf/2404.16632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16632]] Introducing Systems Thinking as a Framework for Teaching and Assessing  Threat Modeling Competency(https://arxiv.org/abs/2404.16632)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Computing systems face diverse and substantial cybersecurity threats. To mitigate these cybersecurity threats, software engineers need to be competent in the skill of threat modeling. In industry and academia, there are many frameworks for teaching threat modeling, but our analysis of these frameworks suggests that (1) these approaches tend to be focused on component-level analysis rather than educating students to reason holistically about a system's cybersecurity, and (2) there is no rubric for assessing a student's threat modeling competency. To address these concerns, we propose using systems thinking in conjunction with popular and industry-standard threat modeling frameworks like STRIDE for teaching and assessing threat modeling competency. Prior studies suggest a holistic approach, like systems thinking, can help understand and mitigate cybersecurity threats. Thus, we developed and piloted two novel rubrics - one for assessing STRIDE threat modeling performance and the other for assessing systems thinking performance while conducting STRIDE. To conduct this study, we piloted the two rubrics mentioned above to assess threat model artifacts of students enrolled in an upper-level software engineering course at Purdue University in Fall 2021, Spring 2023, and Fall 2023. Students who had both systems thinking and STRIDE instruction identified and attempted to mitigate component-level as well as systems-level threats. Students with only STRIDE instruction tended to focus on identifying and mitigating component-level threats and discounted system-level threats. We contribute to engineering education by: (1) describing a new rubric for assessing threat modeling based on systems thinking; (2) identifying trends and blindspots in students' threat modeling approach; and (3) envisioning the benefits of integrating systems thinking in threat modeling teaching and assessment.</li>
</ul>

<h3>Title: Self-Balanced R-CNN for Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Rossi, Akbar Karimi, Andrea Prati</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16633">https://arxiv.org/abs/2404.16633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16633">https://arxiv.org/pdf/2404.16633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16633]] Self-Balanced R-CNN for Instance Segmentation(https://arxiv.org/abs/2404.16633)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Current state-of-the-art two-stage models on instance segmentation task suffer from several types of imbalances. In this paper, we address the Intersection over the Union (IoU) distribution imbalance of positive input Regions of Interest (RoIs) during the training of the second stage. Our Self-Balanced R-CNN (SBR-CNN), an evolved version of the Hybrid Task Cascade (HTC) model, brings brand new loop mechanisms of bounding box and mask refinements. With an improved Generic RoI Extraction (GRoIE), we also address the feature-level imbalance at the Feature Pyramid Network (FPN) level, originated by a non-uniform integration between low- and high-level features from the backbone layers. In addition, the redesign of the architecture heads toward a fully convolutional approach with FCC further reduces the number of parameters and obtains more clues to the connection between the task to solve and the layers used. Moreover, our SBR-CNN model shows the same or even better improvements if adopted in conjunction with other state-of-the-art models. In fact, with a lightweight ResNet-50 as backbone, evaluated on COCO minival 2017 dataset, our model reaches 45.3% and 41.5% AP for object detection and instance segmentation, with 12 epochs and without extra tricks. The code is available at https://github.com/IMPLabUniPr/mmdetection/tree/sbr_cnn</li>
</ul>

<h3>Title: TinyChart: Efficient Chart Understanding with Visual Token Merging and  Program-of-Thoughts Learning</h3>
<ul>
<li><strong>Authors: </strong>Liang Zhang, Anwen Hu, Haiyang Xu, Ming Yan, Yichen Xu, Qin Jin, Ji Zhang, Fei Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16635">https://arxiv.org/abs/2404.16635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16635">https://arxiv.org/pdf/2404.16635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16635]] TinyChart: Efficient Chart Understanding with Visual Token Merging and  Program-of-Thoughts Learning(https://arxiv.org/abs/2404.16635)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Charts are important for presenting and explaining complex data relationships. Recently, multimodal large language models (MLLMs) have shown remarkable capabilities in various chart understanding tasks. However, the sheer size of these models in terms of parameters and computational requirements limits their use in resource-constrained environments. In this paper, we present TinyChart, an efficient MLLM for chart understanding with only 3B parameters. TinyChart overcomes two key challenges in efficient chart understanding: (1) reduce the burden of learning numerical computations through a Program-of-Thoughts (PoT) learning strategy, which trains the model to generate Python programs for numerical calculations, and (2) reduce lengthy vision feature sequences produced by the vision transformer for high-resolution images through a Vision Token Merging module, which gradually merges most similar vision tokens. Extensive experiments demonstrate that our 3B TinyChart achieves SOTA performance on a variety of chart understanding benchmarks including ChartQA, Chart-to-Text, Chart-to-Table, OpenCQA, and ChartX. It outperforms several chart understanding MLLM with up to 13B parameters such as ChartLlama and ChartAst, and close-sourced general-purpose MLLM GPT-4V on ChartQA. It also demonstrates its superior efficiency with higher throughput during inference due to a smaller model scale and more efficient vision encoding. Our code and model are available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart.</li>
</ul>

<h3>Title: Privacy-Preserving Statistical Data Generation: Application to Sepsis  Detection</h3>
<ul>
<li><strong>Authors: </strong>Eric Macias-Fassio, Aythami Morales, Cristina Pruenza, Julian Fierrez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16638">https://arxiv.org/abs/2404.16638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16638">https://arxiv.org/pdf/2404.16638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16638]] Privacy-Preserving Statistical Data Generation: Application to Sepsis  Detection(https://arxiv.org/abs/2404.16638)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>The biomedical field is among the sectors most impacted by the increasing regulation of Artificial Intelligence (AI) and data protection legislation, given the sensitivity of patient information. However, the rise of synthetic data generation methods offers a promising opportunity for data-driven technologies. In this study, we propose a statistical approach for synthetic data generation applicable in classification problems. We assess the utility and privacy implications of synthetic data generated by Kernel Density Estimator and K-Nearest Neighbors sampling (KDE-KNN) within a real-world context, specifically focusing on its application in sepsis detection. The detection of sepsis is a critical challenge in clinical practice due to its rapid progression and potentially life-threatening consequences. Moreover, we emphasize the benefits of KDE-KNN compared to current synthetic data generation methodologies. Additionally, our study examines the effects of incorporating synthetic data into model training procedures. This investigation provides valuable insights into the effectiveness of synthetic data generation techniques in mitigating regulatory constraints within the biomedical field.</li>
</ul>

<h3>Title: Tele-FLM Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Chao Wang, Xinzhang Liu, Zihan Wang, Yu Zhao, Xin Wang, Yuyao Huang, Shuangyong Song, Yongxiang Li, Zheng Zhang, Bo Zhao, Aixin Sun, Yequan Wang, Zhongjiang He, Zhongyuan Wang, Xuelong Li, Tiejun Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16645">https://arxiv.org/abs/2404.16645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16645">https://arxiv.org/pdf/2404.16645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16645]] Tele-FLM Technical Report(https://arxiv.org/abs/2404.16645)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have showcased profound capabilities in language understanding and generation, facilitating a wide array of applications. However, there is a notable paucity of detailed, open-sourced methodologies on efficiently scaling LLMs beyond 50 billion parameters with minimum trial-and-error cost and computational resources. In this report, we introduce Tele-FLM (aka FLM-2), a 52B open-sourced multilingual large language model that features a stable, efficient pre-training paradigm and enhanced factual judgment capabilities. Tele-FLM demonstrates superior multilingual language modeling abilities, measured by BPB on textual corpus. Besides, in both English and Chinese foundation model evaluation, it is comparable to strong open-sourced models that involve larger pre-training FLOPs, such as Llama2-70B and DeepSeek-67B. In addition to the model weights, we share the core designs, engineering practices, and training details, which we expect to benefit both the academic and industrial communities.</li>
</ul>

<h3>Title: Evolutionary Large Language Models for Hardware Security: A Comparative  Survey</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Akyash, Hadi Mardani Kamali</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16651">https://arxiv.org/abs/2404.16651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16651">https://arxiv.org/pdf/2404.16651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16651]] Evolutionary Large Language Models for Hardware Security: A Comparative  Survey(https://arxiv.org/abs/2404.16651)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Automating hardware (HW) security vulnerability detection and mitigation during the design phase is imperative for two reasons: (i) It must be before chip fabrication, as post-fabrication fixes can be costly or even impractical; (ii) The size and complexity of modern HW raise concerns about unknown vulnerabilities compromising CIA triad. While Large Language Models (LLMs) can revolutionize both HW design and testing processes, within the semiconductor context, LLMs can be harnessed to automatically rectify security-relevant vulnerabilities inherent in HW designs. This study explores the seeds of LLM integration in register transfer level (RTL) designs, focusing on their capacity for autonomously resolving security-related vulnerabilities. The analysis involves comparing methodologies, assessing scalability, interpretability, and identifying future research directions. Potential areas for exploration include developing specialized LLM architectures for HW security tasks and enhancing model performance with domain-specific knowledge, leading to reliable automated security measurement and risk mitigation associated with HW vulnerabilities.</li>
</ul>

<h3>Title: Análise de ambiguidade linguística em modelos de linguagem de grande  escala (LLMs)</h3>
<ul>
<li><strong>Authors: </strong>Lavínia de Carvalho Moraes, Irene Cristina Silvério, Rafael Alexandre Sousa Marques, Bianca de Castro Anaia, Dandara Freitas de Paula, Maria Carolina Schincariol de Faria, Iury Cleveston, Alana de Santana Correia, Raquel Meister Ko Freitag</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16653">https://arxiv.org/abs/2404.16653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16653">https://arxiv.org/pdf/2404.16653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16653]] Análise de ambiguidade linguística em modelos de linguagem de grande  escala (LLMs)(https://arxiv.org/abs/2404.16653)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Linguistic ambiguity continues to represent a significant challenge for natural language processing (NLP) systems, notwithstanding the advancements in architectures such as Transformers and BERT. Inspired by the recent success of instructional models like ChatGPT and Gemini (In 2023, the artificial intelligence was called Bard.), this study aims to analyze and discuss linguistic ambiguity within these models, focusing on three types prevalent in Brazilian Portuguese: semantic, syntactic, and lexical ambiguity. We create a corpus comprising 120 sentences, both ambiguous and unambiguous, for classification, explanation, and disambiguation. The models capability to generate ambiguous sentences was also explored by soliciting sets of sentences for each type of ambiguity. The results underwent qualitative analysis, drawing on recognized linguistic references, and quantitative assessment based on the accuracy of the responses obtained. It was evidenced that even the most sophisticated models, such as ChatGPT and Gemini, exhibit errors and deficiencies in their responses, with explanations often providing inconsistent. Furthermore, the accuracy peaked at 49.58 percent, indicating the need for descriptive studies for supervised learning.</li>
</ul>

<h3>Title: A Self-Organizing Clustering System for Unsupervised Distribution Shift  Detection</h3>
<ul>
<li><strong>Authors: </strong>Sebastián Basterrech, Line Clemmensen, Gerardo Rubino</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16656">https://arxiv.org/abs/2404.16656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16656">https://arxiv.org/pdf/2404.16656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16656]] A Self-Organizing Clustering System for Unsupervised Distribution Shift  Detection(https://arxiv.org/abs/2404.16656)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Modeling non-stationary data is a challenging problem in the field of continual learning, and data distribution shifts may result in negative consequences on the performance of a machine learning model. Classic learning tools are often vulnerable to perturbations of the input covariates, and are sensitive to outliers and noise, and some tools are based on rigid algebraic assumptions. Distribution shifts are frequently occurring due to changes in raw materials for production, seasonality, a different user base, or even adversarial attacks. Therefore, there is a need for more effective distribution shift detection techniques. In this work, we propose a continual learning framework for monitoring and detecting distribution changes. We explore the problem in a latent space generated by a bio-inspired self-organizing clustering and statistical aspects of the latent space. In particular, we investigate the projections made by two topology-preserving maps: the Self-Organizing Map and the Scale Invariant Map. Our method can be applied in both a supervised and an unsupervised context. We construct the assessment of changes in the data distribution as a comparison of Gaussian signals, making the proposed method fast and robust. We compare it to other unsupervised techniques, specifically Principal Component Analysis (PCA) and Kernel-PCA. Our comparison involves conducting experiments using sequences of images (based on MNIST and injected shifts with adversarial samples), chemical sensor measurements, and the environmental variable related to ozone levels. The empirical study reveals the potential of the proposed approach.</li>
</ul>

<h3>Title: Formal Specification, Assessment, and Enforcement of Fairness for  Generative AIs</h3>
<ul>
<li><strong>Authors: </strong>Chih-Hong Cheng, Changshun Wu, Harald Ruess, Xingyu Zhao, Saddek Bensalem</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY, cs.LO, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16663">https://arxiv.org/abs/2404.16663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16663">https://arxiv.org/pdf/2404.16663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16663]] Formal Specification, Assessment, and Enforcement of Fairness for  Generative AIs(https://arxiv.org/abs/2404.16663)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative</a></li>
<li><strong>Abstract: </strong>The risk of reinforcing or exacerbating societal biases and inequalities is growing as generative AI increasingly produces content that resembles human output, from text to images and beyond. Here we formally characterize the notion of fairness for generative AI as a basis for monitoring and enforcing fairness. We define two levels of fairness utilizing the concept of infinite words. The first is the fairness demonstrated on the generated sequences, which is only evaluated on the outputs while agnostic to the prompts/models used. The second is the inherent fairness of the generative AI model, which requires that fairness be manifested when input prompts are neutral, that is, they do not explicitly instruct the generative AI to produce a particular type of output. We also study relative intersectional fairness to counteract the combinatorial explosion of fairness when considering multiple categories together with lazy fairness enforcement. Our implemented specification monitoring and enforcement tool shows interesting results when tested against several generative AI models.</li>
</ul>

<h3>Title: EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Hongxia Xie, Chu-Jun Peng, Yu-Wen Tseng, Hung-Jen Chen, Chan-Feng Hsu, Hong-Han Shuai, Wen-Huang Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16670">https://arxiv.org/abs/2404.16670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16670">https://arxiv.org/pdf/2404.16670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16670]] EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning(https://arxiv.org/abs/2404.16670)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Visual Instruction Tuning represents a novel learning paradigm involving the fine-tuning of pre-trained language models using task-specific instructions. This paradigm shows promising zero-shot results in various natural language processing tasks but is still unexplored in vision emotion understanding. In this work, we focus on enhancing the model's proficiency in understanding and adhering to instructions related to emotional contexts. Initially, we identify key visual clues critical to visual emotion recognition. Subsequently, we introduce a novel GPT-assisted pipeline for generating emotion visual instruction data, effectively addressing the scarcity of annotated instruction data in this domain. Expanding on the groundwork established by InstructBLIP, our proposed EmoVIT architecture incorporates emotion-specific instruction data, leveraging the powerful capabilities of Large Language Models to enhance performance. Through extensive experiments, our model showcases its proficiency in emotion classification, adeptness in affective reasoning, and competence in comprehending humor. The comparative analysis provides a robust benchmark for Emotion Visual Instruction Tuning in the era of LLMs, providing valuable insights and opening avenues for future exploration in this domain. Our code is available at \url{https://github.com/aimmemotion/EmoVIT}.</li>
</ul>

<h3>Title: Multimodal Semantic-Aware Automatic Colorization with Diffusion Prior</h3>
<ul>
<li><strong>Authors: </strong>Han Wang, Xinning Chai, Yiwen Wang, Yuhong Zhang, Rong Xie, Li Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16678">https://arxiv.org/abs/2404.16678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16678">https://arxiv.org/pdf/2404.16678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16678]] Multimodal Semantic-Aware Automatic Colorization with Diffusion Prior(https://arxiv.org/abs/2404.16678)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Colorizing grayscale images offers an engaging visual experience. Existing automatic colorization methods often fail to generate satisfactory results due to incorrect semantic colors and unsaturated colors. In this work, we propose an automatic colorization pipeline to overcome these challenges. We leverage the extraordinary generative ability of the diffusion prior to synthesize color with plausible semantics. To overcome the artifacts introduced by the diffusion prior, we apply the luminance conditional guidance. Moreover, we adopt multimodal high-level semantic priors to help the model understand the image content and deliver saturated colors. Besides, a luminance-aware decoder is designed to restore details and enhance overall visual quality. The proposed pipeline synthesizes saturated colors while maintaining plausible semantics. Experiments indicate that our proposed method considers both diversity and fidelity, surpassing previous methods in terms of perceptual realism and gain most human preference.</li>
</ul>

<h3>Title: NTIRE 2024 Quality Assessment of AI-Generated Content Challenge</h3>
<ul>
<li><strong>Authors: </strong>Xiaohong Liu, Xiongkuo Min, Guangtao Zhai, Chunyi Li, Tengchuan Kou, Wei Sun, Haoning Wu, Yixuan Gao, Yuqin Cao, Zicheng Zhang, Xiele Wu, Radu Timofte</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16687">https://arxiv.org/abs/2404.16687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16687">https://arxiv.org/pdf/2404.16687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16687]] NTIRE 2024 Quality Assessment of AI-Generated Content Challenge(https://arxiv.org/abs/2404.16687)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper reports on the NTIRE 2024 Quality Assessment of AI-Generated Content Challenge, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2024. This challenge is to address a major challenge in the field of image and video processing, namely, Image Quality Assessment (IQA) and Video Quality Assessment (VQA) for AI-Generated Content (AIGC). The challenge is divided into the image track and the video track. The image track uses the AIGIQA-20K, which contains 20,000 AI-Generated Images (AIGIs) generated by 15 popular generative models. The image track has a total of 318 registered participants. A total of 1,646 submissions are received in the development phase, and 221 submissions are received in the test phase. Finally, 16 participating teams submitted their models and fact sheets. The video track uses the T2VQA-DB, which contains 10,000 AI-Generated Videos (AIGVs) generated by 9 popular Text-to-Video (T2V) models. A total of 196 participants have registered in the video track. A total of 991 submissions are received in the development phase, and 185 submissions are received in the test phase. Finally, 12 participating teams submitted their models and fact sheets. Some methods have achieved better results than baseline methods, and the winning methods in both tracks have demonstrated superior prediction performance on AIGC.</li>
</ul>

<h3>Title: Influence of Solution Efficiency and Valence of Instruction on Additive  and Subtractive Solution Strategies in Humans and GPT-4</h3>
<ul>
<li><strong>Authors: </strong>Lydia Uhler, Verena Jordan, Jürgen Buder, Markus Huff, Frank Papenmeier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16692">https://arxiv.org/abs/2404.16692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16692">https://arxiv.org/pdf/2404.16692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16692]] Influence of Solution Efficiency and Valence of Instruction on Additive  and Subtractive Solution Strategies in Humans and GPT-4(https://arxiv.org/abs/2404.16692)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We explored the addition bias, a cognitive tendency to prefer adding elements over removing them to alter an initial state or structure, by conducting four preregistered experiments examining the problem-solving behavior of both humans and OpenAl's GPT-4 large language model. The experiments involved 588 participants from the U.S. and 680 iterations of the GPT-4 model. The problem-solving task was either to create symmetry within a grid (Experiments 1 and 3) or to edit a summary (Experiments 2 and 4). As hypothesized, we found that overall, the addition bias was present. Solution efficiency (Experiments 1 and 2) and valence of the instruction (Experiments 3 and 4) played important roles. Human participants were less likely to use additive strategies when subtraction was relatively more efficient than when addition and subtraction were equally efficient. GPT-4 exhibited the opposite behavior, with a strong addition bias when subtraction was more efficient. In terms of instruction valence, GPT-4 was more likely to add words when asked to "improve" compared to "edit", whereas humans did not show this effect. When we looked at the addition bias under different conditions, we found more biased responses for GPT-4 compared to humans. Our findings highlight the importance of considering comparable and sometimes superior subtractive alternatives, as well as reevaluating one's own and particularly the language models' problem-solving behavior.</li>
</ul>

<h3>Title: Cooperate or Collapse: Emergence of Sustainability Behaviors in a  Society of LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard Schölkopf, Mrinmaya Sachan, Rada Mihalcea</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16698">https://arxiv.org/abs/2404.16698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16698">https://arxiv.org/pdf/2404.16698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16698]] Cooperate or Collapse: Emergence of Sustainability Behaviors in a  Society of LLM Agents(https://arxiv.org/abs/2404.16698)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving field of artificial intelligence, ensuring safe decision-making of Large Language Models (LLMs) is a significant challenge. This paper introduces Governance of the Commons Simulation (GovSim), a simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. Through this simulation environment, we explore the dynamics of resource sharing among AI agents, highlighting the importance of ethical considerations, strategic planning, and negotiation skills. GovSim is versatile and supports any text-based agent, including LLMs agents. Using the Generative Agent framework, we create a standard agent that facilitates the integration of different LLMs. Our findings reveal that within GovSim, only two out of 15 tested LLMs managed to achieve a sustainable outcome, indicating a significant gap in the ability of models to manage shared resources. Furthermore, we find that by removing the ability of agents to communicate, they overuse the shared resource, highlighting the importance of communication for cooperation. Interestingly, most LLMs lack the ability to make universalized hypotheses, which highlights a significant weakness in their reasoning skills. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.</li>
</ul>

<h3>Title: Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A Aly, Beidi Chen, Carole-Jean Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16710">https://arxiv.org/abs/2404.16710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16710">https://arxiv.org/pdf/2404.16710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16710]] Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding(https://arxiv.org/abs/2404.16710)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model. Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model. Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task.</li>
</ul>

<h3>Title: CBRW: A Novel Approach for Cancelable Biometric Template Generation  based on</h3>
<ul>
<li><strong>Authors: </strong>Nitin Kumar, Manisha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16739">https://arxiv.org/abs/2404.16739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16739">https://arxiv.org/pdf/2404.16739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16739]] CBRW: A Novel Approach for Cancelable Biometric Template Generation  based on(https://arxiv.org/abs/2404.16739)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, biometric</a></li>
<li><strong>Abstract: </strong>Cancelable Biometric is a challenging research field in which security of an original biometric image is ensured by transforming the original biometric into another irreversible domain. Several approaches have been suggested in literature for generating cancelable biometric templates. In this paper, two novel and simple cancelable biometric template generation methods based on Random Walk (CBRW) have been proposed. By employing random walk and other steps given in the proposed two algorithms viz. CBRW-BitXOR and CBRW-BitCMP, the original biometric is transformed into a cancellable template. The performance of the proposed methods is compared with other state-of-the-art methods. Experiments have been performed on eight publicly available gray and color datasets i.e. CP (ear) (gray and color), UTIRIS (iris) (gray and color), ORL (face) (gray), IIT Delhi (iris) (gray and color), and AR (face) (color). Performance of the generated templates is measured in terms of Correlation Coefficient (Cr), Root Mean Square Error (RMSE), Peak Signal to Noise Ratio (PSNR), Structural Similarity (SSIM), Mean Absolute Error (MAE), Number of Pixel Change Rate (NPCR), and Unified Average Changing Intensity (UACI). By experimental results, it has been proved that proposed methods are superior than other state-of-the-art methods in qualitative as well as quantitative analysis. Furthermore, CBRW performs better on both gray as well as color images.</li>
</ul>

<h3>Title: JITScanner: Just-in-Time Executable Page Check in the Linux Operating  System</h3>
<ul>
<li><strong>Authors: </strong>Pasquale Caporaso, Giuseppe Bianchi, Francesco Quaglia</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16744">https://arxiv.org/abs/2404.16744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16744">https://arxiv.org/pdf/2404.16744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16744]] JITScanner: Just-in-Time Executable Page Check in the Linux Operating  System(https://arxiv.org/abs/2404.16744)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Modern malware poses a severe threat to cybersecurity, continually evolving in sophistication. To combat this threat, researchers and security professionals continuously explore advanced techniques for malware detection and analysis. Dynamic analysis, a prevalent approach, offers advantages over static analysis by enabling observation of runtime behavior and detecting obfuscated or encrypted code used to evade detection. However, executing programs within a controlled environment can be resource-intensive, often necessitating compromises, such as limiting sandboxing to an initial period. In our article, we propose an alternative method for dynamic executable analysis: examining the presence of malicious signatures within executable virtual pages precisely when their current content, including any updates over time, is accessed for instruction fetching. Our solution, named JITScanner, is developed as a Linux-oriented package built upon a Loadable Kernel Module (LKM). It integrates a user-level component that communicates efficiently with the LKM using scalable multi-processor/core technology. JITScanner's effectiveness in detecting malware programs and its minimal intrusion in normal runtime scenarios have been extensively tested, with the experiment results detailed in this article. These experiments affirm the viability of our approach, showcasing JITScanner's capability to effectively identify malware while minimizing runtime overhead.</li>
</ul>

<h3>Title: TokenHMR: Advancing Human Mesh Recovery with a Tokenized Pose  Representation</h3>
<ul>
<li><strong>Authors: </strong>Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, Yao Feng, Michael J. Black</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16752">https://arxiv.org/abs/2404.16752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16752">https://arxiv.org/pdf/2404.16752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16752]] TokenHMR: Advancing Human Mesh Recovery with a Tokenized Pose  Representation(https://arxiv.org/abs/2404.16752)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We address the problem of regressing 3D human pose and shape from a single image, with a focus on 3D accuracy. The current best methods leverage large datasets of 3D pseudo-ground-truth (p-GT) and 2D keypoints, leading to robust performance. With such methods, we observe a paradoxical decline in 3D pose accuracy with increasing 2D accuracy. This is caused by biases in the p-GT and the use of an approximate camera projection model. We quantify the error induced by current camera models and show that fitting 2D keypoints and p-GT accurately causes incorrect 3D poses. Our analysis defines the invalid distances within which minimizing 2D and p-GT losses is detrimental. We use this to formulate a new loss Threshold-Adaptive Loss Scaling (TALS) that penalizes gross 2D and p-GT losses but not smaller ones. With such a loss, there are many 3D poses that could equally explain the 2D evidence. To reduce this ambiguity we need a prior over valid human poses but such priors can introduce unwanted bias. To address this, we exploit a tokenized representation of human pose and reformulate the problem as token prediction. This restricts the estimated poses to the space of valid poses, effectively providing a uniform prior. Extensive experiments on the EMDB and 3DPW datasets show that our reformulated keypoint loss and tokenization allows us to train on in-the-wild data while improving 3D accuracy over the state-of-the-art. Our models and code are available for research at https://tokenhmr.is.tue.mpg.de.</li>
</ul>

<h3>Title: RadGenome-Chest CT: A Grounded Vision-Language Dataset for Chest CT  Analysis</h3>
<ul>
<li><strong>Authors: </strong>Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Jiayu Lei, Ya Zhang, Yanfeng Wang, Weidi Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16754">https://arxiv.org/abs/2404.16754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16754">https://arxiv.org/pdf/2404.16754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16754]] RadGenome-Chest CT: A Grounded Vision-Language Dataset for Chest CT  Analysis(https://arxiv.org/abs/2404.16754)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Developing generalist foundation model has recently attracted tremendous attention among researchers in the field of AI for Medicine (AI4Medicine). A pivotal insight in developing these models is their reliance on dataset scaling, which emphasizes the requirements on developing open-source medical image datasets that incorporate diverse supervision signals across various imaging modalities. In this paper, we introduce RadGenome-Chest CT, a comprehensive, large-scale, region-guided 3D chest CT interpretation dataset based on CT-RATE. Specifically, we leverage the latest powerful universal segmentation and large language models, to extend the original datasets (over 25,692 non-contrast 3D chest CT volume and reports from 20,000 patients) from the following aspects: (i) organ-level segmentation masks covering 197 categories, which provide intermediate reasoning visual clues for interpretation; (ii) 665 K multi-granularity grounded reports, where each sentence of the report is linked to the corresponding anatomical region of CT volume in the form of a segmentation mask; (iii) 1.3 M grounded VQA pairs, where questions and answers are all linked with reference segmentation masks, enabling models to associate visual evidence with textual explanations. All grounded reports and VQA pairs in the validation set have gone through manual verification to ensure dataset quality. We believe that RadGenome-Chest CT can significantly advance the development of multimodal medical foundation models, by training to generate texts based on given segmentation regions, which is unattainable with previous relevant datasets. We will release all segmentation masks, grounded reports, and VQA pairs to facilitate further research and development in this field.</li>
</ul>

<h3>Title: Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation  Language Model</h3>
<ul>
<li><strong>Authors: </strong>Runzhe Zhan, Xinyi Yang, Derek F. Wong, Lidia S. Chao, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16766">https://arxiv.org/abs/2404.16766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16766">https://arxiv.org/pdf/2404.16766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16766]] Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation  Language Model(https://arxiv.org/abs/2404.16766)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While supervised fine-tuning (SFT) has been a straightforward approach for tailoring the output of foundation large language model (LLM) to specific preferences, concerns have been raised about the depth of this alignment, with some critiques suggesting it is merely "superficial". We critically examine this hypothesis within the scope of cross-lingual generation tasks, proposing that the effectiveness of SFT may be constrained by its reliance on prior tokens to guide cross-lingual generation. Based on this crucial insight, and in response to the challenges posed by the costly and limited availability of non-English data for SFT, we introduce a novel training-free alignment method named PreTTY, which employs minimal task-related prior tokens to bridge the foundation LLM and the SFT LLM, achieving comparable performance without training. Experiments on machine translation and part-of-speech tagging across eight languages demonstrate the efficacy of PreTTY in cross-lingual settings. Remarkably, by initiating the decoding process with only one or two prior tokens, foundation LLMs can achieve performance comparable to their SFT counterparts. This method presents a cost-effective alternative to SFT and advances the democratization of multilingual LLMs.</li>
</ul>

<h3>Title: REBEL: Reinforcement Learning via Regressing Relative Rewards</h3>
<ul>
<li><strong>Authors: </strong>Zhaolin Gao, Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Gokul Swamy, Kianté Brantley, Thorsten Joachims, J. Andrew Bagnell, Jason D. Lee, Wen Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16767">https://arxiv.org/abs/2404.16767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16767">https://arxiv.org/pdf/2404.16767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16767]] REBEL: Reinforcement Learning via Regressing Relative Rewards(https://arxiv.org/abs/2404.16767)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications including the fine-tuning of generative models. Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping) and is notorious for its sensitivity to the precise implementation of these components. In response, we take a step back and ask what a minimalist RL algorithm for the era of generative models would look like. We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the relative rewards via a direct policy parameterization between two completions to a prompt, enabling strikingly lightweight implementation. In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature. REBEL can also cleanly incorporate offline data and handle the intransitive preferences we frequently see in practice. Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally tractable than PPO.</li>
</ul>

<h3>Title: ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity  Preserving</h3>
<ul>
<li><strong>Authors: </strong>Jiehui Huang, Xiao Dong, Wenhui Song, Hanhui Li, Jun Zhou, Yuhao Cheng, Shutao Liao, Long Chen, Yiqiang Yan, Shengcai Liao, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16771">https://arxiv.org/abs/2404.16771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16771">https://arxiv.org/pdf/2404.16771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16771]] ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity  Preserving(https://arxiv.org/abs/2404.16771)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based technologies have made significant strides, particularly in personalized and customized facialgeneration. However, existing methods face challenges in achieving high-fidelity and detailed identity (ID)consistency, primarily due to insufficient fine-grained control over facial areas and the lack of a comprehensive strategy for ID preservation by fully considering intricate facial details and the overall face. To address these limitations, we introduce ConsistentID, an innovative method crafted for diverseidentity-preserving portrait generation under fine-grained multimodal facial prompts, utilizing only a single reference image. ConsistentID comprises two key components: a multimodal facial prompt generator that combines facial features, corresponding facial descriptions and the overall facial context to enhance precision in facial details, and an ID-preservation network optimized through the facial attention localization strategy, aimed at preserving ID consistency in facial regions. Together, these components significantly enhance the accuracy of ID preservation by introducing fine-grained multimodal ID information from facial regions. To facilitate training of ConsistentID, we present a fine-grained portrait dataset, FGID, with over 500,000 facial images, offering greater diversity and comprehensiveness than existing public facial datasets. % such as LAION-Face, CelebA, FFHQ, and SFHQ. Experimental results substantiate that our ConsistentID achieves exceptional precision and diversity in personalized facial generation, surpassing existing methods in the MyStyle dataset. Furthermore, while ConsistentID introduces more multimodal ID information, it maintains a fast inference speed during generation.</li>
</ul>

<h3>Title: Modeling Selective Feature Attention for Representation-based Siamese  Text Matching</h3>
<ul>
<li><strong>Authors: </strong>Jianxiang Zang, Hui Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16776">https://arxiv.org/abs/2404.16776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16776">https://arxiv.org/pdf/2404.16776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16776]] Modeling Selective Feature Attention for Representation-based Siamese  Text Matching(https://arxiv.org/abs/2404.16776)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Representation-based Siamese networks have risen to popularity in lightweight text matching due to their low deployment and inference costs. While word-level attention mechanisms have been implemented within Siamese networks to improve performance, we propose Feature Attention (FA), a novel downstream block designed to enrich the modeling of dependencies among embedding features. Employing "squeeze-and-excitation" techniques, the FA block dynamically adjusts the emphasis on individual features, enabling the network to concentrate more on features that significantly contribute to the final classification. Building upon FA, we introduce a dynamic "selection" mechanism called Selective Feature Attention (SFA), which leverages a stacked BiGRU Inception structure. The SFA block facilitates multi-scale semantic extraction by traversing different stacked BiGRU layers, encouraging the network to selectively concentrate on semantic information and embedding features across varying levels of abstraction. Both the FA and SFA blocks offer a seamless integration capability with various Siamese networks, showcasing a plug-and-play characteristic. Experimental evaluations conducted across diverse text matching baselines and benchmarks underscore the indispensability of modeling feature attention and the superiority of the "selection" mechanism.</li>
</ul>

<h3>Title: Registration by Regression (RbR): a framework for interpretable and  flexible atlas registration</h3>
<ul>
<li><strong>Authors: </strong>Karthik Gopinath, Xiaoling Hu, Malte Hoffmann, Oula Puonti, Juan Eugenio Iglesias</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16781">https://arxiv.org/abs/2404.16781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16781">https://arxiv.org/pdf/2404.16781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16781]] Registration by Regression (RbR): a framework for interpretable and  flexible atlas registration(https://arxiv.org/abs/2404.16781)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>In human neuroimaging studies, atlas registration enables mapping MRI scans to a common coordinate frame, which is necessary to aggregate data from multiple subjects. Machine learning registration methods have achieved excellent speed and accuracy but lack interpretability. More recently, keypoint-based methods have been proposed to tackle this issue, but their accuracy is still subpar, particularly when fitting nonlinear transforms. Here we propose Registration by Regression (RbR), a novel atlas registration framework that is highly robust and flexible, conceptually simple, and can be trained with cheaply obtained data. RbR predicts the (x,y,z) atlas coordinates for every voxel of the input scan (i.e., every voxel is a keypoint), and then uses closed-form expressions to quickly fit transforms using a wide array of possible deformation models, including affine and nonlinear (e.g., Bspline, Demons, invertible diffeomorphic models, etc.). Robustness is provided by the large number of voxels informing the registration and can be further increased by robust estimators like RANSAC. Experiments on independent public datasets show that RbR yields more accurate registration than competing keypoint approaches, while providing full control of the deformation model.</li>
</ul>

<h3>Title: Continual Learning of Large Language Models: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16789">https://arxiv.org/abs/2404.16789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16789">https://arxiv.org/pdf/2404.16789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16789]] Continual Learning of Large Language Models: A Comprehensive Survey(https://arxiv.org/abs/2404.16789)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The recent success of large language models (LLMs) trained on static, pre-collected, general datasets has sparked numerous research directions and applications. One such direction addresses the non-trivial challenge of integrating pre-trained LLMs into dynamic data distributions, task structures, and user preferences. Pre-trained LLMs, when tailored for specific needs, often experience significant performance degradation in previous knowledge domains -- a phenomenon known as "catastrophic forgetting". While extensively studied in the continual learning (CL) community, it presents new manifestations in the realm of LLMs. In this survey, we provide a comprehensive overview of the current research progress on LLMs within the context of CL. This survey is structured into four main sections: we first describe an overview of continually learning LLMs, consisting of two directions of continuity: vertical continuity (or vertical continual learning), i.e., continual adaptation from general to specific capabilities, and horizontal continuity (or horizontal continual learning), i.e., continual adaptation across time and domains (Section 3). We then summarize three stages of learning LLMs in the context of modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of evaluation protocols for continual learning with LLMs, along with the current available data sources (Section 5). Finally, we discuss intriguing questions pertaining to continual learning for LLMs (Section 6). The full list of papers examined in this survey is available at https://github.com/Wang-ML-Lab/llm-continual-learning-survey.</li>
</ul>

<h3>Title: SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with  Text-Rich Visual Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, Ying Shan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16790">https://arxiv.org/abs/2404.16790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16790">https://arxiv.org/pdf/2404.16790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16790]] SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with  Text-Rich Visual Comprehension(https://arxiv.org/abs/2404.16790)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Comprehending text-rich visual content is paramount for the practical application of Multimodal Large Language Models (MLLMs), since text-rich scenarios are ubiquitous in the real world, which are characterized by the presence of extensive texts embedded within images. Recently, the advent of MLLMs with impressive versatility has raised the bar for what we can expect from MLLMs. However, their proficiency in text-rich scenarios has yet to be comprehensively and objectively assessed, since current MLLM benchmarks primarily focus on evaluating general visual comprehension. In this work, we introduce SEED-Bench-2-Plus, a benchmark specifically designed for evaluating \textbf{text-rich visual comprehension} of MLLMs. Our benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs, each of which covers a wide spectrum of text-rich scenarios in the real world. These categories, due to their inherent complexity and diversity, effectively simulate real-world text-rich environments. We further conduct a thorough evaluation involving 34 prominent MLLMs (including GPT-4V, Gemini-Pro-Vision and Claude-3-Opus) and emphasize the current limitations of MLLMs in text-rich visual comprehension. We hope that our work can serve as a valuable addition to existing MLLM benchmarks, providing insightful observations and inspiring further research in the area of text-rich visual comprehension with MLLMs. The dataset and evaluation code can be accessed at https://github.com/AILab-CVC/SEED-Bench.</li>
</ul>

<h3>Title: Weak-to-Strong Extrapolation Expedites Alignment</h3>
<ul>
<li><strong>Authors: </strong>Chujie Zheng, Ziqi Wang, Heng Ji, Minlie Huang, Nanyun Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16792">https://arxiv.org/abs/2404.16792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16792">https://arxiv.org/pdf/2404.16792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16792]] Weak-to-Strong Extrapolation Expedites Alignment(https://arxiv.org/abs/2404.16792)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although the capabilities of large language models (LLMs) ideally scale up with increasing data and compute, they are inevitably constrained by limited resources in reality. Suppose we have a moderately trained LLM (e.g., trained to align with human preference) in hand, can we further exploit its potential and cheaply acquire a stronger model? In this paper, we propose a simple method called ExPO to boost LLMs' alignment with human preference. ExPO assumes that a medium-aligned model can be interpolated between a less-aligned (weaker) model, e.g., the initial SFT model, and a better-aligned (stronger) one, thereby directly obtaining this stronger model by extrapolating from the weights of the former two relatively weaker models. On the AlpacaEval 2.0 benchmark, we show that ExPO pushes models trained with less preference data (e.g., 10% or 20%) to reach and even surpass the fully-trained one, without any additional training. Furthermore, ExPO also significantly improves off-the-shelf DPO/RLHF models and exhibits decent scalability across model sizes from 7B to 70B. Our work demonstrates the efficacy of model extrapolation in exploiting LLMs' capabilities, suggesting a promising direction that deserves future exploration.</li>
</ul>

<h3>Title: In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter  Optimization</h3>
<ul>
<li><strong>Authors: </strong>Herilalaina Rakotoarison, Steven Adriaensen, Neeratyoy Mallik, Samir Garibov, Edward Bergman, Frank Hutter</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16795">https://arxiv.org/abs/2404.16795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16795">https://arxiv.org/pdf/2404.16795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16795]] In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter  Optimization(https://arxiv.org/abs/2404.16795)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>With the increasing computational costs associated with deep learning, automated hyperparameter optimization methods, strongly relying on black-box Bayesian optimization (BO), face limitations. Freeze-thaw BO offers a promising grey-box alternative, strategically allocating scarce resources incrementally to different configurations. However, the frequent surrogate model updates inherent to this approach pose challenges for existing methods, requiring retraining or fine-tuning their neural network surrogates online, introducing overhead, instability, and hyper-hyperparameters. In this work, we propose FT-PFN, a novel surrogate for Freeze-thaw style BO. FT-PFN is a prior-data fitted network (PFN) that leverages the transformers' in-context learning ability to efficiently and reliably do Bayesian learning curve extrapolation in a single forward pass. Our empirical analysis across three benchmark suites shows that the predictions made by FT-PFN are more accurate and 10-100 times faster than those of the deep Gaussian process and deep ensemble surrogates used in previous work. Furthermore, we show that, when combined with our novel acquisition mechanism (MFPI-random), the resulting in-context freeze-thaw BO method (ifBO), yields new state-of-the-art performance in the same three families of deep learning HPO benchmarks considered in prior work.</li>
</ul>

<h3>Title: Improving Diversity of Commonsense Generation by Large Language Models  via In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Tianhui Zhang, Bei Peng, Danushka Bollegala</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16807">https://arxiv.org/abs/2404.16807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16807">https://arxiv.org/pdf/2404.16807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16807]] Improving Diversity of Commonsense Generation by Large Language Models  via In-Context Learning(https://arxiv.org/abs/2404.16807)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative Commonsense Reasoning (GCR) requires a model to reason about a situation using commonsense knowledge, while generating coherent sentences. Although the quality of the generated sentences is crucial, the diversity of the generation is equally important because it reflects the model's ability to use a range of commonsense knowledge facts. Large Language Models (LLMs) have shown proficiency in enhancing the generation quality across various tasks through in-context learning (ICL) using given examples without the need for any fine-tuning. However, the diversity aspect in LLM outputs has not been systematically studied before. To address this, we propose a simple method that diversifies the LLM generations, while preserving their quality. Experimental results on three benchmark GCR datasets show that our method achieves an ideal balance between the quality and diversity. Moreover, the sentences generated by our proposed method can be used as training data to improve diversity in existing commonsense generators.</li>
</ul>

<h3>Title: Make Your LLM Fully Utilize the Context</h3>
<ul>
<li><strong>Authors: </strong>Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16811">https://arxiv.org/abs/2404.16811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16811">https://arxiv.org/pdf/2404.16811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16811]] Make Your LLM Fully Utilize the Context(https://arxiv.org/abs/2404.16811)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost-in-the-middle challenge. We hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information. Based on this intuition, our study presents information-intensive (IN2) training, a purely data-driven solution to overcome lost-in-the-middle. Specifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) fine-grained information awareness on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the integration and reasoning of information from two or more short segments. Through applying this information-intensive training on Mistral-7B, we present FILM-7B (FILl-in-the-Middle). To thoroughly assess the ability of FILM-7B for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval). The probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window. Beyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3->59.2 accuracy on MMLU). Github Link: https://github.com/microsoft/FILM.</li>
</ul>

<h3>Title: IndicGenBench: A Multilingual Benchmark to Evaluate Generation  Capabilities of LLMs on Indic Languages</h3>
<ul>
<li><strong>Authors: </strong>Harman Singh, Nitish Gupta, Shikhar Bharadwaj, Dinesh Tewari, Partha Talukdar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16816">https://arxiv.org/abs/2404.16816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16816">https://arxiv.org/pdf/2404.16816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16816]] IndicGenBench: A Multilingual Benchmark to Evaluate Generation  Capabilities of LLMs on Indic Languages(https://arxiv.org/abs/2404.16816)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world. India is a linguistically diverse country of 1.4 Billion people. To facilitate research on multilingual LLM evaluation, we release IndicGenBench - the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families. IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering. IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time. We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings. The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models. IndicGenBench is released at www.github.com/google-research-datasets/indic-gen-bench</li>
</ul>

<h3>Title: Boosting Unsupervised Semantic Segmentation with Principal Mask  Proposals</h3>
<ul>
<li><strong>Authors: </strong>Oliver Hahn, Nikita Araslanov, Simone Schaub-Meyer, Stefan Roth</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16818">https://arxiv.org/abs/2404.16818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16818">https://arxiv.org/pdf/2404.16818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16818]] Boosting Unsupervised Semantic Segmentation with Principal Mask  Proposals(https://arxiv.org/abs/2404.16818)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Unsupervised semantic segmentation aims to automatically partition images into semantically meaningful regions by identifying global categories within an image corpus without any form of annotation. Building upon recent advances in self-supervised representation learning, we focus on how to leverage these large pre-trained models for the downstream task of unsupervised segmentation. We present PriMaPs - Principal Mask Proposals - decomposing images into semantically meaningful masks based on their feature representation. This allows us to realize unsupervised semantic segmentation by fitting class prototypes to PriMaPs with a stochastic expectation-maximization algorithm, PriMaPs-EM. Despite its conceptual simplicity, PriMaPs-EM leads to competitive results across various pre-trained backbone models, including DINO and DINOv2, and across datasets, such as Cityscapes, COCO-Stuff, and Potsdam-3. Importantly, PriMaPs-EM is able to boost results when applied orthogonally to current state-of-the-art unsupervised semantic segmentation pipelines.</li>
</ul>

<h3>Title: Revisiting Text-to-Image Evaluation with Gecko: On Metrics, Prompts, and  Human Ratings</h3>
<ul>
<li><strong>Authors: </strong>Olivia Wiles, Chuhan Zhang, Isabela Albuquerque, Ivana Kajić, Su Wang, Emanuele Bugliarello, Yasumasa Onoe, Chris Knutsen, Cyrus Rashtchian, Jordi Pont-Tuset, Aida Nematzadeh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16820">https://arxiv.org/abs/2404.16820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16820">https://arxiv.org/pdf/2404.16820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16820]] Revisiting Text-to-Image Evaluation with Gecko: On Metrics, Prompts, and  Human Ratings(https://arxiv.org/abs/2404.16820)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While text-to-image (T2I) generative models have become ubiquitous, they do not necessarily generate images that align with a given prompt. While previous work has evaluated T2I alignment by proposing metrics, benchmarks, and templates for collecting human judgements, the quality of these components is not systematically measured. Human-rated prompt sets are generally small and the reliability of the ratings -- and thereby the prompt set used to compare models -- is not evaluated. We address this gap by performing an extensive study evaluating auto-eval metrics and human templates. We provide three main contributions: (1) We introduce a comprehensive skills-based benchmark that can discriminate models across different human templates. This skills-based benchmark categorises prompts into sub-skills, allowing a practitioner to pinpoint not only which skills are challenging, but at what level of complexity a skill becomes challenging. (2) We gather human ratings across four templates and four T2I models for a total of >100K annotations. This allows us to understand where differences arise due to inherent ambiguity in the prompt and where they arise due to differences in metric and model quality. (3) Finally, we introduce a new QA-based auto-eval metric that is better correlated with human ratings than existing metrics for our new dataset, across different human templates, and on TIFA160.</li>
</ul>

<h3>Title: How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal  Models with Open-Source Suites</h3>
<ul>
<li><strong>Authors: </strong>Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16821">https://arxiv.org/abs/2404.16821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16821">https://arxiv.org/pdf/2404.16821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16821]] How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal  Models with Open-Source Suites(https://arxiv.org/abs/2404.16821)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this report, we introduce InternVL 1.5, an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. We introduce three simple improvements: (1) Strong Vision Encoder: we explored a continuous learning strategy for the large-scale vision foundation model -- InternViT-6B, boosting its visual understanding capabilities, and making it can be transferred and reused in different LLMs. (2) Dynamic High-Resolution: we divide images into tiles ranging from 1 to 40 of 448$\times$448 pixels according to the aspect ratio and resolution of the input images, which supports up to 4K resolution input. (3) High-Quality Bilingual Dataset: we carefully collected a high-quality bilingual dataset that covers common scenes, document images, and annotated them with English and Chinese question-answer pairs, significantly enhancing performance in OCR- and Chinese-related tasks. We evaluate InternVL 1.5 through a series of benchmarks and comparative studies. Compared to both open-source and proprietary models, InternVL 1.5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks. Code has been released at https://github.com/OpenGVLab/InternVL.</li>
</ul>

<h3>Title: V2A-Mark: Versatile Deep Visual-Audio Watermarking for Manipulation  Localization and Copyright Protection</h3>
<ul>
<li><strong>Authors: </strong>Xuanyu Zhang, Youmin Xu, Runyi Li, Jiwen Yu, Weiqi Li, Zhipei Xu, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16824">https://arxiv.org/abs/2404.16824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16824">https://arxiv.org/pdf/2404.16824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16824]] V2A-Mark: Versatile Deep Visual-Audio Watermarking for Manipulation  Localization and Copyright Protection(https://arxiv.org/abs/2404.16824)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, extraction, watermark</a></li>
<li><strong>Abstract: </strong>AI-generated video has revolutionized short video production, filmmaking, and personalized media, making video local editing an essential tool. However, this progress also blurs the line between reality and fiction, posing challenges in multimedia forensics. To solve this urgent issue, V2A-Mark is proposed to address the limitations of current video tampering forensics, such as poor generalizability, singular function, and single modality focus. Combining the fragility of video-into-video steganography with deep robust watermarking, our method can embed invisible visual-audio localization watermarks and copyright watermarks into the original video frames and audio, enabling precise manipulation localization and copyright protection. We also design a temporal alignment and fusion module and degradation prompt learning to enhance the localization accuracy and decoding robustness. Meanwhile, we introduce a sample-level audio localization method and a cross-modal copyright extraction mechanism to couple the information of audio and video frames. The effectiveness of V2A-Mark has been verified on a visual-audio tampering dataset, emphasizing its superiority in localization precision and copyright accuracy, crucial for the sustainable development of video editing in the AIGC video era.</li>
</ul>

<h3>Title: Made to Order: Discovering monotonic temporal changes via  self-supervised video ordering</h3>
<ul>
<li><strong>Authors: </strong>Charig Yang, Weidi Xie, Andrew Zisserman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16828">https://arxiv.org/abs/2404.16828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16828">https://arxiv.org/pdf/2404.16828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16828]] Made to Order: Discovering monotonic temporal changes via  self-supervised video ordering(https://arxiv.org/abs/2404.16828)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Our objective is to discover and localize monotonic temporal changes in a sequence of images. To achieve this, we exploit a simple proxy task of ordering a shuffled image sequence, with `time' serving as a supervisory signal since only changes that are monotonic with time can give rise to the correct ordering. We also introduce a flexible transformer-based model for general-purpose ordering of image sequences of arbitrary length with built-in attribution maps. After training, the model successfully discovers and localizes monotonic changes while ignoring cyclic and stochastic ones. We demonstrate applications of the model in multiple video settings covering different scene and object types, discovering both object-level and environmental changes in unseen sequences. We also demonstrate that the attention-based attribution maps function as effective prompts for segmenting the changing regions, and that the learned representations can be used for downstream applications. Finally, we show that the model achieves the state of the art on standard benchmarks for ordering a set of images.</li>
</ul>

<h3>Title: Make-it-Real: Unleashing Large Multimodal Model's Ability for Painting  3D Objects with Realistic Materials</h3>
<ul>
<li><strong>Authors: </strong>Ye Fang, Zeyi Sun, Tong Wu, Jiaqi Wang, Ziwei Liu, Gordon Wetzstein, Dahua Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16829">https://arxiv.org/abs/2404.16829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16829">https://arxiv.org/pdf/2404.16829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16829]] Make-it-Real: Unleashing Large Multimodal Model's Ability for Painting  3D Objects with Realistic Materials(https://arxiv.org/abs/2404.16829)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Physically realistic materials are pivotal in augmenting the realism of 3D assets across various applications and lighting conditions. However, existing 3D assets and generative models often lack authentic material properties. Manual assignment of materials using graphic software is a tedious and time-consuming task. In this paper, we exploit advancements in Multimodal Large Language Models (MLLMs), particularly GPT-4V, to present a novel approach, Make-it-Real: 1) We demonstrate that GPT-4V can effectively recognize and describe materials, allowing the construction of a detailed material library. 2) Utilizing a combination of visual cues and hierarchical text prompts, GPT-4V precisely identifies and aligns materials with the corresponding components of 3D objects. 3) The correctly matched materials are then meticulously applied as reference for the new SVBRDF material generation according to the original diffuse map, significantly enhancing their visual authenticity. Make-it-Real offers a streamlined integration into the 3D content creation workflow, showcasing its utility as an essential tool for developers of 3D assets.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
