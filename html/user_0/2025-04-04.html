<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-04</h1>
<h3>Title: When Reasoning Meets Compression: Benchmarking Compressed Large Reasoning Models on Complex Reasoning Tasks</h3>
<ul>
<li><strong>Authors: </strong>Nan Zhang, Yusen Zhang, Prasenjit Mitra, Rui Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02010">https://arxiv.org/abs/2504.02010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02010">https://arxiv.org/pdf/2504.02010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02010]] When Reasoning Meets Compression: Benchmarking Compressed Large Reasoning Models on Complex Reasoning Tasks(https://arxiv.org/abs/2504.02010)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent open-source large reasoning models (LRMs) exhibit strong performance on complex reasoning tasks, but their large parameter count makes them prohibitively expensive for individuals. The compression of large language models (LLMs) offers an effective solution to reduce cost of computational resources. However, systematic studies on the performance of compressed LLMs in complex reasoning tasks, especially for LRMs, are lacking. Most works on quantization and pruning focus on preserving language modeling performance, while existing distillation works do not comprehensively benchmark student models based on reasoning difficulty or compression impact on knowledge and reasoning. In this paper, we benchmark compressed DeepSeek-R1 models on four different reasoning datasets (AIME 2024, FOLIO, Temporal Sequences of BIG-Bench Hard, and MuSiQue), ranging from mathematical to multihop reasoning, using quantization, distillation, and pruning methods. We benchmark 2.51-, 1.73-, and 1.58-bit R1 models that adopt dynamic quantization. We also benchmark distilled R1 models that are based on LLaMA or Qwen and run SparseGPT on them to obtain various sparsity levels. Studying the performance and behavior of compressed LRMs, we report their performance scores and test-time compute (number of tokens spent on each question). Notably, using MuSiQue, we find that parameter count has a much greater impact on LRMs' knowledge memorization than on their reasoning capability, which can inform the choice of compression techniques. Through our empirical analysis of test-time compute, we find that shorter model outputs generally achieve better performance than longer ones across several benchmarks for both R1 and its compressed variants, highlighting the need for more concise reasoning chains.</li>
</ul>

<h3>Title: Random Conditioning with Distillation for Data-Efficient Diffusion Model Compression</h3>
<ul>
<li><strong>Authors: </strong>Dohyun Kim, Sehwan Park, Geonhee Han, Seung Wook Kim, Paul Hongsuck Seo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02011">https://arxiv.org/abs/2504.02011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02011">https://arxiv.org/pdf/2504.02011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02011]] Random Conditioning with Distillation for Data-Efficient Diffusion Model Compression(https://arxiv.org/abs/2504.02011)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models generate high-quality images through progressive denoising but are computationally intensive due to large model sizes and repeated sampling. Knowledge distillation, which transfers knowledge from a complex teacher to a simpler student model, has been widely studied in recognition tasks, particularly for transferring concepts unseen during student training. However, its application to diffusion models remains underexplored, especially in enabling student models to generate concepts not covered by the training images. In this work, we propose Random Conditioning, a novel approach that pairs noised images with randomly selected text conditions to enable efficient, image-free knowledge distillation. By leveraging this technique, we show that the student can generate concepts unseen in the training images. When applied to conditional diffusion model distillation, our method allows the student to explore the condition space without generating condition-specific images, resulting in notable improvements in both generation quality and efficiency. This promotes resource-efficient deployment of generative diffusion models, broadening their accessibility for both research and real-world applications. Code, models, and datasets are available at this https URL .</li>
</ul>

<h3>Title: Instruction-Guided Autoregressive Neural Network Parameter Generation</h3>
<ul>
<li><strong>Authors: </strong>Soro Bedionita, Bruno Andreis, Song Chong, Sung Ju Hwang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02012">https://arxiv.org/abs/2504.02012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02012">https://arxiv.org/pdf/2504.02012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02012]] Instruction-Guided Autoregressive Neural Network Parameter Generation(https://arxiv.org/abs/2504.02012)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Learning to generate neural network parameters conditioned on task descriptions and architecture specifications is pivotal for advancing model adaptability and transfer learning. Existing methods especially those based on diffusion models suffer from limited scalability to large architectures, rigidity in handling varying network depths, and disjointed parameter generation that undermines inter-layer coherence. In this work, we propose IGPG (Instruction Guided Parameter Generation), an autoregressive framework that unifies parameter synthesis across diverse tasks and architectures. IGPG leverages a VQ-VAE and an autoregressive model to generate neural network parameters, conditioned on task instructions, dataset, and architecture details. By autoregressively generating neural network weights' tokens, IGPG ensures inter-layer coherence and enables efficient adaptation across models and datasets. Operating at the token level, IGPG effectively captures complex parameter distributions aggregated from a broad spectrum of pretrained models. Extensive experiments on multiple vision datasets demonstrate that IGPG consolidates diverse pretrained models into a single, flexible generative framework. The synthesized parameters achieve competitive or superior performance relative to state-of-the-art methods, especially in terms of scalability and efficiency when applied to large architectures. These results underscore ICPG potential as a powerful tool for pretrained weight retrieval, model selection, and rapid task-specific fine-tuning.</li>
</ul>

<h3>Title: Fourier Feature Attribution: A New Efficiency Attribution Method</h3>
<ul>
<li><strong>Authors: </strong>Zechen Liu, Feiyang Zhang, Wei Song, Xiang Li, Wei Wei</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02016">https://arxiv.org/abs/2504.02016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02016">https://arxiv.org/pdf/2504.02016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02016]] Fourier Feature Attribution: A New Efficiency Attribution Method(https://arxiv.org/abs/2504.02016)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The study of neural networks from the perspective of Fourier features has garnered significant attention. While existing analytical research suggests that neural networks tend to learn low-frequency features, a clear attribution method for identifying the specific learned Fourier features has remained elusive. To bridge this gap, we propose a novel Fourier feature attribution method grounded in signal decomposition theory. Additionally, we analyze the differences between game-theoretic attribution metrics for Fourier and spatial domain features, demonstrating that game-theoretic evaluation metrics are better suited for Fourier-based feature attribution. Our experiments show that Fourier feature attribution exhibits superior feature selection capabilities compared to spatial domain attribution methods. For instance, in the case of Vision Transformers (ViTs) on the ImageNet dataset, only $8\%$ of the Fourier features are required to maintain the original predictions for $80\%$ of the samples. Furthermore, we compare the specificity of features identified by our method against traditional spatial domain attribution methods. Results reveal that Fourier features exhibit greater intra-class concentration and inter-class distinctiveness, indicating their potential for more efficient classification and explainable AI algorithms.</li>
</ul>

<h3>Title: Geometric Reasoning in the Embedding Space</h3>
<ul>
<li><strong>Authors: </strong>Jan Hůla, David Mojžíšek, Jiří Janeček, David Herel, Mikoláš Janota</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02018">https://arxiv.org/abs/2504.02018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02018">https://arxiv.org/pdf/2504.02018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02018]] Geometric Reasoning in the Embedding Space(https://arxiv.org/abs/2504.02018)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this contribution, we demonstrate that Graph Neural Networks and Transformers can learn to reason about geometric constraints. We train them to predict spatial position of points in a discrete 2D grid from a set of constraints that uniquely describe hidden figures containing these points. Both models are able to predict the position of points and interestingly, they form the hidden figures described by the input constraints in the embedding space during the reasoning process. Our analysis shows that both models recover the grid structure during training so that the embeddings corresponding to the points within the grid organize themselves in a 2D subspace and reflect the neighborhood structure of the grid. We also show that the Graph Neural Network we design for the task performs significantly better than the Transformer and is also easier to scale.</li>
</ul>

<h3>Title: LSC-ADL: An Activity of Daily Living (ADL)-Annotated Lifelog Dataset Generated via Semi-Automatic Clustering</h3>
<ul>
<li><strong>Authors: </strong>Minh-Quan Ho-Le, Duy-Khang Ho, Van-Tu Ninh, Cathal Gurrin, Minh-Triet Tran</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02060">https://arxiv.org/abs/2504.02060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02060">https://arxiv.org/pdf/2504.02060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02060]] LSC-ADL: An Activity of Daily Living (ADL)-Annotated Lifelog Dataset Generated via Semi-Automatic Clustering(https://arxiv.org/abs/2504.02060)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Lifelogging involves continuously capturing personal data through wearable cameras, providing an egocentric view of daily activities. Lifelog retrieval aims to search and retrieve relevant moments from this data, yet existing methods largely overlook activity-level annotations, which capture temporal relationships and enrich semantic understanding. In this work, we introduce LSC-ADL, an ADL-annotated lifelog dataset derived from the LSC dataset, incorporating Activities of Daily Living (ADLs) as a structured semantic layer. Using a semi-automatic approach featuring the HDBSCAN algorithm for intra-class clustering and human-in-the-loop verification, we generate accurate ADL annotations to enhance retrieval explainability. By integrating action recognition into lifelog retrieval, LSC-ADL bridges a critical gap in existing research, offering a more context-aware representation of daily life. We believe this dataset will advance research in lifelog retrieval, activity recognition, and egocentric vision, ultimately improving the accuracy and interpretability of retrieved content. The ADL annotations can be downloaded at this https URL.</li>
</ul>

<h3>Title: Aligned Better, Listen Better for Audio-Visual Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Guo, Shuailei Ma, Shijie Ma, Xiaoyi Bao, Chen-Wei Xie, Kecheng Zheng, Tingyu Weng, Siyang Sun, Yun Zheng, Wei Zou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02061">https://arxiv.org/abs/2504.02061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02061">https://arxiv.org/pdf/2504.02061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02061]] Aligned Better, Listen Better for Audio-Visual Large Language Models(https://arxiv.org/abs/2504.02061)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Audio is essential for multimodal video understanding. On the one hand, video inherently contains audio, which supplies complementary information to vision. Besides, video large language models (Video-LLMs) can encounter many audio-centric settings. However, existing Video-LLMs and Audio-Visual Large Language Models (AV-LLMs) exhibit deficiencies in exploiting audio information, leading to weak understanding and hallucinations. To solve the issues, we delve into the model architecture and dataset. (1) From the architectural perspective, we propose a fine-grained AV-LLM, namely Dolphin. The concurrent alignment of audio and visual modalities in both temporal and spatial dimensions ensures a comprehensive and accurate understanding of videos. Specifically, we devise an audio-visual multi-scale adapter for multi-scale information aggregation, which achieves spatial alignment. For temporal alignment, we propose audio-visual interleaved merging. (2) From the dataset perspective, we curate an audio-visual caption and instruction-tuning dataset, called AVU. It comprises 5.2 million diverse, open-ended data tuples (video, audio, question, answer) and introduces a novel data partitioning strategy. Extensive experiments show our model not only achieves remarkable performance in audio-visual understanding, but also mitigates potential hallucinations.</li>
</ul>

<h3>Title: From Text to Graph: Leveraging Graph Neural Networks for Enhanced Explainability in NLP</h3>
<ul>
<li><strong>Authors: </strong>Fabio Yáñez-Romero, Andrés Montoyo, Armando Suárez, Yoan Gutiérrez, Ruslan Mitkov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02064">https://arxiv.org/abs/2504.02064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02064">https://arxiv.org/pdf/2504.02064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02064]] From Text to Graph: Leveraging Graph Neural Networks for Enhanced Explainability in NLP(https://arxiv.org/abs/2504.02064)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, transformer, generative</a></li>
<li><strong>Abstract: </strong>Researchers have relegated natural language processing tasks to Transformer-type models, particularly generative models, because these models exhibit high versatility when performing generation and classification tasks. As the size of these models increases, they achieve outstanding results. Given their widespread use, many explainability techniques are developed based on these models. However, this process becomes computationally expensive due to the large size of the models. Additionally, transformers interpret input information through tokens that fragment input words into sequences lacking inherent semantic meaning, complicating the explanation of the model from the very beginning. This study proposes a novel methodology to achieve explainability in natural language processing tasks by automatically converting sentences into graphs and maintaining semantics through nodes and relations that express fundamental linguistic concepts. It also allows the subsequent exploitation of this knowledge in subsequent tasks, making it possible to obtain trends and understand how the model associates the different elements inside the text with the explained task. The experiments delivered promising results in determining the most critical components within the text structure for a given classification.</li>
</ul>

<h3>Title: Privacy-Preserving Edge Computing from Pairing-Based Inner Product Functional Encryption</h3>
<ul>
<li><strong>Authors: </strong>Utsav Banerjee</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02068">https://arxiv.org/abs/2504.02068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02068">https://arxiv.org/pdf/2504.02068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02068]] Privacy-Preserving Edge Computing from Pairing-Based Inner Product Functional Encryption(https://arxiv.org/abs/2504.02068)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>Pairing-based inner product functional encryption provides an efficient theoretical construction for privacy-preserving edge computing secured by widely deployed elliptic curve cryptography. In this work, an efficient software implementation framework for pairing-based function-hiding inner product encryption (FHIPE) is presented using the recently proposed and widely adopted BLS12-381 pairing-friendly elliptic curve. Algorithmic optimizations provide $\approx 2.6 \times$ and $\approx 3.4 \times$ speedup in FHIPE encryption and decryption respectively, and extensive performance analysis is presented using a Raspberry Pi 4B edge device. The proposed optimizations enable this implementation framework to achieve performance and ciphertext size comparable to previous work despite being implemented on an edge device with a slower processor and supporting a curve at much higher security level with a larger prime field. Practical privacy-preserving edge computing applications such as encrypted biomedical sensor data classification and secure wireless fingerprint-based indoor localization are also demonstrated using the proposed implementation framework.</li>
</ul>

<h3>Title: Evolving Security in LLMs: A Study of Jailbreak Attacks and Defenses</h3>
<ul>
<li><strong>Authors: </strong>Zhengchun Shang, Wenlan Wei</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02080">https://arxiv.org/abs/2504.02080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02080">https://arxiv.org/pdf/2504.02080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02080]] Evolving Security in LLMs: A Study of Jailbreak Attacks and Defenses(https://arxiv.org/abs/2504.02080)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly popular, powering a wide range of applications. Their widespread use has sparked concerns, especially through jailbreak attacks that bypass safety measures to produce harmful content. In this paper, we present a comprehensive security analysis of large language models (LLMs), addressing critical research questions on the evolution and determinants of model safety. Specifically, we begin by identifying the most effective techniques for detecting jailbreak attacks. Next, we investigate whether newer versions of LLMs offer improved security compared to their predecessors. We also assess the impact of model size on overall security and explore the potential benefits of integrating multiple defense strategies to enhance model robustness. Our study evaluates both open-source models (e.g., LLaMA and Mistral) and closed-source systems (e.g., GPT-4) by employing four state-of-the-art attack techniques and assessing the efficacy of three new defensive approaches.</li>
</ul>

<h3>Title: Increasing happiness through conversations with artificial intelligence</h3>
<ul>
<li><strong>Authors: </strong>Joseph Heffner, Chongyu Qin, Martin Chadwick, Chris Knutsen, Christopher Summerfield, Zeb Kurth-Nelson, Robb B. Rutledge</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02091">https://arxiv.org/abs/2504.02091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02091">https://arxiv.org/pdf/2504.02091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02091]] Increasing happiness through conversations with artificial intelligence(https://arxiv.org/abs/2504.02091)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chatbots powered by artificial intelligence (AI) have rapidly become a significant part of everyday life, with over a quarter of American adults using them multiple times per week. While these tools offer potential benefits and risks, a fundamental question remains largely unexplored: How do conversations with AI influence subjective well-being? To investigate this, we conducted a study where participants either engaged in conversations with an AI chatbot (N = 334) or wrote journal entires (N = 193) on the same randomly assigned topics and reported their momentary happiness afterward. We found that happiness after AI chatbot conversations was higher than after journaling, particularly when discussing negative topics such as depression or guilt. Leveraging large language models for sentiment analysis, we found that the AI chatbot mirrored participants' sentiment while maintaining a consistent positivity bias. When discussing negative topics, participants gradually aligned their sentiment with the AI's positivity, leading to an overall increase in happiness. We hypothesized that the history of participants' sentiment prediction errors, the difference between expected and actual emotional tone when responding to the AI chatbot, might explain this happiness effect. Using computational modeling, we find the history of these sentiment prediction errors over the course of a conversation predicts greater post-conversation happiness, demonstrating a central role of emotional expectations during dialogue. Our findings underscore the effect that AI interactions can have on human well-being.</li>
</ul>

<h3>Title: FlowDistill: Scalable Traffic Flow Prediction via Distillation from LLMs</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Yu, Xinpeng Xie, Yan Huang, Chenxi Qiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02094">https://arxiv.org/abs/2504.02094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02094">https://arxiv.org/pdf/2504.02094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02094]] FlowDistill: Scalable Traffic Flow Prediction via Distillation from LLMs(https://arxiv.org/abs/2504.02094)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Accurate traffic flow prediction is vital for optimizing urban mobility, yet it remains difficult in many cities due to complex spatio-temporal dependencies and limited high-quality data. While deep graph-based models demonstrate strong predictive power, their performance often comes at the cost of high computational overhead and substantial training data requirements, making them impractical for deployment in resource-constrained or data-scarce environments. We propose the FlowDistill, a lightweight and scalable traffic prediction framework based on knowledge distillation from large language models (LLMs). In this teacher-student setup, a fine-tuned LLM guides a compact multi-layer perceptron (MLP) student model using a novel combination of the information bottleneck principle and teacher-bounded regression loss, ensuring the distilled model retains only essential and transferable knowledge. Spatial and temporal correlations are explicitly encoded to enhance the model's generalization across diverse urban settings. Despite its simplicity, FlowDistill consistently outperforms state-of-the-art models in prediction accuracy while requiring significantly less training data, and achieving lower memory usage and inference latency, highlighting its efficiency and suitability for real-world, scalable deployment.</li>
</ul>

<h3>Title: Chunking Attacks on File Backup Services using Content-Defined Chunking</h3>
<ul>
<li><strong>Authors: </strong>Boris Alexeev, Colin Percival, Yan X Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02095">https://arxiv.org/abs/2504.02095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02095">https://arxiv.org/pdf/2504.02095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02095]] Chunking Attacks on File Backup Services using Content-Defined Chunking(https://arxiv.org/abs/2504.02095)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, extraction</a></li>
<li><strong>Abstract: </strong>Systems such as file backup services often use content-defined chunking (CDC) algorithms, especially those based on rolling hash techniques, to split files into chunks in a way that allows for data deduplication. These chunking algorithms often depend on per-user parameters in an attempt to avoid leaking information about the data being stored. We present attacks to extract these chunking parameters and discuss protocol-agnostic attacks and loss of security once the parameters are breached (including when these parameters are not setup at all, which is often available as an option). Our parameter-extraction attacks themselves are protocol-specific but their ideas are generalizable to many potential CDC schemes.</li>
</ul>

<h3>Title: ContrastScore: Towards Higher Quality, Less Biased, More Efficient Evaluation Metrics with Contrastive Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Xiao Wang, Daniil Larionov, Siwei Wu, Yiqi Liu, Steffen Eger, Nafise Sadat Moosavi, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02106">https://arxiv.org/abs/2504.02106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02106">https://arxiv.org/pdf/2504.02106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02106]] ContrastScore: Towards Higher Quality, Less Biased, More Efficient Evaluation Metrics with Contrastive Evaluation(https://arxiv.org/abs/2504.02106)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Evaluating the quality of generated text automatically remains a significant challenge. Conventional reference-based metrics have been shown to exhibit relatively weak correlation with human evaluations. Recent research advocates the use of large language models (LLMs) as source-based metrics for natural language generation (NLG) assessment. While promising, LLM-based metrics, particularly those using smaller models, still fall short in aligning with human judgments. In this work, we introduce ContrastScore, a contrastive evaluation metric designed to enable higher-quality, less biased, and more efficient assessment of generated text. We evaluate ContrastScore on two NLG tasks: machine translation and summarization. Experimental results show that ContrastScore consistently achieves stronger correlation with human judgments than both single-model and ensemble-based baselines. Notably, ContrastScore based on Qwen 3B and 0.5B even outperforms Qwen 7B, despite having only half as many parameters, demonstrating its efficiency. Furthermore, it effectively mitigates common evaluation biases such as length and likelihood preferences, resulting in more robust automatic evaluation.</li>
</ul>

<h3>Title: TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Jeffrey Li, Mohammadreza Armandpour, Iman Mirzadeh, Sachin Mehta, Vaishaal Shankar, Raviteja Vemulapalli, Samy Bengio, Oncel Tuzel, Mehrdad Farajtabar, Hadi Pouransari, Fartash Faghri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02107">https://arxiv.org/abs/2504.02107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02107">https://arxiv.org/pdf/2504.02107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02107]] TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining(https://arxiv.org/abs/2504.02107)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) trained on historical web data inevitably become outdated. We investigate evaluation strategies and update methods for LLMs as new data becomes available. We introduce a web-scale dataset for time-continual pretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of magnitude larger than previous continual language modeling benchmarks. We also design time-stratified evaluations across both general CC data and specific domains (Wikipedia, StackExchange, and code documentation) to assess how well various continual learning methods adapt to new data while retaining past knowledge. Our findings demonstrate that, on general CC data, autoregressive meta-schedules combined with a fixed-ratio replay of older data can achieve comparable held-out loss to re-training from scratch, while requiring significantly less computation (2.6x). However, the optimal balance between incorporating new data and replaying old data differs as replay is crucial to avoid forgetting on generic web data but less so on specific domains.</li>
</ul>

<h3>Title: A Systematic Review of Security Communication Strategies: Guidelines and Open Challenges</h3>
<ul>
<li><strong>Authors: </strong>Carolina Carreira, Alexandra Mendes, João F. Ferreira, Nicolas Christin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02109">https://arxiv.org/abs/2504.02109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02109">https://arxiv.org/pdf/2504.02109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02109]] A Systematic Review of Security Communication Strategies: Guidelines and Open Challenges(https://arxiv.org/abs/2504.02109)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Cybersecurity incidents such as data breaches have become increasingly common, affecting millions of users and organizations worldwide. The complexity of cybersecurity threats challenges the effectiveness of existing security communication strategies. Through a systematic review of over 3,400 papers, we identify specific user difficulties including information overload, technical jargon comprehension, and balancing security awareness with comfort. Our findings reveal consistent communication paradoxes: users require technical details for credibility yet struggle with jargon and need risk awareness without experiencing anxiety. We propose seven evidence-based guidelines to improve security communication and identify critical research gaps including limited studies with older adults, children, and non-US populations, insufficient longitudinal research, and limited protocol sharing for reproducibility. Our guidelines emphasize user-centric communication adapted to cultural and demographic differences while ensuring security advice remains actionable. This work contributes to more effective security communication practices that enable users to recognize and respond to cybersecurity threats appropriately.</li>
</ul>

<h3>Title: PolyG: Effective and Efficient GraphRAG with Adaptive Graph Traversal</h3>
<ul>
<li><strong>Authors: </strong>Renjie Liu, Haitian Jiang, Xiao Yan, Bo Tang, Jinyang Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02112">https://arxiv.org/abs/2504.02112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02112">https://arxiv.org/pdf/2504.02112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02112]] PolyG: Effective and Efficient GraphRAG with Adaptive Graph Traversal(https://arxiv.org/abs/2504.02112)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>GraphRAG enhances large language models (LLMs) to generate quality answers for user questions by retrieving related facts from external knowledge graphs. Existing GraphRAG methods adopt a fixed graph traversal strategy for fact retrieval but we observe that user questions come in different types and require different graph traversal strategies. As such, existing GraphRAG methods are limited in effectiveness (i.e., quality of the generated answers) and/or efficiency (i.e., response time or the number of used tokens). In this paper, we propose to classify the questions according to a complete four-class taxonomy and adaptively select the appropriate graph traversal strategy for each type of questions. Our system PolyG is essentially a query planner for GraphRAG and can handle diverse questions with an unified interface and execution engine. Compared with SOTA GraphRAG methods, PolyG achieves an overall win rate of 75% on generation quality and a speedup up to 4x on response time.</li>
</ul>

<h3>Title: On Model Protection in Federated Learning against Eavesdropping Attacks</h3>
<ul>
<li><strong>Authors: </strong>Dipankar Maity, Kushal Chakrabarti</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, eess.SY, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02114">https://arxiv.org/abs/2504.02114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02114">https://arxiv.org/pdf/2504.02114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02114]] On Model Protection in Federated Learning against Eavesdropping Attacks(https://arxiv.org/abs/2504.02114)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, federate</a></li>
<li><strong>Abstract: </strong>In this study, we investigate the protection offered by federated learning algorithms against eavesdropping adversaries. In our model, the adversary is capable of intercepting model updates transmitted from clients to the server, enabling it to create its own estimate of the model. Unlike previous research, which predominantly focuses on safeguarding client data, our work shifts attention protecting the client model itself. Through a theoretical analysis, we examine how various factors, such as the probability of client selection, the structure of local objective functions, global aggregation at the server, and the eavesdropper's capabilities, impact the overall level of protection. We further validate our findings through numerical experiments, assessing the protection by evaluating the model accuracy achieved by the adversary. Finally, we compare our results with methods based on differential privacy, underscoring their limitations in this specific context.</li>
</ul>

<h3>Title: LLMPi: Optimizing LLMs for High-Throughput on Raspberry Pi</h3>
<ul>
<li><strong>Authors: </strong>Mahsa Ardakani, Jinendra Malekar, Ramtin Zand</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02118">https://arxiv.org/abs/2504.02118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02118">https://arxiv.org/pdf/2504.02118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02118]] LLMPi: Optimizing LLMs for High-Throughput on Raspberry Pi(https://arxiv.org/abs/2504.02118)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Deploying Large Language Models (LLMs) on resource-constrained edge devices like the Raspberry Pi presents challenges in computational efficiency, power consumption, and response latency. This paper explores quantization-based optimization techniques to enable high-throughput, energy-efficient execution of LLMs on low-power embedded systems. Our approach leverages k-quantization, a Post-Training Quantization (PTQ) method designed for different bit-widths, enabling efficient 2-bit, 4-bit, 6-bit, and 8-bit weight quantization. Additionally, we employ ternary quantization using Quantization-Aware Training (QAT) for BitNet models, allowing for more effective adaptation to lower-bit representations while preserving accuracy. Our findings highlight the potential of quantized LLMs for real-time conversational AI on edge devices, paving the way for low-power, high-efficiency AI deployment in mobile and embedded applications. This study demonstrates that aggressive quantization strategies can significantly reduce energy consumption while maintaining inference quality, making LLMs practical for resource-limited environments.</li>
</ul>

<h3>Title: Efficient Model Selection for Time Series Forecasting via LLMs</h3>
<ul>
<li><strong>Authors: </strong>Wang Wei, Tiankai Yang, Hongjie Chen, Ryan A. Rossi, Yue Zhao, Franck Dernoncourt, Hoda Eldardiry</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02119">https://arxiv.org/abs/2504.02119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02119">https://arxiv.org/pdf/2504.02119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02119]] Efficient Model Selection for Time Series Forecasting via LLMs(https://arxiv.org/abs/2504.02119)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting.</li>
</ul>

<h3>Title: Graph Analytics for Cyber-Physical System Resilience Quantification</h3>
<ul>
<li><strong>Authors: </strong>Romain Dagnas, Michel Barbeau, Joaquin Garcia-Alfaro, Reda Yaich</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02120">https://arxiv.org/abs/2504.02120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02120">https://arxiv.org/pdf/2504.02120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02120]] Graph Analytics for Cyber-Physical System Resilience Quantification(https://arxiv.org/abs/2504.02120)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack</a></li>
<li><strong>Abstract: </strong>Critical infrastructures integrate a wide range of smart technologies and become highly connected to the cyber world. This is especially true for Cyber-Physical Systems (CPSs), which integrate hardware and software components. Despite the advantages of smart infrastructures, they remain vulnerable to cyberattacks. This work focuses on the cyber resilience of CPSs. We propose a methodology based on knowledge graph modeling and graph analytics to quantify the resilience potential of complex systems by using a multilayered model based on knowledge graphs. Our methodology also allows us to identify critical points. These critical points are components or functions of an architecture that can generate critical failures if attacked. Thus, identifying them can help enhance resilience and avoid cascading effects. We use the SWaT (Secure Water Treatment) testbed as a use case to achieve this objective. This system mimics the actual behavior of a water treatment station in Singapore. We model three resilient designs of SWaT according to our multilayered model. We conduct a resilience assessment based on three relevant metrics used in graph analytics. We compare the results obtained with each metric and discuss their accuracy in identifying critical points. We perform an experimentation analysis based on the knowledge gained by a cyber adversary about the system architecture. We show that the most resilient SWaT design has the necessary potential to bounce back and absorb the attacks. We discuss our results and conclude this work by providing further research axes.</li>
</ul>

<h3>Title: Are Users More Willing to Use Formally Verified Password Managers?</h3>
<ul>
<li><strong>Authors: </strong>Carolina Carreira, João F. Ferreira, Alexandra Mendes, Nicolas Christin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.HC, cs.LO, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02124">https://arxiv.org/abs/2504.02124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02124">https://arxiv.org/pdf/2504.02124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02124]] Are Users More Willing to Use Formally Verified Password Managers?(https://arxiv.org/abs/2504.02124)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Formal verification has recently been increasingly used to prove the correctness and security of many applications. It is attractive because it can prove the absence of errors with the same certainty as mathematicians proving theorems. However, while most security experts recognize the value of formal verification, the views of non-technical users on this topic are unknown. To address this issue, we designed and implemented two experiments to understand how formal verification impacts users. Our approach started with a formative study involving 15 participants, followed by the main quantitative study with 200 individuals. We focus on the application domain of password managers since it has been documented that the lack of trust in password managers might lead to lower adoption. Moreover, recent efforts have focused on formally verifying (parts of) password managers. We conclude that formal verification is seen as desirable by users and identify three actional recommendations to improve formal verification communication efforts.</li>
</ul>

<h3>Title: One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Ezzeldin Shereen, Dan Ristea, Burak Hasircioglu, Shae McFadden, Vasilios Mavroudis, Chris Hicks</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02132">https://arxiv.org/abs/2504.02132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02132">https://arxiv.org/pdf/2504.02132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02132]] One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image(https://arxiv.org/abs/2504.02132)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, generative</a></li>
<li><strong>Abstract: </strong>Multimodal retrieval augmented generation (M-RAG) has recently emerged as a method to inhibit hallucinations of large multimodal models (LMMs) through a factual knowledge base (KB). However, M-RAG also introduces new attack vectors for adversaries that aim to disrupt the system by injecting malicious entries into the KB. In this work, we present a poisoning attack against M-RAG targeting visual document retrieval applications, where the KB contains images of document pages. Our objective is to craft a single image that is retrieved for a variety of different user queries, and consistently influences the output produced by the generative model, thus creating a universal denial-of-service (DoS) attack against the M-RAG system. We demonstrate that while our attack is effective against a diverse range of widely-used, state-of-the-art retrievers (embedding models) and generators (LMMs), it can also be ineffective against robust embedding models. Our attack not only highlights the vulnerability of M-RAG pipelines to poisoning attacks, but also sheds light on a fundamental weakness that potentially hinders their performance even in benign settings.</li>
</ul>

<h3>Title: Base Station Certificate and Multi-Factor Authentication for Cellular Radio Control Communication Security</h3>
<ul>
<li><strong>Authors: </strong>Sourav Purification, Simeon Wuthier, Jinoh Kim, Ikkyun Kim, Sang-Yoon Chang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02133">https://arxiv.org/abs/2504.02133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02133">https://arxiv.org/pdf/2504.02133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02133]] Base Station Certificate and Multi-Factor Authentication for Cellular Radio Control Communication Security(https://arxiv.org/abs/2504.02133)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Current cellular networking remains vulnerable to malicious fake base stations due to the lack of base station authentication mechanism or even a key to enable authentication. We design and build a base station certificate (certifying the base station's public key and location) and a multi-factor authentication (making use of the certificate and the information transmitted in the online radio control communications) to secure the authenticity and message integrity of the base station control communications. We advance beyond the state-of-the-art research by introducing greater authentication factors (and analyzing their individual security properties and benefits), and by using blockchain to deliver the base station digital certificate offline (enabling greater key length or security strength and computational or networking efficiency). We design the certificate construction, delivery, and the multi-factor authentication use on the user equipment. The user verification involves multiple factors verified through the ledger database, the location sensing (GPS in our implementation), and the cryptographic signature verification of the cellular control communication (SIB1 broadcasting). We analyze our scheme's security, performance, and the fit to the existing standardized networking protocols. Our work involves the implementation of building on X.509 certificate (adapted), smart contract-based blockchain, 5G-standardized RRC control communications, and software-defined radios. Our analyses show that our scheme effectively defends against more security threats and can enable stronger security, i.e., ECDSA with greater key lengths. Furthermore, our scheme enables computing and energy to be more than three times efficient than the previous research on the mobile user equipment.</li>
</ul>

<h3>Title: Like Oil and Water: Group Robustness Methods and Poisoning Defenses May Be at Odds</h3>
<ul>
<li><strong>Authors: </strong>Michael-Andrei Panaitescu-Liess, Yigitcan Kaya, Sicheng Zhu, Furong Huang, Tudor Dumitras</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02142">https://arxiv.org/abs/2504.02142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02142">https://arxiv.org/pdf/2504.02142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02142]] Like Oil and Water: Group Robustness Methods and Poisoning Defenses May Be at Odds(https://arxiv.org/abs/2504.02142)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust, federate</a></li>
<li><strong>Abstract: </strong>Group robustness has become a major concern in machine learning (ML) as conventional training paradigms were found to produce high error on minority groups. Without explicit group annotations, proposed solutions rely on heuristics that aim to identify and then amplify the minority samples during training. In our work, we first uncover a critical shortcoming of these methods: an inability to distinguish legitimate minority samples from poison samples in the training set. By amplifying poison samples as well, group robustness methods inadvertently boost the success rate of an adversary -- e.g., from $0\%$ without amplification to over $97\%$ with it. Notably, we supplement our empirical evidence with an impossibility result proving this inability of a standard heuristic under some assumptions. Moreover, scrutinizing recent poisoning defenses both in centralized and federated learning, we observe that they rely on similar heuristics to identify which samples should be eliminated as poisons. In consequence, minority samples are eliminated along with poisons, which damages group robustness -- e.g., from $55\%$ without the removal of the minority samples to $41\%$ with it. Finally, as they pursue opposing goals using similar heuristics, our attempt to alleviate the trade-off by combining group robustness methods and poisoning defenses falls short. By exposing this tension, we also hope to highlight how benchmark-driven ML scholarship can obscure the trade-offs among different metrics with potentially detrimental consequences.</li>
</ul>

<h3>Title: Towards Interpretable Soft Prompts</h3>
<ul>
<li><strong>Authors: </strong>Oam Patel, Jason Wang, Nikhil Shivakumar Nayak, Suraj Srinivas, Himabindu Lakkaraju</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02144">https://arxiv.org/abs/2504.02144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02144">https://arxiv.org/pdf/2504.02144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02144]] Towards Interpretable Soft Prompts(https://arxiv.org/abs/2504.02144)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Soft prompts have been popularized as a cheap and easy way to improve task-specific LLM performance beyond few-shot prompts. Despite their origin as an automated prompting method, however, soft prompts and other trainable prompts remain a black-box method with no immediately interpretable connections to prompting. We create a novel theoretical framework for evaluating the interpretability of trainable prompts based on two desiderata: faithfulness and scrutability. We find that existing methods do not naturally satisfy our proposed interpretability criterion. Instead, our framework inspires a new direction of trainable prompting methods that explicitly optimizes for interpretability. To this end, we formulate and test new interpretability-oriented objective functions for two state-of-the-art prompt tuners: Hard Prompts Made Easy (PEZ) and RLPrompt. Our experiments with GPT-2 demonstrate a fundamental trade-off between interpretability and the task-performance of the trainable prompt, explicating the hardness of the soft prompt interpretability problem and revealing odd behavior that arises when one optimizes for an interpretability proxy.</li>
</ul>

<h3>Title: LL4G: Self-Supervised Dynamic Optimization for Graph-Based Personality Detection</h3>
<ul>
<li><strong>Authors: </strong>Lingzhi Shen, Yunfei Long, Xiaohao Cai, Guanming Chen, Yuhan Wang, Imran Razzak, Shoaib Jameel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02146">https://arxiv.org/abs/2504.02146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02146">https://arxiv.org/pdf/2504.02146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02146]] LL4G: Self-Supervised Dynamic Optimization for Graph-Based Personality Detection(https://arxiv.org/abs/2504.02146)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Graph-based personality detection constructs graph structures from textual data, particularly social media posts. Current methods often struggle with sparse or noisy data and rely on static graphs, limiting their ability to capture dynamic changes between nodes and relationships. This paper introduces LL4G, a self-supervised framework leveraging large language models (LLMs) to optimize graph neural networks (GNNs). LLMs extract rich semantic features to generate node representations and to infer explicit and implicit relationships. The graph structure adaptively adds nodes and edges based on input data, continuously optimizing itself. The GNN then uses these optimized representations for joint training on node reconstruction, edge prediction, and contrastive learning tasks. This integration of semantic and structural information generates robust personality profiles. Experimental results on Kaggle and Pandora datasets show LL4G outperforms state-of-the-art models.</li>
</ul>

<h3>Title: FreSca: Unveiling the Scaling Space in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Chao Huang, Susan Liang, Yunlong Tang, Li Ma, Yapeng Tian, Chenliang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02154">https://arxiv.org/abs/2504.02154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02154">https://arxiv.org/pdf/2504.02154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02154]] FreSca: Unveiling the Scaling Space in Diffusion Models(https://arxiv.org/abs/2504.02154)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models offer impressive controllability for image tasks, primarily through noise predictions that encode task-specific information and classifier-free guidance enabling adjustable scaling. This scaling mechanism implicitly defines a ``scaling space'' whose potential for fine-grained semantic manipulation remains underexplored. We investigate this space, starting with inversion-based editing where the difference between conditional/unconditional noise predictions carries key semantic information. Our core contribution stems from a Fourier analysis of noise predictions, revealing that its low- and high-frequency components evolve differently throughout diffusion. Based on this insight, we introduce FreSca, a straightforward method that applies guidance scaling independently to different frequency bands in the Fourier domain. FreSca demonstrably enhances existing image editing methods without retraining. Excitingly, its effectiveness extends to image understanding tasks such as depth estimation, yielding quantitative gains across multiple datasets.</li>
</ul>

<h3>Title: Less-to-More Generalization: Unlocking More Controllability by In-Context Generation</h3>
<ul>
<li><strong>Authors: </strong>Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, Qian He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02160">https://arxiv.org/abs/2504.02160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02160">https://arxiv.org/pdf/2504.02160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02160]] Less-to-More Generalization: Unlocking More Controllability by In-Context Generation(https://arxiv.org/abs/2504.02160)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose a highly-consistent data synthesis pipeline to tackle this challenge. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding. It is a multi-image conditioned subject-to-image model iteratively trained from a text-to-image model. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation.</li>
</ul>

<h3>Title: MDP: Multidimensional Vision Model Pruning with Latency Constraint</h3>
<ul>
<li><strong>Authors: </strong>Xinglong Sun, Barath Lakshmanan, Maying Shen, Shiyi Lan, Jingde Chen, Jose M. Alvarez</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02168">https://arxiv.org/abs/2504.02168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02168">https://arxiv.org/pdf/2504.02168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02168]] MDP: Multidimensional Vision Model Pruning with Latency Constraint(https://arxiv.org/abs/2504.02168)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Current structural pruning methods face two significant limitations: (i) they often limit pruning to finer-grained levels like channels, making aggressive parameter reduction challenging, and (ii) they focus heavily on parameter and FLOP reduction, with existing latency-aware methods frequently relying on simplistic, suboptimal linear models that fail to generalize well to transformers, where multiple interacting dimensions impact latency. In this paper, we address both limitations by introducing Multi-Dimensional Pruning (MDP), a novel paradigm that jointly optimizes across a variety of pruning granularities-including channels, query, key, heads, embeddings, and blocks. MDP employs an advanced latency modeling technique to accurately capture latency variations across all prunable dimensions, achieving an optimal balance between latency and accuracy. By reformulating pruning as a Mixed-Integer Nonlinear Program (MINLP), MDP efficiently identifies the optimal pruned structure across all prunable dimensions while respecting latency constraints. This versatile framework supports both CNNs and transformers. Extensive experiments demonstrate that MDP significantly outperforms previous methods, especially at high pruning ratios. On ImageNet, MDP achieves a 28% speed increase with a +1.4 Top-1 accuracy improvement over prior work like HALP for ResNet50 pruning. Against the latest transformer pruning method, Isomorphic, MDP delivers an additional 37% acceleration with a +0.7 Top-1 accuracy improvement.</li>
</ul>

<h3>Title: Subasa -- Adapting Language Models for Low-resourced Offensive Language Detection in Sinhala</h3>
<ul>
<li><strong>Authors: </strong>Shanilka Haturusinghe, Tharindu Cyril Weerasooriya, Marcos Zampieri, Christopher M. Homan, S.R. Liyanage</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02178">https://arxiv.org/abs/2504.02178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02178">https://arxiv.org/pdf/2504.02178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02178]] Subasa -- Adapting Language Models for Low-resourced Offensive Language Detection in Sinhala(https://arxiv.org/abs/2504.02178)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Accurate detection of offensive language is essential for a number of applications related to social media safety. There is a sharp contrast in performance in this task between low and high-resource languages. In this paper, we adapt fine-tuning strategies that have not been previously explored for Sinhala in the downstream task of offensive language detection. Using this approach, we introduce four models: "Subasa-XLM-R", which incorporates an intermediate Pre-Finetuning step using Masked Rationale Prediction. Two variants of "Subasa-Llama" and "Subasa-Mistral", are fine-tuned versions of Llama (3.2) and Mistral (v0.3), respectively, with a task-specific strategy. We evaluate our models on the SOLD benchmark dataset for Sinhala offensive language detection. All our models outperform existing baselines. Subasa-XLM-R achieves the highest Macro F1 score (0.84) surpassing state-of-the-art large language models like GPT-4o when evaluated on the same SOLD benchmark dataset under zero-shot settings. The models and code are publicly available.</li>
</ul>

<h3>Title: ESC: Erasing Space Concept for Knowledge Deletion</h3>
<ul>
<li><strong>Authors: </strong>Tae-Young Lee, Sundong Park, Minwoo Jeon, Hyoseok Hwang, Gyeong-Moon Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02199">https://arxiv.org/abs/2504.02199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02199">https://arxiv.org/pdf/2504.02199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02199]] ESC: Erasing Space Concept for Knowledge Deletion(https://arxiv.org/abs/2504.02199)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>As concerns regarding privacy in deep learning continue to grow, individuals are increasingly apprehensive about the potential exploitation of their personal knowledge in trained models. Despite several research efforts to address this, they often fail to consider the real-world demand from users for complete knowledge erasure. Furthermore, our investigation reveals that existing methods have a risk of leaking personal knowledge through embedding features. To address these issues, we introduce a novel concept of Knowledge Deletion (KD), an advanced task that considers both concerns, and provides an appropriate metric, named Knowledge Retention score (KR), for assessing knowledge retention in feature space. To achieve this, we propose a novel training-free erasing approach named Erasing Space Concept (ESC), which restricts the important subspace for the forgetting knowledge by eliminating the relevant activations in the feature. In addition, we suggest ESC with Training (ESC-T), which uses a learnable mask to better balance the trade-off between forgetting and preserving knowledge in KD. Our extensive experiments on various datasets and models demonstrate that our proposed methods achieve the fastest and state-of-the-art performance. Notably, our methods are applicable to diverse forgetting scenarios, such as facial domain setting, demonstrating the generalizability of our methods. The code is available at this http URL .</li>
</ul>

<h3>Title: Secure Generalization through Stochastic Bidirectional Parameter Updates Using Dual-Gradient Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Shourya Goel, Himanshi Tibrewal, Anant Jain, Anshul Pundhir, Pravendra Singh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02213">https://arxiv.org/abs/2504.02213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02213">https://arxiv.org/pdf/2504.02213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02213]] Secure Generalization through Stochastic Bidirectional Parameter Updates Using Dual-Gradient Mechanism(https://arxiv.org/abs/2504.02213)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) has gained increasing attention due to privacy-preserving collaborative training on decentralized clients, mitigating the need to upload sensitive data to a central server directly. Nonetheless, recent research has underscored the risk of exposing private data to adversaries, even within FL frameworks. In general, existing methods sacrifice performance while ensuring resistance to privacy leakage in FL. We overcome these issues and generate diverse models at a global server through the proposed stochastic bidirectional parameter update mechanism. Using diverse models, we improved the generalization and feature representation in the FL setup, which also helped to improve the robustness of the model against privacy leakage without hurting the model's utility. We use global models from past FL rounds to follow systematic perturbation in parameter space at the server to ensure model generalization and resistance against privacy attacks. We generate diverse models (in close neighborhoods) for each client by using systematic perturbations in model parameters at a fine-grained level (i.e., altering each convolutional filter across the layers of the model) to improve the generalization and security perspective. We evaluated our proposed approach on four benchmark datasets to validate its superiority. We surpassed the state-of-the-art methods in terms of model utility and robustness towards privacy leakage. We have proven the effectiveness of our method by evaluating performance using several quantitative and qualitative results.</li>
</ul>

<h3>Title: CRC-SGAD: Conformal Risk Control for Supervised Graph Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Songran Bai, Xiaolong Zheng, Daniel Dajun Zeng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02248">https://arxiv.org/abs/2504.02248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02248">https://arxiv.org/pdf/2504.02248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02248]] CRC-SGAD: Conformal Risk Control for Supervised Graph Anomaly Detection(https://arxiv.org/abs/2504.02248)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Graph Anomaly Detection (GAD) is critical in security-sensitive domains, yet faces reliability challenges: miscalibrated confidence estimation (underconfidence in normal nodes, overconfidence in anomalies), adversarial vulnerability of derived confidence score under structural perturbations, and limited efficacy of conventional calibration methods for sparse anomaly patterns. Thus we propose CRC-SGAD, a framework integrating statistical risk control into GAD via two innovations: (1) A Dual-Threshold Conformal Risk Control mechanism that provides theoretically guaranteed bounds for both False Negative Rate (FNR) and False Positive Rate (FPR) through providing prediction sets; (2) A Subgraph-aware Spectral Graph Neural Calibrator (SSGNC) that optimizes node representations through adaptive spectral filtering while reducing the size of prediction sets via hybrid loss optimization. Experiments on four datasets and five GAD models demonstrate statistically significant improvements in FNR and FPR control and prediction set size. CRC-SGAD establishes a paradigm for statistically rigorous anomaly detection in graph-structured security applications.</li>
</ul>

<h3>Title: LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks</h3>
<ul>
<li><strong>Authors: </strong>Seunghyun Yoo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02254">https://arxiv.org/abs/2504.02254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02254">https://arxiv.org/pdf/2504.02254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02254]] LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks(https://arxiv.org/abs/2504.02254)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have not only showcased impressive creative capabilities but also revealed emerging agentic behaviors that exploit linguistic ambiguity in adversarial settings. In this study, we investigate how an LLM, acting as an autonomous agent, leverages semantic ambiguity to generate deceptive puzzles that mislead and challenge human users. Inspired by the popular puzzle game "Connections", we systematically compare puzzles produced through zero-shot prompting, role-injected adversarial prompts, and human-crafted examples, with an emphasis on understanding the underlying agent decision-making processes. Employing computational analyses with HateBERT to quantify semantic ambiguity, alongside subjective human evaluations, we demonstrate that explicit adversarial agent behaviors significantly heighten semantic ambiguity -- thereby increasing cognitive load and reducing fairness in puzzle solving. These findings provide critical insights into the emergent agentic qualities of LLMs and underscore important ethical considerations for evaluating and safely deploying autonomous language systems in both educational technologies and entertainment.</li>
</ul>

<h3>Title: Implicit Neural Differential Model for Spatiotemporal Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Deepak Akhare, Pan Du, Tengfei Luo, Jian-Xun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02260">https://arxiv.org/abs/2504.02260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02260">https://arxiv.org/pdf/2504.02260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02260]] Implicit Neural Differential Model for Spatiotemporal Dynamics(https://arxiv.org/abs/2504.02260)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Hybrid neural-physics modeling frameworks through differentiable programming have emerged as powerful tools in scientific machine learning, enabling the integration of known physics with data-driven learning to improve prediction accuracy and generalizability. However, most existing hybrid frameworks rely on explicit recurrent formulations, which suffer from numerical instability and error accumulation during long-horizon forecasting. In this work, we introduce Im-PiNDiff, a novel implicit physics-integrated neural differentiable solver for stable and accurate modeling of spatiotemporal dynamics. Inspired by deep equilibrium models, Im-PiNDiff advances the state using implicit fixed-point layers, enabling robust long-term simulation while remaining fully end-to-end differentiable. To enable scalable training, we introduce a hybrid gradient propagation strategy that integrates adjoint-state methods with reverse-mode automatic differentiation. This approach eliminates the need to store intermediate solver states and decouples memory complexity from the number of solver iterations, significantly reducing training overhead. We further incorporate checkpointing techniques to manage memory in long-horizon rollouts. Numerical experiments on various spatiotemporal PDE systems, including advection-diffusion processes, Burgers' dynamics, and multi-physics chemical vapor infiltration processes, demonstrate that Im-PiNDiff achieves superior predictive performance, enhanced numerical stability, and substantial reductions in memory and runtime cost relative to explicit and naive implicit baselines. This work provides a principled, efficient, and scalable framework for hybrid neural-physics modeling.</li>
</ul>

<h3>Title: WonderTurbo: Generating Interactive 3D World in 0.72 Seconds</h3>
<ul>
<li><strong>Authors: </strong>Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Weijie Wang, Haoyun Li, Guosheng Zhao, Jie Li, Wenkang Qin, Guan Huang, Wenjun Mei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02261">https://arxiv.org/abs/2504.02261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02261">https://arxiv.org/pdf/2504.02261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02261]] WonderTurbo: Generating Interactive 3D World in 0.72 Seconds(https://arxiv.org/abs/2504.02261)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Interactive 3D generation is gaining momentum and capturing extensive attention for its potential to create immersive virtual experiences. However, a critical challenge in current 3D generation technologies lies in achieving real-time interactivity. To address this issue, we introduce WonderTurbo, the first real-time interactive 3D scene generation framework capable of generating novel perspectives of 3D scenes within 0.72 seconds. Specifically, WonderTurbo accelerates both geometric and appearance modeling in 3D scene generation. In terms of geometry, we propose StepSplat, an innovative method that constructs efficient 3D geometric representations through dynamic updates, each taking only 0.26 seconds. Additionally, we design QuickDepth, a lightweight depth completion module that provides consistent depth input for StepSplat, further enhancing geometric accuracy. For appearance modeling, we develop FastPaint, a 2-steps diffusion model tailored for instant inpainting, which focuses on maintaining spatial appearance consistency. Experimental results demonstrate that WonderTurbo achieves a remarkable 15X speedup compared to baseline methods, while preserving excellent spatial consistency and delivering high-quality output.</li>
</ul>

<h3>Title: Generative Classifier for Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Shaocong Long, Qianyu Zhou, Xiangtai Li, Chenhao Ying, Yunhai Tong, Lizhuang Ma, Yuan Luo, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02272">https://arxiv.org/abs/2504.02272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02272">https://arxiv.org/pdf/2504.02272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02272]] Generative Classifier for Domain Generalization(https://arxiv.org/abs/2504.02272)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Domain generalization (DG) aims to improve the generalizability of computer vision models toward distribution shifts. The mainstream DG methods focus on learning domain invariance, however, such methods overlook the potential inherent in domain-specific information. While the prevailing practice of discriminative linear classifier has been tailored to domain-invariant features, it struggles when confronted with diverse domain-specific information, e.g., intra-class shifts, that exhibits multi-modality. To address these issues, we explore the theoretical implications of relying on domain invariance, revealing the crucial role of domain-specific information in mitigating the target risk for DG. Drawing from these insights, we propose Generative Classifier-driven Domain Generalization (GCDG), introducing a generative paradigm for the DG classifier based on Gaussian Mixture Models (GMMs) for each class across domains. GCDG consists of three key modules: Heterogeneity Learning Classifier~(HLC), Spurious Correlation Blocking~(SCB), and Diverse Component Balancing~(DCB). Concretely, HLC attempts to model the feature distributions and thereby capture valuable domain-specific information via GMMs. SCB identifies the neural units containing spurious correlations and perturbs them, mitigating the risk of HLC learning spurious patterns. Meanwhile, DCB ensures a balanced contribution of components in HLC, preventing the underestimation or neglect of critical components. In this way, GCDG excels in capturing the nuances of domain-specific information characterized by diverse distributions. GCDG demonstrates the potential to reduce the target risk and encourage flat minima, improving the generalizability. Extensive experiments show GCDG's comparable performance on five DG benchmarks and one face anti-spoofing dataset, seamlessly integrating into existing DG methods with consistent improvements.</li>
</ul>

<h3>Title: Reasoning Under 1 Billion: Memory-Augmented Reinforcement Learning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hung Le, Dai Do, Dung Nguyen, Svetha Venkatesh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02273">https://arxiv.org/abs/2504.02273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02273">https://arxiv.org/pdf/2504.02273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02273]] Reasoning Under 1 Billion: Memory-Augmented Reinforcement Learning for Large Language Models(https://arxiv.org/abs/2504.02273)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in fine-tuning large language models (LLMs) with reinforcement learning (RL) have shown promising improvements in complex reasoning tasks, particularly when paired with chain-of-thought (CoT) prompting. However, these successes have been largely demonstrated on large-scale models with billions of parameters, where a strong pretraining foundation ensures effective initial exploration. In contrast, RL remains challenging for tiny LLMs with 1 billion parameters or fewer because they lack the necessary pretraining strength to explore effectively, often leading to suboptimal reasoning patterns. This work introduces a novel intrinsic motivation approach that leverages episodic memory to address this challenge, improving tiny LLMs in CoT reasoning tasks. Inspired by human memory-driven learning, our method leverages successful reasoning patterns stored in memory while allowing for controlled exploration to generate novel responses. Intrinsic rewards are computed efficiently using a kNN-based episodic memory, allowing the model to discover new reasoning strategies while quickly adapting to effective past solutions. Experiments on fine-tuning GSM8K and AI-MO datasets demonstrate that our approach significantly enhances smaller LLMs' sample efficiency and generalization capability, making RL-based reasoning improvements more accessible in low-resource settings.</li>
</ul>

<h3>Title: Enhancing Customer Contact Efficiency with Graph Neural Networks in Credit Card Fraud Detection Workflow</h3>
<ul>
<li><strong>Authors: </strong>Menghao Huo, Kuan Lu, Qiang Zhu, Zhenrui Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02275">https://arxiv.org/abs/2504.02275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02275">https://arxiv.org/pdf/2504.02275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02275]] Enhancing Customer Contact Efficiency with Graph Neural Networks in Credit Card Fraud Detection Workflow(https://arxiv.org/abs/2504.02275)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Credit card fraud has been a persistent issue since the last century, causing significant financial losses to the industry. The most effective way to prevent fraud is by contacting customers to verify suspicious transactions. However, while these systems are designed to detect fraudulent activity, they often mistakenly flag legitimate transactions, leading to unnecessary declines that disrupt the user experience and erode customer trust. Frequent false positives can frustrate customers, resulting in dissatisfaction, increased complaints, and a diminished sense of security. To address these limitations, we propose a fraud detection framework incorporating Relational Graph Convolutional Networks (RGCN) to enhance the accuracy and efficiency of identifying fraudulent transactions. By leveraging the relational structure of transaction data, our model reduces the need for direct customer confirmation while maintaining high detection performance. Our experiments are conducted using the IBM credit card transaction dataset to evaluate the effectiveness of this approach.</li>
</ul>

<h3>Title: Beyond Conventional Transformers: The Medical X-ray Attention (MXA) Block for Improved Multi-Label Diagnosis Using Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Amit Rand, Hadi Ibrahim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02277">https://arxiv.org/abs/2504.02277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02277">https://arxiv.org/pdf/2504.02277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02277]] Beyond Conventional Transformers: The Medical X-ray Attention (MXA) Block for Improved Multi-Label Diagnosis Using Knowledge Distillation(https://arxiv.org/abs/2504.02277)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Medical imaging, particularly X-ray analysis, often involves detecting multiple conditions simultaneously within a single scan, making multi-label classification crucial for real-world clinical applications. We present the Medical X-ray Attention (MXA) block, a novel attention mechanism tailored specifically to address the unique challenges of X-ray abnormality detection. The MXA block enhances traditional Multi-Head Self Attention (MHSA) by integrating a specialized module that efficiently captures both detailed local information and broader global context. To the best of our knowledge, this is the first work to propose a task-specific attention mechanism for diagnosing chest X-rays, as well as to attempt multi-label classification using an Efficient Vision Transformer (EfficientViT). By embedding the MXA block within the EfficientViT architecture and employing knowledge distillation, our proposed model significantly improves performance on the CheXpert dataset, a widely used benchmark for multi-label chest X-ray abnormality detection. Our approach achieves an area under the curve (AUC) of 0.85, an absolute improvement of 0.19 compared to our baseline model's AUC of 0.66, corresponding to a substantial approximate 233% relative improvement over random guessing (AUC = 0.5).</li>
</ul>

<h3>Title: MultiTSF: Transformer-based Sensor Fusion for Human-Centric Multi-view and Multi-modal Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Trung Thanh Nguyen, Yasutomo Kawanishi, Vijay John, Takahiro Komamizu, Ichiro Ide</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02279">https://arxiv.org/abs/2504.02279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02279">https://arxiv.org/pdf/2504.02279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02279]] MultiTSF: Transformer-based Sensor Fusion for Human-Centric Multi-view and Multi-modal Action Recognition(https://arxiv.org/abs/2504.02279)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Action recognition from multi-modal and multi-view observations holds significant potential for applications in surveillance, robotics, and smart environments. However, existing methods often fall short of addressing real-world challenges such as diverse environmental conditions, strict sensor synchronization, and the need for fine-grained annotations. In this study, we propose the Multi-modal Multi-view Transformer-based Sensor Fusion (MultiTSF). The proposed method leverages a Transformer-based to dynamically model inter-view relationships and capture temporal dependencies across multiple views. Additionally, we introduce a Human Detection Module to generate pseudo-ground-truth labels, enabling the model to prioritize frames containing human activity and enhance spatial feature learning. Comprehensive experiments conducted on our in-house MultiSensor-Home dataset and the existing MM-Office dataset demonstrate that MultiTSF outperforms state-of-the-art methods in both video sequence-level and frame-level action recognition settings.</li>
</ul>

<h3>Title: Ga$_2$O$_3$ TCAD Mobility Parameter Calibration using Simulation Augmented Machine Learning with Physics Informed Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Le Minh Long Nguyen, Edric Ong, Matthew Eng, Yuhao Zhang, Hiu Yung Wong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02283">https://arxiv.org/abs/2504.02283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02283">https://arxiv.org/pdf/2504.02283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02283]] Ga$_2$O$_3$ TCAD Mobility Parameter Calibration using Simulation Augmented Machine Learning with Physics Informed Neural Network(https://arxiv.org/abs/2504.02283)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In this paper, we demonstrate the possibility of performing automatic Technology Computer-Aided-Design (TCAD) parameter calibration using machine learning, verified with experimental data. The machine only needs to be trained by TCAD data. Schottky Barrier Diode (SBD) fabricated with emerging ultra-wide-bandgap material, Gallium Oxide (Ga$_2$O$_3$), is measured and its current-voltage (IV) is used for Ga$_2$O$_3$ Philips Unified Mobility (PhuMob) model parameters, effective anode workfunction, and ambient temperature extraction (7 parameters). A machine comprised of an autoencoder (AE) and a neural network (NN) (AE-NN) is used. Ga$_2$O$_3$ PhuMob parameters are extracted from the noisy experimental curves. TCAD simulation with the extracted parameters shows that the quality of the parameters is as good as an expert's calibration at the pre-turned-on regime but not in the on-state regime. By using a simple physics-informed neural network (PINN) (AE-PINN), the machine performs as well as the human expert in all regimes.</li>
</ul>

<h3>Title: Tree-based Models for Vertical Federated Learning: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Bingchen Qian, Yuexiang Xie, Yaliang Li, Bolin Ding, Jingren Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02285">https://arxiv.org/abs/2504.02285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02285">https://arxiv.org/pdf/2504.02285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02285]] Tree-based Models for Vertical Federated Learning: A Survey(https://arxiv.org/abs/2504.02285)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, federate, interpretability</a></li>
<li><strong>Abstract: </strong>Tree-based models have achieved great success in a wide range of real-world applications due to their effectiveness, robustness, and interpretability, which inspired people to apply them in vertical federated learning (VFL) scenarios in recent years. In this paper, we conduct a comprehensive study to give an overall picture of applying tree-based models in VFL, from the perspective of their communication and computation protocols. We categorize tree-based models in VFL into two types, i.e., feature-gathering models and label-scattering models, and provide a detailed discussion regarding their characteristics, advantages, privacy protection mechanisms, and applications. This study also focuses on the implementation of tree-based models in VFL, summarizing several design principles for better satisfying various requirements from both academic research and industrial deployment. We conduct a series of experiments to provide empirical observations on the differences and advances of different types of tree-based models.</li>
</ul>

<h3>Title: MultiSensor-Home: A Wide-area Multi-modal Multi-view Dataset for Action Recognition and Transformer-based Sensor Fusion</h3>
<ul>
<li><strong>Authors: </strong>Trung Thanh Nguyen, Yasutomo Kawanishi, Vijay John, Takahiro Komamizu, Ichiro Ide</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02287">https://arxiv.org/abs/2504.02287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02287">https://arxiv.org/pdf/2504.02287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02287]] MultiSensor-Home: A Wide-area Multi-modal Multi-view Dataset for Action Recognition and Transformer-based Sensor Fusion(https://arxiv.org/abs/2504.02287)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multi-modal multi-view action recognition is a rapidly growing field in computer vision, offering significant potential for applications in surveillance. However, current datasets often fail to address real-world challenges such as wide-area environmental conditions, asynchronous data streams, and the lack of frame-level annotations. Furthermore, existing methods face difficulties in effectively modeling inter-view relationships and enhancing spatial feature learning. In this study, we propose the Multi-modal Multi-view Transformer-based Sensor Fusion (MultiTSF) method and introduce the MultiSensor-Home dataset, a novel benchmark designed for comprehensive action recognition in home environments. The MultiSensor-Home dataset features untrimmed videos captured by distributed sensors, providing high-resolution RGB and audio data along with detailed multi-view frame-level action labels. The proposed MultiTSF method leverages a Transformer-based fusion mechanism to dynamically model inter-view relationships. Furthermore, the method also integrates a external human detection module to enhance spatial feature learning. Experiments on MultiSensor-Home and MM-Office datasets demonstrate the superiority of MultiTSF over the state-of-the-art methods. The quantitative and qualitative results highlight the effectiveness of the proposed method in advancing real-world multi-modal multi-view action recognition.</li>
</ul>

<h3>Title: SPACE: SPike-Aware Consistency Enhancement for Test-Time Adaptation in Spiking Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Luo, Kecheng Chen, Pao-Sheng Vincent Sun, Chris Xing Tian, Arindam Basu, Haoliang Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02298">https://arxiv.org/abs/2504.02298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02298">https://arxiv.org/pdf/2504.02298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02298]] SPACE: SPike-Aware Consistency Enhancement for Test-Time Adaptation in Spiking Neural Networks(https://arxiv.org/abs/2504.02298)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Spiking Neural Networks (SNNs), as a biologically plausible alternative to Artificial Neural Networks (ANNs), have demonstrated advantages in terms of energy efficiency, temporal processing, and biological plausibility. However, SNNs are highly sensitive to distribution shifts, which can significantly degrade their performance in real-world scenarios. Traditional test-time adaptation (TTA) methods designed for ANNs often fail to address the unique computational dynamics of SNNs, such as sparsity and temporal spiking behavior. To address these challenges, we propose $\textbf{SP}$ike-$\textbf{A}$ware $\textbf{C}$onsistency $\textbf{E}$nhancement (SPACE), the first source-free and single-instance TTA method specifically designed for SNNs. SPACE leverages the inherent spike dynamics of SNNs to maximize the consistency of spike-behavior-based local feature maps across augmented versions of a single test sample, enabling robust adaptation without requiring source data. We evaluate SPACE on multiple datasets, including CIFAR-10-C, CIFAR-100-C, Tiny-ImageNet-C and DVS Gesture-C. Furthermore, SPACE demonstrates strong generalization across different model architectures, achieving consistent performance improvements on both VGG9 and ResNet11. Experimental results show that SPACE outperforms state-of-the-art methods, highlighting its effectiveness and robustness in real-world settings.</li>
</ul>

<h3>Title: Measurement of LLM's Philosophies of Human Nature</h3>
<ul>
<li><strong>Authors: </strong>Minheng Ni, Ennan Wu, Zidong Gong, Zhengyuan Yang, Linjie Li, Chung-Ching Lin, Kevin Lin, Lijuan Wang, Wangmeng Zuo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02304">https://arxiv.org/abs/2504.02304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02304">https://arxiv.org/pdf/2504.02304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02304]] Measurement of LLM's Philosophies of Human Nature(https://arxiv.org/abs/2504.02304)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The widespread application of artificial intelligence (AI) in various tasks, along with frequent reports of conflicts or violations involving AI, has sparked societal concerns about interactions with AI systems. Based on Wrightsman's Philosophies of Human Nature Scale (PHNS), a scale empirically validated over decades to effectively assess individuals' attitudes toward human nature, we design the standardized psychological scale specifically targeting large language models (LLM), named the Machine-based Philosophies of Human Nature Scale (M-PHNS). By evaluating LLMs' attitudes toward human nature across six dimensions, we reveal that current LLMs exhibit a systemic lack of trust in humans, and there is a significant negative correlation between the model's intelligence level and its trust in humans. Furthermore, we propose a mental loop learning framework, which enables LLM to continuously optimize its value system during virtual interactions by constructing moral scenarios, thereby improving its attitude toward human nature. Experiments demonstrate that mental loop learning significantly enhances their trust in humans compared to persona or instruction prompts. This finding highlights the potential of human-based psychological assessments for LLM, which can not only diagnose cognitive biases but also provide a potential solution for ethical learning in artificial intelligence. We release the M-PHNS evaluation code and data at this https URL.</li>
</ul>

<h3>Title: Improving Harmful Text Detection with Joint Retrieval and External Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Zidong Yu, Shuo Wang, Nan Jiang, Weiqiang Huang, Xu Han, Junliang Du</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02310">https://arxiv.org/abs/2504.02310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02310">https://arxiv.org/pdf/2504.02310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02310]] Improving Harmful Text Detection with Joint Retrieval and External Knowledge(https://arxiv.org/abs/2504.02310)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Harmful text detection has become a crucial task in the development and deployment of large language models, especially as AI-generated content continues to expand across digital platforms. This study proposes a joint retrieval framework that integrates pre-trained language models with knowledge graphs to improve the accuracy and robustness of harmful text detection. Experimental results demonstrate that the joint retrieval approach significantly outperforms single-model baselines, particularly in low-resource training scenarios and multilingual environments. The proposed method effectively captures nuanced harmful content by leveraging external contextual information, addressing the limitations of traditional detection models. Future research should focus on optimizing computational efficiency, enhancing model interpretability, and expanding multimodal detection capabilities to better tackle evolving harmful content patterns. This work contributes to the advancement of AI safety, ensuring more trustworthy and reliable content moderation systems.</li>
</ul>

<h3>Title: OmniCam: Unified Multimodal Video Generation via Camera Control</h3>
<ul>
<li><strong>Authors: </strong>Xiaoda Yang, Jiayang Xu, Kaixuan Luan, Xinyu Zhan, Hongshun Qiu, Shijun Shi, Hao Li, Shuai Yang, Li Zhang, Checheng Yu, Cewu Lu, Lixin Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02312">https://arxiv.org/abs/2504.02312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02312">https://arxiv.org/pdf/2504.02312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02312]] OmniCam: Unified Multimodal Video Generation via Camera Control(https://arxiv.org/abs/2504.02312)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Camera control, which achieves diverse visual effects by changing camera position and pose, has attracted widespread attention. However, existing methods face challenges such as complex interaction and limited control capabilities. To address these issues, we present OmniCam, a unified multimodal camera control framework. Leveraging large language models and video diffusion models, OmniCam generates spatio-temporally consistent videos. It supports various combinations of input modalities: the user can provide text or video with expected trajectory as camera path guidance, and image or video as content reference, enabling precise control over camera motion. To facilitate the training of OmniCam, we introduce the OmniTr dataset, which contains a large collection of high-quality long-sequence trajectories, videos, and corresponding descriptions. Experimental results demonstrate that our model achieves state-of-the-art performance in high-quality camera-controlled video generation across various metrics.</li>
</ul>

<h3>Title: Distributed Temporal Graph Learning with Provenance for APT Detection in Supply Chains</h3>
<ul>
<li><strong>Authors: </strong>Zhuoran Tan, Christos Anagnostopoulos, Jeremy Singer</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02313">https://arxiv.org/abs/2504.02313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02313">https://arxiv.org/pdf/2504.02313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02313]] Distributed Temporal Graph Learning with Provenance for APT Detection in Supply Chains(https://arxiv.org/abs/2504.02313)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, steal</a></li>
<li><strong>Abstract: </strong>Cyber supply chain, encompassing digital asserts, software, hardware, has become an essential component of modern Information and Communications Technology (ICT) provisioning. However, the growing inter-dependencies have introduced numerous attack vectors, making supply chains a prime target for exploitation. In particular, advanced persistent threats (APTs) frequently leverage supply chain vulnerabilities (SCVs) as entry points, benefiting from their inherent stealth. Current defense strategies primarly focus on prevention through blockchain for integrity assurance or detection using plain-text source code analysis in open-source software (OSS). However, these approaches overlook scenarios where source code is unavailable and fail to address detection and defense during runtime. To bridge this gap, we propose a novel approach that integrates multi-source data, constructs a comprehensive dynamic provenance graph, and detects APT behavior in real time using temporal graph learning. Given the lack of tailored datasets in both industry and academia, we also aim to simulate a custom dataset by replaying real-world supply chain exploits with multi-source monitoring.</li>
</ul>

<h3>Title: Temporal Gaussian Copula For Clinical Multivariate Time Series Data Imputation</h3>
<ul>
<li><strong>Authors: </strong>Ye Su, Hezhe Qiao, Di Wu, Yuwen Chen, Lin Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02317">https://arxiv.org/abs/2504.02317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02317">https://arxiv.org/pdf/2504.02317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02317]] Temporal Gaussian Copula For Clinical Multivariate Time Series Data Imputation(https://arxiv.org/abs/2504.02317)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>The imputation of the Multivariate time series (MTS) is particularly challenging since the MTS typically contains irregular patterns of missing values due to various factors such as instrument failures, interference from irrelevant data, and privacy regulations. Existing statistical methods and deep learning methods have shown promising results in time series imputation. In this paper, we propose a Temporal Gaussian Copula Model (TGC) for three-order MTS imputation. The key idea is to leverage the Gaussian Copula to explore the cross-variable and temporal relationships based on the latent Gaussian representation. Subsequently, we employ an Expectation-Maximization (EM) algorithm to improve robustness in managing data with varying missing rates. Comprehensive experiments were conducted on three real-world MTS datasets. The results demonstrate that our TGC substantially outperforms the state-of-the-art imputation methods. Additionally, the TGC model exhibits stronger robustness to the varying missing ratios in the test dataset. Our code is available at this https URL.</li>
</ul>

<h3>Title: Distributed Log-driven Anomaly Detection System based on Evolving Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Zhuoran Tan, Qiyuan Wang, Christos Anagnostopoulos, Shameem P. Parambath, Jeremy Singer, Sam Temple</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02322">https://arxiv.org/abs/2504.02322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02322">https://arxiv.org/pdf/2504.02322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02322]] Distributed Log-driven Anomaly Detection System based on Evolving Decision Making(https://arxiv.org/abs/2504.02322)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense</a></li>
<li><strong>Abstract: </strong>Effective anomaly detection from logs is crucial for enhancing cybersecurity defenses by enabling the early identification of threats. Despite advances in anomaly detection, existing systems often fall short in areas such as post-detection validation, scalability, and effective maintenance. These limitations not only hinder the detection of new threats but also impair overall system performance. To address these challenges, we propose CEDLog, a novel practical framework that integrates Elastic Weight Consolidation (EWC) for continual learning and implements distributed computing for scalable processing by integrating Apache Airflow and Dask. In CEDLog, anomalies are detected through the synthesis of Multi-layer Perceptron (MLP) and Graph Convolutional Networks (GCNs) using critical features present in event logs. Through comparisons with update strategies on large-scale datasets, we demonstrate the strengths of CEDLog, showcasing efficient updates and low false positives</li>
</ul>

<h3>Title: CoTAL: Human-in-the-Loop Prompt Engineering, Chain-of-Thought Reasoning, and Active Learning for Generalizable Formative Assessment Scoring</h3>
<ul>
<li><strong>Authors: </strong>Clayton Cohn, Nicole Hutchins, Ashwin T S, Gautam Biswas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02323">https://arxiv.org/abs/2504.02323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02323">https://arxiv.org/pdf/2504.02323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02323]] CoTAL: Human-in-the-Loop Prompt Engineering, Chain-of-Thought Reasoning, and Active Learning for Generalizable Formative Assessment Scoring(https://arxiv.org/abs/2504.02323)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have created new opportunities to assist teachers and support student learning. Methods such as chain-of-thought (CoT) prompting enable LLMs to grade formative assessments in science, providing scores and relevant feedback to students. However, the extent to which these methods generalize across curricula in multiple domains (such as science, computing, and engineering) remains largely untested. In this paper, we introduce Chain-of-Thought Prompting + Active Learning (CoTAL), an LLM-based approach to formative assessment scoring that (1) leverages Evidence-Centered Design (ECD) principles to develop curriculum-aligned formative assessments and rubrics, (2) applies human-in-the-loop prompt engineering to automate response scoring, and (3) incorporates teacher and student feedback to iteratively refine assessment questions, grading rubrics, and LLM prompts for automated grading. Our findings demonstrate that CoTAL improves GPT-4's scoring performance, achieving gains of up to 24.5% over a non-prompt-engineered baseline. Both teachers and students view CoTAL as effective in scoring and explaining student responses, each providing valuable refinements to enhance grading accuracy and explanation quality.</li>
</ul>

<h3>Title: LearNAT: Learning NL2SQL with AST-guided Task Decomposition for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weibin Liao, Xin Gao, Tianyu Jia, Rihong Qiu, Yifan Zhu, Yang Lin, Xu Chu, Junfeng Zhao, Yasha Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02327">https://arxiv.org/abs/2504.02327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02327">https://arxiv.org/pdf/2504.02327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02327]] LearNAT: Learning NL2SQL with AST-guided Task Decomposition for Large Language Models(https://arxiv.org/abs/2504.02327)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Natural Language to SQL (NL2SQL) has emerged as a critical task for enabling seamless interaction with databases. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable performance in this domain. However, existing NL2SQL methods predominantly rely on closed-source LLMs leveraging prompt engineering, while open-source models typically require fine-tuning to acquire domain-specific knowledge. Despite these efforts, open-source LLMs struggle with complex NL2SQL tasks due to the indirect expression of user query objectives and the semantic gap between user queries and database schemas. Inspired by the application of reinforcement learning in mathematical problem-solving to encourage step-by-step reasoning in LLMs, we propose LearNAT (Learning NL2SQL with AST-guided Task Decomposition), a novel framework that improves the performance of open-source LLMs on complex NL2SQL tasks through task decomposition and reinforcement learning. LearNAT introduces three key components: (1) a Decomposition Synthesis Procedure that leverages Abstract Syntax Trees (ASTs) to guide efficient search and pruning strategies for task decomposition, (2) Margin-aware Reinforcement Learning, which employs fine-grained step-level optimization via DPO with AST margins, and (3) Adaptive Demonstration Reasoning, a mechanism for dynamically selecting relevant examples to enhance decomposition capabilities. Extensive experiments on two benchmark datasets, Spider and BIRD, demonstrate that LearNAT enables a 7B-parameter open-source LLM to achieve performance comparable to GPT-4, while offering improved efficiency and accessibility.</li>
</ul>

<h3>Title: Refining CLIP's Spatial Awareness: A Visual-Centric Perspective</h3>
<ul>
<li><strong>Authors: </strong>Congpei Qiu, Yanhao Wu, Wei Ke, Xiuxiu Bai, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02328">https://arxiv.org/abs/2504.02328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02328">https://arxiv.org/pdf/2504.02328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02328]] Refining CLIP's Spatial Awareness: A Visual-Centric Perspective(https://arxiv.org/abs/2504.02328)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Contrastive Language-Image Pre-training (CLIP) excels in global alignment with language but exhibits limited sensitivity to spatial information, leading to strong performance in zero-shot classification tasks but underperformance in tasks requiring precise spatial understanding. Recent approaches have introduced Region-Language Alignment (RLA) to enhance CLIP's performance in dense multimodal tasks by aligning regional visual representations with corresponding text inputs. However, we find that CLIP ViTs fine-tuned with RLA suffer from notable loss in spatial awareness, which is crucial for dense prediction tasks. To address this, we propose the Spatial Correlation Distillation (SCD) framework, which preserves CLIP's inherent spatial structure and mitigates the above degradation. To further enhance spatial correlations, we introduce a lightweight Refiner that extracts refined correlations directly from CLIP before feeding them into SCD, based on an intriguing finding that CLIP naturally captures high-quality dense features. Together, these components form a robust distillation framework that enables CLIP ViTs to integrate both visual-language and visual-centric improvements, achieving state-of-the-art results across various open-vocabulary dense prediction benchmarks.</li>
</ul>

<h3>Title: Towards Assessing Deep Learning Test Input Generators</h3>
<ul>
<li><strong>Authors: </strong>Seif Mzoughi, Ahmed Hajyahmed, Mohamed Elshafei, Foutse Khomh anb Diego Elias Costa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02329">https://arxiv.org/abs/2504.02329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02329">https://arxiv.org/pdf/2504.02329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02329]] Towards Assessing Deep Learning Test Input Generators(https://arxiv.org/abs/2504.02329)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep Learning (DL) systems are increasingly deployed in safety-critical applications, yet they remain vulnerable to robustness issues that can lead to significant failures. While numerous Test Input Generators (TIGs) have been developed to evaluate DL robustness, a comprehensive assessment of their effectiveness across different dimensions is still lacking. This paper presents a comprehensive assessment of four state-of-the-art TIGs--DeepHunter, DeepFault, AdvGAN, and SinVAD--across multiple critical aspects: fault-revealing capability, naturalness, diversity, and efficiency. Our empirical study leverages three pre-trained models (LeNet-5, VGG16, and EfficientNetB3) on datasets of varying complexity (MNIST, CIFAR-10, and ImageNet-1K) to evaluate TIG performance. Our findings reveal important trade-offs in robustness revealing capability, variation in test case generation, and computational efficiency across TIGs. The results also show that TIG performance varies significantly with dataset complexity, as tools that perform well on simpler datasets may struggle with more complex ones. In contrast, others maintain steadier performance or better scalability. This paper offers practical guidance for selecting appropriate TIGs aligned with specific objectives and dataset characteristics. Nonetheless, more work is needed to address TIG limitations and advance TIGs for real-world, safety-critical systems.</li>
</ul>

<h3>Title: Evaluating and Enhancing Segmentation Model Robustness with Metamorphic Testing</h3>
<ul>
<li><strong>Authors: </strong>Seif Mzoughi, Mohamed Elshafeia, Foutse Khomh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02335">https://arxiv.org/abs/2504.02335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02335">https://arxiv.org/pdf/2504.02335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02335]] Evaluating and Enhancing Segmentation Model Robustness with Metamorphic Testing(https://arxiv.org/abs/2504.02335)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Image segmentation is critical for applications such as medical imaging, augmented reality, and video surveillance. However, segmentation models often lack robustness, making them vulnerable to adversarial perturbations from subtle image distortions. In this work, we propose SegRMT, a metamorphic testing approach that leverages genetic algorithms (GA) to optimize sequences of spatial and spectral transformations while preserving image fidelity via a predefined PSNR threshold. Using the Cityscapes dataset, our method generates adversarial examples that effectively challenge the DeepLabV3 segmentation model. Our experiments show that SegRMT reduces DeepLabV3's mean Intersection over Union (mIoU) to 6.4%, outperforming other adversarial baselines that decrease mIoU to between 8.5% and 21.7%. Furthermore, when used for adversarial training, SegRMT boosts model performance, achieving mIoU improvements up to 73% on dedicated adversarial datasets and increasing cross-adversarial mIoU to 53.8%, compared to only 2%-10% for other methods. These findings demonstrate that SegRMT not only simulates realistic image distortions but also enhances the robustness of segmentation models, making it a valuable tool for ensuring reliable performance in safety-critical applications.</li>
</ul>

<h3>Title: LPA3D: 3D Room-Level Scene Generation from In-the-Wild Images</h3>
<ul>
<li><strong>Authors: </strong>Ming-Jia Yang, Yu-Xiao Guo, Yang Liu, Bin Zhou, Xin Tong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02337">https://arxiv.org/abs/2504.02337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02337">https://arxiv.org/pdf/2504.02337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02337]] LPA3D: 3D Room-Level Scene Generation from In-the-Wild Images(https://arxiv.org/abs/2504.02337)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generating realistic, room-level indoor scenes with semantically plausible and detailed appearances from in-the-wild images is crucial for various applications in VR, AR, and robotics. The success of NeRF-based generative methods indicates a promising direction to address this challenge. However, unlike their success at the object level, existing scene-level generative methods require additional information, such as multiple views, depth images, or semantic guidance, rather than relying solely on RGB images. This is because NeRF-based methods necessitate prior knowledge of camera poses, which is challenging to approximate for indoor scenes due to the complexity of defining alignment and the difficulty of globally estimating poses from a single image, given the unseen parts behind the camera. To address this challenge, we redefine global poses within the framework of Local-Pose-Alignment (LPA) -- an anchor-based multi-local-coordinate system that uses a selected number of anchors as the roots of these coordinates. Building on this foundation, we introduce LPA-GAN, a novel NeRF-based generative approach that incorporates specific modifications to estimate the priors of camera poses under LPA. It also co-optimizes the pose predictor and scene generation processes. Our ablation study and comparisons with straightforward extensions of NeRF-based object generative methods demonstrate the effectiveness of our approach. Furthermore, visual comparisons with other techniques reveal that our method achieves superior view-to-view consistency and semantic normality.</li>
</ul>

<h3>Title: Toward General and Robust LLM-enhanced Text-attributed Graph Learning</h3>
<ul>
<li><strong>Authors: </strong>Zihao Zhang, Xunkai Li, Rong-Hua Li, Bing Zhou, Zhenjun Li, Guoren Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02343">https://arxiv.org/abs/2504.02343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02343">https://arxiv.org/pdf/2504.02343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02343]] Toward General and Robust LLM-enhanced Text-attributed Graph Learning(https://arxiv.org/abs/2504.02343)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) and the proliferation of Text-Attributed Graphs (TAGs) across various domains have positioned LLM-enhanced TAG learning as a critical research area. By utilizing rich graph descriptions, this paradigm leverages LLMs to generate high-quality embeddings, thereby enhancing the representational capacity of Graph Neural Networks (GNNs). However, the field faces significant challenges: (1) the absence of a unified framework to systematize the diverse optimization perspectives arising from the complex interactions between LLMs and GNNs, and (2) the lack of a robust method capable of handling real-world TAGs, which often suffer from texts and edge sparsity, leading to suboptimal performance. To address these challenges, we propose UltraTAG, a unified pipeline for LLM-enhanced TAG learning. UltraTAG provides a unified comprehensive and domain-adaptive framework that not only organizes existing methodologies but also paves the way for future advancements in the field. Building on this framework, we propose UltraTAG-S, a robust instantiation of UltraTAG designed to tackle the inherent sparsity issues in real-world TAGs. UltraTAG-S employs LLM-based text propagation and text augmentation to mitigate text sparsity, while leveraging LLM-augmented node selection techniques based on PageRank and edge reconfiguration strategies to address edge sparsity. Our extensive experiments demonstrate that UltraTAG-S significantly outperforms existing baselines, achieving improvements of 2.12\% and 17.47\% in ideal and sparse settings, respectively. Moreover, as the data sparsity ratio increases, the performance improvement of UltraTAG-S also rises, which underscores the effectiveness and robustness of UltraTAG-S.</li>
</ul>

<h3>Title: Agglomerating Large Vision Encoders via Distillation for VFSS Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Chengxi Zeng, Yuxuan Jiang, Fan Zhang, Alberto Gambaruto, Tilo Burghardt</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02351">https://arxiv.org/abs/2504.02351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02351">https://arxiv.org/pdf/2504.02351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02351]] Agglomerating Large Vision Encoders via Distillation for VFSS Segmentation(https://arxiv.org/abs/2504.02351)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The deployment of foundation models for medical imaging has demonstrated considerable success. However, their training overheads associated with downstream tasks remain substantial due to the size of the image encoders employed, and the inference complexity is also significantly high. Although lightweight variants have been obtained for these foundation models, their performance is constrained by their limited model capacity and suboptimal training strategies. In order to achieve an improved tradeoff between complexity and performance, we propose a new framework to improve the performance of low complexity models via knowledge distillation from multiple large medical foundation models (e.g., MedSAM, RAD-DINO, MedCLIP), each specializing in different vision tasks, with the goal to effectively bridge the performance gap for medical image segmentation tasks. The agglomerated model demonstrates superior generalization across 12 segmentation tasks, whereas specialized models require explicit training for each task. Our approach achieved an average performance gain of 2\% in Dice coefficient compared to simple distillation.</li>
</ul>

<h3>Title: All-day Depth Completion via Thermal-LiDAR Fusion</h3>
<ul>
<li><strong>Authors: </strong>Janghyun Kim, Minseong Kweon, Jinsun Park, Ukcheol Shin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02356">https://arxiv.org/abs/2504.02356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02356">https://arxiv.org/pdf/2504.02356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02356]] All-day Depth Completion via Thermal-LiDAR Fusion(https://arxiv.org/abs/2504.02356)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Depth completion, which estimates dense depth from sparse LiDAR and RGB images, has demonstrated outstanding performance in well-lit conditions. However, due to the limitations of RGB sensors, existing methods often struggle to achieve reliable performance in harsh environments, such as heavy rain and low-light conditions. Furthermore, we observe that ground truth depth maps often suffer from large missing measurements in adverse weather conditions such as heavy rain, leading to insufficient supervision. In contrast, thermal cameras are known for providing clear and reliable visibility in such conditions, yet research on thermal-LiDAR depth completion remains underexplored. Moreover, the characteristics of thermal images, such as blurriness, low contrast, and noise, bring unclear depth boundary problems. To address these challenges, we first evaluate the feasibility and robustness of thermal-LiDAR depth completion across diverse lighting (eg., well-lit, low-light), weather (eg., clear-sky, rainy), and environment (eg., indoor, outdoor) conditions, by conducting extensive benchmarks on the MS$^2$ and ViViD datasets. In addition, we propose a framework that utilizes COntrastive learning and Pseudo-Supervision (COPS) to enhance depth boundary clarity and improve completion accuracy by leveraging a depth foundation model in two key ways. First, COPS enforces a depth-aware contrastive loss between different depth points by mining positive and negative samples using a monocular depth foundation model to sharpen depth boundaries. Second, it mitigates the issue of incomplete supervision from ground truth depth maps by leveraging foundation model predictions as dense depth priors. We also provide in-depth analyses of the key challenges in thermal-LiDAR depth completion to aid in understanding the task and encourage future research.</li>
</ul>

<h3>Title: Marine Saliency Segmenter: Object-Focused Conditional Diffusion with Region-Level Semantic Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Laibin Chang, Yunke Wang, JiaXing Huang, Longxiang Deng, Bo Du, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02391">https://arxiv.org/abs/2504.02391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02391">https://arxiv.org/pdf/2504.02391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02391]] Marine Saliency Segmenter: Object-Focused Conditional Diffusion with Region-Level Semantic Knowledge Distillation(https://arxiv.org/abs/2504.02391)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Marine Saliency Segmentation (MSS) plays a pivotal role in various vision-based marine exploration tasks. However, existing marine segmentation techniques face the dilemma of object mislocalization and imprecise boundaries due to the complex underwater environment. Meanwhile, despite the impressive performance of diffusion models in visual segmentation, there remains potential to further leverage contextual semantics to enhance feature learning of region-level salient objects, thereby improving segmentation outcomes. Building on this insight, we propose DiffMSS, a novel marine saliency segmenter based on the diffusion model, which utilizes semantic knowledge distillation to guide the segmentation of marine salient objects. Specifically, we design a region-word similarity matching mechanism to identify salient terms at the word level from the text descriptions. These high-level semantic features guide the conditional feature learning network in generating salient and accurate diffusion conditions with semantic knowledge distillation. To further refine the segmentation of fine-grained structures in unique marine organisms, we develop the dedicated consensus deterministic sampling to suppress overconfident missegmentations. Comprehensive experiments demonstrate the superior performance of DiffMSS over state-of-the-art methods in both quantitative and qualitative evaluations.</li>
</ul>

<h3>Title: The quasi-semantic competence of LLMs: a case study on the part-whole relation</h3>
<ul>
<li><strong>Authors: </strong>Mattia Proietti, Alessandro Lenci</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02395">https://arxiv.org/abs/2504.02395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02395">https://arxiv.org/pdf/2504.02395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02395]] The quasi-semantic competence of LLMs: a case study on the part-whole relation(https://arxiv.org/abs/2504.02395)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding the extent and depth of the semantic competence of \emph{Large Language Models} (LLMs) is at the center of the current scientific agenda in Artificial Intelligence (AI) and Computational Linguistics (CL). We contribute to this endeavor by investigating their knowledge of the \emph{part-whole} relation, a.k.a. \emph{meronymy}, which plays a crucial role in lexical organization, but it is significantly understudied. We used data from ConceptNet relations \citep{speer2016conceptnet} and human-generated semantic feature norms \citep{McRae:2005} to explore the abilities of LLMs to deal with \textit{part-whole} relations. We employed several methods based on three levels of analysis: i.) \textbf{behavioral} testing via prompting, where we directly queried the models on their knowledge of meronymy, ii.) sentence \textbf{probability} scoring, where we tested models' abilities to discriminate correct (real) and incorrect (asymmetric counterfactual) \textit{part-whole} relations, and iii.) \textbf{concept representation} analysis in vector space, where we proved the linear organization of the \textit{part-whole} concept in the embedding and unembedding spaces. These analyses present a complex picture that reveals that the LLMs' knowledge of this relation is only partial. They have just a ``\emph{quasi}-semantic'' competence and still fall short of capturing deep inferential properties.</li>
</ul>

<h3>Title: DaKultur: Evaluating the Cultural Awareness of Language Models for Danish with Native Speakers</h3>
<ul>
<li><strong>Authors: </strong>Max Müller-Eberstein, Mike Zhang, Elisa Bassignana, Peter Brunsgaard Trolle, Rob van der Goot</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02403">https://arxiv.org/abs/2504.02403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02403">https://arxiv.org/pdf/2504.02403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02403]] DaKultur: Evaluating the Cultural Awareness of Language Models for Danish with Native Speakers(https://arxiv.org/abs/2504.02403)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have seen widespread societal adoption. However, while they are able to interact with users in languages beyond English, they have been shown to lack cultural awareness, providing anglocentric or inappropriate responses for underrepresented language communities. To investigate this gap and disentangle linguistic versus cultural proficiency, we conduct the first cultural evaluation study for the mid-resource language of Danish, in which native speakers prompt different models to solve tasks requiring cultural awareness. Our analysis of the resulting 1,038 interactions from 63 demographically diverse participants highlights open challenges to cultural adaptation: Particularly, how currently employed automatically translated data are insufficient to train or measure cultural adaptation, and how training on native-speaker data can more than double response acceptance rates. We release our study data as DaKultur - the first native Danish cultural awareness dataset.</li>
</ul>

<h3>Title: AnesBench: Multi-Dimensional Evaluation of LLM Reasoning in Anesthesiology</h3>
<ul>
<li><strong>Authors: </strong>Xiang Feng, Wentao Jiang, Zengmao Wang, Yong Luo, Pingbo Xu, Baosheng Yu, Hua Jin, Bo Du, Jing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02404">https://arxiv.org/abs/2504.02404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02404">https://arxiv.org/pdf/2504.02404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02404]] AnesBench: Multi-Dimensional Evaluation of LLM Reasoning in Anesthesiology(https://arxiv.org/abs/2504.02404)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The application of large language models (LLMs) in the medical field has gained significant attention, yet their reasoning capabilities in more specialized domains like anesthesiology remain underexplored. In this paper, we systematically evaluate the reasoning capabilities of LLMs in anesthesiology and analyze key factors influencing their performance. To this end, we introduce AnesBench, a cross-lingual benchmark designed to assess anesthesiology-related reasoning across three levels: factual retrieval (System 1), hybrid reasoning (System 1.x), and complex decision-making (System 2). Through extensive experiments, we first explore how model characteristics, including model scale, Chain of Thought (CoT) length, and language transferability, affect reasoning performance. Then, we further evaluate the effectiveness of different training strategies, leveraging our curated anesthesiology-related dataset, including continuous pre-training (CPT) and supervised fine-tuning (SFT). Additionally, we also investigate how the test-time reasoning techniques, such as Best-of-N sampling and beam search, influence reasoning performance, and assess the impact of reasoning-enhanced model distillation, specifically DeepSeek-R1. We will publicly release AnesBench, along with our CPT and SFT training datasets and evaluation code at this https URL.</li>
</ul>

<h3>Title: Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Misrahi, Nadezhda Chirkova, Maxime Louis, Vassilina Nikoulina</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02411">https://arxiv.org/abs/2504.02411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02411">https://arxiv.org/pdf/2504.02411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02411]] Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation(https://arxiv.org/abs/2504.02411)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) enhances LLM factuality, but multi-domain applications face challenges like lack of diverse benchmarks and poor out-of-domain generalization. The first contribution of this work is to introduce a diverse benchmark comprising a variety of question-answering tasks from 8 sources and covering 13 domains. Our second contribution consists in systematically testing out-of-domain generalization for typical RAG tuning strategies. While our findings reveal that standard fine-tuning fails to generalize effectively, we show that sequence-level distillation with teacher-generated labels improves out-of-domain performance by providing more coherent supervision. Our findings highlight key strategies for improving multi-domain RAG robustness.</li>
</ul>

<h3>Title: Bridging the Theoretical Gap in Randomized Smoothing</h3>
<ul>
<li><strong>Authors: </strong>Blaise Delattre, Paul Caillon, Quentin Barthélemy, Erwan Fagnou, Alexandre Allauzen</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02412">https://arxiv.org/abs/2504.02412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02412">https://arxiv.org/pdf/2504.02412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02412]] Bridging the Theoretical Gap in Randomized Smoothing(https://arxiv.org/abs/2504.02412)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Randomized smoothing has become a leading approach for certifying adversarial robustness in machine learning models. However, a persistent gap remains between theoretical certified robustness and empirical robustness accuracy. This paper introduces a new framework that bridges this gap by leveraging Lipschitz continuity for certification and proposing a novel, less conservative method for computing confidence intervals in randomized smoothing. Our approach tightens the bounds of certified robustness, offering a more accurate reflection of model robustness in practice. Through rigorous experimentation we show that our method improves the robust accuracy, compressing the gap between empirical findings and previous theoretical results. We argue that investigating local Lipschitz constants and designing ad-hoc confidence intervals can further enhance the performance of randomized smoothing. These results pave the way for a deeper understanding of the relationship between Lipschitz continuity and certified robustness.</li>
</ul>

<h3>Title: Hyperspectral Remote Sensing Images Salient Object Detection: The First Benchmark Dataset and Baseline</h3>
<ul>
<li><strong>Authors: </strong>Peifu Liu, Huiyan Bai, Tingfa Xu, Jihui Wang, Huan Chen, Jianan Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02416">https://arxiv.org/abs/2504.02416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02416">https://arxiv.org/pdf/2504.02416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02416]] Hyperspectral Remote Sensing Images Salient Object Detection: The First Benchmark Dataset and Baseline(https://arxiv.org/abs/2504.02416)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The objective of hyperspectral remote sensing image salient object detection (HRSI-SOD) is to identify objects or regions that exhibit distinct spectrum contrasts with the background. This area holds significant promise for practical applications; however, progress has been limited by a notable scarcity of dedicated datasets and methodologies. To bridge this gap and stimulate further research, we introduce the first HRSI-SOD dataset, termed HRSSD, which includes 704 hyperspectral images and 5327 pixel-level annotated salient objects. The HRSSD dataset poses substantial challenges for salient object detection algorithms due to large scale variation, diverse foreground-background relations, and multi-salient objects. Additionally, we propose an innovative and efficient baseline model for HRSI-SOD, termed the Deep Spectral Saliency Network (DSSN). The core of DSSN is the Cross-level Saliency Assessment Block, which performs pixel-wise attention and evaluates the contributions of multi-scale similarity maps at each spatial location, effectively reducing erroneous responses in cluttered regions and emphasizes salient regions across scales. Additionally, the High-resolution Fusion Module combines bottom-up fusion strategy and learned spatial upsampling to leverage the strengths of multi-scale saliency maps, ensuring accurate localization of small objects. Experiments on the HRSSD dataset robustly validate the superiority of DSSN, underscoring the critical need for specialized datasets and methodologies in this domain. Further evaluations on the HSOD-BIT and HS-SOD datasets demonstrate the generalizability of the proposed method. The dataset and source code are publicly available at this https URL.</li>
</ul>

<h3>Title: Koney: A Cyber Deception Orchestration Framework for Kubernetes</h3>
<ul>
<li><strong>Authors: </strong>Mario Kahlhofer, Matteo Golinelli, Stefan Rass</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02431">https://arxiv.org/abs/2504.02431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02431">https://arxiv.org/pdf/2504.02431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02431]] Koney: A Cyber Deception Orchestration Framework for Kubernetes(https://arxiv.org/abs/2504.02431)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>System operators responsible for protecting software applications remain hesitant to implement cyber deception technology, including methods that place traps to catch attackers, despite its proven benefits. Overcoming their concerns removes a barrier that currently hinders industry adoption of deception technology. Our work introduces deception policy documents to describe deception technology "as code" and pairs them with Koney, a Kubernetes operator, which facilitates the setup, rotation, monitoring, and removal of traps in Kubernetes. We leverage cloud-native technologies, such as service meshes and eBPF, to automatically add traps to containerized software applications, without having access to the source code. We focus specifically on operational properties, such as maintainability, scalability, and simplicity, which we consider essential to accelerate the adoption of cyber deception technology and to facilitate further research on cyber deception.</li>
</ul>

<h3>Title: Robust Randomized Low-Rank Approximation with Row-Wise Outlier Detection</h3>
<ul>
<li><strong>Authors: </strong>Aidan Tiruvan</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02432">https://arxiv.org/abs/2504.02432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02432">https://arxiv.org/pdf/2504.02432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02432]] Robust Randomized Low-Rank Approximation with Row-Wise Outlier Detection(https://arxiv.org/abs/2504.02432)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robust low-rank approximation under row-wise adversarial corruption can be achieved with a single pass, randomized procedure that detects and removes outlier rows by thresholding their projected norms. We propose a scalable, non-iterative algorithm that efficiently recovers the underlying low-rank structure in the presence of row-wise adversarial corruption. By first compressing the data with a Johnson Lindenstrauss projection, our approach preserves the geometry of clean rows while dramatically reducing dimensionality. Robust statistical techniques based on the median and median absolute deviation then enable precise identification and removal of outlier rows with abnormally high norms. The subsequent rank-k approximation achieves near-optimal error bounds with a one pass procedure that scales linearly with the number of observations. Empirical results confirm that combining random sketches with robust statistics yields efficient, accurate decompositions even in the presence of large fractions of corrupted rows.</li>
</ul>

<h3>Title: OmniTalker: Real-Time Text-Driven Talking Head Generation with In-Context Audio-Visual Style Replication</h3>
<ul>
<li><strong>Authors: </strong>Zhongjian Wang, Peng Zhang, Jinwei Qi, Guangyuan Wang Sheng Xu, Bang Zhang, Liefeng Bo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02433">https://arxiv.org/abs/2504.02433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02433">https://arxiv.org/pdf/2504.02433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02433]] OmniTalker: Real-Time Text-Driven Talking Head Generation with In-Context Audio-Visual Style Replication(https://arxiv.org/abs/2504.02433)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Recent years have witnessed remarkable advances in talking head generation, owing to its potential to revolutionize the human-AI interaction from text interfaces into realistic video chats. However, research on text-driven talking heads remains underexplored, with existing methods predominantly adopting a cascaded pipeline that combines TTS systems with audio-driven talking head models. This conventional pipeline not only introduces system complexity and latency overhead but also fundamentally suffers from asynchronous audiovisual output and stylistic discrepancies between generated speech and visual expressions. To address these limitations, we introduce OmniTalker, an end-to-end unified framework that simultaneously generates synchronized speech and talking head videos from text and reference video in real-time zero-shot scenarios, while preserving both speech style and facial styles. The framework employs a dual-branch diffusion transformer architecture: the audio branch synthesizes mel-spectrograms from text, while the visual branch predicts fine-grained head poses and facial dynamics. To bridge modalities, we introduce a novel audio-visual fusion module that integrates cross-modal information to ensure temporal synchronization and stylistic coherence between audio and visual outputs. Furthermore, our in-context reference learning module effectively captures both speech and facial style characteristics from a single reference video without introducing an extra style extracting module. To the best of our knowledge, OmniTalker presents the first unified framework that jointly models speech style and facial style in a zero-shot setting, achieving real-time inference speed of 25 FPS. Extensive experiments demonstrate that our method surpasses existing approaches in generation quality, particularly excelling in style preservation and audio-video synchronization.</li>
</ul>

<h3>Title: SkyReels-A2: Compose Anything in Video Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Zhengcong Fei, Debang Li, Di Qiu, Jiahua Wang, Yikun Dou, Rui Wang, Jingtao Xu, Mingyuan Fan, Guibin Chen, Yang Li, Yahui Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02436">https://arxiv.org/abs/2504.02436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02436">https://arxiv.org/pdf/2504.02436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02436]] SkyReels-A2: Compose Anything in Video Diffusion Transformers(https://arxiv.org/abs/2504.02436)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.</li>
</ul>

<h3>Title: HGFormer: Topology-Aware Vision Transformer with HyperGraph Learning</h3>
<ul>
<li><strong>Authors: </strong>Hao Wang, Shuo Zhang, Biao Leng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02440">https://arxiv.org/abs/2504.02440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02440">https://arxiv.org/pdf/2504.02440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02440]] HGFormer: Topology-Aware Vision Transformer with HyperGraph Learning(https://arxiv.org/abs/2504.02440)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The computer vision community has witnessed an extensive exploration of vision transformers in the past two years. Drawing inspiration from traditional schemes, numerous works focus on introducing vision-specific inductive biases. However, the implicit modeling of permutation invariance and fully-connected interaction with individual tokens disrupts the regional context and spatial topology, further hindering higher-order modeling. This deviates from the principle of perceptual organization that emphasizes the local groups and overall topology of visual elements. Thus, we introduce the concept of hypergraph for perceptual exploration. Specifically, we propose a topology-aware vision transformer called HyperGraph Transformer (HGFormer). Firstly, we present a Center Sampling K-Nearest Neighbors (CS-KNN) algorithm for semantic guidance during hypergraph construction. Secondly, we present a topology-aware HyperGraph Attention (HGA) mechanism that integrates hypergraph topology as perceptual indications to guide the aggregation of global and unbiased information during hypergraph messaging. Using HGFormer as visual backbone, we develop an effective and unitive representation, achieving distinct and detailed scene depictions. Empirical experiments show that the proposed HGFormer achieves competitive performance compared to the recent SoTA counterparts on various visual benchmarks. Extensive ablation and visualization studies provide comprehensive explanations of our ideas and contributions.</li>
</ul>

<h3>Title: Cognitive Memory in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, Yong Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02441">https://arxiv.org/abs/2504.02441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02441">https://arxiv.org/pdf/2504.02441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02441]] Cognitive Memory in Large Language Models(https://arxiv.org/abs/2504.02441)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long-term, with sensory memory corresponding to input prompts, short-term memory processing immediate context, and long-term memory implemented via external databases or structures. The text-based memory section covers acquisition (selection and summarization), management (updating, accessing, storing, and resolving conflicts), and utilization (full-text search, SQL queries, semantic search). The KV cache-based memory section discusses selection methods (regularity-based summarization, score-based approaches, special token embeddings) and compression techniques (low-rank compression, KV merging, multimodal compression), along with management strategies like offloading and shared attention mechanisms. Parameter-based memory methods (LoRA, TTT, MoE) transform memories into model parameters to enhance efficiency, while hidden-state-based memory approaches (chunk mechanisms, recurrent transformers, Mamba model) improve long-text processing by combining RNN hidden states with current methods. Overall, the paper offers a comprehensive analysis of LLM memory mechanisms, highlighting their significance and future research directions.</li>
</ul>

<h3>Title: Taylor Series-Inspired Local Structure Fitting Network for Few-shot Point Cloud Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Changshuo Wang, Shuting He, Xiang Fang, Meiqing Wu, Siew-Kei Lam, Prayag Tiwari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02454">https://arxiv.org/abs/2504.02454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02454">https://arxiv.org/pdf/2504.02454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02454]] Taylor Series-Inspired Local Structure Fitting Network for Few-shot Point Cloud Semantic Segmentation(https://arxiv.org/abs/2504.02454)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Few-shot point cloud semantic segmentation aims to accurately segment "unseen" new categories in point cloud scenes using limited labeled data. However, pretraining-based methods not only introduce excessive time overhead but also overlook the local structure representation among irregular point clouds. To address these issues, we propose a pretraining-free local structure fitting network for few-shot point cloud semantic segmentation, named TaylorSeg. Specifically, inspired by Taylor series, we treat the local structure representation of irregular point clouds as a polynomial fitting problem and propose a novel local structure fitting convolution, called TaylorConv. This convolution learns the low-order basic information and high-order refined information of point clouds from explicit encoding of local geometric structures. Then, using TaylorConv as the basic component, we construct two variants of TaylorSeg: a non-parametric TaylorSeg-NN and a parametric TaylorSeg-PN. The former can achieve performance comparable to existing parametric models without pretraining. For the latter, we equip it with an Adaptive Push-Pull (APP) module to mitigate the feature distribution differences between the query set and the support set. Extensive experiments validate the effectiveness of the proposed method. Notably, under the 2-way 1-shot setting, TaylorSeg-PN achieves improvements of +2.28% and +4.37% mIoU on the S3DIS and ScanNet datasets respectively, compared to the previous state-of-the-art methods. Our code is available at this https URL.</li>
</ul>

<h3>Title: The Amenability Framework: Rethinking Causal Ordering Without Estimating Causal Effects</h3>
<ul>
<li><strong>Authors: </strong>Carlos Fernández-Loría, Jorge Loría</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02456">https://arxiv.org/abs/2504.02456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02456">https://arxiv.org/pdf/2504.02456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02456]] The Amenability Framework: Rethinking Causal Ordering Without Estimating Causal Effects(https://arxiv.org/abs/2504.02456)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Who should we prioritize for intervention when we cannot estimate intervention effects? In many applied domains (e.g., advertising, customer retention, and behavioral nudging) prioritization is guided by predictive models that estimate outcome probabilities rather than causal effects. This paper investigates when these predictions (scores) can effectively rank individuals by their intervention effects, particularly when direct effect estimation is infeasible or unreliable. We propose a conceptual framework based on amenability: an individual's latent proclivity to be influenced by an intervention. We then formalize conditions under which predictive scores serve as effective proxies for amenability. These conditions justify using non-causal scores for intervention prioritization, even when the scores do not directly estimate effects. We further show that, under plausible assumptions, predictive models can outperform causal effect estimators in ranking individuals by intervention effects. Empirical evidence from an advertising context supports our theoretical findings, demonstrating that predictive modeling can offer a more robust approach to targeting than effect estimation. Our framework suggests a shift in focus, from estimating effects to inferring who is amenable, as a practical and theoretically grounded strategy for prioritizing interventions in resource-constrained environments.</li>
</ul>

<h3>Title: CornerPoint3D: Look at the Nearest Corner Instead of the Center</h3>
<ul>
<li><strong>Authors: </strong>Ruixiao Zhang, Runwei Guan, Xiangyu Chen, Adam Prugel-Bennett, Xiaohao Cai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02464">https://arxiv.org/abs/2504.02464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02464">https://arxiv.org/pdf/2504.02464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02464]] CornerPoint3D: Look at the Nearest Corner Instead of the Center(https://arxiv.org/abs/2504.02464)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>3D object detection aims to predict object centers, dimensions, and rotations from LiDAR point clouds. Despite its simplicity, LiDAR captures only the near side of objects, making center-based detectors prone to poor localization accuracy in cross-domain tasks with varying point distributions. Meanwhile, existing evaluation metrics designed for single-domain assessment also suffer from overfitting due to dataset-specific size variations. A key question arises: Do we really need models to maintain excellent performance in the entire 3D bounding boxes after being applied across domains? Actually, one of our main focuses is on preventing collisions between vehicles and other obstacles, especially in cross-domain scenarios where correctly predicting the sizes is much more difficult. To address these issues, we rethink cross-domain 3D object detection from a practical perspective. We propose two new metrics that evaluate a model's ability to detect objects' closer-surfaces to the LiDAR sensor. Additionally, we introduce EdgeHead, a refinement head that guides models to focus more on learnable closer surfaces, significantly improving cross-domain performance under both our new and traditional BEV/3D metrics. Furthermore, we argue that predicting the nearest corner rather than the object center enhances robustness. We propose a novel 3D object detector, coined as CornerPoint3D, which is built upon CenterPoint and uses heatmaps to supervise the learning and detection of the nearest corner of each object. Our proposed methods realize a balanced trade-off between the detection quality of entire bounding boxes and the locating accuracy of closer surfaces to the LiDAR sensor, outperforming the traditional center-based detector CenterPoint in multiple cross-domain tasks and providing a more practically reasonable and robust cross-domain 3D object detection solution.</li>
</ul>

<h3>Title: Semantic segmentation of forest stands using deep learning</h3>
<ul>
<li><strong>Authors: </strong>Håkon Næss Sandum (1), Hans Ole Ørka (1), Oliver Tomic (2), Erik Næsset (1), Terje Gobakken (1) ((1) Faculty of Environmental Sciences and Natural Resource Management, Norwegian University of Life Sciences, NMBU, Ås, Norway, (2) Faculty of Science and Technology, Norwegian University of Life Sciences, NMBU, Ås, Norway)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02471">https://arxiv.org/abs/2504.02471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02471">https://arxiv.org/pdf/2504.02471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02471]] Semantic segmentation of forest stands using deep learning(https://arxiv.org/abs/2504.02471)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Forest stands are the fundamental units in forest management inventories, silviculture, and financial analysis within operational forestry. Over the past two decades, a common method for mapping stand borders has involved delineation through manual interpretation of stereographic aerial images. This is a time-consuming and subjective process, limiting operational efficiency and introducing inconsistencies. Substantial effort has been devoted to automating the process, using various algorithms together with aerial images and canopy height models constructed from airborne laser scanning (ALS) data, but manual interpretation remains the preferred method. Deep learning (DL) methods have demonstrated great potential in computer vision, yet their application to forest stand delineation remains unexplored in published research. This study presents a novel approach, framing stand delineation as a multiclass segmentation problem and applying a U-Net based DL framework. The model was trained and evaluated using multispectral images, ALS data, and an existing stand map created by an expert interpreter. Performance was assessed on independent data using overall accuracy, a standard metric for classification tasks that measures the proportions of correctly classified pixels. The model achieved an overall accuracy of 0.73. These results demonstrate strong potential for DL in automated stand delineation. However, a few key challenges were noted, especially for complex forest environments.</li>
</ul>

<h3>Title: MG-MotionLLM: A Unified Framework for Motion Comprehension and Generation across Multiple Granularities</h3>
<ul>
<li><strong>Authors: </strong>Bizhu Wu, Jinheng Xie, Keming Shen, Zhe Kong, Jianfeng Ren, Ruibin Bai, Rong Qu, Linlin Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02478">https://arxiv.org/abs/2504.02478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02478">https://arxiv.org/pdf/2504.02478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02478]] MG-MotionLLM: A Unified Framework for Motion Comprehension and Generation across Multiple Granularities(https://arxiv.org/abs/2504.02478)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent motion-aware large language models have demonstrated promising potential in unifying motion comprehension and generation. However, existing approaches primarily focus on coarse-grained motion-text modeling, where text describes the overall semantics of an entire motion sequence in just a few words. This limits their ability to handle fine-grained motion-relevant tasks, such as understanding and controlling the movements of specific body parts. To overcome this limitation, we pioneer MG-MotionLLM, a unified motion-language model for multi-granular motion comprehension and generation. We further introduce a comprehensive multi-granularity training scheme by incorporating a set of novel auxiliary tasks, such as localizing temporal boundaries of motion segments via detailed text as well as motion detailed captioning, to facilitate mutual reinforcement for motion-text modeling across various levels of granularity. Extensive experiments show that our MG-MotionLLM achieves superior performance on classical text-to-motion and motion-to-text tasks, and exhibits potential in novel fine-grained motion comprehension and editing tasks. Project page: CVI-SZU/MG-MotionLLM</li>
</ul>

<h3>Title: Graph Attention-Driven Bayesian Deep Unrolling for Dual-Peak Single-Photon Lidar Imaging</h3>
<ul>
<li><strong>Authors: </strong>Kyungmin Choi, JaKeoung Koo, Stephen McLaughlin, Abderrahim Halimi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02480">https://arxiv.org/abs/2504.02480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02480">https://arxiv.org/pdf/2504.02480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02480]] Graph Attention-Driven Bayesian Deep Unrolling for Dual-Peak Single-Photon Lidar Imaging(https://arxiv.org/abs/2504.02480)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Single-photon Lidar imaging offers a significant advantage in 3D imaging due to its high resolution and long-range capabilities, however it is challenging to apply in noisy environments with multiple targets per pixel. To tackle these challenges, several methods have been proposed. Statistical methods demonstrate interpretability on the inferred parameters, but they are often limited in their ability to handle complex scenes. Deep learning-based methods have shown superior performance in terms of accuracy and robustness, but they lack interpretability or they are limited to a single-peak per pixel. In this paper, we propose a deep unrolling algorithm for dual-peak single-photon Lidar imaging. We introduce a hierarchical Bayesian model for multiple targets and propose a neural network that unrolls the underlying statistical method. To support multiple targets, we adopt a dual depth maps representation and exploit geometric deep learning to extract features from the point cloud. The proposed method takes advantages of statistical methods and learning-based methods in terms of accuracy and quantifying uncertainty. The experimental results on synthetic and real data demonstrate the competitive performance when compared to existing methods, while also providing uncertainty information.</li>
</ul>

<h3>Title: Semiconductor Wafer Map Defect Classification with Tiny Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Faisal Mohammad, Duksan Ryu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02494">https://arxiv.org/abs/2504.02494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02494">https://arxiv.org/pdf/2504.02494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02494]] Semiconductor Wafer Map Defect Classification with Tiny Vision Transformers(https://arxiv.org/abs/2504.02494)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Semiconductor wafer defect classification is critical for ensuring high precision and yield in manufacturing. Traditional CNN-based models often struggle with class imbalances and recognition of the multiple overlapping defect types in wafer maps. To address these challenges, we propose ViT-Tiny, a lightweight Vision Transformer (ViT) framework optimized for wafer defect classification. Trained on the WM-38k dataset. ViT-Tiny outperforms its ViT-Base counterpart and state-of-the-art (SOTA) models, such as MSF-Trans and CNN-based architectures. Through extensive ablation studies, we determine that a patch size of 16 provides optimal performance. ViT-Tiny achieves an F1-score of 98.4%, surpassing MSF-Trans by 2.94% in four-defect classification, improving recall by 2.86% in two-defect classification, and increasing precision by 3.13% in three-defect classification. Additionally, it demonstrates enhanced robustness under limited labeled data conditions, making it a computationally efficient and reliable solution for real-world semiconductor defect detection.</li>
</ul>

<h3>Title: Inference-Time Scaling for Generalist Reward Modeling</h3>
<ul>
<li><strong>Authors: </strong>Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, Yu Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02495">https://arxiv.org/abs/2504.02495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02495">https://arxiv.org/pdf/2504.02495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02495]] Inference-Time Scaling for Generalist Reward Modeling(https://arxiv.org/abs/2504.02495)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that $\textit{proper learning methods could enable effective inference-time scalability}$. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the $\textbf{inference-time scalability of generalist RM}$, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, we adopt pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, we propose Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in $\textbf{DeepSeek-GRM}$ models. Furthermore, for effective inference-time scaling, we use parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced.</li>
</ul>

<h3>Title: ZClip: Adaptive Spike Mitigation for LLM Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Abhay Kumar, Louis Owen, Nilabhra Roy Chowdhury, Fabian Güra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02507">https://arxiv.org/abs/2504.02507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02507">https://arxiv.org/pdf/2504.02507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02507]] ZClip: Adaptive Spike Mitigation for LLM Pre-Training(https://arxiv.org/abs/2504.02507)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes. These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping. Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention. In this work, we propose ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time. Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms. At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise. Our code is available at: this https URL.</li>
</ul>

<h3>Title: APHQ-ViT: Post-Training Quantization with Average Perturbation Hessian Based Reconstruction for Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Zhuguanyu Wu, Jiayi Zhang, Jiaxin Chen, Jinyang Guo, Di Huang, Yunhong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02508">https://arxiv.org/abs/2504.02508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02508">https://arxiv.org/pdf/2504.02508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02508]] APHQ-ViT: Post-Training Quantization with Average Perturbation Hessian Based Reconstruction for Vision Transformers(https://arxiv.org/abs/2504.02508)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs) have become one of the most commonly used backbones for vision tasks. Despite their remarkable performance, they often suffer significant accuracy drops when quantized for practical deployment, particularly by post-training quantization (PTQ) under ultra-low bits. Recently, reconstruction-based PTQ methods have shown promising performance in quantizing Convolutional Neural Networks (CNNs). However, they fail when applied to ViTs, primarily due to the inaccurate estimation of output importance and the substantial accuracy degradation in quantizing post-GELU activations. To address these issues, we propose \textbf{APHQ-ViT}, a novel PTQ approach based on importance estimation with Average Perturbation Hessian (APH). Specifically, we first thoroughly analyze the current approximation approaches with Hessian loss, and propose an improved average perturbation Hessian loss. To deal with the quantization of the post-GELU activations, we design an MLP Reconstruction (MR) method by replacing the GELU function in MLP with ReLU and reconstructing it by the APH loss on a small unlabeled calibration set. Extensive experiments demonstrate that APHQ-ViT using linear quantizers outperforms existing PTQ methods by substantial margins in 3-bit and 4-bit across different vision tasks. The source code is available at this https URL.</li>
</ul>

<h3>Title: Towards Generalizing Temporal Action Segmentation to Unseen Views</h3>
<ul>
<li><strong>Authors: </strong>Emad Bahrami, Olga Zatsarynna, Gianpiero Francesca, Juergen Gall</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02512">https://arxiv.org/abs/2504.02512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02512">https://arxiv.org/pdf/2504.02512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02512]] Towards Generalizing Temporal Action Segmentation to Unseen Views(https://arxiv.org/abs/2504.02512)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>While there has been substantial progress in temporal action segmentation, the challenge to generalize to unseen views remains unaddressed. Hence, we define a protocol for unseen view action segmentation where camera views for evaluating the model are unavailable during training. This includes changing from top-frontal views to a side view or even more challenging from exocentric to egocentric views. Furthermore, we present an approach for temporal action segmentation that tackles this challenge. Our approach leverages a shared representation at both the sequence and segment levels to reduce the impact of view differences during training. We achieve this by introducing a sequence loss and an action loss, which together facilitate consistent video and action representations across different views. The evaluation on the Assembly101, IkeaASM, and EgoExoLearn datasets demonstrate significant improvements, with a 12.8% increase in F1@50 for unseen exocentric views and a substantial 54% improvement for unseen egocentric views.</li>
</ul>

<h3>Title: Exploration-Driven Generative Interactive Environments</h3>
<ul>
<li><strong>Authors: </strong>Nedko Savov, Naser Kazemi, Mohammad Mahdi, Danda Pani Paudel, Xi Wang, Luc Van Gool</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02515">https://arxiv.org/abs/2504.02515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02515">https://arxiv.org/pdf/2504.02515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02515]] Exploration-Driven Generative Interactive Environments(https://arxiv.org/abs/2504.02515)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modern world models require costly and time-consuming collection of large video datasets with action demonstrations by people or by environment-specific agents. To simplify training, we focus on using many virtual environments for inexpensive, automatically collected interaction data. Genie, a recent multi-environment world model, demonstrates simulation abilities of many environments with shared behavior. Unfortunately, training their model requires expensive demonstrations. Therefore, we propose a training framework merely using a random agent in virtual environments. While the model trained in this manner exhibits good controls, it is limited by the random exploration possibilities. To address this limitation, we propose AutoExplore Agent - an exploration agent that entirely relies on the uncertainty of the world model, delivering diverse data from which it can learn the best. Our agent is fully independent of environment-specific rewards and thus adapts easily to new environments. With this approach, the pretrained multi-environment model can quickly adapt to new environments achieving video fidelity and controllability improvement. In order to obtain automatically large-scale interaction datasets for pretraining, we group environments with similar behavior and controls. To this end, we annotate the behavior and controls of 974 virtual environments - a dataset that we name RetroAct. For building our model, we first create an open implementation of Genie - GenieRedux and apply enhancements and adaptations in our version GenieRedux-G. Our code and data are available at this https URL.</li>
</ul>

<h3>Title: MultiNeRF: Multiple Watermark Embedding for Neural Radiance Fields</h3>
<ul>
<li><strong>Authors: </strong>Yash Kulthe, Andrew Gilbert, John Collomosse</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02517">https://arxiv.org/abs/2504.02517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02517">https://arxiv.org/pdf/2504.02517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02517]] MultiNeRF: Multiple Watermark Embedding for Neural Radiance Fields(https://arxiv.org/abs/2504.02517)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark</a></li>
<li><strong>Abstract: </strong>We present MultiNeRF, a 3D watermarking method that embeds multiple uniquely keyed watermarks within images rendered by a single Neural Radiance Field (NeRF) model, whilst maintaining high visual quality. Our approach extends the TensoRF NeRF model by incorporating a dedicated watermark grid alongside the existing geometry and appearance grids. This extension ensures higher watermark capacity without entangling watermark signals with scene content. We propose a FiLM-based conditional modulation mechanism that dynamically activates watermarks based on input identifiers, allowing multiple independent watermarks to be embedded and extracted without requiring model retraining. MultiNeRF is validated on the NeRF-Synthetic and LLFF datasets, with statistically significant improvements in robust capacity without compromising rendering quality. By generalizing single-watermark NeRF methods into a flexible multi-watermarking framework, MultiNeRF provides a scalable solution for 3D content. attribution.</li>
</ul>

<h3>Title: Data-Driven Object Tracking: Integrating Modular Neural Networks into a Kalman Framework</h3>
<ul>
<li><strong>Authors: </strong>Christian Alexander Holz, Christian Bader, Markus Enzweiler, Matthias Drüppel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02519">https://arxiv.org/abs/2504.02519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02519">https://arxiv.org/pdf/2504.02519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02519]] Data-Driven Object Tracking: Integrating Modular Neural Networks into a Kalman Framework(https://arxiv.org/abs/2504.02519)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>This paper presents novel Machine Learning (ML) methodologies for Multi-Object Tracking (MOT), specifically designed to meet the increasing complexity and precision demands of Advanced Driver Assistance Systems (ADAS). We introduce three Neural Network (NN) models that address key challenges in MOT: (i) the Single-Prediction Network (SPENT) for trajectory prediction, (ii) the Single-Association Network (SANT) for mapping individual Sensor Object (SO) to existing tracks, and (iii) the Multi-Association Network (MANTa) for associating multiple SOs to multiple tracks. These models are seamlessly integrated into a traditional Kalman Filter (KF) framework, maintaining the system's modularity by replacing relevant components without disrupting the overall architecture. Importantly, all three networks are designed to be run in a realtime, embedded environment. Each network contains less than 50k trainable parameters. Our evaluation, conducted on the public KITTI tracking dataset, demonstrates significant improvements in tracking performance. SPENT reduces the Root Mean Square Error (RMSE) by 50% compared to a standard KF, while SANT and MANTa achieve up to 95% accuracy in sensor object-to-track assignments. These results underscore the effectiveness of incorporating task-specific NNs into traditional tracking systems, boosting performance and robustness while preserving modularity, maintainability, and interpretability.</li>
</ul>

<h3>Title: UNDO: Understanding Distillation as Optimization</h3>
<ul>
<li><strong>Authors: </strong>Kushal Jain, Piyushi Goyal, Kumar Shridhar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02521">https://arxiv.org/abs/2504.02521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02521">https://arxiv.org/pdf/2504.02521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02521]] UNDO: Understanding Distillation as Optimization(https://arxiv.org/abs/2504.02521)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge distillation has emerged as an effective strategy for compressing large language models' (LLMs) knowledge into smaller, more efficient student models. However, standard one-shot distillation methods often produce suboptimal results due to a mismatch between teacher-generated rationales and the student's specific learning requirements. In this paper, we introduce the UNDO: UNderstanding Distillation as Optimization framework, designed to bridge this gap by iteratively identifying the student's errors and prompting the teacher to refine its explanations accordingly. Each iteration directly targets the student's learning deficiencies, motivating the teacher to provide tailored and enhanced rationales that specifically address these weaknesses. Empirical evaluations on various challenging mathematical and commonsense reasoning tasks demonstrate that our iterative distillation method, UNDO, significantly outperforms standard one-step distillation methods, achieving performance gains of up to 20%. Additionally, we show that teacher-generated data refined through our iterative process remains effective even when applied to different student models, underscoring the broad applicability of our approach. Our work fundamentally reframes knowledge distillation as an iterative teacher-student interaction, effectively leveraging dynamic refinement by the teacher for better knowledge distillation.</li>
</ul>

<h3>Title: Charm: The Missing Piece in ViT fine-tuning for Image Aesthetic Assessment</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Behrad, Tinne Tuytelaars, Johan Wagemans</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02522">https://arxiv.org/abs/2504.02522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02522">https://arxiv.org/pdf/2504.02522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02522]] Charm: The Missing Piece in ViT fine-tuning for Image Aesthetic Assessment(https://arxiv.org/abs/2504.02522)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The capacity of Vision transformers (ViTs) to handle variable-sized inputs is often constrained by computational complexity and batch processing limitations. Consequently, ViTs are typically trained on small, fixed-size images obtained through downscaling or cropping. While reducing computational burden, these methods result in significant information loss, negatively affecting tasks like image aesthetic assessment. We introduce Charm, a novel tokenization approach that preserves Composition, High-resolution, Aspect Ratio, and Multi-scale information simultaneously. Charm prioritizes high-resolution details in specific regions while downscaling others, enabling shorter fixed-size input sequences for ViTs while incorporating essential information. Charm is designed to be compatible with pre-trained ViTs and their learned positional embeddings. By providing multiscale input and introducing variety to input tokens, Charm improves ViT performance and generalizability for image aesthetic assessment. We avoid cropping or changing the aspect ratio to further preserve information. Extensive experiments demonstrate significant performance improvements on various image aesthetic and quality assessment datasets (up to 8.1 %) using a lightweight ViT backbone. Code and pre-trained models are available at this https URL.</li>
</ul>

<h3>Title: SelfMedHPM: Self Pre-training With Hard Patches Mining Masked Autoencoders For Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yunhao Lv, Lingyu Chen, Jian Wang, Yangxi Li, Fang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02524">https://arxiv.org/abs/2504.02524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02524">https://arxiv.org/pdf/2504.02524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02524]] SelfMedHPM: Self Pre-training With Hard Patches Mining Masked Autoencoders For Medical Image Segmentation(https://arxiv.org/abs/2504.02524)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>In recent years, deep learning methods such as convolutional neural network (CNN) and transformers have made significant progress in CT multi-organ segmentation. However, CT multi-organ segmentation methods based on masked image modeling (MIM) are very limited. There are already methods using MAE for CT multi-organ segmentation task, we believe that the existing methods do not identify the most difficult areas to reconstruct. To this end, we propose a MIM self-training framework with hard patches mining masked autoencoders for CT multi-organ segmentation tasks (selfMedHPM). The method performs ViT self-pretraining on the training set of the target data and introduces an auxiliary loss predictor, which first predicts the patch loss and determines the location of the next mask. SelfMedHPM implementation is better than various competitive methods in abdominal CT multi-organ segmentation and body CT multi-organ segmentation. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for abdomen mult-organ segmentation and the SinoMed Whole Body (SMWB) dataset for body multi-organ segmentation tasks.</li>
</ul>

<h3>Title: Delineate Anything: Resolution-Agnostic Field Boundary Delineation on Satellite Imagery</h3>
<ul>
<li><strong>Authors: </strong>Mykola Lavreniuk, Nataliia Kussul, Andrii Shelestov, Bohdan Yailymov, Yevhenii Salii, Volodymyr Kuzin, Zoltan Szantoi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02534">https://arxiv.org/abs/2504.02534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02534">https://arxiv.org/pdf/2504.02534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02534]] Delineate Anything: Resolution-Agnostic Field Boundary Delineation on Satellite Imagery(https://arxiv.org/abs/2504.02534)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The accurate delineation of agricultural field boundaries from satellite imagery is vital for land management and crop monitoring. However, current methods face challenges due to limited dataset sizes, resolution discrepancies, and diverse environmental conditions. We address this by reformulating the task as instance segmentation and introducing the Field Boundary Instance Segmentation - 22M dataset (FBIS-22M), a large-scale, multi-resolution dataset comprising 672,909 high-resolution satellite image patches (ranging from 0.25 m to 10 m) and 22,926,427 instance masks of individual fields, significantly narrowing the gap between agricultural datasets and those in other computer vision domains. We further propose Delineate Anything, an instance segmentation model trained on our new FBIS-22M dataset. Our proposed model sets a new state-of-the-art, achieving a substantial improvement of 88.5% in mAP@0.5 and 103% in mAP@0.5:0.95 over existing methods, while also demonstrating significantly faster inference and strong zero-shot generalization across diverse image resolutions and unseen geographic regions. Code, pre-trained models, and the FBIS-22M dataset are available at this https URL.</li>
</ul>

<h3>Title: A Sensorimotor Vision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Konrad Gadzicki, Kerstin Schill, Christoph Zetzsche</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02536">https://arxiv.org/abs/2504.02536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02536">https://arxiv.org/pdf/2504.02536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02536]] A Sensorimotor Vision Transformer(https://arxiv.org/abs/2504.02536)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper presents the Sensorimotor Transformer (SMT), a vision model inspired by human saccadic eye movements that prioritize high-saliency regions in visual input to enhance computational efficiency and reduce memory consumption. Unlike traditional models that process all image patches uniformly, SMT identifies and selects the most salient patches based on intrinsic two-dimensional (i2D) features, such as corners and occlusions, which are known to convey high-information content and align with human fixation patterns. The SMT architecture uses this biological principle to leverage vision transformers to process only the most informative patches, allowing for a substantial reduction in memory usage that scales with the sequence length of selected patches. This approach aligns with visual neuroscience findings, suggesting that the human visual system optimizes information gathering through selective, spatially dynamic focus. Experimental evaluations on Imagenet-1k demonstrate that SMT achieves competitive top-1 accuracy while significantly reducing memory consumption and computational complexity, particularly when a limited number of patches is used. This work introduces a saccade-like selection mechanism into transformer-based vision models, offering an efficient alternative for image analysis and providing new insights into biologically motivated architectures for resource-constrained applications.</li>
</ul>

<h3>Title: Blockchain and Distributed Ledger Technologies for Cyberthreat Intelligence Sharing</h3>
<ul>
<li><strong>Authors: </strong>Asadullah Tariq, Tariq Qayyum, Saed Alrabaee, Mohamed Adel Serhani</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02537">https://arxiv.org/abs/2504.02537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02537">https://arxiv.org/pdf/2504.02537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02537]] Blockchain and Distributed Ledger Technologies for Cyberthreat Intelligence Sharing(https://arxiv.org/abs/2504.02537)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Cyberthreat intelligence sharing is a critical aspect of cybersecurity, and it is essential to understand its definition, objectives, benefits, and impact on society. Blockchain and Distributed Ledger Technology (DLT) are emerging technologies that have the potential to transform intelligence sharing. This paper aims to provide a comprehensive understanding of intelligence sharing and the role of blockchain and DLT in enhancing it. The paper addresses questions related to the definition, objectives, benefits, and impact of intelligence sharing and provides a review of the existing literature. Additionally, the paper explores the challenges associated with blockchain and DLT and their potential impact on security and privacy. The paper also discusses the use of DLT and blockchain in security and intelligence sharing and highlights the associated challenges and risks. Furthermore, the paper examines the potential impact of a National Cybersecurity Strategy on addressing cybersecurity risks. Finally, the paper explores the experimental set up required for implementing blockchain and DLT for intelligence sharing and discusses the curricular ramifications of intelligence sharing.</li>
</ul>

<h3>Title: Audio-visual Controlled Video Diffusion with Masked Selective State Spaces Modeling for Natural Talking Head Generation</h3>
<ul>
<li><strong>Authors: </strong>Fa-Ting Hong, Zunnan Xu, Zixiang Zhou, Jun Zhou, Xiu Li, Qin Lin, Qinglin Lu, Dan Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02542">https://arxiv.org/abs/2504.02542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02542">https://arxiv.org/pdf/2504.02542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02542]] Audio-visual Controlled Video Diffusion with Masked Selective State Spaces Modeling for Natural Talking Head Generation(https://arxiv.org/abs/2504.02542)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce \textbf{ACTalker}, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict.</li>
</ul>

<h3>Title: Fourier Sliced-Wasserstein Embedding for Multisets and Measures</h3>
<ul>
<li><strong>Authors: </strong>Tal Amir, Nadav Dym</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02544">https://arxiv.org/abs/2504.02544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02544">https://arxiv.org/pdf/2504.02544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02544]] Fourier Sliced-Wasserstein Embedding for Multisets and Measures(https://arxiv.org/abs/2504.02544)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present the Fourier Sliced-Wasserstein (FSW) embedding - a novel method to embed multisets and measures over $\mathbb{R}^d$ into Euclidean space. Our proposed embedding approximately preserves the sliced Wasserstein distance on distributions, thereby yielding geometrically meaningful representations that better capture the structure of the input. Moreover, it is injective on measures and bi-Lipschitz on multisets - a significant advantage over prevalent methods based on sum- or max-pooling, which are provably not bi-Lipschitz, and, in many cases, not even injective. The required output dimension for these guarantees is near-optimal: roughly $2 N d$, where $N$ is the maximal input multiset size. Furthermore, we prove that it is impossible to embed distributions over $\mathbb{R}^d$ into Euclidean space in a bi-Lipschitz manner. Thus, the metric properties of our embedding are, in a sense, the best possible. Through numerical experiments, we demonstrate that our method yields superior multiset representations that improve performance in practical learning tasks. Specifically, we show that (a) a simple combination of the FSW embedding with an MLP achieves state-of-the-art performance in learning the (non-sliced) Wasserstein distance; and (b) replacing max-pooling with the FSW embedding makes PointNet significantly more robust to parameter reduction, with only minor performance degradation even after a 40-fold reduction.</li>
</ul>

<h3>Title: MAD: Makeup All-in-One with Cross-Domain Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Bo-Kai Ruan, Hong-Han Shuai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02545">https://arxiv.org/abs/2504.02545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02545">https://arxiv.org/pdf/2504.02545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02545]] MAD: Makeup All-in-One with Cross-Domain Diffusion Model(https://arxiv.org/abs/2504.02545)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing makeup techniques often require designing multiple models to handle different inputs and align features across domains for different makeup tasks, e.g., beauty filter, makeup transfer, and makeup removal, leading to increased complexity. Another limitation is the absence of text-guided makeup try-on, which is more user-friendly without needing reference images. In this study, we make the first attempt to use a single model for various makeup tasks. Specifically, we formulate different makeup tasks as cross-domain translations and leverage a cross-domain diffusion model to accomplish all tasks. Unlike existing methods that rely on separate encoder-decoder configurations or cycle-based mechanisms, we propose using different domain embeddings to facilitate domain control. This allows for seamless domain switching by merely changing embeddings with a single model, thereby reducing the reliance on additional modules for different tasks. Moreover, to support precise text-to-makeup applications, we introduce the MT-Text dataset by extending the MT dataset with textual annotations, advancing the practicality of makeup technologies.</li>
</ul>

<h3>Title: GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, Yong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02546">https://arxiv.org/abs/2504.02546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02546">https://arxiv.org/pdf/2504.02546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02546]] GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning(https://arxiv.org/abs/2504.02546)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) can directly enhance the reasoning capabilities of large language models without extensive reliance on Supervised Fine-Tuning (SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism and propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike conventional methods, GPG directly optimize the original RL objective, thus obviating the need for surrogate loss functions. As illustrated in our paper, by eliminating both the critic and reference models, and avoiding KL divergence constraints, our approach significantly simplifies the training process when compared to Group Relative Policy Optimization (GRPO). Our approach achieves superior performance without relying on auxiliary techniques or adjustments. Extensive experiments demonstrate that our method not only reduces computational costs but also consistently outperforms GRPO across various unimodal and multimodal tasks. Our code is available at this https URL.</li>
</ul>

<h3>Title: Rip Current Segmentation: A Novel Benchmark and YOLOv8 Baseline Results</h3>
<ul>
<li><strong>Authors: </strong>Andrei Dumitriu, Florin Tatui, Florin Miron, Radu Tudor Ionescu, Radu Timofte</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02558">https://arxiv.org/abs/2504.02558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02558">https://arxiv.org/pdf/2504.02558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02558]] Rip Current Segmentation: A Novel Benchmark and YOLOv8 Baseline Results(https://arxiv.org/abs/2504.02558)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Rip currents are the leading cause of fatal accidents and injuries on many beaches worldwide, emphasizing the importance of automatically detecting these hazardous surface water currents. In this paper, we address a novel task: rip current instance segmentation. We introduce a comprehensive dataset containing $2,466$ images with newly created polygonal annotations for instance segmentation, used for training and validation. Additionally, we present a novel dataset comprising $17$ drone videos (comprising about $24K$ frames) captured at $30 FPS$, annotated with both polygons for instance segmentation and bounding boxes for object detection, employed for testing purposes. We train various versions of YOLOv8 for instance segmentation on static images and assess their performance on the test dataset (videos). The best results were achieved by the YOLOv8-nano model (runnable on a portable device), with an mAP50 of $88.94%$ on the validation dataset and $81.21%$ macro average on the test dataset. The results provide a baseline for future research in rip current segmentation. Our work contributes to the existing literature by introducing a detailed, annotated dataset, and training a deep learning model for instance segmentation of rip currents. The code, training details and the annotated dataset are made publicly available at this https URL.</li>
</ul>

<h3>Title: Leveraging LLM For Synchronizing Information Across Multilingual Tables</h3>
<ul>
<li><strong>Authors: </strong>Siddharth Khincha, Tushar Kataria, Ankita Anand, Dan Roth, Vivek Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02559">https://arxiv.org/abs/2504.02559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02559">https://arxiv.org/pdf/2504.02559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02559]] Leveraging LLM For Synchronizing Information Across Multilingual Tables(https://arxiv.org/abs/2504.02559)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The vast amount of online information today poses challenges for non-English speakers, as much of it is concentrated in high-resource languages such as English and French. Wikipedia reflects this imbalance, with content in low-resource languages frequently outdated or incomplete. Recent research has sought to improve cross-language synchronization of Wikipedia tables using rule-based methods. These approaches can be effective, but they struggle with complexity and generalization. This paper explores large language models (LLMs) for multilingual information synchronization, using zero-shot prompting as a scalable solution. We introduce the Information Updation dataset, simulating the real-world process of updating outdated Wikipedia tables, and evaluate LLM performance. Our findings reveal that single-prompt approaches often produce suboptimal results, prompting us to introduce a task decomposition strategy that enhances coherence and accuracy. Our proposed method outperforms existing baselines, particularly in Information Updation (1.79%) and Information Addition (20.58%), highlighting the model strength in dynamically updating and enriching data across architectures</li>
</ul>

<h3>Title: Language Models reach higher Agreement than Humans in Historical Interpretation</h3>
<ul>
<li><strong>Authors: </strong>Fabio Celli, Georgios Spathulas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02572">https://arxiv.org/abs/2504.02572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02572">https://arxiv.org/pdf/2504.02572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02572]] Language Models reach higher Agreement than Humans in Historical Interpretation(https://arxiv.org/abs/2504.02572)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper compares historical annotations by humans and Large Language Models. The findings reveal that both exhibit some cultural bias, but Large Language Models achieve a higher consensus on the interpretation of historical facts from short texts. While humans tend to disagree on the basis of their personal biases, Large Models disagree when they skip information or produce hallucinations. These findings have significant implications for digital humanities, enabling large-scale annotation and quantitative analysis of historical data. This offers new educational and research opportunities to explore historical interpretations from different Language Models, fostering critical thinking about bias.</li>
</ul>

<h3>Title: Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme</h3>
<ul>
<li><strong>Authors: </strong>Yan Ma, Steffi Chern, Xuyang Shen, Yiran Zhong, Pengfei Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02587">https://arxiv.org/abs/2504.02587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02587">https://arxiv.org/pdf/2504.02587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02587]] Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme(https://arxiv.org/abs/2504.02587)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research.</li>
</ul>

<h3>Title: Leveraging Sparse Annotations for Leukemia Diagnosis on the Large Leukemia Dataset</h3>
<ul>
<li><strong>Authors: </strong>Abdul Rehman, Talha Meraj, Aiman Mahmood Minhas, Ayisha Imran, Mohsen Ali, Waqas Sultani, Mubarak Shah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02602">https://arxiv.org/abs/2504.02602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02602">https://arxiv.org/pdf/2504.02602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02602]] Leveraging Sparse Annotations for Leukemia Diagnosis on the Large Leukemia Dataset(https://arxiv.org/abs/2504.02602)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Leukemia is 10th most frequently diagnosed cancer and one of the leading causes of cancer related deaths worldwide. Realistic analysis of Leukemia requires White Blook Cells (WBC) localization, classification, and morphological assessment. Despite deep learning advances in medical imaging, leukemia analysis lacks a large, diverse multi-task dataset, while existing small datasets lack domain diversity, limiting real world applicability. To overcome dataset challenges, we present a large scale WBC dataset named Large Leukemia Dataset (LLD) and novel methods for detecting WBC with their attributes. Our contribution here is threefold. First, we present a large-scale Leukemia dataset collected through Peripheral Blood Films (PBF) from several patients, through multiple microscopes, multi cameras, and multi magnification. To enhance diagnosis explainability and medical expert acceptance, each leukemia cell is annotated at 100x with 7 morphological attributes, ranging from Cell Size to Nuclear Shape. Secondly, we propose a multi task model that not only detects WBCs but also predicts their attributes, providing an interpretable and clinically meaningful solution. Third, we propose a method for WBC detection with attribute analysis using sparse annotations. This approach reduces the annotation burden on hematologists, requiring them to mark only a small area within the field of view. Our method enables the model to leverage the entire field of view rather than just the annotated regions, enhancing learning efficiency and diagnostic accuracy. From diagnosis explainability to overcoming domain shift challenges, presented datasets could be used for many challenging aspects of microscopic image analysis. The datasets, code, and demo are available at: this https URL</li>
</ul>

<h3>Title: Improving Counterfactual Truthfulness for Molecular Property Prediction through Uncertainty Quantification</h3>
<ul>
<li><strong>Authors: </strong>Jonas Teufel, Annika Leinweber, Pascal Friederich</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02606">https://arxiv.org/abs/2504.02606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02606">https://arxiv.org/pdf/2504.02606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02606]] Improving Counterfactual Truthfulness for Molecular Property Prediction through Uncertainty Quantification(https://arxiv.org/abs/2504.02606)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Explainable AI (xAI) interventions aim to improve interpretability for complex black-box models, not only to improve user trust but also as a means to extract scientific insights from high-performing predictive systems. In molecular property prediction, counterfactual explanations offer a way to understand predictive behavior by highlighting which minimal perturbations in the input molecular structure cause the greatest deviation in the predicted property. However, such explanations only allow for meaningful scientific insights if they reflect the distribution of the true underlying property -- a feature we define as counterfactual truthfulness. To increase this truthfulness, we propose the integration of uncertainty estimation techniques to filter counterfactual candidates with high predicted uncertainty. Through computational experiments with synthetic and real-world datasets, we demonstrate that traditional uncertainty estimation methods, such as ensembles and mean-variance estimation, can already substantially reduce the average prediction error and increase counterfactual truthfulness, especially for out-of-distribution settings. Our results highlight the importance and potential impact of incorporating uncertainty estimation into explainability methods, especially considering the relatively high effectiveness of low-effort interventions like model ensembles.</li>
</ul>

<h3>Title: Fine-Tuning Visual Autoregressive Models for Subject-Driven Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiwoo Chung, Sangeek Hyun, Hyunjun Kim, Eunseo Koh, MinKyu Lee, Jae-Pil Heo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02612">https://arxiv.org/abs/2504.02612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02612">https://arxiv.org/pdf/2504.02612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02612]] Fine-Tuning Visual Autoregressive Models for Subject-Driven Generation(https://arxiv.org/abs/2504.02612)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-image generative models have enabled numerous practical applications, including subject-driven generation, which fine-tunes pretrained models to capture subject semantics from only a few examples. While diffusion-based models produce high-quality images, their extensive denoising steps result in significant computational overhead, limiting real-world applicability. Visual autoregressive~(VAR) models, which predict next-scale tokens rather than spatially adjacent ones, offer significantly faster inference suitable for practical deployment. In this paper, we propose the first VAR-based approach for subject-driven generation. However, na\"ıve fine-tuning VAR leads to computational overhead, language drift, and reduced diversity. To address these challenges, we introduce selective layer tuning to reduce complexity and prior distillation to mitigate language drift. Additionally, we found that the early stages have a greater influence on the generation of subject than the latter stages, which merely synthesize local details. Based on this finding, we propose scale-wise weighted tuning, which prioritizes coarser resolutions for promoting the model to focus on the subject-relevant information instead of local details. Extensive experiments validate that our method significantly outperforms diffusion-based baselines across various metrics and demonstrates its practical usage.</li>
</ul>

<h3>Title: Variational Online Mirror Descent for Robust Learning in Schrödinger Bridge</h3>
<ul>
<li><strong>Authors: </strong>Dong-Sig Han, Jaein Kim, Hee Bin Yoo, Byoung-Tak Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02618">https://arxiv.org/abs/2504.02618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02618">https://arxiv.org/pdf/2504.02618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02618]] Variational Online Mirror Descent for Robust Learning in Schrödinger Bridge(https://arxiv.org/abs/2504.02618)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Schödinger bridge (SB) has evolved into a universal class of probabilistic generative models. In practice, however, estimated learning signals are often uncertain, and the reliability promised by existing methods is often based on speculative optimal-case scenarios. Recent studies regarding the Sinkhorn algorithm through mirror descent (MD) have gained attention, revealing geometric insights into solution acquisition of the SB problems. In this paper, we propose a variational online MD (OMD) framework for the SB problems, which provides further stability to SB solvers. We formally prove convergence and a regret bound for the novel OMD formulation of SB acquisition. As a result, we propose a simulation-free SB algorithm called Variational Mirrored Schrödinger Bridge (VMSB) by utilizing the Wasserstein-Fisher-Rao geometry of the Gaussian mixture parameterization for Schrödinger potentials. Based on the Wasserstein gradient flow theory, the algorithm offers tractable learning dynamics that precisely approximate each OMD step. In experiments, we validate the performance of the proposed VMSB algorithm across an extensive suite of benchmarks. VMSB consistently outperforms contemporary SB solvers on a range of SB problems, demonstrating the robustness predicted by our theory.</li>
</ul>

<h3>Title: Grammar-based Ordinary Differential Equation Discovery</h3>
<ul>
<li><strong>Authors: </strong>Karin L. Yu, Eleni Chatzi, Georgios Kissas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, cs.SC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02630">https://arxiv.org/abs/2504.02630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02630">https://arxiv.org/pdf/2504.02630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02630]] Grammar-based Ordinary Differential Equation Discovery(https://arxiv.org/abs/2504.02630)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The understanding and modeling of complex physical phenomena through dynamical systems has historically driven scientific progress, as it provides the tools for predicting the behavior of different systems under diverse conditions through time. The discovery of dynamical systems has been indispensable in engineering, as it allows for the analysis and prediction of complex behaviors for computational modeling, diagnostics, prognostics, and control of engineered systems. Joining recent efforts that harness the power of symbolic regression in this domain, we propose a novel framework for the end-to-end discovery of ordinary differential equations (ODEs), termed Grammar-based ODE Discovery Engine (GODE). The proposed methodology combines formal grammars with dimensionality reduction and stochastic search for efficiently navigating high-dimensional combinatorial spaces. Grammars allow us to seed domain knowledge and structure for both constraining, as well as, exploring the space of candidate expressions. GODE proves to be more sample- and parameter-efficient than state-of-the-art transformer-based models and to discover more accurate and parsimonious ODE expressions than both genetic programming- and other grammar-based methods for more complex inference tasks, such as the discovery of structural dynamics. Thus, we introduce a tool that could play a catalytic role in dynamics discovery tasks, including modeling, system identification, and monitoring tasks.</li>
</ul>

<h3>Title: Solving the Paint Shop Problem with Flexible Management of Multi-Lane Buffers Using Reinforcement Learning and Action Masking</h3>
<ul>
<li><strong>Authors: </strong>Mirko Stappert, Bernhard Lutz, Janis Brammer, Dirk Neumann</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02644">https://arxiv.org/abs/2504.02644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02644">https://arxiv.org/pdf/2504.02644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02644]] Solving the Paint Shop Problem with Flexible Management of Multi-Lane Buffers Using Reinforcement Learning and Action Masking(https://arxiv.org/abs/2504.02644)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the paint shop problem, an unordered incoming sequence of cars assigned to different colors has to be reshuffled with the objective of minimizing the number of color changes. To reshuffle the incoming sequence, manufacturers can employ a first-in-first-out multi-lane buffer system allowing store and retrieve operations. So far, prior studies primarily focused on simple decision heuristics like greedy or simplified problem variants that do not allow full flexibility when performing store and retrieve operations. In this study, we propose a reinforcement learning approach to minimize color changes for the flexible problem variant, where store and retrieve operations can be performed in an arbitrary order. After proving that greedy retrieval is optimal, we incorporate this finding into the model using action masking. Our evaluation, based on 170 problem instances with 2-8 buffer lanes and 5-15 colors, shows that our approach reduces color changes compared to existing methods by considerable margins depending on the problem size. Furthermore, we demonstrate the robustness of our approach towards different buffer sizes and imbalanced color distributions.</li>
</ul>

<h3>Title: Prompt Optimization with Logged Bandit Data</h3>
<ul>
<li><strong>Authors: </strong>Haruka Kiyohara, Daniel Yiming Cao, Yuta Saito, Thorsten Joachims</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02646">https://arxiv.org/abs/2504.02646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02646">https://arxiv.org/pdf/2504.02646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02646]] Prompt Optimization with Logged Bandit Data(https://arxiv.org/abs/2504.02646)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We study how to use naturally available user feedback, such as clicks, to optimize large language model (LLM) pipelines for generating personalized sentences using prompts. Naive approaches, which estimate the policy gradient in the prompt space, suffer either from variance caused by the large action space of prompts or bias caused by inaccurate reward predictions. To circumvent these challenges, we propose a novel kernel-based off-policy gradient method, which estimates the policy gradient by leveraging similarity among generated sentences, substantially reducing variance while suppressing the bias. Empirical results on our newly established suite of benchmarks demonstrate the effectiveness of the proposed approach in generating personalized descriptions for movie recommendations, particularly when the number of candidate prompts is large.</li>
</ul>

<h3>Title: Compositionality Unlocks Deep Interpretable Models</h3>
<ul>
<li><strong>Authors: </strong>Thomas Dooms, Ward Gauderis, Geraint A. Wiggins, Jose Oramas</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02667">https://arxiv.org/abs/2504.02667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02667">https://arxiv.org/pdf/2504.02667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02667]] Compositionality Unlocks Deep Interpretable Models(https://arxiv.org/abs/2504.02667)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We propose $\chi$-net, an intrinsically interpretable architecture combining the compositional multilinear structure of tensor networks with the expressivity and efficiency of deep neural networks. $\chi$-nets retain equal accuracy compared to their baseline counterparts. Our novel, efficient diagonalisation algorithm, ODT, reveals linear low-rank structure in a multilayer SVHN model. We leverage this toward formal weight-based interpretability and model compression.</li>
</ul>

<h3>Title: LLM for Complex Reasoning Task: An Exploratory Study in Fermi Problems</h3>
<ul>
<li><strong>Authors: </strong>Zishuo Liu, Carlos Rabat Villarreal, Mostafa Rahgouy, Amit Das, Zheng Zhang, Chang Ren, Dongji Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02671">https://arxiv.org/abs/2504.02671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02671">https://arxiv.org/pdf/2504.02671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02671]] LLM for Complex Reasoning Task: An Exploratory Study in Fermi Problems(https://arxiv.org/abs/2504.02671)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fermi Problems (FPs) are mathematical reasoning tasks that require human-like logic and numerical reasoning. Unlike other reasoning questions, FPs often involve real-world impracticalities or ambiguous concepts, making them challenging even for humans to solve. Despite advancements in AI, particularly with large language models (LLMs) in various reasoning tasks, FPs remain relatively under-explored. This work conducted an exploratory study to examine the capabilities and limitations of LLMs in solving FPs. We first evaluated the overall performance of three advanced LLMs using a publicly available FP dataset. We designed prompts according to the recently proposed TELeR taxonomy, including a zero-shot scenario. Results indicated that all three LLMs achieved a fp_score (range between 0 - 1) below 0.5, underscoring the inherent difficulty of these reasoning tasks. To further investigate, we categorized FPs into standard and specific questions, hypothesizing that LLMs would perform better on standard questions, which are characterized by clarity and conciseness, than on specific ones. Comparative experiments confirmed this hypothesis, demonstrating that LLMs performed better on standard FPs in terms of both accuracy and efficiency.</li>
</ul>

<h3>Title: Limitations of Religious Data and the Importance of the Target Domain: Towards Machine Translation for Guinea-Bissau Creole</h3>
<ul>
<li><strong>Authors: </strong>Jacqueline Rowe, Edward Gow-Smith, Mark Hepple</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02674">https://arxiv.org/abs/2504.02674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02674">https://arxiv.org/pdf/2504.02674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02674]] Limitations of Religious Data and the Importance of the Target Domain: Towards Machine Translation for Guinea-Bissau Creole(https://arxiv.org/abs/2504.02674)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce a new dataset for machine translation of Guinea-Bissau Creole (Kiriol), comprising around 40 thousand parallel sentences to English and Portuguese. This dataset is made up of predominantly religious data (from the Bible and texts from the Jehovah's Witnesses), but also a small amount of general domain data (from a dictionary). This mirrors the typical resource availability of many low resource languages. We train a number of transformer-based models to investigate how to improve domain transfer from religious data to a more general domain. We find that adding even 300 sentences from the target domain when training substantially improves the translation performance, highlighting the importance and need for data collection for low-resource languages, even on a small-scale. We additionally find that Portuguese-to-Kiriol translation models perform better on average than other source and target language pairs, and investigate how this relates to the morphological complexity of the languages involved and the degree of lexical overlap between creoles and lexifiers. Overall, we hope our work will stimulate research into Kiriol and into how machine translation might better support creole languages in general.</li>
</ul>

<h3>Title: STOOD-X methodology: using statistical nonparametric test for OOD Detection Large-Scale datasets enhanced with explainability</h3>
<ul>
<li><strong>Authors: </strong>Iván Sevillano-García, Julián Luengo, Francisco Herrera</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.HC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02685">https://arxiv.org/abs/2504.02685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02685">https://arxiv.org/pdf/2504.02685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02685]] STOOD-X methodology: using statistical nonparametric test for OOD Detection Large-Scale datasets enhanced with explainability(https://arxiv.org/abs/2504.02685)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Out-of-Distribution (OOD) detection is a critical task in machine learning, particularly in safety-sensitive applications where model failures can have serious consequences. However, current OOD detection methods often suffer from restrictive distributional assumptions, limited scalability, and a lack of interpretability. To address these challenges, we propose STOOD-X, a two-stage methodology that combines a Statistical nonparametric Test for OOD Detection with eXplainability enhancements. In the first stage, STOOD-X uses feature-space distances and a Wilcoxon-Mann-Whitney test to identify OOD samples without assuming a specific feature distribution. In the second stage, it generates user-friendly, concept-based visual explanations that reveal the features driving each decision, aligning with the BLUE XAI paradigm. Through extensive experiments on benchmark datasets and multiple architectures, STOOD-X achieves competitive performance against state-of-the-art post hoc OOD detectors, particularly in high-dimensional and complex settings. In addition, its explainability framework enables human oversight, bias detection, and model debugging, fostering trust and collaboration between humans and AI systems. The STOOD-X methodology therefore offers a robust, explainable, and scalable solution for real-world OOD detection tasks.</li>
</ul>

<h3>Title: GPTQv2: Efficient Finetuning-Free Quantization for Asymmetric Calibration</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Li, Ruokai Yin, Donghyun Lee, Shiting Xiao, Priyadarshini Panda</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02692">https://arxiv.org/abs/2504.02692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02692">https://arxiv.org/pdf/2504.02692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02692]] GPTQv2: Efficient Finetuning-Free Quantization for Asymmetric Calibration(https://arxiv.org/abs/2504.02692)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce GPTQv2, a novel finetuning-free quantization method for compressing large-scale transformer architectures. Unlike the previous GPTQ method, which independently calibrates each layer, we always match the quantized layer's output to the exact output in the full-precision model, resulting in a scheme that we call asymmetric calibration. Such a scheme can effectively reduce the quantization error accumulated in previous layers. We analyze this problem using optimal brain compression to derive a close-formed solution. The new solution explicitly minimizes the quantization error as well as the accumulated asymmetry error. Furthermore, we utilize various techniques to parallelize the solution calculation, including channel parallelization, neuron decomposition, and Cholesky reformulation for matrix fusion. As a result, GPTQv2 is easy to implement, simply using 20 more lines of code than GPTQ but improving its performance under low-bit quantization. Remarkably, on a single GPU, we quantize a 405B language transformer as well as EVA-02 the rank first vision transformer that achieves 90% pretraining Imagenet accuracy. Code is available at this http URL.</li>
</ul>

<h3>Title: SCMPPI: Supervised Contrastive Multimodal Framework for Predicting Protein-Protein Interactions</h3>
<ul>
<li><strong>Authors: </strong>Shengrui XU, Tianchi Lu, Zikun Wang, Jixiu Zhai, Jingwan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02698">https://arxiv.org/abs/2504.02698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02698">https://arxiv.org/pdf/2504.02698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02698]] SCMPPI: Supervised Contrastive Multimodal Framework for Predicting Protein-Protein Interactions(https://arxiv.org/abs/2504.02698)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Protein-Protein Interaction (PPI) prediction is a key task in uncovering cellular functional networks and disease mechanisms. However, traditional experimental methods are time-consuming and costly, and existing computational models face challenges in cross-modal feature fusion, robustness, and false-negative suppression. In this paper, we propose a novel supervised contrastive multimodal framework, SCMPPI, for PPI prediction. By integrating protein sequence features (AAC, DPC, CKSAAP-ESMC) with PPI network topology information (Node2Vec graph embedding), and combining an improved supervised contrastive learning strategy, SCMPPI significantly enhances PPI prediction performance. For the PPI task, SCMPPI introduces a negative sample filtering mechanism and modifies the contrastive loss function, effectively optimizing multimodal features. Experiments on eight benchmark datasets, including yeast, human, and this http URL, show that SCMPPI outperforms existing state-of-the-art methods (such as DF-PPI and TAGPPI) in key metrics such as accuracy ( 98.01%) and AUC (99.62%), and demonstrates strong generalization in cross-species prediction (AUC > 99% on multi-species datasets). Furthermore, SCMPPI has been successfully applied to CD9 networks, the Wnt pathway, and cancer-specific networks, providing a reliable tool for disease target discovery. This framework also offers a new paradigm for multimodal biological information fusion and contrastive learning in collaborative optimization for various combined predictions.</li>
</ul>

<h3>Title: The Hidden Space of Safety: Understanding Preference-Tuned LLMs in Multilingual context</h3>
<ul>
<li><strong>Authors: </strong>Nikhil Verma, Manasa Bharadwaj</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02708">https://arxiv.org/abs/2504.02708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02708">https://arxiv.org/pdf/2504.02708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02708]] The Hidden Space of Safety: Understanding Preference-Tuned LLMs in Multilingual context(https://arxiv.org/abs/2504.02708)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Alignment tuning has enabled large language models to excel in reasoning, instruction-following, and minimizing harmful generations. However, despite their widespread deployment, these models exhibit a monolingual bias, raising concerns about the effectiveness of alignment across languages. Current alignment methods predominantly focus on English, leaving it unclear how alignment mechanism generalize to multilingual settings. To address this, we conduct a systematic analysis of distributional shifts in the embedding space of LLMs before and after alignment, uncovering its impact on model behavior across diverse languages. We leverage the alignment-induced separation in safety space as a quantitative tool to measure how alignment enforces safety constraints. Our study evaluates seven LLMs using balanced toxicity datasets and parallel text-detoxification benchmarks, revealing substantial disparities in the latent representation space between high-resource and low-resource languages. These findings underscore the need for language-specific fine-tuning to ensure fair, reliable and robust multilingual alignment. Our insights provide a foundation for developing truly safe multilingual LLMs, emphasizing the urgency of addressing alignment gaps in underrepresented languages.</li>
</ul>

<h3>Title: ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Kehua Feng, Keyan Ding, Jing Yu, Menghan Li, Yuhao Wang, Tong Xu, Xinda Wang, Qiang Zhang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02725">https://arxiv.org/abs/2504.02725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02725">https://arxiv.org/pdf/2504.02725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02725]] ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization(https://arxiv.org/abs/2504.02725)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have accelerated progress toward artificial general intelligence, yet their potential to generate harmful content poses critical safety challenges. Existing alignment methods often struggle to cover diverse safety scenarios and remain vulnerable to adversarial attacks. In this work, we propose Ex-Ante Reasoning Preference Optimization (ERPO), a novel safety alignment framework that equips LLMs with explicit preemptive reasoning through Chain-of-Thought and provides clear evidence for safety judgments by embedding predefined safety rules. Specifically, our approach consists of three stages: first, equipping the model with Ex-Ante reasoning through supervised fine-tuning (SFT) using a constructed reasoning module; second, enhancing safety, usefulness, and efficiency via Direct Preference Optimization (DPO); and third, mitigating inference latency with a length-controlled iterative preference optimization strategy. Experiments on multiple open-source LLMs demonstrate that ERPO significantly enhances safety performance while maintaining response efficiency.</li>
</ul>

<h3>Title: HQViT: Hybrid Quantum Vision Transformer for Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Hui Zhang, Qinglin Zhao, Mengchu Zhou, Li Feng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02730">https://arxiv.org/abs/2504.02730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02730">https://arxiv.org/pdf/2504.02730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02730]] HQViT: Hybrid Quantum Vision Transformer for Image Classification(https://arxiv.org/abs/2504.02730)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based architectures have revolutionized the landscape of deep learning. In computer vision domain, Vision Transformer demonstrates remarkable performance on par with or even surpassing that of convolutional neural networks. However, the quadratic computational complexity of its self-attention mechanism poses challenges for classical computing, making model training with high-dimensional input data, e.g., images, particularly expensive. To address such limitations, we propose a Hybrid Quantum Vision Transformer (HQViT), that leverages the principles of quantum computing to accelerate model training while enhancing model performance. HQViT introduces whole-image processing with amplitude encoding to better preserve global image information without additional positional encoding. By leveraging quantum computation on the most critical steps and selectively handling other components in a classical way, we lower the cost of quantum resources for HQViT. The qubit requirement is minimized to $O(log_2N)$ and the number of parameterized quantum gates is only $O(log_2d)$, making it well-suited for Noisy Intermediate-Scale Quantum devices. By offloading the computationally intensive attention coefficient matrix calculation to the quantum framework, HQViT reduces the classical computational load by $O(T^2d)$. Extensive experiments across various computer vision datasets demonstrate that HQViT outperforms existing models, achieving a maximum improvement of up to $10.9\%$ (on the MNIST 10-classification task) over the state of the art. This work highlights the great potential to combine quantum and classical computing to cope with complex image classification tasks.</li>
</ul>

<h3>Title: Why do LLMs attend to the first token?</h3>
<ul>
<li><strong>Authors: </strong>Federico Barbero, Álvaro Arroyo, Xiangming Gu, Christos Perivolaropoulos, Michael Bronstein, Petar Veličkovi ć, Razvan Pascanu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02732">https://arxiv.org/abs/2504.02732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02732">https://arxiv.org/pdf/2504.02732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02732]] Why do LLMs attend to the first token?(https://arxiv.org/abs/2504.02732)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) tend to attend heavily to the first token in the sequence -- creating a so-called attention sink. Many works have studied this phenomenon in detail, proposing various ways to either leverage or alleviate it. Attention sinks have been connected to quantisation difficulties, security issues, and streaming attention. Yet, while many works have provided conditions in which they occur or not, a critical question remains shallowly answered: Why do LLMs learn such patterns and how are they being used? In this work, we argue theoretically and empirically that this mechanism provides a method for LLMs to avoid over-mixing, connecting this to existing lines of work that study mathematically how information propagates in Transformers. We conduct experiments to validate our theoretical intuitions and show how choices such as context length, depth, and data packing influence the sink behaviour. We hope that this study provides a new practical perspective on why attention sinks are useful in LLMs, leading to a better understanding of the attention patterns that form during training.</li>
</ul>

<h3>Title: Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study</h3>
<ul>
<li><strong>Authors: </strong>Aryan Agrawal, Lisa Alazraki, Shahin Honarvar, Marek Rei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02733">https://arxiv.org/abs/2504.02733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02733">https://arxiv.org/pdf/2504.02733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02733]] Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study(https://arxiv.org/abs/2504.02733)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are highly vulnerable to input perturbations, as even a small prompt change may result in a substantially different output. Existing methods to enhance LLM robustness are primarily focused on perturbed data samples, whereas improving resiliency to perturbations of task-level instructions has remained relatively underexplored. In this work, we focus on character- and word-level edits of task-specific instructions, which substantially degrade downstream performance. We experiment with a variety of techniques to enhance the robustness of LLMs, including self-denoising and representation alignment, testing different models (Llama 3 and Flan-T5), datasets (CoLA, QNLI, SST-2) and instructions (both task-oriented and role-oriented). We find that, on average, self-denoising -- whether performed by a frozen LLM or a fine-tuned model -- achieves substantially higher performance gains than alternative strategies, including more complex baselines such as ensembling and supervised methods.</li>
</ul>

<h3>Title: MD-ProjTex: Texturing 3D Shapes with Multi-Diffusion Projection</h3>
<ul>
<li><strong>Authors: </strong>Ahmet Burak Yildirim, Mustafa Utku Aydogdu, Duygu Ceylan, Aysegul Dundar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02762">https://arxiv.org/abs/2504.02762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02762">https://arxiv.org/pdf/2504.02762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02762]] MD-ProjTex: Texturing 3D Shapes with Multi-Diffusion Projection(https://arxiv.org/abs/2504.02762)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce MD-ProjTex, a method for fast and consistent text-guided texture generation for 3D shapes using pretrained text-to-image diffusion models. At the core of our approach is a multi-view consistency mechanism in UV space, which ensures coherent textures across different viewpoints. Specifically, MD-ProjTex fuses noise predictions from multiple views at each diffusion step and jointly updates the per-view denoising directions to maintain 3D consistency. In contrast to existing state-of-the-art methods that rely on optimization or sequential view synthesis, MD-ProjTex is computationally more efficient and achieves better quantitative and qualitative results.</li>
</ul>

<h3>Title: Scene Splatter: Momentum 3D Scene Generation from Single Image with Video Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Shengjun Zhang, Jinzhao Li, Xin Fei, Hao Liu, Yueqi Duan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02764">https://arxiv.org/abs/2504.02764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02764">https://arxiv.org/pdf/2504.02764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02764]] Scene Splatter: Momentum 3D Scene Generation from Single Image with Video Diffusion Model(https://arxiv.org/abs/2504.02764)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we propose Scene Splatter, a momentum-based paradigm for video diffusion to generate generic scenes from single image. Existing methods, which employ video generation models to synthesize novel views, suffer from limited video length and scene inconsistency, leading to artifacts and distortions during further reconstruction. To address this issue, we construct noisy samples from original features as momentum to enhance video details and maintain scene consistency. However, for latent features with the perception field that spans both known and unknown regions, such latent-level momentum restricts the generative ability of video diffusion in unknown regions. Therefore, we further introduce the aforementioned consistent video as a pixel-level momentum to a directly generated video without momentum for better recovery of unseen regions. Our cascaded momentum enables video diffusion models to generate both high-fidelity and consistent novel views. We further finetune the global Gaussian representations with enhanced frames and render new frames for momentum update in the next step. In this manner, we can iteratively recover a 3D scene, avoiding the limitation of video length. Extensive experiments demonstrate the generalization capability and superior performance of our method in high-fidelity and consistent scene generation.</li>
</ul>

<h3>Title: TailedCore: Few-Shot Sampling for Unsupervised Long-Tail Noisy Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Yoon Gyo Jung, Jaewoo Park, Jaeho Yoon, Kuan-Chuan Peng, Wonchul Kim, Andrew Beng Jin Teoh, Octavia Camps</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02775">https://arxiv.org/abs/2504.02775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02775">https://arxiv.org/pdf/2504.02775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02775]] TailedCore: Few-Shot Sampling for Unsupervised Long-Tail Noisy Anomaly Detection(https://arxiv.org/abs/2504.02775)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We aim to solve unsupervised anomaly detection in a practical challenging environment where the normal dataset is both contaminated with defective regions and its product class distribution is tailed but unknown. We observe that existing models suffer from tail-versus-noise trade-off where if a model is robust against pixel noise, then its performance deteriorates on tail class samples, and vice versa. To mitigate the issue, we handle the tail class and noise samples independently. To this end, we propose TailSampler, a novel class size predictor that estimates the class cardinality of samples based on a symmetric assumption on the class-wise distribution of embedding similarities. TailSampler can be utilized to sample the tail class samples exclusively, allowing to handle them separately. Based on these facets, we build a memory-based anomaly detection model TailedCore, whose memory both well captures tail class information and is noise-robust. We extensively validate the effectiveness of TailedCore on the unsupervised long-tail noisy anomaly detection setting, and show that TailedCore outperforms the state-of-the-art in most settings.</li>
</ul>

<h3>Title: Multi-Head Adaptive Graph Convolution Network for Sparse Point Cloud-Based Human Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Vincent Gbouna Zakka, Luis J. Manso, Zhuangzhuang Dai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02778">https://arxiv.org/abs/2504.02778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02778">https://arxiv.org/pdf/2504.02778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02778]] Multi-Head Adaptive Graph Convolution Network for Sparse Point Cloud-Based Human Activity Recognition(https://arxiv.org/abs/2504.02778)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Human activity recognition is increasingly vital for supporting independent living, particularly for the elderly and those in need of assistance. Domestic service robots with monitoring capabilities can enhance safety and provide essential support. Although image-based methods have advanced considerably in the past decade, their adoption remains limited by concerns over privacy and sensitivity to low-light or dark conditions. As an alternative, millimetre-wave (mmWave) radar can produce point cloud data which is privacy-preserving. However, processing the sparse and noisy point clouds remains a long-standing challenge. While graph-based methods and attention mechanisms show promise, they predominantly rely on "fixed" kernels; kernels that are applied uniformly across all neighbourhoods, highlighting the need for adaptive approaches that can dynamically adjust their kernels to the specific geometry of each local neighbourhood in point cloud data. To overcome this limitation, we introduce an adaptive approach within the graph convolutional framework. Instead of a single shared weight function, our Multi-Head Adaptive Kernel (MAK) module generates multiple dynamic kernels, each capturing different aspects of the local feature space. By progressively refining local features while maintaining global spatial context, our method enables convolution kernels to adapt to varying local features. Experimental results on benchmark datasets confirm the effectiveness of our approach, achieving state-of-the-art performance in human activity recognition. Our source code is made publicly available at: this https URL</li>
</ul>

<h3>Title: Towards Green AI-Native Networks: Evaluation of Neural Circuit Policy for Estimating Energy Consumption of Base Stations</h3>
<ul>
<li><strong>Authors: </strong>Selim Ickin, Shruti Bothe, Aman Raparia, Nitin Khanna, Erik Sanders</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02781">https://arxiv.org/abs/2504.02781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02781">https://arxiv.org/pdf/2504.02781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02781]] Towards Green AI-Native Networks: Evaluation of Neural Circuit Policy for Estimating Energy Consumption of Base Stations(https://arxiv.org/abs/2504.02781)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Optimization of radio hardware and AI-based network management software yield significant energy savings in radio access networks. The execution of underlying Machine Learning (ML) models, which enable energy savings through recommended actions, may require additional compute and energy, highlighting the opportunity to explore and adopt accurate and energy-efficient ML technologies. This work evaluates the novel use of sparsely structured Neural Circuit Policies (NCPs) in a use case to estimate the energy consumption of base stations. Sparsity in ML models yields reduced memory, computation and energy demand, hence facilitating a low-cost and scalable solution. We also evaluate the generalization capability of NCPs in comparison to traditional and widely used ML models such as Long Short Term Memory (LSTM), via quantifying their sensitivity to varying model hyper-parameters (HPs). NCPs demonstrated a clear reduction in computational overhead and energy consumption. Moreover, results indicated that the NCPs are robust to varying HPs such as number of epochs and neurons in each layer, making them a suitable option to ease model management and to reduce energy consumption in Machine Learning Operations (MLOps) in telecommunications.</li>
</ul>

<h3>Title: GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, Li Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02782">https://arxiv.org/abs/2504.02782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02782">https://arxiv.org/pdf/2504.02782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02782]] GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation(https://arxiv.org/abs/2504.02782)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at this https URL.</li>
</ul>

<h3>Title: A Framework for Robust Cognitive Evaluation of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Karin de Langis, Jong Inn Park, Bin Hu, Khanh Chi Le, Andreas Schramm, Michael C. Mensink, Andrew Elfenbein, Dongyeop Kang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02789">https://arxiv.org/abs/2504.02789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02789">https://arxiv.org/pdf/2504.02789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02789]] A Framework for Robust Cognitive Evaluation of LLMs(https://arxiv.org/abs/2504.02789)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Emergent cognitive abilities in large language models (LLMs) have been widely observed, but their nature and underlying mechanisms remain poorly understood. A growing body of research draws on cognitive science to investigate LLM cognition, but standard methodologies and experimen-tal pipelines have not yet been established. To address this gap we develop CognitivEval, a framework for systematically evaluating the artificial cognitive capabilities of LLMs, with a particular emphasis on robustness in response collection. The key features of CognitivEval include: (i) automatic prompt permutations, and (ii) testing that gathers both generations and model probability estimates. Our experiments demonstrate that these features lead to more robust experimental outcomes. Using CognitivEval, we replicate five classic experiments in cognitive science, illustrating the framework's generalizability across various experimental tasks and obtaining a cognitive profile of several state of the art LLMs. CognitivEval will be released publicly to foster broader collaboration within the cognitive science community.</li>
</ul>

<h3>Title: Spline-based Transformers</h3>
<ul>
<li><strong>Authors: </strong>Prashanth Chandran, Agon Serifi, Markus Gross, Moritz Bächer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02797">https://arxiv.org/abs/2504.02797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02797">https://arxiv.org/pdf/2504.02797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02797]] Spline-based Transformers(https://arxiv.org/abs/2504.02797)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce Spline-based Transformers, a novel class of Transformer models that eliminate the need for positional encoding. Inspired by workflows using splines in computer animation, our Spline-based Transformers embed an input sequence of elements as a smooth trajectory in latent space. Overcoming drawbacks of positional encoding such as sequence length extrapolation, Spline-based Transformers also provide a novel way for users to interact with transformer latent spaces by directly manipulating the latent control points to create new latent trajectories and sequences. We demonstrate the superior performance of our approach in comparison to conventional positional encoding on a variety of datasets, ranging from synthetic 2D to large-scale real-world datasets of images, 3D shapes, and animations.</li>
</ul>

<h3>Title: A Survey of Large Language Models in Mental Health Disorder Detection on Social Media</h3>
<ul>
<li><strong>Authors: </strong>Zhuohan Ge (1), Nicole Hu (2), Darian Li (1), Yubo Wang (3), Shihao Qi (1), Yuming Xu (1), Han Shi (3), Jason Zhang (1) ((1) The Hong Kong Polytechnic University, (2) The Chinese University of Hong Kong, (3) Hong Kong University of Science and Technology)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02800">https://arxiv.org/abs/2504.02800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02800">https://arxiv.org/pdf/2504.02800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02800]] A Survey of Large Language Models in Mental Health Disorder Detection on Social Media(https://arxiv.org/abs/2504.02800)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The detection and intervention of mental health issues represent a critical global research focus, and social media data has been recognized as an important resource for mental health research. However, how to utilize Large Language Models (LLMs) for mental health problem detection on social media poses significant challenges. Hence, this paper aims to explore the potential of LLM applications in social media data analysis, focusing not only on the most common psychological disorders such as depression and anxiety but also incorporating psychotic disorders and externalizing disorders, summarizing the application methods of LLM from different dimensions, such as text data analysis and detection of mental disorders, and revealing the major challenges and shortcomings of current research. In addition, the paper provides an overview of popular datasets, and evaluation metrics. The survey in this paper provides a comprehensive frame of reference for researchers in the field of mental health, while demonstrating the great potential of LLMs in mental health detection to facilitate the further application of LLMs in future mental health interventions.</li>
</ul>

<h3>Title: F-ViTA: Foundation Model Guided Visible to Thermal Translation</h3>
<ul>
<li><strong>Authors: </strong>Jay N. Paranjape, Celso de Melo, Vishal M. Patel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02801">https://arxiv.org/abs/2504.02801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02801">https://arxiv.org/pdf/2504.02801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02801]] F-ViTA: Foundation Model Guided Visible to Thermal Translation(https://arxiv.org/abs/2504.02801)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Thermal imaging is crucial for scene understanding, particularly in low-light and nighttime conditions. However, collecting large thermal datasets is costly and labor-intensive due to the specialized equipment required for infrared image capture. To address this challenge, researchers have explored visible-to-thermal image translation. Most existing methods rely on Generative Adversarial Networks (GANs) or Diffusion Models (DMs), treating the task as a style transfer problem. As a result, these approaches attempt to learn both the modality distribution shift and underlying physical principles from limited training data. In this paper, we propose F-ViTA, a novel approach that leverages the general world knowledge embedded in foundation models to guide the diffusion process for improved translation. Specifically, we condition an InstructPix2Pix Diffusion Model with zero-shot masks and labels from foundation models such as SAM and Grounded DINO. This allows the model to learn meaningful correlations between scene objects and their thermal signatures in infrared imagery. Extensive experiments on five public datasets demonstrate that F-ViTA outperforms state-of-the-art (SOTA) methods. Furthermore, our model generalizes well to out-of-distribution (OOD) scenarios and can generate Long-Wave Infrared (LWIR), Mid-Wave Infrared (MWIR), and Near-Infrared (NIR) translations from the same visible image. Code: this https URL.</li>
</ul>

<h3>Title: MegaMath: Pushing the Limits of Open Math Corpora</h3>
<ul>
<li><strong>Authors: </strong>Fan Zhou, Zengzhi Wang, Nikhil Ranjan, Zhoujun Cheng, Liping Tang, Guowei He, Zhengzhong Liu, Eric P. Xing</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02807">https://arxiv.org/abs/2504.02807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02807">https://arxiv.org/pdf/2504.02807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02807]] MegaMath: Pushing the Limits of Open Math Corpora(https://arxiv.org/abs/2504.02807)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present MegaMath, an open dataset curated from diverse, math-focused sources through following practices: (1) Revisiting web data: We re-extracted mathematical documents from Common Crawl with math-oriented HTML optimizations, fasttext-based filtering and deduplication, all for acquiring higher-quality data on the Internet. (2) Recalling Math-related code data: We identified high quality math-related code from large code training corpus, Stack-V2, further enhancing data diversity. (3) Exploring Synthetic data: We synthesized QA-style text, math-related code, and interleaved text-code blocks from web data or code data. By integrating these strategies and validating their effectiveness through extensive ablations, MegaMath delivers 371B tokens with the largest quantity and top quality among existing open math pre-training datasets.</li>
</ul>

<h3>Title: Generative Evaluation of Complex Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haowei Lin, Xiangyu Wang, Ruilin Yan, Baizhou Huang, Haotian Ye, Jianhua Zhu, Zihao Wang, James Zou, Jianzhu Ma, Yitao Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02810">https://arxiv.org/abs/2504.02810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02810">https://arxiv.org/pdf/2504.02810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02810]] Generative Evaluation of Complex Reasoning in Large Language Models(https://arxiv.org/abs/2504.02810)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.</li>
</ul>

<h3>Title: Efficient Autoregressive Shape Generation via Octree-Based Adaptive Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Kangle Deng, Hsueh-Ti Derek Liu, Yiheng Zhu, Xiaoxia Sun, Chong Shang, Kiran Bhat, Deva Ramanan, Jun-Yan Zhu, Maneesh Agrawala, Tinghui Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02817">https://arxiv.org/abs/2504.02817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02817">https://arxiv.org/pdf/2504.02817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02817]] Efficient Autoregressive Shape Generation via Octree-Based Adaptive Tokenization(https://arxiv.org/abs/2504.02817)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Many 3D generative models rely on variational autoencoders (VAEs) to learn compact shape representations. However, existing methods encode all shapes into a fixed-size token, disregarding the inherent variations in scale and complexity across 3D data. This leads to inefficient latent representations that can compromise downstream generation. We address this challenge by introducing Octree-based Adaptive Tokenization, a novel framework that adjusts the dimension of latent representations according to shape complexity. Our approach constructs an adaptive octree structure guided by a quadric-error-based subdivision criterion and allocates a shape latent vector to each octree cell using a query-based transformer. Building upon this tokenization, we develop an octree-based autoregressive generative model that effectively leverages these variable-sized representations in shape generation. Extensive experiments demonstrate that our approach reduces token counts by 50% compared to fixed-size methods while maintaining comparable visual quality. When using a similar token length, our method produces significantly higher-quality shapes. When incorporated with our downstream generative model, our method creates more detailed and diverse 3D content than existing approaches.</li>
</ul>

<h3>Title: GMR-Conv: An Efficient Rotation and Reflection Equivariant Convolution Kernel Using Gaussian Mixture Rings</h3>
<ul>
<li><strong>Authors: </strong>Yuexi Du, Jiazhen Zhang, Nicha C. Dvornek, John A. Onofrey</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02819">https://arxiv.org/abs/2504.02819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02819">https://arxiv.org/pdf/2504.02819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02819]] GMR-Conv: An Efficient Rotation and Reflection Equivariant Convolution Kernel Using Gaussian Mixture Rings(https://arxiv.org/abs/2504.02819)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Symmetry, where certain features remain invariant under geometric transformations, can often serve as a powerful prior in designing convolutional neural networks (CNNs). While conventional CNNs inherently support translational equivariance, extending this property to rotation and reflection has proven challenging, often forcing a compromise between equivariance, efficiency, and information loss. In this work, we introduce Gaussian Mixture Ring Convolution (GMR-Conv), an efficient convolution kernel that smooths radial symmetry using a mixture of Gaussian-weighted rings. This design mitigates discretization errors of circular kernels, thereby preserving robust rotation and reflection equivariance without incurring computational overhead. We further optimize both the space and speed efficiency of GMR-Conv via a novel parameterization and computation strategy, allowing larger kernels at an acceptable cost. Extensive experiments on eight classification and one segmentation datasets demonstrate that GMR-Conv not only matches conventional CNNs' performance but can also surpass it in applications with orientation-less data. GMR-Conv is also proven to be more robust and efficient than the state-of-the-art equivariant learning methods. Our work provides inspiring empirical evidence that carefully applied radial symmetry can alleviate the challenges of information loss, marking a promising advance in equivariant network architectures. The code is available at this https URL.</li>
</ul>

<h3>Title: Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot, Serge Belongie, Zeynep Akata</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02821">https://arxiv.org/abs/2504.02821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02821">https://arxiv.org/pdf/2504.02821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02821]] Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models(https://arxiv.org/abs/2504.02821)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity in vision representations. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons while also exhibiting hierarchical representations that align well with expert-defined structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that applying SAEs to intervene on a CLIP vision encoder, directly steer output from multimodal LLMs (e.g., LLaVA) without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised approach for enhancing both the interpretability and control of VLMs.</li>
</ul>

<h3>Title: STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection</h3>
<ul>
<li><strong>Authors: </strong>Divya Velayudhan, Abdelfatah Ahmed, Mohamad Alansari, Neha Gour, Abderaouf Behouch, Taimur Hassan, Syed Talal Wasim, Nabil Maalej, Muzammal Naseer, Juergen Gall, Mohammed Bennamoun, Ernesto Damiani, Naoufel Werghi</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02823">https://arxiv.org/abs/2504.02823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02823">https://arxiv.org/pdf/2504.02823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02823]] STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection(https://arxiv.org/abs/2504.02823)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Advancements in Computer-Aided Screening (CAS) systems are essential for improving the detection of security threats in X-ray baggage scans. However, current datasets are limited in representing real-world, sophisticated threats and concealment tactics, and existing approaches are constrained by a closed-set paradigm with predefined labels. To address these challenges, we introduce STCray, the first multimodal X-ray baggage security dataset, comprising 46,642 image-caption paired scans across 21 threat categories, generated using an X-ray scanner for airport security. STCray is meticulously developed with our specialized protocol that ensures domain-aware, coherent captions, that lead to the multi-modal instruction following data in X-ray baggage security. This allows us to train a domain-aware visual AI assistant named STING-BEE that supports a range of vision-language tasks, including scene comprehension, referring threat localization, visual grounding, and visual question answering (VQA), establishing novel baselines for multi-modal learning in X-ray baggage security. Further, STING-BEE shows state-of-the-art generalization in cross-domain settings. Code, data, and models are available at this https URL.</li>
</ul>

<h3>Title: On Vanishing Variance in Transformer Length Generalization</h3>
<ul>
<li><strong>Authors: </strong>Ruining Li, Gabrijel Boduljak, Jensen (Jinghao)Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02827">https://arxiv.org/abs/2504.02827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02827">https://arxiv.org/pdf/2504.02827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02827]] On Vanishing Variance in Transformer Length Generalization(https://arxiv.org/abs/2504.02827)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>It is a widely known issue that Transformers, when trained on shorter sequences, fail to generalize robustly to longer ones at test time. This raises the question of whether Transformer models are real reasoning engines, despite their impressive abilities in mathematical problem solving and code synthesis. In this paper, we offer a vanishing variance perspective on this issue. To the best of our knowledge, we are the first to demonstrate that even for today's frontier models, a longer sequence length results in a decrease in variance in the output of the multi-head attention modules. On the argmax retrieval and dictionary lookup tasks, our experiments show that applying layer normalization after the attention outputs leads to significantly better length generalization. Our analyses attribute this improvement to a reduction-though not a complete elimination-of the distribution shift caused by vanishing variance.</li>
</ul>

<h3>Title: Concept Lancet: Image Editing with Compositional Representation Transplant</h3>
<ul>
<li><strong>Authors: </strong>Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Hancheng Min, Chris Callison-Burch, René Vidal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02828">https://arxiv.org/abs/2504.02828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02828">https://arxiv.org/pdf/2504.02828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02828]] Concept Lancet: Image Editing with Compositional Representation Transplant(https://arxiv.org/abs/2504.02828)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models are widely used for image editing tasks. Existing editing methods often design a representation manipulation procedure by curating an edit direction in the text embedding or score space. However, such a procedure faces a key challenge: overestimating the edit strength harms visual consistency while underestimating it fails the editing task. Notably, each source image may require a different editing strength, and it is costly to search for an appropriate strength via trial-and-error. To address this challenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play framework for principled representation manipulation in diffusion-based image editing. At inference time, we decompose the source input in the latent (text embedding or diffusion score) space as a sparse linear combination of the representations of the collected visual concepts. This allows us to accurately estimate the presence of concepts in each image, which informs the edit. Based on the editing task (replace/add/remove), we perform a customized concept transplant process to impose the corresponding editing direction. To sufficiently model the concept space, we curate a conceptual representation dataset, CoLan-150K, which contains diverse descriptions and scenarios of visual terms and phrases for the latent dictionary. Experiments on multiple diffusion-based image editing baselines show that methods equipped with CoLan achieve state-of-the-art performance in editing effectiveness and consistency preservation.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
