<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-05-28</h1>
<h3>Title: FMEnets: Flow, Material, and Energy networks for non-ideal plug flow reactor design</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Wu, Juan Diego Toscano, Khemraj Shukla, Yingjie Chen, Ali Shahmohammadi, Edward Raymond, Thomas Toupy, Neda Nazemifard, Charles Papageorgiou, George Em Karniadakis</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20300">https://arxiv.org/abs/2505.20300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20300">https://arxiv.org/pdf/2505.20300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20300]] FMEnets: Flow, Material, and Energy networks for non-ideal plug flow reactor design(https://arxiv.org/abs/2505.20300)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose FMEnets, a physics-informed machine learning framework for the design and analysis of non-ideal plug flow reactors. FMEnets integrates the fundamental governing equations (Navier-Stokes for fluid flow, material balance for reactive species transport, and energy balance for temperature distribution) into a unified multi-scale network model. The framework is composed of three interconnected sub-networks with independent optimizers that enable both forward and inverse problem-solving. In the forward mode, FMEnets predicts velocity, pressure, species concentrations, and temperature profiles using only inlet and outlet information. In the inverse mode, FMEnets utilizes sparse multi-residence-time measurements to simultaneously infer unknown kinetic parameters and states. FMEnets can be implemented either as FME-PINNs, which employ conventional multilayer perceptrons, or as FME-KANs, based on Kolmogorov-Arnold Networks. Comprehensive ablation studies highlight the critical role of the FMEnets architecture in achieving accurate predictions. Specifically, FME-KANs are more robust to noise than FME-PINNs, although both representations are comparable in accuracy and speed in noise-free conditions. The proposed framework is applied to three different sets of reaction scenarios and is compared with finite element simulations. FMEnets effectively captures the complex interactions, achieving relative errors less than 2.5% for the unknown kinetic parameters. The new network framework not only provides a computationally efficient alternative for reactor design and optimization, but also opens new avenues for integrating empirical correlations, limited and noisy experimental data, and fundamental physical equations to guide reactor design.</li>
</ul>

<h3>Title: Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Amr Hegazy, Mostafa Elhoushi, Amr Alanwar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20309">https://arxiv.org/abs/2505.20309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20309">https://arxiv.org/pdf/2505.20309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20309]] Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs(https://arxiv.org/abs/2505.20309)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Controlling undesirable Large Language Model (LLM) behaviors, such as the generation of unsafe content or failing to adhere to safety guidelines, often relies on costly fine-tuning. Activation steering provides an alternative for inference-time control, but existing methods typically lack fine-grained, adaptive mechanisms. We introduce a novel approach using a lightweight, trainable controller network integrated during inference. This controller network observes specific intermediate LLM activations and predicts both a global scaling factor and layer-specific weights. The predicted global scaling factor and layer-specific weights then dynamically modulate the intensity of a steering patch, derived from a pre-computed "refusal direction" vector, applied across the LLM's layers during generation. Trained on activations from both harmful and benign prompts, our controller learns to discriminatively apply nuanced, layer-aware interventions, activating steering primarily for harmful inputs. Experiments using safety benchmarks like ToxicChat & In-The-Wild Jailbreak Prompts demonstrate that our weighted steering controller significantly increases refusal rates compared to the base LLM, achieving targeted behavioral modification without altering the original model parameters. Our experiments with Llama-3.1-8B, Llama-3.2-1B & Mistral-7B show our approach outperforms existing methods, presenting an efficient and adaptive method for fine-grained control over LLM behavior at inference time.</li>
</ul>

<h3>Title: Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL</h3>
<ul>
<li><strong>Authors: </strong>Zhewei Yao, Guoheng Sun, Lukasz Borchmann, Zheyu Shen, Minghang Deng, Bohan Zhai, Hao Zhang, Ang Li, Yuxiong He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20315">https://arxiv.org/abs/2505.20315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20315">https://arxiv.org/pdf/2505.20315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20315]] Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL(https://arxiv.org/abs/2505.20315)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Translating natural language into SQL (Test2SQL) is a longstanding challenge at the intersection of natural language understanding and structured data access. While large language models (LLMs) have significantly improved fluency in SQL generation, producing correct and executable SQL--particularly for complex queries--remains a bottleneck. We present Arctic-Text2SQL-R1, a reinforcement learning (RL) framework and model family designed to generate accurate, executable SQL using a lightweight reward signal based solely on execution correctness. Our approach avoids brittle intermediate supervision and complex reward shaping, promoting stable training and alignment with the end task. Combined with carefully curated data, strong supervised initialization, and effective training practices, Arctic-Text2SQL-R1 achieves state-of-the-art execution accuracy across six diverse Test2SQL benchmarks, including the top position on the BIRD leaderboard. Notably, our 7B model outperforms prior 70B-class systems, highlighting the framework's scalability and efficiency. We further demonstrate inference-time robustness through simple extensions like value retrieval and majority voting. Extensive experiments and ablation studies offer both positive and negative insights, providing practical guidance for future Test2SQL research.</li>
</ul>

<h3>Title: Beyond Demonstrations: Dynamic Vector Construction from Latent Representations</h3>
<ul>
<li><strong>Authors: </strong>Wang Cai, Hsiu-Yuan Huang, Zhixiang Wang, Yunfang Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20318">https://arxiv.org/abs/2505.20318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20318">https://arxiv.org/pdf/2505.20318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20318]] Beyond Demonstrations: Dynamic Vector Construction from Latent Representations(https://arxiv.org/abs/2505.20318)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>In-Context derived Vector (ICV) methods extract task-relevant representations from large language models (LLMs) and reinject them during inference, achieving comparable performance to few-shot In-Context Learning (ICL) without repeated demonstration processing. However, existing ICV methods remain sensitive to ICL-specific factors, often use coarse or semantically fragmented representations as the source of the vector, and rely on heuristic-based injection positions, limiting their applicability. To address these issues, we propose Dynamic Vector (DyVec), which incorporates an Exhaustive Query Rotation (EQR) strategy to extract robust semantically aggregated latent representations by mitigating variance introduced by ICL. It then applies Dynamic Latent Segmentation and Injection to adaptively partition representations based on task complexity and leverages REINFORCE-based optimization to learn optimal injection positions for each segment. Experiments results show that DyVec outperforms few-shot ICL, LoRA, and prior ICV baselines. Further analysis highlights the effectiveness of dynamically segmenting and injecting semantically aggregated latent representations. DyVec provides a lightweight and data-efficient solution for inference-time task adaptation.</li>
</ul>

<h3>Title: Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP</h3>
<ul>
<li><strong>Authors: </strong>Satya Narayana Cheetirala, Ganesh Raut, Dhavalkumar Patel, Fabio Sanatana, Robert Freeman, Matthew A Levin, Girish N. Nadkarni, Omar Dawkins, Reba Miller, Randolph M. Steinhagen, Eyal Klang, Prem Timsina</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20320">https://arxiv.org/abs/2505.20320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20320">https://arxiv.org/pdf/2505.20320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20320]] Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP(https://arxiv.org/abs/2505.20320)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long text classification is challenging for Large Language Models (LLMs) due to token limits and high computational costs. This study explores whether a Retrieval Augmented Generation (RAG) approach using only the most relevant text segments can match the performance of processing entire clinical notes with large context LLMs. We begin by splitting clinical documents into smaller chunks, converting them into vector embeddings, and storing these in a FAISS index. We then retrieve the top 4,000 words most pertinent to the classification query and feed these consolidated segments into an LLM. We evaluated three LLMs (GPT4o, LLaMA, and Mistral) on a surgical complication identification task. Metrics such as AUC ROC, precision, recall, and F1 showed no statistically significant differences between the RAG based approach and whole-text processing (p > 0.05p > 0.05). These findings indicate that RAG can significantly reduce token usage without sacrificing classification accuracy, providing a scalable and cost effective solution for analyzing lengthy clinical documents.</li>
</ul>

<h3>Title: BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases</h3>
<ul>
<li><strong>Authors: </strong>Mathew J. Koretsky, Maya Willey, Adi Asija, Owen Bianchi, Chelsea X. Alvarado, Tanay Nayak, Nicole Kuznetsov, Sungwon Kim, Mike A. Nalls, Daniel Khashabi, Faraz Faghri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20321">https://arxiv.org/abs/2505.20321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20321">https://arxiv.org/pdf/2505.20321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20321]] BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases(https://arxiv.org/abs/2505.20321)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in a harmonized BigQuery knowledge base that integrates gene-disease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate a range of open- and closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides a new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases. Our dataset is publicly available at this https URL, and our code is open-source at this https URL.</li>
</ul>

<h3>Title: Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms</h3>
<ul>
<li><strong>Authors: </strong>Mengru Wang, Ziwen Xu, Shengyu Mao, Shumin Deng, Zhaopeng Tu, Huajun Chen, Ningyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20322">https://arxiv.org/abs/2505.20322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20322">https://arxiv.org/pdf/2505.20322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20322]] Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms(https://arxiv.org/abs/2505.20322)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.</li>
</ul>

<h3>Title: PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus</h3>
<ul>
<li><strong>Authors: </strong>Shahriar Noroozizadeh, Sayantan Kumar, George H. Chen, Jeremy C. Weiss</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20323">https://arxiv.org/abs/2505.20323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20323">https://arxiv.org/pdf/2505.20323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20323]] PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus(https://arxiv.org/abs/2505.20323)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Understanding temporal dynamics in clinical narratives is essential for modeling patient trajectories, yet large-scale temporally annotated resources remain limited. We present PMOA-TTS, the first openly available dataset of 124,699 PubMed Open Access (PMOA) case reports, each converted into structured (event, time) timelines via a scalable LLM-based pipeline. Our approach combines heuristic filtering with Llama 3.3 to identify single-patient case reports, followed by prompt-driven extraction using Llama 3.3 and DeepSeek R1, resulting in over 5.6 million timestamped clinical events. To assess timeline quality, we evaluate against a clinician-curated reference set using three metrics: (i) event-level matching (80% match at a cosine similarity threshold of 0.1), (ii) temporal concordance (c-index > 0.90), and (iii) Area Under the Log-Time CDF (AULTC) for timestamp alignment. Corpus-level analysis shows wide diagnostic and demographic coverage. In a downstream survival prediction task, embeddings from extracted timelines achieve time-dependent concordance indices up to 0.82 $\pm$ 0.01, demonstrating the predictive value of temporally structured narratives. PMOA-TTS provides a scalable foundation for timeline extraction, temporal reasoning, and longitudinal modeling in biomedical NLP. The dataset is available at: this https URL .</li>
</ul>

<h3>Title: Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence</h3>
<ul>
<li><strong>Authors: </strong>Amirhosein Ghasemabadi, Keith G. Mills, Baochun Li, Di Niu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20325">https://arxiv.org/abs/2505.20325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20325">https://arxiv.org/pdf/2505.20325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20325]] Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence(https://arxiv.org/abs/2505.20325)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM) reasoning often incur substantial computational costs, primarily due to extensive reliance on external Process Reward Models (PRMs) or sampling methods like Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient self-guided TTS framework that achieves PRM-level performance without costly external verifier models. Our method employs a lightweight tree search guided solely by intrinsic LLM signals, token-level confidence and step novelty. One critical innovation is improving the reliability of internal confidence estimates via a targeted reinforcement learning fine-tuning phase. Empirical evaluations on challenging mathematical reasoning benchmarks demonstrate that GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching or surpassing significantly larger models (e.g., 32B-70B parameters), while reducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG achieves comparable accuracy with 8x faster inference speeds and 4-5x lower memory usage. Additionally, GG reduces KV cache memory usage by approximately 50% compared to the BoN strategy, facilitating more efficient and practical deployment of TTS techniques.</li>
</ul>

<h3>Title: Joint-stochastic-approximation Random Fields with Application to Semi-supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Yunfu Song, Zhijian Ou</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20330">https://arxiv.org/abs/2505.20330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20330">https://arxiv.org/pdf/2505.20330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20330]] Joint-stochastic-approximation Random Fields with Application to Semi-supervised Learning(https://arxiv.org/abs/2505.20330)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Our examination of deep generative models (DGMs) developed for semi-supervised learning (SSL), mainly GANs and VAEs, reveals two problems. First, mode missing and mode covering phenomenons are observed in genertion with GANs and VAEs. Second, there exists an awkward conflict between good classification and good generation in SSL by employing directed generative models. To address these problems, we formally present joint-stochastic-approximation random fields (JRFs) -- a new family of algorithms for building deep undirected generative models, with application to SSL. It is found through synthetic experiments that JRFs work well in balancing mode covering and mode missing, and match the empirical data distribution well. Empirically, JRFs achieve good classification results comparable to the state-of-art methods on widely adopted datasets -- MNIST, SVHN, and CIFAR-10 in SSL, and simultaneously perform good generation.</li>
</ul>

<h3>Title: Multi-Scale Manifold Alignment: A Unified Framework for Enhanced Explainability of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yukun Zhang, Qi Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20333">https://arxiv.org/abs/2505.20333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20333">https://arxiv.org/pdf/2505.20333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20333]] Multi-Scale Manifold Alignment: A Unified Framework for Enhanced Explainability of Large Language Models(https://arxiv.org/abs/2505.20333)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have achieved strong performance, yet their internal reasoning remains opaque, limiting interpretability and trust in critical applications. We propose a novel Multi_Scale Manifold Alignment framework that decomposes the latent space into global, intermediate, and local semantic manifolds capturing themes, context, and word-level details. Our method introduces cross_scale mapping functions that jointly enforce geometric alignment (e.g., Procrustes analysis) and information preservation (via mutual information constraints like MINE or VIB). We further incorporate curvature regularization and hyperparameter tuning for stable optimization. Theoretical analysis shows that alignment error, measured by KL divergence, can be bounded under mild assumptions. This framework offers a unified explanation of how LLMs structure multi-scale semantics, advancing interpretability and enabling applications such as bias detection and robustness enhancement.</li>
</ul>

<h3>Title: Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Wang, Shiyu Ji, Yijun Liu, Yuzhuang Xu, Yang Xu, Qingfu Zhu, Wanxiang Che</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20334">https://arxiv.org/abs/2505.20334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20334">https://arxiv.org/pdf/2505.20334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20334]] Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query(https://arxiv.org/abs/2505.20334)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) rely on key-value cache (KV cache) to accelerate decoding by reducing redundant computations. However, the KV cache memory usage grows substantially with longer text sequences, posing challenges for efficient deployment. Existing KV cache eviction methods prune tokens using prefilling-stage attention scores, causing inconsistency with actual inference queries, especially under tight memory budgets. In this paper, we propose Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries. By using these lookahead queries as the observation window for importance estimation, LAQ achieves more consistent and accurate KV cache eviction aligned with real inference scenarios. Experimental results on LongBench and Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\sim$ 4 point improvement on LongBench under limited cache budget. Moreover, LAQ is complementary to existing approaches and can be flexibly combined to yield further improvements.</li>
</ul>

<h3>Title: Language Model Distillation: A Temporal Difference Imitation Learning Perspective</h3>
<ul>
<li><strong>Authors: </strong>Zishun Yu, Shangzhe Li, Xinhua Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20335">https://arxiv.org/abs/2505.20335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20335">https://arxiv.org/pdf/2505.20335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20335]] Language Model Distillation: A Temporal Difference Imitation Learning Perspective(https://arxiv.org/abs/2505.20335)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have led to significant progress across many NLP tasks, although their massive sizes often incur substantial computational costs. Distillation has become a common practice to compress these large and highly capable models into smaller, more efficient ones. Many existing language model distillation methods can be viewed as behavior cloning from the perspective of imitation learning or inverse reinforcement learning. This viewpoint has inspired subsequent studies that leverage (inverse) reinforcement learning techniques, including variations of behavior cloning and temporal difference learning methods. Rather than proposing yet another specific temporal difference method, we introduce a general framework for temporal difference-based distillation by exploiting the distributional sparsity of the teacher model. Specifically, it is often observed that language models assign most probability mass to a small subset of tokens. Motivated by this observation, we design a temporal difference learning framework that operates on a reduced action space (a subset of vocabulary), and demonstrate how practical algorithms can be derived and the resulting performance improvements.</li>
</ul>

<h3>Title: MOSLIM:Align with diverse preferences in prompts through reward classification</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhang, Wanli Jiang, Zhengyu Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20336">https://arxiv.org/abs/2505.20336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20336">https://arxiv.org/pdf/2505.20336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20336]] MOSLIM:Align with diverse preferences in prompts through reward classification(https://arxiv.org/abs/2505.20336)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The multi-objective alignment of Large Language Models (LLMs) is essential for ensuring foundational models conform to diverse human preferences. Current research in this field typically involves either multiple policies or multiple reward models customized for various preferences, or the need to train a preference-specific supervised fine-tuning (SFT) model. In this work, we introduce a novel multi-objective alignment method, MOSLIM, which utilizes a single reward model and policy model to address diverse objectives. MOSLIM provides a flexible way to control these objectives through prompting and does not require preference training during SFT phase, allowing thousands of off-the-shelf models to be directly utilized within this training framework. MOSLIM leverages a multi-head reward model that classifies question-answer pairs instead of scoring them and then optimize policy model with a scalar reward derived from a mapping function that converts classification results from reward model into reward scores. We demonstrate the efficacy of our proposed method across several multi-objective benchmarks and conduct ablation studies on various reward model sizes and policy optimization methods. The MOSLIM method outperforms current multi-objective approaches in most results while requiring significantly fewer GPU computing resources compared with existing policy optimization methods.</li>
</ul>

<h3>Title: Assessing the Capability of LLMs in Solving POSCOMP Questions</h3>
<ul>
<li><strong>Authors: </strong>Cayo Viegas, Rohit Gheyi, MÃ¡rcio Ribeiro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20338">https://arxiv.org/abs/2505.20338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20338">https://arxiv.org/pdf/2505.20338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20338]] Assessing the Capability of LLMs in Solving POSCOMP Questions(https://arxiv.org/abs/2505.20338)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have significantly expanded the capabilities of artificial intelligence in natural language processing tasks. Despite this progress, their performance in specialized domains such as computer science remains relatively unexplored. Understanding the proficiency of LLMs in these domains is critical for evaluating their practical utility and guiding future developments. The POSCOMP, a prestigious Brazilian examination used for graduate admissions in computer science promoted by the Brazlian Computer Society (SBC), provides a challenging benchmark. This study investigates whether LLMs can match or surpass human performance on the POSCOMP exam. Four LLMs - ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet, and Le Chat Mistral Large - were initially evaluated on the 2022 and 2023 POSCOMP exams. The assessments measured the models' proficiency in handling complex questions typical of the exam. LLM performance was notably better on text-based questions than on image interpretation tasks. In the 2022 exam, ChatGPT-4 led with 57 correct answers out of 69 questions, followed by Gemini 1.0 Advanced (49), Le Chat Mistral (48), and Claude 3 Sonnet (44). Similar trends were observed in the 2023 exam. ChatGPT-4 achieved the highest performance, surpassing all students who took the POSCOMP 2023 exam. LLMs, particularly ChatGPT-4, show promise in text-based tasks on the POSCOMP exam, although image interpretation remains a challenge. Given the rapid evolution of LLMs, we expanded our analysis to include more recent models - o1, Gemini 2.5 Pro, Claude 3.7 Sonnet, and o3-mini-high - evaluated on the 2022-2024 POSCOMP exams. These newer models demonstrate further improvements and consistently surpass both the average and top-performing human participants across all three years.</li>
</ul>

<h3>Title: Dynamic Manifold Evolution Theory: Modeling and Stability Analysis of Latent Representations in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yukun Zhang, Qi Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20340">https://arxiv.org/abs/2505.20340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20340">https://arxiv.org/pdf/2505.20340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20340]] Dynamic Manifold Evolution Theory: Modeling and Stability Analysis of Latent Representations in Large Language Models(https://arxiv.org/abs/2505.20340)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>We introduce Dynamic Manifold Evolution Theory (DMET),a unified framework that models large language model generation as a controlled dynamical system evolving on a low_dimensional semantic manifold. By casting latent_state updates as discrete time Euler approximations of continuous dynamics, we map intrinsic energy_driven flows and context_dependent forces onto Transformer components (residual connections, attention, feed-forward networks). Leveraging Lyapunov stability theory We define three empirical metrics (state continuity, clustering quality, topological persistence) that quantitatively link latent_trajectory properties to text fluency, grammaticality, and semantic coherence. Extensive experiments across decoding parameters validate DMET's predictions and yield principled guidelines for balancing creativity and consistency in text generation.</li>
</ul>

<h3>Title: PDFBench: A Benchmark for De novo Protein Design from Function</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Kuang, Nuowei Liu, Changzhi Sun, Tao Ji, Yuanbin Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20346">https://arxiv.org/abs/2505.20346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20346">https://arxiv.org/pdf/2505.20346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20346]] PDFBench: A Benchmark for De novo Protein Design from Function(https://arxiv.org/abs/2505.20346)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>In recent years, while natural language processing and multimodal learning have seen rapid advancements, the field of de novo protein design has also experienced significant growth. However, most current methods rely on proprietary datasets and evaluation rubrics, making fair comparisons between different approaches challenging. Moreover, these methods often employ evaluation metrics that capture only a subset of the desired properties of designed proteins, lacking a comprehensive assessment framework. To address these, we introduce PDFBench, the first comprehensive benchmark for evaluating de novo protein design from function. PDFBench supports two tasks: description-guided design and keyword-guided design. To ensure fair and multifaceted evaluation, we compile 22 metrics covering sequence plausibility, structural fidelity, and language-protein alignment, along with measures of novelty and diversity. We evaluate five state-of-the-art baselines, revealing their respective strengths and weaknesses across tasks. Finally, we analyze inter-metric correlations, exploring the relationships between four categories of metrics, and offering guidelines for metric selection. PDFBench establishes a unified framework to drive future advances in function-driven de novo protein design.</li>
</ul>

<h3>Title: SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data</h3>
<ul>
<li><strong>Authors: </strong>Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, Kaixuan Chen, Mingli Song, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20347">https://arxiv.org/abs/2505.20347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20347">https://arxiv.org/pdf/2505.20347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20347]] SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data(https://arxiv.org/abs/2505.20347)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances have demonstrated the effectiveness of Reinforcement Learning (RL) in improving the reasoning capabilities of Large Language Models (LLMs). However, existing works inevitably rely on high-quality instructions and verifiable rewards for effective training, both of which are often difficult to obtain in specialized domains. In this paper, we propose Self-play Reinforcement Learning(SeRL) to bootstrap LLM training with limited initial data. Specifically, SeRL comprises two complementary modules: self-instruction and self-rewarding. The former module generates additional instructions based on the available data at each training step, employing robust online filtering strategies to ensure instruction quality, diversity, and difficulty. The latter module introduces a simple yet effective majority-voting mechanism to estimate response rewards for additional instructions, eliminating the need for external annotations. Finally, SeRL performs conventional RL based on the generated data, facilitating iterative self-play learning. Extensive experiments on various reasoning benchmarks and across different LLM backbones demonstrate that the proposed SeRL yields results superior to its counterparts and achieves performance on par with those obtained by high-quality data with verifiable rewards. Our code is available at this https URL.</li>
</ul>

<h3>Title: Decision Flow Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Jifeng Hu, Sili Huang, Siyuan Guo, Zhaogeng Liu, Li Shen, Lichao Sun, Hechang Chen, Yi Chang, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20350">https://arxiv.org/abs/2505.20350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20350">https://arxiv.org/pdf/2505.20350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20350]] Decision Flow Policy Optimization(https://arxiv.org/abs/2505.20350)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In recent years, generative models have shown remarkable capabilities across diverse fields, including images, videos, language, and decision-making. By applying powerful generative models such as flow-based models to reinforcement learning, we can effectively model complex multi-modal action distributions and achieve superior robotic control in continuous action spaces, surpassing the limitations of single-modal action distributions with traditional Gaussian-based policies. Previous methods usually adopt the generative models as behavior models to fit state-conditioned action distributions from datasets, with policy optimization conducted separately through additional policies using value-based sample weighting or gradient-based updates. However, this separation prevents the simultaneous optimization of multi-modal distribution fitting and policy improvement, ultimately hindering the training of models and degrading the performance. To address this issue, we propose Decision Flow, a unified framework that integrates multi-modal action distribution modeling and policy optimization. Specifically, our method formulates the action generation procedure of flow-based models as a flow decision-making process, where each action generation step corresponds to one flow decision. Consequently, our method seamlessly optimizes the flow policy while capturing multi-modal action distributions. We provide rigorous proofs of Decision Flow and validate the effectiveness through extensive experiments across dozens of offline RL environments. Compared with established offline RL baselines, the results demonstrate that our method achieves or matches the SOTA performance.</li>
</ul>

<h3>Title: FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation</h3>
<ul>
<li><strong>Authors: </strong>Dong Liu, Jiayi Zhang, Yifan Li, Yanxuan Yu, Ben Lengerich, Ying Nian Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.MM, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20353">https://arxiv.org/abs/2505.20353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20353">https://arxiv.org/pdf/2505.20353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20353]] FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation(https://arxiv.org/abs/2505.20353)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiT) are powerful generative models but remain computationally intensive due to their iterative structure and deep transformer stacks. To alleviate this inefficiency, we propose FastCache, a hidden-state-level caching and compression framework that accelerates DiT inference by exploiting redundancy within the model's internal representations. FastCache introduces a dual strategy: (1) a spatial-aware token selection mechanism that adaptively filters redundant tokens based on hidden state saliency, and (2) a transformer-level cache that reuses latent activations across timesteps when changes are statistically insignificant. These modules work jointly to reduce unnecessary computation while preserving generation fidelity through learnable linear approximation. Theoretical analysis shows that FastCache maintains bounded approximation error under a hypothesis-testing-based decision rule. Empirical evaluations across multiple DiT variants demonstrate substantial reductions in latency and memory usage, with best generation output quality compared to other cache methods, as measured by FID and t-FID. Code implementation of FastCache is available on GitHub at this https URL.</li>
</ul>

<h3>Title: Rethinking Text-based Protein Understanding: Retrieval or LLM?</h3>
<ul>
<li><strong>Authors: </strong>Juntong Wu, Zijing Liu, He Cao, Hao Li, Bin Feng, Zishan Shu, Ke Yu, Li Yuan, Yu Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20354">https://arxiv.org/abs/2505.20354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20354">https://arxiv.org/pdf/2505.20354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20354]] Rethinking Text-based Protein Understanding: Retrieval or LLM?(https://arxiv.org/abs/2505.20354)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, protein-text models have gained significant attention for their potential in protein generation and understanding. Current approaches focus on integrating protein-related knowledge into large language models through continued pretraining and multi-modal alignment, enabling simultaneous comprehension of textual descriptions and protein sequences. Through a thorough analysis of existing model architectures and text-based protein understanding benchmarks, we identify significant data leakage issues present in current benchmarks. Moreover, conventional metrics derived from natural language processing fail to accurately assess the model's performance in this domain. To address these limitations, we reorganize existing datasets and introduce a novel evaluation framework based on biological entities. Motivated by our observation, we propose a retrieval-enhanced method, which significantly outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy and efficiency in training-free scenarios. Our code and data can be seen at this https URL.</li>
</ul>

<h3>Title: GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yeonjoon Jung, Daehyun Ahn, Hyungjun Kim, Taesu Kim, Eunhyeok Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20355">https://arxiv.org/abs/2505.20355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20355">https://arxiv.org/pdf/2505.20355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20355]] GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning(https://arxiv.org/abs/2505.20355)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA's structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA's limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation and commonsense reasoning benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts are available at this https URL</li>
</ul>

<h3>Title: Learning and Interpreting Gravitational-Wave Features from CNNs with a Random Forest Approach</h3>
<ul>
<li><strong>Authors: </strong>Jun Tian, He Wang, Jibo He, Yu Pan, Shuo Cao, Qingquan Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, gr-qc, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20357">https://arxiv.org/abs/2505.20357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20357">https://arxiv.org/pdf/2505.20357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20357]] Learning and Interpreting Gravitational-Wave Features from CNNs with a Random Forest Approach(https://arxiv.org/abs/2505.20357)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Convolutional neural networks (CNNs) have become widely adopted in gravitational wave (GW) detection pipelines due to their ability to automatically learn hierarchical features from raw strain data. However, the physical meaning of these learned features remains underexplored, limiting the interpretability of such models. In this work, we propose a hybrid architecture that combines a CNN-based feature extractor with a random forest (RF) classifier to improve both detection performance and interpretability. Unlike prior approaches that directly connect classifiers to CNN outputs, our method introduces four physically interpretable metrics - variance, signal-to-noise ratio (SNR), waveform overlap, and peak amplitude - computed from the final convolutional layer. These are jointly used with the CNN output in the RF classifier to enable more informed decision boundaries. Tested on long-duration strain datasets, our hybrid model outperforms a baseline CNN model, achieving a relative improvement of 21\% in sensitivity at a fixed false alarm rate of 10 events per month. Notably, it also shows improved detection of low-SNR signals (SNR $\le$ 10), which are especially vulnerable to misclassification in noisy environments. Feature attribution via the RF model reveals that both CNN-extracted and handcrafted features contribute significantly to classification decisions, with learned variance and CNN outputs ranked among the most informative. These findings suggest that physically motivated post-processing of CNN feature maps can serve as a valuable tool for interpretable and efficient GW detection, bridging the gap between deep learning and domain knowledge.</li>
</ul>

<h3>Title: Risk-aware Direct Preference Optimization under Nested Risk Measure</h3>
<ul>
<li><strong>Authors: </strong>Lijun Zhang, Lin Li, Yajie Qi, Huizhong Song, Yaodong Yang, Jun Wang, Wei Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20359">https://arxiv.org/abs/2505.20359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20359">https://arxiv.org/pdf/2505.20359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20359]] Risk-aware Direct Preference Optimization under Nested Risk Measure(https://arxiv.org/abs/2505.20359)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>When fine-tuning pre-trained Large Language Models (LLMs) to align with human values and intentions, maximizing the estimated reward can lead to superior performance, but it also introduces potential risks due to deviations from the reference model's intended behavior. Most existing methods typically introduce KL divergence to constrain deviations between the trained model and the reference model; however, this may not be sufficient in certain applications that require tight risk control. In this paper, we introduce Risk-aware Direct Preference Optimization (Ra-DPO), a novel approach that incorporates risk-awareness by employing a class of nested risk measures. This approach formulates a constrained risk-aware advantage function maximization problem and then converts the Bradley-Terry model into a token-level representation. The objective function maximizes the likelihood of the policy while suppressing the deviation between a trained model and the reference model using a sequential risk ratio, thereby enhancing the model's risk-awareness. Experimental results across three open-source datasets: IMDb Dataset, Anthropic HH Dataset, and AlpacaEval, demonstrate the proposed method's superior performance in balancing alignment performance and model drift. Our code is opensourced at this https URL.</li>
</ul>

<h3>Title: GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Simin Fan, Maria Ios Glarou, Martin Jaggi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20380">https://arxiv.org/abs/2505.20380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20380">https://arxiv.org/pdf/2505.20380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20380]] GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining(https://arxiv.org/abs/2505.20380)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The performance of large language models (LLMs) across diverse downstream applications is fundamentally governed by the quality and composition of their pretraining corpora. Existing domain reweighting algorithms primarily optimize data mixtures for a single target task, thereby resulting in models that overfit to specialized objectives while exhibiting substantial performance degradation on other benchmarks. This paper introduces Group Robust Multi-target Adaptive PrEtraining (GRAPE), a novel multi-source-multi-target domain reweighting framework designed to calibrate pretraining data mixtures for robust performance across multiple target tasks simultaneously. GRAPE dynamically adjusts sampling weights across source domains (domain weights) while concurrently modulating task weights that quantify the relative importance of each individual target task. This adaptive process prioritizes tasks based on their learning difficulty throughout training. We formulate this interleaved reweighting mechanism as a minimax optimization problem: The inner maximization adjusts task weights leveraging group distributed-robust-optimization (DRO), where those tasks demonstrating the least improvement under the current data mixture are prioritized with higher weights; The outer minimization then optimizes domain weights to maximize loss reduction on the prioritized tasks. Experiments on ClimbLab and SlimPajama datasets demonstrate that GRAPE consistently outperforms baseline methods in terms of reasoning performance across 6 benchmarks. Furthermore, when applied to multilingual targets, GRAPE effectively identifies optimal training mixtures from mainstream languages, achieving superior language modeling capabilities across 8 low-resource target languages.</li>
</ul>

<h3>Title: What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lorenzo Baraldi, Davide Bucciarelli, Federico Betti, Marcella Cornia, Lorenzo Baraldi, Nicu Sebe, Rita Cucchiara</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20405">https://arxiv.org/abs/2505.20405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20405">https://arxiv.org/pdf/2505.20405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20405]] What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models(https://arxiv.org/abs/2505.20405)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Instruction-based image editing models offer increased personalization opportunities in generative tasks. However, properly evaluating their results is challenging, and most of the existing metrics lag in terms of alignment with human judgment and explainability. To tackle these issues, we introduce DICE (DIfference Coherence Estimator), a model designed to detect localized differences between the original and the edited image and to assess their relevance to the given modification request. DICE consists of two key components: a difference detector and a coherence estimator, both built on an autoregressive Multimodal Large Language Model (MLLM) and trained using a strategy that leverages self-supervision, distillation from inpainting networks, and full supervision. Through extensive experiments, we evaluate each stage of our pipeline, comparing different MLLMs within the proposed framework. We demonstrate that DICE effectively identifies coherent edits, effectively evaluating images generated by different editing models with a strong correlation with human judgment. We publicly release our source code, models, and data.</li>
</ul>

<h3>Title: RetroMotion: Retrocausal Motion Forecasting Models are Instructable</h3>
<ul>
<li><strong>Authors: </strong>Royden Wagner, Omer Sahin Tas, Felix Hauser, Marlon Steiner, Dominik Strutz, Abhishek Vivekanandan, Carlos Fernandez, Christoph Stiller</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20414">https://arxiv.org/abs/2505.20414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20414">https://arxiv.org/pdf/2505.20414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20414]] RetroMotion: Retrocausal Motion Forecasting Models are Instructable(https://arxiv.org/abs/2505.20414)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Motion forecasts of road users (i.e., agents) vary in complexity as a function of scene constraints and interactive behavior. We address this with a multi-task learning method for motion forecasting that includes a retrocausal flow of information. The corresponding tasks are to forecast (1) marginal trajectory distributions for all modeled agents and (2) joint trajectory distributions for interacting agents. Using a transformer model, we generate the joint distributions by re-encoding marginal distributions followed by pairwise modeling. This incorporates a retrocausal flow of information from later points in marginal trajectories to earlier points in joint trajectories. Per trajectory point, we model positional uncertainty using compressed exponential power distributions. Notably, our method achieves state-of-the-art results in the Waymo Interaction Prediction dataset and generalizes well to the Argoverse 2 dataset. Additionally, our method provides an interface for issuing instructions through trajectory modifications. Our experiments show that regular training of motion forecasting leads to the ability to follow goal-based instructions and to adapt basic directional instructions to the scene context. Code: this https URL</li>
</ul>

<h3>Title: Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision</h3>
<ul>
<li><strong>Authors: </strong>Xingwei Tan, Marco Valentino, Mahmud Akhter, Maria Liakata, Nikolaos Aletras</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20415">https://arxiv.org/abs/2505.20415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20415">https://arxiv.org/pdf/2505.20415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20415]] Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision(https://arxiv.org/abs/2505.20415)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown promising performance in mathematical and logical reasoning benchmarks. However, recent studies have pointed to memorization, rather than generalization, as one of the leading causes for such performance. LLMs, in fact, are susceptible to content variations, demonstrating a lack of robust symbolic abstractions supporting their reasoning process. To improve reliability, many attempts have been made to combine LLMs with symbolic methods. Nevertheless, existing approaches fail to effectively leverage symbolic representations due to the challenges involved in developing reliable and scalable verification mechanisms. In this paper, we propose to overcome such limitations by generating symbolic reasoning trajectories and select the high-quality ones using a process reward model automatically tuned based on Monte Carlo estimation. The trajectories are then employed via fine-tuning methods to improve logical reasoning and generalization. Our results on logical reasoning benchmarks such as FOLIO and LogicAsker show the effectiveness of the proposed method with large gains on frontier and open-weight models. Moreover, additional experiments on claim verification reveal that fine-tuning on the generated symbolic reasoning trajectories enhances out-of-domain generalizability, suggesting the potential impact of symbolically-guided process supervision in alleviating the effect of memorization on LLM reasoning.</li>
</ul>

<h3>Title: GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Zihong Chen, Wanli Jiang, Jinzhe Li, Zhonghang Yuan, Huanjun Kong, Wanli Ouyang, Nanqing Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20416">https://arxiv.org/abs/2505.20416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20416">https://arxiv.org/pdf/2505.20416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20416]] GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation(https://arxiv.org/abs/2505.20416)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning for large language models (LLMs) typically requires substantial amounts of high-quality supervised data, which is both costly and labor-intensive to acquire. While synthetic data generation has emerged as a promising solution, existing approaches frequently suffer from factual inaccuracies, insufficient long-tail coverage, simplistic knowledge structures, and homogenized outputs. To address these challenges, we introduce GraphGen, a knowledge graph-guided framework designed for three key question-answering (QA) scenarios: atomic QA, aggregated QA, and multi-hop QA. It begins by constructing a fine-grained knowledge graph from the source text. It then identifies knowledge gaps in LLMs using the expected calibration error metric, prioritizing the generation of QA pairs that target high-value, long-tail knowledge. Furthermore, GraphGen incorporates multi-hop neighborhood sampling to capture complex relational information and employs style-controlled generation to diversify the resulting QA data. Experimental results on knowledge-intensive tasks under closed-book settings demonstrate that GraphGen outperforms conventional synthetic data methods, offering a more reliable and comprehensive solution to the data scarcity challenge in supervised fine-tuning. The code and data are publicly available at this https URL.</li>
</ul>

<h3>Title: SEMMA: A Semantic Aware Knowledge Graph Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Arvindh Arun, Sumit Kumar, Mojtaba Nayyeri, Bo Xiong, Ponnurangam Kumaraguru, Antonio Vergari, Steffen Staab</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20422">https://arxiv.org/abs/2505.20422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20422">https://arxiv.org/pdf/2505.20422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20422]] SEMMA: A Semantic Aware Knowledge Graph Foundation Model(https://arxiv.org/abs/2505.20422)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge Graph Foundation Models (KGFMs) have shown promise in enabling zero-shot reasoning over unseen graphs by learning transferable patterns. However, most existing KGFMs rely solely on graph structure, overlooking the rich semantic signals encoded in textual attributes. We introduce SEMMA, a dual-module KGFM that systematically integrates transferable textual semantics alongside structure. SEMMA leverages Large Language Models (LLMs) to enrich relation identifiers, generating semantic embeddings that subsequently form a textual relation graph, which is fused with the structural component. Across 54 diverse KGs, SEMMA outperforms purely structural baselines like ULTRA in fully inductive link prediction. Crucially, we show that in more challenging generalization settings, where the test-time relation vocabulary is entirely unseen, structural methods collapse while SEMMA is 2x more effective. Our findings demonstrate that textual semantics are critical for generalization in settings where structure alone fails, highlighting the need for foundation models that unify structural and linguistic signals in knowledge reasoning.</li>
</ul>

<h3>Title: MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness</h3>
<ul>
<li><strong>Authors: </strong>Yunlong Tang, Pinxin Liu, Mingqian Feng, Zhangyun Tan, Rui Mao, Chao Huang, Jing Bi, Yunzhong Xiao, Susan Liang, Hang Hua, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Chenliang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20426">https://arxiv.org/abs/2505.20426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20426">https://arxiv.org/pdf/2505.20426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20426]] MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness(https://arxiv.org/abs/2505.20426)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs' understanding of perspective through 10 carefully crafted tasks across three complementary dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark comprises 2,711 real-world and synthetic image instances with 5,083 question-answer pairs that probe key capabilities, such as vanishing point perception and counting, perspective type reasoning, line relationship understanding in 3D space, invariance to perspective-preserving transformations, etc. Through a comprehensive evaluation of 43 state-of-the-art MLLMs, we uncover significant limitations: while models demonstrate competence on surface-level perceptual tasks, they struggle with compositional reasoning and maintaining spatial consistency under perturbations. Our analysis further reveals intriguing patterns between model architecture, scale, and perspective capabilities, highlighting both robustness bottlenecks and the benefits of chain-of-thought prompting. MMPerspective establishes a valuable testbed for diagnosing and advancing spatial understanding in vision-language systems. Resources available at: this https URL</li>
</ul>

<h3>Title: The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog Syntactic Annotation Project</h3>
<ul>
<li><strong>Authors: </strong>Angelina A. Aquino, Lester James V. Miranda, Elsie Marie T. Or</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20428">https://arxiv.org/abs/2505.20428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20428">https://arxiv.org/pdf/2505.20428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20428]] The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog Syntactic Annotation Project(https://arxiv.org/abs/2505.20428)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper presents UD-NewsCrawl, the largest Tagalog treebank to date, containing 15.6k trees manually annotated according to the Universal Dependencies framework. We detail our treebank development process, including data collection, pre-processing, manual annotation, and quality assurance procedures. We provide baseline evaluations using multiple transformer-based models to assess the performance of state-of-the-art dependency parsers on Tagalog. We also highlight challenges in the syntactic analysis of Tagalog given its distinctive grammatical properties, and discuss its implications for the annotation of this treebank. We anticipate that UD-NewsCrawl and our baseline model implementations will serve as valuable resources for advancing computational linguistics research in underrepresented languages like Tagalog.</li>
</ul>

<h3>Title: PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy</h3>
<ul>
<li><strong>Authors: </strong>Shuhao Guan, Moule Lin, Cheng Xu, Xinyi Liu, Jinman Zhao, Jiexin Fan, Qi Xu, Derek Greene</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20429">https://arxiv.org/abs/2505.20429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20429">https://arxiv.org/pdf/2505.20429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20429]] PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy(https://arxiv.org/abs/2505.20429)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This paper introduces PreP-OCR, a two-stage pipeline that combines document image restoration with semantic-aware post-OCR correction to improve text extraction from degraded historical documents. Our key innovation lies in jointly optimizing image clarity and linguistic consistency. First, we generate synthetic image pairs with randomized text fonts, layouts, and degradations. An image restoration model is trained on this synthetic data, using multi-directional patch extraction and fusion to process large images. Second, a ByT5 post-corrector, fine-tuned on synthetic historical text training pairs, addresses any remaining OCR errors. Detailed experiments on 13,831 pages of real historical documents in English, French, and Spanish show that PreP-OCR pipeline reduces character error rates by 63.9-70.3\% compared to OCR on raw images. Our pipeline demonstrates the potential of integrating image restoration with linguistic error correction for digitizing historical archives.</li>
</ul>

<h3>Title: Holes in Latent Space: Topological Signatures Under Adversarial Influence</h3>
<ul>
<li><strong>Authors: </strong>Aideen Fay, InÃ©s GarcÃ­a-Redondo, Qiquan Wang, Haim Dubossarsky, Anthea Monod</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CG, math.AT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20435">https://arxiv.org/abs/2505.20435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20435">https://arxiv.org/pdf/2505.20435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20435]] Holes in Latent Space: Topological Signatures Under Adversarial Influence(https://arxiv.org/abs/2505.20435)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Understanding how adversarial conditions affect language models requires techniques that capture both global structure and local detail within high-dimensional activation spaces. We propose persistent homology (PH), a tool from topological data analysis, to systematically characterize multiscale latent space dynamics in LLMs under two distinct attack modes -- backdoor fine-tuning and indirect prompt injection. By analyzing six state-of-the-art LLMs, we show that adversarial conditions consistently compress latent topologies, reducing structural diversity at smaller scales while amplifying dominant features at coarser ones. These topological signatures are statistically robust across layers, architectures, model sizes, and align with the emergence of adversarial effects deeper in the network. To capture finer-grained mechanisms underlying these shifts, we introduce a neuron-level PH framework that quantifies how information flows and transforms within and across layers. Together, our findings demonstrate that PH offers a principled and unifying approach to interpreting representational dynamics in LLMs, particularly under distributional shift.</li>
</ul>

<h3>Title: HAMburger: Accelerating LLM Inference via Token Smashing</h3>
<ul>
<li><strong>Authors: </strong>Jingyu Liu, Ce Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20438">https://arxiv.org/abs/2505.20438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20438">https://arxiv.org/pdf/2505.20438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20438]] HAMburger: Accelerating LLM Inference via Token Smashing(https://arxiv.org/abs/2505.20438)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The growing demand for efficient Large Language Model (LLM) inference requires a holistic optimization on algorithms, systems, and hardware. However, very few works have fundamentally changed the generation pattern: each token needs one forward pass and one KV cache. This can be sub-optimal because we found that LLMs are extremely capable of self-identifying the exact dose of information that a single KV cache can store, and many tokens can be generated confidently without global context. Based on this insight, we introduce HAMburger, a Hierarchically Auto-regressive Model that redefines resource allocation in LLMs by moving beyond uniform computation and storage per token during inference. Stacking a compositional embedder and a micro-step decoder in between a base LLM, HAMburger smashes multiple tokens into a single KV and generates several tokens per step. Additionally, HAMburger functions as a speculative decoding framework where it can blindly trust self-drafted tokens. As a result, HAMburger shifts the growth of KV cache and forward FLOPs from linear to sub-linear with respect to output length, and adjusts its inference speed based on query perplexity and output structure. Extensive evaluations show that HAMburger reduces the KV cache computation by up to 2$\times$ and achieves up to 2$\times$ TPS, while maintaining quality in both short- and long-context tasks. Our method explores an extremely challenging inference regime that requires both computation- and memory-efficiency with a hardware-agnostic design.</li>
</ul>

<h3>Title: HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoran Li, Yingjie Qin, Baoyuan Ou, Lai Xu, Ruiwen Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20444">https://arxiv.org/abs/2505.20444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20444">https://arxiv.org/pdf/2505.20444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20444]] HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models(https://arxiv.org/abs/2505.20444)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have made significant progress in multimodal tasks. However, their performance often deteriorates in long-context scenarios, particularly long videos. While Rotary Position Embedding (RoPE) has been widely adopted for length generalization in Large Language Models (LLMs), extending vanilla RoPE to capture the intricate spatial-temporal dependencies in videos remains an unsolved challenge. Existing methods typically allocate different frequencies within RoPE to encode 3D positional information. However, these allocation strategies mainly rely on heuristics, lacking in-depth theoretical analysis. In this paper, we first study how different allocation strategies impact the long-context capabilities of VLMs. Our analysis reveals that current multimodal RoPEs fail to reliably capture semantic similarities over extended contexts. To address this issue, we propose HoPE, a Hybrid of Position Embedding designed to improve the long-context capabilities of VLMs. HoPE introduces a hybrid frequency allocation strategy for reliable semantic modeling over arbitrarily long context, and a dynamic temporal scaling mechanism to facilitate robust learning and flexible inference across diverse context lengths. Extensive experiments across four video benchmarks on long video understanding and retrieval tasks demonstrate that HoPE consistently outperforms existing methods, confirming its effectiveness. Code is available at this https URL.</li>
</ul>

<h3>Title: In-context Language Learning for Endangered Languages in Speech Recognition</h3>
<ul>
<li><strong>Authors: </strong>Zhaolin Li, Jan Niehues</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20445">https://arxiv.org/abs/2505.20445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20445">https://arxiv.org/pdf/2505.20445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20445]] In-context Language Learning for Endangered Languages in Speech Recognition(https://arxiv.org/abs/2505.20445)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With approximately 7,000 languages spoken worldwide, current large language models (LLMs) support only a small subset. Prior research indicates LLMs can learn new languages for certain tasks without supervised data. We extend this investigation to speech recognition, investigating whether LLMs can learn unseen, low-resource languages through in-context learning (ICL). With experiments on four diverse endangered languages that LLMs have not been trained on, we find that providing more relevant text samples enhances performance in both language modelling and Automatic Speech Recognition (ASR) tasks. Furthermore, we show that the probability-based approach outperforms the traditional instruction-based approach in language learning. Lastly, we show ICL enables LLMs to achieve ASR performance that is comparable to or even surpasses dedicated language models trained specifically for these languages, while preserving the original capabilities of the LLMs.</li>
</ul>

<h3>Title: Time Series Generation Under Data Scarcity: A Unified Generative Modeling Approach</h3>
<ul>
<li><strong>Authors: </strong>Tal Gonen, Itai Pemper, Ilan Naiman, Nimrod Berman, Omri Azencot</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20446">https://arxiv.org/abs/2505.20446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20446">https://arxiv.org/pdf/2505.20446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20446]] Time Series Generation Under Data Scarcity: A Unified Generative Modeling Approach(https://arxiv.org/abs/2505.20446)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative modeling of time series is a central challenge in time series analysis, particularly under data-scarce conditions. Despite recent advances in generative modeling, a comprehensive understanding of how state-of-the-art generative models perform under limited supervision remains lacking. In this work, we conduct the first large-scale study evaluating leading generative models in data-scarce settings, revealing a substantial performance gap between full-data and data-scarce regimes. To close this gap, we propose a unified diffusion-based generative framework that can synthesize high-fidelity time series across diverse domains using just a few examples. Our model is pre-trained on a large, heterogeneous collection of time series datasets, enabling it to learn generalizable temporal representations. It further incorporates architectural innovations such as dynamic convolutional layers for flexible channel adaptation and dataset token conditioning for domain-aware generation. Without requiring abundant supervision, our unified model achieves state-of-the-art performance in few-shot settings-outperforming domain-specific baselines across a wide range of subset sizes. Remarkably, it also surpasses all baselines even when tested on full datasets benchmarks, highlighting the strength of pre-training and cross-domain generalization. We hope this work encourages the community to revisit few-shot generative modeling as a key problem in time series research and pursue unified solutions that scale efficiently across domains. Code is available at this https URL.</li>
</ul>

<h3>Title: Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries</h3>
<ul>
<li><strong>Authors: </strong>Sahana Ramnath, Anurag Mudgil, Brihi Joshi, Skyler Hallinan, Xiang Ren</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20451">https://arxiv.org/abs/2505.20451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20451">https://arxiv.org/pdf/2505.20451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20451]] Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries(https://arxiv.org/abs/2505.20451)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Today, large language models are widely used as judges to evaluate responses from other language models. Hence, it is imperative to benchmark and improve these LLM-judges on real-world language model usage: a typical human-assistant conversation is lengthy, and shows significant diversity in topics, intents, and requirements across turns, e.g. social interactions, task requests, feedback. We present Amulet, a framework that leverages pertinent linguistic concepts of dialog-acts and maxims to improve the accuracy of LLM-judges on preference data with complex, multi-turn conversational context. Amulet presents valuable insights about (a) the communicative structures and intents present in the conversation (dialog acts), and (b) the satisfaction of conversational principles (maxims) by the preference responses, and uses them to make judgments. On four challenging datasets, Amulet shows that (a) humans frequently (60 to 70 percent of the time) change their intents from one turn of the conversation to the next, and (b) in 75 percent of instances, the preference responses can be differentiated via dialog acts and/or maxims, reiterating the latter's significance in judging such data. Amulet can be used either as a judge by applying the framework to a single LLM, or integrated into a jury with different LLM judges; our judges and juries show strong improvements on relevant baselines for all four datasets.</li>
</ul>

<h3>Title: Active Learning for Multiple Change Point Detection in Non-stationary Time Series with Deep Gaussian Processes</h3>
<ul>
<li><strong>Authors: </strong>Hao Zhao, Rong Pan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20452">https://arxiv.org/abs/2505.20452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20452">https://arxiv.org/pdf/2505.20452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20452]] Active Learning for Multiple Change Point Detection in Non-stationary Time Series with Deep Gaussian Processes(https://arxiv.org/abs/2505.20452)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multiple change point (MCP) detection in non-stationary time series is challenging due to the variety of underlying patterns. To address these challenges, we propose a novel algorithm that integrates Active Learning (AL) with Deep Gaussian Processes (DGPs) for robust MCP detection. Our method leverages spectral analysis to identify potential changes and employs AL to strategically select new sampling points for improved efficiency. By incorporating the modeling flexibility of DGPs with the change-identification capabilities of spectral methods, our approach adapts to diverse spectral change behaviors and effectively localizes multiple change points. Experiments on both simulated and real-world data demonstrate that our method outperforms existing techniques in terms of detection accuracy and sampling efficiency for non-stationary time series.</li>
</ul>

<h3>Title: BlastOFormer: Attention and Neural Operator Deep Learning Methods for Explosive Blast Prediction</h3>
<ul>
<li><strong>Authors: </strong>Reid Graves, Anthony Zhou, Amir Barati Farimani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20454">https://arxiv.org/abs/2505.20454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20454">https://arxiv.org/pdf/2505.20454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20454]] BlastOFormer: Attention and Neural Operator Deep Learning Methods for Explosive Blast Prediction(https://arxiv.org/abs/2505.20454)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, transformer</a></li>
<li><strong>Abstract: </strong>Accurate prediction of blast pressure fields is essential for applications in structural safety, defense planning, and hazard mitigation. Traditional methods such as empirical models and computational fluid dynamics (CFD) simulations offer limited trade offs between speed and accuracy; empirical models fail to capture complex interactions in cluttered environments, while CFD simulations are computationally expensive and time consuming. In this work, we introduce BlastOFormer, a novel Transformer based surrogate model for full field maximum pressure prediction from arbitrary obstacle and charge configurations. BlastOFormer leverages a signed distance function (SDF) encoding and a grid to grid attention based architecture inspired by OFormer and Vision Transformer (ViT) frameworks. Trained on a dataset generated using the open source blastFoam CFD solver, our model outperforms convolutional neural networks (CNNs) and Fourier Neural Operators (FNOs) across both log transformed and unscaled domains. Quantitatively, BlastOFormer achieves the highest R2 score (0.9516) and lowest error metrics, while requiring only 6.4 milliseconds for inference, more than 600,000 times faster than CFD simulations. Qualitative visualizations and error analyses further confirm BlastOFormer's superior spatial coherence and generalization capabilities. These results highlight its potential as a real time alternative to conventional CFD approaches for blast pressure estimation in complex environments.</li>
</ul>

<h3>Title: DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data</h3>
<ul>
<li><strong>Authors: </strong>Ruqi Wu, Xinjie Wang, Liu Liu, Chunle Guo, Jiaxiong Qiu, Chongyi Li, Lichao Huang, Zhizhong Su, Ming-Ming Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20460">https://arxiv.org/abs/2505.20460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20460">https://arxiv.org/pdf/2505.20460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20460]] DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data(https://arxiv.org/abs/2505.20460)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>We present DIPO, a novel framework for the controllable generation of articulated 3D objects from a pair of images: one depicting the object in a resting state and the other in an articulated state. Compared to the single-image approach, our dual-image input imposes only a modest overhead for data collection, but at the same time provides important motion information, which is a reliable guide for predicting kinematic relationships between parts. Specifically, we propose a dual-image diffusion model that captures relationships between the image pair to generate part layouts and joint parameters. In addition, we introduce a Chain-of-Thought (CoT) based graph reasoner that explicitly infers part connectivity relationships. To further improve robustness and generalization on complex articulated objects, we develop a fully automated dataset expansion pipeline, name LEGO-Art, that enriches the diversity and complexity of PartNet-Mobility dataset. We propose PM-X, a large-scale dataset of complex articulated 3D objects, accompanied by rendered images, URDF annotations, and textual descriptions. Extensive experiments demonstrate that DIPO significantly outperforms existing baselines in both the resting state and the articulated state, while the proposed PM-X dataset further enhances generalization to diverse and structurally complex articulated objects. Our code and dataset will be released to the community upon publication.</li>
</ul>

<h3>Title: CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Lei Tian, Xiaomin Li, Liqian Ma, Hefei Huang, Zirui Zheng, Hao Yin, Taiqing Li, Huchuan Lu, Xu Jia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20469">https://arxiv.org/abs/2505.20469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20469">https://arxiv.org/pdf/2505.20469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20469]] CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting(https://arxiv.org/abs/2505.20469)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in 3D reconstruction techniques and vision-language models have fueled significant progress in 3D semantic understanding, a capability critical to robotics, autonomous driving, and virtual/augmented reality. However, methods that rely on 2D priors are prone to a critical challenge: cross-view semantic inconsistencies induced by occlusion, image blur, and view-dependent variations. These inconsistencies, when propagated via projection supervision, deteriorate the quality of 3D Gaussian semantic fields and introduce artifacts in the rendered outputs. To mitigate this limitation, we propose CCL-LGS, a novel framework that enforces view-consistent semantic supervision by integrating multi-view semantic cues. Specifically, our approach first employs a zero-shot tracker to align a set of SAM-generated 2D masks and reliably identify their corresponding categories. Next, we utilize CLIP to extract robust semantic encodings across views. Finally, our Contrastive Codebook Learning (CCL) module distills discriminative semantic features by enforcing intra-class compactness and inter-class distinctiveness. In contrast to previous methods that directly apply CLIP to imperfect masks, our framework explicitly resolves semantic conflicts while preserving category discriminability. Extensive experiments demonstrate that CCL-LGS outperforms previous state-of-the-art methods. Our project page is available at this https URL.</li>
</ul>

<h3>Title: WeatherEdit: Controllable Weather Editing with 4D Gaussian Field</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Qian, Wenjing Li, Yuhu Guo, Gustav Markkula</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.ET, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20471">https://arxiv.org/abs/2505.20471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20471">https://arxiv.org/pdf/2505.20471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20471]] WeatherEdit: Controllable Weather Editing with 4D Gaussian Field(https://arxiv.org/abs/2505.20471)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this work, we present WeatherEdit, a novel weather editing pipeline for generating realistic weather effects with controllable types and severity in 3D scenes. Our approach is structured into two key components: weather background editing and weather particle construction. For weather background editing, we introduce an all-in-one adapter that integrates multiple weather styles into a single pretrained diffusion model, enabling the generation of diverse weather effects in 2D image backgrounds. During inference, we design a Temporal-View (TV-) attention mechanism that follows a specific order to aggregate temporal and spatial information, ensuring consistent editing across multi-frame and multi-view images. To construct the weather particles, we first reconstruct a 3D scene using the edited images and then introduce a dynamic 4D Gaussian field to generate snowflakes, raindrops and fog in the scene. The attributes and dynamics of these particles are precisely controlled through physical-based modelling and simulation, ensuring realistic weather representation and flexible severity adjustments. Finally, we integrate the 4D Gaussian field with the 3D scene to render consistent and highly realistic weather effects. Experiments on multiple driving datasets demonstrate that WeatherEdit can generate diverse weather effects with controllable condition severity, highlighting its potential for autonomous driving simulation in adverse weather. See project page: this https URL</li>
</ul>

<h3>Title: Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data</h3>
<ul>
<li><strong>Authors: </strong>Abhijit Chunduru, Majid Morafah, Mahdi Morafah, Vishnu Pandi Chellapandi, Ang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.DC, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20485">https://arxiv.org/abs/2505.20485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20485">https://arxiv.org/pdf/2505.20485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20485]] Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data(https://arxiv.org/abs/2505.20485)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>The inevitable presence of data heterogeneity has made federated learning very challenging. There are numerous methods to deal with this issue, such as local regularization, better model fusion techniques, and data sharing. Though effective, they lack a deep understanding of how data heterogeneity can affect the global decision boundary. In this paper, we bridge this gap by performing an experimental analysis of the learned decision boundary using a toy example. Our observations are surprising: (1) we find that the existing methods suffer from forgetting and clients forget the global decision boundary and only learn the perfect local one, and (2) this happens regardless of the initial weights, and clients forget the global decision boundary even starting from pre-trained optimal weights. In this paper, we present FedProj, a federated learning framework that robustly learns the global decision boundary and avoids its forgetting during local training. To achieve better ensemble knowledge fusion, we design a novel server-side ensemble knowledge transfer loss to further calibrate the learned global decision boundary. To alleviate the issue of learned global decision boundary forgetting, we further propose leveraging an episodic memory of average ensemble logits on a public unlabeled dataset to regulate the gradient updates at each step of local training. Experimental results demonstrate that FedProj outperforms state-of-the-art methods by a large margin.</li>
</ul>

<h3>Title: Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages</h3>
<ul>
<li><strong>Authors: </strong>Asif Shahriar, Rifat Shahriyar, M Saifur Rahman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20496">https://arxiv.org/abs/2505.20496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20496">https://arxiv.org/pdf/2505.20496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20496]] Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages(https://arxiv.org/abs/2505.20496)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Conventional transformer models typically compress the information from all tokens in a sequence into a single \texttt{[CLS]} token to represent global context-- an approach that can lead to information loss in tasks requiring localized or hierarchical cues. In this work, we introduce \textit{Inceptive Transformer}, a modular and lightweight architecture that enriches transformer-based token representations by integrating a multi-scale feature extraction module inspired by inception networks. Our model is designed to balance local and global dependencies by dynamically weighting tokens based on their relevance to a particular task. Evaluation across a diverse range of tasks including emotion recognition (both English and Bangla), irony detection, disease identification, and anti-COVID vaccine tweets classification shows that our models consistently outperform the baselines by 1\% to 14\% while maintaining efficiency. These findings highlight the versatility and cross-lingual applicability of our method for enriching transformer-based representations across diverse domains.</li>
</ul>

<h3>Title: Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism</h3>
<ul>
<li><strong>Authors: </strong>Naba Rizvi, Harper Strickland, Saleha Ahmedi, Aekta Kallepalli, Isha Khirwadkar, William Wu, Imani N. S. Munyaka, Nedjma Ousidhoum</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20500">https://arxiv.org/abs/2505.20500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20500">https://arxiv.org/pdf/2505.20500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20500]] Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism(https://arxiv.org/abs/2505.20500)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used in decision-making tasks like rÃ©sumÃ© screening and content moderation, giving them the power to amplify or suppress certain perspectives. While previous research has identified disability-related biases in LLMs, little is known about how they conceptualize ableism or detect it in text. We evaluate the ability of four LLMs to identify nuanced ableism directed at autistic individuals. We examine the gap between their understanding of relevant terminology and their effectiveness in recognizing ableist content in context. Our results reveal that LLMs can identify autism-related language but often miss harmful or offensive connotations. Further, we conduct a qualitative comparison of human and LLM explanations. We find that LLMs tend to rely on surface-level keyword matching, leading to context misinterpretations, in contrast to human annotators who consider context, speaker identity, and potential impact. On the other hand, both LLMs and humans agree on the annotation scheme, suggesting that a binary classification is adequate for evaluating LLM performance, which is consistent with findings from prior studies involving human annotators.</li>
</ul>

<h3>Title: Gatsby Without the 'E': Crafting Lipograms with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Rohan Balasubramanian, Nitish Gokulakrishnan, Syeda Jannatus Saba, Steven Skiena</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20501">https://arxiv.org/abs/2505.20501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20501">https://arxiv.org/pdf/2505.20501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20501]] Gatsby Without the 'E': Crafting Lipograms with LLMs(https://arxiv.org/abs/2505.20501)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Lipograms are a unique form of constrained writing where all occurrences of a particular letter are excluded from the text, typified by the novel Gadsby, which daringly avoids all usage of the letter 'e'. In this study, we explore the power of modern large language models (LLMs) by transforming the novel F. Scott Fitzgerald's The Great Gatsby into a fully 'e'-less text. We experimented with a range of techniques, from baseline methods like synonym replacement to sophisticated generative models enhanced with beam search and named entity analysis. We show that excluding up to 3.6% of the most common letters (up to the letter 'u') had minimal impact on the text's meaning, although translation fidelity rapidly and predictably decays with stronger lipogram constraints. Our work highlights the surprising flexibility of English under strict constraints, revealing just how adaptable and creative language can be.</li>
</ul>

<h3>Title: Large Language Models for IT Automation Tasks: Are We There Yet?</h3>
<ul>
<li><strong>Authors: </strong>Md Mahadi Hassan, John Salvador, Akond Rahman, Santu Karmaker</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20505">https://arxiv.org/abs/2505.20505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20505">https://arxiv.org/pdf/2505.20505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20505]] Large Language Models for IT Automation Tasks: Are We There Yet?(https://arxiv.org/abs/2505.20505)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>LLMs show promise in code generation, yet their effectiveness for IT automation tasks, particularly for tools like Ansible, remains understudied. Existing benchmarks rely primarily on synthetic tasks that fail to capture the needs of practitioners who use IT automation tools, such as Ansible. We present ITAB (IT Automation Task Benchmark), a benchmark of 126 diverse tasks (e.g., configuring servers, managing files) where each task accounts for state reconciliation: a property unique to IT automation tools. ITAB evaluates LLMs' ability to generate functional Ansible automation scripts via dynamic execution in controlled environments. We evaluate 14 open-source LLMs, none of which accomplish pass@10 at a rate beyond 12%. To explain these low scores, we analyze 1,411 execution failures across the evaluated LLMs and identify two main categories of prevalent semantic errors: failures in state reconciliation related reasoning (44.87% combined from variable (11.43%), host (11.84%), path(11.63%), and template (9.97%) issues) and deficiencies in module-specific execution knowledge (24.37% combined from Attribute and parameter (14.44%) and module (9.93%) errors). Our findings reveal key limitations in open-source LLMs' ability to track state changes and apply specialized module knowledge, indicating that reliable IT automation will require major advances in state reasoning and domain-specific execution understanding.</li>
</ul>

<h3>Title: Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset</h3>
<ul>
<li><strong>Authors: </strong>Elias Arbash, Ahmed Jamal Afifi, Ymane Belahsen, Margret Fuchs, Pedram Ghamisi, Paul Scheunders, Richard Gloaguen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20507">https://arxiv.org/abs/2505.20507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20507">https://arxiv.org/pdf/2505.20507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20507]] Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset(https://arxiv.org/abs/2505.20507)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, transformer</a></li>
<li><strong>Abstract: </strong>The global challenge of sustainable recycling demands automated, fast, and accurate, state-of-the-art (SOTA) material detection systems that act as a bedrock for a circular economy. Democratizing access to these cutting-edge solutions that enable real-time waste analysis is essential for scaling up recycling efforts and fostering the Green Deal. In response, we introduce \textbf{Electrolyzers-HSI}, a novel multimodal benchmark dataset designed to accelerate the recovery of critical raw materials through accurate electrolyzer materials classification. The dataset comprises 55 co-registered high-resolution RGB images and hyperspectral imaging (HSI) data cubes spanning the 400--2500 nm spectral range, yielding over 4.2 million pixel vectors and 424,169 labeled ones. This enables non-invasive spectral analysis of shredded electrolyzer samples, supporting quantitative and qualitative material classification and spectral properties investigation. We evaluate a suite of baseline machine learning (ML) methods alongside SOTA transformer-based deep learning (DL) architectures, including Vision Transformer, SpectralFormer, and the Multimodal Fusion Transformer, to investigate architectural bottlenecks for further efficiency optimisation when deploying transformers in material identification. We implement zero-shot detection techniques and majority voting across pixel-level predictions to establish object-level classification robustness. In adherence to the FAIR data principles, the electrolyzers-HSI dataset and accompanying codebase are openly available at this https URL and this https URL, supporting reproducible research and facilitating the broader adoption of smart and sustainable e-waste recycling solutions.</li>
</ul>

<h3>Title: A Feature-level Bias Evaluation Framework for Facial Expression Recognition Models</h3>
<ul>
<li><strong>Authors: </strong>Tangzheng Lian, Oya Celiktutan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20512">https://arxiv.org/abs/2505.20512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20512">https://arxiv.org/pdf/2505.20512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20512]] A Feature-level Bias Evaluation Framework for Facial Expression Recognition Models(https://arxiv.org/abs/2505.20512)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Recent studies on fairness have shown that Facial Expression Recognition (FER) models exhibit biases toward certain visually perceived demographic groups. However, the limited availability of human-annotated demographic labels in public FER datasets has constrained the scope of such bias analysis. To overcome this limitation, some prior works have resorted to pseudo-demographic labels, which may distort bias evaluation results. Alternatively, in this paper, we propose a feature-level bias evaluation framework for evaluating demographic biases in FER models under the setting where demographic labels are unavailable in the test set. Extensive experiments demonstrate that our method more effectively evaluates demographic biases compared to existing approaches that rely on pseudo-demographic labels. Furthermore, we observe that many existing studies do not include statistical testing in their bias evaluations, raising concerns that some reported biases may not be statistically significant but rather due to randomness. To address this issue, we introduce a plug-and-play statistical module to ensure the statistical significance of biased evaluation results. A comprehensive bias analysis based on the proposed module is then conducted across three sensitive attributes (age, gender, and race), seven facial expressions, and multiple network architectures on a large-scale dataset, revealing the prominent demographic biases in FER and providing insights on selecting a fairer network architecture.</li>
</ul>

<h3>Title: MetaWriter: Personalized Handwritten Text Recognition Using Meta-Learned Prompt Tuning</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Gu, Li Gu, Ching Yee Suen, Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20513">https://arxiv.org/abs/2505.20513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20513">https://arxiv.org/pdf/2505.20513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20513]] MetaWriter: Personalized Handwritten Text Recognition Using Meta-Learned Prompt Tuning(https://arxiv.org/abs/2505.20513)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advancements in handwritten text recognition (HTR) have enabled the effective conversion of handwritten text to digital formats. However, achieving robust recognition across diverse writing styles remains challenging. Traditional HTR methods lack writer-specific personalization at test time due to limitations in model architecture and training strategies. Existing attempts to bridge this gap, through gradient-based meta-learning, still require labeled examples and suffer from parameter-inefficient fine-tuning, leading to substantial computational and memory overhead. To overcome these challenges, we propose an efficient framework that formulates personalization as prompt tuning, incorporating an auxiliary image reconstruction task with a self-supervised loss to guide prompt adaptation with unlabeled test-time examples. To ensure self-supervised loss effectively minimizes text recognition error, we leverage meta-learning to learn the optimal initialization of the prompts. As a result, our method allows the model to efficiently capture unique writing styles by updating less than 1% of its parameters and eliminating the need for time-intensive annotation processes. We validate our approach on the RIMES and IAM Handwriting Database benchmarks, where it consistently outperforms previous state-of-the-art methods while using 20x fewer parameters. We believe this represents a significant advancement in personalized handwritten text recognition, paving the way for more reliable and practical deployment in resource-constrained scenarios.</li>
</ul>

<h3>Title: Semi-Explicit Neural DAEs: Learning Long-Horizon Dynamical Systems with Algebraic Constraints</h3>
<ul>
<li><strong>Authors: </strong>Avik Pal, Alan Edelman, Christopher Rackauckas</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20515">https://arxiv.org/abs/2505.20515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20515">https://arxiv.org/pdf/2505.20515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20515]] Semi-Explicit Neural DAEs: Learning Long-Horizon Dynamical Systems with Algebraic Constraints(https://arxiv.org/abs/2505.20515)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite the promise of scientific machine learning (SciML) in combining data-driven techniques with mechanistic modeling, existing approaches for incorporating hard constraints in neural differential equations (NDEs) face significant limitations. Scalability issues and poor numerical properties prevent these neural models from being used for modeling physical systems with complicated conservation laws. We propose Manifold-Projected Neural ODEs (PNODEs), a method that explicitly enforces algebraic constraints by projecting each ODE step onto the constraint manifold. This framework arises naturally from semi-explicit differential-algebraic equations (DAEs), and includes both a robust iterative variant and a fast approximation requiring a single Jacobian factorization. We further demonstrate that prior works on relaxation methods are special cases of our approach. PNODEs consistently outperform baselines across six benchmark problems achieving a mean constraint violation error below $10^{-10}$. Additionally, PNODEs consistently achieve lower runtime compared to other methods for a given level of error tolerance. These results show that constraint projection offers a simple strategy for learning physically consistent long-horizon dynamics.</li>
</ul>

<h3>Title: Towards Fully FP8 GEMM LLM Training at Scale</h3>
<ul>
<li><strong>Authors: </strong>Alejandro HernÃ¡ndez-Cano, Dhia Garbaya, Imanol Schlag, Martin Jaggi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20524">https://arxiv.org/abs/2505.20524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20524">https://arxiv.org/pdf/2505.20524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20524]] Towards Fully FP8 GEMM LLM Training at Scale(https://arxiv.org/abs/2505.20524)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Despite the significant potential of FP8 data formats for large language model (LLM) pre-training, their adoption has been limited due to challenges in maintaining stability at scale. Existing approaches often rely on suboptimal fine-grained FP8 kernels or fall back to higher-precision matrix multiplications (GEMMs) in sensitive components, such as attention projections, compromising potential throughput gains. We introduce a new class of LLM architectures that, for the first time, support FP8 computation for all GEMMs within transformer blocks during both forward and backward passes. This enables unprecedented throughput gains, particularly at scale, while matching the downstream performance of standard BF16 training. Our architecture design reduces large outlier activations, promoting stable long-term FP8 training. In addition, we identify key metrics to monitor low-precision training and predict potential future divergences.</li>
</ul>

<h3>Title: MultLFG: Training-free Multi-LoRA composition using Frequency-domain Guidance</h3>
<ul>
<li><strong>Authors: </strong>Aniket Roy, Maitreya Suin, Ketul Shah, Rama Chellappa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20525">https://arxiv.org/abs/2505.20525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20525">https://arxiv.org/pdf/2505.20525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20525]] MultLFG: Training-free Multi-LoRA composition using Frequency-domain Guidance(https://arxiv.org/abs/2505.20525)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) has gained prominence as a computationally efficient method for fine-tuning generative models, enabling distinct visual concept synthesis with minimal overhead. However, current methods struggle to effectively merge multiple LoRA adapters without training, particularly in complex compositions involving diverse visual elements. We introduce MultLFG, a novel framework for training-free multi-LoRA composition that utilizes frequency-domain guidance to achieve adaptive fusion of multiple LoRAs. Unlike existing methods that uniformly aggregate concept-specific LoRAs, MultLFG employs a timestep and frequency subband adaptive fusion strategy, selectively activating relevant LoRAs based on content relevance at specific timesteps and frequency bands. This frequency-sensitive guidance not only improves spatial coherence but also provides finer control over multi-LoRA composition, leading to more accurate and consistent results. Experimental evaluations on the ComposLoRA benchmark reveal that MultLFG substantially enhances compositional fidelity and image quality across various styles and concept sets, outperforming state-of-the-art baselines in multi-concept generation tasks. Code will be released.</li>
</ul>

<h3>Title: One-shot Robust Federated Learning of Independent Component Analysis</h3>
<ul>
<li><strong>Authors: </strong>Dian Jin, Xin Bing, Yuqian Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20532">https://arxiv.org/abs/2505.20532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20532">https://arxiv.org/pdf/2505.20532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20532]] One-shot Robust Federated Learning of Independent Component Analysis(https://arxiv.org/abs/2505.20532)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>This paper investigates a general robust one-shot aggregation framework for distributed and federated Independent Component Analysis (ICA) problem. We propose a geometric median-based aggregation algorithm that leverages $k$-means clustering to resolve the permutation ambiguity in local client estimations. Our method first performs k-means to partition client-provided estimators into clusters and then aggregates estimators within each cluster using the geometric median. This approach provably remains effective even in highly heterogeneous scenarios where at most half of the clients can observe only a minimal number of samples. The key theoretical contribution lies in the combined analysis of the geometric median's error bound-aided by sample quantiles-and the maximum misclustering rates of the aforementioned solution of $k$-means. The effectiveness of the proposed approach is further supported by simulation studies conducted under various heterogeneous settings.</li>
</ul>

<h3>Title: Rotary Masked Autoencoders are Versatile Learners</h3>
<ul>
<li><strong>Authors: </strong>Uros Zivanovic, Serafina Di Gioia, Andre Scaffidi, MartÃ­n de los Rios, Gabriella Contardo, Roberto Trotta</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20535">https://arxiv.org/abs/2505.20535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20535">https://arxiv.org/pdf/2505.20535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20535]] Rotary Masked Autoencoders are Versatile Learners(https://arxiv.org/abs/2505.20535)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Applying Transformers to irregular time-series typically requires specializations to their baseline architecture, which can result in additional computational overhead and increased method complexity. We present the Rotary Masked Autoencoder (RoMAE), which utilizes the popular Rotary Positional Embedding (RoPE) method for continuous positions. RoMAE is an extension to the Masked Autoencoder (MAE) that enables representation learning with multidimensional continuous positional information while avoiding any time-series-specific architectural specializations. We showcase RoMAE's performance on a variety of modalities including irregular and multivariate time-series, images, and audio, demonstrating that RoMAE surpasses specialized time-series architectures on difficult datasets such as the DESC ELAsTiCC Challenge while maintaining MAE's usual performance across other modalities. In addition, we investigate RoMAE's ability to reconstruct the embedded continuous positions, demonstrating that including learned embeddings in the input sequence breaks RoPE's relative position property.</li>
</ul>

<h3>Title: AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Antony Joseph, Syed Murtaza Husain, Stella S. R. Offner, StÃ©phanie Juneau, Paul Torrey, Adam S. Bolton, Juan P. Farias, Niall Gaffney, Greg Durrett, Junyi Jessy Li</a></li>
<li><strong>Subjects: </strong>cs.CL, astro-ph.IM, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20538">https://arxiv.org/abs/2505.20538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20538">https://arxiv.org/pdf/2505.20538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20538]] AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy(https://arxiv.org/abs/2505.20538)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are being explored for applications in scientific research, including their capabilities to synthesize literature, answer research questions, generate research ideas, and even conduct computational experiments. Ultimately, our goal is for these to help scientists derive novel scientific insights. In many areas of science, such insights often arise from processing and visualizing data to understand its patterns. However, evaluating whether an LLM-mediated scientific workflow produces outputs conveying the correct scientific insights is challenging to evaluate and has not been addressed in past work. We introduce AstroVisBench, the first benchmark for both scientific computing and visualization in the astronomy domain. AstroVisBench judges a language model's ability to both (1) create astronomy-specific workflows to process and analyze data and (2) visualize the results of these workflows through complex plots. Our evaluation of visualizations uses a novel LLM-as-a-judge workflow, which is validated against annotation by five professional astronomers. Using AstroVisBench we present an evaluation of state-of-the-art language models, showing a significant gap in their ability to engage in astronomy research as useful assistants. This evaluation provides a strong end-to-end evaluation for AI scientists that offers a path forward for the development of visualization-based workflows, which are central to a broad range of domains from physics to biology.</li>
</ul>

<h3>Title: Causality and "In-the-Wild" Video-Based Person Re-ID: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Md Rashidunnabi, Kailash Hambarde, Hugo ProenÃ§a</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20540">https://arxiv.org/abs/2505.20540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20540">https://arxiv.org/pdf/2505.20540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20540]] Causality and "In-the-Wild" Video-Based Person Re-ID: A Survey(https://arxiv.org/abs/2505.20540)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, fair, interpretability, transformer, generative</a></li>
<li><strong>Abstract: </strong>Video-based person re-identification (Re-ID) remains brittle in real-world deployments despite impressive benchmark performance. Most existing models rely on superficial correlations such as clothing, background, or lighting that fail to generalize across domains, viewpoints, and temporal variations. This survey examines the emerging role of causal reasoning as a principled alternative to traditional correlation-based approaches in video-based Re-ID. We provide a structured and critical analysis of methods that leverage structural causal models, interventions, and counterfactual reasoning to isolate identity-specific features from confounding factors. The survey is organized around a novel taxonomy of causal Re-ID methods that spans generative disentanglement, domain-invariant modeling, and causal transformers. We review current evaluation metrics and introduce causal-specific robustness measures. In addition, we assess practical challenges of scalability, fairness, interpretability, and privacy that must be addressed for real-world adoption. Finally, we identify open problems and outline future research directions that integrate causal modeling with efficient architectures and self-supervised learning. This survey aims to establish a coherent foundation for causal video-based person Re-ID and to catalyze the next phase of research in this rapidly evolving domain.</li>
</ul>

<h3>Title: Paths Not Taken: Understanding and Mending the Multilingual Factual Recall Pipeline</h3>
<ul>
<li><strong>Authors: </strong>Meng Lu, Ruochen Zhang, Ellie Pavlick, Carsten Eickhoff</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20546">https://arxiv.org/abs/2505.20546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20546">https://arxiv.org/pdf/2505.20546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20546]] Paths Not Taken: Understanding and Mending the Multilingual Factual Recall Pipeline(https://arxiv.org/abs/2505.20546)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multilingual large language models (LLMs) often exhibit factual inconsistencies across languages, with significantly better performance in factual recall tasks in English than in other languages. The causes of these failures, however, remain poorly understood. Using mechanistic analysis techniques, we uncover the underlying pipeline that LLMs employ, which involves using the English-centric factual recall mechanism to process multilingual queries and then translating English answers back into the target language. We identify two primary sources of error: insufficient engagement of the reliable English-centric mechanism for factual recall, and incorrect translation from English back into the target language for the final answer. To address these vulnerabilities, we introduce two vector interventions, both independent of languages and datasets, to redirect the model toward better internal paths for higher factual consistency. Our interventions combined increase the recall accuracy by over 35 percent for the lowest-performing language. Our findings demonstrate how mechanistic insights can be used to unlock latent multilingual capabilities in LLMs.</li>
</ul>

<h3>Title: Learning a Pessimistic Reward Model in RLHF</h3>
<ul>
<li><strong>Authors: </strong>Yinglun Xu, Hangoo Kang, Tarun Suresh, Yuxuan Wan, Gagandeep Singh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20556">https://arxiv.org/abs/2505.20556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20556">https://arxiv.org/pdf/2505.20556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20556]] Learning a Pessimistic Reward Model in RLHF(https://arxiv.org/abs/2505.20556)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This work proposes `PET', a novel pessimistic reward fine-tuning method, to learn a pessimistic reward model robust against reward hacking in offline reinforcement learning from human feedback (RLHF). Traditional reward modeling techniques in RLHF train an imperfect reward model, on which a KL regularization plays a pivotal role in mitigating reward hacking when optimizing a policy. Such an intuition-based method still suffers from reward hacking, and the policies with large KL divergence from the dataset distribution are excluded during learning. In contrast, we show that when optimizing a policy on a pessimistic reward model fine-tuned through PET, reward hacking can be prevented without relying on any regularization. We test our methods on the standard TL;DR summarization dataset. We find that one can learn a high-quality policy on our pessimistic reward without using any regularization. Such a policy has a high KL divergence from the dataset distribution while having high performance in practice. In summary, our work shows the feasibility of learning a pessimistic reward model against reward hacking. The agent can greedily search for the policy with a high pessimistic reward without suffering from reward hacking.</li>
</ul>

<h3>Title: Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Shenao Zhang, Yaqing Wang, Yinxiao Liu, Tianqi Liu, Peter Grabowski, Eugene Ie, Zhaoran Wang, Yunxuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20561">https://arxiv.org/abs/2505.20561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20561">https://arxiv.org/pdf/2505.20561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20561]] Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning(https://arxiv.org/abs/2505.20561)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under a posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with improved exploration effectiveness. Our code is available at this https URL.</li>
</ul>

<h3>Title: The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages</h3>
<ul>
<li><strong>Authors: </strong>Chris Emezue, The NaijaVoices Community, Busayo Awobade, Abraham Owodunni, Handel Emezue, Gloria Monica Tobechukwu Emezue, Nefertiti Nneoma Emezue, Sewade Ogun, Bunmi Akinremi, David Ifeoluwa Adelani, Chris Pal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20564">https://arxiv.org/abs/2505.20564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20564">https://arxiv.org/pdf/2505.20564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20564]] The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages(https://arxiv.org/abs/2505.20564)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The development of high-performing, robust, and reliable speech technologies depends on large, high-quality datasets. However, African languages -- including our focus, Igbo, Hausa, and Yoruba -- remain under-represented due to insufficient data. Popular voice-enabled technologies do not support any of the 2000+ African languages, limiting accessibility for circa one billion people. While previous dataset efforts exist for the target languages, they lack the scale and diversity needed for robust speech models. To bridge this gap, we introduce the NaijaVoices dataset, a 1,800-hour speech-text dataset with 5,000+ speakers. We outline our unique data collection approach, analyze its acoustic diversity, and demonstrate its impact through finetuning experiments on automatic speech recognition, averagely achieving 75.86% (Whisper), 52.06% (MMS), and 42.33% (XLSR) WER improvements. These results highlight NaijaVoices' potential to advance multilingual speech processing for African languages.</li>
</ul>

<h3>Title: Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via Constrained RL</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Chen, Shihao Ma, Runsheng Lin, Jiecong Lin, Bo Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20578">https://arxiv.org/abs/2505.20578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20578">https://arxiv.org/pdf/2505.20578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20578]] Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via Constrained RL(https://arxiv.org/abs/2505.20578)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Designing regulatory DNA sequences that achieve precise cell-type-specific gene expression is crucial for advancements in synthetic biology, gene therapy and precision medicine. Although transformer-based language models (LMs) can effectively capture patterns in regulatory DNA, their generative approaches often struggle to produce novel sequences with reliable cell-specific activity. Here, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL) framework tailored for designing regulatory DNA sequences with controllable cell-type specificity. By formulating regulatory sequence design as a biologically informed constrained optimization problem, we apply RL to autoregressive genomic LMs, enabling the models to iteratively refine sequences that maximize regulatory activity in targeted cell types while constraining off-target effects. Our evaluation on human promoters and enhancers demonstrates that Ctrl-DNA consistently outperforms existing generative and RL-based approaches, generating high-fitness regulatory sequences and achieving state-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences capture key cell-type-specific transcription factor binding sites (TFBS), short DNA motifs recognized by regulatory proteins that control gene expression, demonstrating the biological plausibility of the generated sequences.</li>
</ul>

<h3>Title: Effectiveness of Prompt Optimization in NL2SQL Systems</h3>
<ul>
<li><strong>Authors: </strong>Sairam Gurajada, Eser Kandogan, Sajjadur Rahman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20591">https://arxiv.org/abs/2505.20591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20591">https://arxiv.org/pdf/2505.20591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20591]] Effectiveness of Prompt Optimization in NL2SQL Systems(https://arxiv.org/abs/2505.20591)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>NL2SQL approaches have greatly benefited from the impressive capabilities of large language models (LLMs). In particular, bootstrapping an NL2SQL system for a specific domain can be as simple as instructing an LLM with sufficient contextual information, such as schema details and translation demonstrations. However, building an accurate system still requires the rigorous task of selecting the right context for each query-including identifying relevant schema elements, cell values, and suitable exemplars that help the LLM understand domain-specific nuances. Retrieval-based methods have become the go-to approach for identifying such context. While effective, these methods introduce additional inference-time costs due to the retrieval process. In this paper, we argue that production scenarios demand high-precision, high-performance NL2SQL systems, rather than simply high-quality SQL generation, which is the focus of most current NL2SQL approaches. In such scenarios, the careful selection of a static set of exemplars-capturing the intricacies of the query log, target database, SQL constructs, and execution latencies-plays a more crucial role than exemplar selection based solely on similarity. The key challenge, however, lies in identifying a representative set of exemplars for a given production setting. To this end, we propose a prompt optimization framework that not only addresses the high-precision requirement but also optimizes the performance of the generated SQL through multi-objective optimization. Preliminary empirical analysis demonstrates the effectiveness of the proposed framework.</li>
</ul>

<h3>Title: Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Dancheng Liu, Amir Nassereldine, Chenhui Xu, Jinjun Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20606">https://arxiv.org/abs/2505.20606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20606">https://arxiv.org/pdf/2505.20606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20606]] Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation(https://arxiv.org/abs/2505.20606)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Whisper's robust performance in automatic speech recognition (ASR) is often attributed to its massive 680k-hour training set, an impractical scale for most researchers. In this work, we examine how linguistic and acoustic diversity in training data affect the robustness of the ASR model and reveal that transcription generalization is primarily driven by acoustic variation rather than linguistic richness. We find that targeted acoustic augmentation methods could significantly improve the generalization ability of ASR models, reducing word-error rates by up to 19.24 percent on unseen datasets when training on the 960-hour Librispeech dataset. These findings highlight strategic acoustically focused data augmentation as a promising alternative to massive datasets for building robust ASR models, offering a potential solution to future foundation ASR models when massive human speech data is lacking.</li>
</ul>

<h3>Title: OmniIndoor3D: Comprehensive Indoor 3D Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Xiaobao Wei, Xiaoan Zhang, Hao Wang, Qingpo Wuwu, Ming Lu, Wenzhao Zheng, Shanghang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20610">https://arxiv.org/abs/2505.20610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20610">https://arxiv.org/pdf/2505.20610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20610]] OmniIndoor3D: Comprehensive Indoor 3D Reconstruction(https://arxiv.org/abs/2505.20610)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose a novel framework for comprehensive indoor 3D reconstruction using Gaussian representations, called OmniIndoor3D. This framework enables accurate appearance, geometry, and panoptic reconstruction of diverse indoor scenes captured by a consumer-level RGB-D camera. Since 3DGS is primarily optimized for photorealistic rendering, it lacks the precise geometry critical for high-quality panoptic reconstruction. Therefore, OmniIndoor3D first combines multiple RGB-D images to create a coarse 3D reconstruction, which is then used to initialize the 3D Gaussians and guide the 3DGS training. To decouple the optimization conflict between appearance and geometry, we introduce a lightweight MLP that adjusts the geometric properties of 3D Gaussians. The introduced lightweight MLP serves as a low-pass filter for geometry reconstruction and significantly reduces noise in indoor scenes. To improve the distribution of Gaussian primitives, we propose a densification strategy guided by panoptic priors to encourage smoothness on planar surfaces. Through the joint optimization of appearance, geometry, and panoptic reconstruction, OmniIndoor3D provides comprehensive 3D indoor scene understanding, which facilitates accurate and robust robotic navigation. We perform thorough evaluations across multiple datasets, and OmniIndoor3D achieves state-of-the-art results in appearance, geometry, and panoptic reconstruction. We believe our work bridges a critical gap in indoor 3D reconstruction. The code will be released at: this https URL</li>
</ul>

<h3>Title: Mamba-Driven Topology Fusion for Monocular 3-D Human Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zenghao Zheng, Lianping Yang, Jinshan Pan, Hegui Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20611">https://arxiv.org/abs/2505.20611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20611">https://arxiv.org/pdf/2505.20611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20611]] Mamba-Driven Topology Fusion for Monocular 3-D Human Pose Estimation(https://arxiv.org/abs/2505.20611)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based methods for 3-D human pose estimation face significant computational challenges due to the quadratic growth of self-attention mechanism complexity with sequence length. Recently, the Mamba model has substantially reduced computational overhead and demonstrated outstanding performance in modeling long sequences by leveraging state space model (SSM). However, the ability of SSM to process sequential data is not suitable for 3-D joint sequences with topological structures, and the causal convolution structure in Mamba also lacks insight into local joint relationships. To address these issues, we propose the Mamba-Driven Topology Fusion framework in this paper. Specifically, the proposed Bone Aware Module infers the direction and length of bone vectors in the spherical coordinate system, providing effective topological guidance for the Mamba model in processing joint sequences. Furthermore, we enhance the convolutional structure within the Mamba model by integrating forward and backward graph convolutional network, enabling it to better capture local joint dependencies. Finally, we design a Spatiotemporal Refinement Module to model both temporal and spatial relationships within the sequence. Through the incorporation of skeletal topology, our approach effectively alleviates Mamba's limitations in capturing human structural relationships. We conduct extensive experiments on the Human3.6M and MPI-INF-3DHP datasets for testing and comparison, and the results show that the proposed method greatly reduces computational cost while achieving higher accuracy. Ablation studies further demonstrate the effectiveness of each proposed module. The code and models will be released.</li>
</ul>

<h3>Title: REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Ziju Shen, Naohao Huang, Fanyi Yang, Yutong Wang, Guoxiong Gao, Tianyi Xu, Jiedong Jiang, Wanyi He, Pu Yang, Mengzhou Sun, Haocheng Ju, Peihao Wu, Bryan Dai, Bin Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20613">https://arxiv.org/abs/2505.20613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20613">https://arxiv.org/pdf/2505.20613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20613]] REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning(https://arxiv.org/abs/2505.20613)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Nowadays, formal theorem provers have made monumental progress on high-school and competition-level mathematics, but few of them generalize to more advanced mathematics. In this paper, we present REAL-Prover, a new open-source stepwise theorem prover for Lean 4 to push this boundary. This prover, based on our fine-tuned large language model (REAL-Prover-v1) and integrated with a retrieval system (Leansearch-PS), notably boosts performance on solving college-level mathematics problems. To train REAL-Prover-v1, we developed HERALD-AF, a data extraction pipeline that converts natural language math problems into formal statements, and a new open-source Lean 4 interactive environment (Jixia-interactive) to facilitate synthesis data collection. In our experiments, our prover using only supervised fine-tune achieves competitive results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable to state-of-the-art (SOTA) models. To further evaluate our approach, we introduce FATE-M, a new benchmark focused on algebraic problems, where our prover achieves a SOTA success rate of 56.7% (Pass@64).</li>
</ul>

<h3>Title: EarthOL: A Proof-of-Human-Contribution Consensus Protocol -- Addressing Fundamental Challenges in Decentralized Value Assessment with Enhanced Verification and Security Mechanisms</h3>
<ul>
<li><strong>Authors: </strong>Jiaxiong He</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20614">https://arxiv.org/abs/2505.20614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20614">https://arxiv.org/pdf/2505.20614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20614]] EarthOL: A Proof-of-Human-Contribution Consensus Protocol -- Addressing Fundamental Challenges in Decentralized Value Assessment with Enhanced Verification and Security Mechanisms(https://arxiv.org/abs/2505.20614)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>This paper introduces EarthOL, a novel consensus protocol that attempts to replace computational waste in blockchain systems with verifiable human contributions within bounded domains. While recognizing the fundamental impossibility of universal value assessment, we propose a domain-restricted approach that acknowledges cultural diversity and subjective preferences while maintaining cryptographic security. Our enhanced Proof-of-Human-Contribution (PoHC) protocol uses a multi-layered verification system with domain-specific evaluation criteria, time-dependent validation mechanisms, and comprehensive security frameworks. We present theoretical analysis demonstrating meaningful progress toward incentive-compatible human contribution verification in high-consensus domains, achieving Byzantine fault tolerance in controlled scenarios while addressing significant scalability and cultural bias challenges. Through game-theoretic analysis, probabilistic modeling, and enhanced security protocols, we identify specific conditions under which the protocol remains stable and examine failure modes with comprehensive mitigation strategies. This work contributes to understanding the boundaries of decentralized value assessment and provides a framework for future research in human-centered consensus mechanisms for specific application domains, with particular emphasis on validator and security specialist incentive systems.</li>
</ul>

<h3>Title: Intelligent Incident Hypertension Prediction in Obstructive Sleep Apnea</h3>
<ul>
<li><strong>Authors: </strong>Omid Halimi Milani, Ahmet Enis Cetin, Bharati Prasad</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20615">https://arxiv.org/abs/2505.20615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20615">https://arxiv.org/pdf/2505.20615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20615]] Intelligent Incident Hypertension Prediction in Obstructive Sleep Apnea(https://arxiv.org/abs/2505.20615)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Obstructive sleep apnea (OSA) is a significant risk factor for hypertension, primarily due to intermittent hypoxia and sleep fragmentation. Predicting whether individuals with OSA will develop hypertension within five years remains a complex challenge. This study introduces a novel deep learning approach that integrates Discrete Cosine Transform (DCT)-based transfer learning to enhance prediction accuracy. We are the first to incorporate all polysomnography signals together for hypertension prediction, leveraging their collective information to improve model performance. Features were extracted from these signals and transformed into a 2D representation to utilize pre-trained 2D neural networks such as MobileNet, EfficientNet, and ResNet variants. To further improve feature learning, we introduced a DCT layer, which transforms input features into a frequency-based representation, preserving essential spectral information, decorrelating features, and enhancing robustness to noise. This frequency-domain approach, coupled with transfer learning, is especially beneficial for limited medical datasets, as it leverages rich representations from pre-trained networks to improve generalization. By strategically placing the DCT layer at deeper truncation depths within EfficientNet, our model achieved a best area under the curve (AUC) of 72.88%, demonstrating the effectiveness of frequency-domain feature extraction and transfer learning in predicting hypertension risk in OSA patients over a five-year period.</li>
</ul>

<h3>Title: Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Shijie Liu, Andrew C. Cullen, Paul Montague, Sarah Erfani, Benjamin I. P. Rubinstein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20621">https://arxiv.org/abs/2505.20621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20621">https://arxiv.org/pdf/2505.20621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20621]] Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning(https://arxiv.org/abs/2505.20621)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Similar to other machine learning frameworks, Offline Reinforcement Learning (RL) is shown to be vulnerable to poisoning attacks, due to its reliance on externally sourced datasets, a vulnerability that is exacerbated by its sequential nature. To mitigate the risks posed by RL poisoning, we extend certified defenses to provide larger guarantees against adversarial manipulation, ensuring robustness for both per-state actions, and the overall expected cumulative reward. Our approach leverages properties of Differential Privacy, in a manner that allows this work to span both continuous and discrete spaces, as well as stochastic and deterministic environments -- significantly expanding the scope and applicability of achievable guarantees. Empirical evaluations demonstrate that our approach ensures the performance drops to no more than $50\%$ with up to $7\%$ of the training data poisoned, significantly improving over the $0.008\%$ in prior work~\citep{wu_copa_2022}, while producing certified radii that is $5$ times larger as well. This highlights the potential of our framework to enhance safety and reliability in offline RL.</li>
</ul>

<h3>Title: POLAR: A Benchmark for Multilingual, Multicultural, and Multi-Event Online Polarization</h3>
<ul>
<li><strong>Authors: </strong>Usman Naseem, Juan Ren, Saba Anwar, Sarah Kohail, Rudy Alexandro Garrido Veliz, Robert Geislinger, Aisha Jabr, Idris Abdulmumin, Laiba Qureshi, Aarushi Ajay Borkar, Maryam Ibrahim Mukhtar, Abinew Ali Ayele, Ibrahim Said Ahmad, Adem Ali, Martin Semmann, Shamsuddeen Hassan Muhammad, Seid Muhie Yimam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20624">https://arxiv.org/abs/2505.20624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20624">https://arxiv.org/pdf/2505.20624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20624]] POLAR: A Benchmark for Multilingual, Multicultural, and Multi-Event Online Polarization(https://arxiv.org/abs/2505.20624)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Online polarization poses a growing challenge for democratic discourse, yet most computational social science research remains monolingual, culturally narrow, or event-specific. We introduce POLAR, a multilingual, multicultural, and multievent dataset with over 23k instances in seven languages from diverse online platforms and real-world events. Polarization is annotated along three axes: presence, type, and manifestation, using a variety of annotation platforms adapted to each cultural context. We conduct two main experiments: (1) we fine-tune six multilingual pretrained language models in both monolingual and cross-lingual setups; and (2) we evaluate a range of open and closed large language models (LLMs) in few-shot and zero-shot scenarios. Results show that while most models perform well on binary polarization detection, they achieve substantially lower scores when predicting polarization types and manifestations. These findings highlight the complex, highly contextual nature of polarization and the need for robust, adaptable approaches in NLP and computational social science. All resources will be released to support further research and effective mitigation of digital polarization globally.</li>
</ul>

<h3>Title: Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Sibo Xiao, Zixin Lin, Wenyang Gao, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20625">https://arxiv.org/abs/2505.20625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20625">https://arxiv.org/pdf/2505.20625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20625]] Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration(https://arxiv.org/abs/2505.20625)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Processing long contexts has become a critical capability for modern large language models (LLMs). Existing works leverage agent-based divide-and-conquer methods for processing long contexts. But these methods face crucial limitations, including prohibitive accumulated latency and amplified information loss from excessive agent invocations, and the disruption of inherent textual dependencies by immoderate partitioning. In this paper, we propose a novel multi-agent framework XpandA (Expand-Agent) coupled with question-driven workflow and dynamic partitioning for robust long-context processing. XpandA overcomes these limitations through: 1) dynamic partitioning of long texts, which adaptively modulates the filling rate of context windows for input sequences of vastly varying lengths; 2) question-guided protocol to update flat information ensembles within centralized shared memory, constructing consistent inter-agent knowledge across partitions; and 3) selectively replaying specific partitions based on the state-tracking of question-information couples to promote the resolution of inverted-order structures across partitions (e.g., flashbacks). We perform a comprehensive evaluation of XpandA on multiple long-context benchmarks with length varying from 1k to 1M, demonstrating XpandA's feasibility for processing ultra-long sequences and its significant effectiveness in enhancing the long-context capabilities of various LLMs by achieving 20\% improvements and 1.5x inference speedup over baselines of full-context, RAG and previous agent-based methods.</li>
</ul>

<h3>Title: Incorporating Flexible Image Conditioning into Text-to-Video Diffusion Models without Training</h3>
<ul>
<li><strong>Authors: </strong>Bolin Lai, Sangmin Lee, Xu Cao, Xiang Li, James M. Rehg</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20629">https://arxiv.org/abs/2505.20629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20629">https://arxiv.org/pdf/2505.20629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20629]] Incorporating Flexible Image Conditioning into Text-to-Video Diffusion Models without Training(https://arxiv.org/abs/2505.20629)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-image-to-video (TI2V) generation is a critical problem for controllable video generation using both semantic and visual conditions. Most existing methods typically add visual conditions to text-to-video (T2V) foundation models by finetuning, which is costly in resources and only limited to a few predefined conditioning settings. To tackle this issue, we introduce a unified formulation for TI2V generation with flexible visual conditioning. Furthermore, we propose an innovative training-free approach, dubbed FlexTI2V, that can condition T2V foundation models on an arbitrary amount of images at arbitrary positions. Specifically, we firstly invert the condition images to noisy representation in a latent space. Then, in the denoising process of T2V models, our method uses a novel random patch swapping strategy to incorporate visual features into video representations through local image patches. To balance creativity and fidelity, we use a dynamic control mechanism to adjust the strength of visual conditioning to each video frame. Extensive experiments validate that our method surpasses previous training-free image conditioning methods by a notable margin. We also show more insights of our method by detailed ablation study and analysis.</li>
</ul>

<h3>Title: Test-Time Learning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jinwu Hu, Zhitian Zhang, Guohao Chen, Xutao Wen, Chao Shuai, Wei Luo, Bin Xiao, Yuanqing Li, Mingkui Tan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20633">https://arxiv.org/abs/2505.20633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20633">https://arxiv.org/pdf/2505.20633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20633]] Test-Time Learning for Large Language Models(https://arxiv.org/abs/2505.20633)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) have exhibited remarkable emergent capabilities through extensive pre-training, they still face critical limitations in generalizing to specialized domains and handling diverse linguistic variations, known as distribution shifts. In this paper, we propose a Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically adapts LLMs to target domains using only unlabeled test data during testing. Specifically, we first provide empirical evidence and theoretical insights to reveal that more accurate predictions from LLMs can be achieved by minimizing the input perplexity of the unlabeled test data. Based on this insight, we formulate the Test-Time Learning process of LLMs as input perplexity minimization, enabling self-supervised enhancement of LLM performance. Furthermore, we observe that high-perplexity samples tend to be more informative for model optimization. Accordingly, we introduce a Sample Efficient Learning Strategy that actively selects and emphasizes these high-perplexity samples for test-time updates. Lastly, to mitigate catastrophic forgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA) instead of full-parameter optimization, which allows lightweight model updates while preserving more original knowledge from the model. We introduce the AdaptEval benchmark for TTL and demonstrate through experiments that TLM improves performance by at least 20% compared to original LLMs on domain knowledge adaptation.</li>
</ul>

<h3>Title: TrustSkin: A Fairness Pipeline for Trustworthy Facial Affect Analysis Across Skin Tone</h3>
<ul>
<li><strong>Authors: </strong>Ana M. Cabanas, Alma Pedro, Domingo Mery</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20637">https://arxiv.org/abs/2505.20637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20637">https://arxiv.org/pdf/2505.20637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20637]] TrustSkin: A Fairness Pipeline for Trustworthy Facial Affect Analysis Across Skin Tone(https://arxiv.org/abs/2505.20637)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability</a></li>
<li><strong>Abstract: </strong>Understanding how facial affect analysis (FAA) systems perform across different demographic groups requires reliable measurement of sensitive attributes such as ancestry, often approximated by skin tone, which itself is highly influenced by lighting conditions. This study compares two objective skin tone classification methods: the widely used Individual Typology Angle (ITA) and a perceptually grounded alternative based on Lightness ($L^*$) and Hue ($H^*$). Using AffectNet and a MobileNet-based model, we assess fairness across skin tone groups defined by each method. Results reveal a severe underrepresentation of dark skin tones ($\sim 2 \%$), alongside fairness disparities in F1-score (up to 0.08) and TPR (up to 0.11) across groups. While ITA shows limitations due to its sensitivity to lighting, the $H^*$-$L^*$ method yields more consistent subgrouping and enables clearer diagnostics through metrics such as Equal Opportunity. Grad-CAM analysis further highlights differences in model attention patterns by skin tone, suggesting variation in feature encoding. To support future mitigation efforts, we also propose a modular fairness-aware pipeline that integrates perceptual skin tone estimation, model interpretability, and fairness evaluation. These findings emphasize the relevance of skin tone measurement choices in fairness assessment and suggest that ITA-based evaluations may overlook disparities affecting darker-skinned individuals.</li>
</ul>

<h3>Title: IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Yifan Li, Yuhang Chen, Anh Dao, Lichi Li, Zhongyi Cai, Zhen Tan, Tianlong Chen, Yu Kong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20640">https://arxiv.org/abs/2505.20640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20640">https://arxiv.org/pdf/2505.20640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20640]] IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios(https://arxiv.org/abs/2505.20640)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing Embodied Question Answering (EQA) benchmarks primarily focus on household environments, often overlooking safety-critical aspects and reasoning processes pertinent to industrial settings. This drawback limits the evaluation of agent readiness for real-world industrial applications. To bridge this, we introduce IndustryEQA, the first benchmark dedicated to evaluating embodied agent capabilities within safety-critical warehouse scenarios. Built upon the NVIDIA Isaac Sim platform, IndustryEQA provides high-fidelity episodic memory videos featuring diverse industrial assets, dynamic human agents, and carefully designed hazardous situations inspired by real-world safety guidelines. The benchmark includes rich annotations covering six categories: equipment safety, human safety, object recognition, attribute recognition, temporal understanding, and spatial understanding. Besides, it also provides extra reasoning evaluation based on these categories. Specifically, it comprises 971 question-answer pairs generated from small warehouse and 373 pairs from large ones, incorporating scenarios with and without human. We further propose a comprehensive evaluation framework, including various baseline models, to assess their general perception and reasoning abilities in industrial environments. IndustryEQA aims to steer EQA research towards developing more robust, safety-aware, and practically applicable embodied agents for complex industrial environments. Benchmark and codes are available.</li>
</ul>

<h3>Title: Can Past Experience Accelerate LLM Reasoning?</h3>
<ul>
<li><strong>Authors: </strong>Bo Pan, Liang Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20643">https://arxiv.org/abs/2505.20643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20643">https://arxiv.org/pdf/2505.20643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20643]] Can Past Experience Accelerate LLM Reasoning?(https://arxiv.org/abs/2505.20643)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Allocating more compute to large language models (LLMs) reasoning has generally been demonstrated to improve their effectiveness, but also results in increased inference time. In contrast, humans can perform tasks faster and better with increased experience and exposure. Hence, this paper aims to investigate the question: Can LLMs also become faster at reasoning through recurrent exposure on relevant tasks, and if so, how can it be achieved? To address these questions, we first formalize the problem setting of LLM reasoning speedup systematically in the dimensions of task relevancy and compute budget calculation. We then propose SpeedupLLM, a theoretically guaranteed framework to implement and benchmark such reasoning speedup behaviour based on adaptive compute allocation and memory mechanisms. We further conduct comprehensive experiments to benchmark such behaviour across different question similarity levels, memory methods, and reasoning methods. Results show that LLMs can generally reason faster with past experience, achieving up to a 56% reduction in compute cost when equipped with appropriate memory and reasoning methods.</li>
</ul>

<h3>Title: STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kai Chen, Zihao He, Taiwei Shi, Kristina Lerman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20645">https://arxiv.org/abs/2505.20645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20645">https://arxiv.org/pdf/2505.20645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20645]] STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models(https://arxiv.org/abs/2505.20645)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Steerability, or the ability of large language models (LLMs) to adapt outputs to align with diverse community-specific norms, perspectives, and communication styles, is critical for real-world applications but remains under-evaluated. We introduce Steer-Bench, a benchmark for assessing population-specific steering using contrasting Reddit communities. Covering 30 contrasting subreddit pairs across 19 domains, Steer-Bench includes over 10,000 instruction-response pairs and validated 5,500 multiple-choice question with corresponding silver labels to test alignment with diverse community norms. Our evaluation of 13 popular LLMs using Steer-Bench reveals that while human experts achieve an accuracy of 81% with silver labels, the best-performing models reach only around 65% accuracy depending on the domain and configuration. Some models lag behind human-level alignment by over 15 percentage points, highlighting significant gaps in community-sensitive steerability. Steer-Bench is a benchmark to systematically assess how effectively LLMs understand community-specific instructions, their resilience to adversarial steering attempts, and their ability to accurately represent diverse cultural and ideological perspectives.</li>
</ul>

<h3>Title: Voronoi-grid-based Pareto Front Learning and Its Application to Collaborative Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Mengmeng Chen, Xiaohu Wu, Qiqi Liu, Tiantian He, Yew-Soon Ong, Yaochu Jin, Qicheng Lao, Han Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20648">https://arxiv.org/abs/2505.20648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20648">https://arxiv.org/pdf/2505.20648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20648]] Voronoi-grid-based Pareto Front Learning and Its Application to Collaborative Federated Learning(https://arxiv.org/abs/2505.20648)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Multi-objective optimization (MOO) exists extensively in machine learning, and aims to find a set of Pareto-optimal solutions, called the Pareto front, e.g., it is fundamental for multiple avenues of research in federated learning (FL). Pareto-Front Learning (PFL) is a powerful method implemented using Hypernetworks (PHNs) to approximate the Pareto front. This method enables the acquisition of a mapping function from a given preference vector to the solutions on the Pareto front. However, most existing PFL approaches still face two challenges: (a) sampling rays in high-dimensional spaces; (b) failing to cover the entire Pareto Front which has a convex shape. Here, we introduce a novel PFL framework, called as PHN-HVVS, which decomposes the design space into Voronoi grids and deploys a genetic algorithm (GA) for Voronoi grid partitioning within high-dimensional space. We put forward a new loss function, which effectively contributes to more extensive coverage of the resultant Pareto front and maximizes the HV Indicator. Experimental results on multiple MOO machine learning tasks demonstrate that PHN-HVVS outperforms the baselines significantly in generating Pareto front. Also, we illustrate that PHN-HVVS advances the methodologies of several recent problems in the FL field. The code is available at this https URL}{this https URL.</li>
</ul>

<h3>Title: FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information</h3>
<ul>
<li><strong>Authors: </strong>Yan Wang, Yang Ren, Lingfei Qian, Xueqing Peng, Keyi Wang, Yi Han, Dongji Feng, Xiao-Yang Liu, Jimin Huang, Qianqian Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20650">https://arxiv.org/abs/2505.20650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20650">https://arxiv.org/pdf/2505.20650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20650]] FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information(https://arxiv.org/abs/2505.20650)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>We introduce FinTagging, the first full-scope, table-aware XBRL benchmark designed to evaluate the structured information extraction and semantic alignment capabilities of large language models (LLMs) in the context of XBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL tagging as flat multi-class classification and focus solely on narrative text, FinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for financial entity extraction and FinCL for taxonomy-driven concept alignment. It requires models to jointly extract facts and align them with the full 10k+ US-GAAP taxonomy across both unstructured text and structured tables, enabling realistic, fine-grained evaluation. We assess a diverse set of LLMs under zero-shot settings, systematically analyzing their performance on both subtasks and overall tagging accuracy. Our results reveal that, while LLMs demonstrate strong generalization in information extraction, they struggle with fine-grained concept alignment, particularly in disambiguating closely related taxonomy entries. These findings highlight the limitations of existing LLMs in fully automating XBRL tagging and underscore the need for improved semantic reasoning and schema-aware modeling to meet the demands of accurate financial disclosure. Code is available at our GitHub repository and data is at our Hugging Face repository.</li>
</ul>

<h3>Title: RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment</h3>
<ul>
<li><strong>Authors: </strong>Lingyu Qiu, Ke Jiang, Xiaoyang Tan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20653">https://arxiv.org/abs/2505.20653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20653">https://arxiv.org/pdf/2505.20653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20653]] RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment(https://arxiv.org/abs/2505.20653)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advancements in domain generalization for deepfake detection have attracted significant attention, with previous methods often incorporating additional modules to prevent overfitting to domain-specific patterns. However, such regularization can hinder the optimization of the empirical risk minimization (ERM) objective, ultimately degrading model performance. In this paper, we propose a novel learning objective that aligns generalization gradient updates with ERM gradient updates. The key innovation is the application of perturbations to model parameters, aligning the ascending points across domains, which specifically enhances the robustness of deepfake detection models to domain shifts. This approach effectively preserves domain-invariant features while managing domain-specific characteristics, without introducing additional regularization. Experimental results on multiple challenging deepfake detection datasets demonstrate that our gradient alignment strategy outperforms state-of-the-art domain generalization techniques, confirming the efficacy of our method. The code is available at this https URL.</li>
</ul>

<h3>Title: Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Yue Fang, Zhi Jin, Jie An, Hongshen Chen, Xiaohong Chen, Naijun Zhan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20658">https://arxiv.org/abs/2505.20658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20658">https://arxiv.org/pdf/2505.20658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20658]] Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge(https://arxiv.org/abs/2505.20658)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise formal specification, making it widely used in cyber-physical systems such as autonomous driving and robotics. Automatically transforming NL into STL is an attractive approach to overcome the limitations of manual transformation, which is time-consuming and error-prone. However, due to the lack of datasets, automatic transformation currently faces significant challenges and has not been fully explored. In this paper, we propose an NL-STL dataset named STL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched with diverse patterns. To develop the dataset, we first manually create a small-scale seed set of NL-STL pairs. Next, representative examples are identified through clustering and used to guide large language models (LLMs) in generating additional NL-STL pairs. Finally, diversity and accuracy are ensured through rigorous rule-based filters and human validation. Furthermore, we introduce the Knowledge-Guided STL Transformation (KGST) framework, a novel approach for transforming natural language into STL, involving a generate-then-refine process based on external knowledge. Statistical analysis shows that the STL-DivEn dataset exhibits more diversity than the existing NL-STL dataset. Moreover, both metric-based and human evaluations indicate that our KGST approach outperforms baseline models in transformation accuracy on STL-DivEn and DeepSTL datasets.</li>
</ul>

<h3>Title: An Optimisation Framework for Unsupervised Environment Design</h3>
<ul>
<li><strong>Authors: </strong>Nathan Monette, Alistair Letcher, Michael Beukman, Matthew T. Jackson, Alexander Rutherford, Alexander D. Goldie, Jakob N. Foerster</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20659">https://arxiv.org/abs/2505.20659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20659">https://arxiv.org/pdf/2505.20659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20659]] An Optimisation Framework for Unsupervised Environment Design(https://arxiv.org/abs/2505.20659)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>For reinforcement learning agents to be deployed in high-risk settings, they must achieve a high level of robustness to unfamiliar scenarios. One method for improving robustness is unsupervised environment design (UED), a suite of methods aiming to maximise an agent's generalisability across configurations of an environment. In this work, we study UED from an optimisation perspective, providing stronger theoretical guarantees for practical settings than prior work. Whereas previous methods relied on guarantees if they reach convergence, our framework employs a nonconvex-strongly-concave objective for which we provide a provably convergent algorithm in the zero-sum setting. We empirically verify the efficacy of our method, outperforming prior methods in a number of environments with varying difficulties.</li>
</ul>

<h3>Title: BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Qinzhuo Wu, Pengzhi Gao, Wei Liu, Jian Luan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20660">https://arxiv.org/abs/2505.20660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20660">https://arxiv.org/pdf/2505.20660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20660]] BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism(https://arxiv.org/abs/2505.20660)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graphical User Interface (GUI) agents have gained substantial attention due to their impressive capabilities to complete tasks through multiple interactions within GUI environments. However, existing agents primarily focus on enhancing the accuracy of individual actions and often lack effective mechanisms for detecting and recovering from errors. To address these shortcomings, we propose the BacktrackAgent, a robust framework that incorporates a backtracking mechanism to improve task completion efficiency. BacktrackAgent includes verifier, judger, and reflector components as modules for error detection and recovery, while also applying judgment rewards to further enhance the agent's performance. Additionally, we develop a training dataset specifically designed for the backtracking mechanism, which considers the outcome pages after action executions. Experimental results show that BacktrackAgent has achieved performance improvements in both task success rate and step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be released upon acceptance.</li>
</ul>

<h3>Title: Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yang He, Xiao Ding, Bibo Cai, Yufei Zhang, Kai Xiong, Zhouhao Sun, Bing Qin, Ting Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20664">https://arxiv.org/abs/2505.20664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20664">https://arxiv.org/pdf/2505.20664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20664]] Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning(https://arxiv.org/abs/2505.20664)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While reasoning-augmented large language models (RLLMs) significantly enhance complex task performance through extended reasoning chains, they inevitably introduce substantial unnecessary token consumption, particularly for simpler problems where Short Chain-of-Thought (Short CoT) suffices. This overthinking phenomenon leads to inefficient resource usage without proportional accuracy gains. To address this issue, we propose Self-Route, a dynamic reasoning framework that automatically selects between general and reasoning modes based on model capability estimation. Our approach introduces a lightweight pre-inference stage to extract capability-aware embeddings from hidden layer representations, enabling real-time evaluation of the model's ability to solve problems. We further construct Gradient-10K, a model difficulty estimation-based dataset with dense complexity sampling, to train the router for precise capability boundary detection. Extensive experiments demonstrate that Self-Route achieves comparable accuracy to reasoning models while reducing token consumption by 30-55\% across diverse benchmarks. The proposed framework demonstrates consistent effectiveness across models with different parameter scales and reasoning paradigms, highlighting its general applicability and practical value.</li>
</ul>

<h3>Title: DriveRX: A Vision-Language Reasoning Model for Cross-Task Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Muxi Diao, Lele Yang, Hongbo Yin, Zhexu Wang, Yejie Wang, Daxin Tian, Kongming Liang, Zhanyu Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20665">https://arxiv.org/abs/2505.20665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20665">https://arxiv.org/pdf/2505.20665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20665]] DriveRX: A Vision-Language Reasoning Model for Cross-Task Autonomous Driving(https://arxiv.org/abs/2505.20665)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Autonomous driving requires real-time, robust reasoning across perception, prediction, planning, and behavior. However, conventional end-to-end models fail to generalize in complex scenarios due to the lack of structured reasoning. Recent vision-language models (VLMs) have been applied to driving tasks, but they typically rely on isolated modules and static supervision, limiting their ability to support multi-stage decision-making. We present AutoDriveRL, a unified training framework that formulates autonomous driving as a structured reasoning process over four core tasks. Each task is independently modeled as a vision-language question-answering problem and optimized using task-specific reward models, enabling fine-grained reinforcement signals at different reasoning stages. Within this framework, we train DriveRX, a cross-task reasoning VLM designed for real-time decision-making. DriveRX achieves strong performance on a public benchmark, outperforming GPT-4o in behavior reasoning and demonstrating robustness under complex or corrupted driving conditions. Our analysis further highlights the impact of vision encoder design and reward-guided reasoning compression. We will release the AutoDriveRL framework and the DriveRX model to support future research.</li>
</ul>

<h3>Title: Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence Transformers</h3>
<ul>
<li><strong>Authors: </strong>Yukun Zhang, Xueqing Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20666">https://arxiv.org/abs/2505.20666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20666">https://arxiv.org/pdf/2505.20666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20666]] Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence Transformers(https://arxiv.org/abs/2505.20666)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We propose a novel framework, Continuous_Time Attention, which infuses partial differential equations (PDEs) into the Transformer's attention mechanism to address the challenges of extremely long input sequences. Instead of relying solely on a static attention matrix, we allow attention weights to evolve over a pseudo_time dimension via diffusion, wave, or reaction_diffusion dynamics. This mechanism systematically smooths local noise, enhances long_range dependencies, and stabilizes gradient flow. Theoretically, our analysis shows that PDE_based attention leads to better optimization landscapes and polynomial rather than exponential decay of distant interactions. Empirically, we benchmark our method on diverse experiments_demonstrating consistent gains over both standard and specialized long sequence Transformer variants. Our findings highlight the potential of PDE_based formulations to enrich attention mechanisms with continuous_time dynamics and global coherence.</li>
</ul>

<h3>Title: Contrastive Desensitization Learning for Cross Domain Face Forgery Detection</h3>
<ul>
<li><strong>Authors: </strong>Lingyu Qiu, Ke Jiang, Xiaoyang Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20675">https://arxiv.org/abs/2505.20675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20675">https://arxiv.org/pdf/2505.20675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20675]] Contrastive Desensitization Learning for Cross Domain Face Forgery Detection(https://arxiv.org/abs/2505.20675)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a new cross-domain face forgery detection method that is insensitive to different and possibly unseen forgery methods while ensuring an acceptable low false positive rate. Although existing face forgery detection methods are applicable to multiple domains to some degree, they often come with a high false positive rate, which can greatly disrupt the usability of the system. To address this issue, we propose an Contrastive Desensitization Network (CDN) based on a robust desensitization algorithm, which captures the essential domain characteristics through learning them from domain transformation over pairs of genuine face images. One advantage of CDN lies in that the learnt face representation is theoretical justified with regard to the its robustness against the domain changes. Extensive experiments over large-scale benchmark datasets demonstrate that our method achieves a much lower false alarm rate with improved detection accuracy compared to several state-of-the-art methods.</li>
</ul>

<h3>Title: Supervised Contrastive Learning for Ordinal Engagement Measurement</h3>
<ul>
<li><strong>Authors: </strong>Sadaf Safa, Ali Abedi, Shehroz S. Khan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20676">https://arxiv.org/abs/2505.20676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20676">https://arxiv.org/pdf/2505.20676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20676]] Supervised Contrastive Learning for Ordinal Engagement Measurement(https://arxiv.org/abs/2505.20676)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Student engagement plays a crucial role in the successful delivery of educational programs. Automated engagement measurement helps instructors monitor student participation, identify disengagement, and adapt their teaching strategies to enhance learning outcomes effectively. This paper identifies two key challenges in this problem: class imbalance and incorporating order into engagement levels rather than treating it as mere categories. Then, a novel approach to video-based student engagement measurement in virtual learning environments is proposed that utilizes supervised contrastive learning for ordinal classification of engagement. Various affective and behavioral features are extracted from video samples and utilized to train ordinal classifiers within a supervised contrastive learning framework (with a sequential classifier as the encoder). A key step involves the application of diverse time-series data augmentation techniques to these feature vectors, enhancing model training. The effectiveness of the proposed method was evaluated using a publicly available dataset for engagement measurement, DAiSEE, containing videos of students who participated in virtual learning programs. The results demonstrate the robust ability of the proposed method for the classification of the engagement level. This approach promises a significant contribution to understanding and enhancing student engagement in virtual learning environments.</li>
</ul>

<h3>Title: SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations</h3>
<ul>
<li><strong>Authors: </strong>Danush Khanna, Pratinav Seth, Sidhaarth Sredharan Murali, Aditya Kumar Guru, Siddharth Shukla, Tanuj Tyagi, Sandeep Chaurasia, Kripabandhu Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20679">https://arxiv.org/abs/2505.20679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20679">https://arxiv.org/pdf/2505.20679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20679]] SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations(https://arxiv.org/abs/2505.20679)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mental manipulation is a subtle yet pervasive form of abuse in interpersonal communication, making its detection critical for safeguarding potential victims. However, due to manipulation's nuanced and context-specific nature, identifying manipulative language in complex, multi-turn, and multi-person conversations remains a significant challenge for large language models (LLMs). To address this gap, we introduce the MultiManip dataset, comprising 220 multi-turn, multi-person dialogues balanced between manipulative and non-manipulative interactions, all drawn from reality shows that mimic real-world scenarios. For manipulative interactions, it includes 11 distinct manipulations depicting real-life scenarios. We conduct extensive evaluations of state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various prompting strategies. Despite their capabilities, these models often struggle to detect manipulation effectively. To overcome this limitation, we propose SELF-PERCEPT, a novel, two-stage prompting framework inspired by Self-Perception Theory, demonstrating strong performance in detecting multi-person, multi-turn mental manipulation. Our code and data are publicly available at this https URL .</li>
</ul>

<h3>Title: Accelerating RL for LLM Reasoning with Optimal Advantage Regression</h3>
<ul>
<li><strong>Authors: </strong>KiantÃ© Brantley, Mingyu Chen, Zhaolin Gao, Jason D. Lee, Wen Sun, Wenhao Zhan, Xuezhou Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20686">https://arxiv.org/abs/2505.20686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20686">https://arxiv.org/pdf/2505.20686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20686]] Accelerating RL for LLM Reasoning with Optimal Advantage Regression(https://arxiv.org/abs/2505.20686)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning large language models (LLMs) to improve complex reasoning abilities. However, state-of-the-art policy optimization methods often suffer from high computational overhead and memory consumption, primarily due to the need for multiple generations per prompt and the reliance on critic networks or advantage estimates of the current policy. In this paper, we propose $A$*-PO, a novel two-stage policy optimization framework that directly approximates the optimal advantage function and enables efficient training of LLMs for reasoning tasks. In the first stage, we leverage offline sampling from a reference policy to estimate the optimal value function $V$*, eliminating the need for costly online value estimation. In the second stage, we perform on-policy updates using a simple least-squares regression loss with only a single generation per prompt. Theoretically, we establish performance guarantees and prove that the KL-regularized RL objective can be optimized without requiring complex exploration strategies. Empirically, $A$*-PO achieves competitive performance across a wide range of mathematical reasoning benchmarks, while reducing training time by up to 2$\times$ and peak memory usage by over 30% compared to PPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at this https URL.</li>
</ul>

<h3>Title: Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages</h3>
<ul>
<li><strong>Authors: </strong>Praveen Srinivasa Varadhan, Srija Anand, Soma Siddhartha, Mitesh M.Khapra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20693">https://arxiv.org/abs/2505.20693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20693">https://arxiv.org/pdf/2505.20693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20693]] Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages(https://arxiv.org/abs/2505.20693)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>What happens when an English Fairytaler is fine-tuned on Indian languages? We evaluate how the English F5-TTS model adapts to 11 Indian languages, measuring polyglot fluency, voice-cloning, style-cloning, and code-mixing. We compare: (i) training from scratch, (ii) fine-tuning English F5 on Indian data, and (iii) fine-tuning on both Indian and English data to prevent forgetting. Fine-tuning with only Indian data proves most effective and the resultant IN-F5 is a near-human polyglot; that enables speakers of one language (e.g., Odia) to fluently speak in another (e.g., Hindi). Our results show English pretraining aids low-resource TTS in reaching human parity. To aid progress in other low-resource languages, we study data-constrained setups and arrive at a compute optimal strategy. Finally, we show IN-F5 can synthesize unseen languages like Bhojpuri and Tulu using a human-in-the-loop approach for zero-resource TTS via synthetic data generation.</li>
</ul>

<h3>Title: Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series</h3>
<ul>
<li><strong>Authors: </strong>Zachary C. Brown, David Carlson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.AP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20697">https://arxiv.org/abs/2505.20697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20697">https://arxiv.org/pdf/2505.20697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20697]] Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series(https://arxiv.org/abs/2505.20697)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The field of hypothesis generation promises to reduce costs in neuroscience by narrowing the range of interventional studies needed to study various phenomena. Existing machine learning methods can generate scientific hypotheses from complex datasets, but many approaches assume causal relationships are static over time, limiting their applicability to systems with dynamic, state-dependent behavior, such as the brain. While some techniques attempt dynamic causal discovery through factor models, they often restrict relationships to linear patterns or impose other simplifying assumptions. We propose a novel method that models dynamic graphs as a conditionally weighted superposition of static graphs, where each static graph can capture nonlinear relationships. This approach enables the detection of complex, time-varying interactions between variables beyond linear limitations. Our method improves f1-scores of predicted dynamic causal patterns by roughly 22-28% on average over baselines in some of our experiments, with some improvements reaching well over 60%. A case study on real brain data demonstrates our method's ability to uncover relationships linked to specific behavioral states, offering valuable insights into neural dynamics.</li>
</ul>

<h3>Title: Sparsified State-Space Models are Efficient Highway Networks</h3>
<ul>
<li><strong>Authors: </strong>Woomin Song, Jihoon Tack, Sangwoo Mo, Seunghyuk Oh, Jinwoo Shin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20698">https://arxiv.org/abs/2505.20698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20698">https://arxiv.org/pdf/2505.20698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20698]] Sparsified State-Space Models are Efficient Highway Networks(https://arxiv.org/abs/2505.20698)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>State-space models (SSMs) offer a promising architecture for sequence modeling, providing an alternative to Transformers by replacing expensive self-attention with linear recurrences. In this paper, we propose a simple yet effective trick to enhance SSMs within given computational budgets by sparsifying them. Our intuition is that tokens in SSMs are highly redundant due to gradual recurrent updates, and dense recurrence operations block the delivery of past information. In particular, we observe that upper layers of SSMs tend to be more redundant as they encode global information, while lower layers encode local information. Motivated by this, we introduce Simba, a hierarchical sparsification method for SSMs based on token pruning. Simba sparsifies upper layers more than lower layers, encouraging the upper layers to behave like highways. To achieve this, we propose a novel token pruning criterion for SSMs, measuring the global impact of tokens on the final output by accumulating local recurrences. We demonstrate that Simba outperforms the baseline model, Mamba, with the same FLOPS in various natural language tasks. Moreover, we illustrate the effect of highways, showing that Simba not only enhances efficiency but also improves the information flow across long sequences. Code is available at this https URL.</li>
</ul>

<h3>Title: Beyond Templates: Dynamic Adaptation of Reasoning Demonstrations via Feasibility-Aware Exploration</h3>
<ul>
<li><strong>Authors: </strong>Yong Wu, Weihang Pan, Ke Li, Chen Binhui, Ping Li, Binbin Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20700">https://arxiv.org/abs/2505.20700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20700">https://arxiv.org/pdf/2505.20700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20700]] Beyond Templates: Dynamic Adaptation of Reasoning Demonstrations via Feasibility-Aware Exploration(https://arxiv.org/abs/2505.20700)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable reasoning capabilities, yet aligning such abilities to small language models (SLMs) remains a challenge due to distributional mismatches and limited model capacity. Existing reasoning datasets, typically designed for powerful LLMs, often lead to degraded performance when directly applied to weaker models. In this work, we introduce Dynamic Adaptation of Reasoning Trajectories (DART), a novel data adaptation framework that bridges the capability gap between expert reasoning trajectories and diverse SLMs. Instead of uniformly imitating expert steps, DART employs a selective imitation strategy guided by step-wise adaptability estimation via solution simulation. When expert steps surpass the student's capacity -- signaled by an Imitation Gap -- the student autonomously explores alternative reasoning paths, constrained by outcome consistency. We validate DART across multiple reasoning benchmarks and model scales, demonstrating that it significantly improves generalization and data efficiency over static fine-tuning. Our method enhances supervision quality by aligning training signals with the student's reasoning capabilities, offering a scalable solution for reasoning alignment in resource-constrained models.</li>
</ul>

<h3>Title: Hierarchical Instruction-aware Embodied Visual Tracking</h3>
<ul>
<li><strong>Authors: </strong>Kui Wu, Hao Chen, Churan Wang, Fakhri Karray, Zhoujun Li, Yizhou Wang, Fangwei Zhong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20710">https://arxiv.org/abs/2505.20710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20710">https://arxiv.org/pdf/2505.20710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20710]] Hierarchical Instruction-aware Embodied Visual Tracking(https://arxiv.org/abs/2505.20710)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>User-Centric Embodied Visual Tracking (UC-EVT) presents a novel challenge for reinforcement learning-based models due to the substantial gap between high-level user instructions and low-level agent actions. While recent advancements in language models (e.g., LLMs, VLMs, VLAs) have improved instruction comprehension, these models face critical limitations in either inference speed (LLMs, VLMs) or generalizability (VLAs) for UC-EVT tasks. To address these challenges, we propose \textbf{Hierarchical Instruction-aware Embodied Visual Tracking (HIEVT)} agent, which bridges instruction comprehension and action generation using \textit{spatial goals} as intermediaries. HIEVT first introduces \textit{LLM-based Semantic-Spatial Goal Aligner} to translate diverse human instructions into spatial goals that directly annotate the desired spatial position. Then the \textit{RL-based Adaptive Goal-Aligned Policy}, a general offline policy, enables the tracker to position the target as specified by the spatial goal. To benchmark UC-EVT tasks, we collect over ten million trajectories for training and evaluate across one seen environment and nine unseen challenging environments. Extensive experiments and real-world deployments demonstrate the robustness and generalizability of HIEVT across diverse environments, varying target dynamics, and complex instruction combinations. The complete project is available at this https URL.</li>
</ul>

<h3>Title: MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding</h3>
<ul>
<li><strong>Authors: </strong>Fuwen Luo, Shengfeng Lou, Chi Chen, Ziyue Wang, Chenliang Li, Weizhou Shen, Jiyue Guo, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20715">https://arxiv.org/abs/2505.20715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20715">https://arxiv.org/pdf/2505.20715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20715]] MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding(https://arxiv.org/abs/2505.20715)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video temporal understanding is crucial for multimodal large language models (MLLMs) to reason over events in videos. Despite recent advances in general video understanding, current MLLMs still struggle with fine-grained temporal reasoning. While reinforcement learning (RL) has been explored to address this issue recently, existing RL approaches remain limited in effectiveness. In this work, we propose MUSEG, a novel RL-based method that enhances temporal understanding by introducing timestamp-aware multi-segment grounding. MUSEG enables MLLMs to align queries with multiple relevant video segments, promoting more comprehensive temporal reasoning. To facilitate effective learning, we design a customized RL training recipe with phased rewards that progressively guides the model toward temporally grounded reasoning. Extensive experiments on temporal grounding and time-sensitive video QA tasks demonstrate that MUSEG significantly outperforms existing methods and generalizes well across diverse temporal understanding scenarios. View our project at this https URL.</li>
</ul>

<h3>Title: Recurrent Neural Operators: Stable Long-Term PDE Prediction</h3>
<ul>
<li><strong>Authors: </strong>Zaijun Ye, Chen-Song Zhang, Wansheng Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20721">https://arxiv.org/abs/2505.20721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20721">https://arxiv.org/pdf/2505.20721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20721]] Recurrent Neural Operators: Stable Long-Term PDE Prediction(https://arxiv.org/abs/2505.20721)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural operators have emerged as powerful tools for learning solution operators of partial differential equations. However, in time-dependent problems, standard training strategies such as teacher forcing introduce a mismatch between training and inference, leading to compounding errors in long-term autoregressive predictions. To address this issue, we propose Recurrent Neural Operators (RNOs)-a novel framework that integrates recurrent training into neural operator architectures. Instead of conditioning each training step on ground-truth inputs, RNOs recursively apply the operator to their own predictions over a temporal window, effectively simulating inference-time dynamics during training. This alignment mitigates exposure bias and enhances robustness to error accumulation. Theoretically, we show that recurrent training can reduce the worst-case exponential error growth typical of teacher forcing to linear growth. Empirically, we demonstrate that recurrently trained Multigrid Neural Operators significantly outperform their teacher-forced counterparts in long-term accuracy and stability on standard benchmarks. Our results underscore the importance of aligning training with inference dynamics for robust temporal generalization in neural operator learning.</li>
</ul>

<h3>Title: LeDiFlow: Learned Distribution-guided Flow Matching to Accelerate Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Pascal Zwick, Nils Friederich, Maximilian Beichter, Lennart Hilbert, Ralf Mikut, Oliver Bringmann</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20723">https://arxiv.org/abs/2505.20723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20723">https://arxiv.org/pdf/2505.20723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20723]] LeDiFlow: Learned Distribution-guided Flow Matching to Accelerate Image Generation(https://arxiv.org/abs/2505.20723)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Enhancing the efficiency of high-quality image generation using Diffusion Models (DMs) is a significant challenge due to the iterative nature of the process. Flow Matching (FM) is emerging as a powerful generative modeling paradigm based on a simulation-free training objective instead of a score-based one used in DMs. Typical FM approaches rely on a Gaussian distribution prior, which induces curved, conditional probability paths between the prior and target data distribution. These curved paths pose a challenge for the Ordinary Differential Equation (ODE) solver, requiring a large number of inference calls to the flow prediction network. To address this issue, we present Learned Distribution-guided Flow Matching (LeDiFlow), a novel scalable method for training FM-based image generation models using a better-suited prior distribution learned via a regression-based auxiliary model. By initializing the ODE solver with a prior closer to the target data distribution, LeDiFlow enables the learning of more computationally tractable probability paths. These paths directly translate to fewer solver steps needed for high-quality image generation at inference time. Our method utilizes a State-Of-The-Art (SOTA) transformer architecture combined with latent space sampling and can be trained on a consumer workstation. We empirically demonstrate that LeDiFlow remarkably outperforms the respective FM baselines. For instance, when operating directly on pixels, our model accelerates inference by up to 3.75x compared to the corresponding pixel-space baseline. Simultaneously, our latent FM model enhances image quality on average by 1.32x in CLIP Maximum Mean Discrepancy (CMMD) metric against its respective baseline.</li>
</ul>

<h3>Title: Detecting Informative Channels: ActionFormer</h3>
<ul>
<li><strong>Authors: </strong>Kunpeng Zhao, Asahi Miyazaki, Tsuyoshi Okita</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20739">https://arxiv.org/abs/2505.20739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20739">https://arxiv.org/pdf/2505.20739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20739]] Detecting Informative Channels: ActionFormer(https://arxiv.org/abs/2505.20739)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Human Activity Recognition (HAR) has recently witnessed advancements with Transformer-based models. Especially, ActionFormer shows us a new perspectives for HAR in the sense that this approach gives us additional outputs which detect the border of the activities as well as the activity labels. ActionFormer was originally proposed with its input as image/video. However, this was converted to with its input as sensor signals as well. We analyze this extensively in terms of deep learning architectures. Based on the report of high temporal dynamics which limits the model's ability to capture subtle changes effectively and of the interdependencies between the spatial and temporal features. We propose the modified ActionFormer which will decrease these defects for sensor signals. The key to our approach lies in accordance with the Sequence-and-Excitation strategy to minimize the increase in additional parameters and opt for the swish activation function to retain the information about direction in the negative range. Experiments on the WEAR dataset show that our method achieves substantial improvement of a 16.01\% in terms of average mAP for inertial data.</li>
</ul>

<h3>Title: 'Hello, World!': Making GNNs Talk with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Sunwoo Kim, Soo Yong Lee, Jaemin Yoo, Kijung Shin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20742">https://arxiv.org/abs/2505.20742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20742">https://arxiv.org/pdf/2505.20742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20742]] 'Hello, World!': Making GNNs Talk with LLMs(https://arxiv.org/abs/2505.20742)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While graph neural networks (GNNs) have shown remarkable performance across diverse graph-related tasks, their high-dimensional hidden representations render them black boxes. In this work, we propose Graph Lingual Network (GLN), a GNN built on large language models (LLMs), with hidden representations in the form of human-readable text. Through careful prompt design, GLN incorporates not only the message passing module of GNNs but also advanced GNN techniques, including graph attention and initial residual connection. The comprehensibility of GLN's hidden representations enables an intuitive analysis of how node representations change (1) across layers and (2) under advanced GNN techniques, shedding light on the inner workings of GNNs. Furthermore, we demonstrate that GLN achieves strong zero-shot performance on node classification and link prediction, outperforming existing LLM-based baseline methods.</li>
</ul>

<h3>Title: MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Hao Zhang, Zhan Zhuang, Xuehao Wang, Xiaodong Yang, Yu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20744">https://arxiv.org/abs/2505.20744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20744">https://arxiv.org/pdf/2505.20744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20744]] MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition(https://arxiv.org/abs/2505.20744)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Human Activity Recognition (HAR) with wearable sensors is challenged by limited interpretability, which significantly impacts cross-dataset generalization. To address this challenge, we propose Motion-Primitive Transformer (MoPFormer), a novel self-supervised framework that enhances interpretability by tokenizing inertial measurement unit signals into semantically meaningful motion primitives and leverages a Transformer architecture to learn rich temporal representations. MoPFormer comprises two-stages. first stage is to partition multi-channel sensor streams into short segments and quantizing them into discrete "motion primitive" codewords, while the second stage enriches those tokenized sequences through a context-aware embedding module and then processes them with a Transformer encoder. The proposed MoPFormer can be pre-trained using a masked motion-modeling objective that reconstructs missing primitives, enabling it to develop robust representations across diverse sensor configurations. Experiments on six HAR benchmarks demonstrate that MoPFormer not only outperforms state-of-the-art methods but also successfully generalizes across multiple datasets. Most importantly, the learned motion primitives significantly enhance both interpretability and cross-dataset performance by capturing fundamental movement patterns that remain consistent across similar activities regardless of dataset origin.</li>
</ul>

<h3>Title: Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction</h3>
<ul>
<li><strong>Authors: </strong>Yifei Wang, Weimin Bai, Colin Zhang, Debing Zhang, Weijian Luo, He Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20755">https://arxiv.org/abs/2505.20755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20755">https://arxiv.org/pdf/2505.20755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20755]] Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction(https://arxiv.org/abs/2505.20755)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we unify more than 10 existing one-step diffusion distillation approaches, such as Diff-Instruct, DMD, SIM, SiD, $f$-distill, etc, inside a theory-driven framework which we name the \textbf{\emph{Uni-Instruct}}. Uni-Instruct is motivated by our proposed diffusion expansion theory of the $f$-divergence family. Then we introduce key theories that overcome the intractability issue of the original expanded $f$-divergence, resulting in an equivalent yet tractable loss that effectively trains one-step diffusion models by minimizing the expanded $f$-divergence family. The novel unification introduced by Uni-Instruct not only offers new theoretical contributions that help understand existing approaches from a high-level perspective but also leads to state-of-the-art one-step diffusion generation performances. On the CIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet Inception Distance (FID) values of \textbf{\emph{1.46}} for unconditional generation and \textbf{\emph{1.38}} for conditional generation. On the ImageNet-$64\times 64$ generation benchmark, Uni-Instruct achieves a new SoTA one-step generation FID of \textbf{\emph{1.02}}, which outperforms its 79-step teacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35). We also apply Uni-Instruct on broader tasks like text-to-3D generation. For text-to-3D generation, Uni-Instruct gives decent results, which slightly outperforms previous methods, such as SDS and VSD, in terms of both generation quality and diversity. Both the solid theoretical and empirical contributions of Uni-Instruct will potentially help future studies on one-step diffusion distillation and knowledge transferring of diffusion models.</li>
</ul>

<h3>Title: PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding</h3>
<ul>
<li><strong>Authors: </strong>Ansel Blume, Jeonghwan Kim, Hyeonjeong Ha, Elen Chatikyan, Xiaomeng Jin, Khanh Duy Nguyen, Nanyun Peng, Kai-Wei Chang, Derek Hoiem, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20759">https://arxiv.org/abs/2505.20759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20759">https://arxiv.org/pdf/2505.20759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20759]] PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding(https://arxiv.org/abs/2505.20759)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Real-world objects are composed of distinctive, object-specific parts. Identifying these parts is key to performing fine-grained, compositional reasoning-yet, large multimodal models (LMMs) struggle to perform this seemingly straightforward task. In this work, we introduce PARTONOMY, an LMM benchmark designed for pixel-level part grounding. We construct PARTONOMY from existing part datasets and our own rigorously annotated set of images, encompassing 862 part labels and 534 object labels for evaluation. Unlike existing datasets that simply ask models to identify generic parts, PARTONOMY uses specialized concepts (e.g., agricultural airplane), and challenges models to compare objects' parts, consider part-whole relationships, and justify textual predictions with visual segmentations. Our experiments demonstrate significant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only 5.9% gIoU), highlighting a critical gap in their part grounding abilities. We note that existing segmentation-enabled LMMs (segmenting LMMs) have two key architectural shortcomings: they use special [SEG] tokens not seen during pretraining which induce distribution shift, and they discard predicted segmentations instead of using past predictions to guide future ones. To address these deficiencies, we train several part-centric LMMs and propose PLUM, a novel segmenting LMM that uses span tagging instead of segmentation tokens and that conditions on prior predictions in a feedback loop. We find that pretrained PLUM outperforms existing segmenting LMMs on reasoning segmentation, VQA, and visual hallucination benchmarks. In addition, PLUM finetuned on our proposed Explanatory Part Segmentation task is competitive with segmenting LMMs trained on significantly more segmentation data. Our work opens up new avenues towards enabling fine-grained, grounded visual understanding in LMMs.</li>
</ul>

<h3>Title: Practical estimation of the optimal classification error with soft labels and calibration</h3>
<ul>
<li><strong>Authors: </strong>Ryota Ushio, Takashi Ishida, Masashi Sugiyama</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20761">https://arxiv.org/abs/2505.20761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20761">https://arxiv.org/pdf/2505.20761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20761]] Practical estimation of the optimal classification error with soft labels and calibration(https://arxiv.org/abs/2505.20761)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>While the performance of machine learning systems has experienced significant improvement in recent years, relatively little attention has been paid to the fundamental question: to what extent can we improve our models? This paper provides a means of answering this question in the setting of binary classification, which is practical and theoretically supported. We extend a previous work that utilizes soft labels for estimating the Bayes error, the optimal error rate, in two important ways. First, we theoretically investigate the properties of the bias of the hard-label-based estimator discussed in the original work. We reveal that the decay rate of the bias is adaptive to how well the two class-conditional distributions are separated, and it can decay significantly faster than the previous result suggested as the number of hard labels per instance grows. Second, we tackle a more challenging problem setting: estimation with corrupted soft labels. One might be tempted to use calibrated soft labels instead of clean ones. However, we reveal that calibration guarantee is not enough, that is, even perfectly calibrated soft labels can result in a substantially inaccurate estimate. Then, we show that isotonic calibration can provide a statistically consistent estimator under an assumption weaker than that of the previous work. Our method is instance-free, i.e., we do not assume access to any input instances. This feature allows it to be adopted in practical scenarios where the instances are not available due to privacy issues. Experiments with synthetic and real-world datasets show the validity of our methods and theory.</li>
</ul>

<h3>Title: Robust and Explainable Detector of Time Series Anomaly via Augmenting Multiclass Pseudo-Anomalies</h3>
<ul>
<li><strong>Authors: </strong>Kohei Obata, Yasuko Matsubara, Yasushi Sakurai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20765">https://arxiv.org/abs/2505.20765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20765">https://arxiv.org/pdf/2505.20765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20765]] Robust and Explainable Detector of Time Series Anomaly via Augmenting Multiclass Pseudo-Anomalies(https://arxiv.org/abs/2505.20765)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Unsupervised anomaly detection in time series has been a pivotal research area for decades. Current mainstream approaches focus on learning normality, on the assumption that all or most of the samples in the training set are normal. However, anomalies in the training set (i.e., anomaly contamination) can be misleading. Recent studies employ data augmentation to generate pseudo-anomalies and learn the boundary separating the training samples from the augmented samples. Although this approach mitigates anomaly contamination if augmented samples mimic unseen real anomalies, it suffers from several limitations. (1) Covering a wide range of time series anomalies is challenging. (2) It disregards augmented samples that resemble normal samples (i.e., false anomalies). (3) It places too much trust in the labels of training and augmented samples. In response, we propose RedLamp, which employs diverse data augmentations to generate multiclass pseudo-anomalies and learns the multiclass boundary. Such multiclass pseudo-anomalies cover a wide variety of time series anomalies. We conduct multiclass classification using soft labels, which prevents the model from being overconfident and ensures its robustness against contaminated/false anomalies. The learned latent space is inherently explainable as it is trained to separate pseudo-anomalies into multiclasses. Extensive experiments demonstrate the effectiveness of RedLamp in anomaly detection and its robustness against anomaly contamination.</li>
</ul>

<h3>Title: CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaqiang Tang, Jian Li, Keyu Hu, Du Nan, Xiaolong Li, Xi Zhang, Weigao Sun, Sihong Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20767">https://arxiv.org/abs/2505.20767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20767">https://arxiv.org/pdf/2505.20767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20767]] CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models(https://arxiv.org/abs/2505.20767)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Faithfulness hallucination are claims generated by a Large Language Model (LLM) not supported by contexts provided to the LLM. Lacking assessment standard, existing benchmarks only contain "factual statements" that rephrase source materials without marking "cognitive statements" that make inference from the given context, making the consistency evaluation and optimization of cognitive statements difficult. Inspired by how an evidence is assessed in the legislative domain, we design a rigorous framework to assess different levels of faithfulness of cognitive statements and create a benchmark dataset where we reveal insightful statistics. We design an annotation pipeline to create larger benchmarks for different LLMs automatically, and the resulting larger-scale CogniBench-L dataset can be used to train accurate cognitive hallucination detection model. We release our model and dataset at: this https URL</li>
</ul>

<h3>Title: Non-invasive maturity assessment of iPSC-CMs based on optical maturity characteristics using interpretable AI</h3>
<ul>
<li><strong>Authors: </strong>Fabian Scheurer, Alexander Hammer, Mario Schubert, Robert-Patrick Steiner, Oliver Gamm, Kaomei Guan, Frank Sonntag, Hagen Malberg, Martin Schmidt</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, q-bio.CB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20775">https://arxiv.org/abs/2505.20775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20775">https://arxiv.org/pdf/2505.20775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20775]] Non-invasive maturity assessment of iPSC-CMs based on optical maturity characteristics using interpretable AI(https://arxiv.org/abs/2505.20775)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Human induced pluripotent stem cell-derived cardiomyocytes (iPSC-CMs) are an important resource for the identification of new therapeutic targets and cardioprotective drugs. After differentiation iPSC-CMs show an immature, fetal-like phenotype. Cultivation of iPSC-CMs in lipid-supplemented maturation medium (MM) strongly enhances their structural, metabolic and functional phenotype. Nevertheless, assessing iPSC-CM maturation state remains challenging as most methods are time consuming and go in line with cell damage or loss of the sample. To address this issue, we developed a non-invasive approach for automated classification of iPSC-CM maturity through interpretable artificial intelligence (AI)-based analysis of beat characteristics derived from video-based motion analysis. In a prospective study, we evaluated 230 video recordings of early-state, immature iPSC-CMs on day 21 after differentiation (d21) and more mature iPSC-CMs cultured in MM (d42, MM). For each recording, 10 features were extracted using Maia motion analysis software and entered into a support vector machine (SVM). The hyperparameters of the SVM were optimized in a grid search on 80 % of the data using 5-fold cross-validation. The optimized model achieved an accuracy of 99.5 $\pm$ 1.1 % on a hold-out test set. Shapley Additive Explanations (SHAP) identified displacement, relaxation-rise time and beating duration as the most relevant features for assessing maturity level. Our results suggest the use of non-invasive, optical motion analysis combined with AI-based methods as a tool to assess iPSC-CMs maturity and could be applied before performing functional readouts or drug testing. This may potentially reduce the variability and improve the reproducibility of experimental studies.</li>
</ul>

<h3>Title: SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences</h3>
<ul>
<li><strong>Authors: </strong>Jungyoub Cha, Hyunjong Kim, Sungzoon Cho</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20776">https://arxiv.org/abs/2505.20776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20776">https://arxiv.org/pdf/2505.20776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20776]] SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences(https://arxiv.org/abs/2505.20776)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), but its performance degrades on long inputs due to increased attention cost and reduced draft accuracy. We introduce SpecExtend, a drop-in enhancement that improves the performance of speculative decoding on long sequences without any additional training. SpecExtend integrates efficient attention mechanisms such as FlashAttention and Hybrid Tree Attention into both the draft and target models, reducing latency across all stages. To improve draft accuracy and speed, we propose Cross-model Retrieval, a novel KV cache update strategy that uses the target model's attention scores to dynamically select relevant context for the draft model. Extensive evaluations on three long-context understanding datasets show that SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x for inputs up to 16K tokens, providing an effective solution for speculative decoding of long sequences. The code is available at this https URL .</li>
</ul>

<h3>Title: TACO: Think-Answer Consistency for Optimized Long-Chain Reasoning and Efficient Data Learning via Reinforcement Learning in LVLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhehan Kan, Yanlin Liu, Kun Yin, Xinghua Jiang, Xin Li, Haoyu Cao, Yinsong Liu, Deqiang Jiang, Xing Sun, Qingmin Liao, Wenming Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20777">https://arxiv.org/abs/2505.20777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20777">https://arxiv.org/pdf/2505.20777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20777]] TACO: Think-Answer Consistency for Optimized Long-Chain Reasoning and Efficient Data Learning via Reinforcement Learning in LVLMs(https://arxiv.org/abs/2505.20777)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>DeepSeek R1 has significantly advanced complex reasoning for large language models (LLMs). While recent methods have attempted to replicate R1's reasoning capabilities in multimodal settings, they face limitations, including inconsistencies between reasoning and final answers, model instability and crashes during long-chain exploration, and low data learning efficiency. To address these challenges, we propose TACO, a novel reinforcement learning algorithm for visual reasoning. Building on Generalized Reinforcement Policy Optimization (GRPO), TACO introduces Think-Answer Consistency, which tightly couples reasoning with answer consistency to ensure answers are grounded in thoughtful reasoning. We also introduce the Rollback Resample Strategy, which adaptively removes problematic samples and reintroduces them to the sampler, enabling stable long-chain exploration and future learning opportunities. Additionally, TACO employs an adaptive learning schedule that focuses on moderate difficulty samples to optimize data efficiency. Furthermore, we propose the Test-Time-Resolution-Scaling scheme to address performance degradation due to varying resolutions during reasoning while balancing computational overhead. Extensive experiments on in-distribution and out-of-distribution benchmarks for REC and VQA tasks show that fine-tuning LVLMs leads to significant performance improvements.</li>
</ul>

<h3>Title: CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature</h3>
<ul>
<li><strong>Authors: </strong>Noy Sternlicht, Tom Hope</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20779">https://arxiv.org/abs/2505.20779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20779">https://arxiv.org/pdf/2505.20779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20779]] CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature(https://arxiv.org/abs/2505.20779)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>A hallmark of human innovation is the process of recombination -- creating original ideas by integrating elements of existing mechanisms and concepts. In this work, we automatically mine the scientific literature and build CHIMERA: a large-scale knowledge base (KB) of recombination examples. CHIMERA can be used to empirically explore at scale how scientists recombine concepts and take inspiration from different areas, or to train supervised machine learning models that learn to predict new creative cross-domain directions. To build this KB, we present a novel information extraction task of extracting recombination from scientific paper abstracts, collect a high-quality corpus of hundreds of manually annotated abstracts, and use it to train an LLM-based extraction model. The model is applied to a large corpus of papers in the AI domain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to explore the properties of recombination in different subareas of AI. Finally, we train a scientific hypothesis generation model using the KB, which predicts new recombination directions that real-world researchers find inspiring. Our data and code are available at this https URL</li>
</ul>

<h3>Title: Breaking Dataset Boundaries: Class-Agnostic Targeted Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>TaÃ¯ga GonÃ§alves, Tomo Miyazaki, Shinichiro Omachi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20782">https://arxiv.org/abs/2505.20782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20782">https://arxiv.org/pdf/2505.20782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20782]] Breaking Dataset Boundaries: Class-Agnostic Targeted Adversarial Attacks(https://arxiv.org/abs/2505.20782)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>We present Cross-Domain Multi-Targeted Attack (CD-MTA), a method for generating adversarial examples that mislead image classifiers toward any target class, including those not seen during training. Traditional targeted attacks are limited to one class per model, requiring expensive retraining for each target. Multi-targeted attacks address this by introducing a perturbation generator with a conditional input to specify the target class. However, existing methods are constrained to classes observed during training and require access to the black-box model's training data--introducing a form of data leakage that undermines realistic evaluation in practical black-box scenarios. We identify overreliance on class embeddings as a key limitation, leading to overfitting and poor generalization to unseen classes. To address this, CD-MTA replaces class-level supervision with an image-based conditional input and introduces class-agnostic losses that align the perturbed and target images in the feature space. This design removes dependence on class semantics, thereby enabling generalization to unseen classes across datasets. Experiments on ImageNet and seven other datasets show that CD-MTA outperforms prior multi-targeted attacks in both standard and cross-domain settings--without accessing the black-box model's training data.</li>
</ul>

<h3>Title: Integrating Intermediate Layer Optimization and Projected Gradient Descent for Solving Inverse Problems with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Zheng, Wen Li, Zhaoqiang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20789">https://arxiv.org/abs/2505.20789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20789">https://arxiv.org/pdf/2505.20789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20789]] Integrating Intermediate Layer Optimization and Projected Gradient Descent for Solving Inverse Problems with Diffusion Models(https://arxiv.org/abs/2505.20789)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Inverse problems (IPs) involve reconstructing signals from noisy observations. Traditional approaches often rely on handcrafted priors, which can fail to capture the complexity of real-world data. The advent of pre-trained generative models has introduced new paradigms, offering improved reconstructions by learning rich priors from data. Among these, diffusion models (DMs) have emerged as a powerful framework, achieving remarkable reconstruction performance across numerous IPs. However, existing DM-based methods frequently encounter issues such as heavy computational demands and suboptimal convergence. In this work, building upon the idea of the recent work DMPlug~\cite{wang2024dmplug}, we propose two novel methods, DMILO and DMILO-PGD, to address these challenges. Our first method, DMILO, employs intermediate layer optimization (ILO) to alleviate the memory burden inherent in DMPlug. Additionally, by introducing sparse deviations, we expand the range of DMs, enabling the exploration of underlying signals that may lie outside the range of the diffusion model. We further propose DMILO-PGD, which integrates ILO with projected gradient descent (PGD), thereby reducing the risk of suboptimal convergence. We provide an intuitive theoretical analysis of our approach under appropriate conditions and validate its superiority through extensive experiments on diverse image datasets, encompassing both linear and nonlinear IPs. Our results demonstrate significant performance gains over state-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD in addressing common challenges in DM-based IP solvers.</li>
</ul>

<h3>Title: Leaner Transformers: More Heads, Less Depth</h3>
<ul>
<li><strong>Authors: </strong>Hemanth Saratchandran, Damien Teney, Simon Lucey</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20802">https://arxiv.org/abs/2505.20802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20802">https://arxiv.org/pdf/2505.20802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20802]] Leaner Transformers: More Heads, Less Depth(https://arxiv.org/abs/2505.20802)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers have reshaped machine learning by utilizing attention mechanisms to capture complex patterns in large datasets, leading to significant improvements in performance. This success has contributed to the belief that "bigger means better", leading to ever-increasing model sizes. This paper challenge this ideology by showing that many existing transformers might be unnecessarily oversized. We discover a theoretical principle that redefines the role of multi-head attention. An important benefit of the multiple heads is in improving the conditioning of the attention block. We exploit this theoretical insight and redesign popular architectures with an increased number of heads. The improvement in the conditioning proves so significant in practice that model depth can be decreased, reducing the parameter count by up to 30-50% while maintaining accuracy. We obtain consistent benefits across a variety of transformer-based architectures of various scales, on tasks in computer vision (ImageNet-1k) as well as language and sequence modeling (GLUE benchmark, TinyStories, and the Long-Range Arena benchmark).</li>
</ul>

<h3>Title: Not All Thats Rare Is Lost: Causal Paths to Rare Concept Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Bo-Kai Ruan, Zi-Xiang Ni, Bo-Lun Huang, Teng-Fang Hsiao, Hong-Han Shuai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20808">https://arxiv.org/abs/2505.20808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20808">https://arxiv.org/pdf/2505.20808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20808]] Not All Thats Rare Is Lost: Causal Paths to Rare Concept Synthesis(https://arxiv.org/abs/2505.20808)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have shown strong capabilities in high-fidelity image generation but often falter when synthesizing rare concepts, i.e., prompts that are infrequently observed in the training distribution. In this paper, we introduce RAP, a principled framework that treats rare concept generation as navigating a latent causal path: a progressive, model-aligned trajectory through the generative space from frequent concepts to rare targets. Rather than relying on heuristic prompt alternation, we theoretically justify that rare prompt guidance can be approximated by semantically related frequent prompts. We then formulate prompt switching as a dynamic process based on score similarity, enabling adaptive stage transitions. Furthermore, we reinterpret prompt alternation as a second-order denoising mechanism, promoting smooth semantic progression and coherent visual synthesis. Through this causal lens, we align input scheduling with the model's internal generative dynamics. Experiments across diverse diffusion backbones demonstrate that RAP consistently enhances rare concept generation, outperforming strong baselines in both automated evaluations and human studies.</li>
</ul>

<h3>Title: Improved Representation Steering for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhengxuan Wu, Qinan Yu, Aryaman Arora, Christopher D. Manning, Christopher Potts</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20809">https://arxiv.org/abs/2505.20809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20809">https://arxiv.org/pdf/2505.20809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20809]] Improved Representation Steering for Language Models(https://arxiv.org/abs/2505.20809)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, interpretability</a></li>
<li><strong>Abstract: </strong>Steering methods for language models (LMs) seek to provide fine-grained and interpretable control over model generations by variously changing model inputs, weights, or representations to adjust behavior. Recent work has shown that adjusting weights or representations is often less effective than steering by prompting, for instance when wanting to introduce or suppress a particular concept. We demonstrate how to improve representation steering via our new Reference-free Preference Steering (RePS), a bidirectional preference-optimization objective that jointly does concept steering and suppression. We train three parameterizations of RePS and evaluate them on AxBench, a large-scale model steering benchmark. On Gemma models with sizes ranging from 2B to 27B, RePS outperforms all existing steering methods trained with a language modeling objective and substantially narrows the gap with prompting -- while promoting interpretability and minimizing parameter count. In suppression, RePS matches the language-modeling objective on Gemma-2 and outperforms it on the larger Gemma-3 variants while remaining resilient to prompt-based jailbreaking attacks that defeat prompting. Overall, our results suggest that RePS provides an interpretable and robust alternative to prompting for both steering and suppression.</li>
</ul>

<h3>Title: RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph</h3>
<ul>
<li><strong>Authors: </strong>Junsik Kim, Jinwook Park, Kangil Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20813">https://arxiv.org/abs/2505.20813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20813">https://arxiv.org/pdf/2505.20813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20813]] RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph(https://arxiv.org/abs/2505.20813)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In knowledge graph embedding, leveraging relation-specific entity-transformation has markedly enhanced performance. However, the consistency of embedding differences before and after transformation remains unaddressed, risking the loss of valuable inductive bias inherent in the embeddings. This inconsistency stems from two problems. First, transformation representations are specified for relations in a disconnected manner, allowing dissimilar transformations and corresponding entity-embeddings for similar relations. Second, a generalized plug-in approach as a SFBR (Semantic Filter Based on Relations) disrupts this consistency through excessive concentration of entity embeddings under entity-based regularization, generating indistinguishable score distributions among relations. In this paper, we introduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF), containing more consistent entity-transformation characterized by three features: 1) shared affine transformation of relation embeddings across all relations, 2) rooted entity-transformation that adds an entity embedding to its change represented by the transformed vector, and 3) normalization of the change to prevent scale reduction. To amplify the advantages of consistency that preserve semantics on embeddings, RSCF adds relation transformation and prediction modules for enhancing the semantics. In knowledge graph completion tasks with distance-based and tensor decomposition models, RSCF significantly outperforms state-of-the-art KGE methods, showing robustness across all relations and their frequencies.</li>
</ul>

<h3>Title: Interpretable Credit Default Prediction with Ensemble Learning and SHAP</h3>
<ul>
<li><strong>Authors: </strong>Shiqi Yang, Ziyi Huang, Wengran Xiao, Xinyu Shen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20815">https://arxiv.org/abs/2505.20815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20815">https://arxiv.org/pdf/2505.20815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20815]] Interpretable Credit Default Prediction with Ensemble Learning and SHAP(https://arxiv.org/abs/2505.20815)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>This study focuses on the problem of credit default prediction, builds a modeling framework based on machine learning, and conducts comparative experiments on a variety of mainstream classification algorithms. Through preprocessing, feature engineering, and model training of the Home Credit dataset, the performance of multiple models including logistic regression, random forest, XGBoost, LightGBM, etc. in terms of accuracy, precision, and recall is evaluated. The results show that the ensemble learning method has obvious advantages in predictive performance, especially in dealing with complex nonlinear relationships between features and data imbalance problems. It shows strong robustness. At the same time, the SHAP method is used to analyze the importance and dependency of features, and it is found that the external credit score variable plays a dominant role in model decision making, which helps to improve the model's interpretability and practical application value. The research results provide effective reference and technical support for the intelligent development of credit risk control systems.</li>
</ul>

<h3>Title: Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective</h3>
<ul>
<li><strong>Authors: </strong>Krishna Singh Rajput, Tejas Anvekar, Chitta Baral, Vivek Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20816">https://arxiv.org/abs/2505.20816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20816">https://arxiv.org/pdf/2505.20816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20816]] Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective(https://arxiv.org/abs/2505.20816)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in multimodal question answering have primarily focused on combining heterogeneous modalities or fine-tuning multimodal large language models. While these approaches have shown strong performance, they often rely on a single, generalized reasoning strategy, overlooking the unique characteristics of each modality ultimately limiting both accuracy and interpretability. To address these limitations, we propose MAMMQA, a multi-agent QA framework for multimodal inputs spanning text, tables, and images. Our system includes two Visual Language Model (VLM) agents and one text-based Large Language Model (LLM) agent. The first VLM decomposes the user query into sub-questions and sequentially retrieves partial answers from each modality. The second VLM synthesizes and refines these results through cross-modal reasoning. Finally, the LLM integrates the insights into a cohesive answer. This modular design enhances interpretability by making the reasoning process transparent and allows each agent to operate within its domain of expertise. Experiments on diverse multimodal QA benchmarks demonstrate that our cooperative, multi-agent framework consistently outperforms existing baselines in both accuracy and robustness.</li>
</ul>

<h3>Title: Tracing and Reversing Rank-One Model Edits</h3>
<ul>
<li><strong>Authors: </strong>Paul Youssef, Zhixue Zhao, Christin Seifert, JÃ¶rg SchlÃ¶tterer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20819">https://arxiv.org/abs/2505.20819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20819">https://arxiv.org/pdf/2505.20819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20819]] Tracing and Reversing Rank-One Model Edits(https://arxiv.org/abs/2505.20819)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Knowledge editing methods (KEs) are a cost-effective way to update the factual content of large language models (LLMs), but they pose a dual-use risk. While KEs are beneficial for updating outdated or incorrect information, they can be exploited maliciously to implant misinformation or bias. In order to defend against these types of malicious manipulation, we need robust techniques that can reliably detect, interpret, and mitigate adversarial edits. This work investigates the traceability and reversibility of knowledge edits, focusing on the widely used Rank-One Model Editing (ROME) method. We first show that ROME introduces distinctive distributional patterns in the edited weight matrices, which can serve as effective signals for locating the edited weights. Second, we show that these altered weights can reliably be used to predict the edited factual relation, enabling partial reconstruction of the modified fact. Building on this, we propose a method to infer the edited object entity directly from the modified weights, without access to the editing prompt, achieving over 95% accuracy. Finally, we demonstrate that ROME edits can be reversed, recovering the model's original outputs with $\geq$ 80% accuracy. Our findings highlight the feasibility of detecting, tracing, and reversing edits based on the edited weights, offering a robust framework for safeguarding LLMs against adversarial manipulations.</li>
</ul>

<h3>Title: Reinforced Informativeness Optimization for Long-Form Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Wang, Ruiyang Ren, Yucheng Wang, Wayne Xin Zhao, Jing Liu, Hua Wu, Haifeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20825">https://arxiv.org/abs/2505.20825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20825">https://arxiv.org/pdf/2505.20825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20825]] Reinforced Informativeness Optimization for Long-Form Retrieval-Augmented Generation(https://arxiv.org/abs/2505.20825)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long-form question answering (LFQA) presents unique challenges for large language models, requiring the synthesis of coherent, paragraph-length answers. While retrieval-augmented generation (RAG) systems have emerged as a promising solution, existing research struggles with key limitations: the scarcity of high-quality training data for long-form generation, the compounding risk of hallucination in extended outputs, and the absence of reliable evaluation metrics for factual completeness. In this paper, we propose RioRAG, a novel reinforcement learning (RL) framework that advances long-form RAG through reinforced informativeness optimization. Our approach introduces two fundamental innovations to address the core challenges. First, we develop an RL training paradigm of reinforced informativeness optimization that directly optimizes informativeness and effectively addresses the slow-thinking deficit in conventional RAG systems, bypassing the need for expensive supervised data. Second, we propose a nugget-centric hierarchical reward modeling approach that enables precise assessment of long-form answers through a three-stage process: extracting the nugget from every source webpage, constructing a nugget claim checklist, and computing rewards based on factual alignment. Extensive experiments on two LFQA benchmarks LongFact and RAGChecker demonstrate the effectiveness of the proposed method. Our codes are available at this https URL.</li>
</ul>

<h3>Title: AdParaphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset</h3>
<ul>
<li><strong>Authors: </strong>Soichiro Murakami, Peinan Zhang, Hidetaka Kamigaito, Hiroya Takamura, Manabu Okumura</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20826">https://arxiv.org/abs/2505.20826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20826">https://arxiv.org/pdf/2505.20826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20826]] AdParaphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset(https://arxiv.org/abs/2505.20826)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Identifying factors that make ad text attractive is essential for advertising success. This study proposes AdParaphrase v2.0, a dataset for ad text paraphrasing, containing human preference data, to enable the analysis of the linguistic factors and to support the development of methods for generating attractive ad texts. Compared with v1.0, this dataset is 20 times larger, comprising 16,460 ad text paraphrase pairs, each annotated with preference data from ten evaluators, thereby enabling a more comprehensive and reliable analysis. Through the experiments, we identified multiple linguistic features of engaging ad texts that were not observed in v1.0 and explored various methods for generating attractive ad texts. Furthermore, our analysis demonstrated the relationships between human preference and ad performance, and highlighted the potential of reference-free metrics based on large language models for evaluating ad text attractiveness. The dataset is publicly available at: this https URL.</li>
</ul>

<h3>Title: Frame-Level Captions for Long Video Generation with Complex Multi Scenes</h3>
<ul>
<li><strong>Authors: </strong>Guangcong Zheng, Jianlong Yuan, Bo Wang, Haoyang Huang, Guoqing Ma, Nan Duan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20827">https://arxiv.org/abs/2505.20827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20827">https://arxiv.org/pdf/2505.20827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20827]] Frame-Level Captions for Long Video Generation with Complex Multi Scenes(https://arxiv.org/abs/2505.20827)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating long videos that can show complex stories, like movie scenes from scripts, has great promise and offers much more than short clips. However, current methods that use autoregression with diffusion models often struggle because their step-by-step process naturally leads to a serious error accumulation (drift). Also, many existing ways to make long videos focus on single, continuous scenes, making them less useful for stories with many events and changes. This paper introduces a new approach to solve these problems. First, we propose a novel way to annotate datasets at the frame-level, providing detailed text guidance needed for making complex, multi-scene long videos. This detailed guidance works with a Frame-Level Attention Mechanism to make sure text and video match precisely. A key feature is that each part (frame) within these windows can be guided by its own distinct text prompt. Our training uses Diffusion Forcing to provide the model with the ability to handle time flexibly. We tested our approach on difficult VBench 2.0 benchmarks ("Complex Plots" and "Complex Landscapes") based on the WanX2.1-T2V-1.3B model. The results show our method is better at following instructions in complex, changing scenes and creates high-quality long videos. We plan to share our dataset annotation methods and trained models with the research community. Project page: this https URL .</li>
</ul>

<h3>Title: Fully Spiking Neural Networks for Unified Frame-Event Object Tracking</h3>
<ul>
<li><strong>Authors: </strong>Jingjun Yang, Liangwei Fan, Jinpu Zhang, Xiangkai Lian, Hui Shen, Dewen Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20834">https://arxiv.org/abs/2505.20834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20834">https://arxiv.org/pdf/2505.20834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20834]] Fully Spiking Neural Networks for Unified Frame-Event Object Tracking(https://arxiv.org/abs/2505.20834)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>The integration of image and event streams offers a promising approach for achieving robust visual object tracking in complex environments. However, current fusion methods achieve high performance at the cost of significant computational overhead and struggle to efficiently extract the sparse, asynchronous information from event streams, failing to leverage the energy-efficient advantages of event-driven spiking paradigms. To address this challenge, we propose the first fully Spiking Frame-Event Tracking framework called SpikeFET. This network achieves synergistic integration of convolutional local feature extraction and Transformer-based global modeling within the spiking paradigm, effectively fusing frame and event data. To overcome the degradation of translation invariance caused by convolutional padding, we introduce a Random Patchwork Module (RPM) that eliminates positional bias through randomized spatial reorganization and learnable type encoding while preserving residual structures. Furthermore, we propose a Spatial-Temporal Regularization (STR) strategy that overcomes similarity metric degradation from asymmetric features by enforcing spatio-temporal consistency among temporal template features in latent space. Extensive experiments across multiple benchmarks demonstrate that the proposed framework achieves superior tracking accuracy over existing methods while significantly reducing power consumption, attaining an optimal balance between performance and efficiency. The code will be released.</li>
</ul>

<h3>Title: HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic Sequence Modeling</h3>
<ul>
<li><strong>Authors: </strong>Hexiong Yang, Mingrui Chen, Huaibo Huang, Junxian Duan, Jie Cao, Zhen Zhou, Ran He</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20836">https://arxiv.org/abs/2505.20836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20836">https://arxiv.org/pdf/2505.20836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20836]] HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic Sequence Modeling(https://arxiv.org/abs/2505.20836)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Inspired by the great success of Masked Language Modeling (MLM) in the natural language domain, the paradigm of self-supervised pre-training and fine-tuning has also achieved remarkable progress in the field of DNA sequence modeling. However, previous methods often relied on massive pre-training data or large-scale base models with huge parameters, imposing a significant computational burden. To address this, many works attempted to use more compact models to achieve similar outcomes but still fell short by a considerable margin. In this work, we propose a Hybrid Architecture Distillation (HAD) approach, leveraging both distillation and reconstruction tasks for more efficient and effective pre-training. Specifically, we employ the NTv2-500M as the teacher model and devise a grouping masking strategy to align the feature embeddings of visible tokens while concurrently reconstructing the invisible tokens during MLM pre-training. To validate the effectiveness of our proposed method, we conducted comprehensive experiments on the Nucleotide Transformer Benchmark and Genomic Benchmark. Compared to models with similar parameters, our model achieved excellent performance. More surprisingly, it even surpassed the distillation ceiling-teacher model on some sub-tasks, which is more than 500 $\times$ larger. Lastly, we utilize t-SNE for more intuitive visualization, which shows that our model can gain a sophisticated understanding of the intrinsic representation pattern in genomic sequences.</li>
</ul>

<h3>Title: FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration</h3>
<ul>
<li><strong>Authors: </strong>Daehyeon Baek, Jieun Choi, Jimyoung Son, Kyungmin Bin, Seungbeom Choi, Kihyo Moon, Minsung Jang, Hyojung Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20839">https://arxiv.org/abs/2505.20839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20839">https://arxiv.org/pdf/2505.20839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20839]] FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration(https://arxiv.org/abs/2505.20839)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models become increasingly prevalent, memory bandwidth constraints significantly limit inference throughput, motivating post-training quantization (PTQ). In this paper, we propose FireQ, a co-designed PTQ framework and an INT4-FP8 matrix multiplication kernel that accelerates LLM inference across all linear layers. Specifically, FireQ quantizes linear layer weights and key-values to INT4, and activations and queries to FP8, significantly enhancing throughput. Additionally, we introduce a three-stage pipelining for the prefill phase, which modifies the FlashAttention-3 kernel, effectively reducing time-to-first-token in the prefill phase. To minimize accuracy loss from quantization, we develop novel outlier smoothing techniques tailored separately for linear and attention layers. In linear layers, we explicitly use per-tensor scaling to prevent underflow caused by the FP8 quantization scaling factor of INT4 quantization, and channel-wise scaling to compensate for coarse granularity of INT4. In attention layers, we address quantization challenges posed by rotary positional embeddings (RoPE) by combining pre-RoPE and post-RoPE scaling strategies. FireQ significantly outperforms state-of-the-art methods, achieving 1.68x faster inference in feed-forward network layers on Llama2-7B and 1.26x faster prefill phase performance on Llama3-8B compared to QServe, with negligible accuracy loss.</li>
</ul>

<h3>Title: Aggregation Buffer: Revisiting DropEdge with a New Parameter Block</h3>
<ul>
<li><strong>Authors: </strong>Dooho Lee, Myeong Kong, Sagad Hamid, Cheonwoo Lee, Jaemin Yoo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20840">https://arxiv.org/abs/2505.20840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20840">https://arxiv.org/pdf/2505.20840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20840]] Aggregation Buffer: Revisiting DropEdge with a New Parameter Block(https://arxiv.org/abs/2505.20840)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We revisit DropEdge, a data augmentation technique for GNNs which randomly removes edges to expose diverse graph structures during training. While being a promising approach to effectively reduce overfitting on specific connections in the graph, we observe that its potential performance gain in supervised learning tasks is significantly limited. To understand why, we provide a theoretical analysis showing that the limited performance of DropEdge comes from the fundamental limitation that exists in many GNN architectures. Based on this analysis, we propose Aggregation Buffer, a parameter block specifically designed to improve the robustness of GNNs by addressing the limitation of DropEdge. Our method is compatible with any GNN model, and shows consistent performance improvements on multiple datasets. Moreover, our method effectively addresses well-known problems such as degree bias or structural disparity as a unifying solution. Code and datasets are available at this https URL.</li>
</ul>

<h3>Title: Concealment of Intent: A Game-Theoretic Analysis</h3>
<ul>
<li><strong>Authors: </strong>Xinbo Wu, Abhishek Umrawal, Lav R. Varshney</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20841">https://arxiv.org/abs/2505.20841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20841">https://arxiv.org/pdf/2505.20841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20841]] Concealment of Intent: A Game-Theoretic Analysis(https://arxiv.org/abs/2505.20841)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) grow more capable, concerns about their safe deployment have also grown. Although alignment mechanisms have been introduced to deter misuse, they remain vulnerable to carefully designed adversarial prompts. In this work, we present a scalable attack strategy: intent-hiding adversarial prompting, which conceals malicious intent through the composition of skills. We develop a game-theoretic framework to model the interaction between such attacks and defense systems that apply both prompt and response filtering. Our analysis identifies equilibrium points and reveals structural advantages for the attacker. To counter these threats, we propose and analyze a defense mechanism tailored to intent-hiding attacks. Empirically, we validate the attack's effectiveness on multiple real-world LLMs across a range of malicious behaviors, demonstrating clear advantages over existing adversarial prompting techniques.</li>
</ul>

<h3>Title: Cooperation of Experts: Fusing Heterogeneous Information with Large Margin</h3>
<ul>
<li><strong>Authors: </strong>Shuo Wang, Shunyang Huang, Jinghui Yuan, Zhixiang Shen, Zhao Kang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20853">https://arxiv.org/abs/2505.20853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20853">https://arxiv.org/pdf/2505.20853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20853]] Cooperation of Experts: Fusing Heterogeneous Information with Large Margin(https://arxiv.org/abs/2505.20853)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Fusing heterogeneous information remains a persistent challenge in modern data analysis. While significant progress has been made, existing approaches often fail to account for the inherent heterogeneity of object patterns across different semantic spaces. To address this limitation, we propose the Cooperation of Experts (CoE) framework, which encodes multi-typed information into unified heterogeneous multiplex networks. By overcoming modality and connection differences, CoE provides a powerful and flexible model for capturing the intricate structures of real-world complex data. In our framework, dedicated encoders act as domain-specific experts, each specializing in learning distinct relational patterns in specific semantic spaces. To enhance robustness and extract complementary knowledge, these experts collaborate through a novel large margin mechanism supported by a tailored optimization strategy. Rigorous theoretical analyses guarantee the framework's feasibility and stability, while extensive experiments across diverse benchmarks demonstrate its superior performance and broad applicability. Our code is available at this https URL.</li>
</ul>

<h3>Title: ProBA: Probabilistic Bundle Adjustment with the Bhattacharyya Coefficient</h3>
<ul>
<li><strong>Authors: </strong>Jason Chui, Daniel Cremers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20858">https://arxiv.org/abs/2505.20858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20858">https://arxiv.org/pdf/2505.20858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20858]] ProBA: Probabilistic Bundle Adjustment with the Bhattacharyya Coefficient(https://arxiv.org/abs/2505.20858)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Classical Bundle Adjustment (BA) methods require accurate initial estimates for convergence and typically assume known camera intrinsics, which limits their applicability when such information is uncertain or unavailable. We propose a novel probabilistic formulation of BA (ProBA) that explicitly models and propagates uncertainty in both the 2D observations and the 3D scene structure, enabling optimization without any prior knowledge of camera poses or focal length. Our method uses 3D Gaussians instead of point-like landmarks and we introduce uncertainty-aware reprojection losses by projecting the 3D Gaussians onto the 2D image space, and enforce geometric consistency across multiple 3D Gaussians using the Bhattacharyya coefficient to encourage overlap between their corresponding Gaussian distributions. This probabilistic framework leads to more robust and reliable optimization, even in the presence of outliers in the correspondence set, reducing the likelihood of converging to poor local minima. Experimental results show that \textit{ProBA} outperforms traditional methods in challenging real-world conditions. By removing the need for strong initialization and known intrinsics, ProBA enhances the practicality of SLAM systems deployed in unstructured environments.</li>
</ul>

<h3>Title: Exploring Timeline Control for Facial Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Yifeng Ma, Jinwei Qi, Chaonan Ji, Peng Zhang, Bang Zhang, Zhidong Deng, Liefeng Bo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20861">https://arxiv.org/abs/2505.20861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20861">https://arxiv.org/pdf/2505.20861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20861]] Exploring Timeline Control for Facial Motion Generation(https://arxiv.org/abs/2505.20861)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper introduces a new control signal for facial motion generation: timeline control. Compared to audio and text signals, timelines provide more fine-grained control, such as generating specific facial motions with precise timing. Users can specify a multi-track timeline of facial actions arranged in temporal intervals, allowing precise control over the timing of each action. To model the timeline control capability, We first annotate the time intervals of facial actions in natural facial motion sequences at a frame-level granularity. This process is facilitated by Toeplitz Inverse Covariance-based Clustering to minimize human labor. Based on the annotations, we propose a diffusion-based generation model capable of generating facial motions that are natural and accurately aligned with input timelines. Our method supports text-guided motion generation by using ChatGPT to convert text into timelines. Experimental results show that our method can annotate facial action intervals with satisfactory accuracy, and produces natural facial motions accurately aligned with timelines.</li>
</ul>

<h3>Title: AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding</h3>
<ul>
<li><strong>Authors: </strong>Chaeyoung Jung, Youngjoon Jang, Joon Son Chung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20862">https://arxiv.org/abs/2505.20862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20862">https://arxiv.org/pdf/2505.20862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20862]] AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding(https://arxiv.org/abs/2505.20862)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Hallucination remains a major challenge in multimodal large language models (MLLMs). To address this, various contrastive decoding (CD) methods have been proposed that contrasts original logits with hallucinated logits generated from perturbed inputs. While CD has shown promise in vision-language models (VLMs), it is not well-suited for AV-LLMs, where hallucinations often emerge from both unimodal and cross-modal combinations involving audio, video, and language. These intricate interactions call for a more adaptive and modality-aware decoding strategy. In this paper, we propose Audio-Visual Contrastive Decoding (AVCD)-a novel, training-free decoding framework designed to model trimodal interactions and suppress modality-induced hallucinations in AV-LLMs. Unlike previous CD methods in VLMs that corrupt a fixed modality, AVCD leverages attention distributions to dynamically identify less dominant modalities and applies attentive masking to generate perturbed output logits. To support CD in a trimodal setting, we also reformulate the original CD framework to jointly handle audio, visual, and textual inputs. Finally, to improve efficiency, we introduce entropy-guided adaptive decoding, which selectively skips unnecessary decoding steps based on the model's confidence in its predictions. Extensive experiments demonstrate that AVCD consistently outperforms existing decoding methods. Especially, on the AVHBench dataset, it improves accuracy by 6% for VideoLLaMA2 and 11% for video-SALMONN, demonstrating strong robustness and generalizability.</li>
</ul>

<h3>Title: Respond to Change with Constancy: Instruction-tuning with LLM for Non-I.I.D. Network Traffic Classification</h3>
<ul>
<li><strong>Authors: </strong>Xinjie Lin, Gang Xiong, Gaopeng Gou, Wenqi Dong, Jing Yu, Zhen Li, Wei Xia</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20866">https://arxiv.org/abs/2505.20866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20866">https://arxiv.org/pdf/2505.20866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20866]] Respond to Change with Constancy: Instruction-tuning with LLM for Non-I.I.D. Network Traffic Classification(https://arxiv.org/abs/2505.20866)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model</a></li>
<li><strong>Abstract: </strong>Encrypted traffic classification is highly challenging in network security due to the need for extracting robust features from content-agnostic traffic data. Existing approaches face critical issues: (i) Distribution drift, caused by reliance on the closedworld assumption, limits adaptability to realworld, shifting patterns; (ii) Dependence on labeled data restricts applicability where such data is scarce or unavailable. Large language models (LLMs) have demonstrated remarkable potential in offering generalizable solutions across a wide range of tasks, achieving notable success in various specialized fields. However, their effectiveness in traffic analysis remains constrained by challenges in adapting to the unique requirements of the traffic domain. In this paper, we introduce a novel traffic representation model named Encrypted Traffic Out-of-Distribution Instruction Tuning with LLM (ETooL), which integrates LLMs with knowledge of traffic structures through a self-supervised instruction tuning paradigm. This framework establishes connections between textual information and traffic interactions. ETooL demonstrates more robust classification performance and superior generalization in both supervised and zero-shot traffic classification tasks. Notably, it achieves significant improvements in F1 scores: APP53 (I.I.D.) to 93.19%(6.62%) and 92.11%(4.19%), APP53 (O.O.D.) to 74.88%(18.17%) and 72.13%(15.15%), and ISCX-Botnet (O.O.D.) to 95.03%(9.16%) and 81.95%(12.08%). Additionally, we construct NETD, a traffic dataset designed to support dynamic distributional shifts, and use it to validate ETooL's effectiveness under varying distributional conditions. Furthermore, we evaluate the efficiency gains achieved through ETooL's instruction tuning approach.</li>
</ul>

<h3>Title: Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG</h3>
<ul>
<li><strong>Authors: </strong>Xin Sun, Jianan Xie, Zhongqi Chen, Qiang Liu, Shu Wu, Yuehe Chen, Bowen Song, Weiqiang Wang, Zilei Wang, Liang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20871">https://arxiv.org/abs/2505.20871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20871">https://arxiv.org/pdf/2505.20871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20871]] Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG(https://arxiv.org/abs/2505.20871)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) augmented with retrieval systems have significantly advanced natural language processing tasks by integrating external knowledge sources, enabling more accurate and contextually rich responses. To improve the robustness of such systems against noisy retrievals, Retrieval-Augmented Fine-Tuning (RAFT) has emerged as a widely adopted method. However, RAFT conditions models to generate answers even in the absence of reliable knowledge. This behavior undermines their reliability in high-stakes domains, where acknowledging uncertainty is critical. To address this issue, we propose Divide-Then-Align (DTA), a post-training approach designed to endow RAG systems with the ability to respond with "I don't know" when the query is out of the knowledge boundary of both the retrieved passages and the model's internal knowledge. DTA divides data samples into four knowledge quadrants and constructs tailored preference data for each quadrant, resulting in a curated dataset for Direct Preference Optimization (DPO). Experimental results on three benchmark datasets demonstrate that DTA effectively balances accuracy with appropriate abstention, enhancing the reliability and trustworthiness of retrieval-augmented systems.</li>
</ul>

<h3>Title: In Context Learning with Vision Transformers: Case Study</h3>
<ul>
<li><strong>Authors: </strong>Antony Zhao, Alex Proshkin, Fergal Hennessy, Francesco Crivelli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20872">https://arxiv.org/abs/2505.20872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20872">https://arxiv.org/pdf/2505.20872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20872]] In Context Learning with Vision Transformers: Case Study(https://arxiv.org/abs/2505.20872)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Large transformer models have been shown to be capable of performing in-context learning. By using examples in a prompt as well as a query, they are capable of performing tasks such as few-shot, one-shot, or zero-shot learning to output the corresponding answer to this query. One area of interest to us is that these transformer models have been shown to be capable of learning the general class of certain functions, such as linear functions and small 2-layer neural networks, on random data (Garg et al, 2023). We aim to extend this to the image space to analyze their capability to in-context learn more complex functions on the image space, such as convolutional neural networks and other methods.</li>
</ul>

<h3>Title: Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chaeyoung Jung, Youngjoon Jang, Jongmin Choi, Joon Son Chung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20873">https://arxiv.org/abs/2505.20873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20873">https://arxiv.org/pdf/2505.20873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20873]] Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models(https://arxiv.org/abs/2505.20873)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The goal of this work is to enhance balanced multimodal understanding in audio-visual large language models (AV-LLMs) by addressing modality bias without requiring additional training. In current AV-LLMs, audio and video features are typically processed jointly in the decoder. While this strategy facilitates unified multimodal understanding, it may introduce modality bias, where the model tends to over-rely on one modality due to imbalanced training signals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet effective inference-time strategy that requires no additional training or architectural modifications. FMD first performs modality-specific reasoning by processing audio-only and video-only inputs through the early decoder layers (a fork phase), and then merges the resulting hidden states for joint reasoning in the remaining layers (a merge phase). This approach promotes balanced modality contributions and leverages complementary information across modalities. We evaluate our method on two representative AV-LLMs, VideoLLaMA2 and video-SALMONN, using three benchmark datasets. Experimental results demonstrate consistent performance improvements on tasks focused on audio, video, and combined audio-visual reasoning, demonstrating the effectiveness of inference-time interventions for robust multimodal understanding.</li>
</ul>

<h3>Title: Can LLMs Learn to Map the World from Local Descriptions?</h3>
<ul>
<li><strong>Authors: </strong>Sirui Xia, Aili Chen, Xintao Wang, Tinghui Zhu, Yikai Zhang, Jiangjie Chen, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20874">https://arxiv.org/abs/2505.20874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20874">https://arxiv.org/pdf/2505.20874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20874]] Can LLMs Learn to Map the World from Local Descriptions?(https://arxiv.org/abs/2505.20874)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have demonstrated strong capabilities in tasks such as code and mathematics. However, their potential to internalize structured spatial knowledge remains underexplored. This study investigates whether LLMs, grounded in locally relative human observations, can construct coherent global spatial cognition by integrating fragmented relational descriptions. We focus on two core aspects of spatial cognition: spatial perception, where models infer consistent global layouts from local positional relationships, and spatial navigation, where models learn road connectivity from trajectory data and plan optimal paths between unconnected locations. Experiments conducted in a simulated urban environment demonstrate that LLMs not only generalize to unseen spatial relationships between points of interest (POIs) but also exhibit latent representations aligned with real-world spatial distributions. Furthermore, LLMs can learn road connectivity from trajectory descriptions, enabling accurate path planning and dynamic spatial awareness during navigation.</li>
</ul>

<h3>Title: Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties</h3>
<ul>
<li><strong>Authors: </strong>Jiyoung Lee, Seungho Kim, Jieun Han, Jun-Min Lee, Kitaek Kim, Alice Oh, Edward Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20875">https://arxiv.org/abs/2505.20875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20875">https://arxiv.org/pdf/2505.20875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20875]] Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties(https://arxiv.org/abs/2505.20875)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are predominantly evaluated on Standard American English (SAE), often overlooking the diversity of global English varieties. This narrow focus may raise fairness concerns as degraded performance on non-standard varieties can lead to unequal benefits for users worldwide. Therefore, it is critical to extensively evaluate the linguistic robustness of LLMs on multiple non-standard English varieties. We introduce Trans-EnV, a framework that automatically transforms SAE datasets into multiple English varieties to evaluate the linguistic robustness. Our framework combines (1) linguistics expert knowledge to curate variety-specific features and transformation guidelines from linguistic literature and corpora, and (2) LLM-based transformations to ensure both linguistic validity and scalability. Using Trans-EnV, we transform six benchmark datasets into 38 English varieties and evaluate seven state-of-the-art LLMs. Our results reveal significant performance disparities, with accuracy decreasing by up to 46.3% on non-standard varieties. These findings highlight the importance of comprehensive linguistic robustness evaluation across diverse English varieties. Each construction of Trans-EnV was validated through rigorous statistical testing and consultation with a researcher in the field of second language acquisition, ensuring its linguistic validity. Our \href{this https URL}{code} and \href{this https URL}{datasets} are publicly available.</li>
</ul>

<h3>Title: MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection</h3>
<ul>
<li><strong>Authors: </strong>Baraa Hikal, Ahmed Nasreldin, Ali Hamdi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20880">https://arxiv.org/abs/2505.20880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20880">https://arxiv.org/pdf/2505.20880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20880]] MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection(https://arxiv.org/abs/2505.20880)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper describes our submission for SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. The task involves detecting hallucinated spans in text generated by instruction-tuned Large Language Models (LLMs) across multiple languages. Our approach combines task-specific prompt engineering with an LLM ensemble verification mechanism, where a primary model extracts hallucination spans and three independent LLMs adjudicate their validity through probability-based voting. This framework simulates the human annotation workflow used in the shared task validation and test data. Additionally, fuzzy matching refines span alignment. Our system ranked 1st in Arabic and Basque, 2nd in German, Swedish, and Finnish, and 3rd in Czech, Farsi, and French.</li>
</ul>

<h3>Title: Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yiding Shi, Jianan Zhou, Wen Song, Jieyi Bi, Yaoxin Wu, Jie Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20881">https://arxiv.org/abs/2505.20881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20881">https://arxiv.org/pdf/2505.20881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20881]] Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization(https://arxiv.org/abs/2505.20881)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Heuristic design with large language models (LLMs) has emerged as a promising approach for tackling combinatorial optimization problems (COPs). However, existing approaches often rely on manually predefined evolutionary computation (EC) optimizers and single-task training schemes, which may constrain the exploration of diverse heuristic algorithms and hinder the generalization of the resulting heuristics. To address these issues, we propose Meta-Optimization of Heuristics (MoH), a novel framework that operates at the optimizer level, discovering effective optimizers through the principle of meta-learning. Specifically, MoH leverages LLMs to iteratively refine a meta-optimizer that autonomously constructs diverse optimizers through (self-)invocation, thereby eliminating the reliance on a predefined EC optimizer. These constructed optimizers subsequently evolve heuristics for downstream tasks, enabling broader heuristic exploration. Moreover, MoH employs a multi-task training scheme to promote its generalization capability. Experiments on classic COPs demonstrate that MoH constructs an effective and interpretable meta-optimizer, achieving state-of-the-art performance across various downstream tasks, particularly in cross-size settings.</li>
</ul>

<h3>Title: YOLO-FireAD: Efficient Fire Detection via Attention-Guided Inverted Residual Learning and Dual-Pooling Feature Preservation</h3>
<ul>
<li><strong>Authors: </strong>Weichao Pan, Bohan Xu, Xu Wang, Chengze Lv, Shuoyang Wang, Zhenke Duan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20884">https://arxiv.org/abs/2505.20884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20884">https://arxiv.org/pdf/2505.20884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20884]] YOLO-FireAD: Efficient Fire Detection via Attention-Guided Inverted Residual Learning and Dual-Pooling Feature Preservation(https://arxiv.org/abs/2505.20884)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Fire detection in dynamic environments faces continuous challenges, including the interference of illumination changes, many false detections or missed detections, and it is difficult to achieve both efficiency and accuracy. To address the problem of feature extraction limitation and information loss in the existing YOLO-based models, this study propose You Only Look Once for Fire Detection with Attention-guided Inverted Residual and Dual-pooling Downscale Fusion (YOLO-FireAD) with two core innovations: (1) Attention-guided Inverted Residual Block (AIR) integrates hybrid channel-spatial attention with inverted residuals to adaptively enhance fire features and suppress environmental noise; (2) Dual Pool Downscale Fusion Block (DPDF) preserves multi-scale fire patterns through learnable fusion of max-average pooling outputs, mitigating small-fire detection failures. Extensive evaluation on two public datasets shows the efficient performance of our model. Our proposed model keeps the sum amount of parameters (1.45M, 51.8% lower than YOLOv8n) (4.6G, 43.2% lower than YOLOv8n), and mAP75 is higher than the mainstream real-time object detection models YOLOv8n, YOL-Ov9t, YOLOv10n, YOLO11n, YOLOv12n and other YOLOv8 variants 1.3-5.5%.</li>
</ul>

<h3>Title: Improved Bounds for Swap Multicalibration and Swap Omniprediction</h3>
<ul>
<li><strong>Authors: </strong>Haipeng Luo, Spandan Senapati, Vatsal Sharan</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20885">https://arxiv.org/abs/2505.20885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20885">https://arxiv.org/pdf/2505.20885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20885]] Improved Bounds for Swap Multicalibration and Swap Omniprediction(https://arxiv.org/abs/2505.20885)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>In this paper, we consider the related problems of multicalibration -- a multigroup fairness notion and omniprediction -- a simultaneous loss minimization paradigm, both in the distributional and online settings. The recent work of Garg et al. (2024) raised the open problem of whether it is possible to efficiently achieve $O(\sqrt{T})$ $\ell_{2}$-multicalibration error against bounded linear functions. In this paper, we answer this question in a strongly affirmative sense. We propose an efficient algorithm that achieves $O(T^{\frac{1}{3}})$ $\ell_{2}$-swap multicalibration error (both in high probability and expectation). On propagating this bound onward, we obtain significantly improved rates for $\ell_{1}$-swap multicalibration and swap omniprediction for a loss class of convex Lipschitz functions. In particular, we show that our algorithm achieves $O(T^{\frac{2}{3}})$ $\ell_{1}$-swap multicalibration and swap omniprediction errors, thereby improving upon the previous best-known bound of $O(T^{\frac{7}{8}})$. As a consequence of our improved online results, we further obtain several improved sample complexity rates in the distributional setting. In particular, we establish a $O(\varepsilon ^ {-3})$ sample complexity of efficiently learning an $\varepsilon$-swap omnipredictor for the class of convex and Lipschitz functions, $O(\varepsilon ^{-2.5})$ sample complexity of efficiently learning an $\varepsilon$-swap agnostic learner for the squared loss, and $O(\varepsilon ^ {-5}), O(\varepsilon ^ {-2.5})$ sample complexities of learning $\ell_{1}, \ell_{2}$-swap multicalibrated predictors against linear functions, all of which significantly improve on the previous best-known bounds.</li>
</ul>

<h3>Title: EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chengyu Wang, Junbing Yan, Wenrui Cai, Yuanhao Yue, Jun Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20888">https://arxiv.org/abs/2505.20888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20888">https://arxiv.org/pdf/2505.20888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20888]] EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models(https://arxiv.org/abs/2505.20888)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we present EasyDistill, a comprehensive toolkit designed for effective black-box and white-box knowledge distillation (KD) of large language models (LLMs). Our framework offers versatile functionalities, including data synthesis, supervised fine-tuning, ranking optimization, and reinforcement learning techniques specifically tailored for KD scenarios. The toolkit accommodates KD functionalities for both System 1 (fast, intuitive) and System 2 (slow, analytical) models. With its modular design and user-friendly interface, EasyDistill empowers researchers and industry practitioners to seamlessly experiment with and implement state-of-the-art KD strategies for LLMs. In addition, EasyDistill provides a series of robust distilled models and KD-based industrial solutions developed by us, along with the corresponding open-sourced datasets, catering to a variety of use cases. Furthermore, we describe the seamless integration of EasyDistill into Alibaba Cloud's Platform for AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for LLMs more accessible and impactful within the NLP community.</li>
</ul>

<h3>Title: Frequency Composition for Compressed and Domain-Adaptive Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Yoojin Kwon, Hongjun Suh, Wooseok Lee, Taesik Gong, Songyi Han, Hyung-Sin Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20890">https://arxiv.org/abs/2505.20890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20890">https://arxiv.org/pdf/2505.20890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20890]] Frequency Composition for Compressed and Domain-Adaptive Neural Networks(https://arxiv.org/abs/2505.20890)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Modern on-device neural network applications must operate under resource constraints while adapting to unpredictable domain shifts. However, this combined challenge-model compression and domain adaptation-remains largely unaddressed, as prior work has tackled each issue in isolation: compressed networks prioritize efficiency within a fixed domain, whereas large, capable models focus on handling domain shifts. In this work, we propose CoDA, a frequency composition-based framework that unifies compression and domain adaptation. During training, CoDA employs quantization-aware training (QAT) with low-frequency components, enabling a compressed model to selectively learn robust, generalizable features. At test time, it refines the compact model in a source-free manner (i.e., test-time adaptation, TTA), leveraging the full-frequency information from incoming data to adapt to target domains while treating high-frequency components as domain-specific cues. LFC are aligned with the trained distribution, while HFC unique to the target distribution are solely utilized for batch normalization. CoDA can be integrated synergistically into existing QAT and TTA methods. CoDA is evaluated on widely used domain-shift benchmarks, including CIFAR10-C and ImageNet-C, across various model architectures. With significant compression, it achieves accuracy improvements of 7.96%p on CIFAR10-C and 5.37%p on ImageNet-C over the full-precision TTA baseline.</li>
</ul>

<h3>Title: One-Time Soft Alignment Enables Resilient Learning without Weight Transport</h3>
<ul>
<li><strong>Authors: </strong>Jeonghwan Cheon, Jaehyuk Bae, Se-Bum Paik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20892">https://arxiv.org/abs/2505.20892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20892">https://arxiv.org/pdf/2505.20892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20892]] One-Time Soft Alignment Enables Resilient Learning without Weight Transport(https://arxiv.org/abs/2505.20892)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Backpropagation is the cornerstone of deep learning, but its reliance on symmetric weight transport and global synchronization makes it computationally expensive and biologically implausible. Feedback alignment offers a promising alternative by approximating error gradients through fixed random feedback, thereby avoiding symmetric weight transport. However, this approach often struggles with poor learning performance and instability, especially in deep networks. Here, we show that a one-time soft alignment between forward and feedback weights at initialization enables deep networks to achieve performance comparable to backpropagation, without requiring weight transport during learning. This simple initialization condition guides stable error minimization in the loss landscape, improving network trainability. Spectral analyses further reveal that initial alignment promotes smoother gradient flow and convergence to flatter minima, resulting in better generalization and robustness. Notably, we also find that allowing moderate deviations from exact weight symmetry can improve adversarial robustness compared to standard backpropagation. These findings demonstrate that a simple initialization strategy can enable effective learning in deep networks in a biologically plausible and resource-efficient manner.</li>
</ul>

<h3>Title: How Do Transformers Learn Variable Binding in Symbolic Programs?</h3>
<ul>
<li><strong>Authors: </strong>Yiwei Wu, Atticus Geiger, RaphaÃ«l MilliÃ¨re</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20896">https://arxiv.org/abs/2505.20896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20896">https://arxiv.org/pdf/2505.20896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20896]] How Do Transformers Learn Variable Binding in Symbolic Programs?(https://arxiv.org/abs/2505.20896)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Variable binding -- the ability to associate variables with values -- is fundamental to symbolic computation and cognition. Although classical architectures typically implement variable binding via addressable memory, it is not well understood how modern neural networks lacking built-in binding operations may acquire this capacity. We investigate this by training a Transformer to dereference queried variables in symbolic programs where variables are assigned either numerical constants or other variables. Each program requires following chains of variable assignments up to four steps deep to find the queried value, and also contains irrelevant chains of assignments acting as distractors. Our analysis reveals a developmental trajectory with three distinct phases during training: (1) random prediction of numerical constants, (2) a shallow heuristic prioritizing early variable assignments, and (3) the emergence of a systematic mechanism for dereferencing assignment chains. Using causal interventions, we find that the model learns to exploit the residual stream as an addressable memory space, with specialized attention heads routing information across token positions. This mechanism allows the model to dynamically track variable bindings across layers, resulting in accurate dereferencing. Our results show how Transformer models can learn to implement systematic variable binding without explicit architectural support, bridging connectionist and symbolic approaches.</li>
</ul>

<h3>Title: Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation</h3>
<ul>
<li><strong>Authors: </strong>Pingrui Zhang, Yifei Su, Pengyuan Wu, Dong An, Li Zhang, Zhigang Wang, Dong Wang, Yan Ding, Bin Zhao, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20897">https://arxiv.org/abs/2505.20897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20897">https://arxiv.org/pdf/2505.20897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20897]] Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation(https://arxiv.org/abs/2505.20897)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Vision-and-Language Navigation (VLN) requires the agent to navigate by following natural instructions under partial observability, making it difficult to align perception with language. Recent methods mitigate this by imagining future scenes, yet they rely on vision-based synthesis, leading to high computational cost and redundant details. To this end, we propose to adaptively imagine key environmental semantics via \textit{language} form, enabling a more reliable and efficient strategy. Specifically, we introduce a novel Adaptive Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a large language model (LLM). ATD is designed with a human-like left-right brain architecture, where the left brain focuses on logical integration, and the right brain is responsible for imaginative prediction of future scenes. To achieve this, we fine-tune only the Q-former within both brains to efficiently activate domain-specific knowledge in the LLM, enabling dynamic updates of logical reasoning and imagination during navigation. Furthermore, we introduce a cross-interaction mechanism to regularize the imagined outputs and inject them into a navigation expert module, allowing ATD to jointly exploit both the reasoning capacity of the LLM and the expertise of the navigation model. We conduct extensive experiments on the R2R benchmark, where ATD achieves state-of-the-art performance with fewer parameters. The code is \href{this https URL}{here}.</li>
</ul>

<h3>Title: Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing</h3>
<ul>
<li><strong>Authors: </strong>Jeongsoo Choi, Jaehun Kim, Joon Son Chung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20899">https://arxiv.org/abs/2505.20899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20899">https://arxiv.org/pdf/2505.20899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20899]] Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing(https://arxiv.org/abs/2505.20899)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper introduces a cross-lingual dubbing system that translates speech from one language to another while preserving key characteristics such as duration, speaker identity, and speaking speed. Despite the strong translation quality of existing speech translation approaches, they often overlook the transfer of speech patterns, leading to mismatches with source speech and limiting their suitability for dubbing applications. To address this, we propose a discrete diffusion-based speech-to-unit translation model with explicit duration control, enabling time-aligned translation. We then synthesize speech based on the predicted units and source identity with a conditional flow matching model. Additionally, we introduce a unit-based speed adaptation mechanism that guides the translation model to produce speech at a rate consistent with the source, without relying on any text. Extensive experiments demonstrate that our framework generates natural and fluent translations that align with the original speech's duration and speaking pace, while achieving competitive translation performance.</li>
</ul>

<h3>Title: Towards Objective Fine-tuning: How LLMs' Prior Knowledge Causes Potential Poor Calibration?</h3>
<ul>
<li><strong>Authors: </strong>Ziming Wang, Zeyu Shi, Haoyi Zhou, Shiqi Gao, Qingyun Sun, Jianxin Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20903">https://arxiv.org/abs/2505.20903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20903">https://arxiv.org/pdf/2505.20903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20903]] Towards Objective Fine-tuning: How LLMs' Prior Knowledge Causes Potential Poor Calibration?(https://arxiv.org/abs/2505.20903)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuned Large Language Models (LLMs) often demonstrate poor calibration, with their confidence scores misaligned with actual performance. While calibration has been extensively studied in models trained from scratch, the impact of LLMs' prior knowledge on calibration during fine-tuning remains understudied. Our research reveals that LLMs' prior knowledge causes potential poor calibration due to the ubiquitous presence of known data in real-world fine-tuning, which appears harmful for calibration. Specifically, data aligned with LLMs' prior knowledge would induce overconfidence, while new knowledge improves calibration. Our findings expose a tension: LLMs' encyclopedic knowledge, while enabling task versatility, undermines calibration through unavoidable knowledge overlaps. To address this, we propose CogCalib, a cognition-aware framework that applies targeted learning strategies according to the model's prior knowledge. Experiments across 7 tasks using 3 LLM families prove that CogCalib significantly improves calibration while maintaining performance, achieving an average 57\% reduction in ECE compared to standard fine-tuning in Llama3-8B. These improvements generalize well to out-of-domain tasks, enhancing the objectivity and reliability of domain-specific LLMs, and making them more trustworthy for critical human-AI interaction applications.</li>
</ul>

<h3>Title: HTMNet: A Hybrid Network with Transformer-Mamba Bottleneck Multimodal Fusion for Transparent and Reflective Objects Depth Completion</h3>
<ul>
<li><strong>Authors: </strong>Guanghu Xie, Yonglong Zhang, Zhiduo Jiang, Yang Liu, Zongwu Xie, Baoshi Cao, Hong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20904">https://arxiv.org/abs/2505.20904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20904">https://arxiv.org/pdf/2505.20904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20904]] HTMNet: A Hybrid Network with Transformer-Mamba Bottleneck Multimodal Fusion for Transparent and Reflective Objects Depth Completion(https://arxiv.org/abs/2505.20904)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Transparent and reflective objects pose significant challenges for depth sensors, resulting in incomplete depth information that adversely affects downstream robotic perception and manipulation tasks. To address this issue, we propose HTMNet, a novel hybrid model integrating Transformer, CNN, and Mamba architectures. The encoder is constructed based on a dual-branch Transformer-CNN framework, while the multi-scale fusion module leverages a Transformer-Mamba architecture, which also serves as the foundation for the decoder design. We introduce a novel multimodal fusion module grounded in self-attention mechanisms and state space models, marking the first application of the Mamba architecture in the field of transparent object depth completion and revealing its promising potential. Additionally, we design an innovative multi-scale fusion module for the decoder that combines channel attention, spatial attention, and multi-scale feature extraction techniques to effectively integrate multi-scale features through a down-fusion strategy. Extensive evaluations on multiple public datasets demonstrate that our model achieves state-of-the-art(SOTA) performance, validating the effectiveness of our approach.</li>
</ul>

<h3>Title: Create Anything Anywhere: Layout-Controllable Personalized Diffusion Model for Multiple Subjects</h3>
<ul>
<li><strong>Authors: </strong>Wei Li, Hebei Li, Yansong Peng, Siying Wu, Yueyi Zhang, Xiaoyan Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20909">https://arxiv.org/abs/2505.20909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20909">https://arxiv.org/pdf/2505.20909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20909]] Create Anything Anywhere: Layout-Controllable Personalized Diffusion Model for Multiple Subjects(https://arxiv.org/abs/2505.20909)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have significantly advanced text-to-image generation, laying the foundation for the development of personalized generative frameworks. However, existing methods lack precise layout controllability and overlook the potential of dynamic features of reference subjects in improving fidelity. In this work, we propose Layout-Controllable Personalized Diffusion (LCP-Diffusion) model, a novel framework that integrates subject identity preservation with flexible layout guidance in a tuning-free approach. Our model employs a Dynamic-Static Complementary Visual Refining module to comprehensively capture the intricate details of reference subjects, and introduces a Dual Layout Control mechanism to enforce robust spatial control across both training and inference stages. Extensive experiments validate that LCP-Diffusion excels in both identity preservation and layout controllability. To the best of our knowledge, this is a pioneering work enabling users to "create anything anywhere".</li>
</ul>

<h3>Title: Automated Privacy Information Annotation in Large Language Model Interactions</h3>
<ul>
<li><strong>Authors: </strong>Hang Zeng, Xiangyu Liu, Yong Hu, Chaoyue Niu, Fan Wu, Shaojie Tang, Guihai Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20910">https://arxiv.org/abs/2505.20910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20910">https://arxiv.org/pdf/2505.20910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20910]] Automated Privacy Information Annotation in Large Language Model Interactions(https://arxiv.org/abs/2505.20910)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Users interacting with large language models (LLMs) under their real identifiers often unknowingly risk disclosing private information. Automatically notifying users whether their queries leak privacy and which phrases leak what private information has therefore become a practical need. Existing privacy detection methods, however, were designed for different objectives and application scenarios, typically tagging personally identifiable information (PII) in anonymous content. In this work, to support the development and evaluation of privacy detection models for LLM interactions that are deployable on local user devices, we construct a large-scale multilingual dataset with 249K user queries and 154K annotated privacy phrases. In particular, we build an automated privacy annotation pipeline with cloud-based strong LLMs to automatically extract privacy phrases from dialogue datasets and annotate leaked information. We also design evaluation metrics at the levels of privacy leakage, extracted privacy phrase, and privacy information. We further establish baseline methods using light-weight LLMs with both tuning-free and tuning-based methods, and report a comprehensive evaluation of their performance. Evaluation results reveal a gap between current performance and the requirements of real-world LLM applications, motivating future research into more effective local privacy detection methods grounded in our dataset.</li>
</ul>

<h3>Title: Towards a DSL for hybrid secure computation</h3>
<ul>
<li><strong>Authors: </strong>Romain de Laage</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20912">https://arxiv.org/abs/2505.20912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20912">https://arxiv.org/pdf/2505.20912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20912]] Towards a DSL for hybrid secure computation(https://arxiv.org/abs/2505.20912)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Fully homomorphic encryption (FHE) and trusted execution environments (TEE) are two approaches to provide confidentiality during data processing. Each approach has its own strengths and weaknesses. In certain scenarios, computations can be carried out in a hybrid environment, using both FHE and TEE. However, processing data in such hybrid settings presents challenges, as it requires to adapt and rewrite the algorithms for the chosen technique. We propose a domain-specific language (DSL) for secure computation that allows to express the computations to perform and execute them using a backend that leverages either FHE or TEE, depending on what is available.</li>
</ul>

<h3>Title: Geometry-Editable and Appearance-Preserving Object Compositon</h3>
<ul>
<li><strong>Authors: </strong>Jianman Lin, Haojie Li, Chunmei Qing, Zhijing Yang, Liang Lin, Tianshui Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20914">https://arxiv.org/abs/2505.20914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20914">https://arxiv.org/pdf/2505.20914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20914]] Geometry-Editable and Appearance-Preserving Object Compositon(https://arxiv.org/abs/2505.20914)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>General object composition (GOC) aims to seamlessly integrate a target object into a background scene with desired geometric properties, while simultaneously preserving its fine-grained appearance details. Recent approaches derive semantic embeddings and integrate them into advanced diffusion models to enable geometry-editable generation. However, these highly compact embeddings encode only high-level semantic cues and inevitably discard fine-grained appearance details. We introduce a Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model that first leverages semantic embeddings to implicitly capture the desired geometric transformations and then employs a cross-attention retrieval mechanism to align fine-grained appearance features with the geometry-edited representation, facilitating both precise geometry editing and faithful appearance preservation in object composition. Specifically, DGAD builds on CLIP/DINO-derived and reference networks to extract semantic embeddings and appearance-preserving representations, which are then seamlessly integrated into the encoding and decoding pipelines in a disentangled manner. We first integrate the semantic embeddings into pre-trained diffusion models that exhibit strong spatial reasoning capabilities to implicitly capture object geometry, thereby facilitating flexible object manipulation and ensuring effective editability. Then, we design a dense cross-attention mechanism that leverages the implicitly learned object geometry to retrieve and spatially align appearance features with their corresponding regions, ensuring faithful appearance consistency. Extensive experiments on public benchmarks demonstrate the effectiveness of the proposed DGAD framework.</li>
</ul>

<h3>Title: Humble AI in the real-world: the case of algorithmic hiring</h3>
<ul>
<li><strong>Authors: </strong>Rahul Nair, Inge Vejsbjerg, Elizabeth Daly, Christos Varytimidis, Bran Knowles</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20918">https://arxiv.org/abs/2505.20918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20918">https://arxiv.org/pdf/2505.20918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20918]] Humble AI in the real-world: the case of algorithmic hiring(https://arxiv.org/abs/2505.20918)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Humble AI (Knowles et al., 2023) argues for cautiousness in AI development and deployments through scepticism (accounting for limitations of statistical learning), curiosity (accounting for unexpected outcomes), and commitment (accounting for multifaceted values beyond performance). We present a real-world case study for humble AI in the domain of algorithmic hiring. Specifically, we evaluate virtual screening algorithms in a widely used hiring platform that matches candidates to job openings. There are several challenges in misrecognition and stereotyping in such contexts that are difficult to assess through standard fairness and trust frameworks; e.g., someone with a non-traditional background is less likely to rank highly. We demonstrate technical feasibility of how humble AI principles can be translated to practice through uncertainty quantification of ranks, entropy estimates, and a user experience that highlights algorithmic unknowns. We describe preliminary discussions with focus groups made up of recruiters. Future user studies seek to evaluate whether the higher cognitive load of a humble AI system fosters a climate of trust in its outcomes.</li>
</ul>

<h3>Title: Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Injae Na, Keonwoong Noh, Woohwan Jung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20921">https://arxiv.org/abs/2505.20921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20921">https://arxiv.org/pdf/2505.20921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20921]] Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models(https://arxiv.org/abs/2505.20921)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>LLM providers typically offer multiple LLM tiers, varying in performance and price. As NLP tasks become more complex and modularized, selecting the suitable LLM tier for each subtask is a key challenge to balance between cost and performance. To address the problem, we introduce LLM Automatic Transmission (LLM-AT) framework that automatically selects LLM tiers without training. LLM-AT consists of Starter, Generator, and Judge. The starter selects the initial LLM tier expected to solve the given question, the generator produces a response using the LLM of the selected tier, and the judge evaluates the validity of the response. If the response is invalid, LLM-AT iteratively upgrades to a higher-tier model, generates a new response, and re-evaluates until a valid response is obtained. Additionally, we propose accuracy estimator, which enables the suitable initial LLM tier selection without training. Given an input question, accuracy estimator estimates the expected accuracy of each LLM tier by computing the valid response rate across top-k similar queries from past inference records. Experiments demonstrate that LLM-AT achieves superior performance while reducing costs, making it a practical solution for real-world applications.</li>
</ul>

<h3>Title: Label Leakage in Federated Inertial-based Human Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Marius Bock, Maximilian Hopp, Kristof Van Laerhoven, Michael Moeller</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20924">https://arxiv.org/abs/2505.20924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20924">https://arxiv.org/pdf/2505.20924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20924]] Label Leakage in Federated Inertial-based Human Activity Recognition(https://arxiv.org/abs/2505.20924)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, federate</a></li>
<li><strong>Abstract: </strong>While prior work has shown that Federated Learning updates can leak sensitive information, label reconstruction attacks, which aim to recover input labels from shared gradients, have not yet been examined in the context of Human Activity Recognition (HAR). Given the sensitive nature of activity labels, this study evaluates the effectiveness of state-of-the-art gradient-based label leakage attacks on HAR benchmark datasets. Our findings show that the number of activity classes, sampling strategy, and class imbalance are critical factors influencing the extent of label leakage, with reconstruction accuracies reaching up to 90% on two benchmark datasets, even for trained models. Moreover, we find that Local Differential Privacy techniques such as gradient noise and clipping offer only limited protection, as certain attacks still reliably infer both majority and minority class labels. We conclude by offering practical recommendations for the privacy-aware deployment of federated HAR systems and identify open challenges for future research. Code to reproduce our experiments is publicly available via this http URL.</li>
</ul>

<h3>Title: Multi-objective Large Language Model Alignment with Hierarchical Experts</h3>
<ul>
<li><strong>Authors: </strong>Zhuo Li, Guodong Du, Weiyang Guo, Yigeng Zhou, Xiucheng Li, Wenya Wang, Fangming Liu, Yequan Wang, Deheng Ye, Min Zhang, Jing Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20925">https://arxiv.org/abs/2505.20925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20925">https://arxiv.org/pdf/2505.20925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20925]] Multi-objective Large Language Model Alignment with Hierarchical Experts(https://arxiv.org/abs/2505.20925)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLMs) to simultaneously satisfy multiple objectives remains a significant challenge, especially given the diverse and often conflicting nature of human preferences. Existing alignment methods struggle to balance trade-offs effectively, often requiring costly retraining or yielding suboptimal results across the Pareto frontier of preferences. In this paper, we introduce \textit{HoE}(Hierarchical Mixture-of-Experts), a \textit{lightweight}, \textit{parameter-efficient}, and \textit{plug-and-play} approach that eliminates the need for model training, while enabling LLMs to adapt across the entire Pareto frontier and accommodate diverse user preferences. In particular, \textit{HoE} consists of three hierarchical components: LoRA Experts, Router Experts and Preference Routing, reaching optimal Pareto frontiers and achieving a trade-off between parameter size, training cost, and performance. We evaluate \textit{HoE} across various tasks on 14 objectives and 200 different preferences among 6 benchmarks, demonstrating superior performance over 15 recent baselines. Code is available in the supplementary materials.</li>
</ul>

<h3>Title: Good Enough: Is it Worth Improving your Label Quality?</h3>
<ul>
<li><strong>Authors: </strong>Alexander Jaus, Zdravko Marinov, Constantin Seibold, Simon ReiÃ, Jens Kleesiek, Rainer Stiefelhagen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20928">https://arxiv.org/abs/2505.20928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20928">https://arxiv.org/pdf/2505.20928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20928]] Good Enough: Is it Worth Improving your Label Quality?(https://arxiv.org/abs/2505.20928)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Improving label quality in medical image segmentation is costly, but its benefits remain unclear. We systematically evaluate its impact using multiple pseudo-labeled versions of CT datasets, generated by models like nnU-Net, TotalSegmentator, and MedSAM. Our results show that while higher-quality labels improve in-domain performance, gains remain unclear if below a small threshold. For pre-training, label quality has minimal impact, suggesting that models rather transfer general concepts than detailed annotations. These findings provide guidance on when improving label quality is worth the effort.</li>
</ul>

<h3>Title: NatADiff: Adversarial Boundary Guidance for Natural Adversarial Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Max Collins, Jordan Vice, Tim French, Ajmal Mian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20934">https://arxiv.org/abs/2505.20934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20934">https://arxiv.org/pdf/2505.20934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20934]] NatADiff: Adversarial Boundary Guidance for Natural Adversarial Diffusion(https://arxiv.org/abs/2505.20934)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Adversarial samples exploit irregularities in the manifold ``learned'' by deep learning models to cause misclassifications. The study of these adversarial samples provides insight into the features a model uses to classify inputs, which can be leveraged to improve robustness against future attacks. However, much of the existing literature focuses on constrained adversarial samples, which do not accurately reflect test-time errors encountered in real-world settings. To address this, we propose `NatADiff', an adversarial sampling scheme that leverages denoising diffusion to generate natural adversarial samples. Our approach is based on the observation that natural adversarial samples frequently contain structural elements from the adversarial class. Deep learning models can exploit these structural elements to shortcut the classification process, rather than learning to genuinely distinguish between classes. To leverage this behavior, we guide the diffusion trajectory towards the intersection of the true and adversarial classes, combining time-travel sampling with augmented classifier guidance to enhance attack transferability while preserving image fidelity. Our method achieves comparable attack success rates to current state-of-the-art techniques, while exhibiting significantly higher transferability across model architectures and better alignment with natural test-time errors as measured by FID. These results demonstrate that NatADiff produces adversarial samples that not only transfer more effectively across models, but more faithfully resemble naturally occurring test-time errors.</li>
</ul>

<h3>Title: ISAC: Training-Free Instance-to-Semantic Attention Control for Improving Multi-Instance Generation</h3>
<ul>
<li><strong>Authors: </strong>Sanghyun Jo, Wooyeol Lee, Ziseok Lee, Kyungsu Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20935">https://arxiv.org/abs/2505.20935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20935">https://arxiv.org/pdf/2505.20935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20935]] ISAC: Training-Free Instance-to-Semantic Attention Control for Improving Multi-Instance Generation(https://arxiv.org/abs/2505.20935)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models excel at generating single-instance scenes but struggle with multi-instance scenarios, often merging or omitting objects. Unlike previous training-free approaches that rely solely on semantic-level guidance without addressing instance individuation, our training-free method, Instance-to-Semantic Attention Control (ISAC), explicitly resolves incomplete instance formation and semantic entanglement through an instance-first modeling approach. This enables ISAC to effectively leverage a hierarchical, tree-structured prompt mechanism, disentangling multiple object instances and individually aligning them with their corresponding semantic labels. Without employing any external models, ISAC achieves up to 52% average multi-class accuracy and 83% average multi-instance accuracy by effectively forming disentangled instances. The code will be made available upon publication.</li>
</ul>

<h3>Title: IRCopilot: Automated Incident Response with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xihuan Lin, Jie Zhang, Gelei Deng, Tianzhe Liu, Xiaolong Liu, Changcai Yang, Tianwei Zhang, Qing Guo, Riqing Chen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20945">https://arxiv.org/abs/2505.20945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20945">https://arxiv.org/pdf/2505.20945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20945]] IRCopilot: Automated Incident Response with Large Language Models(https://arxiv.org/abs/2505.20945)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, robust, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Incident response plays a pivotal role in mitigating the impact of cyber attacks. In recent years, the intensity and complexity of global cyber threats have grown significantly, making it increasingly challenging for traditional threat detection and incident response methods to operate effectively in complex network environments. While Large Language Models (LLMs) have shown great potential in early threat detection, their capabilities remain limited when it comes to automated incident response after an intrusion. To address this gap, we construct an incremental benchmark based on real-world incident response tasks to thoroughly evaluate the performance of LLMs in this domain. Our analysis reveals several key challenges that hinder the practical application of contemporary LLMs, including context loss, hallucinations, privacy protection concerns, and their limited ability to provide accurate, context-specific recommendations. In response to these challenges, we propose IRCopilot, a novel framework for automated incident response powered by LLMs. IRCopilot mimics the three dynamic phases of a real-world incident response team using four collaborative LLM-based session components. These components are designed with clear divisions of responsibility, reducing issues such as hallucinations and context loss. Our method leverages diverse prompt designs and strategic responsibility segmentation, significantly improving the system's practicality and efficiency. Experimental results demonstrate that IRCopilot outperforms baseline LLMs across key benchmarks, achieving sub-task completion rates of 150%, 138%, 136%, 119%, and 114% for various response tasks. Moreover, IRCopilot exhibits robust performance on public incident response platforms and in real-world attack scenarios, showcasing its strong applicability.</li>
</ul>

<h3>Title: DSOcc: Leveraging Depth Awareness and Semantic Aid to Boost Camera-Based 3D Semantic Occupancy Prediction</h3>
<ul>
<li><strong>Authors: </strong>Naiyu Fang, Zheyuan Zhou, Kang Wang, Ruibo Li, Lemiao Qiu, Shuyou Zhang, Zhe Wang, Guosheng Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20951">https://arxiv.org/abs/2505.20951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20951">https://arxiv.org/pdf/2505.20951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20951]] DSOcc: Leveraging Depth Awareness and Semantic Aid to Boost Camera-Based 3D Semantic Occupancy Prediction(https://arxiv.org/abs/2505.20951)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Camera-based 3D semantic occupancy prediction offers an efficient and cost-effective solution for perceiving surrounding scenes in autonomous driving. However, existing works rely on explicit occupancy state inference, leading to numerous incorrect feature assignments, and insufficient samples restrict the learning of occupancy class inference. To address these challenges, we propose leveraging Depth awareness and Semantic aid to boost camera-based 3D semantic Occupancy prediction (DSOcc). We jointly perform occupancy state and occupancy class inference, where soft occupancy confidence is calculated through non-learning method and multiplied with image features to make the voxel representation aware of depth, enabling adaptive implicit occupancy state inference. Rather than focusing on improving feature learning, we directly utilize well-trained image semantic segmentation and fuse multiple frames with their occupancy probabilities to aid occupancy class inference, thereby enhancing robustness. Experimental results demonstrate that DSOcc achieves state-of-the-art performance on the SemanticKITTI dataset among camera-based methods.</li>
</ul>

<h3>Title: Unveiling Impact of Frequency Components on Membership Inference Attacks for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Puwei Lian, Yujun Cai, Songze Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20955">https://arxiv.org/abs/2505.20955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20955">https://arxiv.org/pdf/2505.20955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20955]] Unveiling Impact of Frequency Components on Membership Inference Attacks for Diffusion Models(https://arxiv.org/abs/2505.20955)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved tremendous success in image generation, but they also raise significant concerns regarding privacy and copyright issues. Membership Inference Attacks (MIAs) are designed to ascertain whether specific data were utilized during a model's training phase. As current MIAs for diffusion models typically exploit the model's image prediction ability, we formalize them into a unified general paradigm which computes the membership score for membership identification. Under this paradigm, we empirically find that existing attacks overlook the inherent deficiency in how diffusion models process high-frequency information. Consequently, this deficiency leads to member data with more high-frequency content being misclassified as hold-out data, and hold-out data with less high-frequency content tend to be misclassified as member data. Moreover, we theoretically demonstrate that this deficiency reduces the membership advantage of attacks, thereby interfering with the effective discrimination of member data and hold-out data. Based on this insight, we propose a plug-and-play high-frequency filter module to mitigate the adverse effects of the deficiency, which can be seamlessly integrated into any attacks within this general paradigm without additional time costs. Extensive experiments corroborate that this module significantly improves the performance of baseline attacks across different datasets and models.</li>
</ul>

<h3>Title: OrienText: Surface Oriented Textual Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Shubham Singh Paliwal, Arushi Jain, Monika Sharma, Vikram Jamwal, Lovekesh Vig</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20958">https://arxiv.org/abs/2505.20958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20958">https://arxiv.org/pdf/2505.20958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20958]] OrienText: Surface Oriented Textual Image Generation(https://arxiv.org/abs/2505.20958)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Textual content in images is crucial in e-commerce sectors, particularly in marketing campaigns, product imaging, advertising, and the entertainment industry. Current text-to-image (T2I) generation diffusion models, though proficient at producing high-quality images, often struggle to incorporate text accurately onto complex surfaces with varied perspectives, such as angled views of architectural elements like buildings, banners, or walls. In this paper, we introduce the Surface Oriented Textual Image Generation (OrienText) method, which leverages region-specific surface normals as conditional input to T2I generation diffusion model. Our approach ensures accurate rendering and correct orientation of the text within the image context. We demonstrate the effectiveness of the OrienText method on a self-curated dataset of images and compare it against the existing textual image generation methods.</li>
</ul>

<h3>Title: Research Community Perspectives on "Intelligence" and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bertram HÃ¸jer, Terne Sasha Thorn Jakobsen, Anna Rogers, Stefan Heinrich</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20959">https://arxiv.org/abs/2505.20959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20959">https://arxiv.org/pdf/2505.20959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20959]] Research Community Perspectives on "Intelligence" and Large Language Models(https://arxiv.org/abs/2505.20959)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the widespread use of ''artificial intelligence'' (AI) framing in Natural Language Processing (NLP) research, it is not clear what researchers mean by ''intelligence''. To that end, we present the results of a survey on the notion of ''intelligence'' among researchers and its role in the research agenda. The survey elicited complete responses from 303 researchers from a variety of fields including NLP, Machine Learning (ML), Cognitive Science, Linguistics, and Neuroscience. We identify 3 criteria of intelligence that the community agrees on the most: generalization, adaptability, & reasoning. Our results suggests that the perception of the current NLP systems as ''intelligent'' is a minority position (29%). Furthermore, only 16.2% of the respondents see developing intelligent systems as a research goal, and these respondents are more likely to consider the current systems intelligent.</li>
</ul>

<h3>Title: Semantic Communication meets System 2 ML: How Abstraction, Compositionality and Emergent Languages Shape Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Mehdi Bennis, Salem Lahlou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20964">https://arxiv.org/abs/2505.20964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20964">https://arxiv.org/pdf/2505.20964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20964]] Semantic Communication meets System 2 ML: How Abstraction, Compositionality and Emergent Languages Shape Intelligence(https://arxiv.org/abs/2505.20964)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The trajectories of 6G and AI are set for a creative collision. However, current visions for 6G remain largely incremental evolutions of 5G, while progress in AI is hampered by brittle, data-hungry models that lack robust reasoning capabilities. This paper argues for a foundational paradigm shift, moving beyond the purely technical level of communication toward systems capable of semantic understanding and effective, goal-oriented interaction. We propose a unified research vision rooted in the principles of System-2 cognition, built upon three pillars: Abstraction, enabling agents to learn meaningful world models from raw sensorimotor data; Compositionality, providing the algebraic tools to combine learned concepts and subsystems; and Emergent Communication, allowing intelligent agents to create their own adaptive and grounded languages. By integrating these principles, we lay the groundwork for truly intelligent systems that can reason, adapt, and collaborate, unifying advances in wireless communications, machine learning, and robotics under a single coherent framework.</li>
</ul>

<h3>Title: Personalized Query Auto-Completion for Long and Short-Term Interests with Adaptive Detoxification Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhibo Wang, Xiaoze Jiang, Zhiheng Qin, Enyun Yu, Han Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20966">https://arxiv.org/abs/2505.20966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20966">https://arxiv.org/pdf/2505.20966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20966]] Personalized Query Auto-Completion for Long and Short-Term Interests with Adaptive Detoxification Generation(https://arxiv.org/abs/2505.20966)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Query auto-completion (QAC) plays a crucial role in modern search systems. However, in real-world applications, there are two pressing challenges that still need to be addressed. First, there is a need for hierarchical personalized representations for users. Previous approaches have typically used users' search behavior as a single, overall representation, which proves inadequate in more nuanced generative scenarios. Additionally, query prefixes are typically short and may contain typos or sensitive information, increasing the likelihood of generating toxic content compared to traditional text generation tasks. Such toxic content can degrade user experience and lead to public relations issues. Therefore, the second critical challenge is detoxifying QAC systems. To address these two limitations, we propose a novel model (LaD) that captures personalized information from both long-term and short-term interests, incorporating adaptive detoxification. In LaD, personalized information is captured hierarchically at both coarse-grained and fine-grained levels. This approach preserves as much personalized information as possible while enabling online generation within time constraints. To move a futher step, we propose an online training method based on Reject Preference Optimization (RPO). By incorporating a special token [Reject] during both the training and inference processes, the model achieves adaptive detoxification. Consequently, the generated text presented to users is both non-toxic and relevant to the given prefix. We conduct comprehensive experiments on industrial-scale datasets and perform online A/B tests, delivering the largest single-experiment metric improvement in nearly two years of our product. Our model has been deployed on Kuaishou search, driving the primary traffic for hundreds of millions of active users. The code is available at this https URL.</li>
</ul>

<h3>Title: RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes</h3>
<ul>
<li><strong>Authors: </strong>Jiarui Zhang, Zhihao Li, Chong Wang, Bihan Wen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20967">https://arxiv.org/abs/2505.20967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20967">https://arxiv.org/pdf/2505.20967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20967]] RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes(https://arxiv.org/abs/2505.20967)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural fields (NFs) have demonstrated remarkable performance in scene reconstruction, powering various tasks such as novel view synthesis. However, existing NF methods relying on RGB or LiDAR inputs often exhibit severe fragility to adverse weather, particularly when applied in outdoor scenarios like autonomous driving. In contrast, millimeter-wave radar is inherently robust to environmental changes, while unfortunately, its integration with NFs remains largely underexplored. Besides, as outdoor driving scenarios frequently involve moving objects, making spatiotemporal modeling essential for temporally consistent novel view synthesis. To this end, we introduce RF4D, a radar-based neural field framework specifically designed for novel view synthesis in outdoor dynamic scenes. RF4D explicitly incorporates temporal information into its representation, significantly enhancing its capability to model moving objects. We further introduce a feature-level flow module that predicts latent temporal offsets between adjacent frames, enforcing temporal coherence in dynamic scene modeling. Moreover, we propose a radar-specific power rendering formulation closely aligned with radar sensing physics, improving synthesis accuracy and interoperability. Extensive experiments on public radar datasets demonstrate the superior performance of RF4D in terms of radar measurement synthesis quality and occupancy estimation accuracy, achieving especially pronounced improvements in dynamic outdoor scenarios.</li>
</ul>

<h3>Title: DreamBoothDPO: Improving Personalized Generation using Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Shamil Ayupov, Maksim Nakhodnov, Anastasia Yaschenko, Andrey Kuznetsov, Aibek Alanov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20975">https://arxiv.org/abs/2505.20975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20975">https://arxiv.org/pdf/2505.20975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20975]] DreamBoothDPO: Improving Personalized Generation using Direct Preference Optimization(https://arxiv.org/abs/2505.20975)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Personalized diffusion models have shown remarkable success in Text-to-Image (T2I) generation by enabling the injection of user-defined concepts into diverse contexts. However, balancing concept fidelity with contextual alignment remains a challenging open problem. In this work, we propose an RL-based approach that leverages the diverse outputs of T2I models to address this issue. Our method eliminates the need for human-annotated scores by generating a synthetic paired dataset for DPO-like training using external quality metrics. These better-worse pairs are specifically constructed to improve both concept fidelity and prompt adherence. Moreover, our approach supports flexible adjustment of the trade-off between image fidelity and textual alignment. Through multi-step training, our approach outperforms a naive baseline in convergence speed and output quality. We conduct extensive qualitative and quantitative analysis, demonstrating the effectiveness of our method across various architectures and fine-tuning techniques. The source code can be found at this https URL.</li>
</ul>

<h3>Title: Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing</h3>
<ul>
<li><strong>Authors: </strong>Peiming Guo, Meishan Zhang, Jianling Li, Min Zhang, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20976">https://arxiv.org/abs/2505.20976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20976">https://arxiv.org/pdf/2505.20976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20976]] Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing(https://arxiv.org/abs/2505.20976)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Cross-domain constituency parsing is still an unsolved challenge in computational linguistics since the available multi-domain constituency treebank is limited. We investigate automatic treebank generation by large language models (LLMs) in this paper. The performance of LLMs on constituency parsing is poor, therefore we propose a novel treebank generation method, LLM back generation, which is similar to the reverse process of constituency parsing. LLM back generation takes the incomplete cross-domain constituency tree with only domain keyword leaf nodes as input and fills the missing words to generate the cross-domain constituency treebank. Besides, we also introduce a span-level contrastive learning pre-training strategy to make full use of the LLM back generation treebank for cross-domain constituency parsing. We verify the effectiveness of our LLM back generation treebank coupled with contrastive learning pre-training on five target domains of MCTB. Experimental results show that our approach achieves state-of-the-art performance on average results compared with various baselines.</li>
</ul>

<h3>Title: Evaluating and Steering Modality Preferences in Multimodal Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhang, Jinlong Ma, Yongshuai Hou, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20977">https://arxiv.org/abs/2505.20977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20977">https://arxiv.org/pdf/2505.20977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20977]] Evaluating and Steering Modality Preferences in Multimodal Large Language Model(https://arxiv.org/abs/2505.20977)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have achieved remarkable performance on complex tasks with multimodal context. However, it is still understudied whether they exhibit modality preference when processing multimodal contexts. To study this question, we first build a \textbf{MC\textsuperscript{2}} benchmark under controlled evidence conflict scenarios to systematically evaluate modality preference, which is the tendency to favor one modality over another when making decisions based on multimodal conflicting evidence. Our extensive evaluation reveals that all 18 tested MLLMs generally demonstrate clear modality bias, and modality preference can be influenced by external interventions. An in-depth analysis reveals that the preference direction can be captured within the latent representations of MLLMs. Built on this, we propose a probing and steering method based on representation engineering to explicitly control modality preference without additional fine-tuning or carefully crafted prompts. Our method effectively amplifies modality preference toward a desired direction and applies to downstream tasks such as hallucination mitigation and multimodal machine translation, yielding promising improvements.</li>
</ul>

<h3>Title: Assessing the Use of Face Swapping Methods as Face Anonymizers in Videos</h3>
<ul>
<li><strong>Authors: </strong>Mustafa Ä°zzet MuÅtu, HazÄ±m Kemal Ekenel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20985">https://arxiv.org/abs/2505.20985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20985">https://arxiv.org/pdf/2505.20985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20985]] Assessing the Use of Face Swapping Methods as Face Anonymizers in Videos(https://arxiv.org/abs/2505.20985)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The increasing demand for large-scale visual data, coupled with strict privacy regulations, has driven research into anonymization methods that hide personal identities without seriously degrading data quality. In this paper, we explore the potential of face swapping methods to preserve privacy in video data. Through extensive evaluations focusing on temporal consistency, anonymity strength, and visual fidelity, we find that face swapping techniques can produce consistent facial transitions and effectively hide identities. These results underscore the suitability of face swapping for privacy-preserving video applications and lay the groundwork for future advancements in anonymization focused face-swapping models.</li>
</ul>

<h3>Title: Efficient Identity and Position Graph Embedding via Spectral-Based Random Feature Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Meng Qin, Jiahong Liu, Irwin King</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20992">https://arxiv.org/abs/2505.20992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20992">https://arxiv.org/pdf/2505.20992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20992]] Efficient Identity and Position Graph Embedding via Spectral-Based Random Feature Aggregation(https://arxiv.org/abs/2505.20992)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNNs), which capture graph structures via a feature aggregation mechanism following the graph embedding framework, have demonstrated a powerful ability to support various tasks. According to the topology properties (e.g., structural roles or community memberships of nodes) to be preserved, graph embedding can be categorized into identity and position embedding. However, it is unclear for most GNN-based methods which property they can capture. Some of them may also suffer from low efficiency and scalability caused by several time- and space-consuming procedures (e.g., feature extraction and training). From a perspective of graph signal processing, we find that high- and low-frequency information in the graph spectral domain may characterize node identities and positions, respectively. Based on this investigation, we propose random feature aggregation (RFA) for efficient identity and position embedding, serving as an extreme ablation study regarding GNN feature aggregation. RFA (i) adopts a spectral-based GNN without learnable parameters as its backbone, (ii) only uses random noises as inputs, and (iii) derives embeddings via just one feed-forward propagation (FFP). Inspired by degree-corrected spectral clustering, we further introduce a degree correction mechanism to the GNN backbone. Surprisingly, our experiments demonstrate that two variants of RFA with high- and low-pass filters can respectively derive informative identity and position embeddings via just one FFP (i.e., without any training). As a result, RFA can achieve a better trade-off between quality and efficiency for both identity and position embedding over various baselines.</li>
</ul>

<h3>Title: Who Reasons in the Large Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Jie Shao, Jianxin Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.20993">https://arxiv.org/abs/2505.20993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.20993">https://arxiv.org/pdf/2505.20993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.20993]] Who Reasons in the Large Language Models?(https://arxiv.org/abs/2505.20993)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Despite the impressive performance of large language models (LLMs), the process of endowing them with new capabilities--such as mathematical reasoning--remains largely empirical and opaque. A critical open question is whether reasoning abilities stem from the entire model, specific modules, or are merely artifacts of overfitting. In this work, we hypothesize that the reasoning capabilities in well-trained LLMs are primarily attributed to the output projection module (oproj) in the Transformer's multi-head self-attention (MHSA) mechanism. To support this hypothesis, we introduce Stethoscope for Networks (SfN), a suite of diagnostic tools designed to probe and analyze the internal behaviors of LLMs. Using SfN, we provide both circumstantial and empirical evidence suggesting that oproj plays a central role in enabling reasoning, whereas other modules contribute more to fluent dialogue. These findings offer a new perspective on LLM interpretability and open avenues for more targeted training strategies, potentially enabling more efficient and specialized LLMs.</li>
</ul>

<h3>Title: Facial Attribute Based Text Guided Face Anonymization</h3>
<ul>
<li><strong>Authors: </strong>Mustafa Ä°zzet MuÅtu, HazÄ±m Kemal Ekenel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21002">https://arxiv.org/abs/2505.21002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21002">https://arxiv.org/pdf/2505.21002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21002]] Facial Attribute Based Text Guided Face Anonymization(https://arxiv.org/abs/2505.21002)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The increasing prevalence of computer vision applications necessitates handling vast amounts of visual data, often containing personal information. While this technology offers significant benefits, it should not compromise privacy. Data privacy regulations emphasize the need for individual consent for processing personal data, hindering researchers' ability to collect high-quality datasets containing the faces of the individuals. This paper presents a deep learning-based face anonymization pipeline to overcome this challenge. Unlike most of the existing methods, our method leverages recent advancements in diffusion-based inpainting models, eliminating the need for training Generative Adversarial Networks. The pipeline employs a three-stage approach: face detection with RetinaNet, feature extraction with VGG-Face, and realistic face generation using the state-of-the-art BrushNet diffusion model. BrushNet utilizes the entire image, face masks, and text prompts specifying desired facial attributes like age, ethnicity, gender, and expression. This enables the generation of natural-looking images with unrecognizable individuals, facilitating the creation of privacy-compliant datasets for computer vision research.</li>
</ul>

<h3>Title: Uncertainty Unveiled: Can Exposure to More In-context Examples Mitigate Uncertainty for Large Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Yifei Wang, Yu Sheng, Linjing Li, Daniel Zeng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21003">https://arxiv.org/abs/2505.21003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21003">https://arxiv.org/pdf/2505.21003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21003]] Uncertainty Unveiled: Can Exposure to More In-context Examples Mitigate Uncertainty for Large Language Models?(https://arxiv.org/abs/2505.21003)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in handling long sequences have facilitated the exploration of long-context in-context learning (ICL). While much of the existing research emphasizes performance improvements driven by additional in-context examples, the influence on the trustworthiness of generated responses remains underexplored. This paper addresses this gap by investigating how increased examples influence predictive uncertainty, an essential aspect in trustworthiness. We begin by systematically quantifying the uncertainty of ICL with varying shot counts, analyzing the impact of example quantity. Through uncertainty decomposition, we introduce a novel perspective on performance enhancement, with a focus on epistemic uncertainty (EU). Our results reveal that additional examples reduce total uncertainty in both simple and complex tasks by injecting task-specific knowledge, thereby diminishing EU and enhancing performance. For complex tasks, these advantages emerge only after addressing the increased noise and uncertainty associated with longer inputs. Finally, we explore the evolution of internal confidence across layers, unveiling the mechanisms driving the reduction in uncertainty.</li>
</ul>

<h3>Title: Efficient and Unbiased Sampling from Boltzmann Distributions via Variance-Tuned Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Fengzhe Zhang, Laurence I. Midgley, JosÃ© Miguel HernÃ¡ndez-Lobato</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21005">https://arxiv.org/abs/2505.21005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21005">https://arxiv.org/pdf/2505.21005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21005]] Efficient and Unbiased Sampling from Boltzmann Distributions via Variance-Tuned Diffusion Models(https://arxiv.org/abs/2505.21005)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Score-based diffusion models (SBDMs) are powerful amortized samplers for Boltzmann distributions; however, imperfect score estimates bias downstream Monte Carlo estimates. Classical importance sampling (IS) can correct this bias, but computing exact likelihoods requires solving the probability-flow ordinary differential equation (PF-ODE), a procedure that is prohibitively costly and scales poorly with dimensionality. We introduce Variance-Tuned Diffusion Importance Sampling (VT-DIS), a lightweight post-training method that adapts the per-step noise covariance of a pretrained SBDM by minimizing the $\alpha$-divergence ($\alpha=2$) between its forward diffusion and reverse denoising trajectories. VT-DIS assigns a single trajectory-wise importance weight to the joint forward-reverse process, yielding unbiased expectation estimates at test time with negligible overhead compared to standard sampling. On the DW-4, LJ-13, and alanine-dipeptide benchmarks, VT-DIS achieves effective sample sizes of approximately 80 %, 35 %, and 3.5 %, respectively, while using only a fraction of the computational budget required by vanilla diffusion + IS or PF-ODE-based IS.</li>
</ul>

<h3>Title: A Hitchhiker's Guide to Privacy-Preserving Cryptocurrencies: A Survey on Anonymity, Confidentiality, and Auditability</h3>
<ul>
<li><strong>Authors: </strong>Matteo Nardelli, Francesco De Sclavis, Michela Iezzi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21008">https://arxiv.org/abs/2505.21008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21008">https://arxiv.org/pdf/2505.21008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21008]] A Hitchhiker's Guide to Privacy-Preserving Cryptocurrencies: A Survey on Anonymity, Confidentiality, and Auditability(https://arxiv.org/abs/2505.21008)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Cryptocurrencies and central bank digital currencies (CBDCs) are reshaping the monetary landscape, offering transparency and efficiency while raising critical concerns about user privacy and regulatory compliance. This survey provides a comprehensive and technically grounded overview of privacy-preserving digital currencies, covering both cryptocurrencies and CBDCs. We propose a taxonomy of privacy goals -- including anonymity, confidentiality, unlinkability, and auditability -- and map them to underlying cryptographic primitives, protocol mechanisms, and system architectures. Unlike previous surveys, our work adopts a design-oriented perspective, linking high-level privacy objectives to concrete implementations. We also trace the evolution of privacy-preserving currencies through three generations, highlighting shifts from basic anonymity guarantees toward more nuanced privacy-accountability trade-offs. Finally, we identify open challenges at the intersection of cryptography, distributed systems, and policy definition, which motivate further investigation into the primitives and design of digital currencies that balance real-world privacy and auditability needs.</li>
</ul>

<h3>Title: Unified Alignment Protocol: Making Sense of the Unlabeled Data in New Domains</h3>
<ul>
<li><strong>Authors: </strong>Sabbir Ahmed, Mamshad Nayeem Rizve, Abdullah Al Arafat, Jacqueline Liu, Rahim Hossain, Mohaiminul Al Nahian, Adnan Siraj Rakin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21010">https://arxiv.org/abs/2505.21010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21010">https://arxiv.org/pdf/2505.21010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21010]] Unified Alignment Protocol: Making Sense of the Unlabeled Data in New Domains(https://arxiv.org/abs/2505.21010)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Semi-Supervised Federated Learning (SSFL) is gaining popularity over conventional Federated Learning in many real-world applications. Due to the practical limitation of limited labeled data on the client side, SSFL considers that participating clients train with unlabeled data, and only the central server has the necessary resources to access limited labeled data, making it an ideal fit for real-world applications (e.g., healthcare). However, traditional SSFL assumes that the data distributions in the training phase and testing phase are the same. In practice, however, domain shifts frequently occur, making it essential for SSFL to incorporate generalization capabilities and enhance their practicality. The core challenge is improving model generalization to new, unseen domains while the client participate in SSFL. However, the decentralized setup of SSFL and unsupervised client training necessitates innovation to achieve improved generalization across domains. To achieve this, we propose a novel framework called the Unified Alignment Protocol (UAP), which consists of an alternating two-stage training process. The first stage involves training the server model to learn and align the features with a parametric distribution, which is subsequently communicated to clients without additional communication overhead. The second stage proposes a novel training algorithm that utilizes the server feature distribution to align client features accordingly. Our extensive experiments on standard domain generalization benchmark datasets across multiple model architectures reveal that proposed UAP successfully achieves SOTA generalization performance in SSFL setting.</li>
</ul>

<h3>Title: Federated Instrumental Variable Analysis via Federated Generalized Method of Moments</h3>
<ul>
<li><strong>Authors: </strong>Geetika, Somya Tyagi, Bapi Chatterjee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21012">https://arxiv.org/abs/2505.21012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21012">https://arxiv.org/pdf/2505.21012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21012]] Federated Instrumental Variable Analysis via Federated Generalized Method of Moments(https://arxiv.org/abs/2505.21012)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Instrumental variables (IV) analysis is an important applied tool for areas such as healthcare and consumer economics. For IV analysis in high-dimensional settings, the Generalized Method of Moments (GMM) using deep neural networks offers an efficient approach. With non-i.i.d. data sourced from scattered decentralized clients, federated learning is a popular paradigm for training the models while promising data privacy. However, to our knowledge, no federated algorithm for either GMM or IV analysis exists to date. In this work, we introduce federated instrumental variables analysis (FedIV) via federated generalized method of moments (FedGMM). We formulate FedGMM as a federated zero-sum game defined by a federated non-convex non-concave minimax optimization problem, which is solved using federated gradient descent ascent (FedGDA) algorithm. One key challenge arises in theoretically characterizing the federated local optimality. To address this, we present properties and existence results of clients' local equilibria via FedGDA limit points. Thereby, we show that the federated solution consistently estimates the local moment conditions of every participating client. The proposed algorithm is backed by extensive experiments to demonstrate the efficacy of our approach.</li>
</ul>

<h3>Title: Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers</h3>
<ul>
<li><strong>Authors: </strong>Charles London, Varun Kanade</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21024">https://arxiv.org/abs/2505.21024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21024">https://arxiv.org/pdf/2505.21024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21024]] Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers(https://arxiv.org/abs/2505.21024)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Pause tokens, simple filler symbols such as "...", consistently improve Transformer performance on both language and mathematical tasks, yet their theoretical effect remains unexplained. We provide the first formal separation result, proving that adding pause tokens to constant-depth, logarithmic-width Transformers strictly increases their computational expressivity. With bounded-precision activations, Transformers without pause tokens compute only a strict subset of $\mathsf{AC}^0$ functions, while adding a polynomial number of pause tokens allows them to express the entire class. For logarithmic-precision Transformers, we show that adding pause tokens achieves expressivity equivalent to $\mathsf{TC}^0$, matching known upper bounds. Empirically, we demonstrate that two-layer causally masked Transformers can learn parity when supplied with pause tokens, a function that they appear unable to learn without them. Our results provide a rigorous theoretical explanation for prior empirical findings, clarify how pause tokens interact with width, depth, and numeric precision, and position them as a distinct mechanism, complementary to chain-of-thought prompting, for enhancing Transformer reasoning.</li>
</ul>

<h3>Title: TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng He, Chun Ouyang, Lijie Wen, Cong Liu, Catarina Moreira</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21027">https://arxiv.org/abs/2505.21027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21027">https://arxiv.org/pdf/2505.21027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21027]] TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data(https://arxiv.org/abs/2505.21027)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Adversarial attacks pose a significant threat to machine learning models by inducing incorrect predictions through imperceptible perturbations to input data. While these attacks have been extensively studied in unstructured data like images, their application to tabular data presents new challenges. These challenges arise from the inherent heterogeneity and complex feature interdependencies in tabular data, which differ significantly from those in image data. To address these differences, it is crucial to consider imperceptibility as a key criterion specific to tabular data. Most current research focuses primarily on achieving effective adversarial attacks, often overlooking the importance of maintaining imperceptibility. To address this gap, we propose a new benchmark for adversarial attacks on tabular data that evaluates both effectiveness and imperceptibility. In this study, we assess the effectiveness and imperceptibility of five adversarial attacks across four models using eleven tabular datasets, including both mixed and numerical-only datasets. Our analysis explores how these factors interact and influence the overall performance of the attacks. We also compare the results across different dataset types to understand the broader implications of these findings. The findings from this benchmark provide valuable insights for improving the design of adversarial attack algorithms, thereby advancing the field of adversarial machine learning on tabular data.</li>
</ul>

<h3>Title: FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Nils Neukirch, Johanna Vielhaben, Nils Strodthoff</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21032">https://arxiv.org/abs/2505.21032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21032">https://arxiv.org/pdf/2505.21032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21032]] FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models(https://arxiv.org/abs/2505.21032)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Internal representations are crucial for understanding deep neural networks, such as their properties and reasoning patterns, but remain difficult to interpret. While mapping from feature space to input space aids in interpreting the former, existing approaches often rely on crude approximations. We propose using a conditional diffusion model - a pretrained high-fidelity diffusion model conditioned on spatially resolved feature maps - to learn such a mapping in a probabilistic manner. We demonstrate the feasibility of this approach across various pretrained image classifiers from CNNs to ViTs, showing excellent reconstruction capabilities. Through qualitative comparisons and robustness analysis, we validate our method and showcase possible applications, such as the visualization of concept steering in input space or investigations of the composite nature of the feature space. This approach has broad potential for improving feature space understanding in computer vision models.</li>
</ul>

<h3>Title: Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Seungmin Lee, Yongsang Yoo, Minhwa Jung, Min Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21033">https://arxiv.org/abs/2505.21033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21033">https://arxiv.org/pdf/2505.21033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21033]] Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation(https://arxiv.org/abs/2505.21033)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Dialogue Topic Segmentation (DTS) aims to divide dialogues into coherent segments. DTS plays a crucial role in various NLP downstream tasks, but suffers from chronic problems: data shortage, labeling ambiguity, and incremental complexity of recently proposed solutions. On the other hand, Despite advances in Large Language Models (LLMs) and reasoning strategies, these have rarely been applied to DTS. This paper introduces Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation, which utilizes LLM-based multi-step deductive reasoning to enhance DTS performance and enable case study using intermediate result. Our method employs a structured prompting approach for bidirectional context summarization, utterance intent classification, and deductive topic shift detection. In the intent classification process, we propose the generalizable intent list for domain-agnostic dialogue intent classification. Experiments in various dialogue settings demonstrate that Def-DTS consistently outperforms traditional and state-of-the-art approaches, with each subtask contributing to improved performance, particularly in reducing type 2 error. We also explore the potential for autolabeling, emphasizing the importance of LLM reasoning techniques in DTS.</li>
</ul>

<h3>Title: LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Wenhu Li, Niki van Stein, Thomas BÃ¤ck, Elena Raponi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21034">https://arxiv.org/abs/2505.21034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21034">https://arxiv.org/pdf/2505.21034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21034]] LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms(https://arxiv.org/abs/2505.21034)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Bayesian optimization (BO) is a powerful class of algorithms for optimizing expensive black-box functions, but designing effective BO algorithms remains a manual, expertise-driven task. Recent advancements in Large Language Models (LLMs) have opened new avenues for automating scientific discovery, including the automatic design of optimization algorithms. While prior work has used LLMs within optimization loops or to generate non-BO algorithms, we tackle a new challenge: Using LLMs to automatically generate full BO algorithm code. Our framework uses an evolution strategy to guide an LLM in generating Python code that preserves the key components of BO algorithms: An initial design, a surrogate model, and an acquisition function. The LLM is prompted to produce multiple candidate algorithms, which are evaluated on the established Black-Box Optimization Benchmarking (BBOB) test suite from the COmparing Continuous Optimizers (COCO) platform. Based on their performance, top candidates are selected, combined, and mutated via controlled prompt variations, enabling iterative refinement. Despite no additional fine-tuning, the LLM-generated algorithms outperform state-of-the-art BO baselines in 19 (out of 24) BBOB functions in dimension 5 and generalize well to higher dimensions, and different tasks (from the Bayesmark framework). This work demonstrates that LLMs can serve as algorithmic co-designers, offering a new paradigm for automating BO development and accelerating the discovery of novel algorithmic combinations. The source code is provided at this https URL.</li>
</ul>

<h3>Title: RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy</h3>
<ul>
<li><strong>Authors: </strong>Aiyue Chen, Bin Dong, Jingru Li, Jing Lin, Yiwu Yao, Gongyi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21036">https://arxiv.org/abs/2505.21036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21036">https://arxiv.org/pdf/2505.21036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21036]] RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy(https://arxiv.org/abs/2505.21036)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Video generation using diffusion models is highly computationally intensive, with 3D attention in Diffusion Transformer (DiT) models accounting for over 80\% of the total computational resources. In this work, we introduce {\bf RainFusion}, a novel training-free sparse attention method that exploits inherent sparsity nature in visual data to accelerate attention computation while preserving video quality. Specifically, we identify three unique sparse patterns in video generation attention calculations--Spatial Pattern, Temporal Pattern and Textural Pattern. The sparse pattern for each attention head is determined online with negligible overhead (\textasciitilde\,0.2\%) with our proposed {\bf ARM} (Adaptive Recognition Module) during inference. Our proposed {\bf RainFusion} is a plug-and-play method, that can be seamlessly integrated into state-of-the-art 3D-attention video generation models without additional training or calibration. We evaluate our method on leading open-sourced models including HunyuanVideo, OpenSoraPlan-1.2 and CogVideoX-5B, demonstrating its broad applicability and effectiveness. Experimental results show that RainFusion achieves over {\bf 2\(\times\)} speedup in attention computation while maintaining video quality, with only a minimal impact on VBench scores (-0.2\%).</li>
</ul>

<h3>Title: FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Wei Chen, Zhao Zhang, Meng Yuan, Kepeng Xu, Fuzhen Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21040">https://arxiv.org/abs/2505.21040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21040">https://arxiv.org/pdf/2505.21040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21040]] FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis(https://arxiv.org/abs/2505.21040)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we address the task of targeted sentiment analysis (TSA), which involves two sub-tasks, i.e., identifying specific aspects from reviews and determining their corresponding sentiments. Aspect extraction forms the foundation for sentiment prediction, highlighting the critical dependency between these two tasks for effective cross-task knowledge transfer. While most existing studies adopt a multi-task learning paradigm to align task-specific features in the latent space, they predominantly rely on coarse-grained knowledge transfer. Such approaches lack fine-grained control over aspect-sentiment relationships, often assuming uniform sentiment polarity within related aspects. This oversimplification neglects contextual cues that differentiate sentiments, leading to negative transfer. To overcome these limitations, we propose FCKT, a fine-grained cross-task knowledge transfer framework tailored for TSA. By explicitly incorporating aspect-level information into sentiment prediction, FCKT achieves fine-grained knowledge transfer, effectively mitigating negative transfer and enhancing task performance. Experiments on three datasets, including comparisons with various baselines and large language models (LLMs), demonstrate the effectiveness of FCKT. The source code is available on this https URL.</li>
</ul>

<h3>Title: A domain adaptation neural network for digital twin-supported fault diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Zhenling Chen, Haiwei Fu, Zhiguo Zeng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21046">https://arxiv.org/abs/2505.21046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21046">https://arxiv.org/pdf/2505.21046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21046]] A domain adaptation neural network for digital twin-supported fault diagnosis(https://arxiv.org/abs/2505.21046)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Digital twins offer a promising solution to the lack of sufficient labeled data in deep learning-based fault diagnosis by generating simulated data for model training. However, discrepancies between simulation and real-world systems can lead to a significant drop in performance when models are applied in real scenarios. To address this issue, we propose a fault diagnosis framework based on Domain-Adversarial Neural Networks (DANN), which enables knowledge transfer from simulated (source domain) to real-world (target domain) data. We evaluate the proposed framework using a publicly available robotics fault diagnosis dataset, which includes 3,600 sequences generated by a digital twin model and 90 real sequences collected from physical systems. The DANN method is compared with commonly used lightweight deep learning models such as CNN, TCN, Transformer, and LSTM. Experimental results show that incorporating domain adaptation significantly improves the diagnostic performance. For example, applying DANN to a baseline CNN model improves its accuracy from 70.00% to 80.22% on real-world test data, demonstrating the effectiveness of domain adaptation in bridging the sim-to-real gap.</li>
</ul>

<h3>Title: Robust Video-Based Pothole Detection and Area Estimation for Intelligent Vehicles with Depth Map and Kalman Smoothing</h3>
<ul>
<li><strong>Authors: </strong>Dehao Wang, Haohang Zhu, Yiwen Xu, Kaiqi Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21049">https://arxiv.org/abs/2505.21049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21049">https://arxiv.org/pdf/2505.21049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21049]] Robust Video-Based Pothole Detection and Area Estimation for Intelligent Vehicles with Depth Map and Kalman Smoothing(https://arxiv.org/abs/2505.21049)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Road potholes pose a serious threat to driving safety and comfort, making their detection and assessment a critical task in fields such as autonomous driving. When driving vehicles, the operators usually avoid large potholes and approach smaller ones at reduced speeds to ensure safety. Therefore, accurately estimating pothole area is of vital importance. Most existing vision-based methods rely on distance priors to construct geometric models. However, their performance is susceptible to variations in camera angles and typically relies on the assumption of a flat road surface, potentially leading to significant errors in complex real-world environments. To address these problems, a robust pothole area estimation framework that integrates object detection and monocular depth estimation in a video stream is proposed in this paper. First, to enhance pothole feature extraction and improve the detection of small potholes, ACSH-YOLOv8 is proposed with ACmix module and the small object detection head. Then, the BoT-SORT algorithm is utilized for pothole tracking, while DepthAnything V2 generates depth maps for each frame. With the obtained depth maps and potholes labels, a novel Minimum Bounding Triangulated Pixel (MBTP) method is proposed for pothole area estimation. Finally, Kalman Filter based on Confidence and Distance (CDKF) is developed to maintain consistency of estimation results across consecutive frames. The results show that ACSH-YOLOv8 model achieves an AP(50) of 76.6%, representing a 7.6% improvement over YOLOv8. Through CDKF optimization across consecutive frames, pothole predictions become more robust, thereby enhancing the method's practical applicability.</li>
</ul>

<h3>Title: Advancing high-fidelity 3D and Texture Generation with 2.5D latents</h3>
<ul>
<li><strong>Authors: </strong>Xin Yang, Jiantao Lin, Yingjie Xu, Haodong Li, Yingcong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21050">https://arxiv.org/abs/2505.21050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21050">https://arxiv.org/pdf/2505.21050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21050]] Advancing high-fidelity 3D and Texture Generation with 2.5D latents(https://arxiv.org/abs/2505.21050)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite the availability of large-scale 3D datasets and advancements in 3D generative models, the complexity and uneven quality of 3D geometry and texture data continue to hinder the performance of 3D generation techniques. In most existing approaches, 3D geometry and texture are generated in separate stages using different models and non-unified representations, frequently leading to unsatisfactory coherence between geometry and texture. To address these challenges, we propose a novel framework for joint generation of 3D geometry and texture. Specifically, we focus in generate a versatile 2.5D representations that can be seamlessly transformed between 2D and 3D. Our approach begins by integrating multiview RGB, normal, and coordinate images into a unified representation, termed as 2.5D latents. Next, we adapt pre-trained 2D foundation models for high-fidelity 2.5D generation, utilizing both text and image conditions. Finally, we introduce a lightweight 2.5D-to-3D refiner-decoder framework that efficiently generates detailed 3D representations from 2.5D images. Extensive experiments demonstrate that our model not only excels in generating high-quality 3D objects with coherent structure and color from text and image inputs but also significantly outperforms existing methods in geometry-conditioned texture generation.</li>
</ul>

<h3>Title: SHE-LoRA: Selective Homomorphic Encryption for Federated Tuning with Heterogeneous LoRA</h3>
<ul>
<li><strong>Authors: </strong>Jianmin Liu, Li Yan, Borui Li, Lei Yu, Chao Shen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21051">https://arxiv.org/abs/2505.21051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21051">https://arxiv.org/pdf/2505.21051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21051]] SHE-LoRA: Selective Homomorphic Encryption for Federated Tuning with Heterogeneous LoRA(https://arxiv.org/abs/2505.21051)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack, federate, large language model</a></li>
<li><strong>Abstract: </strong>Federated fine-tuning of large language models (LLMs) is critical for improving their performance in handling domain-specific tasks. However, prior work has shown that clients' private data can actually be recovered via gradient inversion attacks. Existing privacy preservation techniques against such attacks typically entail performance degradation and high costs, making them ill-suited for clients with heterogeneous data distributions and device capabilities. In this paper, we propose SHE-LoRA, which integrates selective homomorphic encryption (HE) and low-rank adaptation (LoRA) to enable efficient and privacy-preserving federated tuning of LLMs in cross-device environment. Heterogeneous clients adaptively select partial model parameters for homomorphic encryption based on parameter sensitivity assessment, with the encryption subset obtained via negotiation. To ensure accurate model aggregation, we design a column-aware secure aggregation method and customized reparameterization techniques to align the aggregation results with the heterogeneous device capabilities of clients. Extensive experiments demonstrate that SHE-LoRA maintains performance comparable to non-private baselines, achieves strong resistance to the state-of-the-art attacks, and significantly reduces communication overhead by 94.901\% and encryption computation overhead by 99.829\%, compared to baseline. Our code is accessible at this https URL.</li>
</ul>

<h3>Title: Inverse Virtual Try-On: Generating Multi-Category Product-Style Images from Clothed Individuals</h3>
<ul>
<li><strong>Authors: </strong>Davide Lobba, Fulvio Sanguigni, Bin Ren, Marcella Cornia, Rita Cucchiara, Nicu Sebe</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21062">https://arxiv.org/abs/2505.21062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21062">https://arxiv.org/pdf/2505.21062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21062]] Inverse Virtual Try-On: Generating Multi-Category Product-Style Images from Clothed Individuals(https://arxiv.org/abs/2505.21062)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>While virtual try-on (VTON) systems aim to render a garment onto a target person image, this paper tackles the novel task of virtual try-off (VTOFF), which addresses the inverse problem: generating standardized product images of garments from real-world photos of clothed individuals. Unlike VTON, which must resolve diverse pose and style variations, VTOFF benefits from a consistent and well-defined output format -- typically a flat, lay-down-style representation of the garment -- making it a promising tool for data generation and dataset enhancement. However, existing VTOFF approaches face two major limitations: (i) difficulty in disentangling garment features from occlusions and complex poses, often leading to visual artifacts, and (ii) restricted applicability to single-category garments (e.g., upper-body clothes only), limiting generalization. To address these challenges, we present Text-Enhanced MUlti-category Virtual Try-Off (TEMU-VTOFF), a novel architecture featuring a dual DiT-based backbone with a modified multimodal attention mechanism for robust garment feature extraction. Our architecture is designed to receive garment information from multiple modalities like images, text, and masks to work in a multi-category setting. Finally, we propose an additional alignment module to further refine the generated visual details. Experiments on VITON-HD and Dress Code datasets show that TEMU-VTOFF sets a new state-of-the-art on the VTOFF task, significantly improving both visual quality and fidelity to the target garments.</li>
</ul>

<h3>Title: Minute-Long Videos with Dual Parallelisms</h3>
<ul>
<li><strong>Authors: </strong>Zeqing Wang, Bowen Zheng, Xingyi Yang, Yuecong Xu, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21070">https://arxiv.org/abs/2505.21070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21070">https://arxiv.org/pdf/2505.21070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21070]] Minute-Long Videos with Dual Parallelisms(https://arxiv.org/abs/2505.21070)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating an entire video on a single GPU, we parallelize both temporal frames and model layers across GPUs. However, a naive implementation of this division faces a key limitation: since diffusion models require synchronized noise levels across frames, this implementation leads to the serialization of original parallelisms. We leverage a block-wise denoising scheme to handle this. Namely, we process a sequence of frame blocks through the pipeline with progressively decreasing noise levels. Each GPU handles a specific block and layer subset while passing previous results to the next GPU, enabling asynchronous computation and communication. To further optimize performance, we incorporate two key enhancements. Firstly, a feature cache is implemented on each GPU to store and reuse features from the prior block as context, minimizing inter-GPU communication and redundant computation. Secondly, we employ a coordinated noise initialization strategy, ensuring globally consistent temporal dynamics by sharing initial noise patterns across GPUs without extra resource costs. Together, these enable fast, artifact-free, and infinitely long video generation. Applied to the latest diffusion transformer video generator, our method efficiently produces 1,025-frame videos with up to 6.54$\times$ lower latency and 1.48$\times$ lower memory cost on 8$\times$RTX 4090 GPUs.</li>
</ul>

<h3>Title: Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Ekaterina Fadeeva, Aleksandr Rubashevskii, Roman Vashurin, Shehzaad Dhuliawala, Artem Shelmanov, Timothy Baldwin, Preslav Nakov, Mrinmaya Sachan, Maxim Panov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21072">https://arxiv.org/abs/2505.21072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21072">https://arxiv.org/pdf/2505.21072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21072]] Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation(https://arxiv.org/abs/2505.21072)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) enhanced with external knowledge retrieval, an approach known as Retrieval-Augmented Generation (RAG), have shown strong performance in open-domain question answering. However, RAG systems remain susceptible to hallucinations: factually incorrect outputs that may arise either from inconsistencies in the model's internal knowledge or incorrect use of the retrieved context. Existing approaches often conflate factuality with faithfulness to the retrieved context, misclassifying factually correct statements as hallucinations if they are not directly supported by the retrieval. In this paper, we introduce FRANQ (Faithfulness-based Retrieval Augmented UNcertainty Quantification), a novel method for hallucination detection in RAG outputs. FRANQ applies different Uncertainty Quantification (UQ) techniques to estimate factuality based on whether a statement is faithful to the retrieved context or not. To evaluate FRANQ and other UQ techniques for RAG, we present a new long-form Question Answering (QA) dataset annotated for both factuality and faithfulness, combining automated labeling with manual validation of challenging examples. Extensive experiments on long- and short-form QA across multiple datasets and LLMs show that FRANQ achieves more accurate detection of factual errors in RAG-generated responses compared to existing methods.</li>
</ul>

<h3>Title: Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yichuan Cao, Yibo Miao, Xiao-Shan Gao, Yinpeng Dong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21074">https://arxiv.org/abs/2505.21074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21074">https://arxiv.org/pdf/2505.21074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21074]] Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling(https://arxiv.org/abs/2505.21074)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) models raise ethical and safety concerns due to their potential to generate inappropriate or harmful images. Evaluating these models' security through red-teaming is vital, yet white-box approaches are limited by their need for internal access, complicating their use with closed-source models. Moreover, existing black-box methods often assume knowledge about the model's specific defense mechanisms, limiting their utility in real-world commercial API scenarios. A significant challenge is how to evade unknown and diverse defense mechanisms. To overcome this difficulty, we propose a novel Rule-based Preference modeling Guided Red-Teaming (RPG-RT), which iteratively employs LLM to modify prompts to query and leverages feedback from T2I systems for fine-tuning the LLM. RPG-RT treats the feedback from each iteration as a prior, enabling the LLM to dynamically adapt to unknown defense mechanisms. Given that the feedback is often labeled and coarse-grained, making it difficult to utilize directly, we further propose rule-based preference modeling, which employs a set of rules to evaluate desired or undesired feedback, facilitating finer-grained control over the LLM's dynamic adaptation process. Extensive experiments on nineteen T2I systems with varied safety mechanisms, three online commercial API services, and T2V models verify the superiority and practicality of our approach.</li>
</ul>

<h3>Title: DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic City Understanding</h3>
<ul>
<li><strong>Authors: </strong>Weihao Xuan, Junjue Wang, Heli Qi, Zihang Chen, Zhuo Zheng, Yanfei Zhong, Junshi Xia, Naoto Yokoya</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21076">https://arxiv.org/abs/2505.21076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21076">https://arxiv.org/pdf/2505.21076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21076]] DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic City Understanding(https://arxiv.org/abs/2505.21076)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Multimodal large language models have demonstrated remarkable capabilities in visual understanding, but their application to long-term Earth observation analysis remains limited, primarily focusing on single-temporal or bi-temporal imagery. To address this gap, we introduce DVL-Suite, a comprehensive framework for analyzing long-term urban dynamics through remote sensing imagery. Our suite comprises 15,063 high-resolution (1.0m) multi-temporal images spanning 42 megacities in the U.S. from 2005 to 2023, organized into two components: DVL-Bench and DVL-Instruct. The DVL-Bench includes seven urban understanding tasks, from fundamental change detection (pixel-level) to quantitative analyses (regional-level) and comprehensive urban narratives (scene-level), capturing diverse urban dynamics including expansion/transformation patterns, disaster assessment, and environmental challenges. We evaluate 17 state-of-the-art multimodal large language models and reveal their limitations in long-term temporal understanding and quantitative analysis. These challenges motivate the creation of DVL-Instruct, a specialized instruction-tuning dataset designed to enhance models' capabilities in multi-temporal Earth observation. Building upon this dataset, we develop DVLChat, a baseline model capable of both image-level question-answering and pixel-level segmentation, facilitating a comprehensive understanding of city dynamics through language interactions.</li>
</ul>

<h3>Title: Efficient Large Language Model Inference with Neural Block Linearization</h3>
<ul>
<li><strong>Authors: </strong>Mete Erdogan, Francesco Tonin, Volkan Cevher</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21077">https://arxiv.org/abs/2505.21077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21077">https://arxiv.org/pdf/2505.21077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21077]] Efficient Large Language Model Inference with Neural Block Linearization(https://arxiv.org/abs/2505.21077)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The high inference demands of transformer-based Large Language Models (LLMs) pose substantial challenges in their deployment. To this end, we introduce Neural Block Linearization (NBL), a novel framework for accelerating transformer model inference by replacing self-attention layers with linear approximations derived from Linear Minimum Mean Squared Error estimators. NBL leverages Canonical Correlation Analysis to compute a theoretical upper bound on the approximation error. Then, we use this bound as a criterion for substitution, selecting the LLM layers with the lowest linearization error. NBL can be efficiently applied to pre-trained LLMs without the need for fine-tuning. In experiments, NBL achieves notable computational speed-ups while preserving competitive accuracy on multiple reasoning benchmarks. For instance, applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B increases the inference speed by 32% with less than 1% accuracy trade-off, making it a flexible and promising solution to improve the inference efficiency of LLMs.</li>
</ul>

<h3>Title: Uni3D-MoE: Scalable Multimodal 3D Scene Understanding via Mixture of Experts</h3>
<ul>
<li><strong>Authors: </strong>Yue Zhang, Yingzhao Jian, Hehe Fan, Yi Yang, Roger Zimmermann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21079">https://arxiv.org/abs/2505.21079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21079">https://arxiv.org/pdf/2505.21079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21079]] Uni3D-MoE: Scalable Multimodal 3D Scene Understanding via Mixture of Experts(https://arxiv.org/abs/2505.21079)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in multimodal large language models (MLLMs) have demonstrated considerable potential for comprehensive 3D scene understanding. However, existing approaches typically utilize only one or a limited subset of 3D modalities, resulting in incomplete representations of 3D scenes and reduced interpretive accuracy. Furthermore, different types of queries inherently depend on distinct modalities, indicating that uniform processing of all modality tokens may fail to effectively capture query-specific context. To address these challenges, we propose Uni3D-MoE, a sparse Mixture-of-Experts (MoE)-based 3D MLLM designed to enable adaptive 3D multimodal fusion. Specifically, Uni3D-MoE integrates a comprehensive set of 3D modalities, including multi-view RGB and depth images, bird's-eye-view (BEV) maps, point clouds, and voxel representations. At its core, our framework employs a learnable routing mechanism within the sparse MoE-based large language model, dynamically selecting appropriate experts at the token level. Each expert specializes in processing multimodal tokens based on learned modality preferences, thus facilitating flexible collaboration tailored to diverse task-specific requirements. Extensive evaluations on standard 3D scene understanding benchmarks and specialized datasets demonstrate the efficacy of Uni3D-MoE.</li>
</ul>

<h3>Title: LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jieyong Kim, Tongyoung Kim, Soonjin Yoon, Jaehyung Kim, Dongha Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21082">https://arxiv.org/abs/2505.21082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21082">https://arxiv.org/pdf/2505.21082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21082]] LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models(https://arxiv.org/abs/2505.21082)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently achieved impressive performance across a wide range of natural language tasks and are now widely used in real-world applications. Among them, black-box LLMs--served via APIs without access to model internals--are especially dominant due to their scalability and ease of deployment. Despite their strong capabilities, these models typically produce generalized responses that overlook personal preferences and reasoning styles. This has led to growing interest in black-box LLM personalization, which aims to tailor model outputs to user-specific context without modifying model parameters. However, existing approaches primarily focus on response-level personalization, attempting to match final outputs without modeling personal thought process. To address this limitation, we propose RPM, a framework for reasoning-level personalization that aligns the model's reasoning process with a user's personalized logic. RPM first constructs statistical user-specific factors by extracting and grouping response-influential features from user history. It then builds personalized reasoning paths that reflect how these factors are used in context. In the inference stage, RPM retrieves reasoning-aligned examples for new queries via feature-level similarity and performs inference conditioned on the structured factors and retrieved reasoning paths, enabling the model to follow user-specific reasoning trajectories. This reasoning-level personalization enhances both predictive accuracy and interpretability by grounding model outputs in user-specific logic through structured information. Extensive experiments across diverse tasks show that RPM consistently outperforms response-level personalization methods, demonstrating the effectiveness of reasoning-level personalization in black-box LLMs.</li>
</ul>

<h3>Title: DisasterM3: A Remote Sensing Vision-Language Dataset for Disaster Damage Assessment and Response</h3>
<ul>
<li><strong>Authors: </strong>Junjue Wang, Weihao Xuan, Heli Qi, Zhihao Liu, Kunyi Liu, Yuhan Wu, Hongruixuan Chen, Jian Song, Junshi Xia, Zhuo Zheng, Naoto Yokoya</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21089">https://arxiv.org/abs/2505.21089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21089">https://arxiv.org/pdf/2505.21089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21089]] DisasterM3: A Remote Sensing Vision-Language Dataset for Disaster Damage Assessment and Response(https://arxiv.org/abs/2505.21089)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large vision-language models (VLMs) have made great achievements in Earth vision. However, complex disaster scenes with diverse disaster types, geographic regions, and satellite sensors have posed new challenges for VLM applications. To fill this gap, we curate a remote sensing vision-language dataset (DisasterM3) for global-scale disaster assessment and response. DisasterM3 includes 26,988 bi-temporal satellite images and 123k instruction pairs across 5 continents, with three characteristics: 1) Multi-hazard: DisasterM3 involves 36 historical disaster events with significant impacts, which are categorized into 10 common natural and man-made disasters. 2)Multi-sensor: Extreme weather during disasters often hinders optical sensor imaging, making it necessary to combine Synthetic Aperture Radar (SAR) imagery for post-disaster scenes. 3) Multi-task: Based on real-world scenarios, DisasterM3 includes 9 disaster-related visual perception and reasoning tasks, harnessing the full potential of VLM's reasoning ability with progressing from disaster-bearing body recognition to structural damage assessment and object relational reasoning, culminating in the generation of long-form disaster reports. We extensively evaluated 14 generic and remote sensing VLMs on our benchmark, revealing that state-of-the-art models struggle with the disaster tasks, largely due to the lack of a disaster-specific corpus, cross-sensor gap, and damage object counting insensitivity. Focusing on these issues, we fine-tune four VLMs using our dataset and achieve stable improvements across all tasks, with robust cross-sensor and cross-disaster generalization capabilities.</li>
</ul>

<h3>Title: BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Daeen Kabir, Minhajur Rahman Chowdhury Mahim, Sheikh Shafayat, Adnan Sadik, Arian Ahmed, Eunsu Kim, Alice Oh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21092">https://arxiv.org/abs/2505.21092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21092">https://arxiv.org/pdf/2505.21092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21092]] BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge(https://arxiv.org/abs/2505.21092)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this work, we introduce BLUCK, a new dataset designed to measure the performance of Large Language Models (LLMs) in Bengali linguistic understanding and cultural knowledge. Our dataset comprises 2366 multiple-choice questions (MCQs) carefully curated from compiled collections of several college and job level examinations and spans 23 categories covering knowledge on Bangladesh's culture and history and Bengali linguistics. We benchmarked BLUCK using 6 proprietary and 3 open-source LLMs - including GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro, Llama-3.3-70B-Instruct, and DeepSeekV3. Our results show that while these models perform reasonably well overall, they, however, struggles in some areas of Bengali phonetics. Although current LLMs' performance on Bengali cultural and linguistic contexts is still not comparable to that of mainstream languages like English, our results indicate Bengali's status as a mid-resource language. Importantly, BLUCK is also the first MCQ-based evaluation benchmark that is centered around native Bengali culture, history, and linguistics.</li>
</ul>

<h3>Title: Thinker: Learning to Think Fast and Slow</h3>
<ul>
<li><strong>Authors: </strong>Stephen Chung, Wenyu Du, Jie Fu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21097">https://arxiv.org/abs/2505.21097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21097">https://arxiv.org/pdf/2505.21097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21097]] Thinker: Learning to Think Fast and Slow(https://arxiv.org/abs/2505.21097)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training.</li>
</ul>

<h3>Title: Instance Data Condensation for Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Tianhao Peng, Ho Man Kwan, Yuxuan Jiang, Ge Gao, Fan Zhang, Xiaozhong Xu, Shan Liu, David Bull</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21099">https://arxiv.org/abs/2505.21099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21099">https://arxiv.org/pdf/2505.21099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21099]] Instance Data Condensation for Image Super-Resolution(https://arxiv.org/abs/2505.21099)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction</a></li>
<li><strong>Abstract: </strong>Deep learning based image Super-Resolution (ISR) relies on large training datasets to optimize model generalization; this requires substantial computational and storage resources during training. While dataset condensation has shown potential in improving data efficiency and privacy for high-level computer vision tasks, it has not yet been fully exploited for ISR. In this paper, we propose a novel Instance Data Condensation (IDC) framework specifically for ISR, which achieves instance-level data condensation through Random Local Fourier Feature Extraction and Multi-level Feature Distribution Matching. This aims to optimize feature distributions at both global and local levels and obtain high-quality synthesized training content with fine detail. This framework has been utilized to condense the most commonly used training dataset for ISR, DIV2K, with a 10% condensation rate. The resulting synthetic dataset offers comparable or (in certain cases) even better performance compared to the original full dataset and excellent training stability when used to train various popular ISR models. To the best of our knowledge, this is the first time that a condensed/synthetic dataset (with a 10% data volume) has demonstrated such performance. The source code and the synthetic dataset have been made available at this https URL.</li>
</ul>

<h3>Title: Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance</h3>
<ul>
<li><strong>Authors: </strong>Badr Moufad, Yazid Janati, Alain Durmus, Ahmed Ghorbel, Eric Moulines, Jimmy Olsson</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21101">https://arxiv.org/abs/2505.21101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21101">https://arxiv.org/pdf/2505.21101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21101]] Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance(https://arxiv.org/abs/2505.21101)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Classifier-Free Guidance (CFG) is a widely used technique for improving conditional diffusion models by linearly combining the outputs of conditional and unconditional denoisers. While CFG enhances visual quality and improves alignment with prompts, it often reduces sample diversity, leading to a challenging trade-off between quality and diversity. To address this issue, we make two key contributions. First, CFG generally does not correspond to a well-defined denoising diffusion model (DDM). In particular, contrary to common intuition, CFG does not yield samples from the target distribution associated with the limiting CFG score as the noise level approaches zero -- where the data distribution is tilted by a power $w \gt 1$ of the conditional distribution. We identify the missing component: a RÃ©nyi divergence term that acts as a repulsive force and is required to correct CFG and render it consistent with a proper DDM. Our analysis shows that this correction term vanishes in the low-noise limit. Second, motivated by this insight, we propose a Gibbs-like sampling procedure to draw samples from the desired tilted distribution. This method starts with an initial sample from the conditional diffusion model without CFG and iteratively refines it, preserving diversity while progressively enhancing sample quality. We evaluate our approach on both image and text-to-audio generation tasks, demonstrating substantial improvements over CFG across all considered metrics. The code is available at this https URL</li>
</ul>

<h3>Title: A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction</h3>
<ul>
<li><strong>Authors: </strong>Bogdan Bogachov, Yaoyao Fiona Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21109">https://arxiv.org/abs/2505.21109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21109">https://arxiv.org/pdf/2505.21109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21109]] A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction(https://arxiv.org/abs/2505.21109)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>Despite recent advancements in domain adaptation techniques for large language models, these methods remain computationally intensive, and the resulting models can still exhibit hallucination issues. Most existing adaptation methods do not prioritize reducing the computational resources required for fine-tuning and inference of language models. Hallucination issues have gradually decreased with each new model release. However, they remain prevalent in engineering contexts, where generating well-structured text with minimal errors and inconsistencies is critical. This work introduces a novel approach called the Small Language Graph (SLG), which is a lightweight adaptation solution designed to address the two key challenges outlined above. The system is structured in the form of a graph, where each node represents a lightweight expert - a small language model fine-tuned on specific and concise texts. The results of this study have shown that SLG was able to surpass conventional fine-tuning methods on the Exact Match metric by 3 times. Additionally, the fine-tuning process was 1.7 times faster compared to that of a larger stand-alone language model. These findings introduce a potential for small to medium-sized engineering companies to confidently use generative AI technologies, such as LLMs, without the necessity to invest in expensive computational resources. Also, the graph architecture and the small size of expert nodes offer a possible opportunity for distributed AI systems, thus potentially diverting the global need for expensive centralized compute clusters.</li>
</ul>

<h3>Title: Differentiable Solver Search for Fast Diffusion Sampling</h3>
<ul>
<li><strong>Authors: </strong>Shuai Wang, Zexian Li, Qipeng zhang, Tianhui Song, Xubin Li, Tiezheng Ge, Bo Zheng, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21114">https://arxiv.org/abs/2505.21114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21114">https://arxiv.org/pdf/2505.21114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21114]] Differentiable Solver Search for Fast Diffusion Sampling(https://arxiv.org/abs/2505.21114)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable generation quality but at the cost of numerous function evaluations. Recently, advanced ODE-based solvers have been developed to mitigate the substantial computational demands of reverse-diffusion solving under limited sampling steps. However, these solvers, heavily inspired by Adams-like multistep methods, rely solely on t-related Lagrange interpolation. We show that t-related Lagrange interpolation is suboptimal for diffusion model and reveal a compact search space comprised of time steps and solver coefficients. Building on our analysis, we propose a novel differentiable solver search algorithm to identify more optimal solver. Equipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and FlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256 with only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of 2.33 with only 10 steps. Notably, our searched solver outperforms traditional solvers by a significant margin. Moreover, our searched solver demonstrates generality across various model architectures, resolutions, and model sizes.</li>
</ul>

<h3>Title: Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA</h3>
<ul>
<li><strong>Authors: </strong>Sergey Pletenev, Maria Marina, Nikolay Ivanov, Daria Galimzianova, Nikita Krayko, Mikhail Salnikov, Vasily Konovalov, Alexander Panchenko, Viktor Moskvoretskii</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21115">https://arxiv.org/abs/2505.21115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21115">https://arxiv.org/pdf/2505.21115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21115]] Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA(https://arxiv.org/abs/2505.21115)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often hallucinate in question answering (QA) tasks. A key yet underexplored factor contributing to this is the temporality of questions -- whether they are evergreen (answers remain stable over time) or mutable (answers change). In this work, we introduce EverGreenQA, the first multilingual QA dataset with evergreen labels, supporting both evaluation and training. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they encode question temporality explicitly (via verbalized judgments) or implicitly (via uncertainty signals). We also train EG-E5, a lightweight multilingual classifier that achieves SoTA performance on this task. Finally, we demonstrate the practical utility of evergreen classification across three applications: improving self-knowledge estimation, filtering QA datasets, and explaining GPT-4o retrieval behavior.</li>
</ul>

<h3>Title: ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Adeela Islam, Stefano Fiorini, Stuart James, Pietro Morerio, Alessio Del Bue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21117">https://arxiv.org/abs/2505.21117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21117">https://arxiv.org/pdf/2505.21117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21117]] ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco Reconstruction(https://arxiv.org/abs/2505.21117)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The task of reassembly is a significant challenge across multiple domains, including archaeology, genomics, and molecular docking, requiring the precise placement and orientation of elements to reconstruct an original structure. In this work, we address key limitations in state-of-the-art Deep Learning methods for reassembly, namely i) scalability; ii) multimodality; and iii) real-world applicability: beyond square or simple geometric shapes, realistic and complex erosion, or other real-world problems. We propose ReassembleNet, a method that reduces complexity by representing each input piece as a set of contour keypoints and learning to select the most informative ones by Graph Neural Networks pooling inspired techniques. ReassembleNet effectively lowers computational complexity while enabling the integration of features from multiple modalities, including both geometric and texture data. Further enhanced through pretraining on a semi-synthetic dataset. We then apply diffusion-based pose estimation to recover the original structure. We improve on prior methods by 55% and 86% for RMSE Rotation and Translation, respectively.</li>
</ul>

<h3>Title: Universal Value-Function Uncertainties</h3>
<ul>
<li><strong>Authors: </strong>Moritz A. Zanger, Max Weltevrede, Yaniv Oren, Pascal R. Van der Vaart, Caroline Horsch, Wendelin BÃ¶hmer, Matthijs T. J. Spaan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21119">https://arxiv.org/abs/2505.21119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21119">https://arxiv.org/pdf/2505.21119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21119]] Universal Value-Function Uncertainties(https://arxiv.org/abs/2505.21119)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Estimating epistemic uncertainty in value functions is a crucial challenge for many aspects of reinforcement learning (RL), including efficient exploration, safe decision-making, and offline RL. While deep ensembles provide a robust method for quantifying value uncertainty, they come with significant computational overhead. Single-model methods, while computationally favorable, often rely on heuristics and typically require additional propagation mechanisms for myopic uncertainty estimates. In this work we introduce universal value-function uncertainties (UVU), which, similar in spirit to random network distillation (RND), quantify uncertainty as squared prediction errors between an online learner and a fixed, randomly initialized target network. Unlike RND, UVU errors reflect policy-conditional value uncertainty, incorporating the future uncertainties any given policy may encounter. This is due to the training procedure employed in UVU: the online network is trained using temporal difference learning with a synthetic reward derived from the fixed, randomly initialized target network. We provide an extensive theoretical analysis of our approach using neural tangent kernel (NTK) theory and show that in the limit of infinite network width, UVU errors are exactly equivalent to the variance of an ensemble of independent universal value functions. Empirically, we show that UVU achieves equal performance to large ensembles on challenging multi-task offline RL settings, while offering simplicity and substantial computational savings.</li>
</ul>

<h3>Title: ColorGo: Directed Concolic Execution</h3>
<ul>
<li><strong>Authors: </strong>Jia Li, Jiacheng Shen, Yuxin Su, Michael R. Lyu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21130">https://arxiv.org/abs/2505.21130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21130">https://arxiv.org/pdf/2505.21130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21130]] ColorGo: Directed Concolic Execution(https://arxiv.org/abs/2505.21130)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Directed fuzzing is a critical technique in cybersecurity, targeting specific sections of a program. This approach is essential in various security-related domains such as crash reproduction, patch testing, and vulnerability detection. Despite its importance, current directed fuzzing methods exhibit a trade-off between efficiency and effectiveness. For instance, directed grey-box fuzzing, while efficient in generating fuzzing inputs, lacks sufficient precision. The low precision causes time wasted on executing code that cannot help reach the target site. Conversely, interpreter- or observer-based directed symbolic execution can produce high-quality inputs while incurring non-negligible runtime overhead. These limitations undermine the feasibility of directed fuzzers in real-world scenarios. To kill the birds of efficiency and effectiveness with one stone, in this paper, we involve compilation-based concolic execution into directed fuzzing and present ColorGo, achieving high scalability while preserving the high precision from symbolic execution. ColorGo is a new directed whitebox fuzzer that concretely executes the instrumented program with constraint-solving capability on generated input. It guides the exploration by \textit{incremental coloration}, including static reachability analysis and dynamic feasibility analysis. We evaluated ColorGo on diverse real-world programs and demonstrated that ColorGo outperforms AFLGo by up to \textbf{100x} in reaching target sites and reproducing target crashes.</li>
</ul>

<h3>Title: Robust and Computation-Aware Gaussian Processes</h3>
<ul>
<li><strong>Authors: </strong>Marshal Arijona Sinaga, Julien Martinelli, Samuel Kaski</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21133">https://arxiv.org/abs/2505.21133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21133">https://arxiv.org/pdf/2505.21133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21133]] Robust and Computation-Aware Gaussian Processes(https://arxiv.org/abs/2505.21133)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Gaussian processes (GPs) are widely used for regression and optimization tasks such as Bayesian optimization (BO) due to their expressiveness and principled uncertainty estimates. However, in settings with large datasets corrupted by outliers, standard GPs and their sparse approximations struggle with computational tractability and robustness. We introduce Robust Computation-aware Gaussian Process (RCaGP), a novel GP model that jointly addresses these challenges by combining a principled treatment of approximation-induced uncertainty with robust generalized Bayesian updating. The key insight is that robustness and approximation-awareness are not orthogonal but intertwined: approximations can exacerbate the impact of outliers, and mitigating one without the other is insufficient. Unlike previous work that focuses narrowly on either robustness or approximation quality, RCaGP combines both in a principled and scalable framework, thus effectively managing both outliers and computational uncertainties introduced by approximations such as low-rank matrix multiplications. Our model ensures more conservative and reliable uncertainty estimates, a property we rigorously demonstrate. Additionally, we establish a robustness property and show that the mean function is key to preserving it, motivating a tailored model selection scheme for robust mean functions. Empirical results confirm that solving these challenges jointly leads to superior performance across both clean and outlier-contaminated settings, both on regression and high-throughput Bayesian optimization benchmarks.</li>
</ul>

<h3>Title: Learning Single Index Models with Diffusion Priors</h3>
<ul>
<li><strong>Authors: </strong>Anqi Tang, Youming Chen, Shuchen Xue, Zhaoqiang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21135">https://arxiv.org/abs/2505.21135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21135">https://arxiv.org/pdf/2505.21135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21135]] Learning Single Index Models with Diffusion Priors(https://arxiv.org/abs/2505.21135)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) have demonstrated remarkable ability to generate diverse and high-quality images by efficiently modeling complex data distributions. They have also been explored as powerful generative priors for signal recovery, resulting in a substantial improvement in the quality of reconstructed signals. However, existing research on signal recovery with diffusion models either focuses on specific reconstruction problems or is unable to handle nonlinear measurement models with discontinuous or unknown link functions. In this work, we focus on using DMs to achieve accurate recovery from semi-parametric single index models, which encompass a variety of popular nonlinear models that may have {\em discontinuous} and {\em unknown} link functions. We propose an efficient reconstruction method that only requires one round of unconditional sampling and (partial) inversion of DMs. Theoretical analysis on the effectiveness of the proposed methods has been established under appropriate conditions. We perform numerical experiments on image datasets for different nonlinear measurement models. We observe that compared to competing methods, our approach can yield more accurate reconstructions while utilizing significantly fewer neural function evaluations.</li>
</ul>

<h3>Title: Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Xu, Hongjie Chen, Wang Qing, Lv Hang, Jian Kang, Li Jie, Zhennan Lin, Yongxiang Li, Xie Lei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21138">https://arxiv.org/abs/2505.21138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21138">https://arxiv.org/pdf/2505.21138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21138]] Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis(https://arxiv.org/abs/2505.21138)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large-scale training corpora have significantly improved the performance of ASR models. Unfortunately, due to the relative scarcity of data, Chinese accents and dialects remain a challenge for most ASR models. Recent advancements in self-supervised learning have shown that self-supervised pre- training, combined with large language models (LLM), can effectively enhance ASR performance in low-resource scenarios. We aim to investigate the effectiveness of this paradigm for Chinese dialects. Specifically, we pre-train a Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech data and do alignment training on a supervised dataset of 40,000 hours. Then, we systematically examine the impact of various projectors and LLMs on Mandarin, dialect, and accented speech recognition performance under this paradigm. Our method achieved SOTA results on multiple dialect datasets, including Kespeech. We will open-source our work to promote reproducible research</li>
</ul>

<h3>Title: HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs</h3>
<ul>
<li><strong>Authors: </strong>Honglin Gao, Xiang Li, Lan Zhao, Gaoxi Xiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21140">https://arxiv.org/abs/2505.21140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21140">https://arxiv.org/pdf/2505.21140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21140]] HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs(https://arxiv.org/abs/2505.21140)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Heterogeneous graph neural networks (HGNNs) have recently drawn increasing attention for modeling complex multi-relational data in domains such as recommendation, finance, and social networks. While existing research has been largely focused on enhancing HGNNs' predictive performance, their robustness and security, especially under backdoor attacks, remain underexplored. In this paper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework for node classification tasks on heterogeneous graphs. HeteroBA inserts carefully crafted trigger nodes with realistic features and targeted structural connections, leveraging attention-based and clustering-based strategies to select influential auxiliary nodes for effective trigger propagation, thereby causing the model to misclassify specific nodes into a target label while maintaining accuracy on clean data. Experimental results on three datasets and various HGNN architectures demonstrate that HeteroBA achieves high attack success rates with minimal impact on the clean accuracy. Our method sheds light on potential vulnerabilities in HGNNs and calls for more robust defenses against backdoor threats in multi-relational graph scenarios.</li>
</ul>

<h3>Title: A Predicting Phishing Websites Using Support Vector Machine and MultiClass Classification Based on Association Rule Techniques</h3>
<ul>
<li><strong>Authors: </strong>Nancy C. Woods, Virtue Ene Agada, Adebola K. Ojo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21141">https://arxiv.org/abs/2505.21141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21141">https://arxiv.org/pdf/2505.21141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21141]] A Predicting Phishing Websites Using Support Vector Machine and MultiClass Classification Based on Association Rule Techniques(https://arxiv.org/abs/2505.21141)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, extraction</a></li>
<li><strong>Abstract: </strong>Phishing is a semantic attack which targets the user rather than the computer. It is a new Internet crime in comparison with other forms such as virus and hacking. Considering the damage phishing websites has caused to various economies by collapsing organizations, stealing information and financial diversion, various researchers have embarked on different ways of detecting phishing websites but there has been no agreement about the best algorithm to be used for prediction. This study is interested in integrating the strengths of two algorithms, Support Vector Machines (SVM) and Multi-Class Classification Rules based on Association Rules (MCAR) to establish a strong and better means of predicting phishing websites. A total of 11,056 websites were used from both PhishTank and yahoo directory to verify the effectiveness of this approach. Feature extraction and rules generation were done by the MCAR technique; classification and prediction were done by SVM technique. The result showed that the technique achieved 98.30% classification accuracy with a computation time of 2205.33s with minimum error rate. It showed a total of 98% Area under the Curve (AUC) which showed the proportion of accuracy in classifying phishing websites. The model showed 82.84% variance in the prediction of phishing websites based on the coefficient of determination. The use of two techniques together in detecting phishing websites produced a more accurate result as it combined the strength of both techniques respectively. This research work centralized on this advantage by building a hybrid of two techniques to help produce a more accurate result.</li>
</ul>

<h3>Title: FastFace: Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention</h3>
<ul>
<li><strong>Authors: </strong>Sergey Karpukhin, Vadim Titov, Andrey Kuznetsov, Aibek Alanov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21144">https://arxiv.org/abs/2505.21144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21144">https://arxiv.org/pdf/2505.21144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21144]] FastFace: Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention(https://arxiv.org/abs/2505.21144)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In latest years plethora of identity-preserving adapters for a personalized generation with diffusion models have been released. Their main disadvantage is that they are dominantly trained jointly with base diffusion models, which suffer from slow multi-step inference. This work aims to tackle the challenge of training-free adaptation of pretrained ID-adapters to diffusion models accelerated via distillation - through careful re-design of classifier-free guidance for few-step stylistic generation and attention manipulation mechanisms in decoupled blocks to improve identity similarity and fidelity, we propose universal FastFace framework. Additionally, we develop a disentangled public evaluation protocol for id-preserving adapters.</li>
</ul>

<h3>Title: Assessment of L2 Oral Proficiency using Speech Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rao Ma, Mengjie Qian, Siyuan Tang, Stefano BannÃ², Kate M. Knill, Mark J.F. Gales</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21148">https://arxiv.org/abs/2505.21148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21148">https://arxiv.org/pdf/2505.21148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21148]] Assessment of L2 Oral Proficiency using Speech Large Language Models(https://arxiv.org/abs/2505.21148)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The growing population of L2 English speakers has increased the demand for developing automatic graders for spoken language assessment (SLA). Historically, statistical models, text encoders, and self-supervised speech models have been utilised for this task. However, cascaded systems suffer from the loss of information, while E2E graders also have limitations. With the recent advancements of multi-modal large language models (LLMs), we aim to explore their potential as L2 oral proficiency graders and overcome these issues. In this work, we compare various training strategies using regression and classification targets. Our results show that speech LLMs outperform all previous competitive baselines, achieving superior performance on two datasets. Furthermore, the trained grader demonstrates strong generalisation capabilities in the cross-part or cross-task evaluation, facilitated by the audio understanding knowledge acquired during LLM pre-training.</li>
</ul>

<h3>Title: RoBiS: Robust Binary Segmentation for High-Resolution Industrial Images</h3>
<ul>
<li><strong>Authors: </strong>Xurui Li, Zhonesheng Jiang, Tingxuan Ai, Yu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21152">https://arxiv.org/abs/2505.21152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21152">https://arxiv.org/pdf/2505.21152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21152]] RoBiS: Robust Binary Segmentation for High-Resolution Industrial Images(https://arxiv.org/abs/2505.21152)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Robust unsupervised anomaly detection (AD) in real-world scenarios is an important task. Current methods exhibit severe performance degradation on the MVTec AD 2 benchmark due to its complex real-world challenges. To solve this problem, we propose a robust framework RoBiS, which consists of three core modules: (1) Swin-Cropping, a high-resolution image pre-processing strategy to preserve the information of small anomalies through overlapping window cropping. (2) The data augmentation of noise addition and lighting simulation is carried out on the training data to improve the robustness of AD model. We use INP-Former as our baseline, which could generate better results on the various sub-images. (3) The traditional statistical-based binarization strategy (mean+3std) is combined with our previous work, MEBin (published in CVPR2025), for joint adaptive binarization. Then, SAM is further employed to refine the segmentation results. Compared with some methods reported by the MVTec AD 2, our RoBiS achieves a 29.2% SegF1 improvement (from 21.8% to 51.00%) on Test_private and 29.82% SegF1 gains (from 16.7% to 46.52%) on Test_private_mixed. Code is available at this https URL.</li>
</ul>

<h3>Title: STEB: In Search of the Best Evaluation Approach for Synthetic Time Series</h3>
<ul>
<li><strong>Authors: </strong>Michael Stenger, Robert Leppich, AndrÃ© Bauer, Samuel Kounev</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21160">https://arxiv.org/abs/2505.21160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21160">https://arxiv.org/pdf/2505.21160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21160]] STEB: In Search of the Best Evaluation Approach for Synthetic Time Series(https://arxiv.org/abs/2505.21160)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>The growing need for synthetic time series, due to data augmentation or privacy regulations, has led to numerous generative models, frameworks, and evaluation measures alike. Objectively comparing these measures on a large scale remains an open challenge. We propose the Synthetic Time series Evaluation Benchmark (STEB) -- the first benchmark framework that enables comprehensive and interpretable automated comparisons of synthetic time series evaluation measures. Using 10 diverse datasets, randomness injection, and 13 configurable data transformations, STEB computes indicators for measure reliability and score consistency. It tracks running time, test errors, and features sequential and parallel modes of operation. In our experiments, we determine a ranking of 41 measures from literature and confirm that the choice of upstream time series embedding heavily impacts the final score.</li>
</ul>

<h3>Title: TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment</h3>
<ul>
<li><strong>Authors: </strong>Zheng Li, Mao Zheng, Mingyang Song, Wenjie Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21172">https://arxiv.org/abs/2505.21172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21172">https://arxiv.org/pdf/2505.21172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21172]] TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment(https://arxiv.org/abs/2505.21172)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, deep reasoning large language models(LLMs) like DeepSeek-R1 have made significant progress in tasks such as mathematics and coding. Inspired by this, several studies have employed reinforcement learning(RL) to enhance models' deep reasoning capabilities and improve machine translation(MT) quality. However, the terminology translation, an essential task in MT, remains unexplored in deep reasoning LLMs. In this paper, we propose \textbf{TAT-R1}, a terminology-aware translation model trained with reinforcement learning and word alignment. Specifically, we first extract the keyword translation pairs using a word alignment model. Then we carefully design three types of rule-based alignment rewards with the extracted alignment relationships. With those alignment rewards, the RL-trained translation model can learn to focus on the accurate translation of key information, including terminology in the source text. Experimental results show the effectiveness of TAT-R1. Our model significantly improves terminology translation accuracy compared to the baseline models while maintaining comparable performance on general translation tasks. In addition, we conduct detailed ablation studies of the DeepSeek-R1-like training paradigm for machine translation and reveal several key findings.</li>
</ul>

<h3>Title: Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Mingyang Song, Mao Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21178">https://arxiv.org/abs/2505.21178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21178">https://arxiv.org/pdf/2505.21178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21178]] Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning(https://arxiv.org/abs/2505.21178)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek R1-like performance. However, recent studies reveal a persistent overthinking phenomenon in state-of-the-art reasoning models, manifesting as excessive redundancy or repetitive thinking patterns in long CoT responses. To address this issue, in this paper, we propose a simple yet effective two-stage reinforcement learning framework for achieving concise reasoning in LLMs, named ConciseR. Specifically, the first stage, using more training steps, aims to incentivize the model's reasoning capabilities via Group Relative Policy Optimization with clip-higher and dynamic sampling components (GRPO++), and the second stage, using fewer training steps, explicitly enforces conciseness and improves efficiency via Length-aware Group Relative Policy Optimization (L-GRPO). Significantly, ConciseR only optimizes response length once all rollouts of a sample are correct, following the "walk before you run" principle. Extensive experimental results demonstrate that our ConciseR model, which generates more concise CoT reasoning responses, outperforms recent state-of-the-art reasoning models with zero RL paradigm across AIME 2024, MATH-500, AMC 2023, Minerva, and Olympiad benchmarks.</li>
</ul>

<h3>Title: Normalized Attention Guidance: Universal Negative Guidance for Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Dar-Yen Chen, Hmrishav Bandyopadhyay, Kai Zou, Yi-Zhe Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21179">https://arxiv.org/abs/2505.21179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21179">https://arxiv.org/pdf/2505.21179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21179]] Normalized Attention Guidance: Universal Negative Guidance for Diffusion Model(https://arxiv.org/abs/2505.21179)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Negative guidance -- explicitly suppressing unwanted attributes -- remains a fundamental challenge in diffusion models, particularly in few-step sampling regimes. While Classifier-Free Guidance (CFG) works well in standard settings, it fails under aggressive sampling step compression due to divergent predictions between positive and negative branches. We present Normalized Attention Guidance (NAG), an efficient, training-free mechanism that applies extrapolation in attention space with L1-based normalization and refinement. NAG restores effective negative guidance where CFG collapses while maintaining fidelity. Unlike existing approaches, NAG generalizes across architectures (UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image, video), functioning as a \textit{universal} plug-in with minimal computational overhead. Through extensive experimentation, we demonstrate consistent improvements in text alignment (CLIP Score), fidelity (FID, PFID), and human-perceived quality (ImageReward). Our ablation studies validate each design component, while user studies confirm significant preference for NAG-guided outputs. As a model-agnostic inference-time approach requiring no retraining, NAG provides effortless negative guidance for all modern diffusion frameworks -- pseudocode in the Appendix!</li>
</ul>

<h3>Title: Boosting Adversarial Transferability via High-Frequency Augmentation and Hierarchical-Gradient Fusion</h3>
<ul>
<li><strong>Authors: </strong>Yayin Zheng, Chen Wan, Zihong Guo, Hailing Kuang, Xiaohai Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21181">https://arxiv.org/abs/2505.21181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21181">https://arxiv.org/pdf/2505.21181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21181]] Boosting Adversarial Transferability via High-Frequency Augmentation and Hierarchical-Gradient Fusion(https://arxiv.org/abs/2505.21181)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Adversarial attacks have become a significant challenge in the security of machine learning models, particularly in the context of black-box defense strategies. Existing methods for enhancing adversarial transferability primarily focus on the spatial domain. This paper presents Frequency-Space Attack (FSA), a new adversarial attack framework that effectively integrates frequency-domain and spatial-domain transformations. FSA combines two key techniques: (1) High-Frequency Augmentation, which applies Fourier transform with frequency-selective amplification to diversify inputs and emphasize the critical role of high-frequency components in adversarial attacks, and (2) Hierarchical-Gradient Fusion, which merges multi-scale gradient decomposition and fusion to capture both global structures and fine-grained details, resulting in smoother perturbations. Our experiment demonstrates that FSA consistently outperforms state-of-the-art methods across various black-box models. Notably, our proposed FSA achieves an average attack success rate increase of 23.6% compared with BSR (CVPR 2024) on eight black-box defense models.</li>
</ul>

<h3>Title: PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing</h3>
<ul>
<li><strong>Authors: </strong>Yu Yan, Sheng Sun, Zhifei Zheng, Ziji Hao, Teli Liu, Min Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21184">https://arxiv.org/abs/2505.21184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21184">https://arxiv.org/pdf/2505.21184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21184]] PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing(https://arxiv.org/abs/2505.21184)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, large language model</a></li>
<li><strong>Abstract: </strong>To construct responsible and secure AI applications, harmful information data is widely utilized for adversarial testing and the development of safeguards. Existing studies mainly leverage Large Language Models (LLMs) to synthesize data to obtain high-quality task datasets at scale, thereby avoiding costly human annotation. However, limited by the safety alignment mechanisms of LLMs, the synthesis of harmful data still faces challenges in generation reliability and content diversity. In this study, we propose a novel harmful information synthesis framework, PoisonSwarm, which applies the model crowdsourcing strategy to generate diverse harmful data while maintaining a high success rate. Specifically, we generate abundant benign data as the based templates in a counterfactual manner. Subsequently, we decompose each based template into multiple semantic units and perform unit-by-unit toxification and final refinement through dynamic model switching, thus ensuring the success of synthesis. Experimental results demonstrate that PoisonSwarm achieves state-of-the-art performance in synthesizing different categories of harmful data with high scalability and diversity.</li>
</ul>

<h3>Title: Exploring the Latent Capacity of LLMs for One-Step Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Gleb Mezentsev, Ivan Oseledets</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21189">https://arxiv.org/abs/2505.21189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21189">https://arxiv.org/pdf/2505.21189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21189]] Exploring the Latent Capacity of LLMs for One-Step Text Generation(https://arxiv.org/abs/2505.21189)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one specially trained input embedding. In this work, we explore whether such reconstruction is possible without autoregression. We show that frozen LLMs can generate hundreds of accurate tokens in just one forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored capability of LLMs - multi-token generation without iterative decoding. We investigate the behaviour of these embeddings and provide insight into the type of information they encode. We also empirically show that although these representations are not unique for a given text, they form connected and local regions in embedding space - a property that suggests the potential of learning a dedicated encoder into that space.</li>
</ul>

<h3>Title: Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Junyan Zhang, Yubo Gao, Yibo Yan, Jungang Li, Zhaorui Hou, Sicheng Tao, Shuliang Liu, Song Dai, Yonghua Hei, Junzhuo Li, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21191">https://arxiv.org/abs/2505.21191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21191">https://arxiv.org/pdf/2505.21191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21191]] Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities(https://arxiv.org/abs/2505.21191)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The finetuning of Large Language Models (LLMs) has significantly advanced their instruction-following capabilities, yet the underlying computational mechanisms driving these improvements remain poorly understood. This study systematically examines how fine-tuning reconfigures LLM computations by isolating and analyzing instruction-specific sparse components, i.e., neurons in dense models and both neurons and experts in Mixture-of-Experts (MoE) architectures. In particular, we introduce HexaInst, a carefully curated and balanced instructional dataset spanning six distinct categories, and propose SPARCOM, a novel analytical framework comprising three key contributions: (1) a method for identifying these sparse components, (2) an evaluation of their functional generality and uniqueness, and (3) a systematic comparison of their alterations. Through experiments, we demonstrate functional generality, uniqueness, and the critical role of these components in instruction execution. By elucidating the relationship between fine-tuning-induced adaptations and sparse computational substrates, this work provides deeper insights into how LLMs internalize instruction-following behavior for the trustworthy LLM community.</li>
</ul>

<h3>Title: Sci-Fi: Symmetric Constraint for Frame Inbetweening</h3>
<ul>
<li><strong>Authors: </strong>Liuhan Chen, Xiaodong Cun, Xiaoyu Li, Xianyi He, Shenghai Yuan, Jie Chen, Ying Shan, Li Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21205">https://arxiv.org/abs/2505.21205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21205">https://arxiv.org/pdf/2505.21205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21205]] Sci-Fi: Symmetric Constraint for Frame Inbetweening(https://arxiv.org/abs/2505.21205)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitting training. We identify a critical limitation in their design: Their injections of the end-frame constraint usually utilize the same mechanism that originally imposed the start-frame (single image) constraint. However, since the original I2V-DMs are adequately trained for the start-frame condition in advance, naively introducing the end-frame constraint by the same mechanism with much less (even zero) specialized training probably can't make the end frame have a strong enough impact on the intermediate content like the start frame. This asymmetric control strength of the two frames over the intermediate content likely leads to inconsistent motion or appearance collapse in generated frames. To efficiently achieve symmetric constraints of start and end frames, we propose a novel framework, termed Sci-Fi, which applies a stronger injection for the constraint of a smaller training scale. Specifically, it deals with the start-frame constraint as before, while introducing the end-frame constraint by an improved mechanism. The new mechanism is based on a well-designed lightweight module, named EF-Net, which encodes only the end frame and expands it into temporally adaptive frame-wise features injected into the I2V-DM. This makes the end-frame constraint as strong as the start-frame constraint, enabling our Sci-Fi to produce more harmonious transitions in various scenarios. Extensive experiments prove the superiority of our Sci-Fi compared with other baselines.</li>
</ul>

<h3>Title: Pretrained LLMs Learn Multiple Types of Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Roi Cohen, Omri Fahn, Gerard de Melo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21218">https://arxiv.org/abs/2505.21218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21218">https://arxiv.org/pdf/2505.21218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21218]] Pretrained LLMs Learn Multiple Types of Uncertainty(https://arxiv.org/abs/2505.21218)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models are known to capture real-world knowledge, allowing them to excel in many downstream tasks. Despite recent advances, these models are still prone to what are commonly known as hallucinations, causing them to emit unwanted and factually incorrect text. In this work, we study how well LLMs capture uncertainty, without explicitly being trained for that. We show that, if considering uncertainty as a linear concept in the model's latent space, it might indeed be captured, even after only pretraining. We further show that, though unintuitive, LLMs appear to capture several different types of uncertainty, each of which can be useful to predict the correctness for a specific task or benchmark. Furthermore, we provide in-depth results such as demonstrating a correlation between our correction prediction and the model's ability to abstain from misinformation using words, and the lack of impact of model scaling for capturing uncertainty. Finally, we claim that unifying the uncertainty types as a single one using instruction-tuning or [IDK]-token tuning is helpful for the model in terms of correctness prediction.</li>
</ul>

<h3>Title: Addressing Data Quality Decompensation in Federated Learning via Dynamic Client Selection</h3>
<ul>
<li><strong>Authors: </strong>Qinjun Fei, Nuria RodrÃ­guez-Barroso, MarÃ­a Victoria LuzÃ³n, Zhongliang Zhang, Francisco Herrera</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21219">https://arxiv.org/abs/2505.21219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21219">https://arxiv.org/pdf/2505.21219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21219]] Addressing Data Quality Decompensation in Federated Learning via Dynamic Client Selection(https://arxiv.org/abs/2505.21219)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>In cross-silo Federated Learning (FL), client selection is critical to ensure high model performance, yet it remains challenging due to data quality decompensation, budget constraints, and incentive compatibility. As training progresses, these factors exacerbate client heterogeneity and degrade global performance. Most existing approaches treat these challenges in isolation, making jointly optimizing multiple factors difficult. To address this, we propose Shapley-Bid Reputation Optimized Federated Learning (SBRO-FL), a unified framework integrating dynamic bidding, reputation modeling, and cost-aware selection. Clients submit bids based on their perceived data quality, and their contributions are evaluated using Shapley values to quantify their marginal impact on the global model. A reputation system, inspired by prospect theory, captures historical performance while penalizing inconsistency. The client selection problem is formulated as a 0-1 integer program that maximizes reputation-weighted utility under budget constraints. Experiments on FashionMNIST, EMNIST, CIFAR-10, and SVHN datasets show that SBRO-FL improves accuracy, convergence speed, and robustness, even in adversarial and low-bid interference scenarios. Our results highlight the importance of balancing data reliability, incentive compatibility, and cost efficiency to enable scalable and trustworthy FL deployments.</li>
</ul>

<h3>Title: A Representation Level Analysis of NMT Model Robustness to Grammatical Errors</h3>
<ul>
<li><strong>Authors: </strong>Abderrahmane Issam, Yusuf Can Semerci, Jan Scholtes, Gerasimos Spanakis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21224">https://arxiv.org/abs/2505.21224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21224">https://arxiv.org/pdf/2505.21224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21224]] A Representation Level Analysis of NMT Model Robustness to Grammatical Errors(https://arxiv.org/abs/2505.21224)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Understanding robustness is essential for building reliable NLP systems. Unfortunately, in the context of machine translation, previous work mainly focused on documenting robustness failures or improving robustness. In contrast, we study robustness from a model representation perspective by looking at internal model representations of ungrammatical inputs and how they evolve through model layers. For this purpose, we perform Grammatical Error Detection (GED) probing and representational similarity analysis. Our findings indicate that the encoder first detects the grammatical error, then corrects it by moving its representation toward the correct form. To understand what contributes to this process, we turn to the attention mechanism where we identify what we term Robustness Heads. We find that Robustness Heads attend to interpretable linguistic units when responding to grammatical errors, and that when we fine-tune models for robustness, they tend to rely more on Robustness Heads for updating the ungrammatical word representation.</li>
</ul>

<h3>Title: LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners</h3>
<ul>
<li><strong>Authors: </strong>Yu He, Zihan Yao, Chentao Song, Tianyu Qi, Jun Liu, Ming Li, Qing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21239">https://arxiv.org/abs/2505.21239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21239">https://arxiv.org/pdf/2505.21239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21239]] LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners(https://arxiv.org/abs/2505.21239)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Cognitive Diagnosis (CD) has become a critical task in AI-empowered education, supporting personalized learning by accurately assessing students' cognitive states. However, traditional CD models often struggle in cold-start scenarios due to the lack of student-exercise interaction data. Recent NLP-based approaches leveraging pre-trained language models (PLMs) have shown promise by utilizing textual features but fail to fully bridge the gap between semantic understanding and cognitive profiling. In this work, we propose Language Models as Zeroshot Cognitive Diagnosis Learners (LMCD), a novel framework designed to handle cold-start challenges by harnessing large language models (LLMs). LMCD operates via two primary phases: (1) Knowledge Diffusion, where LLMs generate enriched contents of exercises and knowledge concepts (KCs), establishing stronger semantic links; and (2) Semantic-Cognitive Fusion, where LLMs employ causal attention mechanisms to integrate textual information and student cognitive states, creating comprehensive profiles for both students and exercises. These representations are efficiently trained with off-the-shelf CD models. Experiments on two real-world datasets demonstrate that LMCD significantly outperforms state-of-the-art methods in both exercise-cold and domain-cold settings. The code is publicly available at this https URL</li>
</ul>

<h3>Title: BindEnergyCraft: Casting Protein Structure Predictors as Energy-Based Models for Binder Design</h3>
<ul>
<li><strong>Authors: </strong>Divya Nori, Anisha Parsan, Caroline Uhler, Wengong Jin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21241">https://arxiv.org/abs/2505.21241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21241">https://arxiv.org/pdf/2505.21241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21241]] BindEnergyCraft: Casting Protein Structure Predictors as Energy-Based Models for Binder Design(https://arxiv.org/abs/2505.21241)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Protein binder design has been transformed by hallucination-based methods that optimize structure prediction confidence metrics, such as the interface predicted TM-score (ipTM), via backpropagation. However, these metrics do not reflect the statistical likelihood of a binder-target complex under the learned distribution and yield sparse gradients for optimization. In this work, we propose a method to extract such likelihoods from structure predictors by reinterpreting their confidence outputs as an energy-based model (EBM). By leveraging the Joint Energy-based Modeling (JEM) framework, we introduce pTMEnergy, a statistical energy function derived from predicted inter-residue error distributions. We incorporate pTMEnergy into BindEnergyCraft (BECraft), a design pipeline that maintains the same optimization framework as BindCraft but replaces ipTM with our energy-based objective. BECraft outperforms BindCraft, RFDiffusion, and ESM3 across multiple challenging targets, achieving higher in silico binder success rates while reducing structural clashes. Furthermore, pTMEnergy establishes a new state-of-the-art in structure-based virtual screening tasks for miniprotein and RNA aptamer binders.</li>
</ul>

<h3>Title: Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings</h3>
<ul>
<li><strong>Authors: </strong>Gunjan Balde, Soumyadeep Roy, Mainack Mondal, Niloy Ganguly</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21242">https://arxiv.org/abs/2505.21242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21242">https://arxiv.org/pdf/2505.21242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21242]] Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings(https://arxiv.org/abs/2505.21242)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) recently achieved great success in medical text summarization by simply using in-context learning. However, these recent efforts do not perform fine-grained evaluations under difficult settings where LLMs might fail. They typically report performance scores over the entire dataset. Through our benchmarking study, we show that LLMs show a significant performance drop for data points with high concentration of out-of-vocabulary (OOV) words or with high novelty. Vocabulary adaptation is an intuitive solution to this vocabulary mismatch issue where the LLM vocabulary gets updated with certain expert domain (here, medical) words or subwords. An interesting finding from our study is that Llama-3.1, even with a vocabulary size of around 128K tokens, still faces over-fragmentation issue with medical words. To that end, we show vocabulary adaptation helps improve the LLM summarization performance even in difficult settings. Through extensive experimentation of multiple vocabulary adaptation strategies, two continual pretraining strategies, and three benchmark medical summarization datasets, we gain valuable insights into the role of vocabulary adaptation strategies for customizing LLMs to the medical domain. We also performed a human evaluation study with medical experts where they found that vocabulary adaptation results in more relevant and faithful summaries. Our codebase is made publicly available at this https URL.</li>
</ul>

<h3>Title: ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision</h3>
<ul>
<li><strong>Authors: </strong>Dosung Lee, Wonjun Oh, Boyoung Kim, Minyoung Kim, Joonsuk Park, Paul Hongsuck Seo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21250">https://arxiv.org/abs/2505.21250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21250">https://arxiv.org/pdf/2505.21250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21250]] ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision(https://arxiv.org/abs/2505.21250)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-hop question answering (MHQA) involves reasoning across multiple documents to answer complex questions. Dense retrievers typically outperform sparse methods like BM25 by leveraging semantic embeddings; however, they require labeled query-document pairs for fine-tuning. This poses a significant challenge in MHQA due to the high variability of queries (reformulated) questions throughout the reasoning steps. To overcome this limitation, we introduce Retriever Supervision with Consistency and Relevance (ReSCORE), a novel method for training dense retrievers for MHQA without labeled documents. ReSCORE leverages large language models to capture each documents relevance to the question and consistency with the correct answer and use them to train a retriever within an iterative question-answering framework. Experiments on three MHQA benchmarks demonstrate the effectiveness of ReSCORE, with significant improvements in retrieval, and in turn, the state-of-the-art MHQA performance. Our implementation is available at: this https URL.</li>
</ul>

<h3>Title: Plenodium: UnderWater 3D Scene Reconstruction with Plenoptic Medium Representation</h3>
<ul>
<li><strong>Authors: </strong>Changguanng Wu, Jiangxin Dong, Chengjian Li, Jinhui Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21258">https://arxiv.org/abs/2505.21258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21258">https://arxiv.org/pdf/2505.21258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21258]] Plenodium: UnderWater 3D Scene Reconstruction with Plenoptic Medium Representation(https://arxiv.org/abs/2505.21258)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present Plenodium (plenoptic medium), an effective and efficient 3D representation framework capable of jointly modeling both objects and participating media. In contrast to existing medium representations that rely solely on view-dependent modeling, our novel plenoptic medium representation incorporates both directional and positional information through spherical harmonics encoding, enabling highly accurate underwater scene reconstruction. To address the initialization challenge in degraded underwater environments, we propose the pseudo-depth Gaussian complementation to augment COLMAP-derived point clouds with robust depth priors. In addition, a depth ranking regularized loss is developed to optimize the geometry of the scene and improve the ordinal consistency of the depth maps. Extensive experiments on real-world underwater datasets demonstrate that our method achieves significant improvements in 3D reconstruction. Furthermore, we conduct a simulated dataset with ground truth and the controllable scattering medium to demonstrate the restoration capability of our method in underwater scenarios. Our code and dataset are available at this https URL.</li>
</ul>

<h3>Title: JavaSith: A Client-Side Framework for Analyzing Potentially Malicious Extensions in Browsers, VS Code, and NPM Packages</h3>
<ul>
<li><strong>Authors: </strong>Avihay Cohen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21263">https://arxiv.org/abs/2505.21263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21263">https://arxiv.org/pdf/2505.21263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21263]] JavaSith: A Client-Side Framework for Analyzing Potentially Malicious Extensions in Browsers, VS Code, and NPM Packages(https://arxiv.org/abs/2505.21263)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Modern software supply chains face an increasing threat from malicious code hidden in trusted components such as browser extensions, IDE extensions, and open-source packages. This paper introduces JavaSith, a novel client-side framework for analyzing potentially malicious extensions in web browsers, Visual Studio Code (VSCode), and Node's NPM packages. JavaSith combines a runtime sandbox that emulates browser/Node.js extension APIs (with a ``time machine'' to accelerate time-based triggers) with static analysis and a local large language model (LLM) to assess risk from code and metadata. We present the design and architecture of JavaSith, including techniques for intercepting extension behavior over simulated time and extracting suspicious patterns. Through case studies on real-world attacks (such as a supply-chain compromise of a Chrome extension and malicious VSCode extensions installing cryptominers), we demonstrate how JavaSith can catch stealthy malicious behaviors that evade traditional detection. We evaluate the framework's effectiveness and discuss its limitations and future enhancements. JavaSith's client-side approach empowers end-users/organizations to vet extensions and packages before trustingly integrating them into their environments.</li>
</ul>

<h3>Title: Supervised and self-supervised land-cover segmentation & classification of the Biesbosch wetlands</h3>
<ul>
<li><strong>Authors: </strong>Eva Gmelich Meijling, Roberto Del Prete, Arnoud Visser</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21269">https://arxiv.org/abs/2505.21269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21269">https://arxiv.org/pdf/2505.21269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21269]] Supervised and self-supervised land-cover segmentation & classification of the Biesbosch wetlands(https://arxiv.org/abs/2505.21269)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate wetland land-cover classification is essential for environmental monitoring, biodiversity assessment, and sustainable ecosystem management. However, the scarcity of annotated data, especially for high-resolution satellite imagery, poses a significant challenge for supervised learning approaches. To tackle this issue, this study presents a methodology for wetland land-cover segmentation and classification that adopts both supervised and self-supervised learning (SSL). We train a U-Net model from scratch on Sentinel-2 imagery across six wetland regions in the Netherlands, achieving a baseline model accuracy of 85.26%. Addressing the limited availability of labeled data, the results show that SSL pretraining with an autoencoder can improve accuracy, especially for the high-resolution imagery where it is more difficult to obtain labeled data, reaching an accuracy of 88.23%. Furthermore, we introduce a framework to scale manually annotated high-resolution labels to medium-resolution inputs. While the quantitative performance between resolutions is comparable, high-resolution imagery provides significantly sharper segmentation boundaries and finer spatial detail. As part of this work, we also contribute a curated Sentinel-2 dataset with Dynamic World labels, tailored for wetland classification tasks and made publicly available.</li>
</ul>

<h3>Title: Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space</h3>
<ul>
<li><strong>Authors: </strong>Yao Huang, Yitong Sun, Shouwei Ruan, Yichi Zhang, Yinpeng Dong, Xingxing Wei</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21277">https://arxiv.org/abs/2505.21277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21277">https://arxiv.org/pdf/2505.21277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21277]] Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space(https://arxiv.org/abs/2505.21277)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), despite advanced general capabilities, still suffer from numerous safety risks, especially jailbreak attacks that bypass safety protocols. Understanding these vulnerabilities through black-box jailbreak attacks, which better reflect real-world scenarios, offers critical insights into model robustness. While existing methods have shown improvements through various prompt engineering techniques, their success remains limited against safety-aligned models, overlooking a more fundamental problem: the effectiveness is inherently bounded by the predefined strategy spaces. However, expanding this space presents significant challenges in both systematically capturing essential attack patterns and efficiently navigating the increased complexity. To better explore the potential of expanding the strategy space, we address these challenges through a novel framework that decomposes jailbreak strategies into essential components based on the Elaboration Likelihood Model (ELM) theory and develops genetic-based optimization with intention evaluation mechanisms. To be striking, our experiments reveal unprecedented jailbreak capabilities by expanding the strategy space: we achieve over 90% success rate on Claude-3.5 where prior methods completely fail, while demonstrating strong cross-model transferability and surpassing specialized safeguard models in evaluation accuracy. The code is open-sourced at: this https URL.</li>
</ul>

<h3>Title: Learnable Kernel Density Estimation for Graphs</h3>
<ul>
<li><strong>Authors: </strong>Xudong Wang, Ziheng Sun, Chris Ding, Jicong Fan</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21285">https://arxiv.org/abs/2505.21285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21285">https://arxiv.org/pdf/2505.21285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21285]] Learnable Kernel Density Estimation for Graphs(https://arxiv.org/abs/2505.21285)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This work proposes a framework LGKDE that learns kernel density estimation for graphs. The key challenge in graph density estimation lies in effectively capturing both structural patterns and semantic variations while maintaining theoretical guarantees. Combining graph kernels and kernel density estimation (KDE) is a standard approach to graph density estimation, but has unsatisfactory performance due to the handcrafted and fixed features of kernels. Our method LGKDE leverages graph neural networks to represent each graph as a discrete distribution and utilizes maximum mean discrepancy to learn the graph metric for multi-scale KDE, where all parameters are learned by maximizing the density of graphs relative to the density of their well-designed perturbed counterparts. The perturbations are conducted on both node features and graph spectra, which helps better characterize the boundary of normal density regions. Theoretically, we establish consistency and convergence guarantees for LGKDE, including bounds on the mean integrated squared error, robustness, and complexity. We validate LGKDE by demonstrating its effectiveness in recovering the underlying density of synthetic graph distributions and applying it to graph anomaly detection across diverse benchmark datasets. Extensive empirical evaluation shows that LGKDE demonstrates superior performance compared to state-of-the-art baselines on most benchmark datasets.</li>
</ul>

<h3>Title: rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset</h3>
<ul>
<li><strong>Authors: </strong>Yifei Liu, Li Lyna Zhang, Yi Zhu, Bingcheng Dong, Xudong Zhou, Ning Shang, Fan Yang, Mao Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21297">https://arxiv.org/abs/2505.21297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21297">https://arxiv.org/pdf/2505.21297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21297]] rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset(https://arxiv.org/abs/2505.21297)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Advancing code reasoning in large language models (LLMs) is fundamentally limited by the scarcity of high-difficulty datasets, especially those with verifiable input-output test cases necessary for rigorous solution validation at scale. We introduce rStar-Coder, which significantly improves LLM code reasoning capabilities by constructing a large-scale, verified dataset of 418K competition-level code problems, 580K long-reasoning solutions along with rich test cases of varying difficulty. This is achieved through three core contributions: (1) we curate competitive programming code problems and oracle solutions to synthesize new, solvable problems; (2) we introduce a reliable input-output test case synthesis pipeline that decouples the generation into a three-step input generation method and a mutual verification mechanism for effective output labeling; (3) we augment problems with high-quality, test-case-verified long-reasoning solutions. Extensive experiments on Qwen models (1.5B-14B) across various code reasoning benchmarks demonstrate the superiority of rStar-Coder dataset, achieving leading performance comparable to frontier reasoning LLMs with much smaller model sizes. On LiveCodeBench, rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more challenging USA Computing Olympiad, our 7B model achieves an average pass@1 accuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the dataset will be released at this https URL.</li>
</ul>

<h3>Title: Spectral Compression Transformer with Line Pose Graph for Monocular 3D Human Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zenghao Zheng, Lianping Yang, Hegui Zhu, Mingrui Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21309">https://arxiv.org/abs/2505.21309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21309">https://arxiv.org/pdf/2505.21309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21309]] Spectral Compression Transformer with Line Pose Graph for Monocular 3D Human Pose Estimation(https://arxiv.org/abs/2505.21309)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based 3D human pose estimation methods suffer from high computational costs due to the quadratic complexity of self-attention with respect to sequence length. Additionally, pose sequences often contain significant redundancy between frames. However, recent methods typically fail to improve model capacity while effectively eliminating sequence redundancy. In this work, we introduce the Spectral Compression Transformer (SCT) to reduce sequence length and accelerate computation. The SCT encoder treats hidden features between blocks as Temporal Feature Signals (TFS) and applies the Discrete Cosine Transform, a Fourier transform-based technique, to determine the spectral components to be retained. By filtering out certain high-frequency noise components, SCT compresses the sequence length and reduces redundancy. To further enrich the input sequence with prior structural information, we propose the Line Pose Graph (LPG) based on line graph theory. The LPG generates skeletal position information that complements the input 2D joint positions, thereby improving the model's performance. Finally, we design a dual-stream network architecture to effectively model spatial joint relationships and the compressed motion trajectory within the pose sequence. Extensive experiments on two benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demonstrate that our model achieves state-of-the-art performance with improved computational efficiency. For example, on the Human3.6M dataset, our method achieves an MPJPE of 37.7mm while maintaining a low computational cost. Furthermore, we perform ablation studies on each module to assess its effectiveness. The code and models will be released.</li>
</ul>

<h3>Title: Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead</h3>
<ul>
<li><strong>Authors: </strong>Jesujoba O. Alabi, Michael A. Hedderich, David Ifeoluwa Adelani, Dietrich Klakow</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21315">https://arxiv.org/abs/2505.21315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21315">https://arxiv.org/pdf/2505.21315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21315]] Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead(https://arxiv.org/abs/2505.21315)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With over 2,000 languages and potentially millions of speakers, Africa represents one of the richest linguistic regions in the world. Yet, this diversity is scarcely reflected in state-of-the-art natural language processing (NLP) systems and large language models (LLMs), which predominantly support a narrow set of high-resource languages. This exclusion not only limits the reach and utility of modern NLP technologies but also risks widening the digital divide across linguistic communities. Nevertheless, NLP research on African languages is active and growing. In recent years, there has been a surge of interest in this area, driven by several factors-including the creation of multilingual language resources, the rise of community-led initiatives, and increased support through funding programs. In this survey, we analyze 734 research papers on NLP for African languages published over the past five years, offering a comprehensive overview of recent progress across core tasks. We identify key trends shaping the field and conclude by outlining promising directions to foster more inclusive and sustainable NLP research for African languages.</li>
</ul>

<h3>Title: Efficient Leaf Disease Classification and Segmentation using Midpoint Normalization Technique and Attention Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Enam Ahmed Taufik, Antara Firoz Parsa, Seraj Al Mahmud Mostafa</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21316">https://arxiv.org/abs/2505.21316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21316">https://arxiv.org/pdf/2505.21316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21316]] Efficient Leaf Disease Classification and Segmentation using Midpoint Normalization Technique and Attention Mechanism(https://arxiv.org/abs/2505.21316)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Enhancing plant disease detection from leaf imagery remains a persistent challenge due to scarce labeled data and complex contextual factors. We introduce a transformative two-stage methodology, Mid Point Normalization (MPN) for intelligent image preprocessing, coupled with sophisticated attention mechanisms that dynamically recalibrate feature representations. Our classification pipeline, merging MPN with Squeeze-and-Excitation (SE) blocks, achieves remarkable 93% accuracy while maintaining exceptional class-wise balance. The perfect F1 score attained for our target class exemplifies attention's power in adaptive feature refinement. For segmentation tasks, we seamlessly integrate identical attention blocks within U-Net architecture using MPN-enhanced inputs, delivering compelling performance gains with 72.44% Dice score and 58.54% IoU, substantially outperforming baseline implementations. Beyond superior accuracy metrics, our approach yields computationally efficient, lightweight architectures perfectly suited for real-world computer vision applications.</li>
</ul>

<h3>Title: A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features</h3>
<ul>
<li><strong>Authors: </strong>Ihab Bendidi, Yassir El Mesbahi, Alisandra K. Denton, Karush Suri, Kian Kenyon-Dean, Auguste Genovesio, Emmanuel Noutahi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21317">https://arxiv.org/abs/2505.21317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21317">https://arxiv.org/pdf/2505.21317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21317]] A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features(https://arxiv.org/abs/2505.21317)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Understanding cellular responses to stimuli is crucial for biological discovery and drug development. Transcriptomics provides interpretable, gene-level insights, while microscopy imaging offers rich predictive features but is harder to interpret. Weakly paired datasets, where samples share biological states, enable multimodal learning but are scarce, limiting their utility for training and multimodal inference. We propose a framework to enhance transcriptomics by distilling knowledge from microscopy images. Using weakly paired data, our method aligns and binds modalities, enriching gene expression representations with morphological information. To address data scarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal distillation using pretrained foundation models, achieving state-of-the-art results, and (2) PEA (Perturbation Embedding Augmentation), a novel augmentation technique that enhances transcriptomics data while preserving inherent biological information. These strategies improve the predictive power and retain the interpretability of transcriptomics, enabling rich unimodal representations for complex biological tasks.</li>
</ul>

<h3>Title: Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Zhu, Yuting Guo, Noah Marchuck, Abeed Sarker, Yun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21324">https://arxiv.org/abs/2505.21324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21324">https://arxiv.org/pdf/2505.21324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21324]] Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts(https://arxiv.org/abs/2505.21324)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Despite rapid advances in large language models (LLMs), their integration with traditional supervised machine learning (ML) techniques that have proven applicability to medical data remains underexplored. This is particularly true for psychiatric applications, where narrative data often exhibit nuanced linguistic and contextual complexity, and can benefit from the combination of multiple models with differing characteristics. In this study, we introduce an ensemble framework for automatically classifying Attention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using narrative transcripts. Our approach integrates three complementary models: LLaMA3, an open-source LLM that captures long-range semantic structure; RoBERTa, a pre-trained transformer model fine-tuned on labeled clinical narratives; and a Support Vector Machine (SVM) classifier trained using TF-IDF-based lexical features. These models are aggregated through a majority voting mechanism to enhance predictive robustness. The dataset includes 441 instances, including 352 for training and 89 for validation. Empirical results show that the ensemble outperforms individual models, achieving an F$_1$ score of 0.71 (95\% CI: [0.60-0.80]). Compared to the best-performing individual model (SVM), the ensemble improved recall while maintaining competitive precision. This indicates the strong sensitivity of the ensemble in identifying ADHD-related linguistic cues. These findings demonstrate the promise of hybrid architectures that leverage the semantic richness of LLMs alongside the interpretability and pattern recognition capabilities of traditional supervised ML, offering a new direction for robust and generalizable psychiatric text classification.</li>
</ul>

<h3>Title: MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on</h3>
<ul>
<li><strong>Authors: </strong>Guangyuan Li, Siming Zheng, Hao Zhang, Jinwei Chen, Junsheng Luan, Binkai Ou, Lei Zhao, Bo Li, Peng-Tao Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21325">https://arxiv.org/abs/2505.21325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21325">https://arxiv.org/pdf/2505.21325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21325]] MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on(https://arxiv.org/abs/2505.21325)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Video Virtual Try-On (VVT) aims to simulate the natural appearance of garments across consecutive video frames, capturing their dynamic variations and interactions with human body motion. However, current VVT methods still face challenges in terms of spatiotemporal consistency and garment content preservation. First, they use diffusion models based on the U-Net, which are limited in their expressive capability and struggle to reconstruct complex details. Second, they adopt a separative modeling approach for spatial and temporal attention, which hinders the effective capture of structural relationships and dynamic consistency across frames. Third, their expression of garment details remains insufficient, affecting the realism and stability of the overall synthesized results, especially during human motion. To address the above challenges, we propose MagicTryOn, a video virtual try-on framework built upon the large-scale video diffusion this http URL replace the U-Net architecture with a diffusion Transformer and combine full self-attention to jointly model the spatiotemporal consistency of videos. We design a coarse-to-fine garment preservation strategy. The coarse strategy integrates garment tokens during the embedding stage, while the fine strategy incorporates multiple garment-based conditions, such as semantics, textures, and contour lines during the denoising stage. Moreover, we introduce a mask-aware loss to further optimize garment region fidelity. Extensive experiments on both image and video try-on datasets demonstrate that our method outperforms existing SOTA methods in comprehensive evaluations and generalizes to in-the-wild scenarios.</li>
</ul>

<h3>Title: MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Yang Shi, Huanqian Wang, Wulin Xie, Huanyao Zhang, Lijie Zhao, Yi-Fan Zhang, Xinfeng Li, Chaoyou Fu, Zhuoer Wen, Wenting Liu, Zhuoran Zhang, Xinlong Chen, Bohan Zeng, Sihan Yang, Yuanxing Zhang, Pengfei Wan, Haotian Wang, Wenjing Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21333">https://arxiv.org/abs/2505.21333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21333">https://arxiv.org/pdf/2505.21333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21333]] MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios(https://arxiv.org/abs/2505.21333)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have achieved considerable accuracy in Optical Character Recognition (OCR) from static images. However, their efficacy in video OCR is significantly diminished due to factors such as motion blur, temporal variations, and visual effects inherent in video content. To provide clearer guidance for training practical MLLMs, we introduce the MME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR application scenarios. MME-VideoOCR features 10 task categories comprising 25 individual tasks and spans 44 diverse scenarios. These tasks extend beyond text recognition to incorporate deeper comprehension and reasoning of textual content within videos. The benchmark consists of 1,464 videos with varying resolutions, aspect ratios, and durations, along with 2,000 meticulously curated, manually annotated question-answer pairs. We evaluate 18 state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing model (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained analysis indicates that while existing MLLMs demonstrate strong performance on tasks where relevant texts are contained within a single or few frames, they exhibit limited capability in effectively handling tasks that demand holistic video comprehension. These limitations are especially evident in scenarios that require spatio-temporal reasoning, cross-frame information integration, or resistance to language prior bias. Our findings also highlight the importance of high-resolution visual input and sufficient temporal coverage for reliable OCR in dynamic video scenarios.</li>
</ul>

<h3>Title: HoliTom: Holistic Token Merging for Fast Video Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kele Shao, Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21334">https://arxiv.org/abs/2505.21334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21334">https://arxiv.org/pdf/2505.21334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21334]] HoliTom: Holistic Token Merging for Fast Video Large Language Models(https://arxiv.org/abs/2505.21334)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Video large language models (video LLMs) excel at video comprehension but face significant computational inefficiency due to redundant video tokens. Existing token pruning methods offer solutions. However, approaches operating within the LLM (inner-LLM pruning), such as FastV, incur intrinsic computational overhead in shallow layers. In contrast, methods performing token pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy within individual frames or limited temporal windows, neglecting the crucial global temporal dynamics and correlations across longer video sequences. This leads to sub-optimal spatio-temporal reduction and does not leverage video compressibility fully. Crucially, the synergistic potential and mutual influence of combining these strategies remain unexplored. To further reduce redundancy, we introduce HoliTom, a novel training-free holistic token merging framework. HoliTom employs outer-LLM pruning through global redundancy-aware temporal segmentation, followed by spatial-temporal merging to reduce visual tokens by over 90%, significantly alleviating the LLM's computational burden. Complementing this, we introduce a robust inner-LLM token similarity-based merging approach, designed for superior performance and compatibility with outer-LLM pruning. Evaluations demonstrate our method's promising efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational costs to 6.9% of FLOPs while maintaining 99.1% of the original performance. Furthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a 1.32x acceleration in decoding throughput, highlighting the practical benefits of our integrated pruning approach for efficient video LLMs inference.</li>
</ul>

<h3>Title: Beyond Accuracy: Uncovering the Role of Similarity Perception and its Alignment with Semantics in Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Katarzyna Filus, Mateusz Å»arski</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21338">https://arxiv.org/abs/2505.21338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21338">https://arxiv.org/pdf/2505.21338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21338]] Beyond Accuracy: Uncovering the Role of Similarity Perception and its Alignment with Semantics in Supervised Learning(https://arxiv.org/abs/2505.21338)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Similarity manifests in various forms, including semantic similarity that is particularly important, serving as an approximation of human object categorization based on e.g. shared functionalities and evolutionary traits. It also offers practical advantages in computational modeling via lexical structures such as WordNet with constant and interpretable similarity. As in the domain of deep vision, there is still not enough focus on the phenomena regarding the similarity perception emergence. We introduce Deep Similarity Inspector (DSI) -- a systematic framework to inspect how deep vision networks develop their similarity perception and its alignment with semantic similarity. Our experiments show that both Convolutional Neural Networks' (CNNs) and Vision Transformers' (ViTs) develop a rich similarity perception during training with 3 phases (initial similarity surge, refinement, stabilization), with clear differences between CNNs and ViTs. Besides the gradual mistakes elimination, the mistakes refinement phenomenon can be observed.</li>
</ul>

<h3>Title: PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims</h3>
<ul>
<li><strong>Authors: </strong>Valentin Knappich, Annemarie Friedrich, Anna HÃ¤tty, Simon Razniewski</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21342">https://arxiv.org/abs/2505.21342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21342">https://arxiv.org/pdf/2505.21342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21342]] PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims(https://arxiv.org/abs/2505.21342)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>Patent claims define the scope of protection for an invention. If there are ambiguities in a claim, it is rejected by the patent office. In the US, this is referred to as indefiniteness (35 U.S.C Â§ 112(b)) and is among the most frequent reasons for patent application rejection. The development of automatic methods for patent definiteness examination has the potential to make patent drafting and examination more efficient, but no annotated dataset has been published to date. We introduce PEDANTIC (\underline{P}at\underline{e}nt \underline{D}efiniteness Ex\underline{a}mi\underline{n}a\underline{ti}on \underline{C}orpus), a novel dataset of 14k US patent claims from patent applications relating to Natural Language Processing (NLP), annotated with reasons for indefiniteness. We construct PEDANTIC using a fully automatic pipeline that retrieves office action documents from the USPTO and uses Large Language Models (LLMs) to extract the reasons for indefiniteness. A human validation study confirms the pipeline's accuracy in generating high-quality annotations. To gain insight beyond binary classification metrics, we implement an LLM-as-Judge evaluation that compares the free-form reasoning of every model-cited reason with every examiner-cited reason. We show that LLM agents based on Qwen 2.5 32B and 72B struggle to outperform logistic regression baselines on definiteness prediction, even though they often correctly identify the underlying reasons. PEDANTIC provides a valuable resource for patent AI researchers, enabling the development of advanced examination models. We will publicly release the dataset and code.</li>
</ul>

<h3>Title: Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Bidyarthi Paul, Jalisha Jashim Era, Mirazur Rahman Zim, Tahmid Sattar Aothoi, Faisal Muhammad Shah</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21354">https://arxiv.org/abs/2505.21354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21354">https://arxiv.org/pdf/2505.21354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21354]] Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning(https://arxiv.org/abs/2505.21354)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Solving Bengali Math Word Problems (MWPs) remains a major challenge in natural language processing (NLP) due to the language's low-resource status and the multi-step reasoning required. Existing models struggle with complex Bengali MWPs, largely because no human-annotated Bengali dataset has previously addressed this task. This gap has limited progress in Bengali mathematical reasoning. To address this, we created SOMADHAN, a dataset of 8792 complex Bengali MWPs with manually written, step-by-step solutions. We designed this dataset to support reasoning-focused evaluation and model development in a linguistically underrepresented context. Using SOMADHAN, we evaluated a range of large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series models, Deepseek, and Qwen - through both zero-shot and few-shot prompting with and without Chain of Thought (CoT) reasoning. CoT prompting consistently improved performance over standard prompting, especially in tasks requiring multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with few-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune models efficiently, enabling them to adapt to Bengali MWPs with minimal computational cost. Our work fills a critical gap in Bengali NLP by providing a high-quality reasoning dataset and a scalable framework for solving complex MWPs. We aim to advance equitable research in low-resource languages and enhance reasoning capabilities in educational and language technologies.</li>
</ul>

<h3>Title: AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping</h3>
<ul>
<li><strong>Authors: </strong>Wenyuan Li, Shunlin Liang, Keyan Chen, Yongzhe Chen, Han Ma, Jianglei Xu, Yichuan Ma, Shikang Guan, Husheng Fang, Zhenwei Shi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21357">https://arxiv.org/abs/2505.21357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21357">https://arxiv.org/pdf/2505.21357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21357]] AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping(https://arxiv.org/abs/2505.21357)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Accurate crop mapping fundamentally relies on modeling multi-scale spatiotemporal patterns, where spatial scales range from individual field textures to landscape-level context, and temporal scales capture both short-term phenological transitions and full growing-season dynamics. Transformer-based remote sensing foundation models (RSFMs) offer promising potential for crop mapping due to their innate ability for unified spatiotemporal processing. However, current RSFMs remain suboptimal for crop mapping: they either employ fixed spatiotemporal windows that ignore the multi-scale nature of crop systems or completely disregard temporal information by focusing solely on spatial patterns. To bridge these gaps, we present AgriFM, a multi-source remote sensing foundation model specifically designed for agricultural crop mapping. Our approach begins by establishing the necessity of simultaneous hierarchical spatiotemporal feature extraction, leading to the development of a modified Video Swin Transformer architecture where temporal down-sampling is synchronized with spatial scaling operations. This modified backbone enables efficient unified processing of long time-series satellite inputs. AgriFM leverages temporally rich data streams from three satellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is pre-trained on a global representative dataset comprising over 25 million image samples supervised by land cover products. The resulting framework incorporates a versatile decoder architecture that dynamically fuses these learned spatiotemporal representations, supporting diverse downstream tasks. Comprehensive evaluations demonstrate AgriFM's superior performance over conventional deep learning approaches and state-of-the-art general-purpose RSFMs across all downstream tasks. Codes will be available at urlhttps://github.com/flyakon/AgriFM.</li>
</ul>

<h3>Title: CRISP-NAM: Competing Risks Interpretable Survival Prediction with Neural Additive Models</h3>
<ul>
<li><strong>Authors: </strong>Dhanesh Ramachandram</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21360">https://arxiv.org/abs/2505.21360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21360">https://arxiv.org/pdf/2505.21360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21360]] CRISP-NAM: Competing Risks Interpretable Survival Prediction with Neural Additive Models(https://arxiv.org/abs/2505.21360)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Competing risks are crucial considerations in survival modelling, particularly in healthcare domains where patients may experience multiple distinct event types. We propose CRISP-NAM (Competing Risks Interpretable Survival Prediction with Neural Additive Models), an interpretable neural additive model for competing risks survival analysis which extends the neural additive architecture to model cause-specific hazards while preserving feature-level interpretability. Each feature contributes independently to risk estimation through dedicated neural networks, allowing for visualization of complex non-linear relationships between covariates and each competing risk. We demonstrate competitive performance on multiple datasets compared to existing approaches.</li>
</ul>

<h3>Title: Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History</h3>
<ul>
<li><strong>Authors: </strong>Qishuai Zhong, Zongmin Li, Siqi Fan, Aixin Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21362">https://arxiv.org/abs/2505.21362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21362">https://arxiv.org/pdf/2505.21362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21362]] Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History(https://arxiv.org/abs/2505.21362)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Effective engagement by large language models (LLMs) requires adapting responses to users' sociodemographic characteristics, such as age, occupation, and education level. While many real-world applications leverage dialogue history for contextualization, existing evaluations of LLMs' behavioral adaptation often focus on single-turn prompts. In this paper, we propose a framework to evaluate LLM adaptation when attributes are introduced either (1) explicitly via user profiles in the prompt or (2) implicitly through multi-turn dialogue history. We assess the consistency of model behavior across these modalities. Using a multi-agent pipeline, we construct a synthetic dataset pairing dialogue histories with distinct user profiles and employ questions from the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe value expression. Our findings indicate that most models adjust their expressed values in response to demographic changes, particularly in age and education level, but consistency varies. Models with stronger reasoning capabilities demonstrate greater alignment, indicating the importance of reasoning in robust sociodemographic adaptation.</li>
</ul>

<h3>Title: Subgroups Matter for Robust Bias Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Anissa Alloula, Charles Jones, Ben Glocker, BartÅomiej W. PapieÅ¼</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21363">https://arxiv.org/abs/2505.21363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21363">https://arxiv.org/pdf/2505.21363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21363]] Subgroups Matter for Robust Bias Mitigation(https://arxiv.org/abs/2505.21363)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Despite the constant development of new bias mitigation methods for machine learning, no method consistently succeeds, and a fundamental question remains unanswered: when and why do bias mitigation techniques fail? In this paper, we hypothesise that a key factor may be the often-overlooked but crucial step shared by many bias mitigation methods: the definition of subgroups. To investigate this, we conduct a comprehensive evaluation of state-of-the-art bias mitigation methods across multiple vision and language classification tasks, systematically varying subgroup definitions, including coarse, fine-grained, intersectional, and noisy subgroups. Our results reveal that subgroup choice significantly impacts performance, with certain groupings paradoxically leading to worse outcomes than no mitigation at all. Our findings suggest that observing a disparity between a set of subgroups is not a sufficient reason to use those subgroups for mitigation. Through theoretical analysis, we explain these phenomena and uncover a counter-intuitive insight that, in some cases, improving fairness with respect to a particular set of subgroups is best achieved by using a different set of subgroups for mitigation. Our work highlights the importance of careful subgroup definition in bias mitigation and suggest it as a alternative lever for improving the robustness and fairness of machine learning models.</li>
</ul>

<h3>Title: Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders</h3>
<ul>
<li><strong>Authors: </strong>James Oldfield, Shawn Im, Yixuan Li, Mihalis A. Nicolaou, Ioannis Patras, Grigorios G Chrysos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21364">https://arxiv.org/abs/2505.21364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21364">https://arxiv.org/pdf/2505.21364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21364]] Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders(https://arxiv.org/abs/2505.21364)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Multilayer perceptrons (MLPs) are an integral part of large language models, yet their dense representations render them difficult to understand, edit, and steer. Recent methods learn interpretable approximations via neuron-level sparsity, yet fail to faithfully reconstruct the original mapping--significantly increasing model's next-token cross-entropy loss. In this paper, we advocate for moving to layer-level sparsity to overcome the accuracy trade-off in sparse layer approximation. Under this paradigm, we introduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear Units, expanding pre-trained dense layers into tens of thousands of specialized sublayers. Through a flexible form of tensor factorization, each sparsely activating MxD sublayer implements a linear transformation with full-rank weights--preserving the original decoders' expressive capacity even under heavy sparsity. Experimentally, we show that MxDs significantly outperform state-of-the-art methods (e.g., Transcoders) on the sparsity-accuracy frontier in language models with up to 3B parameters. Further evaluations on sparse probing and feature steering demonstrate that MxDs learn similarly specialized features of natural language--opening up a promising new avenue for designing interpretable yet faithful decompositions. Our code is included at: this https URL.</li>
</ul>

<h3>Title: PLANETALIGN: A Comprehensive Python Library for Benchmarking Network Alignment</h3>
<ul>
<li><strong>Authors: </strong>Qi Yu, Zhichen Zeng, Yuchen Yan, Zhining Liu, Baoyu Jing, Ruizhong Qiu, Ariful Azad, Hanghang Tong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21366">https://arxiv.org/abs/2505.21366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21366">https://arxiv.org/pdf/2505.21366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21366]] PLANETALIGN: A Comprehensive Python Library for Benchmarking Network Alignment(https://arxiv.org/abs/2505.21366)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Network alignment (NA) aims to identify node correspondence across different networks and serves as a critical cornerstone behind various downstream multi-network learning tasks. Despite growing research in NA, there lacks a comprehensive library that facilitates the systematic development and benchmarking of NA methods. In this work, we introduce PLANETALIGN, a comprehensive Python library for network alignment that features a rich collection of built-in datasets, methods, and evaluation pipelines with easy-to-use APIs. Specifically, PLANETALIGN integrates 18 datasets and 14 NA methods with extensible APIs for easy use and development of NA methods. Our standardized evaluation pipeline encompasses a wide range of metrics, enabling a systematic assessment of the effectiveness, scalability, and robustness of NA methods. Through extensive comparative studies, we reveal practical insights into the strengths and limitations of existing NA methods. We hope that PLANETALIGN can foster a deeper understanding of the NA problem and facilitate the development and benchmarking of more effective, scalable, and robust methods in the future. The source code of PLANETALIGN is available at this https URL.</li>
</ul>

<h3>Title: Improving LLM-based Global Optimization with Search Space Partitioning</h3>
<ul>
<li><strong>Authors: </strong>Andrej Schwanke, Lyubomir Ivanov, David Salinas, Fabio Ferreira, Aaron Klein, Frank Hutter, Arber Zela</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21372">https://arxiv.org/abs/2505.21372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21372">https://arxiv.org/pdf/2505.21372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21372]] Improving LLM-based Global Optimization with Search Space Partitioning(https://arxiv.org/abs/2505.21372)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently emerged as effective surrogate models and candidate generators within global optimization frameworks for expensive blackbox functions. Despite promising results, LLM-based methods often struggle in high-dimensional search spaces or when lacking domain-specific priors, leading to sparse or uninformative suggestions. To overcome these limitations, we propose HOLLM, a novel global optimization algorithm that enhances LLM-driven sampling by partitioning the search space into promising subregions. Each subregion acts as a ``meta-arm'' selected via a bandit-inspired scoring mechanism that effectively balances exploration and exploitation. Within each selected subregion, an LLM then proposes high-quality candidate points, without any explicit domain knowledge. Empirical evaluation on standard optimization benchmarks shows that HOLLM consistently matches or surpasses leading Bayesian optimization and trust-region methods, while substantially outperforming global LLM-based sampling strategies.</li>
</ul>

<h3>Title: GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution</h3>
<ul>
<li><strong>Authors: </strong>Fengxiang Wang, Mingshuo Chen, Yueying Li, Di Wang, Haotian Wang, Zonghao Guo, Zefan Wang, Boqi Shan, Long Lan, Yulin Wang, Hongzhen Wang, Wenjing Yang, Bo Du, Jing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21375">https://arxiv.org/abs/2505.21375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21375">https://arxiv.org/pdf/2505.21375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21375]] GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution(https://arxiv.org/abs/2505.21375)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data for Earth observation but pose challenges for existing multimodal foundation models due to two key bottlenecks: (1) limited availability of UHR training data, and (2) token explosion caused by the large image size. To address data scarcity, we introduce SuperRS-VQA (avg. 8,376$\times$8,376) and HighRS-VQA (avg. 2,000$\times$1,912), the highest-resolution vision-language datasets in RS to date, covering 22 real-world dialogue tasks. To mitigate token explosion, our pilot studies reveal significant redundancy in RS images: crucial information is concentrated in a small subset of object-centric tokens, while pruning background tokens (e.g., ocean or forest) can even improve performance. Motivated by these findings, we propose two strategies: Background Token Pruning and Anchored Token Selection, to reduce the memory footprint while preserving key this http URL these techniques, we introduce GeoLLaVA-8K, the first RS-focused multimodal large language model capable of handling inputs up to 8K$\times$8K resolution, built on the LLaVA framework. Trained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art on the XLRS-Bench.</li>
</ul>

<h3>Title: PHISH in MESH: Korean Adversarial Phonetic Substitution and Phonetic-Semantic Feature Integration Defense</h3>
<ul>
<li><strong>Authors: </strong>Byungjun Kim, Minju Kim, Hyeonchu Park, Bugeun Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21380">https://arxiv.org/abs/2505.21380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21380">https://arxiv.org/pdf/2505.21380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21380]] PHISH in MESH: Korean Adversarial Phonetic Substitution and Phonetic-Semantic Feature Integration Defense(https://arxiv.org/abs/2505.21380)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust</a></li>
<li><strong>Abstract: </strong>As malicious users increasingly employ phonetic substitution to evade hate speech detection, researchers have investigated such strategies. However, two key challenges remain. First, existing studies have overlooked the Korean language, despite its vulnerability to phonetic perturbations due to its phonographic nature. Second, prior work has primarily focused on constructing datasets rather than developing architectural defenses. To address these challenges, we propose (1) PHonetic-Informed Substitution for Hangul (PHISH) that exploits the phonological characteristics of the Korean writing system, and (2) Mixed Encoding of Semantic-pHonetic features (MESH) that enhances the detector's robustness by incorporating phonetic information at the architectural level. Our experimental results demonstrate the effectiveness of our proposed methods on both perturbed and unperturbed datasets, suggesting that they not only improve detection performance but also reflect realistic adversarial behaviors employed by malicious users.</li>
</ul>

<h3>Title: ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding</h3>
<ul>
<li><strong>Authors: </strong>Linshuang Diao, Dayong Ren, Sensen Song, Yurong Qian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21381">https://arxiv.org/abs/2505.21381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21381">https://arxiv.org/pdf/2505.21381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21381]] ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding(https://arxiv.org/abs/2505.21381)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>State Space models (SSMs) such as PointMamba enable efficient feature extraction for point cloud self-supervised learning with linear complexity, outperforming Transformers in computational efficiency. However, existing PointMamba-based methods depend on complex token ordering and random masking, which disrupt spatial continuity and local semantic correlations. We propose ZigzagPointMamba to tackle these challenges. The core of our approach is a simple zigzag scan path that globally sequences point cloud tokens, enhancing spatial continuity by preserving the proximity of spatially adjacent point tokens. Nevertheless, random masking undermines local semantic modeling in self-supervised learning. To address this, we introduce a Semantic-Siamese Masking Strategy (SMS), which masks semantically similar tokens to facilitate reconstruction by integrating local features of original and similar tokens. This overcomes the dependence on isolated local features and enables robust global semantic modeling. Our pre-trained ZigzagPointMamba weights significantly improve downstream tasks, achieving a 1.59% mIoU gain on ShapeNetPart for part segmentation, a 0.4% higher accuracy on ModelNet40 for classification, and 0.19%, 1.22%, and 0.72% higher accuracies respectively for the classification tasks on the OBJ-BG, OBJ-ONLY, and PB-T50-RS subsets of ScanObjectNN. The code is available at: this https URL</li>
</ul>

<h3>Title: DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Nastaran Saadati, Zhanhong Jiang, Joshua R. Waite, Shreyan Ganguly, Aditya Balu, Chinmay Hegde, Soumik Sarkar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21382">https://arxiv.org/abs/2505.21382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21382">https://arxiv.org/pdf/2505.21382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21382]] DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models(https://arxiv.org/abs/2505.21382)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, large language model</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) has emerged as one of the most effective, computationally tractable fine-tuning approaches for training Vision-Language Models (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by freezing the pre-trained model weights and injecting trainable low-rank matrices, allowing for efficient learning of these foundation models even on edge devices. However, LoRA in decentralized settings still remains under explored, particularly for the theoretical underpinnings due to the lack of smoothness guarantee and model consensus interference (defined formally below). This work improves the convergence rate of decentralized LoRA (DLoRA) to match the rate of decentralized SGD by ensuring gradient smoothness. We also introduce DeCAF, a novel algorithm integrating DLoRA with truncated singular value decomposition (TSVD)-based matrix factorization to resolve consensus interference. Theoretical analysis shows TSVD's approximation error is bounded and consensus differences between DLoRA and DeCAF vanish as rank increases, yielding DeCAF's matching convergence rate. Extensive experiments across vision/language tasks demonstrate our algorithms outperform local training and rivals federated learning under both IID and non-IID data distributions.</li>
</ul>

<h3>Title: Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Xihong Yang, Siwei Wang, Fangdi Wang, Jiaqi Jin, Suyuan Liu, Yue Liu, En Zhu, Xinwang Liu, Yueming Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21387">https://arxiv.org/abs/2505.21387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21387">https://arxiv.org/pdf/2505.21387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21387]] Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios(https://arxiv.org/abs/2505.21387)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Leveraging the powerful representation learning capabilities, deep multi-view clustering methods have demonstrated reliable performance by effectively integrating multi-source information from diverse views in recent years. Most existing methods rely on the assumption of clean views. However, noise is pervasive in real-world scenarios, leading to a significant degradation in performance. To tackle this problem, we propose a novel multi-view clustering framework for the automatic identification and rectification of noisy data, termed AIRMVC. Specifically, we reformulate noisy identification as an anomaly identification problem using GMM. We then design a hybrid rectification strategy to mitigate the adverse effects of noisy data based on the identification results. Furthermore, we introduce a noise-robust contrastive mechanism to generate reliable representations. Additionally, we provide a theoretical proof demonstrating that these representations can discard noisy information, thereby improving the performance of downstream tasks. Extensive experiments on six benchmark datasets demonstrate that AIRMVC outperforms state-of-the-art algorithms in terms of robustness in noisy scenarios. The code of AIRMVC are available at this https URL on Github.</li>
</ul>

<h3>Title: AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Xuanwen Ding, Chengjun Pan, Zejun Li, Jiwen Zhang, Siyuan Wang, Zhongyu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21389">https://arxiv.org/abs/2505.21389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21389">https://arxiv.org/pdf/2505.21389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21389]] AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs(https://arxiv.org/abs/2505.21389)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluating multimodal large language models (MLLMs) is increasingly expensive, as the growing size and cross-modality complexity of benchmarks demand significant scoring efforts. To tackle with this difficulty, we introduce AutoJudger, an agent-driven framework for efficient and adaptive benchmarking of MLLMs that tackles this escalating cost. AutoJudger employs the Item Response Theory (IRT) to estimate the question difficulty and an autonomous evaluation agent to dynamically select the most informative test questions based on the model's real-time performance. Specifically, AutoJudger incorporates two pivotal components: a semantic-aware retrieval mechanism to ensure that selected questions cover diverse and challenging scenarios across both vision and language modalities, and a dynamic memory that maintains contextual statistics of previously evaluated questions to guide coherent and globally informed question selection throughout the evaluation process. Extensive experiments on four representative multimodal benchmarks demonstrate that our adaptive framework dramatically reduces evaluation expenses, i.e. AutoJudger uses only 4% of the data to achieve over 90% ranking accuracy with the full benchmark evaluation on MMT-Bench.</li>
</ul>

<h3>Title: Square$Ï$PO: Differentially Private and Robust $Ï^2$-Preference Optimization in Offline Direct Alignment</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Zhou, Yulian Wu, Wenqian Weng, Francesco Orabona</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21395">https://arxiv.org/abs/2505.21395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21395">https://arxiv.org/pdf/2505.21395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21395]] Square$Ï$PO: Differentially Private and Robust $Ï^2$-Preference Optimization in Offline Direct Alignment(https://arxiv.org/abs/2505.21395)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust</a></li>
<li><strong>Abstract: </strong>In this paper, we theoretically study the offline alignment of language models with human preference feedback, under both preference label corruption and privacy protections. To this end, we propose Square$\chi$PO, a simple one-line change to $\chi$PO where the standard log-loss is replaced by a new square loss over probability. Thanks to the inherent properties of this new loss, we have advanced the state-of-the-art of differentially private and robust offline direct alignment. Specifically, for the local model of label privacy, Square$\chi$PO is the first algorithm that attains an optimal rate based on single-policy concentrability even with general function approximations. It also gives the first result under the central model of privacy protection over both prompts (responses) and labels. On the robustness side against Huber label corruption, Square$\chi$PO is the first alignment method that has a meaningful theoretical guarantee under general function approximations. More importantly, Square$\chi$PO can address privacy protection and corruption simultaneously, where an interesting separation is observed, implying that the order of privacy and corruption matters. Furthermore, we show that Square$\chi$PO can also be easily extended to handle the scenario of the general preference model with state-of-the-art guarantees under corruption and privacy. Last but not least, all of our theoretical guarantees enjoy a unified analysis, building upon a new result on the generalization error bounds of least-square regression under corruption and privacy constraints, which we believe is of independent interest to the community.</li>
</ul>

<h3>Title: Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science</h3>
<ul>
<li><strong>Authors: </strong>Xiao Liu, Xinyi Dong, Xinyang Gao, Yansong Feng, Xun Pang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21396">https://arxiv.org/abs/2505.21396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21396">https://arxiv.org/pdf/2505.21396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21396]] Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science(https://arxiv.org/abs/2505.21396)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have shown promise in generating novel research ideas. However, these ideas often face challenges related to feasibility and expected effectiveness. This paper explores how augmenting LLMs with relevant data during the idea generation process can enhance the quality of generated ideas. We introduce two ways of incorporating data: (1) providing metadata during the idea generation stage to guide LLMs toward feasible directions, and (2) adding automatic validation during the idea selection stage to assess the empirical plausibility of hypotheses within ideas. We conduct experiments in the social science domain, specifically with climate negotiation topics, and find that metadata improves the feasibility of generated ideas by 20%, while automatic validation improves the overall quality of selected ideas by 7%. A human study shows that LLM-generated ideas, along with their related data and validation processes, inspire researchers to propose research ideas with higher quality. Our work highlights the potential of data-driven research idea generation, and underscores the practical utility of LLM-assisted ideation in real-world academic settings.</li>
</ul>

<h3>Title: DecisionFlow: Advancing Large Language Model as Principled Decision Maker</h3>
<ul>
<li><strong>Authors: </strong>Xiusi Chen, Shanyong Wang, Cheng Qian, Hongru Wang, Peixuan Han, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21397">https://arxiv.org/abs/2505.21397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21397">https://arxiv.org/pdf/2505.21397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21397]] DecisionFlow: Advancing Large Language Model as Principled Decision Maker(https://arxiv.org/abs/2505.21397)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In high-stakes domains such as healthcare and finance, effective decision-making demands not just accurate outcomes but transparent and explainable reasoning. However, current language models often lack the structured deliberation needed for such tasks, instead generating decisions and justifications in a disconnected, post-hoc manner. To address this, we propose DecisionFlow, a novel decision modeling framework that guides models to reason over structured representations of actions, attributes, and constraints. Rather than predicting answers directly from prompts, DecisionFlow builds a semantically grounded decision space and infers a latent utility function to evaluate trade-offs in a transparent, utility-driven manner. This process produces decisions tightly coupled with interpretable rationales reflecting the model's reasoning. Empirical results on two high-stakes benchmarks show that DecisionFlow not only achieves up to 30% accuracy gains over strong prompting baselines but also enhances alignment in outcomes. Our work is a critical step toward integrating symbolic reasoning with LLMs, enabling more accountable, explainable, and reliable LLM decision support systems. We release the data and code at this https URL.</li>
</ul>

<h3>Title: Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling</h3>
<ul>
<li><strong>Authors: </strong>Hovhannes Tamoyan, Subhabrata Dutta, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21399">https://arxiv.org/abs/2505.21399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21399">https://arxiv.org/pdf/2505.21399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21399]] Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling(https://arxiv.org/abs/2505.21399)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Factual incorrectness in generated content is one of the primary concerns in ubiquitous deployment of large language models (LLMs). Prior findings suggest LLMs can (sometimes) detect factual incorrectness in their generated content (i.e., fact-checking post-generation). In this work, we provide evidence supporting the presence of LLMs' internal compass that dictate the correctness of factual recall at the time of generation. We demonstrate that for a given subject entity and a relation, LLMs internally encode linear features in the Transformer's residual stream that dictate whether it will be able to recall the correct attribute (that forms a valid entity-relation-attribute triplet). This self-awareness signal is robust to minor formatting variations. We investigate the effects of context perturbation via different example selection strategies. Scaling experiments across model sizes and training dynamics highlight that self-awareness emerges rapidly during training and peaks in intermediate layers. These findings uncover intrinsic self-monitoring capabilities within LLMs, contributing to their interpretability and reliability.</li>
</ul>

<h3>Title: A Convergence Theory for Diffusion Language Models: An Information-Theoretic Perspective</h3>
<ul>
<li><strong>Authors: </strong>Gen Li, Changxiao Cai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21400">https://arxiv.org/abs/2505.21400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21400">https://arxiv.org/pdf/2505.21400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21400]] A Convergence Theory for Diffusion Language Models: An Information-Theoretic Perspective(https://arxiv.org/abs/2505.21400)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as a powerful paradigm for modern generative modeling, demonstrating strong potential for large language models (LLMs). Unlike conventional autoregressive (AR) models that generate tokens sequentially, diffusion models enable parallel token sampling, leading to faster generation and eliminating left-to-right generation constraints. Despite their empirical success, the theoretical understanding of diffusion model approaches remains underdeveloped. In this work, we develop convergence guarantees for diffusion language models from an information-theoretic perspective. Our analysis demonstrates that the sampling error, measured by the Kullback-Leibler (KL) divergence, decays inversely with the number of iterations $T$ and scales linearly with the mutual information between tokens in the target text sequence. In particular, we establish matching upper and lower bounds, up to some constant factor, to demonstrate the tightness of our convergence analysis. These results offer novel theoretical insights into the practical effectiveness of diffusion language models.</li>
</ul>

<h3>Title: Enhancing JavaScript Malware Detection through Weighted Behavioral DFAs</h3>
<ul>
<li><strong>Authors: </strong>Pedro Pereira, JosÃ© GonÃ§alves, JoÃ£o Vitorino, Eva Maia, Isabel PraÃ§a</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21406">https://arxiv.org/abs/2505.21406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21406">https://arxiv.org/pdf/2505.21406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21406]] Enhancing JavaScript Malware Detection through Weighted Behavioral DFAs(https://arxiv.org/abs/2505.21406)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>This work addresses JavaScript malware detection to enhance client-side web application security with a behavior-based system. The ability to detect malicious JavaScript execution sequences is a critical problem in modern web security as attack techniques become more sophisticated. This study introduces a new system for detecting JavaScript malware using a Deterministic Finite Automaton (DFA) along with a weighted-behavior system, which we call behavior DFA. This system captures malicious patterns and provides a dynamic mechanism to classify new sequences that exhibit partial similarity to known attacks, differentiating them between benign, partially malicious, and fully malicious behaviors. Experimental evaluation on a dataset of 1,058 sequences captured in a real-world environment demonstrates the capability of the system to detect and classify threats effectively, with the behavior DFA successfully identifying exact matches and partial similarities to known malicious behaviors. The results highlight the adaptability of the system in detecting emerging threats while maintaining transparency in decision making.</li>
</ul>

<h3>Title: RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dario Satriani, Enzo Veltri, Donatello Santoro, Paolo Papotti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21409">https://arxiv.org/abs/2505.21409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21409">https://arxiv.org/pdf/2505.21409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21409]] RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models(https://arxiv.org/abs/2505.21409)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Factuality in Large Language Models (LLMs) is a persistent challenge. Current benchmarks often assess short factual answers, overlooking the critical ability to generate structured, multi-record tabular outputs from parametric knowledge. We demonstrate that this relational fact retrieval is substantially more difficult than isolated point-wise queries, even when individual facts are known to the model, exposing distinct failure modes sensitive to output dimensionality (e.g., number of attributes or records). To systematically evaluate this under-explored capability, we introduce RelationalFactQA, a new benchmark featuring diverse natural language questions (paired with SQL) and gold-standard tabular answers, specifically designed to assess knowledge retrieval in a structured format. RelationalFactQA enables analysis across varying query complexities, output sizes, and data characteristics. Our experiments reveal that even state-of-the-art LLMs struggle significantly, not exceeding 25% factual accuracy in generating relational outputs, with performance notably degrading as output dimensionality increases. These findings underscore critical limitations in current LLMs' ability to synthesize structured factual knowledge and establish RelationalFactQA as a crucial resource for measuring future progress in LLM factuality.</li>
</ul>

<h3>Title: Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity</h3>
<ul>
<li><strong>Authors: </strong>Yehui Tang, Xiaosong Li, Fangcheng Liu, Wei Guo, Hang Zhou, Yaoyuan Wang, Kai Han, Xianzhi Yu, Jinpeng Li, Hui Zang, Fei Mi, Xiaojun Meng, Zhicheng Liu, Hanting Chen, Binfan Zheng, Can Chen, Youliang Yan, Ruiming Tang, Peifeng Qin, Xinghao Chen, Dacheng Tao, Yunhe Wang (and Other Contributors)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21411">https://arxiv.org/abs/2505.21411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21411">https://arxiv.org/pdf/2505.21411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21411]] Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity(https://arxiv.org/abs/2505.21411)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The surgence of Mixture of Experts (MoE) in Large Language Models promises a small price of execution cost for a much larger model parameter count and learning capacity, because only a small fraction of parameters are activated for each input token. However, it is commonly observed that some experts are activated far more often than others, leading to system inefficiency when running the experts on different devices in parallel. Therefore, we introduce Mixture of Grouped Experts (MoGE), which groups the experts during selection and balances the expert workload better than MoE in nature. It constrains tokens to activate an equal number of experts within each predefined expert group. When a model execution is distributed on multiple devices, this architectural design ensures a balanced computational load across devices, significantly enhancing throughput, particularly for the inference phase. Further, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE with 72 billion total parameters, 16 billion of which are activated for each token. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and 800I A2 through extensive system simulation studies. Our experiments indicate that MoGE indeed leads to better expert load balancing and more efficient execution for both model training and inference on Ascend NPUs. The inference performance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further improved to 1528 tokens/s per card by speculative acceleration, outperforming comparable 32B and 72B Dense models. Furthermore, we achieve an excellent cost-to-performance ratio for model inference on Ascend 300I this http URL studies show that Ascend NPUs are capable of training Pangu Pro MoE with massive parallelization to make it a leading model within the sub-100B total parameter class, outperforming prominent open-source models like GLM-Z1-32B and Qwen3-32B.</li>
</ul>

<h3>Title: RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation</h3>
<ul>
<li><strong>Authors: </strong>Xiao Liu, Da Yin, Zirui Wu, Yansong Feng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21413">https://arxiv.org/abs/2505.21413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21413">https://arxiv.org/pdf/2505.21413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21413]] RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation(https://arxiv.org/abs/2505.21413)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tools enhance the reasoning capabilities of large language models (LLMs) in complex problem-solving tasks, but not all tasks have available tools. In the absence of predefined tools, prior works have explored instructing LLMs to generate tools on their own. However, such approaches rely heavily on the models' internal knowledge and would fail in domains beyond the LLMs' knowledge scope. To address this limitation, we propose RefTool, a reference-guided framework for automatic tool creation that leverages structured external materials such as textbooks. RefTool consists of two modules: (1) tool creation, where LLMs generate executable tools from reference content, validate them using illustrative examples, and organize them hierarchically into a toolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to select and apply the appropriate tools to solve problems. Experiments on causality, physics, and chemistry benchmarks demonstrate that RefTool outperforms existing tool-creation and domain-specific reasoning methods by 11.3% on average accuracy, while being cost-efficient and broadly generalizable. Analyses reveal that grounding tool creation in references produces accurate and faithful tools, and that the hierarchical structure facilitates effective tool selection. RefTool enables LLMs to overcome knowledge limitations, demonstrating the value of grounding tool creation in external references for enhanced and generalizable reasoning.</li>
</ul>

<h3>Title: A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment</h3>
<ul>
<li><strong>Authors: </strong>Brett Bissey, Kyle Gatesman, Walker Dimon, Mohammad Alam, Luis Robaina, Joseph Weissman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21414">https://arxiv.org/abs/2505.21414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21414">https://arxiv.org/pdf/2505.21414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21414]] A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment(https://arxiv.org/abs/2505.21414)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>This paper introduces a comprehensive framework designed to analyze and secure decision-support systems trained with Deep Reinforcement Learning (DRL), prior to deployment, by providing insights into learned behavior patterns and vulnerabilities discovered through simulation. The introduced framework aids in the development of precisely timed and targeted observation perturbations, enabling researchers to assess adversarial attack outcomes within a strategic decision-making context. We validate our framework, visualize agent behavior, and evaluate adversarial outcomes within the context of a custom-built strategic game, CyberStrike. Utilizing the proposed framework, we introduce a method for systematically discovering and ranking the impact of attacks on various observation indices and time-steps, and we conduct experiments to evaluate the transferability of adversarial attacks across agent architectures and DRL training algorithms. The findings underscore the critical need for robust adversarial defense mechanisms to protect decision-making policies in high-stakes environments.</li>
</ul>

<h3>Title: When Shift Happens - Confounding Is to Blame</h3>
<ul>
<li><strong>Authors: </strong>Abbavaram Gowtham Reddy, Celia Rubio-Madrigal, Rebekka Burkholz, Krikamol Muandet</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21422">https://arxiv.org/abs/2505.21422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21422">https://arxiv.org/pdf/2505.21422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21422]] When Shift Happens - Confounding Is to Blame(https://arxiv.org/abs/2505.21422)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Distribution shifts introduce uncertainty that undermines the robustness and generalization capabilities of machine learning models. While conventional wisdom suggests that learning causal-invariant representations enhances robustness to such shifts, recent empirical studies present a counterintuitive finding: (i) empirical risk minimization (ERM) can rival or even outperform state-of-the-art out-of-distribution (OOD) generalization methods, and (ii) its OOD generalization performance improves when all available covariates, not just causal ones, are utilized. Drawing on both empirical and theoretical evidence, we attribute this phenomenon to hidden confounding. Shifts in hidden confounding induce changes in data distributions that violate assumptions commonly made by existing OOD generalization approaches. Under such conditions, we prove that effective generalization requires learning environment-specific relationships, rather than relying solely on invariant ones. Furthermore, we show that models augmented with proxies for hidden confounders can mitigate the challenges posed by hidden confounding shifts. These findings offer new theoretical insights and practical guidance for designing robust OOD generalization algorithms and principled covariate selection strategies.</li>
</ul>

<h3>Title: Attribute-Efficient PAC Learning of Sparse Halfspaces with Constant Malicious Noise Rate</h3>
<ul>
<li><strong>Authors: </strong>Shiwei Zeng, Jie Shen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21430">https://arxiv.org/abs/2505.21430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21430">https://arxiv.org/pdf/2505.21430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21430]] Attribute-Efficient PAC Learning of Sparse Halfspaces with Constant Malicious Noise Rate(https://arxiv.org/abs/2505.21430)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Attribute-efficient learning of sparse halfspaces has been a fundamental problem in machine learning theory. In recent years, machine learning algorithms are faced with prevalent data corruptions or even adversarial attacks. It is of central interest to design efficient algorithms that are robust to noise corruptions. In this paper, we consider that there exists a constant amount of malicious noise in the data and the goal is to learn an underlying $s$-sparse halfspace $w^* \in \mathbb{R}^d$ with $\text{poly}(s,\log d)$ samples. Specifically, we follow a recent line of works and assume that the underlying distribution satisfies a certain concentration condition and a margin condition at the same time. Under such conditions, we show that attribute-efficiency can be achieved by simple variants to existing hinge loss minimization programs. Our key contribution includes: 1) an attribute-efficient PAC learning algorithm that works under constant malicious noise rate; 2) a new gradient analysis that carefully handles the sparsity constraint in hinge loss minimization.</li>
</ul>

<h3>Title: Can Large Reasoning Models Self-Train?</h3>
<ul>
<li><strong>Authors: </strong>Sheikh Shafayat, Fahim Tajwar, Ruslan Salakhutdinov, Jeff Schneider, Andrea Zanette</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21444">https://arxiv.org/abs/2505.21444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21444">https://arxiv.org/pdf/2505.21444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21444]] Can Large Reasoning Models Self-Train?(https://arxiv.org/abs/2505.21444)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scaling the performance of large language models (LLMs) increasingly depends on methods that reduce reliance on human supervision. Reinforcement learning from automated verification offers an alternative, but it incurs scalability limitations due to dependency upon human-designed verifiers. Self-training, where the model's own judgment provides the supervisory signal, presents a compelling direction. We propose an online self-training reinforcement learning algorithm that leverages the model's self-consistency to infer correctness signals and train without any ground-truth supervision. We apply the algorithm to challenging mathematical reasoning tasks and show that it quickly reaches performance levels rivaling reinforcement-learning methods trained explicitly on gold-standard answers. Additionally, we analyze inherent limitations of the algorithm, highlighting how the self-generated proxy reward initially correlated with correctness can incentivize reward hacking, where confidently incorrect outputs are favored. Our results illustrate how self-supervised improvement can achieve significant performance gains without external labels, while also revealing its fundamental challenges.</li>
</ul>

<h3>Title: OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Ziqiao Peng, Jiwen Liu, Haoxian Zhang, Xiaoqiang Liu, Songlin Tang, Pengfei Wan, Di Zhang, Hongyan Liu, Jun He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21448">https://arxiv.org/abs/2505.21448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21448">https://arxiv.org/pdf/2505.21448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21448]] OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers(https://arxiv.org/abs/2505.21448)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Lip synchronization is the task of aligning a speaker's lip movements in video with corresponding speech audio, and it is essential for creating realistic, expressive video content. However, existing methods often rely on reference frames and masked-frame inpainting, which limit their robustness to identity consistency, pose variations, facial occlusions, and stylized content. In addition, since audio signals provide weaker conditioning than visual cues, lip shape leakage from the original video will affect lip sync quality. In this paper, we present OmniSync, a universal lip synchronization framework for diverse visual scenarios. Our approach introduces a mask-free training paradigm using Diffusion Transformer models for direct frame editing without explicit masks, enabling unlimited-duration inference while maintaining natural facial dynamics and preserving character identity. During inference, we propose a flow-matching-based progressive noise initialization to ensure pose and identity consistency, while allowing precise mouth-region editing. To address the weak conditioning signal of audio, we develop a Dynamic Spatiotemporal Classifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance strength over time and space. We also establish the AIGC-LipSync Benchmark, the first evaluation suite for lip synchronization in diverse AI-generated videos. Extensive experiments demonstrate that OmniSync significantly outperforms prior methods in both visual quality and lip sync accuracy, achieving superior results in both real-world and AI-generated videos.</li>
</ul>

<h3>Title: Designing Cyclic Peptides via Harmonic SDE with Atom-Bond Modeling</h3>
<ul>
<li><strong>Authors: </strong>Xiangxin Zhou, Mingyu Li, Yi Xiao, Jiahan Li, Dongyu Xue, Zaixiang Zheng, Jianzhu Ma, Quanquan Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21452">https://arxiv.org/abs/2505.21452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21452">https://arxiv.org/pdf/2505.21452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21452]] Designing Cyclic Peptides via Harmonic SDE with Atom-Bond Modeling(https://arxiv.org/abs/2505.21452)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Cyclic peptides offer inherent advantages in pharmaceuticals. For example, cyclic peptides are more resistant to enzymatic hydrolysis compared to linear peptides and usually exhibit excellent stability and affinity. Although deep generative models have achieved great success in linear peptide design, several challenges prevent the development of computational methods for designing diverse types of cyclic peptides. These challenges include the scarcity of 3D structural data on target proteins and associated cyclic peptide ligands, the geometric constraints that cyclization imposes, and the involvement of non-canonical amino acids in cyclization. To address the above challenges, we introduce CpSDE, which consists of two key components: AtomSDE, a generative structure prediction model based on harmonic SDE, and ResRouter, a residue type predictor. Utilizing a routed sampling algorithm that alternates between these two models to iteratively update sequences and structures, CpSDE facilitates the generation of cyclic peptides. By employing explicit all-atom and bond modeling, CpSDE overcomes existing data limitations and is proficient in designing a wide variety of cyclic peptides. Our experimental results demonstrate that the cyclic peptides designed by our method exhibit reliable stability and affinity.</li>
</ul>

<h3>Title: Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO</h3>
<ul>
<li><strong>Authors: </strong>Muzhi Zhu, Hao Zhong, Canyu Zhao, Zongze Du, Zheng Huang, Mingyu Liu, Hao Chen, Cheng Zou, Jingdong Chen, Ming Yang, Chunhua Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21457">https://arxiv.org/abs/2505.21457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21457">https://arxiv.org/pdf/2505.21457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21457]] Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO(https://arxiv.org/abs/2505.21457)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimodal Large Language Models (MLLMs) as central planning and decision-making modules in robotic systems has gained extensive attention. However, despite the importance of active perception in embodied intelligence, there is little to no exploration of how MLLMs can be equipped with or learn active perception capabilities. In this paper, we first provide a systematic definition of MLLM-based active perception tasks. We point out that the recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a special case of active perception; however, it still suffers from low search efficiency and inaccurate region selection. To address these issues, we propose ACTIVE-O3, a purely reinforcement learning based training framework built on top of GRPO, designed to equip MLLMs with active perception capabilities. We further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across both general open-world tasks, such as small-object and dense object grounding, and domain-specific scenarios, including small object detection in remote sensing and autonomous driving, as well as fine-grained interactive segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot reasoning abilities on the V* Benchmark, without relying on any explicit reasoning data. We hope that our work can provide a simple codebase and evaluation protocol to facilitate future research on active perception in MLLMs.</li>
</ul>

<h3>Title: Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance</h3>
<ul>
<li><strong>Authors: </strong>Shintaro Ozaki, Tatsuya Hiraoka, Hiroto Otake, Hiroki Ouchi, Masaru Isonuma, Benjamin Heinzerling, Kentaro Inui, Taro Watanabe, Yusuke Miyao, Yohei Oseki, Yu Takagi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21458">https://arxiv.org/abs/2505.21458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21458">https://arxiv.org/pdf/2505.21458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21458]] Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance(https://arxiv.org/abs/2505.21458)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are known to process information using a proficient internal language consistently, referred to as latent language, which may differ from the input or output languages. However, how the discrepancy between the latent language and the input and output language affects downstream task performance remains largely unexplored. While many studies research the latent language of LLMs, few address its importance in influencing task performance. In our study, we hypothesize that thinking in latent language consistently enhances downstream task performance. To validate this, our work varies the input prompt languages across multiple downstream tasks and analyzes the correlation between consistency in latent language and task performance. We create datasets consisting of questions from diverse domains such as translation and geo-culture, which are influenced by the choice of latent language. Experimental results across multiple LLMs on translation and geo-culture tasks, which are sensitive to the choice of language, indicate that maintaining consistency in latent language is not always necessary for optimal downstream task performance. This is because these models adapt their internal representations near the final layers to match the target language, reducing the impact of consistency on overall performance.</li>
</ul>

<h3>Title: Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Zhanqiu Hu, Jian Meng, Yash Akhauri, Mohamed S. Abdelfattah, Jae-sun Seo, Zhiru Zhang, Udit Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21467">https://arxiv.org/abs/2505.21467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21467">https://arxiv.org/pdf/2505.21467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21467]] Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion(https://arxiv.org/abs/2505.21467)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion language models offer parallel token generation and inherent bidirectionality, promising more efficient and powerful sequence modeling compared to autoregressive approaches. However, state-of-the-art diffusion models (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match the quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B, Llama3 8B), their iterative denoising requires multiple full-sequence forward passes, resulting in high computational costs and latency, particularly for long input prompts and long-context scenarios. Furthermore, parallel token generation introduces token incoherence problems, and current sampling heuristics suffer from significant quality drops with decreasing denoising steps. We address these limitations with two training-free techniques. First, we propose FreeCache, a Key-Value (KV) approximation caching technique that reuses stable KV projections across denoising steps, effectively reducing the computational cost of DLM inference. Second, we introduce Guided Diffusion, a training-free method that uses a lightweight pretrained autoregressive model to supervise token unmasking, dramatically reducing the total number of denoising iterations without sacrificing quality. We conduct extensive evaluations on open-source reasoning benchmarks, and our combined methods deliver up to a 34x end-to-end speedup without compromising accuracy. For the first time, diffusion language models achieve a comparable and even faster latency as the widely adopted autoregressive models. Our work successfully paved the way for scaling up the diffusion language model to a broader scope of applications across different domains.</li>
</ul>

<h3>Title: Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Zijun Liu, Zhennan Wan, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21471">https://arxiv.org/abs/2505.21471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21471">https://arxiv.org/pdf/2505.21471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21471]] Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration(https://arxiv.org/abs/2505.21471)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of post-training techniques for reasoning and information seeking, large language models (LLMs) can incorporate a large quantity of retrieved knowledge to solve complex tasks. However, the limited context window of LLMs obstructs scaling the amount of external knowledge input, prohibiting further improvement, especially for tasks requiring significant amount of external knowledge. Existing context window extension methods inevitably cause information loss. LLM-based multi-agent methods emerge as a new paradigm to handle massive input in a distributional manner, where we identify two core bottlenecks in existing knowledge synchronization and reasoning processes. In this work, we develop a multi-agent framework, $\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability in inference-time knowledge integration without longer-context training. Benchmarked with our enhanced multi-hop question answering test, $\textbf{$\boldsymbol{\infty}$Bench+}$, and other public test sets including long survey generation, ExtAgents significantly enhances the performance over existing non-training methods with the same amount of external knowledge input, regardless of whether it falls $\textit{within or exceeds the context window}$. Moreover, the method maintains high efficiency due to high parallelism. Further study in the coordination of LLM agents on increasing external knowledge input could benefit real-world applications.</li>
</ul>

<h3>Title: Algorithms and SQ Lower Bounds for Robustly Learning Real-valued Multi-index Models</h3>
<ul>
<li><strong>Authors: </strong>Ilias Diakonikolas, Giannis Iakovidis, Daniel M. Kane, Lisheng Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21475">https://arxiv.org/abs/2505.21475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21475">https://arxiv.org/pdf/2505.21475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21475]] Algorithms and SQ Lower Bounds for Robustly Learning Real-valued Multi-index Models(https://arxiv.org/abs/2505.21475)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We study the complexity of learning real-valued Multi-Index Models (MIMs) under the Gaussian distribution. A $K$-MIM is a function $f:\mathbb{R}^d\to \mathbb{R}$ that depends only on the projection of its input onto a $K$-dimensional subspace. We give a general algorithm for PAC learning a broad class of MIMs with respect to the square loss, even in the presence of adversarial label noise. Moreover, we establish a nearly matching Statistical Query (SQ) lower bound, providing evidence that the complexity of our algorithm is qualitatively optimal as a function of the dimension. Specifically, we consider the class of bounded variation MIMs with the property that degree at most $m$ distinguishing moments exist with respect to projections onto any subspace. In the presence of adversarial label noise, the complexity of our learning algorithm is $d^{O(m)}2^{\mathrm{poly}(K/\epsilon)}$. For the realizable and independent noise settings, our algorithm incurs complexity $d^{O(m)}2^{\mathrm{poly}(K)}(1/\epsilon)^{O(K)}$. To complement our upper bound, we show that if for some subspace degree-$m$ distinguishing moments do not exist, then any SQ learner for the corresponding class of MIMs requires complexity $d^{\Omega(m)}$. As an application, we give the first efficient learner for the class of positive-homogeneous $L$-Lipschitz $K$-MIMs. The resulting algorithm has complexity $\mathrm{poly}(d) 2^{\mathrm{poly}(KL/\epsilon)}$. This gives a new PAC learning algorithm for Lipschitz homogeneous ReLU networks with complexity independent of the network size, removing the exponential dependence incurred in prior work.</li>
</ul>

<h3>Title: Policy Optimized Text-to-Image Pipeline Design</h3>
<ul>
<li><strong>Authors: </strong>Uri Gadot, Rinon Gal, Yftah Ziser, Gal Chechik, Shie Mannor</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21478">https://arxiv.org/abs/2505.21478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21478">https://arxiv.org/pdf/2505.21478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21478]] Policy Optimized Text-to-Image Pipeline Design(https://arxiv.org/abs/2505.21478)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text-to-image generation has evolved beyond single monolithic models to complex multi-component pipelines. These combine fine-tuned generators, adapters, upscaling blocks and even editing steps, leading to significant improvements in image quality. However, their effective design requires substantial expertise. Recent approaches have shown promise in automating this process through large language models (LLMs), but they suffer from two critical limitations: extensive computational requirements from generating images with hundreds of predefined pipelines, and poor generalization beyond memorized training examples. We introduce a novel reinforcement learning-based framework that addresses these inefficiencies. Our approach first trains an ensemble of reward models capable of predicting image quality scores directly from prompt-workflow combinations, eliminating the need for costly image generation during training. We then implement a two-phase training strategy: initial workflow vocabulary training followed by GRPO-based optimization that guides the model toward higher-performing regions of the workflow space. Additionally, we incorporate a classifier-free guidance based enhancement technique that extrapolates along the path between the initial and GRPO-tuned models, further improving output quality. We validate our approach through a set of comparisons, showing that it can successfully create new flows with greater diversity and lead to superior image quality compared to existing baselines.</li>
</ul>

<h3>Title: Are Language Models Consequentialist or Deontological Moral Reasoners?</h3>
<ul>
<li><strong>Authors: </strong>Keenan Samway, Max Kleiman-Weiner, David Guzman Piedrahita, Rada Mihalcea, Bernhard SchÃ¶lkopf, Zhijing Jin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21479">https://arxiv.org/abs/2505.21479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21479">https://arxiv.org/pdf/2505.21479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21479]] Are Language Models Consequentialist or Deontological Moral Reasoners?(https://arxiv.org/abs/2505.21479)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As AI systems increasingly navigate applications in healthcare, law, and governance, understanding how they handle ethically complex scenarios becomes critical. Previous work has mainly examined the moral judgments in large language models (LLMs), rather than their underlying moral reasoning process. In contrast, we focus on a large-scale analysis of the moral reasoning traces provided by LLMs. Furthermore, unlike prior work that attempted to draw inferences from only a handful of moral dilemmas, our study leverages over 600 distinct trolley problems as probes for revealing the reasoning patterns that emerge within different LLMs. We introduce and test a taxonomy of moral rationales to systematically classify reasoning traces according to two main normative ethical theories: consequentialism and deontology. Our analysis reveals that LLM chains-of-thought tend to favor deontological principles based on moral obligations, while post-hoc explanations shift notably toward consequentialist rationales that emphasize utility. Our framework provides a foundation for understanding how LLMs process and articulate ethical considerations, an important step toward safe and interpretable deployment of LLMs in high-stakes decision-making environments. Our code is available at this https URL .</li>
</ul>

<h3>Title: MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation</h3>
<ul>
<li><strong>Authors: </strong>Kerui Ren, Jiayang Bai, Linning Xu, Lihan Jiang, Jiangmiao Pang, Mulin Yu, Bo Dai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21483">https://arxiv.org/abs/2505.21483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21483">https://arxiv.org/pdf/2505.21483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21483]] MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation(https://arxiv.org/abs/2505.21483)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Object compositing offers significant promise for augmented reality (AR) and embodied intelligence applications. Existing approaches predominantly focus on single-image scenarios or intrinsic decomposition techniques, facing challenges with multi-view consistency, complex scenes, and diverse lighting conditions. Recent inverse rendering advancements, such as 3D Gaussian and diffusion-based methods, have enhanced consistency but are limited by scalability, heavy data requirements, or prolonged reconstruction time per scene. To broaden its applicability, we introduce MV-CoLight, a two-stage framework for illumination-consistent object compositing in both 2D images and 3D scenes. Our novel feed-forward architecture models lighting and shadows directly, avoiding the iterative biases of diffusion-based methods. We employ a Hilbert curve-based mapping to align 2D image inputs with 3D Gaussian scene representations seamlessly. To facilitate training and evaluation, we further introduce a large-scale 3D compositing dataset. Experiments demonstrate state-of-the-art harmonized results across standard benchmarks and our dataset, as well as casually captured real-world scenes demonstrate the framework's robustness and wide generalization.</li>
</ul>

<h3>Title: Be Decisive: Noise-Induced Layouts for Multi-Subject Generation</h3>
<ul>
<li><strong>Authors: </strong>Omer Dahary, Yehonathan Cohen, Or Patashnik, Kfir Aberman, Daniel Cohen-Or</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21488">https://arxiv.org/abs/2505.21488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21488">https://arxiv.org/pdf/2505.21488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21488]] Be Decisive: Noise-Induced Layouts for Multi-Subject Generation(https://arxiv.org/abs/2505.21488)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating multiple distinct subjects remains a challenge for existing text-to-image diffusion models. Complex prompts often lead to subject leakage, causing inaccuracies in quantities, attributes, and visual features. Preventing leakage among subjects necessitates knowledge of each subject's spatial location. Recent methods provide these spatial locations via an external layout control. However, enforcing such a prescribed layout often conflicts with the innate layout dictated by the sampled initial noise, leading to misalignment with the model's prior. In this work, we introduce a new approach that predicts a spatial layout aligned with the prompt, derived from the initial noise, and refines it throughout the denoising process. By relying on this noise-induced layout, we avoid conflicts with externally imposed layouts and better preserve the model's prior. Our method employs a small neural network to predict and refine the evolving noise-induced layout at each denoising step, ensuring clear boundaries between subjects while maintaining consistency. Experimental results show that this noise-aligned strategy achieves improved text-image alignment and more stable multi-subject generation compared to existing layout-guided techniques, while preserving the rich diversity of the model's original distribution.</li>
</ul>

<h3>Title: Frame In-N-Out: Unbounded Controllable Image-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Boyang Wang, Xuweiyi Chen, Matheus Gadelha, Zezhou Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21491">https://arxiv.org/abs/2505.21491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21491">https://arxiv.org/pdf/2505.21491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21491]] Frame In-N-Out: Unbounded Controllable Image-to-Video Generation(https://arxiv.org/abs/2505.21491)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can control the objects in the image to naturally leave the scene or provide breaking new identity references to enter the scene, guided by user-specified motion trajectory. To support this task, we introduce a new dataset curated semi-automatically, a comprehensive evaluation protocol targeting this setting, and an efficient identity-preserving motion-controllable video Diffusion Transformer architecture. Our evaluation shows that our proposed approach significantly outperforms existing baselines.</li>
</ul>

<h3>Title: Reinforcing General Reasoning without Verifiers</h3>
<ul>
<li><strong>Authors: </strong>Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, Chao Du</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21493">https://arxiv.org/abs/2505.21493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21493">https://arxiv.org/pdf/2505.21493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21493]] Reinforcing General Reasoning without Verifiers(https://arxiv.org/abs/2505.21493)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at this https URL.</li>
</ul>

<h3>Title: Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment</h3>
<ul>
<li><strong>Authors: </strong>Xiaojun Jia, Sensen Gao, Simeng Qin, Tianyu Pang, Chao Du, Yihao Huang, Xinfeng Li, Yiming Li, Bo Li, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21494">https://arxiv.org/abs/2505.21494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21494">https://arxiv.org/pdf/2505.21494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21494]] Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment(https://arxiv.org/abs/2505.21494)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information encoded in patch tokens. This leads to suboptimal alignment and limited transferability, particularly for closed-source models. To address this limitation, we propose a targeted transferable adversarial attack method based on feature optimal alignment, called FOA-Attack, to improve adversarial transfer capability. Specifically, at the global level, we introduce a global feature loss based on cosine similarity to align the coarse-grained features of adversarial samples with those of target samples. At the local level, given the rich local representations within Transformers, we leverage clustering techniques to extract compact local patterns to alleviate redundant local features. We then formulate local feature alignment between adversarial and target samples as an optimal transport (OT) problem and propose a local clustering optimal transport loss to refine fine-grained feature alignment. Additionally, we propose a dynamic ensemble model weighting strategy to adaptively balance the influence of multiple models during adversarial example generation, thereby further improving transferability. Extensive experiments across various models demonstrate the superiority of the proposed method, outperforming state-of-the-art methods, especially in transferring to closed-source MLLMs. The code is released at this https URL.</li>
</ul>

<h3>Title: AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery</h3>
<ul>
<li><strong>Authors: </strong>Haowei Wang, Junjie Wang, Xiaojun Jia, Rupeng Zhang, Mingyang Li, Zhe Liu, Yang Liu, Qing Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21499">https://arxiv.org/abs/2505.21499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21499">https://arxiv.org/pdf/2505.21499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21499]] AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery(https://arxiv.org/abs/2505.21499)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Vision-Language Model (VLM) based Web Agents represent a significant step towards automating complex tasks by simulating human-like interaction with websites. However, their deployment in uncontrolled web environments introduces significant security vulnerabilities. Existing research on adversarial environmental injection attacks often relies on unrealistic assumptions, such as direct HTML manipulation, knowledge of user intent, or access to agent model parameters, limiting their practical applicability. In this paper, we propose AdInject, a novel and real-world black-box attack method that leverages the internet advertising delivery to inject malicious content into the Web Agent's environment. AdInject operates under a significantly more realistic threat model than prior work, assuming a black-box agent, static malicious content constraints, and no specific knowledge of user intent. AdInject includes strategies for designing malicious ad content aimed at misleading agents into clicking, and a VLM-based ad content optimization technique that infers potential user intents from the target website's context and integrates these intents into the ad content to make it appear more relevant or critical to the agent's task, thus enhancing attack effectiveness. Experimental evaluations demonstrate the effectiveness of AdInject, attack success rates exceeding 60% in most scenarios and approaching 100% in certain cases. This strongly demonstrates that prevalent advertising delivery constitutes a potent and real-world vector for environment injection attacks against Web Agents. This work highlights a critical vulnerability in Web Agent security arising from real-world environment manipulation channels, underscoring the urgent need for developing robust defense mechanisms against such threats. Our code is available at this https URL.</li>
</ul>

<h3>Title: Vision Transformers with Self-Distilled Registers</h3>
<ul>
<li><strong>Authors: </strong>Yinjie Chen, Zipeng Yan, Chong Zhou, Bo Dai, Andrew F. Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21501">https://arxiv.org/abs/2505.21501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21501">https://arxiv.org/pdf/2505.21501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21501]] Vision Transformers with Self-Distilled Registers(https://arxiv.org/abs/2505.21501)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs) have emerged as the dominant architecture for visual processing tasks, demonstrating excellent scalability with increased training data and model size. However, recent work has identified the emergence of artifact tokens in ViTs that are incongruous with the local semantics. These anomalous tokens degrade ViT performance in tasks that require fine-grained localization or structural coherence. An effective mitigation of this issue is to the addition of register tokens to ViTs, which implicitly "absorb" the artifact term during training. Given the availability of various large-scale pre-trained ViTs, in this paper we aim at equipping them with such register tokens without the need of re-training them from scratch, which is infeasible considering their size. Specifically, we propose Post Hoc Registers (PH-Reg), an efficient self-distillation method that integrates registers into an existing ViT without requiring additional labeled data and full retraining. PH-Reg initializes both teacher and student networks from the same pre-trained ViT. The teacher remains frozen and unmodified, while the student is augmented with randomly initialized register tokens. By applying test-time augmentation to the teacher's inputs, we generate denoised dense embeddings free of artifacts, which are then used to optimize only a small subset of unlocked student weights. We show that our approach can effectively reduce the number of artifact tokens, improving the segmentation and depth prediction of the student ViT under zero-shot and linear probing.</li>
</ul>

<h3>Title: Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Yihan Wang, Qiao Yan, Zhenghao Xing, Lihao Liu, Junjun He, Chi-Wing Fu, Xiaowei Hu, Pheng-Ann Heng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, q-bio.OT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21503">https://arxiv.org/abs/2505.21503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21503">https://arxiv.org/pdf/2505.21503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21503]] Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making(https://arxiv.org/abs/2505.21503)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated strong potential in clinical question answering, with recent multi-agent frameworks further improving diagnostic accuracy via collaborative reasoning. However, we identify a recurring issue of Silent Agreement, where agents prematurely converge on diagnoses without sufficient critical analysis, particularly in complex or ambiguous cases. We present a new concept called Catfish Agent, a role-specialized LLM designed to inject structured dissent and counter silent agreement. Inspired by the ``catfish effect'' in organizational psychology, the Catfish Agent is designed to challenge emerging consensus to stimulate deeper reasoning. We formulate two mechanisms to encourage effective and context-aware interventions: (i) a complexity-aware intervention that modulates agent engagement based on case difficulty, and (ii) a tone-calibrated intervention articulated to balance critique and collaboration. Evaluations on nine medical Q&A and three medical VQA benchmarks show that our approach consistently outperforms both single- and multi-agent LLMs frameworks, including leading commercial models such as GPT-4o and DeepSeek-R1.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
