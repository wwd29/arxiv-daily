<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: MPCFormer: fast, performant and private Transformer inference with MPC. (arXiv:2211.01452v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01452">http://arxiv.org/abs/2211.01452</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01452] MPCFormer: fast, performant and private Transformer inference with MPC](http://arxiv.org/abs/2211.01452)</code></li>
<li>Summary: <p>Enabling private inference is crucial for many cloud inference services that
are based on Transformer models. However, existing private inference solutions
for Transformers can increase the inference latency by more than 60x or
significantly compromise the quality of inference results. In this paper, we
design the framework MPCFORMER using secure multi-party computation (MPC) and
Knowledge Distillation (KD). It can be used in tandem with many specifically
designed MPC-friendly approximations and trained Transformer models. MPCFORMER
significantly speeds up Transformer model inference in MPC settings while
achieving similar ML performance to the input model. We evaluate MPCFORMER with
various settings in MPC. On the IMDb dataset, we achieve similar performance to
BERTBASE, while being 5.3x faster. On the GLUE benchmark, we achieve 97%
performance of BERTBASE with a 2.2x speedup. We show that MPCFORMER remains
effective with different trained Transformer weights such as ROBERTABASE and
larger models including BERTLarge. In particular, we achieve similar
performance to BERTLARGE, while being 5.93x faster on the IMDb dataset.
</p></li>
</ul>

<h3>Title: GRAIMATTER Green Paper: Recommendations for disclosure control of trained Machine Learning (ML) models from Trusted Research Environments (TREs). (arXiv:2211.01656v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01656">http://arxiv.org/abs/2211.01656</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01656] GRAIMATTER Green Paper: Recommendations for disclosure control of trained Machine Learning (ML) models from Trusted Research Environments (TREs)](http://arxiv.org/abs/2211.01656)</code></li>
<li>Summary: <p>TREs are widely, and increasingly used to support statistical analysis of
sensitive data across a range of sectors (e.g., health, police, tax and
education) as they enable secure and transparent research whilst protecting
data confidentiality. There is an increasing desire from academia and industry
to train AI models in TREs. The field of AI is developing quickly with
applications including spotting human errors, streamlining processes, task
automation and decision support. These complex AI models require more
information to describe and reproduce, increasing the possibility that
sensitive personal data can be inferred from such descriptions. TREs do not
have mature processes and controls against these risks. This is a complex
topic, and it is unreasonable to expect all TREs to be aware of all risks or
that TRE researchers have addressed these risks in AI-specific training.
GRAIMATTER has developed a draft set of usable recommendations for TREs to
guard against the additional risks when disclosing trained AI models from TREs.
The development of these recommendations has been funded by the GRAIMATTER UKRI
DARE UK sprint research project. This version of our recommendations was
published at the end of the project in September 2022. During the course of the
project, we have identified many areas for future investigations to expand and
test these recommendations in practice. Therefore, we expect that this document
will evolve over time.
</p></li>
</ul>

<h3>Title: Single SMPC Invocation DPHelmet: Differentially Private Distributed Learning on a Large Scale. (arXiv:2211.02003v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02003">http://arxiv.org/abs/2211.02003</a></li>
<li>Code URL: <a href="https://github.com/kirschte/dphelmet">https://github.com/kirschte/dphelmet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02003] Single SMPC Invocation DPHelmet: Differentially Private Distributed Learning on a Large Scale](http://arxiv.org/abs/2211.02003)</code></li>
<li>Summary: <p>Distributing machine learning predictors enables the collection of
large-scale datasets while leaving sensitive raw data at trustworthy sites. We
show that locally training support vector machines (SVMs) and computing their
averages leads to a learning technique that is scalable to a large number of
users, satisfies differential privacy, and is applicable to non-trivial tasks,
such as CIFAR-10. For a large number of participants, communication cost is one
of the main challenges. We achieve a low communication cost by requiring only a
single invocation of an efficient secure multiparty summation protocol. By
relying on state-of-the-art feature extractors (SimCLR), we are able to utilize
differentially private convex learners for non-trivial tasks such as CIFAR-10.
Our experimental results illustrate that for $1{,}000$ users with $50$ data
points each, our scheme outperforms state-of-the-art scalable distributed
learning methods (differentially private federated learning, short DP-FL) while
requiring around $500$ times fewer communication costs: For CIFAR-10, we
achieve a classification accuracy of $79.7\,\%$ for an $\varepsilon = 0.59$
while DP-FL achieves $57.6\,\%$. More generally, we prove learnability
properties for the average of such locally trained models: convergence and
uniform stability. By only requiring strongly convex, smooth, and
Lipschitz-continuous objective functions, locally trained via stochastic
gradient descent (SGD), we achieve a strong utility-privacy tradeoff.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Expanding Accurate Person Recognition to New Altitudes and Ranges: The BRIAR Dataset. (arXiv:2211.01917v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01917">http://arxiv.org/abs/2211.01917</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01917] Expanding Accurate Person Recognition to New Altitudes and Ranges: The BRIAR Dataset](http://arxiv.org/abs/2211.01917)</code></li>
<li>Summary: <p>Face recognition technology has advanced significantly in recent years due
largely to the availability of large and increasingly complex training datasets
for use in deep learning models. These datasets, however, typically comprise
images scraped from news sites or social media platforms and, therefore, have
limited utility in more advanced security, forensics, and military
applications. These applications require lower resolution, longer ranges, and
elevated viewpoints. To meet these critical needs, we collected and curated the
first and second subsets of a large multi-modal biometric dataset designed for
use in the research and development (R&amp;D) of biometric recognition technologies
under extremely challenging conditions. Thus far, the dataset includes more
than 350,000 still images and over 1,300 hours of video footage of
approximately 1,000 subjects. To collect this data, we used Nikon DSLR cameras,
a variety of commercial surveillance cameras, specialized long-rage R&amp;D
cameras, and Group 1 and Group 2 UAV platforms. The goal is to support the
development of algorithms capable of accurately recognizing people at ranges up
to 1,000 m and from high angles of elevation. These advances will include
improvements to the state of the art in face recognition and will support new
research in the area of whole-body recognition using methods based on gait and
anthropometry. This paper describes methods used to collect and curate the
dataset, and the dataset's characteristics at the current stage.
</p></li>
</ul>

<h3>Title: Partially-Observable Security Games for Automating Attack-Defense Analysis. (arXiv:2211.01508v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01508">http://arxiv.org/abs/2211.01508</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01508] Partially-Observable Security Games for Automating Attack-Defense Analysis](http://arxiv.org/abs/2211.01508)</code></li>
<li>Summary: <p>Network systems often contain vulnerabilities that remain unfixed in a
network for various reasons, such as the lack of a patch or knowledge to fix
them. With the presence of such residual vulnerabilities, the network
administrator should properly react to the malicious activities or proactively
prevent them, by applying suitable countermeasures that minimize the likelihood
of an attack by the attacker. In this paper, we propose a stochastic
game-theoretic approach for analyzing network security and synthesizing defense
strategies to protect a network. To support analysis under partial observation,
where some of the attacker's activities are unobservable or undetectable by the
defender, we construct a one-sided partially observable security game and
transform it into a perfect game for further analysis. We prove that this
transformation is sound for a sub-class of security games and a subset of
properties specified in the logic rPATL. We implement a prototype that fully
automates our approach, and evaluate it by conducting experiments on a
real-life network.
</p></li>
</ul>

<h3>Title: AdaChain: A Learned Adaptive Blockchain. (arXiv:2211.01580v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01580">http://arxiv.org/abs/2211.01580</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01580] AdaChain: A Learned Adaptive Blockchain](http://arxiv.org/abs/2211.01580)</code></li>
<li>Summary: <p>This paper presents AdaChain, a learning-based blockchain framework that
adaptively chooses the best permissioned blockchain architecture in order to
optimize effective throughput for dynamic transaction workloads. AdaChain
addresses the challenge in the Blockchain-as-a-Service (BaaS) environments,
where a large variety of possible smart contracts are deployed with different
workload characteristics. AdaChain supports automatically adapting to an
underlying, dynamically changing workload through the use of reinforcement
learning. When a promising architecture is identified, AdaChain switches from
the current architecture to the promising one at runtime in a way that respects
correctness and security concerns. Experimentally, we show that AdaChain can
converge quickly to optimal architectures under changing workloads,
significantly outperform fixed architectures in terms of the number of
successfully committed transactions, all while incurring low additional
overhead.
</p></li>
</ul>

<h3>Title: Machine Learning Methods for Device Identification Using Wireless Fingerprinting. (arXiv:2211.01963v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01963">http://arxiv.org/abs/2211.01963</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01963] Machine Learning Methods for Device Identification Using Wireless Fingerprinting](http://arxiv.org/abs/2211.01963)</code></li>
<li>Summary: <p>Industrial Internet of Things (IoT) systems increasingly rely on wireless
communication standards. In a common industrial scenario, indoor wireless IoT
devices communicate with access points to deliver data collected from
industrial sensors, robots and factory machines. Due to static or quasi-static
locations of IoT devices and access points, historical observations of IoT
device channel conditions provide a possibility to precisely identify the
device without observing its traditional identifiers (e.g., MAC or IP address).
Such device identification methods based on wireless fingerprinting gained
increased attention lately as an additional cyber-security mechanism for
critical IoT infrastructures. In this paper, we perform a systematic study of a
large class of machine learning algorithms for device identification using
wireless fingerprints for the most popular cellular and Wi-Fi IoT technologies.
We design, implement, deploy, collect relevant data sets, train and test a
multitude of machine learning algorithms, as a part of the complete end-to-end
solution design for device identification via wireless fingerprinting. The
proposed solution is currently being deployed in a real-world industrial IoT
environment as part of H2020 project COLLABS.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: PromptEHR: Conditional Electronic Healthcare Records Generation with Prompt Learning. (arXiv:2211.01761v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01761">http://arxiv.org/abs/2211.01761</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01761] PromptEHR: Conditional Electronic Healthcare Records Generation with Prompt Learning](http://arxiv.org/abs/2211.01761)</code></li>
<li>Summary: <p>Accessing longitudinal multimodal Electronic Healthcare Records (EHRs) is
challenging due to privacy concerns, which hinders the use of ML for healthcare
applications. Synthetic EHRs generation bypasses the need to share sensitive
real patient records. However, existing methods generate single-modal EHRs by
unconditional generation or by longitudinal inference, which falls short of low
flexibility and makes unrealistic EHRs. In this work, we propose to formulate
EHRs generation as a text-to-text translation task by language models (LMs),
which suffices to highly flexible event imputation during generation. We also
design prompt learning to control the generation conditioned by numerical and
categorical demographic features. We evaluate synthetic EHRs quality by two
perplexity measures accounting for their longitudinal pattern (longitudinal
imputation perplexity, lpl) and the connections cross modalities
(cross-modality imputation perplexity, mpl). Moreover, we utilize two
adversaries: membership and attribute inference attacks for privacy-preserving
evaluation. Experiments on MIMIC-III data demonstrate the superiority of our
methods on realistic EHRs generation (53.1\% decrease of lpl and 45.3\%
decrease of mpl on average compared to the best baselines) with low privacy
risks. Software is available at https://github.com/RyanWangZf/PromptEHR.
</p></li>
</ul>

<h3>Title: Private Semi-supervised Knowledge Transfer for Deep Learning from Noisy Labels. (arXiv:2211.01628v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01628">http://arxiv.org/abs/2211.01628</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01628] Private Semi-supervised Knowledge Transfer for Deep Learning from Noisy Labels](http://arxiv.org/abs/2211.01628)</code></li>
<li>Summary: <p>Deep learning models trained on large-scale data have achieved encouraging
performance in many real-world tasks. Meanwhile, publishing those models
trained on sensitive datasets, such as medical records, could pose serious
privacy concerns. To counter these issues, one of the current state-of-the-art
approaches is the Private Aggregation of Teacher Ensembles, or PATE, which
achieved promising results in preserving the utility of the model while
providing a strong privacy guarantee. PATE combines an ensemble of "teacher
models" trained on sensitive data and transfers the knowledge to a "student"
model through the noisy aggregation of teachers' votes for labeling unlabeled
public data which the student model will be trained on. However, the knowledge
or voted labels learned by the student are noisy due to private aggregation.
Learning directly from noisy labels can significantly impact the accuracy of
the student model.
</p></li>
</ul>

<p>In this paper, we propose the PATE++ mechanism, which combines the current
advanced noisy label training mechanisms with the original PATE framework to
enhance its accuracy. A novel structure of Generative Adversarial Nets (GANs)
is developed in order to integrate them effectively. In addition, we develop a
novel noisy label detection mechanism for semi-supervised model training to
further improve student model performance when training with noisy labels. We
evaluate our method on Fashion-MNIST and SVHN to show the improvements on the
original PATE on all measures.
</p>

<h3>Title: Demo: LE3D: A Privacy-preserving Lightweight Data Drift Detection Framework. (arXiv:2211.01827v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01827">http://arxiv.org/abs/2211.01827</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01827] Demo: LE3D: A Privacy-preserving Lightweight Data Drift Detection Framework](http://arxiv.org/abs/2211.01827)</code></li>
<li>Summary: <p>This paper presents LE3D; a novel data drift detection framework for
preserving data integrity and confidentiality. LE3D is a generalisable platform
for evaluating novel drift detection mechanisms within the Internet of Things
(IoT) sensor deployments. Our framework operates in a distributed manner,
preserving data privacy while still being adaptable to new sensors with minimal
online reconfiguration. Our framework currently supports multiple drift
estimators for time-series IoT data and can easily be extended to accommodate
new data types and drift detection mechanisms. This demo will illustrate the
functionality of LE3D under a real-world-like scenario.
</p></li>
</ul>

<h3>Title: Revisiting Hyperparameter Tuning with Differential Privacy. (arXiv:2211.01852v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01852">http://arxiv.org/abs/2211.01852</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01852] Revisiting Hyperparameter Tuning with Differential Privacy](http://arxiv.org/abs/2211.01852)</code></li>
<li>Summary: <p>Hyperparameter tuning is a common practice in the application of machine
learning but is a typically ignored aspect in the literature on
privacy-preserving machine learning due to its negative effect on the overall
privacy parameter. In this paper, we aim to tackle this fundamental yet
challenging problem by providing an effective hyperparameter tuning framework
with differential privacy. The proposed method allows us to adopt a broader
hyperparameter search space and even to perform a grid search over the whole
space, since its privacy loss parameter is independent of the number of
hyperparameter candidates. Interestingly, it instead correlates with the
utility gained from hyperparameter searching, revealing an explicit and
mandatory trade-off between privacy and utility. Theoretically, we show that
its additional privacy loss bound incurred by hyperparameter tuning is
upper-bounded by the squared root of the gained utility. However, we note that
the additional privacy loss bound would empirically scale like a squared root
of the logarithm of the utility term, benefiting from the design of doubling
step.
</p></li>
</ul>

<h3>Title: FUNCK: Information Funnels and Bottlenecks for Invariant Representation Learning. (arXiv:2211.01446v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01446">http://arxiv.org/abs/2211.01446</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01446] FUNCK: Information Funnels and Bottlenecks for Invariant Representation Learning](http://arxiv.org/abs/2211.01446)</code></li>
<li>Summary: <p>Learning invariant representations that remain useful for a downstream task
is still a key challenge in machine learning. We investigate a set of related
information funnels and bottleneck problems that claim to learn invariant
representations from the data. We also propose a new element to this family of
information-theoretic objectives: The Conditional Privacy Funnel with Side
Information, which we investigate in fully and semi-supervised settings. Given
the generally intractable objectives, we derive tractable approximations using
amortized variational inference parameterized by neural networks and study the
intrinsic trade-offs of these objectives. We describe empirically the proposed
approach and show that with a few labels it is possible to learn fair
classifiers and generate useful representations approximately invariant to
unwanted sources of variation. Furthermore, we provide insights about the
applicability of these methods in real-world scenarios with ordinary tabular
datasets when the data is scarce.
</p></li>
</ul>

<h3>Title: Privacy-preserving Non-negative Matrix Factorization with Outliers. (arXiv:2211.01451v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01451">http://arxiv.org/abs/2211.01451</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01451] Privacy-preserving Non-negative Matrix Factorization with Outliers](http://arxiv.org/abs/2211.01451)</code></li>
<li>Summary: <p>Non-negative matrix factorization is a popular unsupervised machine learning
algorithm for extracting meaningful features from data which are inherently
non-negative. However, such data sets may often contain privacy-sensitive user
data, and therefore, we may need to take necessary steps to ensure the privacy
of the users while analyzing the data. In this work, we focus on developing a
Non-negative matrix factorization algorithm in the privacy-preserving
framework. More specifically, we propose a novel privacy-preserving algorithm
for non-negative matrix factorisation capable of operating on private data,
while achieving results comparable to those of the non-private algorithm. We
design the framework such that one has the control to select the degree of
privacy grantee based on the utility gap. We show our proposed framework's
performance in six real data sets. The experimental results show that our
proposed method can achieve very close performance with the non-private
algorithm under some parameter regime, while ensuring strict privacy.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Liability regimes in the age of AI: a use-case driven analysis of the burden of proof. (arXiv:2211.01817v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01817">http://arxiv.org/abs/2211.01817</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01817] Liability regimes in the age of AI: a use-case driven analysis of the burden of proof](http://arxiv.org/abs/2211.01817)</code></li>
<li>Summary: <p>New emerging technologies powered by Artificial Intelligence (AI) have the
potential to disruptively transform our societies for the better. In
particular, data-driven learning approaches (i.e., Machine Learning (ML)) have
been a true revolution in the advancement of multiple technologies in various
application domains. But at the same time there is growing concerns about
certain intrinsic characteristics of these methodologies that carry potential
risks to both safety and fundamental rights. Although there are mechanisms in
the adoption process to minimize these risks (e.g., safety regulations), these
do not exclude the possibility of harm occurring, and if this happens, victims
should be able to seek compensation. Liability regimes will therefore play a
key role in ensuring basic protection for victims using or interacting with
these systems. However, the same characteristics that make AI systems
inherently risky, such as lack of causality, opacity, unpredictability or their
self and continuous learning capabilities, lead to considerable difficulties
when it comes to proving causation. This paper presents three case studies, as
well as the methodology to reach them, that illustrate these difficulties.
Specifically, we address the cases of cleaning robots, delivery drones and
robots in education. The outcome of the proposed analysis suggests the need to
revise liability regimes to alleviate the burden of proof on victims in cases
involving AI technologies.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Data-free Defense of Black Box Models Against Adversarial Attacks. (arXiv:2211.01579v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01579">http://arxiv.org/abs/2211.01579</a></li>
<li>Code URL: <a href="https://github.com/vcl-iisc/data-free-black-box-defense">https://github.com/vcl-iisc/data-free-black-box-defense</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01579] Data-free Defense of Black Box Models Against Adversarial Attacks](http://arxiv.org/abs/2211.01579)</code></li>
<li>Summary: <p>Several companies often safeguard their trained deep models (i.e. details of
architecture, learnt weights, training details etc.) from third-party users by
exposing them only as black boxes through APIs. Moreover, they may not even
provide access to the training data due to proprietary reasons or sensitivity
concerns. We make the first attempt to provide adversarial robustness to the
black box models in a data-free set up. We construct synthetic data via
generative model and train surrogate network using model stealing techniques.
To minimize adversarial contamination on perturbed samples, we propose <code>wavelet
noise remover' (WNR) that performs discrete wavelet decomposition on input
images and carefully select only a few important coefficients determined by our</code>wavelet coefficient selection module' (WCSM). To recover the high-frequency
content of the image after noise removal via WNR, we further train a
`regenerator' network with an objective to retrieve the coefficients such that
the reconstructed image yields similar to original predictions on the surrogate
model. At test time, WNR combined with trained regenerator network is prepended
to the black box network, resulting in a high boost in adversarial accuracy.
Our method improves the adversarial accuracy on CIFAR-10 by 38.98% and 32.01%
on state-of-the-art Auto Attack compared to baseline, even when the attacker
uses surrogate architecture (Alexnet-half and Alexnet) similar to the black box
architecture (Alexnet) with same model stealing strategy as defender. The code
is available at https://github.com/vcl-iisc/data-free-black-box-defense
</p></li>
</ul>

<h3>Title: Physically Adversarial Attacks and Defenses in Computer Vision: A Survey. (arXiv:2211.01671v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01671">http://arxiv.org/abs/2211.01671</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01671] Physically Adversarial Attacks and Defenses in Computer Vision: A Survey](http://arxiv.org/abs/2211.01671)</code></li>
<li>Summary: <p>Although Deep Neural Networks (DNNs) have been widely applied in various
real-world scenarios, they are vulnerable to adversarial examples. The current
adversarial attacks in computer vision can be divided into digital attacks and
physical attacks according to their different attack forms. Compared with
digital attacks, which generate perturbations in the digital pixels, physical
attacks are more practical in the real world. Owing to the serious security
problem caused by physically adversarial examples, many works have been
proposed to evaluate the physically adversarial robustness of DNNs in the past
years. In this paper, we summarize a survey versus the current physically
adversarial attacks and physically adversarial defenses in computer vision. To
establish a taxonomy, we organize the current physical attacks from attack
tasks, attack forms, and attack methods, respectively. Thus, readers can have a
systematic knowledge about this topic from different aspects. For the physical
defenses, we establish the taxonomy from pre-processing, in-processing, and
post-processing for the DNN models to achieve a full coverage of the
adversarial defenses. Based on the above survey, we finally discuss the
challenges of this research field and further outlook the future direction.
</p></li>
</ul>

<h3>Title: Try to Avoid Attacks: A Federated Data Sanitization Defense for Healthcare IoMT Systems. (arXiv:2211.01592v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01592">http://arxiv.org/abs/2211.01592</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01592] Try to Avoid Attacks: A Federated Data Sanitization Defense for Healthcare IoMT Systems](http://arxiv.org/abs/2211.01592)</code></li>
<li>Summary: <p>Healthcare IoMT systems are becoming intelligent, miniaturized, and more
integrated into daily life. As for the distributed devices in the IoMT,
federated learning has become a topical area with cloud-based training
procedures when meeting data security. However, the distribution of IoMT has
the risk of protection from data poisoning attacks. Poisoned data can be
fabricated by falsifying medical data, which urges a security defense to IoMT
systems. Due to the lack of specific labels, the filtering of malicious data is
a unique unsupervised scenario. One of the main challenges is finding robust
data filtering methods for various poisoning attacks. This paper introduces a
Federated Data Sanitization Defense, a novel approach to protect the system
from data poisoning attacks. To solve this unsupervised problem, we first use
federated learning to project all the data to the subspace domain, allowing
unified feature mapping to be established since the data is stored locally.
Then we adopt the federated clustering to re-group their features to clarify
the poisoned data. The clustering is based on the consistent association of
data and its semantics. After we get the clustering of the private data, we do
the data sanitization with a simple yet efficient strategy. In the end, each
device of distributed ImOT is enabled to filter malicious data according to
federated data sanitization. Extensive experiments are conducted to evaluate
the efficacy of the proposed defense method against data poisoning attacks.
Further, we consider our approach in the different poisoning ratios and achieve
a high Accuracy and a low attack success rate.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: BATT: Backdoor Attack with Transformation-based Triggers. (arXiv:2211.01806v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01806">http://arxiv.org/abs/2211.01806</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01806] BATT: Backdoor Attack with Transformation-based Triggers](http://arxiv.org/abs/2211.01806)</code></li>
<li>Summary: <p>Deep neural networks (DNNs) are vulnerable to backdoor attacks. The backdoor
adversaries intend to maliciously control the predictions of attacked DNNs by
injecting hidden backdoors that can be activated by adversary-specified trigger
patterns during the training process. One recent research revealed that most of
the existing attacks failed in the real physical world since the trigger
contained in the digitized test samples may be different from that of the one
used for training. Accordingly, users can adopt spatial transformations as the
image pre-processing to deactivate hidden backdoors. In this paper, we explore
the previous findings from another side. We exploit classical spatial
transformations (i.e. rotation and translation) with the specific parameter as
trigger patterns to design a simple yet effective poisoning-based backdoor
attack. For example, only images rotated to a particular angle can activate the
embedded backdoor of attacked DNNs. Extensive experiments are conducted,
verifying the effectiveness of our attack under both digital and physical
settings and its resistance to existing backdoor defenses.
</p></li>
</ul>

<h3>Title: Looking Beyond IoCs: Automatically Extracting Attack Patterns from External CTI. (arXiv:2211.01753v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01753">http://arxiv.org/abs/2211.01753</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01753] Looking Beyond IoCs: Automatically Extracting Attack Patterns from External CTI](http://arxiv.org/abs/2211.01753)</code></li>
<li>Summary: <p>Public and commercial companies extensively share cyber threat intelligence
(CTI) to prepare systems to defend against emerging cyberattacks. Most used
intelligence thus far has been limited to tracking known threat indicators such
as IP addresses and domain names as they are easier to extract using regular
expressions. Due to the limited long-term usage and difficulty of performing a
long-term analysis on indicators, we propose using significantly more robust
threat intelligence signals called attack patterns. However, extracting attack
patterns at scale is a challenging task. In this paper, we present LADDER, a
knowledge extraction framework that can extract text-based attack patterns from
CTI reports at scale. The model characterizes attack patterns by capturing
phases of an attack in android and enterprise networks. It then systematically
maps them to the MITRE ATT\&amp;CK pattern framework. We present several use cases
to demonstrate the application of LADDER for SOC analysts in determining the
presence of attack vectors belonging to emerging attacks in preparation for
defenses in advance.
</p></li>
</ul>

<h3>Title: Dormant Neural Trojans. (arXiv:2211.01808v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01808">http://arxiv.org/abs/2211.01808</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01808] Dormant Neural Trojans](http://arxiv.org/abs/2211.01808)</code></li>
<li>Summary: <p>We present a novel methodology for neural network backdoor attacks. Unlike
existing training-time attacks where the Trojaned network would respond to the
Trojan trigger after training, our approach inserts a Trojan that will remain
dormant until it is activated. The activation is realized through a specific
perturbation to the network's weight parameters only known to the attacker. Our
analysis and the experimental results demonstrate that dormant Trojaned
networks can effectively evade detection by state-of-the-art backdoor detection
methods.
</p></li>
</ul>

<h3>Title: Reinforcement Learning based Cyberattack Model for Adaptive Traffic Signal Controller in Connected Transportation Systems. (arXiv:2211.01845v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01845">http://arxiv.org/abs/2211.01845</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01845] Reinforcement Learning based Cyberattack Model for Adaptive Traffic Signal Controller in Connected Transportation Systems](http://arxiv.org/abs/2211.01845)</code></li>
<li>Summary: <p>In a connected transportation system, adaptive traffic signal controllers
(ATSC) utilize real-time vehicle trajectory data received from vehicles through
wireless connectivity (i.e., connected vehicles) to regulate green time.
However, this wirelessly connected ATSC increases cyber-attack surfaces and
increases their vulnerability to various cyber-attack modes, which can be
leveraged to induce significant congestion in a roadway network. An attacker
may receive financial benefits to create such a congestion for a specific
roadway. One such mode is a 'sybil' attack in which an attacker creates fake
vehicles in the network by generating fake Basic Safety Messages (BSMs)
imitating actual connected vehicles following roadway traffic rules. The
ultimate goal of an attacker will be to block a route(s) by generating fake or
'sybil' vehicles at a rate such that the signal timing and phasing changes
occur without flagging any abrupt change in number of vehicles. Because of the
highly non-linear and unpredictable nature of vehicle arrival rates and the
ATSC algorithm, it is difficult to find an optimal rate of sybil vehicles,
which will be injected from different approaches of an intersection. Thus, it
is necessary to develop an intelligent cyber-attack model to prove the
existence of such attacks. In this study, a reinforcement learning based
cyber-attack model is developed for a waiting time-based ATSC. Specifically, an
RL agent is trained to learn an optimal rate of sybil vehicle injection to
create congestion for an approach(s). Our analyses revealed that the RL agent
can learn an optimal policy for creating an intelligent attack.
</p></li>
</ul>

<h3>Title: M-to-N Backdoor Paradigm: A Stealthy and Fuzzy Attack to Deep Learning Models. (arXiv:2211.01875v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01875">http://arxiv.org/abs/2211.01875</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01875] M-to-N Backdoor Paradigm: A Stealthy and Fuzzy Attack to Deep Learning Models](http://arxiv.org/abs/2211.01875)</code></li>
<li>Summary: <p>Recent studies show that deep neural networks (DNNs) are vulnerable to
backdoor attacks. A backdoor DNN model behaves normally with clean inputs,
whereas outputs attacker's expected behaviors when the inputs contain a
pre-defined pattern called a trigger. However, in some tasks, the attacker
cannot know the exact target that shows his/her expected behavior, because the
task may contain a large number of classes and the attacker does not have full
access to know the semantic details of these classes. Thus, the attacker is
willing to attack multiple suspected targets to achieve his/her purpose. In
light of this, in this paper, we propose the M-to-N backdoor attack, a new
attack paradigm that allows an attacker to launch a fuzzy attack by
simultaneously attacking N suspected targets, and each of the N targets can be
activated by any one of its M triggers. To achieve a better stealthiness, we
randomly select M clean images from the training dataset as our triggers for
each target. Since the triggers used in our attack have the same distribution
as the clean images, the inputs poisoned by the triggers are difficult to be
detected by the input-based defenses, and the backdoor models trained on the
poisoned training dataset are also difficult to be detected by the model-based
defenses. Thus, our attack is stealthier and has a higher probability of
achieving the attack purpose by attacking multiple suspected targets
simultaneously in contrast to prior backdoor attacks. Extensive experiments
show that our attack is effective against different datasets with various
models and achieves high attack success rates (e.g., 99.43% for attacking 2
targets and 98.23% for attacking 4 targets on the CIFAR-10 dataset) when
poisoning only an extremely small portion of the training dataset (e.g., less
than 2%). Besides, it is robust to pre-processing operations and can resist
state-of-the-art defenses.
</p></li>
</ul>

<h3>Title: Manipulation of individual judgments in the quantitative pairwise comparisons method. (arXiv:2211.01809v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01809">http://arxiv.org/abs/2211.01809</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01809] Manipulation of individual judgments in the quantitative pairwise comparisons method](http://arxiv.org/abs/2211.01809)</code></li>
<li>Summary: <p>Decision-making methods very often use the technique of comparing
alternatives in pairs. In this approach, experts are asked to compare different
options, and then a quantitative ranking is created from the results obtained.
It is commonly believed that experts (decision-makers) are honest in their
judgments. In our work, we consider a scenario in which experts are vulnerable
to bribery. For this purpose, we define a framework that allows us to determine
the intended manipulation and present three algorithms for achieving the
intended goal. Analyzing these algorithms may provide clues to help defend
against such attacks.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Optimizing Fiducial Marker Placement for Improved Visual Localization. (arXiv:2211.01513v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01513">http://arxiv.org/abs/2211.01513</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01513] Optimizing Fiducial Marker Placement for Improved Visual Localization](http://arxiv.org/abs/2211.01513)</code></li>
<li>Summary: <p>Adding fiducial markers to a scene is a well-known strategy for making visual
localization algorithms more robust. Traditionally, these marker locations are
selected by humans who are familiar with visual localization techniques. This
paper explores the problem of automatic marker placement within a scene.
Specifically, given a predetermined set of markers and a scene model, we
compute optimized marker positions within the scene that can improve accuracy
in visual localization. Our main contribution is a novel framework for modeling
camera localizability that incorporates both natural scene features and
artificial fiducial markers added to the scene. We present optimized marker
placement (OMP), a greedy algorithm that is based on the camera localizability
framework. We have also designed a simulation framework for testing marker
placement algorithms on 3D models and images generated from synthetic scenes.
We have evaluated OMP within this testbed and demonstrate an improvement in the
localization rate by up to 20 percent on three different scenes.
</p></li>
</ul>

<h3>Title: Sensor Control for Information Gain in Dynamic, Sparse and Partially Observed Environments. (arXiv:2211.01527v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01527">http://arxiv.org/abs/2211.01527</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01527] Sensor Control for Information Gain in Dynamic, Sparse and Partially Observed Environments](http://arxiv.org/abs/2211.01527)</code></li>
<li>Summary: <p>We present an approach for autonomous sensor control for information
gathering under partially observable, dynamic and sparsely sampled
environments. We consider the problem of controlling a sensor that makes
partial observations in some space of interest such that it maximizes
information about entities present in that space. We describe our approach for
the task of Radio-Frequency (RF) spectrum monitoring, where the goal is to
search for and track unknown, dynamic signals in the environment. To this end,
we develop and demonstrate enhancements of the Deep Anticipatory Network (DAN)
Reinforcement Learning (RL) framework that uses prediction and information-gain
rewards to learn information-maximization policies in reward-sparse
environments. We also extend this problem to situations in which taking samples
from the actual RF spectrum/field is limited and expensive, and propose a
model-based version of the original RL algorithm that fine-tunes the controller
using a model of the environment that is iteratively improved from limited
samples taken from the RF field. Our approach was thoroughly validated by
testing against baseline expert-designed controllers in simulated RF
environments of different complexity, using different rewards schemes and
evaluation metrics. The results show that our system outperforms the standard
DAN architecture and is more flexible and robust than several hand-coded
agents. We also show that our approach is adaptable to non-stationary
environments where the agent has to learn to adapt to changes from the emitting
sources.
</p></li>
</ul>

<h3>Title: Robust Few-shot Learning Without Using any Adversarial Samples. (arXiv:2211.01598v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01598">http://arxiv.org/abs/2211.01598</a></li>
<li>Code URL: <a href="https://github.com/vcl-iisc/robust-few-shot-learning">https://github.com/vcl-iisc/robust-few-shot-learning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01598] Robust Few-shot Learning Without Using any Adversarial Samples](http://arxiv.org/abs/2211.01598)</code></li>
<li>Summary: <p>The high cost of acquiring and annotating samples has made the `few-shot'
learning problem of prime importance. Existing works mainly focus on improving
performance on clean data and overlook robustness concerns on the data
perturbed with adversarial noise. Recently, a few efforts have been made to
combine the few-shot problem with the robustness objective using sophisticated
Meta-Learning techniques. These methods rely on the generation of adversarial
samples in every episode of training, which further adds a computational
burden. To avoid such time-consuming and complicated procedures, we propose a
simple but effective alternative that does not require any adversarial samples.
Inspired by the cognitive decision-making process in humans, we enforce
high-level feature matching between the base class data and their corresponding
low-frequency samples in the pretraining stage via self distillation. The model
is then fine-tuned on the samples of novel classes where we additionally
improve the discriminability of low-frequency query set features via cosine
similarity. On a 1-shot setting of the CIFAR-FS dataset, our method yields a
massive improvement of $60.55\%$ &amp; $62.05\%$ in adversarial accuracy on the PGD
and state-of-the-art Auto Attack, respectively, with a minor drop in clean
accuracy compared to the baseline. Moreover, our method only takes $1.69\times$
of the standard training time while being $\approx$ $5\times$ faster than
state-of-the-art adversarial meta-learning methods. The code is available at
https://github.com/vcl-iisc/robust-few-shot-learning.
</p></li>
</ul>

<h3>Title: nerf2nerf: Pairwise Registration of Neural Radiance Fields. (arXiv:2211.01600v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01600">http://arxiv.org/abs/2211.01600</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01600] nerf2nerf: Pairwise Registration of Neural Radiance Fields](http://arxiv.org/abs/2211.01600)</code></li>
<li>Summary: <p>We introduce a technique for pairwise registration of neural fields that
extends classical optimization-based local registration (i.e. ICP) to operate
on Neural Radiance Fields (NeRF) -- neural 3D scene representations trained
from collections of calibrated images. NeRF does not decompose illumination and
color, so to make registration invariant to illumination, we introduce the
concept of a ''surface field'' -- a field distilled from a pre-trained NeRF
model that measures the likelihood of a point being on the surface of an
object. We then cast nerf2nerf registration as a robust optimization that
iteratively seeks a rigid transformation that aligns the surface fields of the
two scenes. We evaluate the effectiveness of our technique by introducing a
dataset of pre-trained NeRF scenes -- our synthetic scenes enable quantitative
evaluations and comparisons to classical registration techniques, while our
real scenes demonstrate the validity of our technique in real-world scenarios.
Additional results available at: https://nerf2nerf.github.io
</p></li>
</ul>

<h3>Title: Fast Noise Removal in Hyperspectral Images via Representative Coefficient Total Variation. (arXiv:2211.01825v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01825">http://arxiv.org/abs/2211.01825</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01825] Fast Noise Removal in Hyperspectral Images via Representative Coefficient Total Variation](http://arxiv.org/abs/2211.01825)</code></li>
<li>Summary: <p>Mining structural priors in data is a widely recognized technique for
hyperspectral image (HSI) denoising tasks, whose typical ways include
model-based methods and data-based methods. The model-based methods have good
generalization ability, while the runtime cannot meet the fast processing
requirements of the practical situations due to the large size of an HSI data $
\mathbf{X} \in \mathbb{R}^{MN\times B}$. For the data-based methods, they
perform very fast on new test data once they have been trained. However, their
generalization ability is always insufficient. In this paper, we propose a fast
model-based HSI denoising approach. Specifically, we propose a novel
regularizer named Representative Coefficient Total Variation (RCTV) to
simultaneously characterize the low rank and local smooth properties. The RCTV
regularizer is proposed based on the observation that the representative
coefficient matrix $\mathbf{U}\in\mathbb{R}^{MN\times R} (R\ll B)$ obtained by
orthogonally transforming the original HSI $\mathbf{X}$ can inherit the strong
local-smooth prior of $\mathbf{X}$. Since $R/B$ is very small, the HSI
denoising model based on the RCTV regularizer has lower time complexity.
Additionally, we find that the representative coefficient matrix $\mathbf{U}$
is robust to noise, and thus the RCTV regularizer can somewhat promote the
robustness of the HSI denoising model. Extensive experiments on mixed noise
removal demonstrate the superiority of the proposed method both in denoising
performance and denoising speed compared with other state-of-the-art methods.
Remarkably, the denoising speed of our proposed method outperforms all the
model-based techniques and is comparable with the deep learning-based
approaches.
</p></li>
</ul>

<h3>Title: ImageNet-X: Understanding Model Mistakes with Factor of Variation Annotations. (arXiv:2211.01866v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01866">http://arxiv.org/abs/2211.01866</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01866] ImageNet-X: Understanding Model Mistakes with Factor of Variation Annotations](http://arxiv.org/abs/2211.01866)</code></li>
<li>Summary: <p>Deep learning vision systems are widely deployed across applications where
reliability is critical. However, even today's best models can fail to
recognize an object when its pose, lighting, or background varies. While
existing benchmarks surface examples challenging for models, they do not
explain why such mistakes arise. To address this need, we introduce ImageNet-X,
a set of sixteen human annotations of factors such as pose, background, or
lighting the entire ImageNet-1k validation set as well as a random subset of
12k training images. Equipped with ImageNet-X, we investigate 2,200 current
recognition models and study the types of mistakes as a function of model's (1)
architecture, e.g. transformer vs. convolutional, (2) learning paradigm, e.g.
supervised vs. self-supervised, and (3) training procedures, e.g., data
augmentation. Regardless of these choices, we find models have consistent
failure modes across ImageNet-X categories. We also find that while data
augmentation can improve robustness to certain factors, they induce spill-over
effects to other factors. For example, strong random cropping hurts robustness
on smaller objects. Together, these insights suggest to advance the robustness
of modern vision models, future research should focus on collecting additional
data and understanding data augmentation schemes. Along with these insights, we
release a toolkit based on ImageNet-X to spur further study into the mistakes
image recognition systems make.
</p></li>
</ul>

<h3>Title: Analysing the effectiveness of a generative model for semi-supervised medical image segmentation. (arXiv:2211.01886v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01886">http://arxiv.org/abs/2211.01886</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01886] Analysing the effectiveness of a generative model for semi-supervised medical image segmentation](http://arxiv.org/abs/2211.01886)</code></li>
<li>Summary: <p>Image segmentation is important in medical imaging, providing valuable,
quantitative information for clinical decision-making in diagnosis, therapy,
and intervention. The state-of-the-art in automated segmentation remains
supervised learning, employing discriminative models such as U-Net. However,
training these models requires access to large amounts of manually labelled
data which is often difficult to obtain in real medical applications. In such
settings, semi-supervised learning (SSL) attempts to leverage the abundance of
unlabelled data to obtain more robust and reliable models. Recently, generative
models have been proposed for semantic segmentation, as they make an attractive
choice for SSL. Their ability to capture the joint distribution over input
images and output label maps provides a natural way to incorporate information
from unlabelled images. This paper analyses whether deep generative models such
as the SemanticGAN are truly viable alternatives to tackle challenging medical
image segmentation problems. To that end, we thoroughly evaluate the
segmentation performance, robustness, and potential subgroup disparities of
discriminative and generative segmentation methods when applied to large-scale,
publicly available chest X-ray datasets.
</p></li>
</ul>

<h3>Title: MarginNCE: Robust Sound Localization with a Negative Margin. (arXiv:2211.01966v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01966">http://arxiv.org/abs/2211.01966</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01966] MarginNCE: Robust Sound Localization with a Negative Margin](http://arxiv.org/abs/2211.01966)</code></li>
<li>Summary: <p>The goal of this work is to localize sound sources in visual scenes with a
self-supervised approach. Contrastive learning in the context of sound source
localization leverages the natural correspondence between audio and visual
signals where the audio-visual pairs from the same source are assumed as
positive, while randomly selected pairs are negatives. However, this approach
brings in noisy correspondences; for example, positive audio and visual pair
signals that may be unrelated to each other, or negative pairs that may contain
semantically similar samples to the positive one. Our key contribution in this
work is to show that using a less strict decision boundary in contrastive
learning can alleviate the effect of noisy correspondences in sound source
localization. We propose a simple yet effective approach by slightly modifying
the contrastive loss with a negative margin. Extensive experimental results
show that our approach gives on-par or better performance than the
state-of-the-art methods. Furthermore, we demonstrate that the introduction of
a negative margin to existing methods results in a consistent improvement in
performance.
</p></li>
</ul>

<h3>Title: RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question. (arXiv:2211.01482v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01482">http://arxiv.org/abs/2211.01482</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01482] RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question](http://arxiv.org/abs/2211.01482)</code></li>
<li>Summary: <p>Existing metrics for evaluating the quality of automatically generated
questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and
predicted questions, providing a high score when there is a considerable
lexical overlap or semantic similarity between the candidate and the reference
questions. This approach has two major shortcomings. First, we need expensive
human-provided reference questions. Second, it penalises valid questions that
may not have high lexical or semantic similarity to the reference questions. In
this paper, we propose a new metric, RQUGE, based on the answerability of the
candidate question given the context. The metric consists of a
question-answering and a span scorer module, in which we use pre-trained models
from the existing literature, and therefore, our metric can be used without
further training. We show that RQUGE has a higher correlation with human
judgment without relying on the reference question. RQUGE is shown to be
significantly more robust to several adversarial corruptions. Additionally, we
illustrate that we can significantly improve the performance of QA models on
out-of-domain datasets by fine-tuning on the synthetic data generated by a
question generation model and re-ranked by RQUGE.
</p></li>
</ul>

<h3>Title: Revisiting Grammatical Error Correction Evaluation and Beyond. (arXiv:2211.01635v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01635">http://arxiv.org/abs/2211.01635</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01635] Revisiting Grammatical Error Correction Evaluation and Beyond](http://arxiv.org/abs/2211.01635)</code></li>
<li>Summary: <p>Pretraining-based (PT-based) automatic evaluation metrics (e.g., BERTScore
and BARTScore) have been widely used in several sentence generation tasks
(e.g., machine translation and text summarization) due to their better
correlation with human judgments over traditional overlap-based methods.
Although PT-based methods have become the de facto standard for training
grammatical error correction (GEC) systems, GEC evaluation still does not
benefit from pretrained knowledge. This paper takes the first step towards
understanding and improving GEC evaluation with pretraining. We first find that
arbitrarily applying PT-based metrics to GEC evaluation brings unsatisfactory
correlation results because of the excessive attention to inessential systems
outputs (e.g., unchanged parts). To alleviate the limitation, we propose a
novel GEC evaluation metric to achieve the best of both worlds, namely PT-M2
which only uses PT-based metrics to score those corrected parts. Experimental
results on the CoNLL14 evaluation task show that PT-M2 significantly
outperforms existing methods, achieving a new state-of-the-art result of 0.949
Pearson correlation. Further analysis reveals that PT-M2 is robust to evaluate
competitive GEC systems. Source code and scripts are freely available at
https://github.com/pygongnlp/PT-M2.
</p></li>
</ul>

<h3>Title: Spam Review Detection Using Deep Learning. (arXiv:2211.01675v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01675">http://arxiv.org/abs/2211.01675</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01675] Spam Review Detection Using Deep Learning](http://arxiv.org/abs/2211.01675)</code></li>
<li>Summary: <p>A robust and reliable system of detecting spam reviews is a crying need in
todays world in order to purchase products without being cheated from online
sites. In many online sites, there are options for posting reviews, and thus
creating scopes for fake paid reviews or untruthful reviews. These concocted
reviews can mislead the general public and put them in a perplexity whether to
believe the review or not. Prominent machine learning techniques have been
introduced to solve the problem of spam review detection. The majority of
current research has concentrated on supervised learning methods, which require
labeled data - an inadequacy when it comes to online review. Our focus in this
article is to detect any deceptive text reviews. In order to achieve that we
have worked with both labeled and unlabeled data and proposed deep learning
methods for spam review detection which includes Multi-Layer Perceptron (MLP),
Convolutional Neural Network (CNN) and a variant of Recurrent Neural Network
(RNN) that is Long Short-Term Memory (LSTM). We have also applied some
traditional machine learning classifiers such as Nave Bayes (NB), K Nearest
Neighbor (KNN) and Support Vector Machine (SVM) to detect spam reviews and
finally, we have shown the performance comparison for both traditional and deep
learning classifiers.
</p></li>
</ul>

<h3>Title: Efficiently Trained Mongolian Text-to-Speech System Based On FullConv. (arXiv:2211.01948v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01948">http://arxiv.org/abs/2211.01948</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01948] Efficiently Trained Mongolian Text-to-Speech System Based On FullConv](http://arxiv.org/abs/2211.01948)</code></li>
<li>Summary: <p>Recurrent Neural Networks (RNNs) have become the standard modeling technique
for sequence data, and are used in a number of novel text-to-speech models.
However, training a TTS model including RNN components has certain requirements
for GPU performance and takes a long time. In contrast, studies have shown that
CNN-based sequence synthesis technology can greatly reduce training time in
text-to-speech models while ensuring a certain performance due to its high
parallelism. We propose a new text-to-speech system based on deep convolutional
neural networks that does not employ any RNN components (recurrent units). At
the same time, we improve the generality and robustness of our model through a
series of data augmentation methods such as Time Warping, Frequency Mask, and
Time Mask. The final experimental results show that the TTS model using only
the CNN component can reduce the training time compared to the classic TTS
models such as Tacotron while ensuring the quality of the synthesized speech.
</p></li>
</ul>

<h3>Title: Reliable Malware Analysis and Detection using Topology Data Analysis. (arXiv:2211.01535v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01535">http://arxiv.org/abs/2211.01535</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01535] Reliable Malware Analysis and Detection using Topology Data Analysis](http://arxiv.org/abs/2211.01535)</code></li>
<li>Summary: <p>Increasingly, malwares are becoming complex and they are spreading on
networks targeting different infrastructures and personal-end devices to
collect, modify, and destroy victim information. Malware behaviors are
polymorphic, metamorphic, persistent, able to hide to bypass detectors and
adapt to new environments, and even leverage machine learning techniques to
better damage targets. Thus, it makes them difficult to analyze and detect with
traditional endpoint detection and response, intrusion detection and prevention
systems. To defend against malwares, recent work has proposed different
techniques based on signatures and machine learning. In this paper, we propose
to use an algebraic topological approach called topological-based data analysis
(TDA) to efficiently analyze and detect complex malware patterns. Next, we
compare the different TDA techniques (i.e., persistence homology, tomato, TDA
Mapper) and existing techniques (i.e., PCA, UMAP, t-SNE) using different
classifiers including random forest, decision tree, xgboost, and lightgbm. We
also propose some recommendations to deploy the best-identified models for
malware detection at scale. Results show that TDA Mapper (combined with PCA) is
better for clustering and for identifying hidden relationships between malware
clusters compared to PCA. Persistent diagrams are better to identify
overlapping malware clusters with low execution time compared to UMAP and
t-SNE. For malware detection, malware analysts can use Random Forest and
Decision Tree with t-SNE and Persistent Diagram to achieve better performance
and robustness on noised data.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: PolyBuilding: Polygon Transformer for End-to-End Building Extraction. (arXiv:2211.01589v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01589">http://arxiv.org/abs/2211.01589</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01589] PolyBuilding: Polygon Transformer for End-to-End Building Extraction](http://arxiv.org/abs/2211.01589)</code></li>
<li>Summary: <p>We present PolyBuilding, a fully end-to-end polygon Transformer for building
extraction. PolyBuilding direct predicts vector representation of buildings
from remote sensing images. It builds upon an encoder-decoder transformer
architecture and simultaneously outputs building bounding boxes and polygons.
Given a set of polygon queries, the model learns the relations among them and
encodes context information from the image to predict the final set of building
polygons with fixed vertex numbers. Corner classification is performed to
distinguish the building corners from the sampled points, which can be used to
remove redundant vertices along the building walls during inference. A 1-d
non-maximum suppression (NMS) is further applied to reduce vertex redundancy
near the building corners. With the refinement operations, polygons with
regular shapes and low complexity can be effectively obtained. Comprehensive
experiments are conducted on the CrowdAI dataset. Quantitative and qualitative
results show that our approach outperforms prior polygonal building extraction
methods by a large margin. It also achieves a new state-of-the-art in terms of
pixel-level coverage, instance-level precision and recall, and geometry-level
properties (including contour regularity and polygon complexity).
</p></li>
</ul>

<h3>Title: Video Event Extraction via Tracking Visual States of Arguments. (arXiv:2211.01781v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01781">http://arxiv.org/abs/2211.01781</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01781] Video Event Extraction via Tracking Visual States of Arguments](http://arxiv.org/abs/2211.01781)</code></li>
<li>Summary: <p>Video event extraction aims to detect salient events from a video and
identify the arguments for each event as well as their semantic roles. Existing
methods focus on capturing the overall visual scene of each frame, ignoring
fine-grained argument-level information. Inspired by the definition of events
as changes of states, we propose a novel framework to detect video events by
tracking the changes in the visual states of all involved arguments, which are
expected to provide the most informative evidence for the extraction of video
events. In order to capture the visual state changes of arguments, we decompose
them into changes in pixels within objects, displacements of objects, and
interactions among multiple arguments. We further propose Object State
Embedding, Object Motion-aware Embedding and Argument Interaction Embedding to
encode and track these changes respectively. Experiments on various video event
extraction tasks demonstrate significant improvements compared to
state-of-the-art models. In particular, on verb classification, we achieve
3.49% absolute gains (19.53% relative gains) in F1@5 on Video Situation
Recognition.
</p></li>
</ul>

<h3>Title: Quantifying Model Uncertainty for Semantic Segmentation using Operators in the RKHS. (arXiv:2211.01999v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01999">http://arxiv.org/abs/2211.01999</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01999] Quantifying Model Uncertainty for Semantic Segmentation using Operators in the RKHS](http://arxiv.org/abs/2211.01999)</code></li>
<li>Summary: <p>Deep learning models for semantic segmentation are prone to poor performance
in real-world applications due to the highly challenging nature of the task.
Model uncertainty quantification (UQ) is one way to address this issue of lack
of model trustworthiness by enabling the practitioner to know how much to trust
a segmentation output. Current UQ methods in this application domain are mainly
restricted to Bayesian based methods which are computationally expensive and
are only able to extract central moments of uncertainty thereby limiting the
quality of their uncertainty estimates. We present a simple framework for
high-resolution predictive uncertainty quantification of semantic segmentation
models that leverages a multi-moment functional definition of uncertainty
associated with the model's feature space in the reproducing kernel Hilbert
space (RKHS). The multiple uncertainty functionals extracted from this
framework are defined by the local density dynamics of the model's feature
space and hence automatically align themselves at the tail-regions of the
intrinsic probability density function of the feature space (where uncertainty
is the highest) in such a way that the successively higher order moments
quantify the more uncertain regions. This leads to a significantly more
accurate view of model uncertainty than conventional Bayesian methods.
Moreover, the extraction of such moments is done in a single-shot computation
making it much faster than Bayesian and ensemble approaches (that involve a
high number of forward stochastic passes of the model to quantify its
uncertainty). We demonstrate these advantages through experimental evaluations
of our framework implemented over four different state-of-the-art model
architectures that are trained and evaluated on two benchmark road-scene
segmentation datasets (Camvid and Cityscapes).
</p></li>
</ul>

<h3>Title: Cross-stitching Text and Knowledge Graph Encoders for Distantly Supervised Relation Extraction. (arXiv:2211.01432v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01432">http://arxiv.org/abs/2211.01432</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01432] Cross-stitching Text and Knowledge Graph Encoders for Distantly Supervised Relation Extraction](http://arxiv.org/abs/2211.01432)</code></li>
<li>Summary: <p>Bi-encoder architectures for distantly-supervised relation extraction are
designed to make use of the complementary information found in text and
knowledge graphs (KG). However, current architectures suffer from two
drawbacks. They either do not allow any sharing between the text encoder and
the KG encoder at all, or, in case of models with KG-to-text attention, only
share information in one direction. Here, we introduce cross-stitch
bi-encoders, which allow full interaction between the text encoder and the KG
encoder via a cross-stitch mechanism. The cross-stitch mechanism allows sharing
and updating representations between the two encoders at any layer, with the
amount of sharing being dynamically controlled via cross-attention-based gates.
Experimental results on two relation extraction benchmarks from two different
domains show that enabling full interaction between the two encoders yields
strong improvements.
</p></li>
</ul>

<h3>Title: Open-Vocabulary Argument Role Prediction for Event Extraction. (arXiv:2211.01577v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01577">http://arxiv.org/abs/2211.01577</a></li>
<li>Code URL: <a href="https://github.com/yzjiao/rolepred">https://github.com/yzjiao/rolepred</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01577] Open-Vocabulary Argument Role Prediction for Event Extraction](http://arxiv.org/abs/2211.01577)</code></li>
<li>Summary: <p>The argument role in event extraction refers to the relation between an event
and an argument participating in it. Despite the great progress in event
extraction, existing studies still depend on roles pre-defined by domain
experts. These studies expose obvious weakness when extending to emerging event
types or new domains without available roles. Therefore, more attention and
effort needs to be devoted to automatically customizing argument roles. In this
paper, we define this essential but under-explored task: open-vocabulary
argument role prediction. The goal of this task is to infer a set of argument
roles for a given event type. We propose a novel unsupervised framework,
RolePred for this task. Specifically, we formulate the role prediction problem
as an in-filling task and construct prompts for a pre-trained language model to
generate candidate roles. By extracting and analyzing the candidate arguments,
the event-specific roles are further merged and selected. To standardize the
research of this task, we collect a new event extraction dataset from
WikiPpedia including 142 customized argument roles with rich semantics. On this
dataset, RolePred outperforms the existing methods by a large margin. Source
code and dataset are available on our GitHub repository:
https://github.com/yzjiao/RolePred
</p></li>
</ul>

<h3>Title: Data-efficient End-to-end Information Extraction for Statistical Legal Analysis. (arXiv:2211.01692v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01692">http://arxiv.org/abs/2211.01692</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01692] Data-efficient End-to-end Information Extraction for Statistical Legal Analysis](http://arxiv.org/abs/2211.01692)</code></li>
<li>Summary: <p>Legal practitioners often face a vast amount of documents. Lawyers, for
instance, search for appropriate precedents favorable to their clients, while
the number of legal precedents is ever-growing. Although legal search engines
can assist finding individual target documents and narrowing down the number of
candidates, retrieved information is often presented as unstructured text and
users have to examine each document thoroughly which could lead to information
overloading. This also makes their statistical analysis challenging. Here, we
present an end-to-end information extraction (IE) system for legal documents.
By formulating IE as a generation task, our system can be easily applied to
various tasks without domain-specific engineering effort. The experimental
results of four IE tasks on Korean precedents shows that our IE system can
achieve competent scores (-2.3 on average) compared to the rule-based baseline
with as few as 50 training examples per task and higher score (+5.4 on average)
with 200 examples. Finally, our statistical analysis on two case
categories--drunk driving and fraud--with 35k precedents reveals the resulting
structured information from our IE system faithfully reflects the macroscopic
features of Korean legal system.
</p></li>
</ul>

<h3>Title: Query-based Instance Discrimination Network for Relational Triple Extraction. (arXiv:2211.01797v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01797">http://arxiv.org/abs/2211.01797</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01797] Query-based Instance Discrimination Network for Relational Triple Extraction](http://arxiv.org/abs/2211.01797)</code></li>
<li>Summary: <p>Joint entity and relation extraction has been a core task in the field of
information extraction. Recent approaches usually consider the extraction of
relational triples from a stereoscopic perspective, either learning a
relation-specific tagger or separate classifiers for each relation type.
However, they still suffer from error propagation, relation redundancy and lack
of high-level connections between triples. To address these issues, we propose
a novel query-based approach to construct instance-level representations for
relational triples. By metric-based comparison between query embeddings and
token embeddings, we can extract all types of triples in one step, thus
eliminating the error propagation problem. In addition, we learn the
instance-level representation of relational triples via contrastive learning.
In this way, relational triples can not only enclose rich class-level semantics
but also access to high-order global connections. Experimental results show
that our proposed method achieves the state of the art on five widely used
benchmarks.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FedMint: Intelligent Bilateral Client Selection in Federated Learning with Newcomer IoT Devices. (arXiv:2211.01805v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01805">http://arxiv.org/abs/2211.01805</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01805] FedMint: Intelligent Bilateral Client Selection in Federated Learning with Newcomer IoT Devices](http://arxiv.org/abs/2211.01805)</code></li>
<li>Summary: <p>Federated Learning (FL) is a novel distributed privacy-preserving learning
paradigm, which enables the collaboration among several participants (e.g.,
Internet of Things devices) for the training of machine learning models.
However, selecting the participants that would contribute to this collaborative
training is highly challenging. Adopting a random selection strategy would
entail substantial problems due to the heterogeneity in terms of data quality,
and computational and communication resources across the participants. Although
several approaches have been proposed in the literature to overcome the problem
of random selection, most of these approaches follow a unilateral selection
strategy. In fact, they base their selection strategy on only the federated
server's side, while overlooking the interests of the client devices in the
process. To overcome this problem, we present in this paper FedMint, an
intelligent client selection approach for federated learning on IoT devices
using game theory and bootstrapping mechanism. Our solution involves the design
of: (1) preference functions for the client IoT devices and federated servers
to allow them to rank each other according to several factors such as accuracy
and price, (2) intelligent matching algorithms that take into account the
preferences of both parties in their design, and (3) bootstrapping technique
that capitalizes on the collaboration of multiple federated servers in order to
assign initial accuracy value for the newly connected IoT devices. Based on our
simulation findings, our strategy surpasses the VanillaFL selection approach in
terms of maximizing both the revenues of the client devices and accuracy of the
global federated learning model.
</p></li>
</ul>

<h3>Title: FedTP: Federated Learning by Transformer Personalization. (arXiv:2211.01572v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01572">http://arxiv.org/abs/2211.01572</a></li>
<li>Code URL: <a href="https://github.com/zhyczy/fedtp">https://github.com/zhyczy/fedtp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01572] FedTP: Federated Learning by Transformer Personalization](http://arxiv.org/abs/2211.01572)</code></li>
<li>Summary: <p>Federated learning is an emerging learning paradigm where multiple clients
collaboratively train a machine learning model in a privacy-preserving manner.
Personalized federated learning extends this paradigm to overcome heterogeneity
across clients by learning personalized models. Recently, there have been some
initial attempts to apply Transformers to federated learning. However, the
impacts of federated learning algorithms on self-attention have not yet been
studied. This paper investigates this relationship and reveals that federated
averaging algorithms actually have a negative impact on self-attention where
there is data heterogeneity. These impacts limit the capabilities of the
Transformer model in federated learning settings. Based on this, we propose
FedTP, a novel Transformer-based federated learning framework that learns
personalized self-attention for each client while aggregating the other
parameters among the clients. Instead of using a vanilla personalization
mechanism that maintains personalized self-attention layers of each client
locally, we develop a learn-to-personalize mechanism to further encourage the
cooperation among clients and to increase the scablability and generalization
of FedTP. Specifically, the learn-to-personalize is realized by learning a
hypernetwork on the server that outputs the personalized projection matrices of
self-attention layers to generate client-wise queries, keys and values.
Furthermore, we present the generalization bound for FedTP with the
learn-to-personalize mechanism. Notably, FedTP offers a convenient environment
for performing a range of image and language tasks using the same federated
network architecture - all of which benefit from Transformer personalization.
Extensive experiments verify that FedTP with the learn-to-personalize mechanism
yields state-of-the-art performance in non-IID scenarios. Our code is available
online.
</p></li>
</ul>

<h3>Title: FedGen: Generalizable Federated Learning. (arXiv:2211.01914v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01914">http://arxiv.org/abs/2211.01914</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01914] FedGen: Generalizable Federated Learning](http://arxiv.org/abs/2211.01914)</code></li>
<li>Summary: <p>Existing federated learning models that follow the standard risk minimization
paradigm of machine learning often fail to generalize in the presence of
spurious correlations in the training data. In many real-world distributed
settings, spurious correlations exist due to biases and data sampling issues on
distributed devices or clients that can erroneously influence models. Current
generalization approaches are designed for centralized training and attempt to
identify features that have an invariant causal relationship with the target,
thereby reducing the effect of spurious features. However, such invariant risk
minimization approaches rely on apriori knowledge of training data
distributions which is hard to obtain in many applications. In this work, we
present a generalizable federated learning framework called FedGen, which
allows clients to identify and distinguish between spurious and invariant
features in a collaborative manner without prior knowledge of training
distributions. We evaluate our approach on real-world datasets from different
domains and show that FedGen results in models that achieve significantly
better generalization than current federated learning approaches.
</p></li>
</ul>

<h3>Title: Client Selection in Federated Learning: Principles, Challenges, and Opportunities. (arXiv:2211.01549v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01549">http://arxiv.org/abs/2211.01549</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01549] Client Selection in Federated Learning: Principles, Challenges, and Opportunities](http://arxiv.org/abs/2211.01549)</code></li>
<li>Summary: <p>As a privacy-preserving paradigm for training Machine Learning (ML) models,
Federated Learning (FL) has received tremendous attention from both industry
and academia. In a typical FL scenario, clients exhibit significant
heterogeneity in terms of data distribution and hardware configurations. Thus,
randomly sampling clients in each training round may not fully exploit the
local updates from heterogeneous clients, resulting in lower model accuracy,
slower convergence rate, degraded fairness, etc. To tackle the FL client
heterogeneity problem, various client selection algorithms have been developed,
showing promising performance improvement. In this paper, we systematically
present recent advances in the emerging field of FL client selection and its
challenges and research opportunities. We hope to facilitate practitioners in
choosing the most suitable client selection mechanisms for their applications,
as well as inspire researchers and newcomers to better understand this exciting
research topic.
</p></li>
</ul>

<h3>Title: A Convergence Theory for Federated Average: Beyond Smoothness. (arXiv:2211.01588v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01588">http://arxiv.org/abs/2211.01588</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01588] A Convergence Theory for Federated Average: Beyond Smoothness](http://arxiv.org/abs/2211.01588)</code></li>
<li>Summary: <p>Federated learning enables a large amount of edge computing devices to learn
a model without data sharing jointly. As a leading algorithm in this setting,
Federated Average FedAvg, which runs Stochastic Gradient Descent (SGD) in
parallel on local devices and averages the sequences only once in a while, have
been widely used due to their simplicity and low communication cost. However,
despite recent research efforts, it lacks theoretical analysis under
assumptions beyond smoothness. In this paper, we analyze the convergence of
FedAvg. Different from the existing work, we relax the assumption of strong
smoothness. More specifically, we assume the semi-smoothness and semi-Lipschitz
properties for the loss function, which have an additional first-order term in
assumption definitions. In addition, we also assume bound on the gradient,
which is weaker than the commonly used bounded gradient assumption in the
convergence analysis scheme. As a solution, this paper provides a theoretical
convergence study on Federated Learning.
</p></li>
</ul>

<h3>Title: Faster Adaptive Momentum-Based Federated Methods for Distributed Composition Optimization. (arXiv:2211.01883v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01883">http://arxiv.org/abs/2211.01883</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01883] Faster Adaptive Momentum-Based Federated Methods for Distributed Composition Optimization](http://arxiv.org/abs/2211.01883)</code></li>
<li>Summary: <p>Composition optimization recently appears in many machine learning
applications such as meta learning and reinforcement learning. Recently many
composition optimization algorithms have been proposed and studied, however,
few adaptive algorithm considers the composition optimization under the
distributed setting. Meanwhile, the existing distributed composition
optimization methods still suffer from high sample and communication
complexities. In the paper, thus, we develop a class of faster momentum-based
federated compositional gradient descent algorithms (i.e., MFCGD and AdaMFCGD)
to solve the nonconvex distributed composition problems, which builds on the
momentum-based variance reduced and local-SGD techniques. In particular, our
adaptive algorithm (i.e., AdaMFCGD) uses a unified adaptive matrix to flexibly
incorporate various adaptive learning rates. Moreover, we provide a solid
theoretical analysis for our algorithms under non-i.i.d. setting, and prove our
algorithms obtain a lower sample and communication complexities simultaneously
than the existing federated compositional algorithms. Specifically, our
algorithms obtain lower sample complexity of $\tilde{O}(\epsilon^{-3})$ with
lower communication complexity of $\tilde{O}(\epsilon^{-2})$ in finding an
$\epsilon$-stationary point. We conduct the experiments on robust federated
learning and distributed meta learning tasks to demonstrate efficiency of our
algorithms.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Fair and Optimal Classification via Transports to Wasserstein-Barycenter. (arXiv:2211.01528v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01528">http://arxiv.org/abs/2211.01528</a></li>
<li>Code URL: <a href="https://github.com/rxian/fair-classification">https://github.com/rxian/fair-classification</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01528] Fair and Optimal Classification via Transports to Wasserstein-Barycenter](http://arxiv.org/abs/2211.01528)</code></li>
<li>Summary: <p>Fairness in automated decision-making systems has gained increasing attention
as their applications expand to real-world high-stakes domains. To facilitate
the design of fair ML systems, it is essential to understand the potential
trade-offs between fairness and predictive power, and the construction of the
optimal predictor under a given fairness constraint. In this paper, for general
classification problems under the group fairness criterion of demographic
parity (DP), we precisely characterize the trade-off between DP and
classification accuracy, referred to as the minimum cost of fairness. Our
insight comes from the key observation that finding the optimal fair classifier
is equivalent to solving a Wasserstein-barycenter problem under $\ell_1$-norm
restricted to the vertices of the probability simplex. Inspired by our
characterization, we provide a construction of an optimal fair classifier
achieving this minimum cost via the composition of the Bayes regressor and
optimal transports from its output distributions to the barycenter. Our
construction naturally leads to an algorithm for post-processing any
pre-trained predictor to satisfy DP fairness, complemented with finite sample
guarantees. Experiments on real-world datasets verify and demonstrate the
effectiveness of our approaches.
</p></li>
</ul>

<h3>Title: Empirical Analysis of Model Selection for Heterogenous Causal Effect Estimation. (arXiv:2211.01939v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01939">http://arxiv.org/abs/2211.01939</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01939] Empirical Analysis of Model Selection for Heterogenous Causal Effect Estimation](http://arxiv.org/abs/2211.01939)</code></li>
<li>Summary: <p>We study the problem of model selection in causal inference, specifically for
the case of conditional average treatment effect (CATE) estimation under binary
treatments. Unlike model selection in machine learning, we cannot use the
technique of cross-validation here as we do not observe the counterfactual
potential outcome for any data point. Hence, we need to design model selection
techniques that do not explicitly rely on counterfactual data. As an
alternative to cross-validation, there have been a variety of proxy metrics
proposed in the literature, that depend on auxiliary nuisance models also
estimated from the data (propensity score model, outcome regression model).
However, the effectiveness of these metrics has only been studied on synthetic
datasets as we can observe the counterfactual data for them. We conduct an
extensive empirical analysis to judge the performance of these metrics, where
we utilize the latest advances in generative modeling to incorporate multiple
realistic datasets. We evaluate 9 metrics on 144 datasets for selecting between
415 estimators per dataset, including datasets that closely mimic real-world
datasets. Further, we use the latest techniques from AutoML to ensure
consistent hyperparameter selection for nuisance models for a fair comparison
across metrics.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Exploring explicit coarse-grainend structure in artificial neural networks. (arXiv:2211.01779v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01779">http://arxiv.org/abs/2211.01779</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01779] Exploring explicit coarse-grainend structure in artificial neural networks](http://arxiv.org/abs/2211.01779)</code></li>
<li>Summary: <p>We propose to employ the hierarchical coarse-grained structure in the
artificial neural networks explicitly to improve the interpretability without
degrading performance. The idea has been applied in two situations. One is a
neural network called TaylorNet, which aims to approximate the general mapping
from input data to output result in terms of Taylor series directly, without
resorting to any magic nonlinear activations. The other is a new setup for data
distillation, which can perform multi-level abstraction of the input dataset
and generate new data that possesses the relevant features of the original
dataset and can be used as references for classification. In both cases, the
coarse-grained structure plays an important role in simplifying the network and
improving both the interpretability and efficiency. The validity has been
domonstrated on MNIST and CIFAR-10 datasets. Further improvement and some open
questions related are also discussed.
</p></li>
</ul>

<h3>Title: On the Safety of Interpretable Machine Learning: A Maximum Deviation Approach. (arXiv:2211.01498v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01498">http://arxiv.org/abs/2211.01498</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01498] On the Safety of Interpretable Machine Learning: A Maximum Deviation Approach](http://arxiv.org/abs/2211.01498)</code></li>
<li>Summary: <p>Interpretable and explainable machine learning has seen a recent surge of
interest. We focus on safety as a key motivation behind the surge and make the
relationship between interpretability and safety more quantitative. Toward
assessing safety, we introduce the concept of maximum deviation via an
optimization problem to find the largest deviation of a supervised learning
model from a reference model regarded as safe. We then show how
interpretability facilitates this safety assessment. For models including
decision trees, generalized linear and additive models, the maximum deviation
can be computed exactly and efficiently. For tree ensembles, which are not
regarded as interpretable, discrete optimization techniques can still provide
informative bounds. For a broader class of piecewise Lipschitz functions, we
leverage the multi-armed bandit literature to show that interpretability
produces tighter (regret) bounds on the maximum deviation. We present case
studies, including one on mortgage approval, to illustrate our methods and the
insights about models that may be obtained from deviation maximization.
</p></li>
</ul>

<h3>Title: The role of prior information and computational power in Machine Learning. (arXiv:2211.01972v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.01972">http://arxiv.org/abs/2211.01972</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.01972] The role of prior information and computational power in Machine Learning](http://arxiv.org/abs/2211.01972)</code></li>
<li>Summary: <p>Science consists on conceiving hypotheses, confronting them with empirical
evidence, and keeping only hypotheses which have not yet been falsified. Under
deductive reasoning they are conceived in view of a theory and confronted with
empirical evidence in an attempt to falsify it, and under inductive reasoning
they are conceived based on observation, confronted with empirical evidence and
a theory is established based on the not falsified hypotheses. When the
hypotheses testing can be performed with quantitative data, the confrontation
can be achieved with Machine Learning methods, whose quality is highly
dependent on the hypotheses' complexity, hence on the proper insertion of prior
information into the set of hypotheses seeking to decrease its complexity
without loosing good hypotheses. However, Machine Learning tools have been
applied under the pragmatic view of instrumentalism, which is concerned only
with the performance of the methods and not with the understanding of their
behavior, leading to methods which are not fully understood. In this context,
we discuss how prior information and computational power can be employed to
solve a learning problem, but while prior information and a careful design of
the hypotheses space has as advantage the interpretability of the results,
employing high computational power has the advantage of a higher performance.
We discuss why learning methods which combine both should work better from an
understanding and performance perspective, arguing in favor of basic
theoretical research on Machine Learning, in special about how properties of
classifiers may be identified in parameters of modern learning models.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
