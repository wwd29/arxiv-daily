<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-05-02</h1>
<h3>Title: SIMPLOT: Enhancing Chart Question Answering by Distilling Essentials</h3>
<ul>
<li><strong>Authors: </strong>Wonjoong Kim, Sangwu Park, Yeonjun In, Seokwon Han, Chanyoung Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00021">https://arxiv.org/abs/2405.00021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00021">https://arxiv.org/pdf/2405.00021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00021]] SIMPLOT: Enhancing Chart Question Answering by Distilling Essentials(https://arxiv.org/abs/2405.00021)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Recently, interpreting complex charts with logical reasoning have emerged as challenges due to the development of vision-language models. A prior state-of-the-art (SOTA) model, Deplot, has presented an end-to-end method that leverages the vision-language model to convert charts into table format utilizing Large Language Models (LLMs) for reasoning. However, unlike natural images, charts contain a mix of essential and irrelevant information required for chart reasoning, and we discover that this characteristic can lower the performance of chart-to-table extraction. In this paper, we introduce SIMPLOT, a method designed to extract only the elements necessary for chart reasoning. The proposed method involves two steps: 1) training to mimic a simple plot that contains only the essential information from a complex chart for table extraction, followed by 2) performing reasoning based on the table. Our model enables accurate chart reasoning without the need for additional annotations or datasets, and its effectiveness is demonstrated through various experiments. Furthermore, we propose a novel prompt addressing the shortcoming of recent SOTA model, ignoring visual attributes such as color. Our source code is available at https://github.com/sangwu99/Simplot.</li>
</ul>

<h3>Title: Leveraging Pre-trained CNNs for Efficient Feature Extraction in Rice  Leaf Disease Classification</h3>
<ul>
<li><strong>Authors: </strong>Md. Shohanur Islam Sobuj, Md. Imran Hossen, Md. Foysal Mahmud, Mahbub Ul Islam Khan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00025">https://arxiv.org/abs/2405.00025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00025">https://arxiv.org/pdf/2405.00025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00025]] Leveraging Pre-trained CNNs for Efficient Feature Extraction in Rice  Leaf Disease Classification(https://arxiv.org/abs/2405.00025)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Rice disease classification is a critical task in agricultural research, and in this study, we rigorously evaluate the impact of integrating feature extraction methodologies within pre-trained convolutional neural networks (CNNs). Initial investigations into baseline models, devoid of feature extraction, revealed commendable performance with ResNet-50 and ResNet-101 achieving accuracies of 91% and 92%, respectively. Subsequent integration of Histogram of Oriented Gradients (HOG) yielded substantial improvements across architectures, notably propelling the accuracy of EfficientNet-B7 from 92\% to an impressive 97%. Conversely, the application of Local Binary Patterns (LBP) demonstrated more conservative performance enhancements. Moreover, employing Gradient-weighted Class Activation Mapping (Grad-CAM) unveiled that HOG integration resulted in heightened attention to disease-specific features, corroborating the performance enhancements observed. Visual representations further validated HOG's notable influence, showcasing a discernible surge in accuracy across epochs due to focused attention on disease-affected regions. These results underscore the pivotal role of feature extraction, particularly HOG, in refining representations and bolstering classification accuracy. The study's significant highlight was the achievement of 97% accuracy with EfficientNet-B7 employing HOG and Grad-CAM, a noteworthy advancement in optimizing pre-trained CNN-based rice disease identification systems. The findings advocate for the strategic integration of advanced feature extraction techniques with cutting-edge pre-trained CNN architectures, presenting a promising avenue for substantially augmenting the precision and effectiveness of image-based disease classification systems in agricultural contexts.</li>
</ul>

<h3>Title: Automatic Creative Selection with Cross-Modal Matching</h3>
<ul>
<li><strong>Authors: </strong>Alex Kim, Jia Huang, Rob Monarch, Jerry Kwac, Anikesh Kamath, Parmeshwar Khurd, Kailash Thiyagarajan, Goodman Gu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00029">https://arxiv.org/abs/2405.00029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00029">https://arxiv.org/pdf/2405.00029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00029]] Automatic Creative Selection with Cross-Modal Matching(https://arxiv.org/abs/2405.00029)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Application developers advertise their Apps by creating product pages with App images, and bidding on search terms. It is then crucial for App images to be highly relevant with the search terms. Solutions to this problem require an image-text matching model to predict the quality of the match between the chosen image and the search terms. In this work, we present a novel approach to matching an App image to search terms based on fine-tuning a pre-trained LXMERT model. We show that compared to the CLIP model and a baseline using a Transformer model for search terms, and a ResNet model for images, we significantly improve the matching accuracy. We evaluate our approach using two sets of labels: advertiser associated (image, search term) pairs for a given application, and human ratings for the relevance between (image, search term) pairs. Our approach achieves 0.96 AUC score for advertiser associated ground truth, outperforming the transformer+ResNet baseline and the fine-tuned CLIP model by 8% and 14%. For human labeled ground truth, our approach achieves 0.95 AUC score, outperforming the transformer+ResNet baseline and the fine-tuned CLIP model by 16% and 17%.</li>
</ul>

<h3>Title: SegNet: A Segmented Deep Learning based Convolutional Neural Network  Approach for Drones Wildfire Detection</h3>
<ul>
<li><strong>Authors: </strong>Aditya V. Jonnalagadda, Hashim A. Hashim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00031">https://arxiv.org/abs/2405.00031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00031">https://arxiv.org/pdf/2405.00031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00031]] SegNet: A Segmented Deep Learning based Convolutional Neural Network  Approach for Drones Wildfire Detection(https://arxiv.org/abs/2405.00031)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This research addresses the pressing challenge of enhancing processing times and detection capabilities in Unmanned Aerial Vehicle (UAV)/drone imagery for global wildfire detection, despite limited datasets. Proposing a Segmented Neural Network (SegNet) selection approach, we focus on reducing feature maps to boost both time resolution and accuracy significantly advancing processing speeds and accuracy in real-time wildfire detection. This paper contributes to increased processing speeds enabling real-time detection capabilities for wildfire, increased detection accuracy of wildfire, and improved detection capabilities of early wildfire, through proposing a new direction for image classification of amorphous objects like fire, water, smoke, etc. Employing Convolutional Neural Networks (CNNs) for image classification, emphasizing on the reduction of irrelevant features vital for deep learning processes, especially in live feed data for fire detection. Amidst the complexity of live feed data in fire detection, our study emphasizes on image feed, highlighting the urgency to enhance real-time processing. Our proposed algorithm combats feature overload through segmentation, addressing challenges arising from diverse features like objects, colors, and textures. Notably, a delicate balance of feature map size and dataset adequacy is pivotal. Several research papers use smaller image sizes, compromising feature richness which necessitating a new approach. We illuminate the critical role of pixel density in retaining essential details, especially for early wildfire detection. By carefully selecting number of filters during training, we underscore the significance of higher pixel density for proper feature selection. The proposed SegNet approach is rigorously evaluated using real-world dataset obtained by a drone flight and compared to state-of-the-art literature.</li>
</ul>

<h3>Title: Research and application of artificial intelligence based webshell  detection model: A literature review</h3>
<ul>
<li><strong>Authors: </strong>Mingrui Ma, Lansheng Han, Chunjie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00066">https://arxiv.org/abs/2405.00066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00066">https://arxiv.org/pdf/2405.00066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00066]] Research and application of artificial intelligence based webshell  detection model: A literature review(https://arxiv.org/abs/2405.00066)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>Webshell, as the "culprit" behind numerous network attacks, is one of the research hotspots in the field of cybersecurity. However, the complexity, stealthiness, and confusing nature of webshells pose significant challenges to the corresponding detection schemes. With the rise of Artificial Intelligence (AI) technology, researchers have started to apply different intelligent algorithms and neural network architectures to the task of webshell detection. However, the related research still lacks a systematic and standardized methodological process, which is confusing and redundant. Therefore, following the development timeline, we carefully summarize the progress of relevant research in this field, dividing it into three stages: Start Stage, Initial Development Stage, and In-depth Development Stage. We further elaborate on the main characteristics and core algorithms of each stage. In addition, we analyze the pain points and challenges that still exist in this field and predict the future development trend of this field from our point of view. To the best of our knowledge, this is the first review that details the research related to AI-based webshell detection. It is also hoped that this paper can provide detailed technical information for more researchers interested in AI-based webshell detection tasks.</li>
</ul>

<h3>Title: PAODING: A High-fidelity Data-free Pruning Toolkit for Debloating  Pre-trained Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Mark Huasong Meng, Hao Guan, Liuhuo Wan, Sin Gee Teo, Guangdong Bai, Jin Song Dong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00074">https://arxiv.org/abs/2405.00074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00074">https://arxiv.org/pdf/2405.00074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00074]] PAODING: A High-fidelity Data-free Pruning Toolkit for Debloating  Pre-trained Neural Networks(https://arxiv.org/abs/2405.00074)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, data-free</a></li>
<li><strong>Abstract: </strong>We present PAODING, a toolkit to debloat pretrained neural network models through the lens of data-free pruning. To preserve the model fidelity, PAODING adopts an iterative process, which dynamically measures the effect of deleting a neuron to identify candidates that have the least impact to the output layer. Our evaluation shows that PAODING can significantly reduce the model size, generalize on different datasets and models, and meanwhile preserve the model fidelity in terms of test accuracy and adversarial robustness. PAODING is publicly available on PyPI via https://pypi.org/project/paoding-dl.</li>
</ul>

<h3>Title: On Correcting SHAP Scores</h3>
<ul>
<li><strong>Authors: </strong>Olivier Letoffe, Xuanxiang Huang, Joao Marques-Silva</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00076">https://arxiv.org/abs/2405.00076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00076">https://arxiv.org/pdf/2405.00076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00076]] On Correcting SHAP Scores(https://arxiv.org/abs/2405.00076)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Recent work uncovered examples of classifiers for which SHAP scores yield misleading feature attributions. While such examples might be perceived as suggesting the inadequacy of Shapley values for explainability, this paper shows that the source of the identified shortcomings of SHAP scores resides elsewhere. Concretely, the paper makes the case that the failings of SHAP scores result from the characteristic functions used in earlier works. Furthermore, the paper identifies a number of properties that characteristic functions ought to respect, and proposes several novel characteristic functions, each exhibiting one or more of the desired properties. More importantly, some of the characteristic functions proposed in this paper are guaranteed not to exhibit any of the shortcomings uncovered by earlier work. The paper also investigates the impact of the new characteristic functions on the complexity of computing SHAP scores. Finally, the paper proposes modifications to the tool SHAP to use instead one of our novel characteristic functions, thereby eliminating some of the limitations reported for SHAP scores.</li>
</ul>

<h3>Title: Mitigating Spectre-PHT using Speculation Barriers in Linux BPF</h3>
<ul>
<li><strong>Authors: </strong>Luis Gerhorst, Henriette Herzog, Peter Wägemann, Maximilian Ott, Rüdiger Kapitza, Timo Hönig</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.OS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00078">https://arxiv.org/abs/2405.00078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00078">https://arxiv.org/pdf/2405.00078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00078]] Mitigating Spectre-PHT using Speculation Barriers in Linux BPF(https://arxiv.org/abs/2405.00078)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, defense, attack</a></li>
<li><strong>Abstract: </strong>High-performance IO demands low-overhead communication between user- and kernel space. This demand can no longer be fulfilled by traditional system calls. Linux's extended Berkeley Packet Filter (BPF) avoids user-/kernel transitions by just-in-time compiling user-provided bytecode and executing it in kernel mode with near-native speed. To still isolate BPF programs from the kernel, they are statically analyzed for memory- and type-safety, which imposes some restrictions but allows for good expressiveness and high performance. However, to mitigate the Spectre vulnerabilities disclosed in 2018, defenses which reject potentially-dangerous programs had to be deployed. We find that this affects 24% to 54% of programs in a dataset with 844 real-world BPF programs from popular open-source projects. To solve this, users are forced to disable the defenses to continue using the programs, which puts the entire system at risk. To enable secure and expressive untrusted Linux kernel extensions, we propose Berrify, an enhancement to the kernel's Spectre defenses that reduces the number of BPF application programs rejected from 54% to zero. We measure Berrify's overhead for all mainstream performance-sensitive applications of BPF (i.e., event tracing, profiling, and packet processing) and find that it improves significantly upon the status-quo where affected BPF programs are either unusable or enable transient execution attacks on the kernel.</li>
</ul>

<h3>Title: Rolling in the Shadows: Analyzing the Extraction of MEV Across Layer-2  Rollups</h3>
<ul>
<li><strong>Authors: </strong>Christof Ferreira Torres, Albin Mamuti, Ben Weintraub, Cristina Nita-Rotaru, Shweta Shinde</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00138">https://arxiv.org/abs/2405.00138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00138">https://arxiv.org/pdf/2405.00138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00138]] Rolling in the Shadows: Analyzing the Extraction of MEV Across Layer-2  Rollups(https://arxiv.org/abs/2405.00138)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, extraction</a></li>
<li><strong>Abstract: </strong>The emergence of decentralized finance has transformed asset trading on the blockchain, making traditional financial instruments more accessible while also introducing a series of exploitative economic practices known as Maximal Extractable Value (MEV). Concurrently, decentralized finance has embraced rollup-based Layer-2 solutions to facilitate asset trading at reduced transaction costs compared to Layer-1 solutions such as Ethereum. However, rollups lack a public mempool like Ethereum, making the extraction of MEV more challenging. In this paper, we investigate the prevalence and impact of MEV on Ethereum and prominent rollups such as Arbitrum, Optimism, and zkSync over a nearly three-year period. Our analysis encompasses various metrics including volume, profits, costs, competition, and response time to MEV opportunities. We discover that MEV is widespread on rollups, with trading volume comparable to Ethereum. We also find that, although MEV costs are lower on rollups, profits are also significantly lower compared to Ethereum. Additionally, we examine the prevalence of sandwich attacks on rollups. While our findings did not detect any sandwiching activity on popular rollups, we did identify the potential for cross-layer sandwich attacks facilitated by transactions that are sent across rollups and Ethereum. Consequently, we propose and evaluate the feasibility of three novel attacks that exploit cross-layer transactions, revealing that attackers could have already earned approximately 2 million USD through cross-layer sandwich attacks.</li>
</ul>

<h3>Title: Revisiting RGBT Tracking Benchmarks from the Perspective of Modality  Validity: A New Benchmark, Problem, and Method</h3>
<ul>
<li><strong>Authors: </strong>Zhangyong Tang, Tianyang Xu, Zhenhua Feng, Xuefeng Zhu, He Wang, Pengcheng Shao, Chunyang Cheng, Xiao-Jun Wu, Muhammad Awais, Sara Atito, Josef Kittler</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00168">https://arxiv.org/abs/2405.00168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00168">https://arxiv.org/pdf/2405.00168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00168]] Revisiting RGBT Tracking Benchmarks from the Perspective of Modality  Validity: A New Benchmark, Problem, and Method(https://arxiv.org/abs/2405.00168)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>RGBT tracking draws increasing attention due to its robustness in multi-modality warranting (MMW) scenarios, such as nighttime and bad weather, where relying on a single sensing modality fails to ensure stable tracking results. However, the existing benchmarks predominantly consist of videos collected in common scenarios where both RGB and thermal infrared (TIR) information are of sufficient quality. This makes the data unrepresentative of severe imaging conditions, leading to tracking failures in MMW scenarios. To bridge this gap, we present a new benchmark, MV-RGBT, captured specifically in MMW scenarios. In contrast with the existing datasets, MV-RGBT comprises more object categories and scenes, providing a diverse and challenging benchmark. Furthermore, for severe imaging conditions of MMW scenarios, a new problem is posed, namely \textit{when to fuse}, to stimulate the development of fusion strategies for such data. We propose a new method based on a mixture of experts, namely MoETrack, as a baseline fusion strategy. In MoETrack, each expert generates independent tracking results along with the corresponding confidence score, which is used to control the fusion process. Extensive experimental results demonstrate the significant potential of MV-RGBT in advancing RGBT tracking and elicit the conclusion that fusion is not always beneficial, especially in MMW scenarios. Significantly, the proposed MoETrack method achieves new state-of-the-art results not only on MV-RGBT, but also on standard benchmarks, such as RGBT234, LasHeR, and the short-term split of VTUAV (VTUAV-ST). More information of MV-RGBT and the source code of MoETrack will be released at https://github.com/Zhangyong-Tang/MoETrack.</li>
</ul>

<h3>Title: Towards a Search Engine for Machines: Unified Ranking for Multiple  Retrieval-Augmented Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alireza Salemi, Hamed Zamani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00175">https://arxiv.org/abs/2405.00175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00175">https://arxiv.org/pdf/2405.00175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00175]] Towards a Search Engine for Machines: Unified Ranking for Multiple  Retrieval-Augmented Large Language Models(https://arxiv.org/abs/2405.00175)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces uRAG--a framework with a unified retrieval engine that serves multiple downstream retrieval-augmented generation (RAG) systems. Each RAG system consumes the retrieval results for a unique purpose, such as open-domain question answering, fact verification, entity linking, and relation extraction. We introduce a generic training guideline that standardizes the communication between the search engine and the downstream RAG systems that engage in optimizing the retrieval model. This lays the groundwork for us to build a large-scale experimentation ecosystem consisting of 18 RAG systems that engage in training and 18 unknown RAG systems that use the uRAG as the new users of the search engine. Using this experimentation ecosystem, we answer a number of fundamental research questions that improve our understanding of promises and challenges in developing search engines for machines.</li>
</ul>

<h3>Title: Towards End-to-End Semi-Supervised Table Detection with Semantic Aligned  Matching Transformer</h3>
<ul>
<li><strong>Authors: </strong>Tahira Shehzadi, Shalini Sarode, Didier Stricker, Muhammad Zeshan Afzal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00187">https://arxiv.org/abs/2405.00187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00187">https://arxiv.org/pdf/2405.00187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00187]] Towards End-to-End Semi-Supervised Table Detection with Semantic Aligned  Matching Transformer(https://arxiv.org/abs/2405.00187)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Table detection within document images is a crucial task in document processing, involving the identification and localization of tables. Recent strides in deep learning have substantially improved the accuracy of this task, but it still heavily relies on large labeled datasets for effective training. Several semi-supervised approaches have emerged to overcome this challenge, often employing CNN-based detectors with anchor proposals and post-processing techniques like non-maximal suppression (NMS). However, recent advancements in the field have shifted the focus towards transformer-based techniques, eliminating the need for NMS and emphasizing object queries and attention mechanisms. Previous research has focused on two key areas to improve transformer-based detectors: refining the quality of object queries and optimizing attention mechanisms. However, increasing object queries can introduce redundancy, while adjustments to the attention mechanism can increase complexity. To address these challenges, we introduce a semi-supervised approach employing SAM-DETR, a novel approach for precise alignment between object queries and target features. Our approach demonstrates remarkable reductions in false positives and substantial enhancements in table detection performance, particularly in complex documents characterized by diverse table structures. This work provides more efficient and accurate table detection in semi-supervised settings.</li>
</ul>

<h3>Title: Synthetic Image Verification in the Era of Generative AI: What Works and  What Isn't There Yet</h3>
<ul>
<li><strong>Authors: </strong>Diangarti Tariang, Riccardo Corvi, Davide Cozzolino, Giovanni Poggi, Koki Nagano, Luisa Verdoliva</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00196">https://arxiv.org/abs/2405.00196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00196">https://arxiv.org/pdf/2405.00196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00196]] Synthetic Image Verification in the Era of Generative AI: What Works and  What Isn't There Yet(https://arxiv.org/abs/2405.00196)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work we present an overview of approaches for the detection and attribution of synthetic images and highlight their strengths and weaknesses. We also point out and discuss hot topics in this field and outline promising directions for future research.</li>
</ul>

<h3>Title: SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Samir Arora, Liangliang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00201">https://arxiv.org/abs/2405.00201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00201">https://arxiv.org/pdf/2405.00201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00201]] SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained  Large Language Models(https://arxiv.org/abs/2405.00201)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Full fine-tuning is a popular approach to adapt Transformer-based pre-trained large language models to a specific downstream task. However, the substantial requirements for computational power and storage have discouraged its widespread use. Moreover, increasing evidence of catastrophic forgetting and overparameterization in the Transformer architecture has motivated researchers to seek more efficient fine-tuning (PEFT) methods. Commonly known parameter-efficient fine-tuning methods like LoRA and BitFit are typically applied across all layers of the model. We propose a PEFT method, called Stratified Progressive Adaptation Fine-tuning (SPAFIT), based on the localization of different types of linguistic knowledge to specific layers of the model. Our experiments, conducted on nine tasks from the GLUE benchmark, show that our proposed SPAFIT method outperforms other PEFT methods while fine-tuning only a fraction of the parameters adjusted by other methods.</li>
</ul>

<h3>Title: Leveraging Active Subspaces to Capture Epistemic Model Uncertainty in  Deep Generative Models for Molecular Design</h3>
<ul>
<li><strong>Authors: </strong>A N M Nafiz Abeer, Sanket Jantre, Nathan M Urban, Byung-Jun Yoon</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00202">https://arxiv.org/abs/2405.00202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00202">https://arxiv.org/pdf/2405.00202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00202]] Leveraging Active Subspaces to Capture Epistemic Model Uncertainty in  Deep Generative Models for Molecular Design(https://arxiv.org/abs/2405.00202)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep generative models have been accelerating the inverse design process in material and drug design. Unlike their counterpart property predictors in typical molecular design frameworks, generative molecular design models have seen fewer efforts on uncertainty quantification (UQ) due to computational challenges in Bayesian inference posed by their large number of parameters. In this work, we focus on the junction-tree variational autoencoder (JT-VAE), a popular model for generative molecular design, and address this issue by leveraging the low dimensional active subspace to capture the uncertainty in the model parameters. Specifically, we approximate the posterior distribution over the active subspace parameters to estimate the epistemic model uncertainty in an extremely high dimensional parameter space. The proposed UQ scheme does not require alteration of the model architecture, making it readily applicable to any pre-trained model. Our experiments demonstrate the efficacy of the AS-based UQ and its potential impact on molecular optimization by exploring the model diversity under epistemic uncertainty.</li>
</ul>

<h3>Title: General Purpose Verification for Chain of Thought Prompting</h3>
<ul>
<li><strong>Authors: </strong>Robert Vacareanu, Anurag Pratik, Evangelia Spiliopoulou, Zheng Qi, Giovanni Paolini, Neha Anna John, Jie Ma, Yassine Benajiba, Miguel Ballesteros</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00204">https://arxiv.org/abs/2405.00204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00204">https://arxiv.org/pdf/2405.00204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00204]] General Purpose Verification for Chain of Thought Prompting(https://arxiv.org/abs/2405.00204)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Many of the recent capabilities demonstrated by Large Language Models (LLMs) arise primarily from their ability to exploit contextual information. In this paper, we explore ways to improve reasoning capabilities of LLMs through (1) exploration of different chains of thought and (2) validation of the individual steps of the reasoning process. We propose three general principles that a model should adhere to while reasoning: (i) Relevance, (ii) Mathematical Accuracy, and (iii) Logical Consistency. We apply these constraints to the reasoning steps generated by the LLM to improve the accuracy of the final generation. The constraints are applied in the form of verifiers: the model itself is asked to verify if the generated steps satisfy each constraint. To further steer the generations towards high-quality solutions, we use the perplexity of the reasoning steps as an additional verifier. We evaluate our method on 4 distinct types of reasoning tasks, spanning a total of 9 different datasets. Experiments show that our method is always better than vanilla generation, and, in 6 out of the 9 datasets, it is better than best-of N sampling which samples N reasoning chains and picks the lowest perplexity generation.</li>
</ul>

<h3>Title: A Primer on the Inner Workings of Transformer-based Language Models</h3>
<ul>
<li><strong>Authors: </strong>Javier Ferrando, Gabriele Sarti, Arianna Bisazza, Marta R. Costa-jussà</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00208">https://arxiv.org/abs/2405.00208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00208">https://arxiv.org/pdf/2405.00208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00208]] A Primer on the Inner Workings of Transformer-based Language Models(https://arxiv.org/abs/2405.00208)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>The rapid progress of research aimed at interpreting the inner workings of advanced language models has highlighted a need for contextualizing the insights gained from years of work in this area. This primer provides a concise technical introduction to the current techniques used to interpret the inner workings of Transformer-based language models, focusing on the generative decoder-only architecture. We conclude by presenting a comprehensive overview of the known internal mechanisms implemented by these models, uncovering connections across popular approaches and active research directions in this area.</li>
</ul>

<h3>Title: Graphical Reasoning: LLM-based Semi-Open Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Tao, Yiqun Wang, Longju Bai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00216">https://arxiv.org/abs/2405.00216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00216">https://arxiv.org/pdf/2405.00216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00216]] Graphical Reasoning: LLM-based Semi-Open Relation Extraction(https://arxiv.org/abs/2405.00216)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive exploration of relation extraction utilizing advanced language models, specifically Chain of Thought (CoT) and Graphical Reasoning (GRE) techniques. We demonstrate how leveraging in-context learning with GPT-3.5 can significantly enhance the extraction process, particularly through detailed example-based reasoning. Additionally, we introduce a novel graphical reasoning approach that dissects relation extraction into sequential sub-tasks, improving precision and adaptability in processing complex relational data. Our experiments, conducted on multiple datasets, including manually annotated data, show considerable improvements in performance metrics, underscoring the effectiveness of our methodologies.</li>
</ul>

<h3>Title: Constrained Decoding for Secure Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Yanjun Fu, Ethan Baker, Yizheng Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00218">https://arxiv.org/abs/2405.00218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00218">https://arxiv.org/pdf/2405.00218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00218]] Constrained Decoding for Secure Code Generation(https://arxiv.org/abs/2405.00218)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, large language model</a></li>
<li><strong>Abstract: </strong>Code Large Language Models (Code LLMs) have been increasingly used by developers to boost productivity, but they often generate vulnerable code. Thus, there is an urgent need to ensure that code generated by Code LLMs is correct and secure. Previous research has primarily focused on generating secure code, overlooking the fact that secure code also needs to be correct. This oversight can lead to a false sense of security. Currently, the community lacks a method to measure actual progress in this area, and we need solutions that address both security and correctness of code generation. This paper introduces a new benchmark, CodeGuard+, along with two new metrics, secure-pass@k and secure@$k_{\text{pass}}$, to measure Code LLMs' ability to generate both secure and correct code. Using our new evaluation methods, we show that the state-of-the-art defense technique, prefix tuning, may not be as strong as previously believed, since it generates secure code but sacrifices functional correctness. We also demonstrate that different decoding methods significantly affect the security of Code LLMs. Furthermore, we explore a new defense direction: constrained decoding for secure code generation. We propose new constrained decoding techniques to generate code that satisfies security and correctness constraints simultaneously. Our results reveal that constrained decoding is more effective than prefix tuning to improve the security of Code LLMs, without requiring a specialized training dataset. Moreover, constrained decoding can be used together with prefix tuning to further improve the security of Code LLMs.</li>
</ul>

<h3>Title: Context-Aware Mobile Network Performance Prediction Using Network &  Remote Sensing Data</h3>
<ul>
<li><strong>Authors: </strong>Ali Shibli, Tahar Zanouda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00220">https://arxiv.org/abs/2405.00220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00220">https://arxiv.org/pdf/2405.00220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00220]] Context-Aware Mobile Network Performance Prediction Using Network &  Remote Sensing Data(https://arxiv.org/abs/2405.00220)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate estimation of Network Performance is crucial for several tasks in telecom networks. Telecom networks regularly serve a vast number of radio nodes. Each radio node provides services to end-users in the associated coverage areas. The task of predicting Network Performance for telecom networks necessitates considering complex spatio-temporal interactions and incorporating geospatial information where the radio nodes are deployed. Instead of relying on historical data alone, our approach augments network historical performance datasets with satellite imagery data. Our comprehensive experiments, using real-world data collected from multiple different regions of an operational network, show that the model is robust and can generalize across different scenarios. The results indicate that the model, utilizing satellite imagery, performs very well across the tested regions. Additionally, the model demonstrates a robust approach to the cold-start problem, offering a promising alternative for initial performance estimation in newly deployed sites.</li>
</ul>

<h3>Title: Synthetic Face Datasets Generation via Latent Space Exploration from  Brownian Identity Diffusion</h3>
<ul>
<li><strong>Authors: </strong>David Geissbühler, Hatef Otroshi Shahreza, Sébastien Marcel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00228">https://arxiv.org/abs/2405.00228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00228">https://arxiv.org/pdf/2405.00228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00228]] Synthetic Face Datasets Generation via Latent Space Exploration from  Brownian Identity Diffusion(https://arxiv.org/abs/2405.00228)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Face Recognition (FR) models are trained on large-scale datasets, which have privacy and ethical concerns. Lately, the use of synthetic data to complement or replace genuine data for the training of FR models has been proposed. While promising results have been obtained, it still remains unclear if generative models can yield diverse enough data for such tasks. In this work, we introduce a new method, inspired by the physical motion of soft particles subjected to stochastic Brownian forces, allowing us to sample identities distributions in a latent space under various constraints. With this in hands, we generate several face datasets and benchmark them by training FR models, showing that data generated with our method exceeds the performance of previously GAN-based datasets and achieves competitive performance with state-of-the-art diffusion-based synthetic datasets. We also show that this method can be used to mitigate leakage from the generator's training set and explore the ability of generative models to generate data beyond it.</li>
</ul>

<h3>Title: SemVecNet: Generalizable Vector Map Generation for Arbitrary Sensor  Configurations</h3>
<ul>
<li><strong>Authors: </strong>Narayanan Elavathur Ranganatha, Hengyuan Zhang, Shashank Venkatramani, Jing-Yan Liao, Henrik I. Christensen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00250">https://arxiv.org/abs/2405.00250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00250">https://arxiv.org/pdf/2405.00250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00250]] SemVecNet: Generalizable Vector Map Generation for Arbitrary Sensor  Configurations(https://arxiv.org/abs/2405.00250)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vector maps are essential in autonomous driving for tasks like localization and planning, yet their creation and maintenance are notably costly. While recent advances in online vector map generation for autonomous vehicles are promising, current models lack adaptability to different sensor configurations. They tend to overfit to specific sensor poses, leading to decreased performance and higher retraining costs. This limitation hampers their practical use in real-world applications. In response to this challenge, we propose a modular pipeline for vector map generation with improved generalization to sensor configurations. The pipeline leverages probabilistic semantic mapping to generate a bird's-eye-view (BEV) semantic map as an intermediate representation. This intermediate representation is then converted to a vector map using the MapTRv2 decoder. By adopting a BEV semantic map robust to different sensor configurations, our proposed approach significantly improves the generalization performance. We evaluate the model on datasets with sensor configurations not used during training. Our evaluation sets includes larger public datasets, and smaller scale private data collected on our platform. Our model generalizes significantly better than the state-of-the-art methods.</li>
</ul>

<h3>Title: Semantically Consistent Video Inpainting with Conditional Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Dylan Green, William Harvey, Saeid Naderiparizi, Matthew Niedoba, Yunpeng Liu, Xiaoxuan Liang, Jonathan Lavington, Ke Zhang, Vasileios Lioutas, Setareh Dabiri, Adam Scibior, Berend Zwartsenberg, Frank Wood</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00251">https://arxiv.org/abs/2405.00251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00251">https://arxiv.org/pdf/2405.00251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00251]] Semantically Consistent Video Inpainting with Conditional Diffusion  Models(https://arxiv.org/abs/2405.00251)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Current state-of-the-art methods for video inpainting typically rely on optical flow or attention-based approaches to inpaint masked regions by propagating visual information across frames. While such approaches have led to significant progress on standard benchmarks, they struggle with tasks that require the synthesis of novel content that is not present in other frames. In this paper we reframe video inpainting as a conditional generative modeling problem and present a framework for solving such problems with conditional video diffusion models. We highlight the advantages of using a generative approach for this task, showing that our method is capable of generating diverse, high-quality inpaintings and synthesizing new content that is spatially, temporally, and semantically consistent with the provided context.</li>
</ul>

<h3>Title: CodeHalu: Code Hallucinations in LLMs Driven by Execution-based  Verification</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Tian, Weixiang Yan, Qian Yang, Qian Chen, Wen Wang, Ziyang Luo, Lei Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00253">https://arxiv.org/abs/2405.00253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00253">https://arxiv.org/pdf/2405.00253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00253]] CodeHalu: Code Hallucinations in LLMs Driven by Execution-based  Verification(https://arxiv.org/abs/2405.00253)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have made significant advancements in the field of code generation, offering unprecedented support for automated programming and assisting developers. However, LLMs sometimes generate code that appears plausible but fails to meet the expected requirements or executes incorrectly. This phenomenon of hallucinations in the coding field has not been explored. To advance the community's understanding and research on code hallucinations in LLMs, we propose a definition method for these hallucinations based on execution verification and introduce the concept of code hallucinations for the first time. We categorize code hallucinations into four main types: mapping, naming, resource, and logic hallucinations, each further divided into different subcategories to better understand and address the unique challenges faced by LLMs during code generation. To systematically evaluate code hallucinations, we propose a dynamic detection algorithm for code hallucinations and construct the CodeHalu benchmark, which includes 8,883 samples from 699 tasks, to actively detect hallucination phenomena in LLMs during programming. We tested 16 popular LLMs on this benchmark to evaluate the frequency and nature of their hallucinations during code generation. The findings reveal significant variations in the accuracy and reliability of LLMs in generating code, highlighting the urgent need to improve models and training methods to ensure the functional correctness and safety of automatically generated code. This study not only classifies and quantifies code hallucinations but also provides insights for future improvements in LLM-based code generation research. The CodeHalu benchmark and code are publicly available at https://github.com/yuchen814/CodeHalu.</li>
</ul>

<h3>Title: ASAM: Boosting Segment Anything Model with Adversarial Tuning</h3>
<ul>
<li><strong>Authors: </strong>Bo Li, Haoke Xiao, Lv Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00256">https://arxiv.org/abs/2405.00256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00256">https://arxiv.org/pdf/2405.00256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00256]] ASAM: Boosting Segment Anything Model with Adversarial Tuning(https://arxiv.org/abs/2405.00256)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>In the evolving landscape of computer vision, foundation models have emerged as pivotal tools, exhibiting exceptional adaptability to a myriad of tasks. Among these, the Segment Anything Model (SAM) by Meta AI has distinguished itself in image segmentation. However, SAM, like its counterparts, encounters limitations in specific niche applications, prompting a quest for enhancement strategies that do not compromise its inherent capabilities. This paper introduces ASAM, a novel methodology that amplifies SAM's performance through adversarial tuning. We harness the potential of natural adversarial examples, inspired by their successful implementation in natural language processing. By utilizing a stable diffusion model, we augment a subset (1%) of the SA-1B dataset, generating adversarial instances that are more representative of natural variations rather than conventional imperceptible perturbations. Our approach maintains the photorealism of adversarial examples and ensures alignment with original mask annotations, thereby preserving the integrity of the segmentation task. The fine-tuned ASAM demonstrates significant improvements across a diverse range of segmentation tasks without necessitating additional data or architectural modifications. The results of our extensive evaluations confirm that ASAM establishes new benchmarks in segmentation tasks, thereby contributing to the advancement of foundational models in computer vision. Our project page is in https://asam2024.github.io/.</li>
</ul>

<h3>Title: Clover: Regressive Lightweight Speculative Decoding with Sequential  Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Bin Xiao, Chunan Shi, Xiaonan Nie, Fan Yang, Xiangwei Deng, Lei Su, Weipeng Chen, Bin Cui</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00263">https://arxiv.org/abs/2405.00263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00263">https://arxiv.org/pdf/2405.00263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00263]] Clover: Regressive Lightweight Speculative Decoding with Sequential  Knowledge(https://arxiv.org/abs/2405.00263)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) suffer from low efficiency as the mismatch between the requirement of auto-regressive decoding and the design of most contemporary GPUs. Specifically, billions to trillions of parameters must be loaded to the GPU cache through its limited memory bandwidth for computation, but only a small batch of tokens is actually computed. Consequently, the GPU spends most of its time on memory transfer instead of computation. Recently, parallel decoding, a type of speculative decoding algorithms, is becoming more popular and has demonstrated impressive efficiency improvement in generation. It introduces extra decoding heads to large models, enabling them to predict multiple subsequent tokens simultaneously and verify these candidate continuations in a single decoding step. However, this approach deviates from the training objective of next token prediction used during pre-training, resulting in a low hit rate for candidate tokens. In this paper, we propose a new speculative decoding algorithm, Clover, which integrates sequential knowledge into the parallel decoding process. This enhancement improves the hit rate of speculators and thus boosts the overall efficiency. Clover transmits the sequential knowledge from pre-speculated tokens via the Regressive Connection, then employs an Attention Decoder to integrate these speculated tokens. Additionally, Clover incorporates an Augmenting Block that modifies the hidden states to better align with the purpose of speculative generation rather than next token prediction. The experiment results demonstrate that Clover outperforms the baseline by up to 91% on Baichuan-Small and 146% on Baichuan-Large, respectively, and exceeds the performance of the previously top-performing method, Medusa, by up to 37% on Baichuan-Small and 57% on Baichuan-Large, respectively.</li>
</ul>

<h3>Title: Differentially Private Release of Israel's National Registry of Live  Births</h3>
<ul>
<li><strong>Authors: </strong>Shlomi Hod, Ran Canetti</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00267">https://arxiv.org/abs/2405.00267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00267">https://arxiv.org/pdf/2405.00267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00267]] Differentially Private Release of Israel's National Registry of Live  Births(https://arxiv.org/abs/2405.00267)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>In February 2024, Israel's Ministry of Health released microdata of live births in Israel in 2014. The dataset is based on Israel's National Registry of Live Births and offers substantial value in multiple areas, such as scientific research and policy-making. At the same time, the data was processed so as to protect the privacy of 2014's mothers and newborns. The release was co-designed by the authors together with stakeholders from both inside and outside the Ministry of Health. This paper presents the methodology used to obtain that release. It also describes the considerations involved in choosing the methodology and the process followed. We used differential privacy as our formal measure of the privacy loss incurred by the released dataset. More concretely, we prove that the released dataset is differentially private with privacy loss budget \varepsilon = 9.98. We extensively used the private selection algorithm of Liu and Talwar (STOC 2019) to bundle together multiple steps such as data transformation, model generation algorithm, hyperparameter selection, and evaluation. The model generation algorithm selected was PrivBayes (Zhang et al., SIGMOD 2014). The evaluation was based on a list of acceptance criteria, which were also disclosed only approximately so as to provide an overall differential privacy guarantee. We also discuss concrete challenges and barriers that appear relevant to the next steps of this pilot project, as well as to future differentially private releases.</li>
</ul>

<h3>Title: Social Life Simulation for Non-Cognitive Skills Learning</h3>
<ul>
<li><strong>Authors: </strong>Zihan Yan, Yaohong Xiang, Yun Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00273">https://arxiv.org/abs/2405.00273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00273">https://arxiv.org/pdf/2405.00273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00273]] Social Life Simulation for Non-Cognitive Skills Learning(https://arxiv.org/abs/2405.00273)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Non-cognitive skills are crucial for personal and social life well-being, and such skill development can be supported by narrative-based (e.g., storytelling) technologies. While generative AI enables interactive and role-playing storytelling, little is known about how users engage with and perceive the use of AI in social life simulation for non-cognitive skills learning. To this end, we introduced SimuLife++, an interactive platform enabled by a large language model (LLM). The system allows users to act as protagonists, creating stories with one or multiple AI-based characters in diverse social scenarios. In particular, we expanded the Human-AI interaction to a Human-AI-AI collaboration by including a sage agent, who acts as a bystander to provide users with more insightful perspectives on their choices and conversations. Through a within-subject user study, we found that the inclusion of the sage agent significantly enhanced narrative immersion, according to the narrative transportation scale, leading to more messages, particularly in group chats. Participants' interactions with the sage agent were also associated with significantly higher scores in their perceived motivation, self-perceptions, and resilience and coping, indicating positive impacts on non-cognitive skills reflection. Participants' interview results further explained the sage agent's aid in decision-making, solving ethical dilemmas, and problem-solving; on the other hand, they suggested improvements in user control and balanced responses from multiple characters. We provide design implications on the application of generative AI in narrative solutions for non-cognitive skill development in broader social contexts.</li>
</ul>

<h3>Title: Adversarial Attacks and Defense for Conversation Entailment Task</h3>
<ul>
<li><strong>Authors: </strong>Zhenning Yang, Ryan Krawec, Liang-Yuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00289">https://arxiv.org/abs/2405.00289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00289">https://arxiv.org/pdf/2405.00289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00289]] Adversarial Attacks and Defense for Conversation Entailment Task(https://arxiv.org/abs/2405.00289)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) that are proved to be very powerful on different NLP tasks. However, there are still many ways to attack the model with very low costs. How to defend the model becomes an important problem. In our work, we treat adversarial attack results as a new (unseen) domain of the model, and we frame the defending problem into how to improve the robustness of the model on the new domain. We focus on the task of conversation entailment, where multi-turn natural language dialogues are the premise, and the transformer model is fine-tuned to predict whether a given hypothesis about the given dialogue is true or false. The adversary would attack the hypothesis to fool the model to make the wrong predictions. We apply synonym-swapping as the attack method. To show the robustness of the model, we implement some fine-tuning strategies and propose the embedding perturbation loss as a method to improve the robustness of the model. Finally, we show the importance of our work by discussing the adversarial attacks in NLP in the real world.</li>
</ul>

<h3>Title: How Can I Improve? Using GPT to Highlight the Desired and Undesired  Parts of Open-ended Responses</h3>
<ul>
<li><strong>Authors: </strong>Jionghao Lin, Eason Chen, Zeifei Han, Ashish Gurung, Danielle R. Thomas, Wei Tan, Ngoc Dang Nguyen, Kenneth R. Koedinger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00291">https://arxiv.org/abs/2405.00291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00291">https://arxiv.org/pdf/2405.00291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00291]] How Can I Improve? Using GPT to Highlight the Desired and Undesired  Parts of Open-ended Responses(https://arxiv.org/abs/2405.00291)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Automated explanatory feedback systems play a crucial role in facilitating learning for a large cohort of learners by offering feedback that incorporates explanations, significantly enhancing the learning process. However, delivering such explanatory feedback in real-time poses challenges, particularly when high classification accuracy for domain-specific, nuanced responses is essential. Our study leverages the capabilities of large language models, specifically Generative Pre-Trained Transformers (GPT), to explore a sequence labeling approach focused on identifying components of desired and less desired praise for providing explanatory feedback within a tutor training dataset. Our aim is to equip tutors with actionable, explanatory feedback during online training lessons. To investigate the potential of GPT models for providing the explanatory feedback, we employed two commonly-used approaches: prompting and fine-tuning. To quantify the quality of highlighted praise components identified by GPT models, we introduced a Modified Intersection over Union (M-IoU) score. Our findings demonstrate that: (1) the M-IoU score effectively correlates with human judgment in evaluating sequence quality; (2) using two-shot prompting on GPT-3.5 resulted in decent performance in recognizing effort-based (M-IoU of 0.46) and outcome-based praise (M-IoU of 0.68); and (3) our optimally fine-tuned GPT-3.5 model achieved M-IoU scores of 0.64 for effort-based praise and 0.84 for outcome-based praise, aligning with the satisfaction levels evaluated by human coders. Our results show promise for using GPT models to provide feedback that focuses on specific elements in their open-ended responses that are desirable or could use improvement.</li>
</ul>

<h3>Title: The Reversing Machine: Reconstructing Memory Assumptions</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Sina Karvandi, Soroush Meghdadizanjani, Sima Arasteh, Saleh Khalaj Monfared, Mohammad K. Fallah, Saeid Gorgin, Jeong-A Lee, Erik van der Kouwe</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00298">https://arxiv.org/abs/2405.00298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00298">https://arxiv.org/pdf/2405.00298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00298]] The Reversing Machine: Reconstructing Memory Assumptions(https://arxiv.org/abs/2405.00298)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>Existing anti-malware software and reverse engineering toolkits struggle with stealthy sub-OS rootkits due to limitations of run-time kernel-level monitoring. A malicious kernel-level driver can bypass OS-level anti-virus mechanisms easily. Although static analysis of such malware is possible, obfuscation and packing techniques complicate offline analysis. Moreover, current dynamic analyzers suffer from virtualization performance overhead and create detectable traces that allow modern malware to evade them. To address these issues, we present \textit{The Reversing Machine} (TRM), a new hypervisor-based memory introspection design for reverse engineering, reconstructing memory offsets, and fingerprinting evasive and obfuscated user-level and kernel-level malware. TRM proposes two novel techniques that enable efficient and transparent analysis of evasive malware: hooking a binary using suspended process creation for hypervisor-based memory introspection, and leveraging Mode-Based Execution Control (MBEC) to detect user/kernel mode transitions and memory access patterns. Unlike existing malware detection environments, TRM can extract full memory traces in user and kernel spaces and hook the entire target memory map to reconstruct arrays, structures within the operating system, and possible rootkits. We perform TRM-assisted reverse engineering of kernel-level structures and show that it can speed up manual reverse engineering by 75\% on average. We obfuscate known malware with the latest packing tools and successfully perform similarity detection. Furthermore, we demonstrate a real-world attack by deploying a modified rootkit onto a driver that bypasses state-of-the-art security auditing tools. We show that TRM can detect each threat and that, out of 24 state-of-the-art AV solutions, only TRM can detect the most advanced threats.</li>
</ul>

<h3>Title: LITO: Learnable Intervention for Truthfulness Optimization</h3>
<ul>
<li><strong>Authors: </strong>Farima Fatahi Bayat, Xin Liu, H. V. Jagadish, Lu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00301">https://arxiv.org/abs/2405.00301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00301">https://arxiv.org/pdf/2405.00301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00301]] LITO: Learnable Intervention for Truthfulness Optimization(https://arxiv.org/abs/2405.00301)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can generate long-form and coherent text, but they still frequently hallucinate facts, thus limiting their reliability. To address this issue, inference-time methods that elicit truthful responses have been proposed by shifting LLM representations towards learned "truthful directions". However, applying the truthful directions with the same intensity fails to generalize across different question contexts. We propose LITO, a Learnable Intervention method for Truthfulness Optimization that automatically identifies the optimal intervention intensity tailored to a specific context. LITO explores a sequence of model generations based on increasing levels of intervention intensities. It selects the most accurate response or refuses to answer when the predictions are highly uncertain. Experiments on multiple LLMs and question-answering datasets demonstrate that LITO improves truthfulness while preserving task accuracy. The adaptive nature of LITO counters issues with one-size-fits-all intervention-based solutions, maximizing model truthfulness by reflecting internal knowledge only when the model is confident.</li>
</ul>

<h3>Title: Generating Feedback-Ladders for Logical Errors in Programming using  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hasnain Heickal, Andrew Lan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00302">https://arxiv.org/abs/2405.00302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00302">https://arxiv.org/pdf/2405.00302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00302]] Generating Feedback-Ladders for Logical Errors in Programming using  Large Language Models(https://arxiv.org/abs/2405.00302)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In feedback generation for logical errors in programming assignments, large language model (LLM)-based methods have shown great promise. These methods ask the LLM to generate feedback given the problem statement and a student's (buggy) submission. There are several issues with these types of methods. First, the generated feedback messages are often too direct in revealing the error in the submission and thus diminish valuable opportunities for the student to learn. Second, they do not consider the student's learning context, i.e., their previous submissions, current knowledge, etc. Third, they are not layered since existing methods use a single, shared prompt for all student submissions. In this paper, we explore using LLMs to generate a "feedback-ladder", i.e., multiple levels of feedback for the same problem-submission pair. We evaluate the quality of the generated feedback-ladder via a user study with students, educators, and researchers. We have observed diminishing effectiveness for higher-level feedback and higher-scoring submissions overall in the study. In practice, our method enables teachers to select an appropriate level of feedback to show to a student based on their personal learning context, or in a progressive manner to go more detailed if a higher-level feedback fails to correct the student's error.</li>
</ul>

<h3>Title: Streamlining Image Editing with Layered Diffusion Brushes</h3>
<ul>
<li><strong>Authors: </strong>Peyman Gholami, Robert Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00313">https://arxiv.org/abs/2405.00313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00313">https://arxiv.org/pdf/2405.00313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00313]] Streamlining Image Editing with Layered Diffusion Brushes(https://arxiv.org/abs/2405.00313)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Denoising diffusion models have recently gained prominence as powerful tools for a variety of image generation and manipulation tasks. Building on this, we propose a novel tool for real-time editing of images that provides users with fine-grained region-targeted supervision in addition to existing prompt-based controls. Our novel editing technique, termed Layered Diffusion Brushes, leverages prompt-guided and region-targeted alteration of intermediate denoising steps, enabling precise modifications while maintaining the integrity and context of the input image. We provide an editor based on Layered Diffusion Brushes modifications, which incorporates well-known image editing concepts such as layer masks, visibility toggles, and independent manipulation of layers; regardless of their order. Our system renders a single edit on a 512x512 image within 140 ms using a high-end consumer GPU, enabling real-time feedback and rapid exploration of candidate edits. We validated our method and editing system through a user study involving both natural images (using inversion) and generated images, showcasing its usability and effectiveness compared to existing techniques such as InstructPix2Pix and Stable Diffusion Inpainting for refining images. Our approach demonstrates efficacy across a range of tasks, including object attribute adjustments, error correction, and sequential prompt-based object placement and manipulation, demonstrating its versatility and potential for enhancing creative workflows.</li>
</ul>

<h3>Title: Model Quantization and Hardware Acceleration for Vision Transformers: A  Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Dayou Du, Gu Gong, Xiaowen Chu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.AR, cs.CV, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00314">https://arxiv.org/abs/2405.00314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00314">https://arxiv.org/pdf/2405.00314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00314]] Model Quantization and Hardware Acceleration for Vision Transformers: A  Comprehensive Survey(https://arxiv.org/abs/2405.00314)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs) have recently garnered considerable attention, emerging as a promising alternative to convolutional neural networks (CNNs) in several vision-related applications. However, their large model sizes and high computational and memory demands hinder deployment, especially on resource-constrained devices. This underscores the necessity of algorithm-hardware co-design specific to ViTs, aiming to optimize their performance by tailoring both the algorithmic structure and the underlying hardware accelerator to each other's strengths. Model quantization, by converting high-precision numbers to lower-precision, reduces the computational demands and memory needs of ViTs, allowing the creation of hardware specifically optimized for these quantized algorithms, boosting efficiency. This article provides a comprehensive survey of ViTs quantization and its hardware acceleration. We first delve into the unique architectural attributes of ViTs and their runtime characteristics. Subsequently, we examine the fundamental principles of model quantization, followed by a comparative analysis of the state-of-the-art quantization techniques for ViTs. Additionally, we explore the hardware acceleration of quantized ViTs, highlighting the importance of hardware-friendly algorithm design. In conclusion, this article will discuss ongoing challenges and future research paths. We consistently maintain the related open-source materials at https://github.com/DD-DuDa/awesome-vit-quantization-acceleration.</li>
</ul>

<h3>Title: Data Augmentation Policy Search for Long-Term Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Liran Nochumsohn, Omri Azencot</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00319">https://arxiv.org/abs/2405.00319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00319">https://arxiv.org/pdf/2405.00319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00319]] Data Augmentation Policy Search for Long-Term Forecasting(https://arxiv.org/abs/2405.00319)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Data augmentation serves as a popular regularization technique to combat overfitting challenges in neural networks. While automatic augmentation has demonstrated success in image classification tasks, its application to time-series problems, particularly in long-term forecasting, has received comparatively less attention. To address this gap, we introduce a time-series automatic augmentation approach named TSAA, which is both efficient and easy to implement. The solution involves tackling the associated bilevel optimization problem through a two-step process: initially training a non-augmented model for a limited number of epochs, followed by an iterative split procedure. During this iterative process, we alternate between identifying a robust augmentation policy through Bayesian optimization and refining the model while discarding suboptimal runs. Extensive evaluations on challenging univariate and multivariate forecasting benchmark problems demonstrate that TSAA consistently outperforms several robust baselines, suggesting its potential integration into prediction pipelines.</li>
</ul>

<h3>Title: DFKI-NLP at SemEval-2024 Task 2: Towards Robust LLMs Using Data  Perturbations and MinMax Training</h3>
<ul>
<li><strong>Authors: </strong>Bhuvanesh Verma, Lisa Raithel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00321">https://arxiv.org/abs/2405.00321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00321">https://arxiv.org/pdf/2405.00321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00321]] DFKI-NLP at SemEval-2024 Task 2: Towards Robust LLMs Using Data  Perturbations and MinMax Training(https://arxiv.org/abs/2405.00321)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The NLI4CT task at SemEval-2024 emphasizes the development of robust models for Natural Language Inference on Clinical Trial Reports (CTRs) using large language models (LLMs). This edition introduces interventions specifically targeting the numerical, vocabulary, and semantic aspects of CTRs. Our proposed system harnesses the capabilities of the state-of-the-art Mistral model, complemented by an auxiliary model, to focus on the intricate input space of the NLI4CT dataset. Through the incorporation of numerical and acronym-based perturbations to the data, we train a robust system capable of handling both semantic-altering and numerical contradiction interventions. Our analysis on the dataset sheds light on the challenging sections of the CTRs for reasoning.</li>
</ul>

<h3>Title: Metric geometry of the privacy-utility tradeoff</h3>
<ul>
<li><strong>Authors: </strong>March Boedihardjo, Thomas Strohmer, Roman Vershynin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DS, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00329">https://arxiv.org/abs/2405.00329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00329">https://arxiv.org/pdf/2405.00329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00329]] Metric geometry of the privacy-utility tradeoff(https://arxiv.org/abs/2405.00329)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Synthetic data are an attractive concept to enable privacy in data sharing. A fundamental question is how similar the privacy-preserving synthetic data are compared to the true data. Using metric privacy, an effective generalization of differential privacy beyond the discrete setting, we raise the problem of characterizing the optimal privacy-accuracy tradeoff by the metric geometry of the underlying space. We provide a partial solution to this problem in terms of the "entropic scale", a quantity that captures the multiscale geometry of a metric space via the behavior of its packing numbers. We illustrate the applicability of our privacy-accuracy tradeoff framework via a diverse set of examples of metric spaces.</li>
</ul>

<h3>Title: A Careful Examination of Large Language Model Performance on Grade  School Arithmetic</h3>
<ul>
<li><strong>Authors: </strong>Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele (Mike)Lunati, Summer Yue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00332">https://arxiv.org/abs/2405.00332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00332">https://arxiv.org/pdf/2405.00332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00332]] A Careful Examination of Large Language Model Performance on Grade  School Arithmetic(https://arxiv.org/abs/2405.00332)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved impressive success on many benchmarks for mathematical reasoning. However, there is growing concern that some of this performance actually reflects dataset contamination, where data closely resembling benchmark questions leaks into the training data, instead of true reasoning ability. To investigate this claim rigorously, we commission Grade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and complexity of the established GSM8k benchmark, the gold standard for measuring elementary mathematical reasoning. We ensure that the two benchmarks are comparable across important metrics such as human solve rates, number of steps in solution, answer magnitude, and more. When evaluating leading open- and closed-source LLMs on GSM1k, we observe accuracy drops of up to 13%, with several families of models (e.g., Phi and Mistral) showing evidence of systematic overfitting across almost all model sizes. At the same time, many models, especially those on the frontier, (e.g., Gemini/GPT/Claude) show minimal signs of overfitting. Further analysis suggests a positive relationship (Spearman's r^2=0.32) between a model's probability of generating an example from GSM8k and its performance gap between GSM8k and GSM1k, suggesting that many models may have partially memorized GSM8k.</li>
</ul>

<h3>Title: A Self-explaining Neural Architecture for Generalizable Concept Learning</h3>
<ul>
<li><strong>Authors: </strong>Sanchit Sinha, Guangzhi Xiong, Aidong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00349">https://arxiv.org/abs/2405.00349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00349">https://arxiv.org/pdf/2405.00349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00349]] A Self-explaining Neural Architecture for Generalizable Concept Learning(https://arxiv.org/abs/2405.00349)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>With the wide proliferation of Deep Neural Networks in high-stake applications, there is a growing demand for explainability behind their decision-making process. Concept learning models attempt to learn high-level 'concepts' - abstract entities that align with human understanding, and thus provide interpretability to DNN architectures. However, in this paper, we demonstrate that present SOTA concept learning approaches suffer from two major problems - lack of concept fidelity wherein the models fail to learn consistent concepts among similar classes and limited concept interoperability wherein the models fail to generalize learned concepts to new domains for the same task. Keeping these in mind, we propose a novel self-explaining architecture for concept learning across domains which - i) incorporates a new concept saliency network for representative concept selection, ii) utilizes contrastive learning to capture representative domain invariant concepts, and iii) uses a novel prototype-based concept grounding regularization to improve concept alignment across domains. We demonstrate the efficacy of our proposed approach over current SOTA concept learning approaches on four widely used real-world datasets. Empirical results show that our method improves both concept fidelity measured through concept overlap and concept interoperability measured through domain adaptation performance.</li>
</ul>

<h3>Title: CrossMatch: Enhance Semi-Supervised Medical Image Segmentation with  Perturbation Strategies and Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Bin Zhao, Chunshi Wang, Shuxue Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00354">https://arxiv.org/abs/2405.00354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00354">https://arxiv.org/pdf/2405.00354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00354]] CrossMatch: Enhance Semi-Supervised Medical Image Segmentation with  Perturbation Strategies and Knowledge Distillation(https://arxiv.org/abs/2405.00354)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Semi-supervised learning for medical image segmentation presents a unique challenge of efficiently using limited labeled data while leveraging abundant unlabeled data. Despite advancements, existing methods often do not fully exploit the potential of the unlabeled data for enhancing model robustness and accuracy. In this paper, we introduce CrossMatch, a novel framework that integrates knowledge distillation with dual perturbation strategies-image-level and feature-level-to improve the model's learning from both labeled and unlabeled data. CrossMatch employs multiple encoders and decoders to generate diverse data streams, which undergo self-knowledge distillation to enhance consistency and reliability of predictions across varied perturbations. Our method significantly surpasses other state-of-the-art techniques in standard benchmarks by effectively minimizing the gap between training on labeled and unlabeled data and improving edge accuracy and generalization in medical image segmentation. The efficacy of CrossMatch is demonstrated through extensive experimental validations, showing remarkable performance improvements without increasing computational costs. Code for this implementation is made available at https://github.com/AiEson/CrossMatch.git.</li>
</ul>

<h3>Title: Exploring Self-Supervised Vision Transformers for Deepfake Detection: A  Comparative Analysis</h3>
<ul>
<li><strong>Authors: </strong>Huy H. Nguyen, Junichi Yamagishi, Isao Echizen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00355">https://arxiv.org/abs/2405.00355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00355">https://arxiv.org/pdf/2405.00355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00355]] Exploring Self-Supervised Vision Transformers for Deepfake Detection: A  Comparative Analysis(https://arxiv.org/abs/2405.00355)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>This paper investigates the effectiveness of self-supervised pre-trained transformers compared to supervised pre-trained transformers and conventional neural networks (ConvNets) for detecting various types of deepfakes. We focus on their potential for improved generalization, particularly when training data is limited. Despite the notable success of large vision-language models utilizing transformer architectures in various tasks, including zero-shot and few-shot learning, the deepfake detection community has still shown some reluctance to adopt pre-trained vision transformers (ViTs), especially large ones, as feature extractors. One concern is their perceived excessive capacity, which often demands extensive data, and the resulting suboptimal generalization when training or fine-tuning data is small or less diverse. This contrasts poorly with ConvNets, which have already established themselves as robust feature extractors. Additionally, training and optimizing transformers from scratch requires significant computational resources, making this accessible primarily to large companies and hindering broader investigation within the academic community. Recent advancements in using self-supervised learning (SSL) in transformers, such as DINO and its derivatives, have showcased significant adaptability across diverse vision tasks and possess explicit semantic segmentation capabilities. By leveraging DINO for deepfake detection with modest training data and implementing partial fine-tuning, we observe comparable adaptability to the task and the natural explainability of the detection result via the attention mechanism. Moreover, partial fine-tuning of transformers for deepfake detection offers a more resource-efficient alternative, requiring significantly fewer computational resources.</li>
</ul>

<h3>Title: AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of  Low-Rank Adaptation Experts</h3>
<ul>
<li><strong>Authors: </strong>Zefang Liu, Jiahua Luo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00361">https://arxiv.org/abs/2405.00361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00361">https://arxiv.org/pdf/2405.00361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00361]] AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of  Low-Rank Adaptation Experts(https://arxiv.org/abs/2405.00361)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We introduce AdaMoLE, a novel method for fine-tuning large language models (LLMs) through an Adaptive Mixture of Low-Rank Adaptation (LoRA) Experts. Moving beyond conventional methods that employ a static top-k strategy for activating experts, AdaMoLE dynamically adjusts the activation threshold using a dedicated threshold network, adaptively responding to the varying complexities of different tasks. By replacing a single LoRA in a layer with multiple LoRA experts and integrating a gating function with the threshold mechanism, AdaMoLE effectively selects and activates the most appropriate experts based on the input context. Our extensive evaluations across a variety of commonsense reasoning and natural language processing tasks show that AdaMoLE exceeds baseline performance. This enhancement highlights the advantages of AdaMoLE's adaptive selection of LoRA experts, improving model effectiveness without a corresponding increase in the expert count. The experimental validation not only confirms AdaMoLE as a robust approach for enhancing LLMs but also suggests valuable directions for future research in adaptive expert selection mechanisms, potentially broadening the scope for optimizing model performance across diverse language processing tasks.</li>
</ul>

<h3>Title: Adaptive Bidirectional Displacement for Semi-Supervised Medical Image  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hanyang Chi, Jian Pang, Bingfeng Zhang, Weifeng Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00378">https://arxiv.org/abs/2405.00378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00378">https://arxiv.org/pdf/2405.00378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00378]] Adaptive Bidirectional Displacement for Semi-Supervised Medical Image  Segmentation(https://arxiv.org/abs/2405.00378)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Consistency learning is a central strategy to tackle unlabeled data in semi-supervised medical image segmentation (SSMIS), which enforces the model to produce consistent predictions under the perturbation. However, most current approaches solely focus on utilizing a specific single perturbation, which can only cope with limited cases, while employing multiple perturbations simultaneously is hard to guarantee the quality of consistency learning. In this paper, we propose an Adaptive Bidirectional Displacement (ABD) approach to solve the above challenge. Specifically, we first design a bidirectional patch displacement based on reliable prediction confidence for unlabeled data to generate new samples, which can effectively suppress uncontrollable regions and still retain the influence of input perturbations. Meanwhile, to enforce the model to learn the potentially uncontrollable content, a bidirectional displacement operation with inverse confidence is proposed for the labeled images, which generates samples with more unreliable information to facilitate model learning. Extensive experiments show that ABD achieves new state-of-the-art performances for SSMIS, significantly improving different baselines. Source code is available at https://github.com/chy-upc/ABD.</li>
</ul>

<h3>Title: CofiPara: A Coarse-to-fine Paradigm for Multimodal Sarcasm Target  Identification with Large Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Hongzhan Lin, Zixin Chen, Ziyang Luo, Mingfei Cheng, Jing Ma, Guang Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00390">https://arxiv.org/abs/2405.00390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00390">https://arxiv.org/pdf/2405.00390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00390]] CofiPara: A Coarse-to-fine Paradigm for Multimodal Sarcasm Target  Identification with Large Multimodal Models(https://arxiv.org/abs/2405.00390)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Social media abounds with multimodal sarcasm, and identifying sarcasm targets is particularly challenging due to the implicit incongruity not directly evident in the text and image modalities. Current methods for Multimodal Sarcasm Target Identification (MSTI) predominantly focus on superficial indicators in an end-to-end manner, overlooking the nuanced understanding of multimodal sarcasm conveyed through both the text and image. This paper proposes a versatile MSTI framework with a coarse-to-fine paradigm, by augmenting sarcasm explainability with reasoning and pre-training knowledge. Inspired by the powerful capacity of Large Multimodal Models (LMMs) on multimodal reasoning, we first engage LMMs to generate competing rationales for coarser-grained pre-training of a small language model on multimodal sarcasm detection. We then propose fine-tuning the model for finer-grained sarcasm target identification. Our framework is thus empowered to adeptly unveil the intricate targets within multimodal sarcasm and mitigate the negative impact posed by potential noise inherently in LMMs. Experimental results demonstrate that our model far outperforms state-of-the-art MSTI methods, and markedly exhibits explainability in deciphering sarcasm as well.</li>
</ul>

<h3>Title: Certified Adversarial Robustness of Machine Learning-based Malware  Detectors via (De)Randomized Smoothing</h3>
<ul>
<li><strong>Authors: </strong>Daniel Gibert, Luca Demetrio, Giulio Zizzo, Quan Le, Jordi Planes, Battista Biggio</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00392">https://arxiv.org/abs/2405.00392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00392">https://arxiv.org/pdf/2405.00392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00392]] Certified Adversarial Robustness of Machine Learning-based Malware  Detectors via (De)Randomized Smoothing(https://arxiv.org/abs/2405.00392)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep learning-based malware detection systems are vulnerable to adversarial EXEmples - carefully-crafted malicious programs that evade detection with minimal perturbation. As such, the community is dedicating effort to develop mechanisms to defend against adversarial EXEmples. However, current randomized smoothing-based defenses are still vulnerable to attacks that inject blocks of adversarial content. In this paper, we introduce a certifiable defense against patch attacks that guarantees, for a given executable and an adversarial patch size, no adversarial EXEmple exist. Our method is inspired by (de)randomized smoothing which provides deterministic robustness certificates. During training, a base classifier is trained using subsets of continguous bytes. At inference time, our defense splits the executable into non-overlapping chunks, classifies each chunk independently, and computes the final prediction through majority voting to minimize the influence of injected content. Furthermore, we introduce a preprocessing step that fixes the size of the sections and headers to a multiple of the chunk size. As a consequence, the injected content is confined to an integer number of chunks without tampering the other chunks containing the real bytes of the input examples, allowing us to extend our certified robustness guarantees to content insertion attacks. We perform an extensive ablation study, by comparing our defense with randomized smoothing-based defenses against a plethora of content manipulation attacks and neural network architectures. Results show that our method exhibits unmatched robustness against strong content-insertion attacks, outperforming randomized smoothing-based defenses in the literature.</li>
</ul>

<h3>Title: Inferring State Machine from the Protocol Implementation via Large  Langeuage Model</h3>
<ul>
<li><strong>Authors: </strong>Haiyang Wei, Zhengjie Du, Haohui Huang, Yue Liu, Guang Cheng, Linzhang Wang, Bing Mao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00393">https://arxiv.org/abs/2405.00393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00393">https://arxiv.org/pdf/2405.00393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00393]] Inferring State Machine from the Protocol Implementation via Large  Langeuage Model(https://arxiv.org/abs/2405.00393)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>State machines play a pivotal role in augmenting the efficacy of protocol analyzing to unveil more vulnerabilities. However, the task of inferring state machines from network protocol implementations presents significant challenges. Traditional methods based on dynamic analysis often overlook crucial state transitions due to limited coverage, while static analysis faces difficulties with complex code structures and behaviors. To address these limitations, we propose an innovative state machine inference approach powered by Large Language Models (LLMs). Utilizing text-embedding technology, this method allows LLMs to dissect and analyze the intricacies of protocol implementation code. Through targeted prompt engineering, we systematically identify and infer the underlying state machines. Our evaluation across six protocol implementations demonstrates the method's high efficacy, achieving an accuracy rate exceeding 90% and successfully delineating differences on state machines among various implementations of the same protocol. Importantly, integrating this approach with protocol fuzzing has notably enhanced AFLNet's code coverage by 10% over RFCNLP, showcasing the considerable potential of LLMs in advancing network protocol security analysis. Our proposed method not only marks a significant step forward in accurate state machine inference but also opens new avenues for improving the security and reliability of protocol implementations.</li>
</ul>

<h3>Title: Trust Driven On-Demand Scheme for Client Deployment in Federated  Learning</h3>
<ul>
<li><strong>Authors: </strong>Mario Chahoud, Azzam Mourad, Hadi Otrok, Jamal Bentahar, Mohsen Guizani</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00395">https://arxiv.org/abs/2405.00395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00395">https://arxiv.org/pdf/2405.00395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00395]] Trust Driven On-Demand Scheme for Client Deployment in Federated  Learning(https://arxiv.org/abs/2405.00395)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, federate</a></li>
<li><strong>Abstract: </strong>Containerization technology plays a crucial role in Federated Learning (FL) setups, expanding the pool of potential clients and ensuring the availability of specific subsets for each learning iteration. However, doubts arise about the trustworthiness of devices deployed as clients in FL scenarios, especially when container deployment processes are involved. Addressing these challenges is important, particularly in managing potentially malicious clients capable of disrupting the learning process or compromising the entire model. In our research, we are motivated to integrate a trust element into the client selection and model deployment processes within our system architecture. This is a feature lacking in the initial client selection and deployment mechanism of the On-Demand architecture. We introduce a trust mechanism, named "Trusted-On-Demand-FL", which establishes a relationship of trust between the server and the pool of eligible clients. Utilizing Docker in our deployment strategy enables us to monitor and validate participant actions effectively, ensuring strict adherence to agreed-upon protocols while strengthening defenses against unauthorized data access or tampering. Our simulations rely on a continuous user behavior dataset, deploying an optimization model powered by a genetic algorithm to efficiently select clients for participation. By assigning trust values to individual clients and dynamically adjusting these values, combined with penalizing malicious clients through decreased trust scores, our proposed framework identifies and isolates harmful clients. This approach not only reduces disruptions to regular rounds but also minimizes instances of round dismissal, Consequently enhancing both system stability and security.</li>
</ul>

<h3>Title: Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Ranaldi, Andrè Freitas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00402">https://arxiv.org/abs/2405.00402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00402">https://arxiv.org/pdf/2405.00402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00402]] Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models(https://arxiv.org/abs/2405.00402)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches deliver more performant models, they do not show sufficiently strong generalization ability as the training only relies on the provided demonstrations. In this paper, we propose the Self-refine Instruction-tuning method that elicits Smaller Language Models to self-refine their abilities. Our approach is based on a two-stage process, where reasoning abilities are first transferred between LLMs and Small Language Models (SLMs) via Instruction-tuning on demonstrations provided by LLMs, and then the instructed models Self-refine their abilities through preference optimization strategies. In particular, the second phase operates refinement heuristics based on the Direct Preference Optimization algorithm, where the SLMs are elicited to deliver a series of reasoning paths by automatically sampling the generated responses and providing rewards using ground truths from the LLMs. Results obtained on commonsense and math reasoning tasks show that this approach significantly outperforms Instruction-tuning in both in-domain and out-domain scenarios, aligning the reasoning abilities of Smaller and Larger Language Models.</li>
</ul>

<h3>Title: Detection of ransomware attacks using federated learning based on the  CNN model</h3>
<ul>
<li><strong>Authors: </strong>Hong-Nhung Nguyen, Ha-Thanh Nguyen, Damien Lescos</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00418">https://arxiv.org/abs/2405.00418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00418">https://arxiv.org/pdf/2405.00418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00418]] Detection of ransomware attacks using federated learning based on the  CNN model(https://arxiv.org/abs/2405.00418)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, federate</a></li>
<li><strong>Abstract: </strong>Computing is still under a significant threat from ransomware, which necessitates prompt action to prevent it. Ransomware attacks can have a negative impact on how smart grids, particularly digital substations. In addition to examining a ransomware detection method using artificial intelligence (AI), this paper offers a ransomware attack modeling technique that targets the disrupted operation of a digital substation. The first, binary data is transformed into image data and fed into the convolution neural network model using federated learning. The experimental findings demonstrate that the suggested technique detects ransomware with a high accuracy rate.</li>
</ul>

<h3>Title: On the Potential of RIS in the Context of PLA in Wireless Communication  Systems</h3>
<ul>
<li><strong>Authors: </strong>Hama Amin, Waqas Aman, Saif Al-Kuwari</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00426">https://arxiv.org/abs/2405.00426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00426">https://arxiv.org/pdf/2405.00426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00426]] On the Potential of RIS in the Context of PLA in Wireless Communication  Systems(https://arxiv.org/abs/2405.00426)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Re-configurable Intelligent Surfaces (RIS) technology has proven itself a promising candidate for the next generation of wireless networks through its enhanced performance in terms of throughput, spectral, and energy efficiency. However, the broadcast nature of RIS-assisted wireless communication makes it vulnerable to malicious attacks at the physical layer. On the other hand, physical layer authentication is an emerging area in the security domain to thwart different attacks such as cloning, spoofing, and impersonation by using the random features of the physical layer. In this paper, we investigate RIS-assisted wireless communication systems to unlock the potential of using RIS for physical layer authentication (PLA). Specifically, we exploit two distinct features of the physical layer: pathloss and channel impulse response (CIR) for PLA in RIS-assisted wireless communication. We construct hypothesis tests for the estimated features and derive the closed-form errors' expressions. Further, we chose the critical error, i.e., missed detection as our objective function for minimization by optimizing the phase shift of the RIS pannel. We compare the performance of our proposed mechanisms with baseline mechanisms which are PLA schemes using the same features but with no RIS assistance. Furthermore, we thoroughly evaluate our proposed schemes using performance metrics such as the probability of false alarm (PFA), the probability of missed detection (PMD), and the receiver operating characteristic (ROC) curves. The results demonstrate the significant positive impact of RIS on PLA, as it effectively reduces PMD values to zero when determining the optimal phase shift.</li>
</ul>

<h3>Title: Detail-Enhancing Framework for Reference-Based Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Zihan Wang, Ziliang Xiong, Hongying Tang, Xiaobing Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00431">https://arxiv.org/abs/2405.00431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00431">https://arxiv.org/pdf/2405.00431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00431]] Detail-Enhancing Framework for Reference-Based Image Super-Resolution(https://arxiv.org/abs/2405.00431)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent years have witnessed the prosperity of reference-based image super-resolution (Ref-SR). By importing the high-resolution (HR) reference images into the single image super-resolution (SISR) approach, the ill-posed nature of this long-standing field has been alleviated with the assistance of texture transferred from reference images. Although the significant improvement in quantitative and qualitative results has verified the superiority of Ref-SR methods, the presence of misalignment before texture transfer indicates room for further performance improvement. Existing methods tend to neglect the significance of details in the context of comparison, therefore not fully leveraging the information contained within low-resolution (LR) images. In this paper, we propose a Detail-Enhancing Framework (DEF) for reference-based super-resolution, which introduces the diffusion model to generate and enhance the underlying detail in LR images. If corresponding parts are present in the reference image, our method can facilitate rigorous alignment. In cases where the reference image lacks corresponding parts, it ensures a fundamental improvement while avoiding the influence of the reference image. Extensive experiments demonstrate that our proposed method achieves superior visual results while maintaining comparable numerical outcomes.</li>
</ul>

<h3>Title: Modeling Linear and Non-linear Layers: An MILP Approach Towards Finding  Differential and Impossible Differential Propagations</h3>
<ul>
<li><strong>Authors: </strong>Debranjan Pal, Vishal Pankaj Chandratreya, Abhijit Das, Dipanwita Roy Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00441">https://arxiv.org/abs/2405.00441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00441">https://arxiv.org/pdf/2405.00441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00441]] Modeling Linear and Non-linear Layers: An MILP Approach Towards Finding  Differential and Impossible Differential Propagations(https://arxiv.org/abs/2405.00441)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Symmetric key cryptography stands as a fundamental cornerstone in ensuring security within contemporary electronic communication frameworks. The cryptanalysis of classical symmetric key ciphers involves traditional methods and techniques aimed at breaking or analyzing these cryptographic systems. In the evaluation of new ciphers, the resistance against linear and differential cryptanalysis is commonly a key design criterion. The wide trail design technique for block ciphers facilitates the demonstration of security against linear and differential cryptanalysis. Assessing the scheme's security against differential attacks often involves determining the minimum number of active SBoxes for all rounds of a cipher. The propagation characteristics of a cryptographic component, such as an SBox, can be expressed using Boolean functions. Mixed Integer Linear Programming (MILP) proves to be a valuable technique for solving Boolean functions. We formulate a set of inequalities to model a Boolean function, which is subsequently solved by an MILP solver. To efficiently model a Boolean function and select a minimal set of inequalities, two key challenges must be addressed. We propose algorithms to address the second challenge, aiming to find more optimized linear and non-linear components. Our approaches are applied to modeling SBoxes (up to six bits) and EXOR operations with any number of inputs. Additionally, we introduce an MILP-based automatic tool for exploring differential and impossible differential propagations within a cipher. The tool is successfully applied to five lightweight block ciphers: Lilliput, GIFT64, SKINNY64, Klein, and MIBS.</li>
</ul>

<h3>Title: MMTryon: Multi-Modal Multi-Reference Control for High-Quality Fashion  Generation</h3>
<ul>
<li><strong>Authors: </strong>Xujie Zhang, Ente Lin, Xiu Li, Yuxuan Luo, Michael Kampffmeyer, Xin Dong, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00448">https://arxiv.org/abs/2405.00448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00448">https://arxiv.org/pdf/2405.00448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00448]] MMTryon: Multi-Modal Multi-Reference Control for High-Quality Fashion  Generation(https://arxiv.org/abs/2405.00448)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper introduces MMTryon, a multi-modal multi-reference VIrtual Try-ON (VITON) framework, which can generate high-quality compositional try-on results by taking as inputs a text instruction and multiple garment images. Our MMTryon mainly addresses two problems overlooked in prior literature: 1) Support of multiple try-on items and dressing styleExisting methods are commonly designed for single-item try-on tasks (e.g., upper/lower garments, dresses) and fall short on customizing dressing styles (e.g., zipped/unzipped, tuck-in/tuck-out, etc.) 2) Segmentation Dependency. They further heavily rely on category-specific segmentation models to identify the replacement regions, with segmentation errors directly leading to significant artifacts in the try-on results. For the first issue, our MMTryon introduces a novel multi-modality and multi-reference attention mechanism to combine the garment information from reference images and dressing-style information from text instructions. Besides, to remove the segmentation dependency, MMTryon uses a parsing-free garment encoder and leverages a novel scalable data generation pipeline to convert existing VITON datasets to a form that allows MMTryon to be trained without requiring any explicit segmentation. Extensive experiments on high-resolution benchmarks and in-the-wild test sets demonstrate MMTryon's superiority over existing SOTA methods both qualitatively and quantitatively. Besides, MMTryon's impressive performance on multi-items and style-controllable virtual try-on scenarios and its ability to try on any outfit in a large variety of scenarios from any source image, opens up a new avenue for future investigation in the fashion community.</li>
</ul>

<h3>Title: RAG-based Explainable Prediction of Road Users Behaviors for Automated  Driving using Knowledge Graphs and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Manzour Hussien, Angie Nataly Melo, Augusto Luis Ballardini, Carlota Salinas Maldonado, Rubén Izquierdo, Miguel Ángel Sotelo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.IR, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00449">https://arxiv.org/abs/2405.00449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00449">https://arxiv.org/pdf/2405.00449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00449]] RAG-based Explainable Prediction of Road Users Behaviors for Automated  Driving using Knowledge Graphs and Large Language Models(https://arxiv.org/abs/2405.00449)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prediction of road users' behaviors in the context of autonomous driving has gained considerable attention by the scientific community in the last years. Most works focus on predicting behaviors based on kinematic information alone, a simplification of the reality since road users are humans, and as such they are highly influenced by their surrounding context. In addition, a large plethora of research works rely on powerful Deep Learning techniques, which exhibit high performance metrics in prediction tasks but may lack the ability to fully understand and exploit the contextual semantic information contained in the road scene, not to mention their inability to provide explainable predictions that can be understood by humans. In this work, we propose an explainable road users' behavior prediction system that integrates the reasoning abilities of Knowledge Graphs (KG) and the expressiveness capabilities of Large Language Models (LLM) by using Retrieval Augmented Generation (RAG) techniques. For that purpose, Knowledge Graph Embeddings (KGE) and Bayesian inference are combined to allow the deployment of a fully inductive reasoning system that enables the issuing of predictions that rely on legacy information contained in the graph as well as on current evidence gathered in real time by onboard sensors. Two use cases have been implemented following the proposed approach: 1) Prediction of pedestrians' crossing actions; 2) Prediction of lane change maneuvers. In both cases, the performance attained surpasses the current state of the art in terms of anticipation and F1-score, showing a promising avenue for future research in this field.</li>
</ul>

<h3>Title: Predictive Accuracy-Based Active Learning for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jun Shi, Shulan Ruan, Ziqi Zhu, Minfan Zhao, Hong An, Xudong Xue, Bing Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00452">https://arxiv.org/abs/2405.00452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00452">https://arxiv.org/pdf/2405.00452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00452]] Predictive Accuracy-Based Active Learning for Medical Image Segmentation(https://arxiv.org/abs/2405.00452)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Active learning is considered a viable solution to alleviate the contradiction between the high dependency of deep learning-based segmentation methods on annotated data and the expensive pixel-level annotation cost of medical images. However, most existing methods suffer from unreliable uncertainty assessment and the struggle to balance diversity and informativeness, leading to poor performance in segmentation tasks. In response, we propose an efficient Predictive Accuracy-based Active Learning (PAAL) method for medical image segmentation, first introducing predictive accuracy to define uncertainty. Specifically, PAAL mainly consists of an Accuracy Predictor (AP) and a Weighted Polling Strategy (WPS). The former is an attached learnable module that can accurately predict the segmentation accuracy of unlabeled samples relative to the target model with the predicted posterior probability. The latter provides an efficient hybrid querying scheme by combining predicted accuracy and feature representation, aiming to ensure the uncertainty and diversity of the acquired samples. Extensive experiment results on multiple datasets demonstrate the superiority of PAAL. PAAL achieves comparable accuracy to fully annotated data while reducing annotation costs by approximately 50% to 80%, showcasing significant potential in clinical applications. The code is available at https://github.com/shijun18/PAAL-MedSeg.</li>
</ul>

<h3>Title: Robust Semi-supervised Learning via $f$-Divergence and $α$-Rényi  Divergence</h3>
<ul>
<li><strong>Authors: </strong>Gholamali Aminian, Amirhossien Bagheri, Mahyar JafariNodeh, Radmehr Karimian, Mohammad-Hossein Yassaee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00454">https://arxiv.org/abs/2405.00454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00454">https://arxiv.org/pdf/2405.00454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00454]] Robust Semi-supervised Learning via $f$-Divergence and $α$-Rényi  Divergence(https://arxiv.org/abs/2405.00454)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper investigates a range of empirical risk functions and regularization methods suitable for self-training methods in semi-supervised learning. These approaches draw inspiration from various divergence measures, such as $f$-divergences and $\alpha$-R\'enyi divergences. Inspired by the theoretical foundations rooted in divergences, i.e., $f$-divergences and $\alpha$-R\'enyi divergence, we also provide valuable insights to enhance the understanding of our empirical risk functions and regularization techniques. In the pseudo-labeling and entropy minimization techniques as self-training methods for effective semi-supervised learning, the self-training process has some inherent mismatch between the true label and pseudo-label (noisy pseudo-labels) and some of our empirical risk functions are robust, concerning noisy pseudo-labels. Under some conditions, our empirical risk functions demonstrate better performance when compared to traditional self-training methods.</li>
</ul>

<h3>Title: Counterfactual Explanations for Deep Learning-Based Traffic Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Rushan Wang, Yanan Xin, Yatao Zhang, Fernando Perez-Cruz, Martin Raubal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00456">https://arxiv.org/abs/2405.00456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00456">https://arxiv.org/pdf/2405.00456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00456]] Counterfactual Explanations for Deep Learning-Based Traffic Forecasting(https://arxiv.org/abs/2405.00456)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Deep learning models are widely used in traffic forecasting and have achieved state-of-the-art prediction accuracy. However, the black-box nature of those models makes the results difficult to interpret by users. This study aims to leverage an Explainable AI approach, counterfactual explanations, to enhance the explainability and usability of deep learning-based traffic forecasting models. Specifically, the goal is to elucidate relationships between various input contextual features and their corresponding predictions. We present a comprehensive framework that generates counterfactual explanations for traffic forecasting and provides usable insights through the proposed scenario-driven counterfactual explanations. The study first implements a deep learning model to predict traffic speed based on historical traffic data and contextual variables. Counterfactual explanations are then used to illuminate how alterations in these input variables affect predicted outcomes, thereby enhancing the transparency of the deep learning model. We investigated the impact of contextual features on traffic speed prediction under varying spatial and temporal conditions. The scenario-driven counterfactual explanations integrate two types of user-defined constraints, directional and weighting constraints, to tailor the search for counterfactual explanations to specific use cases. These tailored explanations benefit machine learning practitioners who aim to understand the model's learning mechanisms and domain experts who seek insights for real-world applications. The results showcase the effectiveness of counterfactual explanations in revealing traffic patterns learned by deep learning models, showing its potential for interpreting black-box deep learning models used for spatiotemporal predictions in general.</li>
</ul>

<h3>Title: BiomedRAG: A Retrieval Augmented Large Language Model for Biomedicine</h3>
<ul>
<li><strong>Authors: </strong>Mingchen Li, Halil Kilicoglu, Hua Xu, Rui Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00465">https://arxiv.org/abs/2405.00465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00465">https://arxiv.org/pdf/2405.00465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00465]] BiomedRAG: A Retrieval Augmented Large Language Model for Biomedicine(https://arxiv.org/abs/2405.00465)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have swiftly emerged as vital resources for different applications in the biomedical and healthcare domains; however, these models encounter issues such as generating inaccurate information or hallucinations. Retrieval-augmented generation provided a solution for these models to update knowledge and enhance their performance. In contrast to previous retrieval-augmented LMs, which utilize specialized cross-attention mechanisms to help LLM encode retrieved text, BiomedRAG adopts a simpler approach by directly inputting the retrieved chunk-based documents into the LLM. This straightforward design is easily applicable to existing retrieval and language models, effectively bypassing noise information in retrieved documents, particularly in noise-intensive tasks. Moreover, we demonstrate the potential for utilizing the LLM to supervise the retrieval model in the biomedical domain, enabling it to retrieve the document that assists the LM in improving its predictions. Our experiments reveal that with the tuned scorer,\textsc{ BiomedRAG} attains superior performance across 5 biomedical NLP tasks, encompassing information extraction (triple extraction, relation extraction), text classification, link prediction, and question-answering, leveraging over 9 datasets. For instance, in the triple extraction task, \textsc{BiomedRAG} outperforms other triple extraction systems with micro-F1 scores of 81.42 and 88.83 on GIT and ChemProt corpora, respectively.</li>
</ul>

<h3>Title: Lazy Layers to Make Fine-Tuned Diffusion Models More Traceable</h3>
<ul>
<li><strong>Authors: </strong>Haozhe Liu, Wentian Zhang, Bing Li, Bernard Ghanem, Jürgen Schmidhuber</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00466">https://arxiv.org/abs/2405.00466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00466">https://arxiv.org/pdf/2405.00466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00466]] Lazy Layers to Make Fine-Tuned Diffusion Models More Traceable(https://arxiv.org/abs/2405.00466)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, watermark, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Foundational generative models should be traceable to protect their owners and facilitate safety regulation. To achieve this, traditional approaches embed identifiers based on supervisory trigger-response signals, which are commonly known as backdoor watermarks. They are prone to failure when the model is fine-tuned with nontrigger data. Our experiments show that this vulnerability is due to energetic changes in only a few 'busy' layers during fine-tuning. This yields a novel arbitrary-in-arbitrary-out (AIAO) strategy that makes watermarks resilient to fine-tuning-based removal. The trigger-response pairs of AIAO samples across various neural network depths can be used to construct watermarked subpaths, employing Monte Carlo sampling to achieve stable verification results. In addition, unlike the existing methods of designing a backdoor for the input/output space of diffusion models, in our method, we propose to embed the backdoor into the feature space of sampled subpaths, where a mask-controlled trigger function is proposed to preserve the generation performance and ensure the invisibility of the embedded backdoor. Our empirical studies on the MS-COCO, AFHQ, LSUN, CUB-200, and DreamBooth datasets confirm the robustness of AIAO; while the verification rates of other trigger-based methods fall from ~90% to ~70% after fine-tuning, those of our method remain consistently above 90%.</li>
</ul>

<h3>Title: Harnessing the Power of Multiple Minds: Lessons Learned from LLM Routing</h3>
<ul>
<li><strong>Authors: </strong>KV Aditya Srivatsa, Kaushal Kumar Maurya, Ekaterina Kochmar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00467">https://arxiv.org/abs/2405.00467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00467">https://arxiv.org/pdf/2405.00467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00467]] Harnessing the Power of Multiple Minds: Lessons Learned from LLM Routing(https://arxiv.org/abs/2405.00467)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the rapid development of LLMs, it is natural to ask how to harness their capabilities efficiently. In this paper, we explore whether it is feasible to direct each input query to a single most suitable LLM. To this end, we propose LLM routing for challenging reasoning tasks. Our extensive experiments suggest that such routing shows promise but is not feasible in all scenarios, so more robust approaches should be investigated to fill this gap.</li>
</ul>

<h3>Title: Enhanced Visual Question Answering: A Comparative Analysis and Textual  Feature Extraction Via Convolutions</h3>
<ul>
<li><strong>Authors: </strong>Zhilin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00479">https://arxiv.org/abs/2405.00479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00479">https://arxiv.org/pdf/2405.00479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00479]] Enhanced Visual Question Answering: A Comparative Analysis and Textual  Feature Extraction Via Convolutions(https://arxiv.org/abs/2405.00479)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Visual Question Answering (VQA) has emerged as a highly engaging field in recent years, attracting increasing research efforts aiming to enhance VQA accuracy through the deployment of advanced models such as Transformers. Despite this growing interest, there has been limited exploration into the comparative analysis and impact of textual modalities within VQA, particularly in terms of model complexity and its effect on performance. In this work, we conduct a comprehensive comparison between complex textual models that leverage long dependency mechanisms and simpler models focusing on local textual features within a well-established VQA framework. Our findings reveal that employing complex textual encoders is not invariably the optimal approach for the VQA-v2 dataset. Motivated by this insight, we introduce an improved model, ConvGRU, which incorporates convolutional layers to enhance the representation of question text. Tested on the VQA-v2 dataset, ConvGRU achieves better performance without substantially increasing parameter complexity.</li>
</ul>

<h3>Title: PackVFL: Efficient HE Packing for Vertical Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Liu Yang, Shuowei Cai, Di Chai, Junxue Zhang, Han Tian, Yilun Jin, Kun Guo, Kai Chen, Qiang Yang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00482">https://arxiv.org/abs/2405.00482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00482">https://arxiv.org/pdf/2405.00482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00482]] PackVFL: Efficient HE Packing for Vertical Federated Learning(https://arxiv.org/abs/2405.00482)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, federate</a></li>
<li><strong>Abstract: </strong>As an essential tool of secure distributed machine learning, vertical federated learning (VFL) based on homomorphic encryption (HE) suffers from severe efficiency problems due to data inflation and time-consuming operations. To this core, we propose PackVFL, an efficient VFL framework based on packed HE (PackedHE), to accelerate the existing HE-based VFL algorithms. PackVFL packs multiple cleartexts into one ciphertext and supports single-instruction-multiple-data (SIMD)-style parallelism. We focus on designing a high-performant matrix multiplication (MatMult) method since it takes up most of the ciphertext computation time in HE-based VFL. Besides, devising the MatMult method is also challenging for PackedHE because a slight difference in the packing way could predominantly affect its computation and communication costs. Without domain-specific design, directly applying SOTA MatMult methods is hard to achieve optimal. Therefore, we make a three-fold design: 1) we systematically explore the current design space of MatMult and quantify the complexity of existing approaches to provide guidance; 2) we propose a hybrid MatMult method according to the unique characteristics of VFL; 3) we adaptively apply our hybrid method in representative VFL algorithms, leveraging distinctive algorithmic properties to further improve efficiency. As the batch size, feature dimension and model size of VFL scale up to large sizes, PackVFL consistently delivers enhanced performance. Empirically, PackVFL propels existing VFL algorithms to new heights, achieving up to a 51.52X end-to-end speedup. This represents a substantial 34.51X greater speedup compared to the direct application of SOTA MatMult methods.</li>
</ul>

<h3>Title: In Anticipation of Perfect Deepfake: Identity-anchored Artifact-agnostic  Detection under Rebalanced Deepfake Detection Protocol</h3>
<ul>
<li><strong>Authors: </strong>Wei-Han Wang, Chin-Yuan Yeh, Hsi-Wen Chen, De-Nian Yang, Ming-Syan Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00483">https://arxiv.org/abs/2405.00483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00483">https://arxiv.org/pdf/2405.00483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00483]] In Anticipation of Perfect Deepfake: Identity-anchored Artifact-agnostic  Detection under Rebalanced Deepfake Detection Protocol(https://arxiv.org/abs/2405.00483)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As deep generative models advance, we anticipate deepfakes achieving "perfection"-generating no discernible artifacts or noise. However, current deepfake detectors, intentionally or inadvertently, rely on such artifacts for detection, as they are exclusive to deepfakes and absent in genuine examples. To bridge this gap, we introduce the Rebalanced Deepfake Detection Protocol (RDDP) to stress-test detectors under balanced scenarios where genuine and forged examples bear similar artifacts. We offer two RDDP variants: RDDP-WHITEHAT uses white-hat deepfake algorithms to create 'self-deepfakes,' genuine portrait videos with the resemblance of the underlying identity, yet carry similar artifacts to deepfake videos; RDDP-SURROGATE employs surrogate functions (e.g., Gaussian noise) to process both genuine and forged examples, introducing equivalent noise, thereby sidestepping the need of deepfake algorithms. Towards detecting perfect deepfake videos that aligns with genuine ones, we present ID-Miner, a detector that identifies the puppeteer behind the disguise by focusing on motion over artifacts or appearances. As an identity-based detector, it authenticates videos by comparing them with reference footage. Equipped with the artifact-agnostic loss at frame-level and the identity-anchored loss at video-level, ID-Miner effectively singles out identity signals amidst distracting variations. Extensive experiments comparing ID-Miner with 12 baseline detectors under both conventional and RDDP evaluations with two deepfake datasets, along with additional qualitative studies, affirm the superiority of our method and the necessity for detectors designed to counter perfect deepfakes.</li>
</ul>

<h3>Title: The Pyramid of Captions</h3>
<ul>
<li><strong>Authors: </strong>Delong Chen, Samuel Cahyawijaya, Etsuko Ishii, Ho Shu Chan, Yejin Bang, Pascale Fung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00485">https://arxiv.org/abs/2405.00485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00485">https://arxiv.org/pdf/2405.00485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00485]] The Pyramid of Captions(https://arxiv.org/abs/2405.00485)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>We introduce a formal information-theoretic framework for image captioning by regarding it as a representation learning task. Our framework defines three key objectives: task sufficiency, minimal redundancy, and human interpretability. Building upon this foundation, we propose a novel Pyramid of Captions (PoCa) method, which constructs caption pyramids by generating localized captions for zoomed-in image patches and integrating them with global caption information using large language models. This approach leverages intuition that the detailed examination of local patches can reduce error risks and address inaccuracies in global captions, either by correcting the hallucination or adding missing details. Based on our theoretical framework, we formalize this intuition and provide formal proof demonstrating the effectiveness of PoCa under certain assumptions. Empirical tests with various image captioning models and large language models show that PoCa consistently yields more informative and semantically aligned captions, maintaining brevity and interpretability.</li>
</ul>

<h3>Title: Explainable Automatic Grading with Neural Additive Models</h3>
<ul>
<li><strong>Authors: </strong>Aubrey Condor, Zachary Pardos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00489">https://arxiv.org/abs/2405.00489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00489">https://arxiv.org/pdf/2405.00489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00489]] Explainable Automatic Grading with Neural Additive Models(https://arxiv.org/abs/2405.00489)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>The use of automatic short answer grading (ASAG) models may help alleviate the time burden of grading while encouraging educators to frequently incorporate open-ended items in their curriculum. However, current state-of-the-art ASAG models are large neural networks (NN) often described as "black box", providing no explanation for which characteristics of an input are important for the produced output. This inexplicable nature can be frustrating to teachers and students when trying to interpret, or learn from an automatically-generated grade. To create a powerful yet intelligible ASAG model, we experiment with a type of model called a Neural Additive Model that combines the performance of a NN with the explainability of an additive model. We use a Knowledge Integration (KI) framework from the learning sciences to guide feature engineering to create inputs that reflect whether a student includes certain ideas in their response. We hypothesize that indicating the inclusion (or exclusion) of predefined ideas as features will be sufficient for the NAM to have good predictive power and interpretability, as this may guide a human scorer using a KI rubric. We compare the performance of the NAM with another explainable model, logistic regression, using the same features, and to a non-explainable neural model, DeBERTa, that does not require feature engineering.</li>
</ul>

<h3>Title: On the Relevance of Byzantine Robust Optimization Against Data Poisoning</h3>
<ul>
<li><strong>Authors: </strong>Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, Rafael Pinot</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00491">https://arxiv.org/abs/2405.00491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00491">https://arxiv.org/pdf/2405.00491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00491]] On the Relevance of Byzantine Robust Optimization Against Data Poisoning(https://arxiv.org/abs/2405.00491)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The success of machine learning (ML) has been intimately linked with the availability of large amounts of data, typically collected from heterogeneous sources and processed on vast networks of computing devices (also called {\em workers}). Beyond accuracy, the use of ML in critical domains such as healthcare and autonomous driving calls for robustness against {\em data poisoning}and some {\em faulty workers}. The problem of {\em Byzantine ML} formalizes these robustness issues by considering a distributed ML environment in which workers (storing a portion of the global dataset) can deviate arbitrarily from the prescribed algorithm. Although the problem has attracted a lot of attention from a theoretical point of view, its practical importance for addressing realistic faults (where the behavior of any worker is locally constrained) remains unclear. It has been argued that the seemingly weaker threat model where only workers' local datasets get poisoned is more reasonable. We prove that, while tolerating a wider range of faulty behaviors, Byzantine ML yields solutions that are, in a precise sense, optimal even under the weaker data poisoning threat model. Then, we study a generic data poisoning model wherein some workers have {\em fully-poisonous local data}, i.e., their datasets are entirely corruptible, and the remainders have {\em partially-poisonous local data}, i.e., only a fraction of their local datasets is corruptible. We prove that Byzantine-robust schemes yield optimal solutions against both these forms of data poisoning, and that the former is more harmful when workers have {\em heterogeneous} local data.</li>
</ul>

<h3>Title: Is Temperature the Creativity Parameter of Large Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Max Peeperkorn, Tom Kouwenhoven, Dan Brown, Anna Jordanous</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00492">https://arxiv.org/abs/2405.00492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00492">https://arxiv.org/pdf/2405.00492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00492]] Is Temperature the Creativity Parameter of Large Language Models?(https://arxiv.org/abs/2405.00492)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are applied to all sorts of creative tasks, and their outputs vary from beautiful, to peculiar, to pastiche, into plain plagiarism. The temperature parameter of an LLM regulates the amount of randomness, leading to more diverse outputs; therefore, it is often claimed to be the creativity parameter. Here, we investigate this claim using a narrative generation task with a predetermined fixed context, model and prompt. Specifically, we present an empirical analysis of the LLM output for different temperature values using four necessary conditions for creativity in narrative generation: novelty, typicality, cohesion, and coherence. We find that temperature is weakly correlated with novelty, and unsurprisingly, moderately correlated with incoherence, but there is no relationship with either cohesion or typicality. However, the influence of temperature on creativity is far more nuanced and weak than suggested by the "creativity parameter" claim; overall results suggest that the LLM generates slightly more novel outputs as temperatures get higher. Finally, we discuss ideas to allow more controlled LLM creativity, rather than relying on chance via changing the temperature parameter.</li>
</ul>

<h3>Title: NeRF-Guided Unsupervised Learning of RGB-D Registration</h3>
<ul>
<li><strong>Authors: </strong>Zhinan Yu, Zheng Qin, Yijie Tang, Yongjun Wang, Renjiao Yi, Chenyang Zhu, Kai Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00507">https://arxiv.org/abs/2405.00507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00507">https://arxiv.org/pdf/2405.00507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00507]] NeRF-Guided Unsupervised Learning of RGB-D Registration(https://arxiv.org/abs/2405.00507)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>This paper focuses on training a robust RGB-D registration model without ground-truth pose supervision. Existing methods usually adopt a pairwise training strategy based on differentiable rendering, which enforces the photometric and the geometric consistency between the two registered frames as supervision. However, this frame-to-frame framework suffers from poor multi-view consistency due to factors such as lighting changes, geometry occlusion and reflective materials. In this paper, we present NeRF-UR, a novel frame-to-model optimization framework for unsupervised RGB-D registration. Instead of frame-to-frame consistency, we leverage the neural radiance field (NeRF) as a global model of the scene and use the consistency between the input and the NeRF-rerendered frames for pose optimization. This design can significantly improve the robustness in scenarios with poor multi-view consistency and provides better learning signal for the registration model. Furthermore, to bootstrap the NeRF optimization, we create a synthetic dataset, Sim-RGBD, through a photo-realistic simulator to warm up the registration model. By first training the registration model on Sim-RGBD and later unsupervisedly fine-tuning on real data, our framework enables distilling the capability of feature extraction and registration from simulation to reality. Our method outperforms the state-of-the-art counterparts on two popular indoor RGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper reproduction.</li>
</ul>

<h3>Title: Get Your Embedding Space in Order: Domain-Adaptive Regression for Forest  Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Sizhuo Li, Dimitri Gominski, Martin Brandt, Xiaoye Tong, Philippe Ciais</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00514">https://arxiv.org/abs/2405.00514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00514">https://arxiv.org/pdf/2405.00514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00514]] Get Your Embedding Space in Order: Domain-Adaptive Regression for Forest  Monitoring(https://arxiv.org/abs/2405.00514)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image-level regression is an important task in Earth observation, where visual domain and label shifts are a core challenge hampering generalization. However, cross-domain regression with remote sensing data remains understudied due to the absence of suited datasets. We introduce a new dataset with aerial and satellite imagery in five countries with three forest-related regression tasks. To match real-world applicative interests, we compare methods through a restrictive setup where no prior on the target domain is available during training, and models are adapted with limited information during testing. Building on the assumption that ordered relationships generalize better, we propose manifold diffusion for regression as a strong baseline for transduction in low-data regimes. Our comparison highlights the comparative advantages of inductive and transductive methods in cross-domain regression.</li>
</ul>

<h3>Title: FMLFS: A federated multi-label feature selection based on information  theory in IoT environment</h3>
<ul>
<li><strong>Authors: </strong>Afsaneh Mahanipour, Hana Khamfroush</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00524">https://arxiv.org/abs/2405.00524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00524">https://arxiv.org/pdf/2405.00524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00524]] FMLFS: A federated multi-label feature selection based on information  theory in IoT environment(https://arxiv.org/abs/2405.00524)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>In certain emerging applications such as health monitoring wearable and traffic monitoring systems, Internet-of-Things (IoT) devices generate or collect a huge amount of multi-label datasets. Within these datasets, each instance is linked to a set of labels. The presence of noisy, redundant, or irrelevant features in these datasets, along with the curse of dimensionality, poses challenges for multi-label classifiers. Feature selection (FS) proves to be an effective strategy in enhancing classifier performance and addressing these challenges. Yet, there is currently no existing distributed multi-label FS method documented in the literature that is suitable for distributed multi-label datasets within IoT environments. This paper introduces FMLFS, the first federated multi-label feature selection method. Here, mutual information between features and labels serves as the relevancy metric, while the correlation distance between features, derived from mutual information and joint entropy, is utilized as the redundancy measure. Following aggregation of these metrics on the edge server and employing Pareto-based bi-objective and crowding distance strategies, the sorted features are subsequently sent back to the IoT devices. The proposed method is evaluated through two scenarios: 1) transmitting reduced-size datasets to the edge server for centralized classifier usage, and 2) employing federated learning with reduced-size datasets. Evaluation across three metrics - performance, time complexity, and communication cost - demonstrates that FMLFS outperforms five other comparable methods in the literature and provides a good trade-off on three real-world datasets.</li>
</ul>

<h3>Title: JNI Global References Are Still Vulnerable: Attacks and Defenses</h3>
<ul>
<li><strong>Authors: </strong>Yi He, Yuan Zhou, Yacong Gu, Purui Su, Qi Li, Yajin Zhou, Yong Jiang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00526">https://arxiv.org/abs/2405.00526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00526">https://arxiv.org/pdf/2405.00526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00526]] JNI Global References Are Still Vulnerable: Attacks and Defenses(https://arxiv.org/abs/2405.00526)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>System services and resources in Android are accessed through IPC based mechanisms. Previous research has demonstrated that they are vulnerable to the denial-of-service attack (DoS attack). For instance, the JNI global reference (JGR), which is widely used by system services, can be exhausted to cause the system reboot (hence the name JGRE attack). Even though the Android team tries to fix the problem by enforcing security checks, we find that it is still possible to construct a JGR exhaustion DoS attack in the latest Android system. In this paper, we propose a new JGR exhaustion DoS attack, which is effective in different Android versions, including the latest one (i.e., Android 10). Specifically, we developed JGREAnalyzer, a tool that can systematically detect JGR vulnerable services APIs via a call graph analysis and a forwarding reachability analysis. We applied this tool to different Android versions and found multiple vulnerabilities. In particular, among 148 system services in Android 10, 12 of them have 21 vulnerabilities. Among them, 9 can be successfully exploited without any permissions. We further analyze the root cause of the vulnerabilities and propose a new defense to mitigate the JGRE attack by restricting resource consumption via global reference counting.</li>
</ul>

<h3>Title: Byzantine-Secure Relying Party for Resilient RPKI</h3>
<ul>
<li><strong>Authors: </strong>Jens Friess, Donika Mirdita, Haya Schulmann, Michael Waidner</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00531">https://arxiv.org/abs/2405.00531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00531">https://arxiv.org/pdf/2405.00531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00531]] Byzantine-Secure Relying Party for Resilient RPKI(https://arxiv.org/abs/2405.00531)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack, robust</a></li>
<li><strong>Abstract: </strong>To protect against prefix hijacks, Resource Public Key Infrastructure (RPKI) has been standardized. To enjoy the security guarantees of RPKI validation, networks need to install a new component, the relying party validator, which fetches and validates RPKI objects and provides them to border routers. However, recent work shows that relying parties experience failures when retrieving RPKI objects and are vulnerable to attacks, all of which can disable RPKI validation. Therefore even the few adopters are not necessarily secure. We make the first proposal that significantly improves the resilience and security of RPKI. We develop BRP, a Byzantine-Secure relying party implementation. In BRP the relying party nodes redundantly validate RPKI objects and reach a global consensus through voting. BRP provides an RPKI equivalent of public DNS, removing the need for networks to install, operate, and upgrade their own relying party instances while avoiding the need to trust operators of BRP nodes. We show through simulations and experiments that BRP, as an intermediate RPKI service, results in less load on RPKI publication points and a robust output despite RPKI repository failures, jitter, and attacks. We engineer BRP to be fully backward compatible and readily deployable - it does not require any changes to the border routers and the RPKI repositories. We demonstrate that BRP can protect many networks transparently, with either a decentralized or centralized deployment. BRP can be set up as a network of decentralized volunteer deployments, similarly to NTP and TOR, where different operators participate in the peering process with their node, and provide resilient and secure relying party validation to the Internet. BRP can also be hosted by a single operator as a centralized service, e.g., on one cloud or CDN, and provides RPKI validation benefits even when hosted on a single network.</li>
</ul>

<h3>Title: A Legal Framework for Natural Language Processing Model Training in  Portugal</h3>
<ul>
<li><strong>Authors: </strong>Rúben Almeida, Evelin Amorim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00536">https://arxiv.org/abs/2405.00536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00536">https://arxiv.org/pdf/2405.00536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00536]] A Legal Framework for Natural Language Processing Model Training in  Portugal(https://arxiv.org/abs/2405.00536)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Recent advances in deep learning have promoted the advent of many computational systems capable of performing intelligent actions that, until then, were restricted to the human intellect. In the particular case of human languages, these advances allowed the introduction of applications like ChatGPT that are capable of generating coherent text without being explicitly programmed to do so. Instead, these models use large volumes of textual data to learn meaningful representations of human languages. Associated with these advances, concerns about copyright and data privacy infringements caused by these applications have emerged. Despite these concerns, the pace at which new natural language processing applications continued to be developed largely outperformed the introduction of new regulations. Today, communication barriers between legal experts and computer scientists motivate many unintentional legal infringements during the development of such applications. In this paper, a multidisciplinary team intends to bridge this communication gap and promote more compliant Portuguese NLP research by presenting a series of everyday NLP use cases, while highlighting the Portuguese legislation that may arise during its development.</li>
</ul>

<h3>Title: Swarm Learning: A Survey of Concepts, Applications, and Trends</h3>
<ul>
<li><strong>Authors: </strong>Elham Shammar, Xiaohui Cui, Mohammed A. A. Al-qaness</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00556">https://arxiv.org/abs/2405.00556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00556">https://arxiv.org/pdf/2405.00556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00556]] Swarm Learning: A Survey of Concepts, Applications, and Trends(https://arxiv.org/abs/2405.00556)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, federate</a></li>
<li><strong>Abstract: </strong>Deep learning models have raised privacy and security concerns due to their reliance on large datasets on central servers. As the number of Internet of Things (IoT) devices increases, artificial intelligence (AI) will be crucial for resource management, data processing, and knowledge acquisition. To address those issues, federated learning (FL) has introduced a novel approach to building a versatile, large-scale machine learning framework that operates in a decentralized and hardware-agnostic manner. However, FL faces network bandwidth limitations and data breaches. To reduce the central dependency in FL and increase scalability, swarm learning (SL) has been proposed in collaboration with Hewlett Packard Enterprise (HPE). SL represents a decentralized machine learning framework that leverages blockchain technology for secure, scalable, and private data management. A blockchain-based network enables the exchange and aggregation of model parameters among participants, thus mitigating the risk of a single point of failure and eliminating communication bottlenecks. To the best of our knowledge, this survey is the first to introduce the principles of Swarm Learning, its architectural design, and its fields of application. In addition, it highlights numerous research avenues that require further exploration by academic and industry communities to unlock the full potential and applications of SL.</li>
</ul>

<h3>Title: Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and  Expert Mixtures in Self-Alignment</h3>
<ul>
<li><strong>Authors: </strong>Zhili Liu, Yunhao Gou, Kai Chen, Lanqing Hong, Jiahui Gao, Fei Mi, Yu Zhang, Zhenguo Li, Xin Jiang, Qun Liu, James T. Kwok</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00557">https://arxiv.org/abs/2405.00557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00557">https://arxiv.org/pdf/2405.00557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00557]] Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and  Expert Mixtures in Self-Alignment(https://arxiv.org/abs/2405.00557)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As the capabilities of large language models (LLMs) have expanded dramatically, aligning these models with human values presents a significant challenge, posing potential risks during deployment. Traditional alignment strategies rely heavily on human intervention, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), or on the self-alignment capacities of LLMs, which usually require a strong LLM's emergent ability to improve its original bad answer. To address these challenges, we propose a novel self-alignment method that utilizes a Chain of Thought (CoT) approach, termed AlignCoT. This method encompasses stages of Question Analysis, Answer Guidance, and Safe Answer production. It is designed to enable LLMs to generate high-quality, safe responses throughout various stages of their development. Furthermore, we introduce the Mixture of insighTful Experts (MoTE) architecture, which applies the mixture of experts to enhance each component of the AlignCoT process, markedly increasing alignment efficiency. The MoTE approach not only outperforms existing methods in aligning LLMs with human values but also highlights the benefits of using self-generated data, revealing the dual benefits of improved alignment and training efficiency.</li>
</ul>

<h3>Title: EALD-MLLM: Emotion Analysis in Long-sequential and De-identity videos  with Multi-modal Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Deng Li, Xin Liu, Bohao Xing, Baiqiang Xia, Yuan Zong, Bihan Wen, Heikki Kälviäinen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00574">https://arxiv.org/abs/2405.00574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00574">https://arxiv.org/pdf/2405.00574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00574]] EALD-MLLM: Emotion Analysis in Long-sequential and De-identity videos  with Multi-modal Large Language Model(https://arxiv.org/abs/2405.00574)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Emotion AI is the ability of computers to understand human emotional states. Existing works have achieved promising progress, but two limitations remain to be solved: 1) Previous studies have been more focused on short sequential video emotion analysis while overlooking long sequential video. However, the emotions in short sequential videos only reflect instantaneous emotions, which may be deliberately guided or hidden. In contrast, long sequential videos can reveal authentic emotions; 2) Previous studies commonly utilize various signals such as facial, speech, and even sensitive biological signals (e.g., electrocardiogram). However, due to the increasing demand for privacy, developing Emotion AI without relying on sensitive signals is becoming important. To address the aforementioned limitations, in this paper, we construct a dataset for Emotion Analysis in Long-sequential and De-identity videos called EALD by collecting and processing the sequences of athletes' post-match interviews. In addition to providing annotations of the overall emotional state of each video, we also provide the Non-Facial Body Language (NFBL) annotations for each player. NFBL is an inner-driven emotional expression and can serve as an identity-free clue to understanding the emotional state. Moreover, we provide a simple but effective baseline for further research. More precisely, we evaluate the Multimodal Large Language Models (MLLMs) with de-identification signals (e.g., visual, speech, and NFBLs) to perform emotion analysis. Our experimental results demonstrate that: 1) MLLMs can achieve comparable, even better performance than the supervised single-modal models, even in a zero-shot scenario; 2) NFBL is an important cue in long sequential emotion analysis. EALD will be available on the open-source platform.</li>
</ul>

<h3>Title: Discovering robust biomarkers of neurological disorders from functional  MRI using graph neural networks: A Review</h3>
<ul>
<li><strong>Authors: </strong>Yi Hao Chan, Deepank Girish, Sukrit Gupta, Jing Xia, Chockalingam Kasi, Yinan He, Conghao Wang, Jagath C. Rajapakse</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00577">https://arxiv.org/abs/2405.00577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00577">https://arxiv.org/pdf/2405.00577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00577]] Discovering robust biomarkers of neurological disorders from functional  MRI using graph neural networks: A Review(https://arxiv.org/abs/2405.00577)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability, generative</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNN) have emerged as a popular tool for modelling functional magnetic resonance imaging (fMRI) datasets. Many recent studies have reported significant improvements in disorder classification performance via more sophisticated GNN designs and highlighted salient features that could be potential biomarkers of the disorder. In this review, we provide an overview of how GNN and model explainability techniques have been applied on fMRI datasets for disorder prediction tasks, with a particular emphasis on the robustness of biomarkers produced for neurodegenerative diseases and neuropsychiatric disorders. We found that while most studies have performant models, salient features highlighted in these studies vary greatly across studies on the same disorder and little has been done to evaluate their robustness. To address these issues, we suggest establishing new standards that are based on objective evaluation metrics to determine the robustness of these potential biomarkers. We further highlight gaps in the existing literature and put together a prediction-attribution-evaluation framework that could set the foundations for future research on improving the robustness of potential biomarkers discovered via GNNs.</li>
</ul>

<h3>Title: The Real, the Better: Aligning Large Language Models with Online Human  Behaviors</h3>
<ul>
<li><strong>Authors: </strong>Guanying Jiang, Lingyong Yan, Haibo Shi, Dawei Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00578">https://arxiv.org/abs/2405.00578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00578">https://arxiv.org/pdf/2405.00578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00578]] The Real, the Better: Aligning Large Language Models with Online Human  Behaviors(https://arxiv.org/abs/2405.00578)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language model alignment is widely used and studied to avoid LLM producing unhelpful and harmful responses. However, the lengthy training process and predefined preference bias hinder adaptation to online diverse human preferences. To this end, this paper proposes an alignment framework, called Reinforcement Learning with Human Behavior (RLHB), to align LLMs by directly leveraging real online human behaviors. By taking the generative adversarial framework, the generator is trained to respond following expected human behavior; while the discriminator tries to verify whether the triplets of query, response, and human behavior come from real online environments. Behavior modeling in natural-language form and the multi-model joint training mechanism enable an active and sustainable online alignment. Experimental results confirm the effectiveness of our proposed methods by both human and automatic evaluations.</li>
</ul>

<h3>Title: GraCo: Granularity-Controllable Interactive Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yian Zhao, Kehan Li, Zesen Cheng, Pengchong Qiao, Xiawu Zheng, Rongrong Ji, Chang Liu, Li Yuan, Jie Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00587">https://arxiv.org/abs/2405.00587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00587">https://arxiv.org/pdf/2405.00587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00587]] GraCo: Granularity-Controllable Interactive Segmentation(https://arxiv.org/abs/2405.00587)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Interactive Segmentation (IS) segments specific objects or parts in the image according to user input. Current IS pipelines fall into two categories: single-granularity output and multi-granularity output. The latter aims to alleviate the spatial ambiguity present in the former. However, the multi-granularity output pipeline suffers from limited interaction flexibility and produces redundant results. In this work, we introduce Granularity-Controllable Interactive Segmentation (GraCo), a novel approach that allows precise control of prediction granularity by introducing additional parameters to input. This enhances the customization of the interactive system and eliminates redundancy while resolving ambiguity. Nevertheless, the exorbitant cost of annotating multi-granularity masks and the lack of available datasets with granularity annotations make it difficult for models to acquire the necessary guidance to control output granularity. To address this problem, we design an any-granularity mask generator that exploits the semantic property of the pre-trained IS model to automatically generate abundant mask-granularity pairs without requiring additional manual annotation. Based on these pairs, we propose a granularity-controllable learning strategy that efficiently imparts the granularity controllability to the IS model. Extensive experiments on intricate scenarios at object and part levels demonstrate that our GraCo has significant advantages over previous methods. This highlights the potential of GraCo to be a flexible annotation tool, capable of adapting to diverse segmentation scenarios. The project page: https://zhao-yian.github.io/GraCo.</li>
</ul>

<h3>Title: Are Models Biased on Text without Gender-related Language?</h3>
<ul>
<li><strong>Authors: </strong>Catarina G Belém, Preethi Seshadri, Yasaman Razeghi, Sameer Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00588">https://arxiv.org/abs/2405.00588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00588">https://arxiv.org/pdf/2405.00588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00588]] Are Models Biased on Text without Gender-related Language?(https://arxiv.org/abs/2405.00588)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Gender bias research has been pivotal in revealing undesirable behaviors in large language models, exposing serious gender stereotypes associated with occupations, and emotions. A key observation in prior work is that models reinforce stereotypes as a consequence of the gendered correlations that are present in the training data. In this paper, we focus on bias where the effect from training data is unclear, and instead address the question: Do language models still exhibit gender bias in non-stereotypical settings? To do so, we introduce UnStereoEval (USE), a novel framework tailored for investigating gender bias in stereotype-free scenarios. USE defines a sentence-level score based on pretraining data statistics to determine if the sentence contain minimal word-gender associations. To systematically benchmark the fairness of popular language models in stereotype-free scenarios, we utilize USE to automatically generate benchmarks without any gender-related language. By leveraging USE's sentence-level score, we also repurpose prior gender bias benchmarks (Winobias and Winogender) for non-stereotypical evaluation. Surprisingly, we find low fairness across all 28 tested models. Concretely, models demonstrate fair behavior in only 9%-41% of stereotype-free sentences, suggesting that bias does not solely stem from the presence of gender-related words. These results raise important questions about where underlying model biases come from and highlight the need for more systematic and comprehensive bias evaluation. We release the full dataset and code at https://ucinlp.github.io/unstereo-eval.</li>
</ul>

<h3>Title: Unbundle-Rewrite-Rebundle: Runtime Detection and Rewriting of  Privacy-Harming Code in JavaScript Bundles</h3>
<ul>
<li><strong>Authors: </strong>Mir Masood Ali, Peter Snyder, Chris Kanich, Hamed Haddadi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00596">https://arxiv.org/abs/2405.00596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00596">https://arxiv.org/pdf/2405.00596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00596]] Unbundle-Rewrite-Rebundle: Runtime Detection and Rewriting of  Privacy-Harming Code in JavaScript Bundles(https://arxiv.org/abs/2405.00596)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>This work presents Unbundle-Rewrite-Rebundle (URR), a system for detecting privacy-harming portions of bundled JavaScript code, and rewriting that code at runtime to remove the privacy harming behavior without breaking the surrounding code or overall application. URR is a novel solution to the problem of JavaScript bundles, where websites pre-compile multiple code units into a single file, making it impossible for content filters and ad-blockers to differentiate between desired and unwanted resources. Where traditional content filtering tools rely on URLs, URR analyzes the code at the AST level, and replaces harmful AST sub-trees with privacy-and-functionality maintaining alternatives. We present an open-sourced implementation of URR as a Firefox extension, and evaluate it against JavaScript bundles generated by the most popular bundling system (Webpack) deployed on the Tranco 10k. We measure the performance, measured by precision (1.00), recall (0.95), and speed (0.43s per-script) when detecting and rewriting three representative privacy harming libraries often included in JavaScript bundles, and find URR to be an effective approach to a large-and-growing blind spot unaddressed by current privacy tools.</li>
</ul>

<h3>Title: Investigating Automatic Scoring and Feedback using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gloria Ashiya Katuka, Alexander Gain, Yen-Yun Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00602">https://arxiv.org/abs/2405.00602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00602">https://arxiv.org/pdf/2405.00602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00602]] Investigating Automatic Scoring and Feedback using Large Language Models(https://arxiv.org/abs/2405.00602)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatic grading and feedback have been long studied using traditional machine learning and deep learning techniques using language models. With the recent accessibility to high performing large language models (LLMs) like LLaMA-2, there is an opportunity to investigate the use of these LLMs for automatic grading and feedback generation. Despite the increase in performance, LLMs require significant computational resources for fine-tuning and additional specific adjustments to enhance their performance for such tasks. To address these issues, Parameter Efficient Fine-tuning (PEFT) methods, such as LoRA and QLoRA, have been adopted to decrease memory and computational requirements in model fine-tuning. This paper explores the efficacy of PEFT-based quantized models, employing classification or regression head, to fine-tune LLMs for automatically assigning continuous numerical grades to short answers and essays, as well as generating corresponding feedback. We conducted experiments on both proprietary and open-source datasets for our tasks. The results show that prediction of grade scores via finetuned LLMs are highly accurate, achieving less than 3% error in grade percentage on average. For providing graded feedback fine-tuned 4-bit quantized LLaMA-2 13B models outperform competitive base models and achieve high similarity with subject matter expert feedback in terms of high BLEU and ROUGE scores and qualitatively in terms of feedback. The findings from this study provide important insights into the impacts of the emerging capabilities of using quantization approaches to fine-tune LLMs for various downstream tasks, such as automatic short answer scoring and feedback generation at comparatively lower costs and latency.</li>
</ul>

<h3>Title: Addressing Topic Granularity and Hallucination in Large Language Models  for Topic Modelling</h3>
<ul>
<li><strong>Authors: </strong>Yida Mu, Peizhen Bai, Kalina Bontcheva, Xingyi Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00611">https://arxiv.org/abs/2405.00611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00611">https://arxiv.org/pdf/2405.00611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00611]] Addressing Topic Granularity and Hallucination in Large Language Models  for Topic Modelling(https://arxiv.org/abs/2405.00611)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) with their strong zero-shot topic extraction capabilities offer an alternative to probabilistic topic modelling and closed-set topic classification approaches. As zero-shot topic extractors, LLMs are expected to understand human instructions to generate relevant and non-hallucinated topics based on the given documents. However, LLM-based topic modelling approaches often face difficulties in generating topics with adherence to granularity as specified in human instructions, often resulting in many near-duplicate topics. Furthermore, methods for addressing hallucinated topics generated by LLMs have not yet been investigated. In this paper, we focus on addressing the issues of topic granularity and hallucinations for better LLM-based topic modelling. To this end, we introduce a novel approach that leverages Direct Preference Optimisation (DPO) to fine-tune open-source LLMs, such as Mistral-7B. Our approach does not rely on traditional human annotation to rank preferred answers but employs a reconstruction pipeline to modify raw topics generated by LLMs, thus enabling a fast and efficient training and inference framework. Comparative experiments show that our fine-tuning approach not only significantly improves the LLM's capability to produce more coherent, relevant, and precise topics, but also reduces the number of hallucinated topics.</li>
</ul>

<h3>Title: Multigroup Robustness</h3>
<ul>
<li><strong>Authors: </strong>Lunjia Hu, Charlotte Peale, Judy Hanwen Shen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00614">https://arxiv.org/abs/2405.00614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00614">https://arxiv.org/pdf/2405.00614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00614]] Multigroup Robustness(https://arxiv.org/abs/2405.00614)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>To address the shortcomings of real-world datasets, robust learning algorithms have been designed to overcome arbitrary and indiscriminate data corruption. However, practical processes of gathering data may lead to patterns of data corruption that are localized to specific partitions of the training dataset. Motivated by critical applications where the learned model is deployed to make predictions about people from a rich collection of overlapping subpopulations, we initiate the study of multigroup robust algorithms whose robustness guarantees for each subpopulation only degrade with the amount of data corruption inside that subpopulation. When the data corruption is not distributed uniformly over subpopulations, our algorithms provide more meaningful robustness guarantees than standard guarantees that are oblivious to how the data corruption and the affected subpopulations are related. Our techniques establish a new connection between multigroup fairness and robustness.</li>
</ul>

<h3>Title: Lane Segmentation Refinement with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Antonio Ruiz, Andrew Melnik, Dong Wang, Helge Ritter</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00620">https://arxiv.org/abs/2405.00620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00620">https://arxiv.org/pdf/2405.00620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00620]] Lane Segmentation Refinement with Diffusion Models(https://arxiv.org/abs/2405.00620)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>The lane graph is a key component for building high-definition (HD) maps and crucial for downstream tasks such as autonomous driving or navigation planning. Previously, He et al. (2022) explored the extraction of the lane-level graph from aerial imagery utilizing a segmentation based approach. However, segmentation networks struggle to achieve perfect segmentation masks resulting in inaccurate lane graph extraction. We explore additional enhancements to refine this segmentation-based approach and extend it with a diffusion probabilistic model (DPM) component. This combination further improves the GEO F1 and TOPO F1 scores, which are crucial indicators of the quality of a lane graph, in the undirected graph in non-intersection areas. We conduct experiments on a publicly available dataset, demonstrating that our method outperforms the previous approach, particularly in enhancing the connectivity of such a graph, as measured by the TOPO F1 score. Moreover, we perform ablation studies on the individual components of our method to understand their contribution and evaluate their effectiveness.</li>
</ul>

<h3>Title: Queue-based Eco-Driving at Roundabouts with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Anna-Lena Schlamp, Werner Huber, Stefanie Schmidtner</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00625">https://arxiv.org/abs/2405.00625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00625">https://arxiv.org/pdf/2405.00625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00625]] Queue-based Eco-Driving at Roundabouts with Reinforcement Learning(https://arxiv.org/abs/2405.00625)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We address eco-driving at roundabouts in mixed traffic to enhance traffic flow and traffic efficiency in urban areas. The aim is to proactively optimize speed of automated or non-automated connected vehicles (CVs), ensuring both an efficient approach and smooth entry into roundabouts. We incorporate the traffic situation ahead, i.e. preceding vehicles and waiting queues. Further, we develop two approaches: a rule-based and an Reinforcement Learning (RL) based eco-driving system, with both using the approach link and information from conflicting CVs for speed optimization. A fair comparison of rule-based and RL-based approaches is performed to explore RL as a viable alternative to classical optimization. Results show that both approaches outperform the baseline. Improvements significantly increase with growing traffic volumes, leading to best results on average being obtained at high volumes. Near capacity, performance deteriorates, indicating limited applicability at capacity limits. Examining different CV penetration rates, a decline in performance is observed, but with substantial results still being achieved at lower CV rates. RL agents can discover effective policies for speed optimization in dynamic roundabout settings, but they do not offer a substantial advantage over classical approaches, especially at higher traffic volumes or lower CV penetration rates.</li>
</ul>

<h3>Title: HUGO -- Highlighting Unseen Grid Options: Combining Deep Reinforcement  Learning with a Heuristic Target Topology Approach</h3>
<ul>
<li><strong>Authors: </strong>Malte Lehna, Clara Holzhüter, Sven Tomforde, Christoph Scholz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00629">https://arxiv.org/abs/2405.00629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00629">https://arxiv.org/pdf/2405.00629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00629]] HUGO -- Highlighting Unseen Grid Options: Combining Deep Reinforcement  Learning with a Heuristic Target Topology Approach(https://arxiv.org/abs/2405.00629)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the growth of Renewable Energy (RE) generation, the operation of power grids has become increasingly complex. One solution is automated grid operation, where Deep Reinforcement Learning (DRL) has repeatedly shown significant potential in Learning to Run a Power Network (L2RPN) challenges. However, only individual actions at the substation level have been subjected to topology optimization by most existing DRL algorithms. In contrast, we propose a more holistic approach in this paper by proposing specific Target Topologies (TTs) as actions. These topologies are selected based on their robustness. As part of this paper, we present a search algorithm to find the TTs and upgrade our previously developed DRL agent CurriculumAgent (CAgent) to a novel topology agent. We compare the upgrade to the previous CAgent agent and can increase their scores significantly by 10%. Further, we achieve a 25% better median survival with our TTs included. Later analysis shows that almost all TTs are close to the base topology, explaining their robustness.</li>
</ul>

<h3>Title: Depth Priors in Removal Neural Radiance Fields</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Guo, Peng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00630">https://arxiv.org/abs/2405.00630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00630">https://arxiv.org/pdf/2405.00630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00630]] Depth Priors in Removal Neural Radiance Fields(https://arxiv.org/abs/2405.00630)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural Radiance Fields (NeRF) have shown impressive results in 3D reconstruction and generating novel views. A key challenge within NeRF is the editing of reconstructed scenes, such as object removal, which requires maintaining consistency across multiple views and ensuring high-quality synthesised perspectives. Previous studies have incorporated depth priors, typically from LiDAR or sparse depth measurements provided by COLMAP, to improve the performance of object removal in NeRF. However, these methods are either costly or time-consuming. In this paper, we propose a novel approach that integrates monocular depth estimates with NeRF-based object removal models to significantly reduce time consumption and enhance the robustness and quality of scene generation and object removal. We conducted a thorough evaluation of COLMAP's dense depth reconstruction on the KITTI dataset to verify its accuracy in depth map generation. Our findings suggest that COLMAP can serve as an effective alternative to a ground truth depth map where such information is missing or costly to obtain. Additionally, we integrated various monocular depth estimation methods into the removal NeRF model, i.e., SpinNeRF, to assess their capacity to improve object removal performance. Our experimental results highlight the potential of monocular depth estimation to substantially improve NeRF applications.</li>
</ul>

<h3>Title: Deep Metric Learning-Based Out-of-Distribution Detection with Synthetic  Outlier Exposure</h3>
<ul>
<li><strong>Authors: </strong>Assefa Seyoum Wahd</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00631">https://arxiv.org/abs/2405.00631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00631">https://arxiv.org/pdf/2405.00631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00631]] Deep Metric Learning-Based Out-of-Distribution Detection with Synthetic  Outlier Exposure(https://arxiv.org/abs/2405.00631)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we present a novel approach that combines deep metric learning and synthetic data generation using diffusion models for out-of-distribution (OOD) detection. One popular approach for OOD detection is outlier exposure, where models are trained using a mixture of in-distribution (ID) samples and ``seen" OOD samples. For the OOD samples, the model is trained to minimize the KL divergence between the output probability and the uniform distribution while correctly classifying the in-distribution (ID) data. In this paper, we propose a label-mixup approach to generate synthetic OOD data using Denoising Diffusion Probabilistic Models (DDPMs). Additionally, we explore recent advancements in metric learning to train our models. In the experiments, we found that metric learning-based loss functions perform better than the softmax. Furthermore, the baseline models (including softmax, and metric learning) show a significant improvement when trained with the generated OOD data. Our approach outperforms strong baselines in conventional OOD detection metrics.</li>
</ul>

<h3>Title: When Quantization Affects Confidence of Large Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Irina Proskurina, Luc Brun, Guillaume Metzler, Julien Velcin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00632">https://arxiv.org/abs/2405.00632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00632">https://arxiv.org/pdf/2405.00632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00632]] When Quantization Affects Confidence of Large Language Models?(https://arxiv.org/abs/2405.00632)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent studies introduced effective compression techniques for Large Language Models (LLMs) via post-training quantization or low-bit weight representation. Although quantized weights offer storage efficiency and allow for faster inference, existing works have indicated that quantization might compromise performance and exacerbate biases in LLMs. This study investigates the confidence and calibration of quantized models, considering factors such as language model type and scale as contributors to quantization loss. Firstly, we reveal that quantization with GPTQ to 4-bit results in a decrease in confidence regarding true labels, with varying impacts observed among different language models. Secondly, we observe fluctuations in the impact on confidence across different scales. Finally, we propose an explanation for quantization loss based on confidence levels, indicating that quantization disproportionately affects samples where the full model exhibited low confidence levels in the first place.</li>
</ul>

<h3>Title: Learning to Compose: Improving Object Centric Learning by Injecting  Compositionality</h3>
<ul>
<li><strong>Authors: </strong>Whie Jung, Jaehoon Yoo, Sungjin Ahn, Seunghoon Hong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00646">https://arxiv.org/abs/2405.00646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00646">https://arxiv.org/pdf/2405.00646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00646]] Learning to Compose: Improving Object Centric Learning by Injecting  Compositionality(https://arxiv.org/abs/2405.00646)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Learning compositional representation is a key aspect of object-centric learning as it enables flexible systematic generalization and supports complex visual reasoning. However, most of the existing approaches rely on auto-encoding objective, while the compositionality is implicitly imposed by the architectural or algorithmic bias in the encoder. This misalignment between auto-encoding objective and learning compositionality often results in failure of capturing meaningful object representations. In this study, we propose a novel objective that explicitly encourages compositionality of the representations. Built upon the existing object-centric learning framework (e.g., slot attention), our method incorporates additional constraints that an arbitrary mixture of object representations from two images should be valid by maximizing the likelihood of the composite data. We demonstrate that incorporating our objective to the existing framework consistently improves the objective-centric learning and enhances the robustness to the architectural choices.</li>
</ul>

<h3>Title: Grains of Saliency: Optimizing Saliency-based Training of Biometric  Attack Detection Models</h3>
<ul>
<li><strong>Authors: </strong>Colton R. Crum, Samuel Webster, Adam Czajka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00650">https://arxiv.org/abs/2405.00650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00650">https://arxiv.org/pdf/2405.00650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00650]] Grains of Saliency: Optimizing Saliency-based Training of Biometric  Attack Detection Models(https://arxiv.org/abs/2405.00650)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, biometric</a></li>
<li><strong>Abstract: </strong>Incorporating human-perceptual intelligence into model training has shown to increase the generalization capability of models in several difficult biometric tasks, such as presentation attack detection (PAD) and detection of synthetic samples. After the initial collection phase, human visual saliency (e.g., eye-tracking data, or handwritten annotations) can be integrated into model training through attention mechanisms, augmented training samples, or through human perception-related components of loss functions. Despite their successes, a vital, but seemingly neglected, aspect of any saliency-based training is the level of salience granularity (e.g., bounding boxes, single saliency maps, or saliency aggregated from multiple subjects) necessary to find a balance between reaping the full benefits of human saliency and the cost of its collection. In this paper, we explore several different levels of salience granularity and demonstrate that increased generalization capabilities of PAD and synthetic face detection can be achieved by using simple yet effective saliency post-processing techniques across several different CNNs.</li>
</ul>

<h3>Title: NLU-STR at SemEval-2024 Task 1: Generative-based Augmentation and  Encoder-based Scoring for Semantic Textual Relatedness</h3>
<ul>
<li><strong>Authors: </strong>Sanad Malaysha, Mustafa Jarrar, Mohammed Khalilia</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00659">https://arxiv.org/abs/2405.00659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00659">https://arxiv.org/pdf/2405.00659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00659]] NLU-STR at SemEval-2024 Task 1: Generative-based Augmentation and  Encoder-based Scoring for Semantic Textual Relatedness(https://arxiv.org/abs/2405.00659)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Semantic textual relatedness is a broader concept of semantic similarity. It measures the extent to which two chunks of text convey similar meaning or topics, or share related concepts or contexts. This notion of relatedness can be applied in various applications, such as document clustering and summarizing. SemRel-2024, a shared task in SemEval-2024, aims at reducing the gap in the semantic relatedness task by providing datasets for fourteen languages and dialects including Arabic. This paper reports on our participation in Track A (Algerian and Moroccan dialects) and Track B (Modern Standard Arabic). A BERT-based model is augmented and fine-tuned for regression scoring in supervised track (A), while BERT-based cosine similarity is employed for unsupervised track (B). Our system ranked 1st in SemRel-2024 for MSA with a Spearman correlation score of 0.49. We ranked 5th for Moroccan and 12th for Algerian with scores of 0.83 and 0.53, respectively.</li>
</ul>

<h3>Title: Is Bigger Edit Batch Size Always Better? -- An Empirical Study on Model  Editing with Llama-3</h3>
<ul>
<li><strong>Authors: </strong>Junsang Yoon, Akshat Gupta, Gopala Anumanchipalli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00664">https://arxiv.org/abs/2405.00664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00664">https://arxiv.org/pdf/2405.00664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00664]] Is Bigger Edit Batch Size Always Better? -- An Empirical Study on Model  Editing with Llama-3(https://arxiv.org/abs/2405.00664)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study presents a targeted model editing analysis focused on the latest large language model, Llama-3. We explore the efficacy of popular model editing techniques - ROME, MEMIT, and EMMET, which are designed for precise layer interventions. We identify the most effective layers for targeted edits through an evaluation that encompasses up to 4096 edits across three distinct strategies: sequential editing, batch editing, and a hybrid approach we call as sequential-batch editing. Our findings indicate that increasing edit batch-sizes may degrade model performance more significantly than using smaller edit batches sequentially for equal number of edits. With this, we argue that sequential model editing is an important component for scaling model editing methods and future research should focus on methods that combine both batched and sequential editing. This observation suggests a potential limitation in current model editing methods which push towards bigger edit batch sizes, and we hope it paves way for future investigations into optimizing batch sizes and model editing performance.</li>
</ul>

<h3>Title: RGB$\leftrightarrow$X: Image decomposition and synthesis using material-  and lighting-aware diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, Miloš Hašan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00666">https://arxiv.org/abs/2405.00666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00666">https://arxiv.org/pdf/2405.00666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00666]] RGB$\leftrightarrow$X: Image decomposition and synthesis using material-  and lighting-aware diffusion models(https://arxiv.org/abs/2405.00666)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The three areas of realistic forward rendering, per-pixel inverse rendering, and generative image synthesis may seem like separate and unrelated sub-fields of graphics and vision. However, recent work has demonstrated improved estimation of per-pixel intrinsic channels (albedo, roughness, metallicity) based on a diffusion architecture; we call this the RGB$\rightarrow$X problem. We further show that the reverse problem of synthesizing realistic images given intrinsic channels, X$\rightarrow$RGB, can also be addressed in a diffusion framework. Focusing on the image domain of interior scenes, we introduce an improved diffusion model for RGB$\rightarrow$X, which also estimates lighting, as well as the first diffusion X$\rightarrow$RGB model capable of synthesizing realistic images from (full or partial) intrinsic channels. Our X$\rightarrow$RGB model explores a middle ground between traditional rendering and generative models: we can specify only certain appearance properties that should be followed, and give freedom to the model to hallucinate a plausible version of the rest. This flexibility makes it possible to use a mix of heterogeneous training datasets, which differ in the available channels. We use multiple existing datasets and extend them with our own synthetic and real data, resulting in a model capable of extracting scene properties better than previous work and of generating highly realistic images of interior scenes.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
