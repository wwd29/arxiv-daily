<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: A More Secure Split: Enhancing the Security of Privacy-Preserving Split Learning. (arXiv:2309.08697v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08697">http://arxiv.org/abs/2309.08697</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08697]] A More Secure Split: Enhancing the Security of Privacy-Preserving Split Learning(http://arxiv.org/abs/2309.08697)</code></li>
<li>Summary: <p>Split learning (SL) is a new collaborative learning technique that allows
participants, e.g. a client and a server, to train machine learning models
without the client sharing raw data. In this setting, the client initially
applies its part of the machine learning model on the raw data to generate
Activation Maps (AMs) and then sends them to the server to continue the
training process. Previous works in the field demonstrated that reconstructing
AMs could result in privacy leakage of client data. In addition to that,
existing mitigation techniques that overcome the privacy leakage of SL prove to
be significantly worse in terms of accuracy. In this paper, we improve upon
previous works by constructing a protocol based on U-shaped SL that can operate
on homomorphically encrypted data. More precisely, in our approach, the client
applies homomorphic encryption on the AMs before sending them to the server,
thus protecting user privacy. This is an important improvement that reduces
privacy leakage in comparison to other SL-based works. Finally, our results
show that, with the optimum set of parameters, training with HE data in the
U-shaped SL setting only reduces accuracy by 2.65% compared to training on
plaintext. In addition, raw training data privacy is preserved.
</p></li>
</ul>

<h2>security</h2>
<h2>privacy</h2>
<h3>Title: Recovering from Privacy-Preserving Masking with Large Language Models. (arXiv:2309.08628v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08628">http://arxiv.org/abs/2309.08628</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08628]] Recovering from Privacy-Preserving Masking with Large Language Models(http://arxiv.org/abs/2309.08628)</code></li>
<li>Summary: <p>Model adaptation is crucial to handle the discrepancy between proxy training
data and actual users data received. To effectively perform adaptation, textual
data of users is typically stored on servers or their local devices, where
downstream natural language processing (NLP) models can be directly trained
using such in-domain data. However, this might raise privacy and security
concerns due to the extra risks of exposing user information to adversaries.
Replacing identifying information in textual data with a generic marker has
been recently explored. In this work, we leverage large language models (LLMs)
to suggest substitutes of masked tokens and have their effectiveness evaluated
on downstream language modeling tasks. Specifically, we propose multiple
pre-trained and fine-tuned LLM-based approaches and perform empirical studies
on various datasets for the comparison of these methods. Experimental results
show that models trained on the obfuscation corpora are able to achieve
comparable performance with the ones trained on the original data without
privacy-preserving token masking.
</p></li>
</ul>

<h3>Title: Evaluating the Impact of Local Differential Privacy on Utility Loss via Influence Functions. (arXiv:2309.08678v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08678">http://arxiv.org/abs/2309.08678</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08678]] Evaluating the Impact of Local Differential Privacy on Utility Loss via Influence Functions(http://arxiv.org/abs/2309.08678)</code></li>
<li>Summary: <p>How to properly set the privacy parameter in differential privacy (DP) has
been an open question in DP research since it was first proposed in 2006. In
this work, we demonstrate the ability of influence functions to offer insight
into how a specific privacy parameter value will affect a model's test loss in
the randomized response-based local DP setting. Our proposed method allows a
data curator to select the privacy parameter best aligned with their allowed
privacy-utility trade-off without requiring heavy computation such as extensive
model retraining and data privatization. We consider multiple common
randomization scenarios, such as performing randomized response over the
features, and/or over the labels, as well as the more complex case of applying
a class-dependent label noise correction method to offset the noise incurred by
randomization. Further, we provide a detailed discussion over the computational
complexity of our proposed approach inclusive of an empirical analysis. Through
empirical evaluations we show that for both binary and multi-class settings,
influence functions are able to approximate the true change in test loss that
occurs when randomized response is applied over features and/or labels with
small mean absolute error, especially in cases where noise correction methods
are applied.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Adversarial Attacks on Tables with Entity Swap. (arXiv:2309.08650v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08650">http://arxiv.org/abs/2309.08650</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08650]] Adversarial Attacks on Tables with Entity Swap(http://arxiv.org/abs/2309.08650)</code></li>
<li>Summary: <p>The capabilities of large language models (LLMs) have been successfully
applied in the context of table representation learning. The recently proposed
tabular language models have reported state-of-the-art results across various
tasks for table interpretation. However, a closer look into the datasets
commonly used for evaluation reveals an entity leakage from the train set into
the test set. Motivated by this observation, we explore adversarial attacks
that represent a more realistic inference setup. Adversarial attacks on text
have been shown to greatly affect the performance of LLMs, but currently, there
are no attacks targeting tabular language models. In this paper, we propose an
evasive entity-swap attack for the column type annotation (CTA) task. Our CTA
attack is the first black-box attack on tables, where we employ a
similarity-based sampling strategy to generate adversarial examples. The
experimental results show that the proposed attack generates up to a 70% drop
in performance.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Towards Robust and Smooth 3D Multi-Person Pose Estimation from Monocular Videos in the Wild. (arXiv:2309.08644v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08644">http://arxiv.org/abs/2309.08644</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08644]] Towards Robust and Smooth 3D Multi-Person Pose Estimation from Monocular Videos in the Wild(http://arxiv.org/abs/2309.08644)</code></li>
<li>Summary: <p>3D pose estimation is an invaluable task in computer vision with various
practical applications. Especially, 3D pose estimation for multi-person from a
monocular video (3DMPPE) is particularly challenging and is still largely
uncharted, far from applying to in-the-wild scenarios yet. We pose three
unresolved issues with the existing methods: lack of robustness on unseen views
during training, vulnerability to occlusion, and severe jittering in the
output. As a remedy, we propose POTR-3D, the first realization of a
sequence-to-sequence 2D-to-3D lifting model for 3DMPPE, powered by a novel
geometry-aware data augmentation strategy, capable of generating unbounded data
with a variety of views while caring about the ground plane and occlusions.
Through extensive experiments, we verify that the proposed model and data
augmentation robustly generalizes to diverse unseen views, robustly recovers
the poses against heavy occlusions, and reliably generates more natural and
smoother outputs. The effectiveness of our approach is verified not only by
achieving the state-of-the-art performance on public benchmarks, but also by
qualitative results on more challenging in-the-wild videos. Demo videos are
available at https://www.youtube.com/@potr3d.
</p></li>
</ul>

<h3>Title: BANSAC: A dynamic BAyesian Network for adaptive SAmple Consensus. (arXiv:2309.08690v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08690">http://arxiv.org/abs/2309.08690</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08690]] BANSAC: A dynamic BAyesian Network for adaptive SAmple Consensus(http://arxiv.org/abs/2309.08690)</code></li>
<li>Summary: <p>RANSAC-based algorithms are the standard techniques for robust estimation in
computer vision. These algorithms are iterative and computationally expensive;
they alternate between random sampling of data, computing hypotheses, and
running inlier counting. Many authors tried different approaches to improve
efficiency. One of the major improvements is having a guided sampling, letting
the RANSAC cycle stop sooner. This paper presents a new adaptive sampling
process for RANSAC. Previous methods either assume no prior information about
the inlier/outlier classification of data points or use some previously
computed scores in the sampling. In this paper, we derive a dynamic Bayesian
network that updates individual data points' inlier scores while iterating
RANSAC. At each iteration, we apply weighted sampling using the updated scores.
Our method works with or without prior data point scorings. In addition, we use
the updated inlier/outlier scoring for deriving a new stopping criterion for
the RANSAC loop. We test our method in multiple real-world datasets for several
applications and obtain state-of-the-art results. Our method outperforms the
baselines in accuracy while needing less computational time.
</p></li>
</ul>

<h3>Title: The Use of Multi-Scale Fiducial Markers To Aid Takeoff and Landing Navigation by Rotorcraft. (arXiv:2309.08769v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08769">http://arxiv.org/abs/2309.08769</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08769]] The Use of Multi-Scale Fiducial Markers To Aid Takeoff and Landing Navigation by Rotorcraft(http://arxiv.org/abs/2309.08769)</code></li>
<li>Summary: <p>This paper quantifies the impact of adverse environmental conditions on the
detection of fiducial markers (i.e., artificial landmarks) by color cameras
mounted on rotorcraft. We restrict our attention to square markers with a
black-and-white pattern of grid cells that can be nested to allow detection at
multiple scales. These markers have the potential to enhance the reliability of
precision takeoff and landing at vertiports by flying vehicles in urban
settings. Prior work has shown, in particular, that these markers can be
detected with high precision (i.e., few false positives) and high recall (i.e.,
few false negatives). However, most of this prior work has been based on image
sequences collected indoors with hand-held cameras. Our work is based on image
sequences collected outdoors with cameras mounted on a quadrotor during
semi-autonomous takeoff and landing operations under adverse environmental
conditions that include variations in temperature, illumination, wind speed,
humidity, visibility, and precipitation. In addition to precision and recall,
performance measures include continuity, availability, robustness, resiliency,
and coverage volume. We release both our dataset and the code we used for
analysis to the public as open source.
</p></li>
</ul>

<h3>Title: Enhancing Visual Perception in Novel Environments via Incremental Data Augmentation Based on Style Transfer. (arXiv:2309.08851v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08851">http://arxiv.org/abs/2309.08851</a></li>
<li>Code URL: https://github.com/abhibha1807/robustifying_visual_perception</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08851]] Enhancing Visual Perception in Novel Environments via Incremental Data Augmentation Based on Style Transfer(http://arxiv.org/abs/2309.08851)</code></li>
<li>Summary: <p>The deployment of autonomous agents in real-world scenarios is challenged by
"unknown unknowns", i.e. novel unexpected environments not encountered during
training, such as degraded signs. While existing research focuses on anomaly
detection and class imbalance, it often fails to address truly novel scenarios.
Our approach enhances visual perception by leveraging the Variational
Prototyping Encoder (VPE) to adeptly identify and handle novel inputs, then
incrementally augmenting data using neural style transfer to enrich
underrepresented data. By comparing models trained solely on original datasets
with those trained on a combination of original and augmented datasets, we
observed a notable improvement in the performance of the latter. This
underscores the critical role of data augmentation in enhancing model
robustness. Our findings suggest the potential benefits of incorporating
generative models for domain-specific augmentation strategies.
</p></li>
</ul>

<h3>Title: Pixel Adapter: A Graph-Based Post-Processing Approach for Scene Text Image Super-Resolution. (arXiv:2309.08919v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08919">http://arxiv.org/abs/2309.08919</a></li>
<li>Code URL: https://github.com/wenyu1009/rtsrn</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08919]] Pixel Adapter: A Graph-Based Post-Processing Approach for Scene Text Image Super-Resolution(http://arxiv.org/abs/2309.08919)</code></li>
<li>Summary: <p>Current Scene text image super-resolution approaches primarily focus on
extracting robust features, acquiring text information, and complex training
strategies to generate super-resolution images. However, the upsampling module,
which is crucial in the process of converting low-resolution images to
high-resolution ones, has received little attention in existing works. To
address this issue, we propose the Pixel Adapter Module (PAM) based on graph
attention to address pixel distortion caused by upsampling. The PAM effectively
captures local structural information by allowing each pixel to interact with
its neighbors and update features. Unlike previous graph attention mechanisms,
our approach achieves 2-3 orders of magnitude improvement in efficiency and
memory utilization by eliminating the dependency on sparse adjacency matrices
and introducing a sliding window approach for efficient parallel computation.
Additionally, we introduce the MLP-based Sequential Residual Block (MSRB) for
robust feature extraction from text images, and a Local Contour Awareness loss
($\mathcal{L}_{lca}$) to enhance the model's perception of details.
Comprehensive experiments on TextZoom demonstrate that our proposed method
generates high-quality super-resolution images, surpassing existing methods in
recognition accuracy. For single-stage and multi-stage strategies, we achieved
improvements of 0.7\% and 2.6\%, respectively, increasing the performance from
52.6\% and 53.7\% to 53.3\% and 56.3\%. The code is available at
https://github.com/wenyu1009/RTSRN.
</p></li>
</ul>

<h3>Title: Improving Robustness of Neural Inverse Text Normalization via Data-Augmentation, Semi-Supervised Learning, and Post-Aligning Method. (arXiv:2309.08626v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08626">http://arxiv.org/abs/2309.08626</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08626]] Improving Robustness of Neural Inverse Text Normalization via Data-Augmentation, Semi-Supervised Learning, and Post-Aligning Method(http://arxiv.org/abs/2309.08626)</code></li>
<li>Summary: <p>Inverse text normalization (ITN) is crucial for converting spoken-form into
written-form, especially in the context of automatic speech recognition (ASR).
While most downstream tasks of ASR rely on written-form, ASR systems often
output spoken-form, highlighting the necessity for robust ITN in product-level
ASR-based applications. Although neural ITN methods have shown promise, they
still encounter performance challenges, particularly when dealing with
ASR-generated spoken text. These challenges arise from the out-of-domain
problem between training data and ASR-generated text. To address this, we
propose a direct training approach that utilizes ASR-generated written or
spoken text, with pairs augmented through ASR linguistic context emulation and
a semi-supervised learning method enhanced by a large language model,
respectively. Additionally, we introduce a post-aligning method to manage
unpredictable errors, thereby enhancing the reliability of ITN. Our experiments
show that our proposed methods remarkably improved ITN performance in various
ASR scenarios.
</p></li>
</ul>

<h3>Title: Has Sentiment Returned to the Pre-pandemic Level? A Sentiment Analysis Using U.S. College Subreddit Data from 2019 to 2022. (arXiv:2309.08845v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08845">http://arxiv.org/abs/2309.08845</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08845]] Has Sentiment Returned to the Pre-pandemic Level? A Sentiment Analysis Using U(http://arxiv.org/abs/2309.08845)</code></li>
<li>Summary: <p>As impact of COVID-19 pandemic winds down, both individuals and society
gradually return to pre-pandemic activities. This study aims to explore how
people's emotions have changed from the pre-pandemic during the pandemic to
post-emergency period and whether it has returned to pre-pandemic level. We
collected Reddit data in 2019 (pre-pandemic), 2020 (peak pandemic), 2021, and
2022 (late stages of pandemic, transitioning period to post-emergency period)
from subreddits in 128 universities/colleges in the U.S., and a set of
school-level characteristics. We predicted two sets of sentiments from a
pre-trained Robustly Optimized BERT pre-training approach (RoBERTa) and graph
attention network (GAT) that leverages both rich semantic and relational
information among posted messages and then applied a logistic stacking method
to obtain the final sentiment classification. After obtaining sentiment label
for each message, we used a generalized linear mixed-effects model to estimate
temporal trend in sentiment from 2019 to 2022 and how school-level factors may
affect sentiment. Compared to the year 2019, the odds of negative sentiment in
years 2020, 2021, and 2022 are 24%, 4.3%, and 10.3% higher, respectively, which
are all statistically significant(adjusted $p$&lt;0.05). Our study findings
suggest a partial recovery in the sentiment composition in the
post-pandemic-emergency era. The results align with common expectations and
provide a detailed quantification of how sentiments have evolved from 2019 to
2022.
</p></li>
</ul>

<h3>Title: Robust Online Covariance and Sparse Precision Estimation Under Arbitrary Data Corruption. (arXiv:2309.08884v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08884">http://arxiv.org/abs/2309.08884</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08884]] Robust Online Covariance and Sparse Precision Estimation Under Arbitrary Data Corruption(http://arxiv.org/abs/2309.08884)</code></li>
<li>Summary: <p>Gaussian graphical models are widely used to represent correlations among
entities but remain vulnerable to data corruption. In this work, we introduce a
modified trimmed-inner-product algorithm to robustly estimate the covariance in
an online scenario even in the presence of arbitrary and adversarial data
attacks. At each time step, data points, drawn nominally independently and
identically from a multivariate Gaussian distribution, arrive. However, a
certain fraction of these points may have been arbitrarily corrupted. We
propose an online algorithm to estimate the sparse inverse covariance (i.e.,
precision) matrix despite this corruption. We provide the error-bound and
convergence properties of the estimates to the true precision matrix under our
algorithms.
</p></li>
</ul>

<h3>Title: Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits. (arXiv:2309.08748v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08748">http://arxiv.org/abs/2309.08748</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08748]] Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits(http://arxiv.org/abs/2309.08748)</code></li>
<li>Summary: <p>Without direct interaction with the environment. Often, the environment in
which the data are collected differs from the environment in which the learned
policy is applied. To account for the effect of different environments during
learning and execution, distributionally robust optimization (DRO) methods have
been developed that compute worst-case bounds on the policy values assuming
that the distribution of the new environment lies within an uncertainty set.
Typically, this uncertainty set is defined based on the KL divergence around
the empirical distribution computed from the logging dataset. However, the KL
uncertainty set fails to encompass distributions with varying support and lacks
awareness of the geometry of the distribution support. As a result, KL
approaches fall short in addressing practical environment mismatches and lead
to over-fitting to worst-case scenarios. To overcome these limitations, we
propose a novel DRO approach that employs the Wasserstein distance instead.
While Wasserstein DRO is generally computationally more expensive compared to
KL DRO, we present a regularized method and a practical (biased) stochastic
gradient descent method to optimize the policy efficiently. We also provide a
theoretical analysis of the finite sample complexity and iteration complexity
for our proposed method. We further validate our approach using a public
dataset that was recorded in a randomized stoke trial.
</p></li>
</ul>

<h3>Title: SHAPNN: Shapley Value Regularized Tabular Neural Network. (arXiv:2309.08799v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08799">http://arxiv.org/abs/2309.08799</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08799]] SHAPNN: Shapley Value Regularized Tabular Neural Network(http://arxiv.org/abs/2309.08799)</code></li>
<li>Summary: <p>We present SHAPNN, a novel deep tabular data modeling architecture designed
for supervised learning. Our approach leverages Shapley values, a
well-established technique for explaining black-box models. Our neural network
is trained using standard backward propagation optimization methods, and is
regularized with realtime estimated Shapley values. Our method offers several
advantages, including the ability to provide valid explanations with no
computational overhead for data instances and datasets. Additionally,
prediction with explanation serves as a regularizer, which improves the model's
performance. Moreover, the regularized prediction enhances the model's
capability for continual learning. We evaluate our method on various publicly
available datasets and compare it with state-of-the-art deep neural network
models, demonstrating the superior performance of SHAPNN in terms of AUROC,
transparency, as well as robustness to streaming data.
</p></li>
</ul>

<h3>Title: Distributionally Robust Post-hoc Classifiers under Prior Shifts. (arXiv:2309.08825v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08825">http://arxiv.org/abs/2309.08825</a></li>
<li>Code URL: https://github.com/weijiaheng/drops</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08825]] Distributionally Robust Post-hoc Classifiers under Prior Shifts(http://arxiv.org/abs/2309.08825)</code></li>
<li>Summary: <p>The generalization ability of machine learning models degrades significantly
when the test distribution shifts away from the training distribution. We
investigate the problem of training models that are robust to shifts caused by
changes in the distribution of class-priors or group-priors. The presence of
skewed training priors can often lead to the models overfitting to spurious
features. Unlike existing methods, which optimize for either the worst or the
average performance over classes or groups, our work is motivated by the need
for finer control over the robustness properties of the model. We present an
extremely lightweight post-hoc approach that performs scaling adjustments to
predictions from a pre-trained model, with the goal of minimizing a
distributionally robust loss around a chosen target distribution. These
adjustments are computed by solving a constrained optimization problem on a
validation set and applied to the model during test time. Our constrained
optimization objective is inspired by a natural notion of robustness to
controlled distribution shifts. Our method comes with provable guarantees and
empirically makes a strong case for distributional robust post-hoc classifiers.
An empirical implementation is available at
https://github.com/weijiaheng/Drops.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Semantic Information Extraction for Text Data with Probability Graph. (arXiv:2309.08879v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08879">http://arxiv.org/abs/2309.08879</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08879]] Semantic Information Extraction for Text Data with Probability Graph(http://arxiv.org/abs/2309.08879)</code></li>
<li>Summary: <p>In this paper, the problem of semantic information extraction for resource
constrained text data transmission is studied. In the considered model, a
sequence of text data need to be transmitted within a communication
resource-constrained network, which only allows limited data transmission.
Thus, at the transmitter, the original text data is extracted with natural
language processing techniques. Then, the extracted semantic information is
captured in a knowledge graph. An additional probability dimension is
introduced in this graph to capture the importance of each information. This
semantic information extraction problem is posed as an optimization framework
whose goal is to extract most important semantic information for transmission.
To find an optimal solution for this problem, a Floyd's algorithm based
solution coupled with an efficient sorting mechanism is proposed. Numerical
results testify the effectiveness of the proposed algorithm with regards to two
novel performance metrics including semantic uncertainty and semantic
similarity.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: EgoObjects: A Large-Scale Egocentric Dataset for Fine-Grained Object Understanding. (arXiv:2309.08816v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08816">http://arxiv.org/abs/2309.08816</a></li>
<li>Code URL: https://github.com/facebookresearch/egoobjects</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08816]] EgoObjects: A Large-Scale Egocentric Dataset for Fine-Grained Object Understanding(http://arxiv.org/abs/2309.08816)</code></li>
<li>Summary: <p>Object understanding in egocentric visual data is arguably a fundamental
research topic in egocentric vision. However, existing object datasets are
either non-egocentric or have limitations in object categories, visual content,
and annotation granularities. In this work, we introduce EgoObjects, a
large-scale egocentric dataset for fine-grained object understanding. Its Pilot
version contains over 9K videos collected by 250 participants from 50+
countries using 4 wearable devices, and over 650K object annotations from 368
object categories. Unlike prior datasets containing only object category
labels, EgoObjects also annotates each object with an instance-level
identifier, and includes over 14K unique object instances. EgoObjects was
designed to capture the same object under diverse background complexities,
surrounding objects, distance, lighting and camera motion. In parallel to the
data collection, we conducted data annotation by developing a multi-stage
federated annotation process to accommodate the growing nature of the dataset.
To bootstrap the research on EgoObjects, we present a suite of 4 benchmark
tasks around the egocentric object understanding, including a novel instance
level- and the classical category level object detection. Moreover, we also
introduce 2 novel continual learning object detection tasks. The dataset and
API are available at https://github.com/facebookresearch/EgoObjects.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Bias and Fairness in Chatbots: An Overview. (arXiv:2309.08836v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08836">http://arxiv.org/abs/2309.08836</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08836]] Bias and Fairness in Chatbots: An Overview(http://arxiv.org/abs/2309.08836)</code></li>
<li>Summary: <p>Chatbots have been studied for more than half a century. With the rapid
development of natural language processing (NLP) technologies in recent years,
chatbots using large language models (LLMs) have received much attention
nowadays. Compared with traditional ones, modern chatbots are more powerful and
have been used in real-world applications. There are however, bias and fairness
concerns in modern chatbot design. Due to the huge amounts of training data,
extremely large model sizes, and lack of interpretability, bias mitigation and
fairness preservation of modern chatbots are challenging. Thus, a comprehensive
overview on bias and fairness in chatbot systems is given in this paper. The
history of chatbots and their categories are first reviewed. Then, bias sources
and potential harms in applications are analyzed. Considerations in designing
fair and unbiased chatbot systems are examined. Finally, future research
directions are discussed.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>explainability</h2>
<h3>Title: Concept explainability for plant diseases classification. (arXiv:2309.08739v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08739">http://arxiv.org/abs/2309.08739</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08739]] Concept explainability for plant diseases classification(http://arxiv.org/abs/2309.08739)</code></li>
<li>Summary: <p>Plant diseases remain a considerable threat to food security and agricultural
sustainability. Rapid and early identification of these diseases has become a
significant concern motivating several studies to rely on the increasing global
digitalization and the recent advances in computer vision based on deep
learning. In fact, plant disease classification based on deep convolutional
neural networks has shown impressive performance. However, these methods have
yet to be adopted globally due to concerns regarding their robustness,
transparency, and the lack of explainability compared with their human experts
counterparts. Methods such as saliency-based approaches associating the network
output to perturbations of the input pixels have been proposed to give insights
into these algorithms. Still, they are not easily comprehensible and not
intuitive for human users and are threatened by bias. In this work, we deploy a
method called Testing with Concept Activation Vectors (TCAV) that shifts the
focus from pixels to user-defined concepts. To the best of our knowledge, our
paper is the first to employ this method in the field of plant disease
classification. Important concepts such as color, texture and disease related
concepts were analyzed. The results suggest that concept-based explanation
methods can significantly benefit automated plant disease identification.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Biased Attention: Do Vision Transformers Amplify Gender Bias More than Convolutional Neural Networks?. (arXiv:2309.08760v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08760">http://arxiv.org/abs/2309.08760</a></li>
<li>Code URL: https://github.com/aibhishek/Biased-Attention</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08760]] Biased Attention: Do Vision Transformers Amplify Gender Bias More than Convolutional Neural Networks?(http://arxiv.org/abs/2309.08760)</code></li>
<li>Summary: <p>Deep neural networks used in computer vision have been shown to exhibit many
social biases such as gender bias. Vision Transformers (ViTs) have become
increasingly popular in computer vision applications, outperforming
Convolutional Neural Networks (CNNs) in many tasks such as image
classification. However, given that research on mitigating bias in computer
vision has primarily focused on CNNs, it is important to evaluate the effect of
a different network architecture on the potential for bias amplification. In
this paper we therefore introduce a novel metric to measure bias in
architectures, Accuracy Difference. We examine bias amplification when models
belonging to these two architectures are used as a part of large multimodal
models, evaluating the different image encoders of Contrastive Language Image
Pretraining which is an important model used in many generative models such as
DALL-E and Stable Diffusion. Our experiments demonstrate that architecture can
play a role in amplifying social biases due to the different techniques
employed by the models for feature extraction and embedding as well as their
different learning properties. This research found that ViTs amplified gender
bias to a greater extent than CNNs
</p></li>
</ul>

<h3>Title: Pretraining on the Test Set Is All You Need. (arXiv:2309.08632v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08632">http://arxiv.org/abs/2309.08632</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08632]] Pretraining on the Test Set Is All You Need(http://arxiv.org/abs/2309.08632)</code></li>
<li>Summary: <p>Inspired by recent work demonstrating the promise of smaller
Transformer-based language models pretrained on carefully curated data, we
supercharge such approaches by investing heavily in curating a novel, high
quality, non-synthetic data mixture based solely on evaluation benchmarks.
Using our novel dataset mixture consisting of less than 100 thousand tokens, we
pretrain a 1 million parameter transformer-based LLM \textbf{phi-CTNL}
(pronounced ``fictional") that achieves perfect results across diverse academic
benchmarks, strictly outperforming all known foundation models.
\textbf{phi-CTNL} also beats power-law scaling and exhibits a never-before-seen
grokking-like ability to accurately predict downstream evaluation benchmarks'
canaries.
</p></li>
</ul>

<h3>Title: Cure the headache of Transformers via Collinear Constrained Attention. (arXiv:2309.08646v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08646">http://arxiv.org/abs/2309.08646</a></li>
<li>Code URL: https://github.com/luban-agi/coca</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08646]] Cure the headache of Transformers via Collinear Constrained Attention(http://arxiv.org/abs/2309.08646)</code></li>
<li>Summary: <p>As the rapid progression of practical applications based on Large Language
Models continues, the importance of extrapolating performance has grown
exponentially in the research domain. In our study, we identified an anomalous
behavior in Transformer models that had been previously overlooked, leading to
a chaos around closest tokens which carried the most important information.
We've coined this discovery the "headache of Transformers". To address this at
its core, we introduced a novel self-attention structure named Collinear
Constrained Attention (CoCA). This structure can be seamlessly integrated with
existing extrapolation, interpolation methods, and other optimization
strategies designed for traditional Transformer models. We have achieved
excellent extrapolating performance even for 16 times to 24 times of sequence
lengths during inference without any fine-tuning on our model. We have also
enhanced CoCA's computational and spatial efficiency to ensure its
practicality. We plan to open-source CoCA shortly. In the meantime, we've made
our code available in the appendix for reappearing experiments.
</p></li>
</ul>

<h3>Title: Frustratingly Simple Memory Efficiency for Pre-trained Language Models via Dynamic Embedding Pruning. (arXiv:2309.08708v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08708">http://arxiv.org/abs/2309.08708</a></li>
<li>Code URL: https://github.com/mlsw/dynamic-embedding-pruning</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08708]] Frustratingly Simple Memory Efficiency for Pre-trained Language Models via Dynamic Embedding Pruning(http://arxiv.org/abs/2309.08708)</code></li>
<li>Summary: <p>The extensive memory footprint of pre-trained language models (PLMs) can
hinder deployment in memory-constrained settings, such as cloud environments or
on-device. PLMs use embedding matrices to represent extensive vocabularies,
forming a large proportion of the model parameters. While previous work towards
parameter-efficient PLM development has considered pruning parameters within
the transformer layers, pruning the embedding matrix as part of fine-tuning or
inference has yet to be explored. We first demonstrate that a significant
proportion of the vocabulary remains unused in these scenarios. We then propose
a simple yet effective approach that leverages this finding to minimize the
memory footprint of the embedding matrix. We show that this approach provides
substantial reductions in memory usage across a wide range of models and tasks.
Notably, our approach maintains equivalent downstream task performance while
allowing a more efficient use of compute resources.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing? (ver. 23Q3). (arXiv:2309.08636v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08636">http://arxiv.org/abs/2309.08636</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08636]] ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert(http://arxiv.org/abs/2309.08636)</code></li>
<li>Summary: <p>Historically, proficient writing was deemed essential for human advancement,
with creative expression viewed as one of the hallmarks of human achievement.
However, recent advances in generative AI have marked an inflection point in
this narrative, including for scientific writing. This article provides a
comprehensive analysis of the capabilities and limitations of six AI chatbots
in scholarly writing in the humanities and archaeology. The methodology was
based on tagging AI generated content for quantitative accuracy and qualitative
precision by human experts. Quantitative accuracy assessed the factual
correctness, while qualitative precision gauged the scientific contribution.
While the AI chatbots, especially ChatGPT-4, demonstrated proficiency in
recombining existing knowledge, they failed in generating original scientific
content. As a side note, our results also suggest that with ChatGPT-4 the size
of the LLMs has plateaued. Furthermore, the paper underscores the intricate and
recursive nature of human research. This process of transforming raw data into
refined knowledge is computationally irreducible, which highlights the
challenges AI chatbots face in emulating human originality in scientific
writing. In conclusion, while large language models have revolutionised content
generation, their ability to produce original scientific contributions in the
humanities remains limited. We expect that this will change in the near future
with the evolution of current LLM-based AI chatbots towards LLM-powered
software.
</p></li>
</ul>

<h3>Title: Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models. (arXiv:2309.08902v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08902">http://arxiv.org/abs/2309.08902</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08902]] Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models(http://arxiv.org/abs/2309.08902)</code></li>
<li>Summary: <p>LLMs are increasingly powerful and widely used to assist users in a variety
of tasks. This use risks the introduction of LLM biases to consequential
decisions such as job hiring, human performance evaluation, and criminal
sentencing. Bias in NLP systems along the lines of gender and ethnicity has
been widely studied, especially for specific stereotypes (e.g., Asians are good
at math). In this paper, we investigate bias along less studied, but still
consequential, dimensions, such as age and beauty, measuring subtler correlated
decisions that LLMs (specially autoregressive language models) make between
social groups and unrelated positive and negative attributes. We ask whether
LLMs hold wide-reaching biases of positive or negative sentiment for specific
social groups similar to the ``what is beautiful is good'' bias found in people
in experimental psychology. We introduce a template-generated dataset of
sentence completion tasks that asks the model to select the most appropriate
attribute to complete an evaluative statement about a person described as a
member of a specific social group. We also reverse the completion task to
select the social group based on an attribute. Finally, we report the
correlations that we find for multiple cutting-edge LLMs. This dataset can be
used as a benchmark to evaluate progress in more generalized biases and the
templating technique can be used to expand the benchmark with minimal
additional human annotation.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Challenges in Annotating Datasets to Quantify Bias in Under-represented Society. (arXiv:2309.08624v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08624">http://arxiv.org/abs/2309.08624</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08624]] Challenges in Annotating Datasets to Quantify Bias in Under-represented Society(http://arxiv.org/abs/2309.08624)</code></li>
<li>Summary: <p>Recent advances in artificial intelligence, including the development of
highly sophisticated large language models (LLM), have proven beneficial in
many real-world applications. However, evidence of inherent bias encoded in
these LLMs has raised concerns about equity. In response, there has been an
increase in research dealing with bias, including studies focusing on
quantifying bias and developing debiasing techniques. Benchmark bias datasets
have also been developed for binary gender classification and ethical/racial
considerations, focusing predominantly on American demographics. However, there
is minimal research in understanding and quantifying bias related to
under-represented societies. Motivated by the lack of annotated datasets for
quantifying bias in under-represented societies, we endeavoured to create
benchmark datasets for the New Zealand (NZ) population. We faced many
challenges in this process, despite the availability of three annotators. This
research outlines the manual annotation process, provides an overview of the
challenges we encountered and lessons learnt, and presents recommendations for
future research.
</p></li>
</ul>

<h3>Title: Performance of ChatGPT-3.5 and GPT-4 on the United States Medical Licensing Examination With and Without Distractions. (arXiv:2309.08625v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08625">http://arxiv.org/abs/2309.08625</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08625]] Performance of ChatGPT-3(http://arxiv.org/abs/2309.08625)</code></li>
<li>Summary: <p>As Large Language Models (LLMs) are predictive models building their response
based on the words in the prompts, there is a risk that small talk and
irrelevant information may alter the response and the suggestion given.
Therefore, this study aims to investigate the impact of medical data mixed with
small talk on the accuracy of medical advice provided by ChatGPT. USMLE step 3
questions were used as a model for relevant medical data. We use both multiple
choice and open ended questions. We gathered small talk sentences from human
participants using the Mechanical Turk platform. Both sets of USLME questions
were arranged in a pattern where each sentence from the original questions was
followed by a small talk sentence. ChatGPT 3.5 and 4 were asked to answer both
sets of questions with and without the small talk sentences. A board-certified
physician analyzed the answers by ChatGPT and compared them to the formal
correct answer. The analysis results demonstrate that the ability of
ChatGPT-3.5 to answer correctly was impaired when small talk was added to
medical data for multiple-choice questions (72.1\% vs. 68.9\%) and open
questions (61.5\% vs. 44.3\%; p=0.01), respectively. In contrast, small talk
phrases did not impair ChatGPT-4 ability in both types of questions (83.6\% and
66.2\%, respectively). According to these results, ChatGPT-4 seems more
accurate than the earlier 3.5 version, and it appears that small talk does not
impair its capability to provide medical recommendations. Our results are an
important first step in understanding the potential and limitations of
utilizing ChatGPT and other LLMs for physician-patient interactions, which
include casual conversations.
</p></li>
</ul>

<h3>Title: Large Language Models Can Infer Psychological Dispositions of Social Media Users. (arXiv:2309.08631v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08631">http://arxiv.org/abs/2309.08631</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08631]] Large Language Models Can Infer Psychological Dispositions of Social Media Users(http://arxiv.org/abs/2309.08631)</code></li>
<li>Summary: <p>As Large Language Models (LLMs) demonstrate increasingly human-like abilities
in various natural language processing (NLP) tasks that are bound to become
integral to personalized technologies, understanding their capabilities and
inherent biases is crucial. Our study investigates the potential of LLMs like
ChatGPT to infer psychological dispositions of individuals from their digital
footprints. Specifically, we assess the ability of GPT-3.5 and GPT-4 to derive
the Big Five personality traits from users' Facebook status updates in a
zero-shot learning scenario. Our results show an average correlation of r = .29
(range = [.22, .33]) between LLM-inferred and self-reported trait scores.
Furthermore, our findings suggest biases in personality inferences with regard
to gender and age: inferred scores demonstrated smaller errors for women and
younger individuals on several traits, suggesting a potential systematic bias
stemming from the underlying training data or differences in online
self-expression.
</p></li>
</ul>

<h3>Title: TextBind: Multi-turn Interleaved Multimodal Instruction-following. (arXiv:2309.08637v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08637">http://arxiv.org/abs/2309.08637</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08637]] TextBind: Multi-turn Interleaved Multimodal Instruction-following(http://arxiv.org/abs/2309.08637)</code></li>
<li>Summary: <p>Large language models with instruction-following abilities have
revolutionized the field of artificial intelligence. These models show
exceptional generalizability to tackle various real-world tasks through their
natural language interfaces. However, their performance heavily relies on
high-quality exemplar data, which is often difficult to obtain. This challenge
is further exacerbated when it comes to multimodal instruction following. We
introduce TextBind, an almost annotation-free framework for empowering larger
language models with the multi-turn interleaved multimodal
instruction-following capabilities. Our approach requires only image-caption
pairs and generates multi-turn multimodal instruction-response conversations
from a language model. We release our dataset, model, and demo to foster future
research in the area of multimodal instruction following.
</p></li>
</ul>

<h3>Title: MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings. (arXiv:2309.08648v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08648">http://arxiv.org/abs/2309.08648</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08648]] MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings(http://arxiv.org/abs/2309.08648)</code></li>
<li>Summary: <p>Despite the rapid advancement of mobile applications, predicting app usage
remains a formidable challenge due to intricate user behaviours and
ever-evolving contexts. To address these issues, this paper introduces the
Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE) model.
This innovative approach utilizes Large Language Models (LLMs) to predict app
usage accurately. Rigorous testing on two public datasets highlights MAPLE's
capability to decipher intricate patterns and comprehend user contexts. These
robust results confirm MAPLE's versatility and resilience across various
scenarios. While its primary design caters to app prediction, the outcomes also
emphasize the broader applicability of LLMs in different domains. Through this
research, we emphasize the potential of LLMs in app usage prediction and
suggest their transformative capacity in modelling human behaviours across
diverse fields.
</p></li>
</ul>

<h3>Title: Fake News Detectors are Biased against Texts Generated by Large Language Models. (arXiv:2309.08674v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08674">http://arxiv.org/abs/2309.08674</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08674]] Fake News Detectors are Biased against Texts Generated by Large Language Models(http://arxiv.org/abs/2309.08674)</code></li>
<li>Summary: <p>The spread of fake news has emerged as a critical challenge, undermining
trust and posing threats to society. In the era of Large Language Models
(LLMs), the capability to generate believable fake content has intensified
these concerns. In this study, we present a novel paradigm to evaluate fake
news detectors in scenarios involving both human-written and LLM-generated
misinformation. Intriguingly, our findings reveal a significant bias in many
existing detectors: they are more prone to flagging LLM-generated content as
fake news while often misclassifying human-written fake news as genuine. This
unexpected bias appears to arise from distinct linguistic patterns inherent to
LLM outputs. To address this, we introduce a mitigation strategy that leverages
adversarial training with LLM-paraphrased genuine news. The resulting model
yielded marked improvements in detection accuracy for both human and
LLM-generated news. To further catalyze research in this domain, we release two
comprehensive datasets, \texttt{GossipCop++} and \texttt{PolitiFact++}, thus
amalgamating human-validated articles with LLM-generated fake and real news.
</p></li>
</ul>

<h3>Title: PDFTriage: Question Answering over Long, Structured Documents. (arXiv:2309.08872v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08872">http://arxiv.org/abs/2309.08872</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08872]] PDFTriage: Question Answering over Long, Structured Documents(http://arxiv.org/abs/2309.08872)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have issues with document question answering
(QA) in situations where the document is unable to fit in the small context
length of an LLM. To overcome this issue, most existing works focus on
retrieving the relevant context from the document, representing them as plain
text. However, documents such as PDFs, web pages, and presentations are
naturally structured with different pages, tables, sections, and so on.
Representing such structured documents as plain text is incongruous with the
user's mental model of these documents with rich structure. When a system has
to query the document for context, this incongruity is brought to the fore, and
seemingly trivial questions can trip up the QA system. To bridge this
fundamental gap in handling structured documents, we propose an approach called
PDFTriage that enables models to retrieve the context based on either structure
or content. Our experiments demonstrate the effectiveness of the proposed
PDFTriage-augmented models across several classes of questions where existing
retrieval-augmented LLMs fail. To facilitate further research on this
fundamental problem, we release our benchmark dataset consisting of 900+
human-generated questions over 80 structured documents from 10 different
categories of question types for document QA.
</p></li>
</ul>

<h3>Title: X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs. (arXiv:2309.08873v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08873">http://arxiv.org/abs/2309.08873</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08873]] X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs(http://arxiv.org/abs/2309.08873)</code></li>
<li>Summary: <p>Understanding when two pieces of text convey the same information is a goal
touching many subproblems in NLP, including textual entailment and
fact-checking. This problem becomes more complex when those two pieces of text
are in different languages. Here, we introduce X-PARADE (Cross-lingual
Paragraph-level Analysis of Divergences and Entailments), the first
cross-lingual dataset of paragraph-level information divergences. Annotators
label a paragraph in a target language at the span level and evaluate it with
respect to a corresponding paragraph in a source language, indicating whether a
given piece of information is the same, new, or new but can be inferred. This
last notion establishes a link with cross-language NLI. Aligned paragraphs are
sourced from Wikipedia pages in different languages, reflecting real
information divergences observed in the wild. Armed with our dataset, we
investigate a diverse set of approaches for this problem, including classic
token alignment from machine translation, textual entailment methods that
localize their decisions, and prompting of large language models. Our results
show that these methods vary in their capability to handle inferable
information, but they all fall short of human performance.
</p></li>
</ul>

<h3>Title: Rethinking Learning Rate Tuning in the Era of Large Language Models. (arXiv:2309.08859v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08859">http://arxiv.org/abs/2309.08859</a></li>
<li>Code URL: https://github.com/mlsysx/lrbenchplusplus</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08859]] Rethinking Learning Rate Tuning in the Era of Large Language Models(http://arxiv.org/abs/2309.08859)</code></li>
<li>Summary: <p>Large Language Models (LLMs) represent the recent success of deep learning in
achieving remarkable human-like predictive performance. It has become a
mainstream strategy to leverage fine-tuning to adapt LLMs for various
real-world applications due to the prohibitive expenses associated with LLM
training. The learning rate is one of the most important hyperparameters in LLM
fine-tuning with direct impacts on both fine-tuning efficiency and fine-tuned
LLM quality. Existing learning rate policies are primarily designed for
training traditional deep neural networks (DNNs), which may not work well for
LLM fine-tuning. We reassess the research challenges and opportunities of
learning rate tuning in the coming era of Large Language Models. This paper
makes three original contributions. First, we revisit existing learning rate
policies to analyze the critical challenges of learning rate tuning in the era
of LLMs. Second, we present LRBench++ to benchmark learning rate policies and
facilitate learning rate tuning for both traditional DNNs and LLMs. Third, our
experimental analysis with LRBench++ demonstrates the key differences between
LLM fine-tuning and traditional DNN training and validates our analysis.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Segmentation of Tubular Structures Using Iterative Training with Tailored Samples. (arXiv:2309.08727v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08727">http://arxiv.org/abs/2309.08727</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08727]] Segmentation of Tubular Structures Using Iterative Training with Tailored Samples(http://arxiv.org/abs/2309.08727)</code></li>
<li>Summary: <p>We propose a minimal path method to simultaneously compute segmentation masks
and extract centerlines of tubular structures with line-topology. Minimal path
methods are commonly used for the segmentation of tubular structures in a wide
variety of applications. Recent methods use features extracted by CNNs, and
often outperform methods using hand-tuned features. However, for CNN-based
methods, the samples used for training may be generated inappropriately, so
that they can be very different from samples encountered during inference. We
approach this discrepancy by introducing a novel iterative training scheme,
which enables generating better training samples specifically tailored for the
minimal path methods without changing existing annotations. In our method,
segmentation masks and centerlines are not determined after one another by
post-processing, but obtained using the same steps. Our method requires only
very few annotated training images. Comparison with seven previous approaches
on three public datasets, including satellite images and medical images, shows
that our method achieves state-of-the-art results both for segmentation masks
and centerlines.
</p></li>
</ul>

<h3>Title: MA-SAM: Modality-agnostic SAM Adaptation for 3D Medical Image Segmentation. (arXiv:2309.08842v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08842">http://arxiv.org/abs/2309.08842</a></li>
<li>Code URL: https://github.com/cchen-cc/ma-sam</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08842]] MA-SAM: Modality-agnostic SAM Adaptation for 3D Medical Image Segmentation(http://arxiv.org/abs/2309.08842)</code></li>
<li>Summary: <p>The Segment Anything Model (SAM), a foundation model for general image
segmentation, has demonstrated impressive zero-shot performance across numerous
natural image segmentation tasks. However, SAM's performance significantly
declines when applied to medical images, primarily due to the substantial
disparity between natural and medical image domains. To effectively adapt SAM
to medical images, it is important to incorporate critical third-dimensional
information, i.e., volumetric or temporal knowledge, during fine-tuning.
Simultaneously, we aim to harness SAM's pre-trained weights within its original
2D backbone to the fullest extent. In this paper, we introduce a
modality-agnostic SAM adaptation framework, named as MA-SAM, that is applicable
to various volumetric and video medical data. Our method roots in the
parameter-efficient fine-tuning strategy to update only a small portion of
weight increments while preserving the majority of SAM's pre-trained weights.
By injecting a series of 3D adapters into the transformer blocks of the image
encoder, our method enables the pre-trained 2D backbone to extract
third-dimensional information from input data. The effectiveness of our method
has been comprehensively evaluated on four medical image segmentation tasks, by
using 10 public datasets across CT, MRI, and surgical video data. Remarkably,
without using any prompt, our method consistently outperforms various
state-of-the-art 3D approaches, surpassing nnU-Net by 0.9%, 2.6%, and 9.9% in
Dice for CT multi-organ segmentation, MRI prostate segmentation, and surgical
scene segmentation respectively. Our model also demonstrates strong
generalization, and excels in challenging tumor segmentation when prompts are
used. Our code is available at: https://github.com/cchen-cc/MA-SAM.
</p></li>
</ul>

<h3>Title: GCL: Gradient-Guided Contrastive Learning for Medical Image Segmentation with Multi-Perspective Meta Labels. (arXiv:2309.08888v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08888">http://arxiv.org/abs/2309.08888</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08888]] GCL: Gradient-Guided Contrastive Learning for Medical Image Segmentation with Multi-Perspective Meta Labels(http://arxiv.org/abs/2309.08888)</code></li>
<li>Summary: <p>Since annotating medical images for segmentation tasks commonly incurs
expensive costs, it is highly desirable to design an annotation-efficient
method to alleviate the annotation burden. Recently, contrastive learning has
exhibited a great potential in learning robust representations to boost
downstream tasks with limited labels. In medical imaging scenarios, ready-made
meta labels (i.e., specific attribute information of medical images) inherently
reveal semantic relationships among images, which have been used to define
positive pairs in previous work. However, the multi-perspective semantics
revealed by various meta labels are usually incompatible and can incur
intractable "semantic contradiction" when combining different meta labels. In
this paper, we tackle the issue of "semantic contradiction" in a
gradient-guided manner using our proposed Gradient Mitigator method, which
systematically unifies multi-perspective meta labels to enable a pre-trained
model to attain a better high-level semantic recognition ability. Moreover, we
emphasize that the fine-grained discrimination ability is vital for
segmentation-oriented pre-training, and develop a novel method called Gradient
Filter to dynamically screen pixel pairs with the most discriminating power
based on the magnitude of gradients. Comprehensive experiments on four medical
image segmentation datasets verify that our new method GCL: (1) learns
informative image representations and considerably boosts segmentation
performance with limited labels, and (2) shows promising generalizability on
out-of-distribution datasets.
</p></li>
</ul>

<h3>Title: S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking in the Era of LLMs. (arXiv:2309.08827v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08827">http://arxiv.org/abs/2309.08827</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08827]] S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking in the Era of LLMs(http://arxiv.org/abs/2309.08827)</code></li>
<li>Summary: <p>The traditional Dialogue State Tracking (DST) problem aims to track user
preferences and intents in user-agent conversations. While sufficient for
task-oriented dialogue systems supporting narrow domain applications, the
advent of Large Language Model (LLM)-based chat systems has introduced many
real-world intricacies in open-domain dialogues. These intricacies manifest in
the form of increased complexity in contextual interactions, extended dialogue
sessions encompassing a diverse array of topics, and more frequent contextual
shifts. To handle these intricacies arising from evolving LLM-based chat
systems, we propose joint dialogue segmentation and state tracking per segment
in open-domain dialogue systems. Assuming a zero-shot setting appropriate to a
true open-domain dialogue system, we propose S3-DST, a structured prompting
technique that harnesses Pre-Analytical Recollection, a novel grounding
mechanism we designed for improving long context tracking. To demonstrate the
efficacy of our proposed approach in joint segmentation and state tracking, we
evaluate S3-DST on a proprietary anonymized open-domain dialogue dataset, as
well as publicly available DST and segmentation datasets. Across all datasets
and settings, S3-DST consistently outperforms the state-of-the-art,
demonstrating its potency and robustness the next generation of LLM-based chat
systems.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
