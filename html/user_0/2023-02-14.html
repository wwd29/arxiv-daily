<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Transient Attack against the KLJN Secure Key Exchanger. (arXiv:2302.05607v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05607">http://arxiv.org/abs/2302.05607</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05607] Transient Attack against the KLJN Secure Key Exchanger](http://arxiv.org/abs/2302.05607) #secure</code></li>
<li>Summary: <p>We demonstrate the security vulnerability of the ideal
Kirchhoff-Law-Johnson-Noise (KLJN) key exchanger against transient attacks.
Transients start when Alice and Bob connect the wire to their chosen resistor
at the beginning of each clock cycle. A transient attack takes place during a
short duration of time, before the transients reflected from the end of Alice
and Bob mix together. The information leak arises from the fact that Eve (the
eavesdropper) monitors the cable, and analyzes the transients during this time
period. We will demonstrate such a transient attack, and, then we introduce a
defense protocol to protect against the attack. Computer simulations
demonstrate that after applying the defense method the information leak becomes
negligible.
</p></li>
</ul>

<h3>Title: Powerful Primitives in the Bounded Quantum Storage Model. (arXiv:2302.05724v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05724">http://arxiv.org/abs/2302.05724</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05724] Powerful Primitives in the Bounded Quantum Storage Model](http://arxiv.org/abs/2302.05724) #secure</code></li>
<li>Summary: <p>The bounded quantum storage model aims to achieve security against
computationally unbounded adversaries that are restricted only with respect to
their quantum memories. In this work, we show the power of this model by
providing everlasting and information-theoretic secure constructions for the
following primitives: (1) Symmetric key encryption, message-authentication and
one-time programs. These schemes require no quantum memory for the honest user
while they can be made secure against adversaries with arbitrarily large
memories. (2) Program broadcast, asymmetric key encryption, encryption tokens,
signatures, and signature tokens. These schemes are secure against adversaries
with roughly $e^{\sqrt{m}}$ quantum memory where $m$ is the quantum memory
required of the honest user. All of the constructions additionally satisfy
notions of disappearing and unclonable security.
</p></li>
</ul>

<h3>Title: Cross-center Early Sepsis Recognition by Medical Knowledge Guided Collaborative Learning for Data-scarce Hospitals. (arXiv:2302.05702v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05702">http://arxiv.org/abs/2302.05702</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05702] Cross-center Early Sepsis Recognition by Medical Knowledge Guided Collaborative Learning for Data-scarce Hospitals](http://arxiv.org/abs/2302.05702) #secure</code></li>
<li>Summary: <p>There are significant regional inequities in health resources around the
world. It has become one of the most focused topics to improve health services
for data-scarce hospitals and promote health equity through knowledge sharing
among medical institutions. Because electronic medical records (EMRs) contain
sensitive personal information, privacy protection is unavoidable and essential
for multi-hospital collaboration. In this paper, for a common disease in ICU
patients, sepsis, we propose a novel cross-center collaborative learning
framework guided by medical knowledge, SofaNet, to achieve early recognition of
this disease. The Sepsis-3 guideline, published in 2016, defines that sepsis
can be diagnosed by satisfying both suspicion of infection and Sequential Organ
Failure Assessment (SOFA) greater than or equal to 2. Based on this knowledge,
SofaNet adopts a multi-channel GRU structure to predict SOFA values of
different systems, which can be seen as an auxiliary task to generate better
health status representations for sepsis recognition. Moreover, we only achieve
feature distribution alignment in the hidden space during cross-center
collaborative learning, which ensures secure and compliant knowledge transfer
without raw data exchange. Extensive experiments on two open clinical datasets,
MIMIC-III and Challenge, demonstrate that SofaNet can benefit early sepsis
recognition when hospitals only have limited EMRs.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Flexible-modal Deception Detection with Audio-Visual Adapter. (arXiv:2302.05727v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05727">http://arxiv.org/abs/2302.05727</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05727] Flexible-modal Deception Detection with Audio-Visual Adapter](http://arxiv.org/abs/2302.05727) #security</code></li>
<li>Summary: <p>Detecting deception by human behaviors is vital in many fields such as custom
security and multimedia anti-fraud. Recently, audio-visual deception detection
attracts more attention due to its better performance than using only a single
modality. However, in real-world multi-modal settings, the integrity of data
can be an issue (e.g., sometimes only partial modalities are available). The
missing modality might lead to a decrease in performance, but the model still
learns the features of the missed modality. In this paper, to further improve
the performance and overcome the missing modality problem, we propose a novel
Transformer-based framework with an Audio-Visual Adapter (AVA) to fuse temporal
features across two modalities efficiently. Extensive experiments conducted on
two benchmark datasets demonstrate that the proposed method can achieve
superior performance compared with other multi-modal fusion methods under
flexible-modal (multiple and missing modalities) settings.
</p></li>
</ul>

<h3>Title: Sequential Embedding-based Attentive (SEA) classifier for malware classification. (arXiv:2302.05728v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05728">http://arxiv.org/abs/2302.05728</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05728] Sequential Embedding-based Attentive (SEA) classifier for malware classification](http://arxiv.org/abs/2302.05728) #security</code></li>
<li>Summary: <p>The tremendous growth in smart devices has uplifted several security threats.
One of the most prominent threats is malicious software also known as malware.
Malware has the capability of corrupting a device and collapsing an entire
network. Therefore, its early detection and mitigation are extremely important
to avoid catastrophic effects. In this work, we came up with a solution for
malware detection using state-of-the-art natural language processing (NLP)
techniques. Our main focus is to provide a lightweight yet effective classifier
for malware detection which can be used for heterogeneous devices, be it a
resource constraint device or a resourceful machine. Our proposed model is
tested on the benchmark data set with an accuracy and log loss score of 99.13
percent and 0.04 respectively.
</p></li>
</ul>

<h3>Title: Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks. (arXiv:2302.05733v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05733">http://arxiv.org/abs/2302.05733</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05733] Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks](http://arxiv.org/abs/2302.05733) #security</code></li>
<li>Summary: <p>Recent advances in instruction-following large language models (LLMs) have
led to dramatic improvements in a range of NLP tasks. Unfortunately, we find
that the same improved capabilities amplify the dual-use risks for malicious
purposes of these models. Dual-use is difficult to prevent as
instruction-following capabilities now enable standard attacks from computer
security. The capabilities of these instruction-following LLMs provide strong
economic incentives for dual-use by malicious actors. In particular, we show
that instruction-following LLMs can produce targeted malicious content,
including hate speech and scams, bypassing in-the-wild defenses implemented by
LLM API vendors. Our analysis shows that this content can be generated
economically and at cost likely lower than with human effort alone. Together,
our findings suggest that LLMs will increasingly attract more sophisticated
adversaries and attacks, and addressing these attacks may require new
approaches to mitigations.
</p></li>
</ul>

<h3>Title: Bl0ck: Paralyzing 802.11 connections through Block Ack frames. (arXiv:2302.05899v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05899">http://arxiv.org/abs/2302.05899</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05899] Bl0ck: Paralyzing 802](http://arxiv.org/abs/2302.05899) #security</code></li>
<li>Summary: <p>Despite Wi-Fi is at the eve of its seventh generation, security concerns
regarding this omnipresent technology remain in the spotlight of the research
community. This work introduces two new denial of service attacks against
contemporary Wi-Fi 5 and 6 networks. Differently to similar works in the
literature which focus on 802.11 management frames, the introduced assaults
exploit control frames. Both the attacks target the central element of any
infrastructure-based 802.11 network, i.e., the access point (AP), and result in
depriving the associated stations from any service. We demonstrate that, at the
very least, the attacks affect a great mass of off-the-self AP implementations
by different renowned vendors, and it can be mounted with inexpensive
equipment, little effort, and a low level of expertise. With reference to the
latest standard, namely, 802.11-2020, we elaborate on the root cause of the
respected vulnerabilities, pinpointing shortcomings. Following a coordinated
vulnerability disclosure process, our findings have been promptly communicated
to each affected AP vendor, already receiving positive feedback as well as a -
currently reserved - common vulnerabilities and exposures (CVE) id, namely
CVE-2022-32666.
</p></li>
</ul>

<h3>Title: Machine Learning Assisted Bad Data Detection for High-throughput Substation Communication. (arXiv:2302.05949v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05949">http://arxiv.org/abs/2302.05949</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05949] Machine Learning Assisted Bad Data Detection for High-throughput Substation Communication](http://arxiv.org/abs/2302.05949) #security</code></li>
<li>Summary: <p>Electrical substations are becoming more prone to cyber-attacks due to
increasing digitalization. Prevailing defense measures based on cyber rules are
often inadequate to detect attacks that use legitimate-looking measurements. In
this work, we design and implement a bad data detection solution for electrical
substations called ResiGate, that effectively combines a physics-based approach
and a machine-learning-based approach to provide substantial speed-up in
high-throughput substation communication scenarios, while still maintaining
high detection accuracy and confidence. While many existing physics-based
schemes are designed for deployment in control centers (due to their high
computational requirement), ResiGate is designed as a security appliance that
can be deployed on low-cost industrial computers at the edge of the smart grid
so that it can detect local substation-level attacks in a timely manner. A key
challenge for this is to continuously run the computationally demanding
physics-based analysis to monitor the measurement data frequently transmitted
in a typical substation. To provide high throughput without sacrificing
accuracy, ResiGate uses machine learning to effectively filter out most of the
non-suspicious (normal) data and thereby reducing the overall computational
load, allowing efficient performance even with a high volume of network
traffic. We implement ResiGate on a low-cost industrial computer and our
experiments confirm that ResiGate can detect attacks with zero error while
sustaining a high throughput.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Privacy Against Agnostic Inference Attack in Vertical Federated Learning. (arXiv:2302.05545v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05545">http://arxiv.org/abs/2302.05545</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05545] Privacy Against Agnostic Inference Attack in Vertical Federated Learning](http://arxiv.org/abs/2302.05545) #privacy</code></li>
<li>Summary: <p>A novel form of inference attack in vertical federated learning (VFL) is
proposed, where two parties collaborate in training a machine learning (ML)
model. Logistic regression is considered for the VFL model. One party, referred
to as the active party, possesses the ground truth labels of the samples in the
training phase, while the other, referred to as the passive party, only shares
a separate set of features corresponding to these samples. It is shown that the
active party can carry out inference attacks on both training and prediction
phase samples by acquiring an ML model independently trained on the training
samples available to them. This type of inference attack does not require the
active party to be aware of the score of a specific sample, hence it is
referred to as an agnostic inference attack. It is shown that utilizing the
observed confidence scores during the prediction phase, before the time of the
attack, can improve the performance of the active party's autonomous model, and
thus improve the quality of the agnostic inference attack. As a countermeasure,
privacy-preserving schemes (PPSs) are proposed. While the proposed schemes
preserve the utility of the VFL model, they systematically distort the VFL
parameters corresponding to the passive party's features. The level of the
distortion imposed on the passive party's parameters is adjustable, giving rise
to a trade-off between privacy of the passive party and interpretabiliy of the
VFL outcomes by the active party. The distortion level of the passive party's
parameters could be chosen carefully according to the privacy and
interpretabiliy concerns of the passive and active parties, respectively, with
the hope of keeping both parties (partially) satisfied. Finally, experimental
results demonstrate the effectiveness of the proposed attack and the PPSs.
</p></li>
</ul>

<h3>Title: On Differential Privacy and Adaptive Data Analysis with Bounded Space. (arXiv:2302.05707v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05707">http://arxiv.org/abs/2302.05707</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05707] On Differential Privacy and Adaptive Data Analysis with Bounded Space](http://arxiv.org/abs/2302.05707) #privacy</code></li>
<li>Summary: <p>We study the space complexity of the two related fields of differential
privacy and adaptive data analysis. Specifically,
</p></li>
</ul>

<p>(1) Under standard cryptographic assumptions, we show that there exists a
problem P that requires exponentially more space to be solved efficiently with
differential privacy, compared to the space needed without privacy. To the best
of our knowledge, this is the first separation between the space complexity of
private and non-private algorithms.
</p>
<p>(2) The line of work on adaptive data analysis focuses on understanding the
number of samples needed for answering a sequence of adaptive queries. We
revisit previous lower bounds at a foundational level, and show that they are a
consequence of a space bottleneck rather than a sampling bottleneck.
</p>
<p>To obtain our results, we define and construct an encryption scheme with
multiple keys that is built to withstand a limited amount of key leakage in a
very particular way.
</p>

<h2>protect</h2>
<h3>Title: Removing Image Artifacts From Scratched Lens Protectors. (arXiv:2302.05746v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05746">http://arxiv.org/abs/2302.05746</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05746] Removing Image Artifacts From Scratched Lens Protectors](http://arxiv.org/abs/2302.05746) #protect</code></li>
<li>Summary: <p>A protector is placed in front of the camera lens for mobile devices to avoid
damage, while the protector itself can be easily scratched accidentally,
especially for plastic ones. The artifacts appear in a wide variety of
patterns, making it difficult to see through them clearly. Removing image
artifacts from the scratched lens protector is inherently challenging due to
the occasional flare artifacts and the co-occurring interference within mixed
artifacts. Though different methods have been proposed for some specific
distortions, they seldom consider such inherent challenges. In our work, we
consider the inherent challenges in a unified framework with two cooperative
modules, which facilitate the performance boost of each other. We also collect
a new dataset from the real world to facilitate training and evaluation
purposes. The experimental results demonstrate that our method outperforms the
baselines qualitatively and quantitatively. The code and datasets will be
released after acceptance.
</p></li>
</ul>

<h3>Title: A Brief Report on LawGPT 1.0: A Virtual Legal Assistant Based on GPT-3. (arXiv:2302.05729v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05729">http://arxiv.org/abs/2302.05729</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05729] A Brief Report on LawGPT 1](http://arxiv.org/abs/2302.05729) #protect</code></li>
<li>Summary: <p>LawGPT 1.0 is a virtual legal assistant built on the state-of-the-art
language model GPT-3, fine-tuned for the legal domain. The system is designed
to provide legal assistance to users in a conversational manner, helping them
with tasks such as answering legal questions, generating legal documents, and
providing legal advice. In this paper, we provide a brief overview of LawGPT
1.0, its architecture, and its performance on a set of legal benchmark tasks.
Please note that the detailed information about the model is protected by a
non-disclosure agreement (NDA) and cannot be disclosed in this report.
</p></li>
</ul>

<h3>Title: Multi-dimensional discrimination in Law and Machine Learning -- A comparative overview. (arXiv:2302.05995v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05995">http://arxiv.org/abs/2302.05995</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05995] Multi-dimensional discrimination in Law and Machine Learning -- A comparative overview](http://arxiv.org/abs/2302.05995) #protect</code></li>
<li>Summary: <p>AI-driven decision-making can lead to discrimination against certain
individuals or social groups based on protected characteristics/attributes such
as race, gender, or age. The domain of fairness-aware machine learning focuses
on methods and algorithms for understanding, mitigating, and accounting for
bias in AI/ML models. Still, thus far, the vast majority of the proposed
methods assess fairness based on a single protected attribute, e.g. only gender
or race. In reality, though, human identities are multi-dimensional, and
discrimination can occur based on more than one protected characteristic,
leading to the so-called <code>multi-dimensional discrimination'' or</code>multi-dimensional fairness'' problem. While well-elaborated in legal
literature, the multi-dimensionality of discrimination is less explored in the
machine learning community. Recent approaches in this direction mainly follow
the so-called intersectional fairness definition from the legal domain, whereas
other notions like additive and sequential discrimination are less studied or
not considered thus far. In this work, we overview the different definitions of
multi-dimensional discrimination/fairness in the legal domain as well as how
they have been transferred/ operationalized (if) in the fairness-aware machine
learning domain. By juxtaposing these two domains, we draw the connections,
identify the limitations, and point out open research directions.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: TextDefense: Adversarial Text Detection based on Word Importance Entropy. (arXiv:2302.05892v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05892">http://arxiv.org/abs/2302.05892</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05892] TextDefense: Adversarial Text Detection based on Word Importance Entropy](http://arxiv.org/abs/2302.05892) #defense</code></li>
<li>Summary: <p>Currently, natural language processing (NLP) models are wildly used in
various scenarios. However, NLP models, like all deep models, are vulnerable to
adversarially generated text. Numerous works have been working on mitigating
the vulnerability from adversarial attacks. Nevertheless, there is no
comprehensive defense in existing works where each work targets a specific
attack category or suffers from the limitation of computation overhead,
irresistible to adaptive attack, etc.
</p></li>
</ul>

<p>In this paper, we exhaustively investigate the adversarial attack algorithms
in NLP, and our empirical studies have discovered that the attack algorithms
mainly disrupt the importance distribution of words in a text. A well-trained
model can distinguish subtle importance distribution differences between clean
and adversarial texts. Based on this intuition, we propose TextDefense, a new
adversarial example detection framework that utilizes the target model's
capability to defend against adversarial attacks while requiring no prior
knowledge. TextDefense differs from previous approaches, where it utilizes the
target model for detection and thus is attack type agnostic. Our extensive
experiments show that TextDefense can be applied to different architectures,
datasets, and attack methods and outperforms existing methods. We also discover
that the leading factor influencing the performance of TextDefense is the
target model's generalizability. By analyzing the property of the target model
and the property of the adversarial example, we provide our insights into the
adversarial attacks in NLP and the principles of our defense method.
</p>

<h2>attack</h2>
<h3>Title: Investigating the Effect of Relative Positional Embeddings on AMR-to-Text Generation with Structural Adapters. (arXiv:2302.05900v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05900">http://arxiv.org/abs/2302.05900</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05900] Investigating the Effect of Relative Positional Embeddings on AMR-to-Text Generation with Structural Adapters](http://arxiv.org/abs/2302.05900) #attack</code></li>
<li>Summary: <p>Text generation from Abstract Meaning Representation (AMR) has substantially
benefited from the popularized Pretrained Language Models (PLMs). Myriad
approaches have linearized the input graph as a sequence of tokens to fit the
PLM tokenization requirements. Nevertheless, this transformation jeopardizes
the structural integrity of the graph and is therefore detrimental to its
resulting representation. To overcome this issue, Ribeiro et al. have recently
proposed StructAdapt, a structure-aware adapter which injects the input graph
connectivity within PLMs using Graph Neural Networks (GNNs). In this paper, we
investigate the influence of Relative Position Embeddings (RPE) on AMR-to-Text,
and, in parallel, we examine the robustness of StructAdapt. Through ablation
studies, graph attack and link prediction, we reveal that RPE might be
partially encoding input graphs. We suggest further research regarding the role
of RPE will provide valuable insights for Graph-to-Text generation.
</p></li>
</ul>

<h3>Title: High Recovery with Fewer Injections: Practical Binary Volumetric Injection Attacks against Dynamic Searchable Encryption. (arXiv:2302.05628v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05628">http://arxiv.org/abs/2302.05628</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05628] High Recovery with Fewer Injections: Practical Binary Volumetric Injection Attacks against Dynamic Searchable Encryption](http://arxiv.org/abs/2302.05628) #attack</code></li>
<li>Summary: <p>Searchable symmetric encryption enables private queries over an encrypted
database, but it also yields information leakages. Adversaries can exploit
these leakages to launch injection attacks (Zhang et al., USENIX'16) to recover
the underlying keywords from queries. The performance of the existing injection
attacks is strongly dependent on the amount of leaked information or injection.
In this work, we propose two new injection attacks, namely BVA and BVMA, by
leveraging a binary volumetric approach. We enable adversaries to inject fewer
files than the existing volumetric attacks by using the known keywords and
reveal the queries by observing the volume of the query results. Our attacks
can thwart well-studied defenses (e.g., threshold countermeasure, static
padding) without exploiting the distribution of target queries and client
databases. We evaluate the proposed attacks empirically in real-world datasets
with practical queries. The results show that our attacks can obtain a high
recovery rate (>80%) in the best case and a roughly 60% recovery even under a
large-scale dataset with a small number of injections (<20 files).
</p></li>
</ul>

<h3>Title: Mutation-Based Adversarial Attacks on Neural Text Detectors. (arXiv:2302.05794v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05794">http://arxiv.org/abs/2302.05794</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05794] Mutation-Based Adversarial Attacks on Neural Text Detectors](http://arxiv.org/abs/2302.05794) #attack</code></li>
<li>Summary: <p>Neural text detectors aim to decide the characteristics that distinguish
neural (machine-generated) from human texts. To challenge such detectors,
adversarial attacks can alter the statistical characteristics of the generated
text, making the detection task more and more difficult. Inspired by the
advances of mutation analysis in software development and testing, in this
paper, we propose character- and word-based mutation operators for generating
adversarial samples to attack state-of-the-art natural text detectors. This
falls under white-box adversarial attacks. In such attacks, attackers have
access to the original text and create mutation instances based on this
original text. The ultimate goal is to confuse machine learning models and
classifiers and decrease their prediction accuracy.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: RAFaRe: Learning Robust and Accurate Non-parametric 3D Face Reconstruction from Pseudo 2D&amp;3D Pairs. (arXiv:2302.05486v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05486">http://arxiv.org/abs/2302.05486</a></li>
<li>Code URL: <a href="https://github.com/zhuhao-nju/rafare">https://github.com/zhuhao-nju/rafare</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05486] RAFaRe: Learning Robust and Accurate Non-parametric 3D Face Reconstruction from Pseudo 2D&amp;3D Pairs](http://arxiv.org/abs/2302.05486) #robust</code></li>
<li>Summary: <p>We propose a robust and accurate non-parametric method for single-view 3D
face reconstruction (SVFR). While tremendous efforts have been devoted to
parametric SVFR, a visible gap still lies between the result 3D shape and the
ground truth. We believe there are two major obstacles: 1) the representation
of the parametric model is limited to a certain face database; 2) 2D images and
3D shapes in the fitted datasets are distinctly misaligned. To resolve these
issues, a large-scale pseudo 2D\&amp;3D dataset is created by first rendering the
detailed 3D faces, then swapping the face in the wild images with the rendered
face. These pseudo 2D&amp;3D pairs are created from publicly available datasets
which eliminate the gaps between 2D and 3D data while covering diverse
appearances, poses, scenes, and illumination. We further propose a
non-parametric scheme to learn a well-generalized SVFR model from the created
dataset, and the proposed hierarchical signed distance function turns out to be
effective in predicting middle-scale and small-scale 3D facial geometry. Our
model outperforms previous methods on FaceScape-wild/lab and MICC benchmarks
and is well generalized to various appearances, poses, expressions, and
in-the-wild environments. The code is released at
<a href="http://github.com/zhuhao-nju/rafare">this http URL</a> .
</p></li>
</ul>

<h3>Title: Element-Wise Attention Layers: an option for optimization. (arXiv:2302.05488v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05488">http://arxiv.org/abs/2302.05488</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05488] Element-Wise Attention Layers: an option for optimization](http://arxiv.org/abs/2302.05488) #robust</code></li>
<li>Summary: <p>The use of Attention Layers has become a trend since the popularization of
the Transformer-based models, being the key element for many state-of-the-art
models that have been developed through recent years. However, one of the
biggest obstacles in implementing these architectures - as well as many others
in Deep Learning Field - is the enormous amount of optimizing parameters they
possess, which make its use conditioned on the availability of robust hardware.
In this paper, it's proposed a new method of attention mechanism that adapts
the Dot-Product Attention, which uses matrices multiplications, to become
element-wise through the use of arrays multiplications. To test the
effectiveness of such approach, two models (one with a VGG-like architecture
and one with the proposed method) have been trained in a classification task
using Fashion MNIST and CIFAR10 datasets. Each model has been trained for 10
epochs in a single Tesla T4 GPU from Google Colaboratory. The results show that
this mechanism allows for an accuracy of 92% of the VGG-like counterpart in
Fashion MNIST dataset, while reducing the number of parameters in 97%. For
CIFAR10, the accuracy is still equivalent to 60% of the VGG-like counterpart
while using 50% less parameters.
</p></li>
</ul>

<h3>Title: Semi-supervised Large-scale Fiber Detection in Material Images with Synthetic Data. (arXiv:2302.05541v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05541">http://arxiv.org/abs/2302.05541</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05541] Semi-supervised Large-scale Fiber Detection in Material Images with Synthetic Data](http://arxiv.org/abs/2302.05541) #robust</code></li>
<li>Summary: <p>Accurate detection of large-scale, elliptical-shape fibers, including their
parameters of center, orientation and major/minor axes, on the 2D
cross-sectioned image slices is very important for characterizing the
underlying cylinder 3D structures in microscopic material images. Detecting
fibers in a degraded image poses a challenge to both current fiber detection
and ellipse detection methods. This paper proposes a new semi-supervised deep
learning method for large-scale elliptical fiber detection with synthetic data,
which frees people from heavy data annotations and is robust to various kinds
of image degradations. A domain adaptation strategy is utilized to reduce the
domain distribution discrepancy between the synthetic data and the real data,
and a new Region of Interest (RoI)-ellipse learning and a novel RoI ranking
with the symmetry constraint are embedded in the proposed method. Experiments
on real microscopic material images demonstrate the effectiveness of the
proposed approach in large-scale fiber detection.
</p></li>
</ul>

<h3>Title: Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis. (arXiv:2302.05608v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05608">http://arxiv.org/abs/2302.05608</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05608] Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis](http://arxiv.org/abs/2302.05608) #robust</code></li>
<li>Summary: <p>Often, deep network models are purely inductive during training and while
performing inference on unseen data. Thus, when such models are used for
predictions, it is well known that they often fail to capture the semantic
information and implicit dependencies that exist among objects (or concepts) on
a population level. Moreover, it is still unclear how domain or prior modal
knowledge can be specified in a backpropagation friendly manner, especially in
large-scale and noisy settings. In this work, we propose an end-to-end vision
and language model incorporating explicit knowledge graphs. We also introduce
an interactive out-of-distribution (OOD) layer using implicit network operator.
The layer is used to filter noise that is brought by external knowledge base.
In practice, we apply our model on several vision and language downstream tasks
including visual question answering, visual reasoning, and image-text retrieval
on different datasets. Our experiments show that it is possible to design
models that perform similarly to state-of-art results but with significantly
fewer samples and training time.
</p></li>
</ul>

<h3>Title: Operation-level Progressive Differentiable Architecture Search. (arXiv:2302.05632v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05632">http://arxiv.org/abs/2302.05632</a></li>
<li>Code URL: <a href="https://github.com/zxunyu/opp-darts">https://github.com/zxunyu/opp-darts</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05632] Operation-level Progressive Differentiable Architecture Search](http://arxiv.org/abs/2302.05632) #robust</code></li>
<li>Summary: <p>Differentiable Neural Architecture Search (DARTS) is becoming more and more
popular among Neural Architecture Search (NAS) methods because of its high
search efficiency and low compute cost. However, the stability of DARTS is very
inferior, especially skip connections aggregation that leads to performance
collapse. Though existing methods leverage Hessian eigenvalues to alleviate
skip connections aggregation, they make DARTS unable to explore architectures
with better performance. In the paper, we propose operation-level progressive
differentiable neural architecture search (OPP-DARTS) to avoid skip connections
aggregation and explore better architectures simultaneously. We first divide
the search process into several stages during the search phase and increase
candidate operations into the search space progressively at the beginning of
each stage. It can effectively alleviate the unfair competition between
operations during the search phase of DARTS by offsetting the inherent unfair
advantage of the skip connection over other operations. Besides, to keep the
competition between operations relatively fair and select the operation from
the candidate operations set that makes training loss of the supernet largest.
The experiment results indicate that our method is effective and efficient. Our
method's performance on CIFAR-10 is superior to the architecture found by
standard DARTS, and the transferability of our method also surpasses standard
DARTS. We further demonstrate the robustness of our method on three simple
search spaces, i.e., S2, S3, S4, and the results show us that our method is
more robust than standard DARTS. Our code is available at
https://github.com/zxunyu/OPP-DARTS.
</p></li>
</ul>

<h3>Title: Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing. (arXiv:2302.05744v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05744">http://arxiv.org/abs/2302.05744</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05744] Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing](http://arxiv.org/abs/2302.05744) #robust</code></li>
<li>Summary: <p>Recently, vision transformer (ViT) based multimodal learning methods have
been proposed to improve the robustness of face anti-spoofing (FAS) systems.
However, there are still no works to explore the fundamental natures
(\textit{e.g.}, modality-aware inputs, suitable multimodal pre-training, and
efficient finetuning) in vanilla ViT for multimodal FAS. In this paper, we
investigate three key factors (i.e., inputs, pre-training, and finetuning) in
ViT for multimodal FAS with RGB, Infrared (IR), and Depth. First, in terms of
the ViT inputs, we find that leveraging local feature descriptors benefits the
ViT on IR modality but not RGB or Depth modalities. Second, in observation of
the inefficiency on direct finetuning the whole or partial ViT, we design an
adaptive multimodal adapter (AMA), which can efficiently aggregate local
multimodal features while freezing majority of ViT parameters. Finally, in
consideration of the task (FAS vs. generic object classification) and modality
(multimodal vs. unimodal) gaps, ImageNet pre-trained models might be
sub-optimal for the multimodal FAS task. To bridge these gaps, we propose the
modality-asymmetric masked autoencoder (M$^{2}$A$^{2}$E) for multimodal FAS
self-supervised pre-training without costly annotated labels. Compared with the
previous modality-symmetric autoencoder, the proposed M$^{2}$A$^{2}$E is able
to learn more intrinsic task-aware representation and compatible with
modality-agnostic (e.g., unimodal, bimodal, and trimodal) downstream settings.
Extensive experiments with both unimodal (RGB, Depth, IR) and multimodal
(RGB+Depth, RGB+IR, Depth+IR, RGB+Depth+IR) settings conducted on multimodal
FAS benchmarks demonstrate the superior performance of the proposed methods. We
hope these findings and solutions can facilitate the future research for
ViT-based multimodal FAS.
</p></li>
</ul>

<h3>Title: DaliID: Distortion-Adaptive Learned Invariance for Identification Models. (arXiv:2302.05753v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05753">http://arxiv.org/abs/2302.05753</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05753] DaliID: Distortion-Adaptive Learned Invariance for Identification Models](http://arxiv.org/abs/2302.05753) #robust</code></li>
<li>Summary: <p>In unconstrained scenarios, face recognition and person re-identification are
subject to distortions such as motion blur, atmospheric turbulence, or
upsampling artifacts. To improve robustness in these scenarios, we propose a
methodology called Distortion-Adaptive Learned Invariance for Identification
(DaliID) models. We contend that distortion augmentations, which degrade image
quality, can be successfully leveraged to a greater degree than has been shown
in the literature. Aided by an adaptive weighting schedule, a novel distortion
augmentation is applied at severe levels during training. This training
strategy increases feature-level invariance to distortions and decreases domain
shift to unconstrained scenarios. At inference, we use a magnitude-weighted
fusion of features from parallel models to retain robustness across the range
of images. DaliID models achieve state-of-the-art (SOTA) for both face
recognition and person re-identification on seven benchmark datasets, including
IJB-S, TinyFace, DeepChange, and MSMT17. Additionally, we provide recaptured
evaluation data at a distance of 750+ meters and further validate on real
long-distance face imagery.
</p></li>
</ul>

<h3>Title: Stochastic Surprisal: An inferential measurement of Free Energy in Neural Networks. (arXiv:2302.05776v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05776">http://arxiv.org/abs/2302.05776</a></li>
<li>Code URL: <a href="https://github.com/olivesgatech/stochastic-surprisal">https://github.com/olivesgatech/stochastic-surprisal</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05776] Stochastic Surprisal: An inferential measurement of Free Energy in Neural Networks](http://arxiv.org/abs/2302.05776) #robust</code></li>
<li>Summary: <p>This paper conjectures and validates a framework that allows for action
during inference in supervised neural networks. Supervised neural networks are
constructed with the objective to maximize their performance metric in any
given task. This is done by reducing free energy and its associated surprisal
during training. However, the bottom-up inference nature of supervised networks
is a passive process that renders them fallible to noise. In this paper, we
provide a thorough background of supervised neural networks, both generative
and discriminative, and discuss their functionality from the perspective of
free energy principle. We then provide a framework for introducing action
during inference. We introduce a new measurement called stochastic surprisal
that is a function of the network, the input, and any possible action. This
action can be any one of the outputs that the neural network has learnt,
thereby lending stochasticity to the measurement. Stochastic surprisal is
validated on two applications: Image Quality Assessment and Recognition under
noisy conditions. We show that, while noise characteristics are ignored to make
robust recognition, they are analyzed to estimate image quality scores. We
apply stochastic surprisal on two applications, three datasets, and as a
plug-in on twelve networks. In all, it provides a statistically significant
increase among all measures. We conclude by discussing the implications of the
proposed stochastic surprisal in other areas of cognitive psychology including
expectancy-mismatch and abductive reasoning.
</p></li>
</ul>

<h3>Title: OAMatcher: An Overlapping Areas-based Network for Accurate Local Feature Matching. (arXiv:2302.05846v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05846">http://arxiv.org/abs/2302.05846</a></li>
<li>Code URL: <a href="https://github.com/dk-hu/oamatcher">https://github.com/dk-hu/oamatcher</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05846] OAMatcher: An Overlapping Areas-based Network for Accurate Local Feature Matching](http://arxiv.org/abs/2302.05846) #robust</code></li>
<li>Summary: <p>Local feature matching is an essential component in many visual applications.
In this work, we propose OAMatcher, a Tranformer-based detector-free method
that imitates humans behavior to generate dense and accurate matches. Firstly,
OAMatcher predicts overlapping areas to promote effective and clean global
context aggregation, with the key insight that humans focus on the overlapping
areas instead of the entire images after multiple observations when matching
keypoints in image pairs. Technically, we first perform global information
integration across all keypoints to imitate the humans behavior of observing
the entire images at the beginning of feature matching. Then, we propose
Overlapping Areas Prediction Module (OAPM) to capture the keypoints in
co-visible regions and conduct feature enhancement among them to simulate that
humans transit the focus regions from the entire images to overlapping regions,
hence realizeing effective information exchange without the interference coming
from the keypoints in non overlapping areas. Besides, since humans tend to
leverage probability to determine whether the match labels are correct or not,
we propose a Match Labels Weight Strategy (MLWS) to generate the coefficients
used to appraise the reliability of the ground-truth match labels, while
alleviating the influence of measurement noise coming from the data. Moreover,
we integrate depth-wise convolution into Tranformer encoder layers to ensure
OAMatcher extracts local and global feature representation concurrently.
Comprehensive experiments demonstrate that OAMatcher outperforms the
state-of-the-art methods on several benchmarks, while exhibiting excellent
robustness to extreme appearance variants. The source code is available at
https://github.com/DK-HU/OAMatcher.
</p></li>
</ul>

<h3>Title: Policy-Induced Self-Supervision Improves Representation Finetuning in Visual RL. (arXiv:2302.06009v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.06009">http://arxiv.org/abs/2302.06009</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.06009] Policy-Induced Self-Supervision Improves Representation Finetuning in Visual RL](http://arxiv.org/abs/2302.06009) #robust</code></li>
<li>Summary: <p>We study how to transfer representations pretrained on source tasks to target
tasks in visual percept based RL. We analyze two popular approaches: freezing
or finetuning the pretrained representations. Empirical studies on a set of
popular tasks reveal several properties of pretrained representations. First,
finetuning is required even when pretrained representations perfectly capture
the information required to solve the target task. Second, finetuned
representations improve learnability and are more robust to noise. Third,
pretrained bottom layers are task-agnostic and readily transferable to new
tasks, while top layers encode task-specific information and require
adaptation. Building on these insights, we propose a self-supervised objective
that clusters representations according to the policy they induce, as opposed
to traditional representation similarity measures which are policy-agnostic
(e.g. Euclidean norm, cosine similarity). Together with freezing the bottom
layers, this objective results in significantly better representation than
frozen, finetuned, and self-supervised alternatives on a wide range of
benchmarks.
</p></li>
</ul>

<h3>Title: Evaluating the Robustness of Discrete Prompts. (arXiv:2302.05619v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05619">http://arxiv.org/abs/2302.05619</a></li>
<li>Code URL: <a href="https://github.com/livnlp/prompt-robustness">https://github.com/livnlp/prompt-robustness</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05619] Evaluating the Robustness of Discrete Prompts](http://arxiv.org/abs/2302.05619) #robust</code></li>
<li>Summary: <p>Discrete prompts have been used for fine-tuning Pre-trained Language Models
for diverse NLP tasks. In particular, automatic methods that generate discrete
prompts from a small set of training instances have reported superior
performance. However, a closer look at the learnt prompts reveals that they
contain noisy and counter-intuitive lexical constructs that would not be
encountered in manually-written prompts. This raises an important yet
understudied question regarding the robustness of automatically learnt discrete
prompts when used in downstream tasks. To address this question, we conduct a
systematic study of the robustness of discrete prompts by applying carefully
designed perturbations into an application using AutoPrompt and then measure
their performance in two Natural Language Inference (NLI) datasets. Our
experimental results show that although the discrete prompt-based method
remains relatively robust against perturbations to NLI inputs, they are highly
sensitive to other types of perturbations such as shuffling and deletion of
prompt tokens. Moreover, they generalize poorly across different NLI datasets.
We hope our findings will inspire future work on robust discrete prompt
learning.
</p></li>
</ul>

<h3>Title: HateProof: Are Hateful Meme Detection Systems really Robust?. (arXiv:2302.05703v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05703">http://arxiv.org/abs/2302.05703</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05703] HateProof: Are Hateful Meme Detection Systems really Robust?](http://arxiv.org/abs/2302.05703) #robust</code></li>
<li>Summary: <p>Exploiting social media to spread hate has tremendously increased over the
years. Lately, multi-modal hateful content such as memes has drawn relatively
more traction than uni-modal content. Moreover, the availability of implicit
content payloads makes them fairly challenging to be detected by existing
hateful meme detection systems. In this paper, we present a use case study to
analyze such systems' vulnerabilities against external adversarial attacks. We
find that even very simple perturbations in uni-modal and multi-modal settings
performed by humans with little knowledge about the model can make the existing
detection models highly vulnerable. Empirically, we find a noticeable
performance drop of as high as 10% in the macro-F1 score for certain attacks.
As a remedy, we attempt to boost the model's robustness using contrastive
learning as well as an adversarial training-based method - VILLA. Using an
ensemble of the above two approaches, in two of our high resolution datasets,
we are able to (re)gain back the performance to a large extent for certain
attacks. We believe that ours is a first step toward addressing this crucial
problem in an adversarial setting and would inspire more such investigations in
the future.
</p></li>
</ul>

<h3>Title: MTTM: Metamorphic Testing for Textual Content Moderation Software. (arXiv:2302.05706v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05706">http://arxiv.org/abs/2302.05706</a></li>
<li>Code URL: <a href="https://github.com/jarviswang94/mttm">https://github.com/jarviswang94/mttm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05706] MTTM: Metamorphic Testing for Textual Content Moderation Software](http://arxiv.org/abs/2302.05706) #robust</code></li>
<li>Summary: <p>The exponential growth of social media platforms such as Twitter and Facebook
has revolutionized textual communication and textual content publication in
human society. However, they have been increasingly exploited to propagate
toxic content, such as hate speech, malicious advertisement, and pornography,
which can lead to highly negative impacts (e.g., harmful effects on teen mental
health). Researchers and practitioners have been enthusiastically developing
and extensively deploying textual content moderation software to address this
problem. However, we find that malicious users can evade moderation by changing
only a few words in the toxic content. Moreover, modern content moderation
software performance against malicious inputs remains underexplored. To this
end, we propose MTTM, a Metamorphic Testing framework for Textual content
Moderation software. Specifically, we conduct a pilot study on 2,000 text
messages collected from real users and summarize eleven metamorphic relations
across three perturbation levels: character, word, and sentence. MTTM employs
these metamorphic relations on toxic textual contents to generate test cases,
which are still toxic yet likely to evade moderation. In our evaluation, we
employ MTTM to test three commercial textual content moderation software and
two state-of-the-art moderation algorithms against three kinds of toxic
content. The results show that MTTM achieves up to 83.9%, 51%, and 82.5% error
finding rates (EFR) when testing commercial moderation software provided by
Google, Baidu, and Huawei, respectively, and it obtains up to 91.2% EFR when
testing the state-of-the-art algorithms from the academy. In addition, we
leverage the test cases generated by MTTM to retrain the model we explored,
which largely improves model robustness (0% to 5.9% EFR) while maintaining the
accuracy on the original test set.
</p></li>
</ul>

<h3>Title: Analyzing the Effectiveness of the Underlying Reasoning Tasks in Multi-hop Question Answering. (arXiv:2302.05963v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05963">http://arxiv.org/abs/2302.05963</a></li>
<li>Code URL: <a href="https://github.com/alab-nii/multi-hop-analysis">https://github.com/alab-nii/multi-hop-analysis</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05963] Analyzing the Effectiveness of the Underlying Reasoning Tasks in Multi-hop Question Answering](http://arxiv.org/abs/2302.05963) #robust</code></li>
<li>Summary: <p>To explain the predicted answers and evaluate the reasoning abilities of
models, several studies have utilized underlying reasoning (UR) tasks in
multi-hop question answering (QA) datasets. However, it remains an open
question as to how effective UR tasks are for the QA task when training models
on both tasks in an end-to-end manner. In this study, we address this question
by analyzing the effectiveness of UR tasks (including both sentence-level and
entity-level tasks) in three aspects: (1) QA performance, (2) reasoning
shortcuts, and (3) robustness. While the previous models have not been
explicitly trained on an entity-level reasoning prediction task, we build a
multi-task model that performs three tasks together: sentence-level supporting
facts prediction, entity-level reasoning prediction, and answer prediction.
Experimental results on 2WikiMultiHopQA and HotpotQA-small datasets reveal that
(1) UR tasks can improve QA performance. Using four debiased datasets that are
newly created, we demonstrate that (2) UR tasks are helpful in preventing
reasoning shortcuts in the multi-hop QA task. However, we find that (3) UR
tasks do not contribute to improving the robustness of the model on adversarial
questions, such as sub-questions and inverted questions. We encourage future
studies to investigate the effectiveness of entity-level reasoning in the form
of natural language questions (e.g., sub-question forms).
</p></li>
</ul>

<h3>Title: Decoupling the Skeleton Parsing and Schema Linking for Text-to-SQL. (arXiv:2302.05965v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05965">http://arxiv.org/abs/2302.05965</a></li>
<li>Code URL: <a href="https://github.com/ruckbreasoning/resdsql">https://github.com/ruckbreasoning/resdsql</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05965] Decoupling the Skeleton Parsing and Schema Linking for Text-to-SQL](http://arxiv.org/abs/2302.05965) #robust</code></li>
<li>Summary: <p>One of the recent best attempts at Text-to-SQL is the pre-trained language
model. Due to the structural property of the SQL queries, the seq2seq model
takes the responsibility of parsing both the schema items (i.e., tables and
columns) and the skeleton (i.e., SQL keywords). Such coupled targets increase
the difficulty of parsing the correct SQL queries especially when they involve
many schema items and logic operators. This paper proposes a ranking-enhanced
encoding and skeleton-aware decoding framework to decouple the schema linking
and the skeleton parsing. Specifically, for a seq2seq encoder-decode model, its
encoder is injected by the most relevant schema items instead of the whole
unordered ones, which could alleviate the schema linking effort during SQL
parsing, and its decoder first generates the skeleton and then the actual SQL
query, which could implicitly constrain the SQL parsing. We evaluate our
proposed framework on Spider and its three robustness variants: Spider-DK,
Spider-Syn, and Spider-Realistic. The experimental results show that our
framework delivers promising performance and robustness. Our code is available
at https://github.com/RUCKBReasoning/RESDSQL.
</p></li>
</ul>

<h3>Title: Robust Knowledge Transfer in Tiered Reinforcement Learning. (arXiv:2302.05534v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05534">http://arxiv.org/abs/2302.05534</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05534] Robust Knowledge Transfer in Tiered Reinforcement Learning](http://arxiv.org/abs/2302.05534) #robust</code></li>
<li>Summary: <p>In this paper, we study the Tiered Reinforcement Learning setting, a parallel
transfer learning framework, where the goal is to transfer knowledge from the
low-tier (source) task to the high-tier (target) task to reduce the exploration
risk of the latter while solving the two tasks in parallel. Unlike previous
work, we do not assume the low-tier and high-tier tasks share the same dynamics
or reward functions, and focus on robust knowledge transfer without prior
knowledge on the task similarity. We identify a natural and necessary condition
called the "Optimal Value Dominance" for our objective. Under this condition,
we propose novel online learning algorithms such that, for the high-tier task,
it can achieve constant regret on partial states depending on the task
similarity and retain near-optimal regret when the two tasks are dissimilar,
while for the low-tier task, it can keep near-optimal without making sacrifice.
Moreover, we further study the setting with multiple low-tier tasks, and
propose a novel transfer source selection mechanism, which can ensemble the
information from all low-tier tasks and allow provable benefits on a much
larger state-action space.
</p></li>
</ul>

<h3>Title: Pruning Deep Neural Networks from a Sparsity Perspective. (arXiv:2302.05601v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05601">http://arxiv.org/abs/2302.05601</a></li>
<li>Code URL: <a href="https://github.com/dem123456789/Pruning-Deep-Neural-Networks-from-a-Sparsity-Perspective">https://github.com/dem123456789/Pruning-Deep-Neural-Networks-from-a-Sparsity-Perspective</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05601] Pruning Deep Neural Networks from a Sparsity Perspective](http://arxiv.org/abs/2302.05601) #robust</code></li>
<li>Summary: <p>In recent years, deep network pruning has attracted significant attention in
order to enable the rapid deployment of AI into small devices with computation
and memory constraints. Pruning is often achieved by dropping redundant
weights, neurons, or layers of a deep network while attempting to retain a
comparable test performance. Many deep pruning algorithms have been proposed
with impressive empirical success. However, existing approaches lack a
quantifiable measure to estimate the compressibility of a sub-network during
each pruning iteration and thus may under-prune or over-prune the model. In
this work, we propose PQ Index (PQI) to measure the potential compressibility
of deep neural networks and use this to develop a Sparsity-informed Adaptive
Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis
that for a generic pruning procedure, PQI decreases first when a large model is
being effectively regularized and then increases when its compressibility
reaches a limit that appears to correspond to the beginning of underfitting.
Subsequently, PQI decreases again when the model collapse and significant
deterioration in the performance of the model start to occur. Additionally, our
experiments demonstrate that the proposed adaptive pruning algorithm with
proper choice of hyper-parameters is superior to the iterative pruning
algorithms such as the lottery ticket-based pruning methods, in terms of both
compression efficiency and robustness.
</p></li>
</ul>

<h3>Title: Regret Guarantees for Adversarial Online Collaborative Filtering. (arXiv:2302.05765v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05765">http://arxiv.org/abs/2302.05765</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05765] Regret Guarantees for Adversarial Online Collaborative Filtering](http://arxiv.org/abs/2302.05765) #robust</code></li>
<li>Summary: <p>We investigate the problem of online collaborative filtering under
no-repetition constraints, whereby users need to be served content in an online
fashion and a given user cannot be recommended the same content item more than
once. We design and analyze a fully adaptive algorithm that works under
biclustering assumptions on the user-item preference matrix, and show that this
algorithm exhibits an optimal regret guarantee, while being oblivious to any
prior knowledge about the sequence of users, the universe of items, as well as
the biclustering parameters of the preference matrix. We further propose a more
robust version of the algorithm which addresses the scenario when the
preference matrix is adversarially perturbed. We then give regret guarantees
that scale with the amount by which the preference matrix is perturbed from a
biclustered structure. To our knowledge, these are the first results on online
collaborative filtering that hold at this level of generality and adaptivity
under no-repetition constraints.
</p></li>
</ul>

<h3>Title: Pushing the Accuracy-Group Robustness Frontier with Introspective Self-play. (arXiv:2302.05807v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05807">http://arxiv.org/abs/2302.05807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05807] Pushing the Accuracy-Group Robustness Frontier with Introspective Self-play](http://arxiv.org/abs/2302.05807) #robust</code></li>
<li>Summary: <p>Standard empirical risk minimization (ERM) training can produce deep neural
network (DNN) models that are accurate on average but under-perform in
under-represented population subgroups, especially when there are imbalanced
group distributions in the long-tailed training data. Therefore, approaches
that improve the accuracy-group robustness trade-off frontier of a DNN model
(i.e. improving worst-group accuracy without sacrificing average accuracy, or
vice versa) is of crucial importance. Uncertainty-based active learning (AL)
can potentially improve the frontier by preferentially sampling
underrepresented subgroups to create a more balanced training dataset. However,
the quality of uncertainty estimates from modern DNNs tend to degrade in the
presence of spurious correlations and dataset bias, compromising the
effectiveness of AL for sampling tail groups. In this work, we propose
Introspective Self-play (ISP), a simple approach to improve the uncertainty
estimation of a deep neural network under dataset bias, by adding an auxiliary
introspection task requiring a model to predict the bias for each data point in
addition to the label. We show that ISP provably improves the bias-awareness of
the model representation and the resulting uncertainty estimates. On two
real-world tabular and language tasks, ISP serves as a simple "plug-in" for AL
model training, consistently improving both the tail-group sampling rate and
the final accuracy-fairness trade-off frontier of popular AL methods.
</p></li>
</ul>

<h3>Title: Interpretable Diversity Analysis: Visualizing Feature Representations In Low-Cost Ensembles. (arXiv:2302.05822v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05822">http://arxiv.org/abs/2302.05822</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05822] Interpretable Diversity Analysis: Visualizing Feature Representations In Low-Cost Ensembles](http://arxiv.org/abs/2302.05822) #robust</code></li>
<li>Summary: <p>Diversity is an important consideration in the construction of robust neural
network ensembles. A collection of well trained models will generalize better
if they are diverse in the patterns they respond to and the predictions they
make. Diversity is especially important for low-cost ensemble methods because
members often share network structure in order to avoid training several
independent models from scratch. Diversity is traditionally analyzed by
measuring differences between the outputs of models. However, this gives little
insight into how knowledge representations differ between ensemble members.
This paper introduces several interpretability methods that can be used to
qualitatively analyze diversity. We demonstrate these techniques by comparing
the diversity of feature representations between child networks using two
low-cost ensemble algorithms, Snapshot Ensembles and Prune and Tune Ensembles.
We use the same pre-trained parent network as a starting point for both methods
which allows us to explore how feature representations evolve over time. This
approach to diversity analysis can lead to valuable insights and new
perspectives for how we measure and promote diversity in ensemble methods.
</p></li>
</ul>

<h3>Title: Data efficiency and extrapolation trends in neural network interatomic potentials. (arXiv:2302.05823v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05823">http://arxiv.org/abs/2302.05823</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05823] Data efficiency and extrapolation trends in neural network interatomic potentials](http://arxiv.org/abs/2302.05823) #robust</code></li>
<li>Summary: <p>Over the last few years, key architectural advances have been proposed for
neural network interatomic potentials (NNIPs), such as incorporating
message-passing networks, equivariance, or many-body expansion terms. Although
modern NNIP models exhibit nearly negligible differences in energy/forces
errors, improvements in accuracy are still considered the main target when
developing new NNIP architectures. In this work, we investigate how
architectural choices influence the trainability and generalization error in
NNIPs, revealing trends in extrapolation, data efficiency, and loss landscapes.
First, we show that modern NNIP architectures recover the underlying potential
energy surface (PES) of the training data even when trained to corrupted
labels. Second, generalization metrics such as errors on high-temperature
samples from the 3BPA dataset are demonstrated to follow a scaling relation for
a variety of models. Thus, improvements in accuracy metrics may not bring
independent information on the robust generalization of NNIPs. To circumvent
this problem, we relate loss landscapes to model generalization across
datasets. Using this probe, we explain why NNIPs with similar accuracy metrics
exhibit different abilities to extrapolate and how training to forces improves
the optimization landscape of a model. As an example, we show that MACE can
predict PESes with reasonable error after being trained to as few as five data
points, making it an example of a "few-shot" model for learning PESes. On the
other hand, models with similar accuracy metrics such as NequIP show smaller
ability to extrapolate in this extremely low-data regime. Our work provides a
deep learning justification for the performance of many common NNIPs, and
introduces tools beyond accuracy metrics that can be used to inform the
development of next-generation models.
</p></li>
</ul>

<h3>Title: Flag Aggregator: Scalable Distributed Training under Failures and Augmented Losses using Convex Optimization. (arXiv:2302.05865v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05865">http://arxiv.org/abs/2302.05865</a></li>
<li>Code URL: <a href="https://github.com/hamidralmasi/flagaggregator">https://github.com/hamidralmasi/flagaggregator</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05865] Flag Aggregator: Scalable Distributed Training under Failures and Augmented Losses using Convex Optimization](http://arxiv.org/abs/2302.05865) #robust</code></li>
<li>Summary: <p>Modern ML applications increasingly rely on complex deep learning models and
large datasets. There has been an exponential growth in the amount of
computation needed to train the largest models. Therefore, to scale computation
and data, these models are inevitably trained in a distributed manner in
clusters of nodes, and their updates are aggregated before being applied to the
model. However, a distributed setup is prone to byzantine failures of
individual nodes, components, and software. With data augmentation added to
these settings, there is a critical need for robust and efficient aggregation
systems. We extend the current state-of-the-art aggregators and propose an
optimization-based subspace estimator by modeling pairwise distances as
quadratic functions by utilizing the recently introduced Flag Median problem.
The estimator in our loss function favors the pairs that preserve the norm of
the difference vector. We theoretically show that our approach enhances the
robustness of state-of-the-art byzantine resilient aggregators. Also, we
evaluate our method with different tasks in a distributed setup with a
parameter server architecture and show its communication efficiency while
maintaining similar accuracy. The code is publicly available at
https://github.com/hamidralmasi/FlagAggregator
</p></li>
</ul>

<h3>Title: USER: Unsupervised Structural Entropy-based Robust Graph Neural Network. (arXiv:2302.05889v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05889">http://arxiv.org/abs/2302.05889</a></li>
<li>Code URL: <a href="https://github.com/wangyifeibeijing/user">https://github.com/wangyifeibeijing/user</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05889] USER: Unsupervised Structural Entropy-based Robust Graph Neural Network](http://arxiv.org/abs/2302.05889) #robust</code></li>
<li>Summary: <p>Unsupervised/self-supervised graph neural networks (GNN) are vulnerable to
inherent randomness in the input graph data which greatly affects the
performance of the model in downstream tasks. In this paper, we alleviate the
interference of graph randomness and learn appropriate representations of nodes
without label information. To this end, we propose USER, an unsupervised robust
version of graph neural networks that is based on structural entropy. We
analyze the property of intrinsic connectivity and define intrinsic
connectivity graph. We also identify the rank of the adjacency matrix as a
crucial factor in revealing a graph that provides the same embeddings as the
intrinsic connectivity graph. We then introduce structural entropy in the
objective function to capture such a graph. Extensive experiments conducted on
clustering and link prediction tasks under random-noises and meta-attack over
three datasets show USER outperforms benchmarks and is robust to heavier
randomness.
</p></li>
</ul>

<h3>Title: Autoselection of the Ensemble of Convolutional Neural Networks with Second-Order Cone Programming. (arXiv:2302.05950v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05950">http://arxiv.org/abs/2302.05950</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05950] Autoselection of the Ensemble of Convolutional Neural Networks with Second-Order Cone Programming](http://arxiv.org/abs/2302.05950) #robust</code></li>
<li>Summary: <p>Ensemble techniques are frequently encountered in machine learning and
engineering problems since the method combines different models and produces an
optimal predictive solution. The ensemble concept can be adapted to deep
learning models to provide robustness and reliability. Due to the growth of the
models in deep learning, using ensemble pruning is highly important to deal
with computational complexity. Hence, this study proposes a mathematical model
which prunes the ensemble of Convolutional Neural Networks (CNN) consisting of
different depths and layers that maximizes accuracy and diversity
simultaneously with a sparse second order conic optimization model. The
proposed model is tested on CIFAR-10, CIFAR-100 and MNIST data sets which gives
promising results while reducing the complexity of models, significantly.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: TPE-Net: Track Point Extraction and Association Network for Rail Path Proposal Generation. (arXiv:2302.05803v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05803">http://arxiv.org/abs/2302.05803</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05803] TPE-Net: Track Point Extraction and Association Network for Rail Path Proposal Generation](http://arxiv.org/abs/2302.05803) #extraction</code></li>
<li>Summary: <p>One essential feature of an autonomous train is minimizing collision risks
with third-party objects. To estimate the risk, the control system must
identify topological information of all the rail routes ahead on which the
train can possibly move, especially within merging or diverging rails. This
way, the train can figure out the status of potential obstacles with respect to
its route and hence, make a timely decision. Numerous studies have successfully
extracted all rail tracks as a whole within forward-looking images without
considering element instances. Still, some image-based methods have employed
hard-coded prior knowledge of railway geometry on 3D data to associate
left-right rails and generate rail route instances. However, we propose a rail
path extraction pipeline in which left-right rail pixels of each rail route
instance are extracted and associated through a fully convolutional
encoder-decoder architecture called TPE-Net. Two different regression branches
for TPE-Net are proposed to regress the locations of center points of each rail
route, along with their corresponding left-right pixels. Extracted rail pixels
are then spatially clustered to generate topological information of all the
possible train routes (ego-paths), discarding non-ego-path ones. Experimental
results on a challenging, publicly released benchmark show true-positive-pixel
level average precision and recall of 0.9207 and 0.8721, respectively, at about
12 frames per second. Even though our evaluation results are not higher than
the SOTA, the proposed regression pipeline performs remarkably in extracting
the correspondences by looking once at the image. It generates strong rail
route hypotheses without reliance on camera parameters, 3D data, and
geometrical constraints.
</p></li>
</ul>

<h3>Title: MatKB: Semantic Search for Polycrystalline Materials Synthesis Procedures. (arXiv:2302.05597v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05597">http://arxiv.org/abs/2302.05597</a></li>
<li>Code URL: <a href="https://github.com/xianjun-yang/pcmsp">https://github.com/xianjun-yang/pcmsp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05597] MatKB: Semantic Search for Polycrystalline Materials Synthesis Procedures](http://arxiv.org/abs/2302.05597) #extraction</code></li>
<li>Summary: <p>In this paper, we present a novel approach to knowledge extraction and
retrieval using Natural Language Processing (NLP) techniques for material
science. Our goal is to automatically mine structured knowledge from millions
of research articles in the field of polycrystalline materials and make it
easily accessible to the broader community. The proposed method leverages NLP
techniques such as entity recognition and document classification to extract
relevant information and build an extensive knowledge base, from a collection
of 9.5 Million publications. The resulting knowledge base is integrated into a
search engine, which enables users to search for information about specific
materials, properties, and experiments with greater precision than traditional
search engines like Google. We hope our results can enable material scientists
quickly locate desired experimental procedures, compare their differences, and
even inspire them to design new experiments. Our website will be available at
Github \footnote{https://github.com/Xianjun-Yang/PcMSP.git} soon.
</p></li>
</ul>

<h3>Title: DocILE Benchmark for Document Information Localization and Extraction. (arXiv:2302.05658v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05658">http://arxiv.org/abs/2302.05658</a></li>
<li>Code URL: <a href="https://github.com/rossumai/docile">https://github.com/rossumai/docile</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05658] DocILE Benchmark for Document Information Localization and Extraction](http://arxiv.org/abs/2302.05658) #extraction</code></li>
<li>Summary: <p>This paper introduces the DocILE benchmark with the largest dataset of
business documents for the tasks of Key Information Localization and Extraction
and Line Item Recognition. It contains 6.7k annotated business documents, 100k
synthetically generated documents, and nearly~1M unlabeled documents for
unsupervised pre-training. The dataset has been built with knowledge of domain-
and task-specific aspects, resulting in the following key features: (i)
annotations in 55 classes, which surpasses the granularity of previously
published key information extraction datasets by a large margin; (ii) Line Item
Recognition represents a highly practical information extraction task, where
key information has to be assigned to items in a table; (iii) documents come
from numerous layouts and the test set includes zero- and few-shot cases as
well as layouts commonly seen in the training set. The benchmark comes with
several baselines, including RoBERTa, LayoutLMv3 and DETR-based Table
Transformer. These baseline models were applied to both tasks of the DocILE
benchmark, with results shared in this paper, offering a quick starting point
for future work. The dataset and baselines are available at
https://github.com/rossumai/docile.
</p></li>
</ul>

<h3>Title: Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language Models in Dialogues. (arXiv:2302.05895v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05895">http://arxiv.org/abs/2302.05895</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05895] Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language Models in Dialogues](http://arxiv.org/abs/2302.05895) #extraction</code></li>
<li>Summary: <p>Discourse processing suffers from data sparsity, especially for dialogues. As
a result, we explore approaches to build discourse structures for dialogues,
based on attention matrices from Pre-trained Language Models (PLMs). We
investigate multiple tasks for fine-tuning and show that the dialogue-tailored
Sentence Ordering task performs best. To locate and exploit discourse
information in PLMs, we propose an unsupervised and a semi-supervised method.
Our proposals achieve encouraging results on the STAC corpus, with F1 scores of
57.2 and 59.3 for unsupervised and semi-supervised methods, respectively. When
restricted to projective trees, our scores improved to 63.3 and 68.1.
</p></li>
</ul>

<h3>Title: Predicting municipalities in financial distress: a machine learning approach enhanced by domain expertise. (arXiv:2302.05780v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05780">http://arxiv.org/abs/2302.05780</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05780] Predicting municipalities in financial distress: a machine learning approach enhanced by domain expertise](http://arxiv.org/abs/2302.05780) #extraction</code></li>
<li>Summary: <p>Financial distress of municipalities, although comparable to bankruptcy of
private companies, has a far more serious impact on the well-being of
communities. For this reason, it is essential to detect deficits as soon as
possible. Predicting financial distress in municipalities can be a complex
task, as it involves understanding a wide range of factors that can affect a
municipality's financial health. In this paper, we evaluate machine learning
models to predict financial distress in Italian municipalities. Accounting
judiciary experts have specialized knowledge and experience in evaluating the
financial performance of municipalities, and they use a range of financial and
general indicators to make their assessments. By incorporating these indicators
in the feature extraction process, we can ensure that the predictive model is
taking into account a wide range of information that is relevant to the
financial health of municipalities. The results of this study indicate that
using machine learning models in combination with the knowledge of accounting
judiciary experts can aid in the early detection of financial distress in
municipalities, leading to better outcomes for the communities they serve.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Vertical Federated Knowledge Transfer via Representation Distillation for Healthcare Collaboration Networks. (arXiv:2302.05675v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05675">http://arxiv.org/abs/2302.05675</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05675] Vertical Federated Knowledge Transfer via Representation Distillation for Healthcare Collaboration Networks](http://arxiv.org/abs/2302.05675) #federate</code></li>
<li>Summary: <p>Collaboration between healthcare institutions can significantly lessen the
imbalance in medical resources across various geographic areas. However,
directly sharing diagnostic information between institutions is typically not
permitted due to the protection of patients' highly sensitive privacy. As a
novel privacy-preserving machine learning paradigm, federated learning (FL)
makes it possible to maximize the data utility among multiple medical
institutions. These feature-enrichment FL techniques are referred to as
vertical FL (VFL). Traditional VFL can only benefit multi-parties' shared
samples, which strongly restricts its application scope. In order to improve
the information-sharing capability and innovation of various healthcare-related
institutions, and then to establish a next-generation open medical
collaboration network, we propose a unified framework for vertical federated
knowledge transfer mechanism (VFedTrans) based on a novel cross-hospital
representation distillation component. Specifically, our framework includes
three steps. First, shared samples' federated representations are extracted by
collaboratively modeling multi-parties' joint features with current efficient
vertical federated representation learning methods. Second, for each hospital,
we learn a local-representation-distilled module, which can transfer the
knowledge from shared samples' federated representations to enrich local
samples' representations. Finally, each hospital can leverage local samples'
representations enriched by the distillation module to boost arbitrary
downstream machine learning tasks. The experiments on real-life medical
datasets verify the knowledge transfer effectiveness of our framework.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Fairness-aware Multi-view Clustering. (arXiv:2302.05788v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05788">http://arxiv.org/abs/2302.05788</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05788] Fairness-aware Multi-view Clustering](http://arxiv.org/abs/2302.05788) #fair</code></li>
<li>Summary: <p>In the era of big data, we are often facing the challenge of data
heterogeneity and the lack of label information simultaneously. In the
financial domain (e.g., fraud detection), the heterogeneous data may include
not only numerical data (e.g., total debt and yearly income), but also text and
images (e.g., financial statement and invoice images). At the same time, the
label information (e.g., fraud transactions) may be missing for building
predictive models. To address these challenges, many state-of-the-art
multi-view clustering methods have been proposed and achieved outstanding
performance. However, these methods typically do not take into consideration
the fairness aspect and are likely to generate biased results using sensitive
information such as race and gender. Therefore, in this paper, we propose a
fairness-aware multi-view clustering method named FairMVC. It incorporates the
group fairness constraint into the soft membership assignment for each cluster
to ensure that the fraction of different groups in each cluster is
approximately identical to the entire data set. Meanwhile, we adopt the idea of
both contrastive learning and non-contrastive learning and propose novel
regularizers to handle heterogeneous data in complex scenarios with missing
data or noisy features. Experimental results on real-world data sets
demonstrate the effectiveness and efficiency of the proposed framework. We also
derive insights regarding the relative performance of the proposed regularizers
in various scenarios.
</p></li>
</ul>

<h3>Title: FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation in Large Language Models. (arXiv:2302.05508v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05508">http://arxiv.org/abs/2302.05508</a></li>
<li>Code URL: <a href="https://github.com/hrishikeshvish/fairpy">https://github.com/hrishikeshvish/fairpy</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05508] FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation in Large Language Models](http://arxiv.org/abs/2302.05508) #fair</code></li>
<li>Summary: <p>Studies have shown that large pretrained language models exhibit biases
against social groups based on race, gender etc, which they inherit from the
datasets they are trained on. Various researchers have proposed mathematical
tools for quantifying and identifying these biases. There have been methods
proposed to mitigate such biases. In this paper, we present a comprehensive
quantitative evaluation of different kinds of biases such as race, gender,
ethnicity, age etc. exhibited by popular pretrained language models such as
BERT, GPT-2 etc. and also present a toolkit that provides plug-and-play
interfaces to connect mathematical tools to identify biases with large
pretrained language models such as BERT, GPT-2 etc. and also present users with
the opportunity to test custom models against these metrics. The toolkit also
allows users to debias existing and custom models using the debiasing
techniques proposed so far. The toolkit is available at
https://github.com/HrishikeshVish/Fairpy.
</p></li>
</ul>

<h3>Title: Fair Enough: Standardizing Evaluation and Model Selection for Fairness Research in NLP. (arXiv:2302.05711v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05711">http://arxiv.org/abs/2302.05711</a></li>
<li>Code URL: <a href="https://github.com/hanxudong/fair_enough">https://github.com/hanxudong/fair_enough</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05711] Fair Enough: Standardizing Evaluation and Model Selection for Fairness Research in NLP](http://arxiv.org/abs/2302.05711) #fair</code></li>
<li>Summary: <p>Modern NLP systems exhibit a range of biases, which a growing literature on
model debiasing attempts to correct. However current progress is hampered by a
plurality of definitions of bias, means of quantification, and oftentimes vague
relation between debiasing algorithms and theoretical measures of bias. This
paper seeks to clarify the current situation and plot a course for meaningful
progress in fair learning, with two key contributions: (1) making clear
inter-relations among the current gamut of methods, and their relation to
fairness theory; and (2) addressing the practical problem of model selection,
which involves a trade-off between fairness and accuracy and has led to
systemic issues in fairness research. Putting them together, we make several
recommendations to help shape future work.
</p></li>
</ul>

<h3>Title: On Testing and Comparing Fair classifiers under Data Bias. (arXiv:2302.05906v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05906">http://arxiv.org/abs/2302.05906</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05906] On Testing and Comparing Fair classifiers under Data Bias](http://arxiv.org/abs/2302.05906) #fair</code></li>
<li>Summary: <p>In this paper, we consider a theoretical model for injecting data bias,
namely, under-representation and label bias (Blum &amp; Stangl, 2019). We
theoretically and empirically study its effect on the accuracy and fairness of
fair classifiers. Theoretically, we prove that the Bayes optimal group-aware
fair classifier on the original data distribution can be recovered by simply
minimizing a carefully chosen reweighed loss on the bias-injected distribution.
Through extensive experiments on both synthetic and real-world datasets (e.g.,
Adult, German Credit, Bank Marketing, COMPAS), we empirically audit pre-, in-,
and post-processing fair classifiers from standard fairness toolkits for their
fairness and accuracy by injecting varying amounts of under-representation and
label bias in their training data (but not the test data). Our main
observations are: (1) The fairness and accuracy of many standard fair
classifiers degrade severely as the bias injected in their training data
increases, (2) A simple logistic regression model trained on the right data can
often outperform, in both accuracy and fairness, most fair classifiers trained
on biased training data, and (3) A few, simple fairness techniques (e.g.,
reweighing, exponentiated gradients) seem to offer stable accuracy and fairness
guarantees even when their training data is injected with under-representation
and label bias. Our experiments also show how to integrate a measure of data
bias risk in the existing fairness dashboards for real-world deployments
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: A Survey on Spectral Graph Neural Networks. (arXiv:2302.05631v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05631">http://arxiv.org/abs/2302.05631</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05631] A Survey on Spectral Graph Neural Networks](http://arxiv.org/abs/2302.05631) #interpretability</code></li>
<li>Summary: <p>Graph neural networks (GNNs) have attracted considerable attention from the
research community. It is well established that GNNs are usually roughly
divided into spatial and spectral methods. Despite that spectral GNNs play an
important role in both graph signal processing and graph representation
learning, existing studies are biased toward spatial approaches, and there is
no comprehensive review on spectral GNNs so far. In this paper, we summarize
the recent development of spectral GNNs, including model, theory, and
application. Specifically, we first discuss the connection between spatial GNNs
and spectral GNNs, which shows that spectral GNNs can capture global
information and have better expressiveness and interpretability. Next, we
categorize existing spectral GNNs according to the spectrum information they
use, \ie, eigenvalues or eigenvectors. In addition, we review major theoretical
results and applications of spectral GNNs, followed by a quantitative
experiment to benchmark some popular spectral GNNs. Finally, we conclude the
paper with some future directions.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Adding Conditional Control to Text-to-Image Diffusion Models. (arXiv:2302.05543v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05543">http://arxiv.org/abs/2302.05543</a></li>
<li>Code URL: <a href="https://github.com/lllyasviel/controlnet">https://github.com/lllyasviel/controlnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05543] Adding Conditional Control to Text-to-Image Diffusion Models](http://arxiv.org/abs/2302.05543) #diffusion</code></li>
<li>Summary: <p>We present a neural network structure, ControlNet, to control pretrained
large diffusion models to support additional input conditions. The ControlNet
learns task-specific conditions in an end-to-end way, and the learning is
robust even when the training dataset is small (< 50k). Moreover, training a
ControlNet is as fast as fine-tuning a diffusion model, and the model can be
trained on a personal devices. Alternatively, if powerful computation clusters
are available, the model can scale to large amounts (millions to billions) of
data. We report that large diffusion models like Stable Diffusion can be
augmented with ControlNets to enable conditional inputs like edge maps,
segmentation maps, keypoints, etc. This may enrich the methods to control large
diffusion models and further facilitate related applications.
</p></li>
</ul>

<h3>Title: 3D Colored Shape Reconstruction from a Single RGB Image through Diffusion. (arXiv:2302.05573v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05573">http://arxiv.org/abs/2302.05573</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05573] 3D Colored Shape Reconstruction from a Single RGB Image through Diffusion](http://arxiv.org/abs/2302.05573) #diffusion</code></li>
<li>Summary: <p>We propose a novel 3d colored shape reconstruction method from a single RGB
image through diffusion model. Diffusion models have shown great development
potentials for high-quality 3D shape generation. However, most existing work
based on diffusion models only focus on geometric shape generation, they cannot
either accomplish 3D reconstruction from a single image, or produce 3D
geometric shape with color information. In this work, we propose to reconstruct
a 3D colored shape from a single RGB image through a novel conditional
diffusion model. The reverse process of the proposed diffusion model is
consisted of three modules, shape prediction module, color prediction module
and NeRF-like rendering module. In shape prediction module, the reference RGB
image is first encoded into a high-level shape feature and then the shape
feature is utilized as a condition to predict the reverse geometric noise in
diffusion model. Then the color of each 3D point updated in shape prediction
module is predicted by color prediction module. Finally, a NeRF-like rendering
module is designed to render the colored point cloud predicted by the former
two modules to 2D image space to guide the training conditioned only on a
reference image. As far as the authors know, the proposed method is the first
diffusion model for 3D colored shape reconstruction from a single RGB image.
Experimental results demonstrate that the proposed method achieves competitive
performance on colored 3D shape reconstruction, and the ablation study
validates the positive role of the color prediction module in improving the
reconstruction quality of 3D geometric point cloud.
</p></li>
</ul>

<h3>Title: I$^2$SB: Image-to-Image Schr\"odinger Bridge. (arXiv:2302.05872v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05872">http://arxiv.org/abs/2302.05872</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05872] I$^2$SB: Image-to-Image Schr\"odinger Bridge](http://arxiv.org/abs/2302.05872) #diffusion</code></li>
<li>Summary: <p>We propose Image-to-Image Schr\"odinger Bridge (I$^2$SB), a new class of
conditional diffusion models that directly learn the nonlinear diffusion
processes between two given distributions. These diffusion bridges are
particularly useful for image restoration, as the degraded images are
structurally informative priors for reconstructing the clean images. I$^2$SB
belongs to a tractable class of Schr\"odinger bridge, the nonlinear extension
to score-based models, whose marginal distributions can be computed
analytically given boundary pairs. This results in a simulation-free framework
for nonlinear diffusions, where the I$^2$SB training becomes scalable by
adopting practical techniques used in standard diffusion models. We validate
I$^2$SB in solving various image restoration tasks, including inpainting,
super-resolution, deblurring, and JPEG restoration on ImageNet 256x256 and show
that I$^2$SB surpasses standard conditional diffusion models with more
interpretable generative processes. Moreover, I$^2$SB matches the performance
of inverse methods that additionally require the knowledge of the corruption
operators. Our work opens up new algorithmic opportunities for developing
efficient nonlinear diffusion models on a large scale. scale. Project page:
https://i2sb.github.io/
</p></li>
</ul>

<h3>Title: Single Motion Diffusion. (arXiv:2302.05905v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05905">http://arxiv.org/abs/2302.05905</a></li>
<li>Code URL: <a href="https://github.com/sinmdm/sinmdm">https://github.com/sinmdm/sinmdm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05905] Single Motion Diffusion](http://arxiv.org/abs/2302.05905) #diffusion</code></li>
<li>Summary: <p>Synthesizing realistic animations of humans, animals, and even imaginary
creatures, has long been a goal for artists and computer graphics
professionals. Compared to the imaging domain, which is rich with large
available datasets, the number of data instances for the motion domain is
limited, particularly for the animation of animals and exotic creatures (e.g.,
dragons), which have unique skeletons and motion patterns. In this work, we
present a Single Motion Diffusion Model, dubbed SinMDM, a model designed to
learn the internal motifs of a single motion sequence with arbitrary topology
and synthesize motions of arbitrary length that are faithful to them. We
harness the power of diffusion models and present a denoising network designed
specifically for the task of learning from a single input motion. Our
transformer-based architecture avoids overfitting by using local attention
layers that narrow the receptive field, and encourages motion diversity by
using relative positional embedding. SinMDM can be applied in a variety of
contexts, including spatial and temporal in-betweening, motion expansion, style
transfer, and crowd animation. Our results show that SinMDM outperforms
existing methods both in quality and time-space efficiency. Moreover, while
current approaches require additional training for different applications, our
work facilitates these applications at inference time. Our code and trained
models are available at https://sinmdm.github.io/SinMDM-page.
</p></li>
</ul>

<h3>Title: A Reparameterized Discrete Diffusion Model for Text Generation. (arXiv:2302.05737v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05737">http://arxiv.org/abs/2302.05737</a></li>
<li>Code URL: <a href="https://github.com/hkunlp/reparam-discrete-diffusion">https://github.com/hkunlp/reparam-discrete-diffusion</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05737] A Reparameterized Discrete Diffusion Model for Text Generation](http://arxiv.org/abs/2302.05737) #diffusion</code></li>
<li>Summary: <p>This work studies discrete diffusion probabilistic models with applications
to natural language generation. We derive an alternative yet equivalent
formulation of the sampling from discrete diffusion processes and leverage this
insight to develop a family of reparameterized discrete diffusion models. The
derived generic framework is highly flexible, offers a fresh perspective of the
generation process in discrete diffusion models, and features more effective
training and decoding techniques. We conduct extensive experiments to evaluate
the text generation capability of our model, demonstrating significant
improvements over existing diffusion models.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
