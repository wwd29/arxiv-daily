<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: CRYPTO-MINE: Cryptanalysis via Mutual Information Neural Estimation. (arXiv:2309.08019v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08019">http://arxiv.org/abs/2309.08019</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08019]] CRYPTO-MINE: Cryptanalysis via Mutual Information Neural Estimation(http://arxiv.org/abs/2309.08019)</code></li>
<li>Summary: <p>The use of Mutual Information (MI) as a measure to evaluate the efficiency of
cryptosystems has an extensive history. However, estimating MI between unknown
random variables in a high-dimensional space is challenging. Recent advances in
machine learning have enabled progress in estimating MI using neural networks.
This work presents a novel application of MI estimation in the field of
cryptography. We propose applying this methodology directly to estimate the MI
between plaintext and ciphertext in a chosen plaintext attack. The leaked
information, if any, from the encryption could potentially be exploited by
adversaries to compromise the computational security of the cryptosystem. We
evaluate the efficiency of our approach by empirically analyzing multiple
encryption schemes and baseline approaches. Furthermore, we extend the analysis
to novel network coding-based cryptosystems that provide individual secrecy and
study the relationship between information leakage and input distribution.
</p></li>
</ul>

<h3>Title: A Duty to Forget, a Right to be Assured? Exposing Vulnerabilities in Machine Unlearning Services. (arXiv:2309.08230v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08230">http://arxiv.org/abs/2309.08230</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08230]] A Duty to Forget, a Right to be Assured? Exposing Vulnerabilities in Machine Unlearning Services(http://arxiv.org/abs/2309.08230)</code></li>
<li>Summary: <p>The right to be forgotten requires the removal or "unlearning" of a user's
data from machine learning models. However, in the context of Machine Learning
as a Service (MLaaS), retraining a model from scratch to fulfill the unlearning
request is impractical due to the lack of training data on the service
provider's side (the server). Furthermore, approximate unlearning further
embraces a complex trade-off between utility (model performance) and privacy
(unlearning performance). In this paper, we try to explore the potential
threats posed by unlearning services in MLaaS, specifically over-unlearning,
where more information is unlearned than expected. We propose two strategies
that leverage over-unlearning to measure the impact on the trade-off balancing,
under black-box access settings, in which the existing machine unlearning
attacks are not applicable. The effectiveness of these strategies is evaluated
through extensive experiments on benchmark datasets, across various model
architectures and representative unlearning approaches. Results indicate
significant potential for both strategies to undermine model efficacy in
unlearning scenarios. This study uncovers an underexplored gap between
unlearning and contemporary MLaaS, highlighting the need for careful
considerations in balancing data unlearning, model utility, and security.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: hear-your-action: human action recognition by ultrasound active sensing. (arXiv:2309.08087v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08087">http://arxiv.org/abs/2309.08087</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08087]] hear-your-action: human action recognition by ultrasound active sensing(http://arxiv.org/abs/2309.08087)</code></li>
<li>Summary: <p>Action recognition is a key technology for many industrial applications.
Methods using visual information such as images are very popular. However,
privacy issues prevent widespread usage due to the inclusion of private
information, such as visible faces and scene backgrounds, which are not
necessary for recognizing user action. In this paper, we propose a
privacy-preserving action recognition by ultrasound active sensing. As action
recognition from ultrasound active sensing in a non-invasive manner is not well
investigated, we create a new dataset for action recognition and conduct a
comparison of features for classification. We calculated feature values by
focusing on the temporal variation of the amplitude of ultrasound reflected
waves and performed classification using a support vector machine and VGG for
eight fundamental action classes. We confirmed that our method achieved an
accuracy of 97.9% when trained and evaluated on the same person and in the same
environment. Additionally, our method achieved an accuracy of 89.5% even when
trained and evaluated on different people. We also report the analyses of
accuracies in various conditions and limitations.
</p></li>
</ul>

<h3>Title: Privacy-Aware Joint Source-Channel Coding for image transmission based on Disentangled Information Bottleneck. (arXiv:2309.08188v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08188">http://arxiv.org/abs/2309.08188</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08188]] Privacy-Aware Joint Source-Channel Coding for image transmission based on Disentangled Information Bottleneck(http://arxiv.org/abs/2309.08188)</code></li>
<li>Summary: <p>Current privacy-aware joint source-channel coding (JSCC) works aim at
avoiding private information transmission by adversarially training the JSCC
encoder and decoder under specific signal-to-noise ratios (SNRs) of
eavesdroppers. However, these approaches incur additional computational and
storage requirements as multiple neural networks must be trained for various
eavesdroppers' SNRs to determine the transmitted information. To overcome this
challenge, we propose a novel privacy-aware JSCC for image transmission based
on disentangled information bottleneck (DIB-PAJSCC). In particular, we derive a
novel disentangled information bottleneck objective to disentangle private and
public information. Given the separate information, the transmitter can
transmit only public information to the receiver while minimizing
reconstruction distortion. Since DIB-PAJSCC transmits only public information
regardless of the eavesdroppers' SNRs, it can eliminate additional training
adapted to eavesdroppers' SNRs. Experimental results show that DIB-PAJSCC can
reduce the eavesdropping accuracy on private information by up to 20\% compared
to existing methods.
</p></li>
</ul>

<h3>Title: Learning in the Dark: Privacy-Preserving Machine Learning using Function Approximation. (arXiv:2309.08190v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08190">http://arxiv.org/abs/2309.08190</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08190]] Learning in the Dark: Privacy-Preserving Machine Learning using Function Approximation(http://arxiv.org/abs/2309.08190)</code></li>
<li>Summary: <p>Over the past few years, a tremendous growth of machine learning was brought
about by a significant increase in adoption and implementation of cloud-based
services. As a result, various solutions have been proposed in which the
machine learning models run on a remote cloud provider and not locally on a
user's machine. However, when such a model is deployed on an untrusted cloud
provider, it is of vital importance that the users' privacy is preserved. To
this end, we propose Learning in the Dark -- a hybrid machine learning model in
which the training phase occurs in plaintext data, but the classification of
the users' inputs is performed directly on homomorphically encrypted
ciphertexts. To make our construction compatible with homomorphic encryption,
we approximate the ReLU and Sigmoid activation functions using low-degree
Chebyshev polynomials. This allowed us to build Learning in the Dark -- a
privacy-preserving machine learning model that can classify encrypted images
with high accuracy. Learning in the Dark preserves users' privacy since it is
capable of performing high accuracy predictions by performing computations
directly on encrypted data. In addition to that, the output of Learning in the
Dark is generated in a blind and therefore privacy-preserving way by utilizing
the properties of homomorphic encryption.
</p></li>
</ul>

<h3>Title: Verifiable Privacy-Preserving Computing. (arXiv:2309.08248v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08248">http://arxiv.org/abs/2309.08248</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08248]] Verifiable Privacy-Preserving Computing(http://arxiv.org/abs/2309.08248)</code></li>
<li>Summary: <p>Privacy-enhancing technologies (PETs), such as secure multi-party computation
(MPC) and homomorphic encryption (HE), are deployed increasingly often to
guarantee data confidentiality in computations over private, distributed data.
Similarly, we observe a steep increase in the adoption of zero-knowledge proofs
(ZKPs) to guarantee (public) verifiability of locally executed computations. We
project that applications that are data intensive and require strong privacy
guarantees, are also likely to require correctness guarantees. While the
combination of methods for (public) verifiability and privacy protection has
clear significance, many attempts are far from practical adoption.
</p>
<p>In this work, we analyze existing solutions that add (public) verifiability
to privacy-preserving computations over distributed data, in order to preserve
confidentiality and guarantee correctness. To determine the required security
and usability properties and whether these are satisfied, we look at various
application areas including verifiable outsourcing, distributed ledger
technology (DLT), and genomics. We then classify the solutions and describe
frequently used approaches as well as efficiency metrics. Last but not least,
we identify open challenges and discuss directions for future research that
make verifiable, privacy-preserving computations more secure, efficient, and
applicable in the real world.
</p></li>
</ul>

<h3>Title: Local Differential Privacy in Graph Neural Networks: a Reconstruction Approach. (arXiv:2309.08569v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08569">http://arxiv.org/abs/2309.08569</a></li>
<li>Code URL: https://github.com/karuna-bhaila/rgnn</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08569]] Local Differential Privacy in Graph Neural Networks: a Reconstruction Approach(http://arxiv.org/abs/2309.08569)</code></li>
<li>Summary: <p>Graph Neural Networks have achieved tremendous success in modeling complex
graph data in a variety of applications. However, there are limited studies
investigating privacy protection in GNNs. In this work, we propose a learning
framework that can provide node privacy at the user level, while incurring low
utility loss. We focus on a decentralized notion of Differential Privacy,
namely Local Differential Privacy, and apply randomization mechanisms to
perturb both feature and label data at the node level before the data is
collected by a central server for model training. Specifically, we investigate
the application of randomization mechanisms in high-dimensional feature
settings and propose an LDP protocol with strict privacy guarantees. Based on
frequency estimation in statistical analysis of randomized data, we develop
reconstruction methods to approximate features and labels from perturbed data.
We also formulate this learning framework to utilize frequency estimates of
graph clusters to supervise the training procedure at a sub-graph level.
Extensive experiments on real-world and semi-synthetic datasets demonstrate the
validity of our proposed model.
</p></li>
</ul>

<h3>Title: Gaussian Processes with Linear Multiple Kernel: Spectrum Design and Distributed Learning for Multi-Dimensional Data. (arXiv:2309.08201v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08201">http://arxiv.org/abs/2309.08201</a></li>
<li>Code URL: https://github.com/richardcsuwandi/distributed-gsm</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08201]] Gaussian Processes with Linear Multiple Kernel: Spectrum Design and Distributed Learning for Multi-Dimensional Data(http://arxiv.org/abs/2309.08201)</code></li>
<li>Summary: <p>Gaussian processes (GPs) have emerged as a prominent technique for machine
learning and signal processing. A key component in GP modeling is the choice of
kernel, and linear multiple kernels (LMKs) have become an attractive kernel
class due to their powerful modeling capacity and interpretability. This paper
focuses on the grid spectral mixture (GSM) kernel, an LMK that can approximate
arbitrary stationary kernels. Specifically, we propose a novel GSM kernel
formulation for multi-dimensional data that reduces the number of
hyper-parameters compared to existing formulations, while also retaining a
favorable optimization structure and approximation capability. In addition, to
make the large-scale hyper-parameter optimization in the GSM kernel tractable,
we first introduce the distributed SCA (DSCA) algorithm. Building on this, we
propose the doubly distributed SCA (D$^2$SCA) algorithm based on the
alternating direction method of multipliers (ADMM) framework, which allows us
to cooperatively learn the GSM kernel in the context of big data while
maintaining data privacy. Furthermore, we tackle the inherent communication
bandwidth restriction in distributed frameworks, by quantizing the
hyper-parameters in D$^2$SCA, resulting in the quantized doubly distributed SCA
(QD$^2$SCA) algorithm. Theoretical analysis establishes convergence guarantees
for the proposed algorithms, while experiments on diverse datasets demonstrate
the superior prediction performance and efficiency of our methods.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: ICLEF: In-Context Learning with Expert Feedback for Explainable Style Transfer. (arXiv:2309.08583v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08583">http://arxiv.org/abs/2309.08583</a></li>
<li>Code URL: https://github.com/asaakyan/explain-st</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08583]] ICLEF: In-Context Learning with Expert Feedback for Explainable Style Transfer(http://arxiv.org/abs/2309.08583)</code></li>
<li>Summary: <p>While state-of-the-art language models excel at the style transfer task,
current work does not address explainability of style transfer systems.
Explanations could be generated using large language models such as GPT-3.5 and
GPT-4, but the use of such complex systems is inefficient when smaller, widely
distributed, and transparent alternatives are available. We propose a framework
to augment and improve a formality style transfer dataset with explanations via
model distillation from ChatGPT. To further refine the generated explanations,
we propose a novel way to incorporate scarce expert human feedback using
in-context learning (ICLEF: In-Context Learning from Expert Feedback) by
prompting ChatGPT to act as a critic to its own outputs. We use the resulting
dataset of 9,960 explainable formality style transfer instances (e-GYAFC) to
show that current openly distributed instruction-tuned models (and, in some
settings, ChatGPT) perform poorly on the task, and that fine-tuning on our
high-quality dataset leads to significant improvements as shown by automatic
evaluation. In human evaluation, we show that models much smaller than ChatGPT
fine-tuned on our data align better with expert preferences. Finally, we
discuss two potential applications of models fine-tuned on the explainable
style transfer task: interpretable authorship verification and interpretable
adversarial attacks on AI-generated text detectors.
</p></li>
</ul>

<h3>Title: SLMIA-SR: Speaker-Level Membership Inference Attacks against Speaker Recognition Systems. (arXiv:2309.07983v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07983">http://arxiv.org/abs/2309.07983</a></li>
<li>Code URL: https://github.com/s3l-official/slmia-sr</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07983]] SLMIA-SR: Speaker-Level Membership Inference Attacks against Speaker Recognition Systems(http://arxiv.org/abs/2309.07983)</code></li>
<li>Summary: <p>Membership inference attacks allow adversaries to determine whether a
particular example was contained in the model's training dataset. While
previous works have confirmed the feasibility of such attacks in various
applications, none has focused on speaker recognition (SR), a promising
voice-based biometric recognition technique. In this work, we propose SLMIA-SR,
the first membership inference attack tailored to SR. In contrast to
conventional example-level attack, our attack features speaker-level membership
inference, i.e., determining if any voices of a given speaker, either the same
as or different from the given inference voices, have been involved in the
training of a model. It is particularly useful and practical since the training
and inference voices are usually distinct, and it is also meaningful
considering the open-set nature of SR, namely, the recognition speakers were
often not present in the training data. We utilize intra-closeness and
inter-farness, two training objectives of SR, to characterize the differences
between training and non-training speakers and quantify them with two groups of
features driven by carefully-established feature engineering to mount the
attack. To improve the generalizability of our attack, we propose a novel
mixing ratio training strategy to train attack models. To enhance the attack
performance, we introduce voice chunk splitting to cope with the limited number
of inference voices and propose to train attack models dependent on the number
of inference voices. Our attack is versatile and can work in both white-box and
black-box scenarios. Additionally, we propose two novel techniques to reduce
the number of black-box queries while maintaining the attack performance.
Extensive experiments demonstrate the effectiveness of SLMIA-SR.
</p></li>
</ul>

<h3>Title: Unleashing the Adversarial Facet of Software Debloating. (arXiv:2309.08058v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08058">http://arxiv.org/abs/2309.08058</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08058]] Unleashing the Adversarial Facet of Software Debloating(http://arxiv.org/abs/2309.08058)</code></li>
<li>Summary: <p>Software debloating techniques are applied to craft a specialized version of
the program based on the user's requirements and remove irrelevant code
accordingly. The debloated programs presumably maintain better performance and
reduce the attack surface in contrast to the original programs. This work
unleashes the effectiveness of applying software debloating techniques on the
robustness of machine learning systems in the malware classification domain. We
empirically study how an adversarial can leverage software debloating
techniques to mislead machine learning malware classification models. We apply
software debloating techniques to generate adversarial examples and demonstrate
these adversarial examples can reduce the detection rate of VirusTotal. Our
study opens new directions for research into adversarial machine learning not
only in malware detection/classification but also in other software domains.
</p></li>
</ul>

<h3>Title: Lattice attack on group ring NTRU: The case of the dihedral group. (arXiv:2309.08304v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08304">http://arxiv.org/abs/2309.08304</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08304]] Lattice attack on group ring NTRU: The case of the dihedral group(http://arxiv.org/abs/2309.08304)</code></li>
<li>Summary: <p>Group ring NTRU (GR-NTRU) provides a general structure to design different
variants of NTRU-like schemes by employing different groups. Although, most of
the schemes in literature are built over cyclic groups, nonabelian groups can
also be used. Coppersmith and Shamir in 1997 have suggested that
noncommutativity may result in better security against some lattice attacks for
some groups. Lattice attacks on the public key of NTRU-like cryptosystems try
to retrieve the private key by solving the shortest vector problem (SVP) or its
approximation in a lattice of a certain dimension, assuming the knowledge of
the public key only. This paper shows that dihedral groups do not guarantee
better security against this class of attacks. We prove that retrieving the
private key is possible by solving the SVP in two lattices with half the
dimension of the original lattice generated for GR-NTRU based on dihedral
groups. The possibility of such an attack was mentioned by Yasuda et
al.(IACR/2015/1170). In contrast to their proposed approach, we explicitly
provide the lattice reduction without any structure theorem from the
representation theory for finite groups. Furthermore, we demonstrate the
effectiveness of our technique with experimental results.
</p></li>
</ul>

<h3>Title: HINT: Healthy Influential-Noise based Training to Defend against Data Poisoning Attacks. (arXiv:2309.08549v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08549">http://arxiv.org/abs/2309.08549</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08549]] HINT: Healthy Influential-Noise based Training to Defend against Data Poisoning Attacks(http://arxiv.org/abs/2309.08549)</code></li>
<li>Summary: <p>While numerous defense methods have been proposed to prohibit potential
poisoning attacks from untrusted data sources, most research works only defend
against specific attacks, which leaves many avenues for an adversary to
exploit. In this work, we propose an efficient and robust training approach to
defend against data poisoning attacks based on influence functions, named
Healthy Influential-Noise based Training. Using influence functions, we craft
healthy noise that helps to harden the classification model against poisoning
attacks without significantly affecting the generalization ability on test
data. In addition, our method can perform effectively when only a subset of the
training data is modified, instead of the current method of adding noise to all
examples that has been used in several previous works. We conduct comprehensive
evaluations over two image datasets with state-of-the-art poisoning attacks
under different realistic attack scenarios. Our empirical results show that
HINT can efficiently protect deep learning models against the effect of both
untargeted and targeted poisoning attacks.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Vision-based Analysis of Driver Activity and Driving Performance Under the Influence of Alcohol. (arXiv:2309.08021v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08021">http://arxiv.org/abs/2309.08021</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08021]] Vision-based Analysis of Driver Activity and Driving Performance Under the Influence of Alcohol(http://arxiv.org/abs/2309.08021)</code></li>
<li>Summary: <p>About 30% of all traffic crash fatalities in the United States involve drunk
drivers, making the prevention of drunk driving paramount to vehicle safety in
the US and other locations which have a high prevalence of driving while under
the influence of alcohol. Driving impairment can be monitored through active
use of sensors (when drivers are asked to engage in providing breath samples to
a vehicle instrument or when pulled over by a police officer), but a more
passive and robust mechanism of sensing may allow for wider adoption and
benefit of intelligent systems that reduce drunk driving accidents. This could
assist in identifying impaired drivers before they drive, or early in the
driving process (before a crash or detection by law enforcement). In this
research, we introduce a study which adopts a multi-modal ensemble of visual,
thermal, audio, and chemical sensors to (1) examine the impact of acute alcohol
administration on driving performance in a driving simulator, and (2) identify
data-driven methods for detecting driving under the influence of alcohol. We
describe computer vision and machine learning models for analyzing the driver's
face in thermal imagery, and introduce a pipeline for training models on data
collected from drivers with a range of breath-alcohol content levels, including
discussion of relevant machine learning phenomena which can help in future
experiment design for related studies.
</p></li>
</ul>

<h3>Title: DA-RAW: Domain Adaptive Object Detection for Real-World Adverse Weather Conditions. (arXiv:2309.08152v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08152">http://arxiv.org/abs/2309.08152</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08152]] DA-RAW: Domain Adaptive Object Detection for Real-World Adverse Weather Conditions(http://arxiv.org/abs/2309.08152)</code></li>
<li>Summary: <p>Despite the success of deep learning-based object detection methods in recent
years, it is still challenging to make the object detector reliable in adverse
weather conditions such as rain and snow. For the robust performance of object
detectors, unsupervised domain adaptation has been utilized to adapt the
detection network trained on clear weather images to adverse weather images.
While previous methods do not explicitly address weather corruption during
adaptation, the domain gap between clear and adverse weather can be decomposed
into two factors with distinct characteristics: a style gap and a weather gap.
In this paper, we present an unsupervised domain adaptation framework for
object detection that can more effectively adapt to real-world environments
with adverse weather conditions by addressing these two gaps separately. Our
method resolves the style gap by concentrating on style-related information of
high-level features using an attention module. Using self-supervised
contrastive learning, our framework then reduces the weather gap and acquires
instance features that are robust to weather corruption. Extensive experiments
demonstrate that our method outperforms other methods for object detection in
adverse weather conditions.
</p></li>
</ul>

<h3>Title: Human-Inspired Topological Representations for Visual Object Recognition in Unseen Environments. (arXiv:2309.08239v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08239">http://arxiv.org/abs/2309.08239</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08239]] Human-Inspired Topological Representations for Visual Object Recognition in Unseen Environments(http://arxiv.org/abs/2309.08239)</code></li>
<li>Summary: <p>Visual object recognition in unseen and cluttered indoor environments is a
challenging problem for mobile robots. Toward this goal, we extend our previous
work to propose the TOPS2 descriptor, and an accompanying recognition
framework, THOR2, inspired by a human reasoning mechanism known as object
unity. We interleave color embeddings obtained using the Mapper algorithm for
topological soft clustering with the shape-based TOPS descriptor to obtain the
TOPS2 descriptor. THOR2, trained using synthetic data, achieves substantially
higher recognition accuracy than the shape-based THOR framework and outperforms
RGB-D ViT on two real-world datasets: the benchmark OCID dataset and the UW-IS
Occluded dataset. Therefore, THOR2 is a promising step toward achieving robust
recognition in low-cost robots.
</p></li>
</ul>

<h3>Title: Optimization of Rank Losses for Image Retrieval. (arXiv:2309.08250v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08250">http://arxiv.org/abs/2309.08250</a></li>
<li>Code URL: https://github.com/cvdfoundation/google-landmark</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08250]] Optimization of Rank Losses for Image Retrieval(http://arxiv.org/abs/2309.08250)</code></li>
<li>Summary: <p>In image retrieval, standard evaluation metrics rely on score ranking, \eg
average precision (AP), recall at k (R@k), normalized discounted cumulative
gain (NDCG). In this work we introduce a general framework for robust and
decomposable rank losses optimization. It addresses two major challenges for
end-to-end training of deep neural networks with rank losses:
non-differentiability and non-decomposability. Firstly we propose a general
surrogate for ranking operator, SupRank, that is amenable to stochastic
gradient descent. It provides an upperbound for rank losses and ensures robust
training. Secondly, we use a simple yet effective loss function to reduce the
decomposability gap between the averaged batch approximation of ranking losses
and their values on the whole training set. We apply our framework to two
standard metrics for image retrieval: AP and R@k. Additionally we apply our
framework to hierarchical image retrieval. We introduce an extension of AP, the
hierarchical average precision $\mathcal{H}$-AP, and optimize it as well as the
NDCG. Finally we create the first hierarchical landmarks retrieval dataset. We
use a semi-automatic pipeline to create hierarchical labels, extending the
large scale Google Landmarks v2 dataset. The hierarchical dataset is publicly
available at https://github.com/cvdfoundation/google-landmark. Code will be
released at https://github.com/elias-ramzi/SupRank.
</p></li>
</ul>

<h3>Title: BROW: Better featuRes fOr Whole slide image based on self-distillation. (arXiv:2309.08259v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08259">http://arxiv.org/abs/2309.08259</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08259]] BROW: Better featuRes fOr Whole slide image based on self-distillation(http://arxiv.org/abs/2309.08259)</code></li>
<li>Summary: <p>Whole slide image (WSI) processing is becoming part of the key components of
standard clinical diagnosis for various diseases. However, the direct
application of conventional image processing algorithms to WSI faces certain
obstacles because of WSIs' distinct property: the super-high resolution. The
performance of most WSI-related tasks relies on the efficacy of the backbone
which extracts WSI patch feature representations. Hence, we proposed BROW, a
foundation model for extracting better feature representations for WSIs, which
can be conveniently adapted to downstream tasks without or with slight
fine-tuning. The model takes transformer architecture, pretrained using
self-distillation framework. To improve model's robustness, techniques such as
patch shuffling have been employed. Additionally, the model leverages the
unique properties of WSIs, utilizing WSI's multi-scale pyramid to incorporate
an additional global view, thereby further enhancing its performance. We used
both private and public data to make up a large pretraining dataset, containing
more than 11000 slides, over 180M extracted patches, encompassing WSIs related
to various organs and tissues. To assess the effectiveness of \ourmodel, we run
a wide range of downstream tasks, including slide-level subtyping, patch-level
classification and nuclei instance segmentation. The results confirmed the
efficacy, robustness and good generalization ability of the proposed model.
This substantiates its potential as foundation model for WSI feature extraction
and highlights promising prospects for its application in WSI processing.
</p></li>
</ul>

<h3>Title: Robust Burned Area Delineation through Multitask Learning. (arXiv:2309.08368v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08368">http://arxiv.org/abs/2309.08368</a></li>
<li>Code URL: https://github.com/links-ads/burned-area-seg</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08368]] Robust Burned Area Delineation through Multitask Learning(http://arxiv.org/abs/2309.08368)</code></li>
<li>Summary: <p>In recent years, wildfires have posed a significant challenge due to their
increasing frequency and severity. For this reason, accurate delineation of
burned areas is crucial for environmental monitoring and post-fire assessment.
However, traditional approaches relying on binary segmentation models often
struggle to achieve robust and accurate results, especially when trained from
scratch, due to limited resources and the inherent imbalance of this
segmentation task. We propose to address these limitations in two ways: first,
we construct an ad-hoc dataset to cope with the limited resources, combining
information from Sentinel-2 feeds with Copernicus activations and other data
sources. In this dataset, we provide annotations for multiple tasks, including
burned area delineation and land cover segmentation. Second, we propose a
multitask learning framework that incorporates land cover classification as an
auxiliary task to enhance the robustness and performance of the burned area
segmentation models. We compare the performance of different models, including
UPerNet and SegFormer, demonstrating the effectiveness of our approach in
comparison to standard binary segmentation.
</p></li>
</ul>

<h3>Title: Robust Frame-to-Frame Camera Rotation Estimation in Crowded Scenes. (arXiv:2309.08588v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08588">http://arxiv.org/abs/2309.08588</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08588]] Robust Frame-to-Frame Camera Rotation Estimation in Crowded Scenes(http://arxiv.org/abs/2309.08588)</code></li>
<li>Summary: <p>We present an approach to estimating camera rotation in crowded, real-world
scenes from handheld monocular video. While camera rotation estimation is a
well-studied problem, no previous methods exhibit both high accuracy and
acceptable speed in this setting. Because the setting is not addressed well by
other datasets, we provide a new dataset and benchmark, with high-accuracy,
rigorously verified ground truth, on 17 video sequences. Methods developed for
wide baseline stereo (e.g., 5-point methods) perform poorly on monocular video.
On the other hand, methods used in autonomous driving (e.g., SLAM) leverage
specific sensor setups, specific motion models, or local optimization
strategies (lagging batch processing) and do not generalize well to handheld
video. Finally, for dynamic scenes, commonly used robustification techniques
like RANSAC require large numbers of iterations, and become prohibitively slow.
We introduce a novel generalization of the Hough transform on SO(3) to
efficiently and robustly find the camera rotation most compatible with optical
flow. Among comparably fast methods, ours reduces error by almost 50\% over the
next best, and is more accurate than any method, irrespective of speed. This
represents a strong new performance point for crowded scenes, an important
setting for computer vision. The code and the dataset are available at
https://fabiendelattre.com/robust-rotation-estimation.
</p></li>
</ul>

<h3>Title: Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion. (arXiv:2309.08596v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08596">http://arxiv.org/abs/2309.08596</a></li>
<li>Code URL: https://github.com/wengflow/robust-e-nerf</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08596]] Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion(http://arxiv.org/abs/2309.08596)</code></li>
<li>Summary: <p>Event cameras offer many advantages over standard cameras due to their
distinctive principle of operation: low power, low latency, high temporal
resolution and high dynamic range. Nonetheless, the success of many downstream
visual applications also hinges on an efficient and effective scene
representation, where Neural Radiance Field (NeRF) is seen as the leading
candidate. Such promise and potential of event cameras and NeRF inspired recent
works to investigate on the reconstruction of NeRF from moving event cameras.
However, these works are mainly limited in terms of the dependence on dense and
low-noise event streams, as well as generalization to arbitrary contrast
threshold values and camera speed profiles. In this work, we propose Robust
e-NeRF, a novel method to directly and robustly reconstruct NeRFs from moving
event cameras under various real-world conditions, especially from sparse and
noisy events generated under non-uniform motion. It consists of two key
components: a realistic event generation model that accounts for various
intrinsic parameters (e.g. time-independent, asymmetric threshold and
refractory period) and non-idealities (e.g. pixel-to-pixel threshold
variation), as well as a complementary pair of normalized reconstruction losses
that can effectively generalize to arbitrary speed profiles and intrinsic
parameter values without such prior knowledge. Experiments on real and novel
realistically simulated sequences verify our effectiveness. Our code, synthetic
dataset and improved event simulator are public.
</p></li>
</ul>

<h3>Title: Exploring the Impact of Human Evaluator Group on Chat-Oriented Dialogue Evaluation. (arXiv:2309.07998v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07998">http://arxiv.org/abs/2309.07998</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07998]] Exploring the Impact of Human Evaluator Group on Chat-Oriented Dialogue Evaluation(http://arxiv.org/abs/2309.07998)</code></li>
<li>Summary: <p>Human evaluation has been widely accepted as the standard for evaluating
chat-oriented dialogue systems. However, there is a significant variation in
previous work regarding who gets recruited as evaluators. Evaluator groups such
as domain experts, university students, and professional annotators have been
used to assess and compare dialogue systems, although it is unclear to what
extent the choice of an evaluator group can affect results. This paper analyzes
the evaluator group impact on dialogue system evaluation by testing 4
state-of-the-art dialogue systems using 4 distinct evaluator groups. Our
analysis reveals a robustness towards evaluator groups for Likert evaluations
that is not seen for Pairwise, with only minor differences observed when
changing evaluator groups. Furthermore, two notable limitations to this
robustness are observed, which reveal discrepancies between evaluators with
different levels of chatbot expertise and indicate that evaluator objectivity
is beneficial for certain dialogue metrics.
</p></li>
</ul>

<h3>Title: Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases. (arXiv:2309.08345v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08345">http://arxiv.org/abs/2309.08345</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08345]] Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases(http://arxiv.org/abs/2309.08345)</code></li>
<li>Summary: <p>Language models (LMs) have already demonstrated remarkable abilities in
understanding and generating both natural and formal language. Despite these
advances, their integration with real-world environments such as large-scale
knowledge bases (KBs) remains an underdeveloped area, affecting applications
such as semantic parsing and indulging in "hallucinated" information. This
paper is an experimental investigation aimed at uncovering the robustness
challenges that LMs encounter when tasked with knowledge base question
answering (KBQA). The investigation covers scenarios with inconsistent data
distribution between training and inference, such as generalization to unseen
domains, adaptation to various language variations, and transferability across
different datasets. Our comprehensive experiments reveal that even when
employed with our proposed data augmentation techniques, advanced small and
large language models exhibit poor performance in various dimensions. While the
LM is a promising technology, the robustness of the current form in dealing
with complex environments is fragile and of limited practicality because of the
data distribution issue. This calls for future research on data collection and
LM learning paradims.
</p></li>
</ul>

<h3>Title: How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?. (arXiv:2309.08565v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08565">http://arxiv.org/abs/2309.08565</a></li>
<li>Code URL: https://github.com/dannigt/attribute-controller-transfer</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08565]] How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?(http://arxiv.org/abs/2309.08565)</code></li>
<li>Summary: <p>Customizing machine translation models to comply with fine-grained attributes
such as formality has seen tremendous progress recently. However, current
approaches mostly rely on at least some supervised data with attribute
annotation. Data scarcity therefore remains a bottleneck to democratizing such
customization possibilities to a wider range of languages, lower-resource ones
in particular. Given recent progress in pretrained massively multilingual
translation models, we use them as a foundation to transfer the attribute
controlling capabilities to languages without supervised data. In this work, we
present a comprehensive analysis of transferring attribute controllers based on
a pretrained NLLB-200 model. We investigate both training- and inference-time
control techniques under various data scenarios, and uncover their relative
strengths and weaknesses in zero-shot performance and domain robustness. We
show that both paradigms are complementary, as shown by consistent improvements
on 5 zero-shot directions. Moreover, a human evaluation on a real low-resource
language, Bengali, confirms our findings on zero-shot transfer to new target
languages. The code is
$\href{https://github.com/dannigt/attribute-controller-transfer}{\text{here}}$.
</p></li>
</ul>

<h3>Title: On Prediction Feature Assignment in the Heckman Selection Model. (arXiv:2309.08043v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08043">http://arxiv.org/abs/2309.08043</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08043]] On Prediction Feature Assignment in the Heckman Selection Model(http://arxiv.org/abs/2309.08043)</code></li>
<li>Summary: <p>Under missing-not-at-random (MNAR) sample selection bias, the performance of
a prediction model is often degraded. This paper focuses on one classic
instance of MNAR sample selection bias where a subset of samples have
non-randomly missing outcomes. The Heckman selection model and its variants
have commonly been used to handle this type of sample selection bias. The
Heckman model uses two separate equations to model the prediction and selection
of samples, where the selection features include all prediction features. When
using the Heckman model, the prediction features must be properly chosen from
the set of selection features. However, choosing the proper prediction features
is a challenging task for the Heckman model. This is especially the case when
the number of selection features is large. Existing approaches that use the
Heckman model often provide a manually chosen set of prediction features. In
this paper, we propose Heckman-FA as a novel data-driven framework for
obtaining prediction features for the Heckman model. Heckman-FA first trains an
assignment function that determines whether or not a selection feature is
assigned as a prediction feature. Using the parameters of the trained function,
the framework extracts a suitable set of prediction features based on the
goodness-of-fit of the prediction model given the chosen prediction features
and the correlation between noise terms of the prediction and selection
equations. Experimental results on real-world datasets show that Heckman-FA
produces a robust regression model under MNAR sample selection bias.
</p></li>
</ul>

<h3>Title: Towards Last-layer Retraining for Group Robustness with Fewer Annotations. (arXiv:2309.08534v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08534">http://arxiv.org/abs/2309.08534</a></li>
<li>Code URL: https://github.com/tmlabonte/last-layer-retraining</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08534]] Towards Last-layer Retraining for Group Robustness with Fewer Annotations(http://arxiv.org/abs/2309.08534)</code></li>
<li>Summary: <p>Empirical risk minimization (ERM) of neural networks is prone to
over-reliance on spurious correlations and poor generalization on minority
groups. The recent deep feature reweighting (DFR) technique achieves
state-of-the-art group robustness via simple last-layer retraining, but it
requires held-out group and class annotations to construct a group-balanced
reweighting dataset. In this work, we examine this impractical requirement and
find that last-layer retraining can be surprisingly effective with no group
annotations (other than for model selection) and only a handful of class
annotations. We first show that last-layer retraining can greatly improve
worst-group accuracy even when the reweighting dataset has only a small
proportion of worst-group data. This implies a "free lunch" where holding out a
subset of training data to retrain the last layer can substantially outperform
ERM on the entire dataset with no additional data or annotations. To further
improve group robustness, we introduce a lightweight method called selective
last-layer finetuning (SELF), which constructs the reweighting dataset using
misclassifications or disagreements. Our empirical and theoretical results
present the first evidence that model disagreement upsamples worst-group data,
enabling SELF to nearly match DFR on four well-established benchmarks across
vision and language tasks with no group annotations and less than 3% of the
held-out class annotations. Our code is available at
https://github.com/tmlabonte/last-layer-retraining.
</p></li>
</ul>

<h3>Title: Efficient and robust Sensor Placement in Complex Environments. (arXiv:2309.08545v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08545">http://arxiv.org/abs/2309.08545</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08545]] Efficient and robust Sensor Placement in Complex Environments(http://arxiv.org/abs/2309.08545)</code></li>
<li>Summary: <p>We address the problem of efficient and unobstructed surveillance or
communication in complex environments. On one hand, one wishes to use a minimal
number of sensors to cover the environment. On the other hand, it is often
important to consider solutions that are robust against sensor failure or
adversarial attacks. This paper addresses these challenges of designing minimal
sensor sets that achieve multi-coverage constraints -- every point in the
environment is covered by a prescribed number of sensors. We propose a greedy
algorithm to achieve the objective. Further, we explore deep learning
techniques to accelerate the evaluation of the objective function formulated in
the greedy algorithm. The training of the neural network reveals that the
geometric properties of the data significantly impact the network's
performance, particularly at the end stage. By taking into account these
properties, we discuss the differences in using greedy and $\epsilon$-greedy
algorithms to generate data and their impact on the robustness of the network.
</p></li>
</ul>

<h3>Title: Towards Robust Continual Learning with Bayesian Adaptive Moment Regularization. (arXiv:2309.08546v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08546">http://arxiv.org/abs/2309.08546</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08546]] Towards Robust Continual Learning with Bayesian Adaptive Moment Regularization(http://arxiv.org/abs/2309.08546)</code></li>
<li>Summary: <p>The pursuit of long-term autonomy mandates that robotic agents must
continuously adapt to their changing environments and learn to solve new tasks.
Continual learning seeks to overcome the challenge of catastrophic forgetting,
where learning to solve new tasks causes a model to forget previously learnt
information. Prior-based continual learning methods are appealing for robotic
applications as they are space efficient and typically do not increase in
computational complexity as the number of tasks grows. Despite these desirable
properties, prior-based approaches typically fail on important benchmarks and
consequently are limited in their potential applications compared to their
memory-based counterparts. We introduce Bayesian adaptive moment regularization
(BAdam), a novel prior-based method that better constrains parameter growth,
leading to lower catastrophic forgetting. Our method boasts a range of
desirable properties for robotic applications such as being lightweight and
task label-free, converging quickly, and offering calibrated uncertainty that
is important for safe real-world deployment. Results show that BAdam achieves
state-of-the-art performance for prior-based methods on challenging
single-headed class-incremental experiments such as Split MNIST and Split
FashionMNIST, and does so without relying on task labels or discrete task
boundaries.
</p></li>
</ul>

<h3>Title: A Bayesian Approach to Robust Inverse Reinforcement Learning. (arXiv:2309.08571v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08571">http://arxiv.org/abs/2309.08571</a></li>
<li>Code URL: https://github.com/rw422scarlet/bmirl_tf</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08571]] A Bayesian Approach to Robust Inverse Reinforcement Learning(http://arxiv.org/abs/2309.08571)</code></li>
<li>Summary: <p>We consider a Bayesian approach to offline model-based inverse reinforcement
learning (IRL). The proposed framework differs from existing offline
model-based IRL approaches by performing simultaneous estimation of the
expert's reward function and subjective model of environment dynamics. We make
use of a class of prior distributions which parameterizes how accurate the
expert's model of the environment is to develop efficient algorithms to
estimate the expert's reward and subjective dynamics in high-dimensional
settings. Our analysis reveals a novel insight that the estimated policy
exhibits robust performance when the expert is believed (a priori) to have a
highly accurate model of the environment. We verify this observation in the
MuJoCo environments and show that our algorithms outperform state-of-the-art
offline IRL algorithms.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Morphologically-Aware Consensus Computation via Heuristics-based IterATive Optimization (MACCHIatO). (arXiv:2309.08066v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08066">http://arxiv.org/abs/2309.08066</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08066]] Morphologically-Aware Consensus Computation via Heuristics-based IterATive Optimization (MACCHIatO)(http://arxiv.org/abs/2309.08066)</code></li>
<li>Summary: <p>The extraction of consensus segmentations from several binary or
probabilistic masks is important to solve various tasks such as the analysis of
inter-rater variability or the fusion of several neural network outputs. One of
the most widely used methods to obtain such a consensus segmentation is the
STAPLE algorithm. In this paper, we first demonstrate that the output of that
algorithm is heavily impacted by the background size of images and the choice
of the prior. We then propose a new method to construct a binary or a
probabilistic consensus segmentation based on the Fr\'{e}chet means of
carefully chosen distances which makes it totally independent of the image
background size. We provide a heuristic approach to optimize this criterion
such that a voxel's class is fully determined by its voxel-wise distance to the
different masks, the connected component it belongs to and the group of raters
who segmented it. We compared extensively our method on several datasets with
the STAPLE method and the naive segmentation averaging method, showing that it
leads to binary consensus masks of intermediate size between Majority Voting
and STAPLE and to different posterior probabilities than Mask Averaging and
STAPLE methods. Our code is available at
https://gitlab.inria.fr/dhamzaou/jaccardmap .
</p></li>
</ul>

<h3>Title: MetaF2N: Blind Image Super-Resolution by Learning Efficient Model Adaptation from Faces. (arXiv:2309.08113v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08113">http://arxiv.org/abs/2309.08113</a></li>
<li>Code URL: https://github.com/yinzhicun/metaf2n</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08113]] MetaF2N: Blind Image Super-Resolution by Learning Efficient Model Adaptation from Faces(http://arxiv.org/abs/2309.08113)</code></li>
<li>Summary: <p>Due to their highly structured characteristics, faces are easier to recover
than natural scenes for blind image super-resolution. Therefore, we can extract
the degradation representation of an image from the low-quality and recovered
face pairs. Using the degradation representation, realistic low-quality images
can then be synthesized to fine-tune the super-resolution model for the
real-world low-quality image. However, such a procedure is time-consuming and
laborious, and the gaps between recovered faces and the ground-truths further
increase the optimization uncertainty. To facilitate efficient model adaptation
towards image-specific degradations, we propose a method dubbed MetaF2N, which
leverages the contained Faces to fine-tune model parameters for adapting to the
whole Natural image in a Meta-learning framework. The degradation extraction
and low-quality image synthesis steps are thus circumvented in our MetaF2N, and
it requires only one fine-tuning step to get decent performance. Considering
the gaps between the recovered faces and ground-truths, we further deploy a
MaskNet for adaptively predicting loss weights at different positions to reduce
the impact of low-confidence areas. To evaluate our proposed MetaF2N, we have
collected a real-world low-quality dataset with one or multiple faces in each
image, and our MetaF2N achieves superior performance on both synthetic and
real-world datasets. Source code, pre-trained models, and collected datasets
are available at https://github.com/yinzhicun/MetaF2N.
</p></li>
</ul>

<h3>Title: AnyOKP: One-Shot and Instance-Aware Object Keypoint Extraction with Pretrained ViT. (arXiv:2309.08134v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08134">http://arxiv.org/abs/2309.08134</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08134]] AnyOKP: One-Shot and Instance-Aware Object Keypoint Extraction with Pretrained ViT(http://arxiv.org/abs/2309.08134)</code></li>
<li>Summary: <p>Towards flexible object-centric visual perception, we propose a one-shot
instance-aware object keypoint (OKP) extraction approach, AnyOKP, which
leverages the powerful representation ability of pretrained vision transformer
(ViT), and can obtain keypoints on multiple object instances of arbitrary
category after learning from a support image. An off-the-shelf petrained ViT is
directly deployed for generalizable and transferable feature extraction, which
is followed by training-free feature enhancement. The best-prototype pairs
(BPPs) are searched for in support and query images based on appearance
similarity, to yield instance-unaware candidate keypoints.Then, the entire
graph with all candidate keypoints as vertices are divided to sub-graphs
according to the feature distributions on the graph edges. Finally, each
sub-graph represents an object instance. AnyOKP is evaluated on real object
images collected with the cameras of a robot arm, a mobile robot, and a
surgical robot, which not only demonstrates the cross-category flexibility and
instance awareness, but also show remarkable robustness to domain shift and
viewpoint change.
</p></li>
</ul>

<h3>Title: Multi-Scale Estimation for Omni-Directional Saliency Maps Using Learnable Equator Bias. (arXiv:2309.08139v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08139">http://arxiv.org/abs/2309.08139</a></li>
<li>Code URL: https://github.com/islab-sophia/odisal</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08139]] Multi-Scale Estimation for Omni-Directional Saliency Maps Using Learnable Equator Bias(http://arxiv.org/abs/2309.08139)</code></li>
<li>Summary: <p>Omni-directional images have been used in wide range of applications. For the
applications, it would be useful to estimate saliency maps representing
probability distributions of gazing points with a head-mounted display, to
detect important regions in the omni-directional images. This paper proposes a
novel saliency-map estimation model for the omni-directional images by
extracting overlapping 2-dimensional (2D) plane images from omni-directional
images at various directions and angles of view. While 2D saliency maps tend to
have high probability at the center of images (center bias), the
high-probability region appears at horizontal directions in omni-directional
saliency maps when a head-mounted display is used (equator bias). Therefore,
the 2D saliency model with a center-bias layer was fine-tuned with an
omni-directional dataset by replacing the center-bias layer to an equator-bias
layer conditioned on the elevation angle for the extraction of the 2D plane
image. The limited availability of omni-directional images in saliency datasets
can be compensated by using the well-established 2D saliency model pretrained
by a large number of training images with the ground truth of 2D saliency maps.
In addition, this paper proposes a multi-scale estimation method by extracting
2D images in multiple angles of view to detect objects of various sizes with
variable receptive fields. The saliency maps estimated from the multiple angles
of view were integrated by using pixel-wise attention weights calculated in an
integration layer for weighting the optimal scale to each object. The proposed
method was evaluated using a publicly available dataset with evaluation metrics
for omni-directional saliency maps. It was confirmed that the accuracy of the
saliency maps was improved by the proposed method.
</p></li>
</ul>

<h3>Title: A Real-time Faint Space Debris Detector With Learning-based LCM. (arXiv:2309.08244v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08244">http://arxiv.org/abs/2309.08244</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08244]] A Real-time Faint Space Debris Detector With Learning-based LCM(http://arxiv.org/abs/2309.08244)</code></li>
<li>Summary: <p>With the development of aerospace technology, the increasing population of
space debris has posed a great threat to the safety of spacecraft. However, the
low intensity of reflected light and high angular velocity of space debris
impede the extraction. Besides, due to the limitations of the ground
observation methods, small space debris can hardly be detected, making it
necessary to enhance the spacecraft's capacity for space situational awareness
(SSA). Considering that traditional methods have some defects in low-SNR target
detection, such as low effectiveness and large time consumption, this paper
proposes a method for low-SNR streak extraction based on local contrast and
maximum likelihood estimation (MLE), which can detect space objects with SNR
2.0 efficiently. In the proposed algorithm, local contrast will be applied for
crude classifications, which will return connected components as preliminary
results, and then MLE will be performed to reconstruct the connected components
of targets via orientated growth, further improving the precision. The
algorithm has been verified with both simulated streaks and real star tracker
images, and the average centroid error of the proposed algorithm is close to
the state-of-the-art method like ODCC. At the same time, the algorithm in this
paper has significant advantages in efficiency compared with ODCC. In
conclusion, the algorithm in this paper is of high speed and precision, which
guarantees its promising applications in the extraction of high dynamic
targets.
</p></li>
</ul>

<h3>Title: Automated dermatoscopic pattern discovery by clustering neural network output for human-computer interaction. (arXiv:2309.08533v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08533">http://arxiv.org/abs/2309.08533</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08533]] Automated dermatoscopic pattern discovery by clustering neural network output for human-computer interaction(http://arxiv.org/abs/2309.08533)</code></li>
<li>Summary: <p>Background: As available medical image datasets increase in size, it becomes
infeasible for clinicians to review content manually for knowledge extraction.
The objective of this study was to create an automated clustering resulting in
human-interpretable pattern discovery.
</p>
<p>Methods: Images from the public HAM10000 dataset, including 7 common
pigmented skin lesion diagnoses, were tiled into 29420 tiles and clustered via
k-means using neural network-extracted image features. The final number of
clusters per diagnosis was chosen by either the elbow method or a compactness
metric balancing intra-lesion variance and cluster numbers. The amount of
resulting non-informative clusters, defined as those containing less than six
image tiles, was compared between the two methods.
</p>
<p>Results: Applying k-means, the optimal elbow cutoff resulted in a mean of
24.7 (95%-CI: 16.4-33) clusters for every included diagnosis, including 14.9%
(95% CI: 0.8-29.0) non-informative clusters. The optimal cutoff, as estimated
by the compactness metric, resulted in significantly fewer clusters (13.4;
95%-CI 11.8-15.1; p=0.03) and less non-informative ones (7.5%; 95% CI: 0-19.5;
p=0.017). The majority of clusters (93.6%) from the compactness metric could be
manually mapped to previously described dermatoscopic diagnostic patterns.
</p>
<p>Conclusions: Automatically constraining unsupervised clustering can produce
an automated extraction of diagnostically relevant and human-interpretable
clusters of visual patterns from a large image dataset.
</p></li>
</ul>

<h3>Title: Deep Nonnegative Matrix Factorization with Beta Divergences. (arXiv:2309.08249v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08249">http://arxiv.org/abs/2309.08249</a></li>
<li>Code URL: https://github.com/vleplat/deep-kl-nmf-public</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08249]] Deep Nonnegative Matrix Factorization with Beta Divergences(http://arxiv.org/abs/2309.08249)</code></li>
<li>Summary: <p>Deep Nonnegative Matrix Factorization (deep NMF) has recently emerged as a
valuable technique for extracting multiple layers of features across different
scales. However, all existing deep NMF models and algorithms have primarily
centered their evaluation on the least squares error, which may not be the most
appropriate metric for assessing the quality of approximations on diverse
datasets. For instance, when dealing with data types such as audio signals and
documents, it is widely acknowledged that $\beta$-divergences offer a more
suitable alternative. In this paper, we develop new models and algorithms for
deep NMF using $\beta$-divergences. Subsequently, we apply these techniques to
the extraction of facial features, the identification of topics within document
collections, and the identification of materials within hyperspectral images.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FedJudge: Federated Legal Large Language Model. (arXiv:2309.08173v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08173">http://arxiv.org/abs/2309.08173</a></li>
<li>Code URL: https://github.com/yuelinan/fedjudge</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08173]] FedJudge: Federated Legal Large Language Model(http://arxiv.org/abs/2309.08173)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have gained prominence in the field of Legal
Intelligence, offering potential applications in assisting legal professionals
and laymen. However, the centralized training of these Legal LLMs raises data
privacy concerns, as legal data is distributed among various institutions
containing sensitive individual information. This paper addresses this
challenge by exploring the integration of Legal LLMs with Federated Learning
(FL) methodologies. By employing FL, Legal LLMs can be fine-tuned locally on
devices or clients, and their parameters are aggregated and distributed on a
central server, ensuring data privacy without directly sharing raw data.
However, computation and communication overheads hinder the full fine-tuning of
LLMs under the FL setting. Moreover, the distribution shift of legal data
reduces the effectiveness of FL methods. To this end, in this paper, we propose
the first Federated Legal Large Language Model (FedJudge) framework, which
fine-tunes Legal LLMs efficiently and effectively. Specifically, FedJudge
utilizes parameter-efficient fine-tuning methods to update only a few
additional parameters during the FL training. Besides, we explore the continual
learning methods to preserve the global model's important parameters when
training local clients to mitigate the problem of data shifts. Extensive
experimental results on three real-world datasets clearly validate the
effectiveness of FedJudge. Code is released at
https://github.com/yuelinan/FedJudge.
</p></li>
</ul>

<h3>Title: XFedHunter: An Explainable Federated Learning Framework for Advanced Persistent Threat Detection in SDN. (arXiv:2309.08485v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08485">http://arxiv.org/abs/2309.08485</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08485]] XFedHunter: An Explainable Federated Learning Framework for Advanced Persistent Threat Detection in SDN(http://arxiv.org/abs/2309.08485)</code></li>
<li>Summary: <p>Advanced Persistent Threat (APT) attacks are highly sophisticated and employ
a multitude of advanced methods and techniques to target organizations and
steal sensitive and confidential information. APT attacks consist of multiple
stages and have a defined strategy, utilizing new and innovative techniques and
technologies developed by hackers to evade security software monitoring. To
effectively protect against APTs, detecting and predicting APT indicators with
an explanation from Machine Learning (ML) prediction is crucial to reveal the
characteristics of attackers lurking in the network system. Meanwhile,
Federated Learning (FL) has emerged as a promising approach for building
intelligent applications without compromising privacy. This is particularly
important in cybersecurity, where sensitive data and high-quality labeling play
a critical role in constructing effective machine learning models for detecting
cyber threats. Therefore, this work proposes XFedHunter, an explainable
federated learning framework for APT detection in Software-Defined Networking
(SDN) leveraging local cyber threat knowledge from many training collaborators.
In XFedHunter, Graph Neural Network (GNN) and Deep Learning model are utilized
to reveal the malicious events effectively in the large number of normal ones
in the network system. The experimental results on NF-ToN-IoT and DARPA TCE3
datasets indicate that our framework can enhance the trust and accountability
of ML-based systems utilized for cybersecurity purposes without privacy
leakage.
</p></li>
</ul>

<h3>Title: FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08420">http://arxiv.org/abs/2309.08420</a></li>
<li>Code URL: https://github.com/orion-orion/FedDCSR</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08420]] FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning(http://arxiv.org/abs/2309.08420)</code></li>
<li>Summary: <p>Cross-domain Sequential Recommendation (CSR) which leverages user sequence
data from multiple domains has received extensive attention in recent years.
However, the existing CSR methods require sharing origin user data across
domains, which violates the General Data Protection Regulation (GDPR). Thus, it
is necessary to combine federated learning (FL) and CSR to fully utilize
knowledge from different domains while preserving data privacy. Nonetheless,
the sequence feature heterogeneity across different domains significantly
impacts the overall performance of FL. In this paper, we propose FedDCSR, a
novel federated cross-domain sequential recommendation framework via
disentangled representation learning. Specifically, to address the sequence
feature heterogeneity across domains, we introduce an approach called
inter-intra domain sequence representation disentanglement (SRD) to disentangle
the user sequence features into domain-shared and domain-exclusive features. In
addition, we design an intra domain contrastive infomax (CIM) strategy to learn
richer domain-exclusive features of users by performing data augmentation on
user sequences. Extensive experiments on three real-world scenarios demonstrate
that FedDCSR achieves significant improvements over existing baselines.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Toward responsible face datasets: modeling the distribution of a disentangled latent space for sampling face images from demographic groups. (arXiv:2309.08442v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08442">http://arxiv.org/abs/2309.08442</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08442]] Toward responsible face datasets: modeling the distribution of a disentangled latent space for sampling face images from demographic groups(http://arxiv.org/abs/2309.08442)</code></li>
<li>Summary: <p>Recently, it has been exposed that some modern facial recognition systems
could discriminate specific demographic groups and may lead to unfair attention
with respect to various facial attributes such as gender and origin. The main
reason are the biases inside datasets, unbalanced demographics, used to train
theses models. Unfortunately, collecting a large-scale balanced dataset with
respect to various demographics is impracticable.
</p>
<p>In this paper, we investigate as an alternative the generation of a balanced
and possibly bias-free synthetic dataset that could be used to train, to
regularize or to evaluate deep learning-based facial recognition models. We
propose to use a simple method for modeling and sampling a disentangled
projection of a StyleGAN latent space to generate any combination of
demographic groups (e.g. $hispanic-female$). Our experiments show that we can
synthesis any combination of demographic groups effectively and the identities
are different from the original training dataset. We also released the source
code.
</p></li>
</ul>

<h3>Title: Adaptive Priority Reweighing for Generalizing Fairness Improvement. (arXiv:2309.08375v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08375">http://arxiv.org/abs/2309.08375</a></li>
<li>Code URL: https://github.com/che2198/apw</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08375]] Adaptive Priority Reweighing for Generalizing Fairness Improvement(http://arxiv.org/abs/2309.08375)</code></li>
<li>Summary: <p>With the increasing penetration of machine learning applications in critical
decision-making areas, calls for algorithmic fairness are more prominent.
Although there have been various modalities to improve algorithmic fairness
through learning with fairness constraints, their performance does not
generalize well in the test set. A performance-promising fair algorithm with
better generalizability is needed. This paper proposes a novel adaptive
reweighing method to eliminate the impact of the distribution shifts between
training and test data on model generalizability. Most previous reweighing
methods propose to assign a unified weight for each (sub)group. Rather, our
method granularly models the distance from the sample predictions to the
decision boundary. Our adaptive reweighing method prioritizes samples closer to
the decision boundary and assigns a higher weight to improve the
generalizability of fair classifiers. Extensive experiments are performed to
validate the generalizability of our adaptive priority reweighing method for
accuracy and fairness measures (i.e., equal opportunity, equalized odds, and
demographic parity) in tabular benchmarks. We also highlight the performance of
our method in improving the fairness of language and vision models. The code is
available at https://github.com/che2198/APW.
</p></li>
</ul>

<h3>Title: Deep Reinforcement Learning for Efficient and Fair Allocation of Health Care Resources. (arXiv:2309.08560v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08560">http://arxiv.org/abs/2309.08560</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08560]] Deep Reinforcement Learning for Efficient and Fair Allocation of Health Care Resources(http://arxiv.org/abs/2309.08560)</code></li>
<li>Summary: <p>Scarcity of health care resources could result in the unavoidable consequence
of rationing. For example, ventilators are often limited in supply, especially
during public health emergencies or in resource-constrained health care
settings, such as amid the pandemic of COVID-19. Currently, there is no
universally accepted standard for health care resource allocation protocols,
resulting in different governments prioritizing patients based on various
criteria and heuristic-based protocols. In this study, we investigate the use
of reinforcement learning for critical care resource allocation policy
optimization to fairly and effectively ration resources. We propose a
transformer-based deep Q-network to integrate the disease progression of
individual patients and the interaction effects among patients during the
critical care resource allocation. We aim to improve both fairness of
allocation and overall patient outcomes. Our experiments demonstrate that our
method significantly reduces excess deaths and achieves a more equitable
distribution under different levels of ventilator shortage, when compared to
existing severity-based and comorbidity-based methods in use by different
governments. Our source code is included in the supplement and will be released
on Github upon publication.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Interpretability-Aware Vision Transformer. (arXiv:2309.08035v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08035">http://arxiv.org/abs/2309.08035</a></li>
<li>Code URL: https://github.com/qiangyao1988/ia-vit</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08035]] Interpretability-Aware Vision Transformer(http://arxiv.org/abs/2309.08035)</code></li>
<li>Summary: <p>Vision Transformers (ViTs) have become prominent models for solving various
vision tasks. However, the interpretability of ViTs has not kept pace with
their promising performance. While there has been a surge of interest in
developing {\it post hoc} solutions to explain ViTs' outputs, these methods do
not generalize to different downstream tasks and various transformer
architectures. Furthermore, if ViTs are not properly trained with the given
data and do not prioritize the region of interest, the {\it post hoc} methods
would be less effective. Instead of developing another {\it post hoc} approach,
we introduce a novel training procedure that inherently enhances model
interpretability. Our interpretability-aware ViT (IA-ViT) draws inspiration
from a fresh insight: both the class patch and image patches consistently
generate predicted distributions and attention maps. IA-ViT is composed of a
feature extractor, a predictor, and an interpreter, which are trained jointly
with an interpretability-aware training objective. Consequently, the
interpreter simulates the behavior of the predictor and provides a faithful
explanation through its single-head self-attention mechanism. Our comprehensive
experimental results demonstrate the effectiveness of IA-ViT in several image
classification tasks, with both qualitative and quantitative evaluations of
model performance and interpretability. Source code is available from:
https://github.com/qiangyao1988/IA-ViT.
</p></li>
</ul>

<h3>Title: Sparse Autoencoders Find Highly Interpretable Features in Language Models. (arXiv:2309.08600v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08600">http://arxiv.org/abs/2309.08600</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08600]] Sparse Autoencoders Find Highly Interpretable Features in Language Models(http://arxiv.org/abs/2309.08600)</code></li>
<li>Summary: <p>One of the roadblocks to a better understanding of neural networks' internals
is \textit{polysemanticity}, where neurons appear to activate in multiple,
semantically distinct contexts. Polysemanticity prevents us from identifying
concise, human-understandable explanations for what neural networks are doing
internally. One hypothesised cause of polysemanticity is
\textit{superposition}, where neural networks represent more features than they
have neurons by assigning features to an overcomplete set of directions in
activation space, rather than to individual neurons. Here, we attempt to
identify those directions, using sparse autoencoders to reconstruct the
internal activations of a language model. These autoencoders learn sets of
sparsely activating features that are more interpretable and monosemantic than
directions identified by alternative approaches, where interpretability is
measured by automated methods. Ablating these features enables precise model
editing, for example, by removing capabilities such as pronoun prediction,
while disrupting model behaviour less than prior techniques. This work
indicates that it is possible to resolve superposition in language models using
a scalable, unsupervised method. Our method may serve as a foundation for
future mechanistic interpretability work, which we hope will enable greater
model transparency and steerability.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Text-to-Image Models for Counterfactual Explanations: a Black-Box Approach. (arXiv:2309.07944v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07944">http://arxiv.org/abs/2309.07944</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07944]] Text-to-Image Models for Counterfactual Explanations: a Black-Box Approach(http://arxiv.org/abs/2309.07944)</code></li>
<li>Summary: <p>This paper addresses the challenge of generating Counterfactual Explanations
(CEs), involving the identification and modification of the fewest necessary
features to alter a classifier's prediction for a given image. Our proposed
method, Text-to-Image Models for Counterfactual Explanations (TIME), is a
black-box counterfactual technique based on distillation. Unlike previous
methods, this approach requires solely the image and its prediction, omitting
the need for the classifier's structure, parameters, or gradients. Before
generating the counterfactuals, TIME introduces two distinct biases into Stable
Diffusion in the form of textual embeddings: the context bias, associated with
the image's structure, and the class bias, linked to class-specific features
learned by the target classifier. After learning these biases, we find the
optimal latent code applying the classifier's predicted class token and
regenerate the image using the target embedding as conditioning, producing the
counterfactual explanation. Extensive empirical studies validate that TIME can
generate explanations of comparable effectiveness even when operating within a
black-box setting.
</p></li>
</ul>

<h3>Title: Viewpoint Textual Inversion: Unleashing Novel View Synthesis with Pretrained 2D Diffusion Models. (arXiv:2309.07986v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07986">http://arxiv.org/abs/2309.07986</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07986]] Viewpoint Textual Inversion: Unleashing Novel View Synthesis with Pretrained 2D Diffusion Models(http://arxiv.org/abs/2309.07986)</code></li>
<li>Summary: <p>Text-to-image diffusion models understand spatial relationship between
objects, but do they represent the true 3D structure of the world from only 2D
supervision? We demonstrate that yes, 3D knowledge is encoded in 2D image
diffusion models like Stable Diffusion, and we show that this structure can be
exploited for 3D vision tasks. Our method, Viewpoint Neural Textual Inversion
(ViewNeTI), controls the 3D viewpoint of objects in generated images from
frozen diffusion models. We train a small neural mapper to take camera
viewpoint parameters and predict text encoder latents; the latents then
condition the diffusion generation process to produce images with the desired
camera viewpoint.
</p>
<p>ViewNeTI naturally addresses Novel View Synthesis (NVS). By leveraging the
frozen diffusion model as a prior, we can solve NVS with very few input views;
we can even do single-view novel view synthesis. Our single-view NVS
predictions have good semantic details and photorealism compared to prior
methods. Our approach is well suited for modeling the uncertainty inherent in
sparse 3D vision problems because it can efficiently generate diverse samples.
Our view-control mechanism is general, and can even change the camera view in
images generated by user-defined prompts.
</p></li>
</ul>

<h3>Title: Detail Reinforcement Diffusion Model: Augmentation Fine-Grained Visual Categorization in Few-Shot Conditions. (arXiv:2309.08097v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08097">http://arxiv.org/abs/2309.08097</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08097]] Detail Reinforcement Diffusion Model: Augmentation Fine-Grained Visual Categorization in Few-Shot Conditions(http://arxiv.org/abs/2309.08097)</code></li>
<li>Summary: <p>The challenge in fine-grained visual categorization lies in how to explore
the subtle differences between different subclasses and achieve accurate
discrimination. Previous research has relied on large-scale annotated data and
pre-trained deep models to achieve the objective. However, when only a limited
amount of samples is available, similar methods may become less effective.
Diffusion models have been widely adopted in data augmentation due to their
outstanding diversity in data generation. However, the high level of detail
required for fine-grained images makes it challenging for existing methods to
be directly employed. To address this issue, we propose a novel approach termed
the detail reinforcement diffusion model~(DRDM), which leverages the rich
knowledge of large models for fine-grained data augmentation and comprises two
key components including discriminative semantic recombination (DSR) and
spatial knowledge reference~(SKR). Specifically, DSR is designed to extract
implicit similarity relationships from the labels and reconstruct the semantic
mapping between labels and instances, which enables better discrimination of
subtle differences between different subclasses. Furthermore, we introduce the
SKR module, which incorporates the distributions of different datasets as
references in the feature space. This allows the SKR to aggregate the
high-dimensional distribution of subclass features in few-shot FGVC tasks, thus
expanding the decision boundary. Through these two critical components, we
effectively utilize the knowledge from large models to address the issue of
data scarcity, resulting in improved performance for fine-grained visual
recognition tasks. Extensive experiments demonstrate the consistent performance
gain offered by our DRDM.
</p></li>
</ul>

<h3>Title: Cartoondiff: Training-free Cartoon Image Generation with Diffusion Transformer Models. (arXiv:2309.08251v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08251">http://arxiv.org/abs/2309.08251</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08251]] Cartoondiff: Training-free Cartoon Image Generation with Diffusion Transformer Models(http://arxiv.org/abs/2309.08251)</code></li>
<li>Summary: <p>Image cartoonization has attracted significant interest in the field of image
generation. However, most of the existing image cartoonization techniques
require re-training models using images of cartoon style. In this paper, we
present CartoonDiff, a novel training-free sampling approach which generates
image cartoonization using diffusion transformer models. Specifically, we
decompose the reverse process of diffusion models into the semantic generation
phase and the detail generation phase. Furthermore, we implement the image
cartoonization process by normalizing high-frequency signal of the noisy image
in specific denoising steps. CartoonDiff doesn't require any additional
reference images, complex model designs, or the tedious adjustment of multiple
parameters. Extensive experimental results show the powerful ability of our
CartoonDiff. The project page is available at: https://cartoondiff.github.io/
</p></li>
</ul>

<h3>Title: Unsupervised Disentangling of Facial Representations with 3D-aware Latent Diffusion Models. (arXiv:2309.08273v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08273">http://arxiv.org/abs/2309.08273</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08273]] Unsupervised Disentangling of Facial Representations with 3D-aware Latent Diffusion Models(http://arxiv.org/abs/2309.08273)</code></li>
<li>Summary: <p>Unsupervised learning of facial representations has gained increasing
attention for face understanding ability without heavily relying on large-scale
annotated datasets. However, it remains unsolved due to the coupling of facial
identities, expressions, and external factors like pose and light. Prior
methods primarily focus on 2D factors and pixel-level consistency, leading to
incomplete disentangling and suboptimal performance in downstream tasks. In
this paper, we propose LatentFace, a novel unsupervised disentangling framework
for facial expression and identity representation. We suggest the disentangling
problem should be performed in latent space and propose the solution using a
3D-ware latent diffusion model. First, we introduce a 3D-aware autoencoder to
encode face images into 3D latent embeddings. Second, we propose a novel
representation diffusion model (RDM) to disentangle 3D latent into facial
identity and expression. Consequently, our method achieves state-of-the-art
performance in facial expression recognition and face verification among
unsupervised facial representation learning models.
</p></li>
</ul>

<h3>Title: Large Intestine 3D Shape Refinement Using Point Diffusion Models for Digital Phantom Generation. (arXiv:2309.08289v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08289">http://arxiv.org/abs/2309.08289</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08289]] Large Intestine 3D Shape Refinement Using Point Diffusion Models for Digital Phantom Generation(http://arxiv.org/abs/2309.08289)</code></li>
<li>Summary: <p>Accurate 3D modeling of human organs plays a crucial role in building
computational phantoms for virtual imaging trials. However, generating
anatomically plausible reconstructions of organ surfaces from computed
tomography scans remains challenging for many structures in the human body.
This challenge is particularly evident when dealing with the large intestine.
In this study, we leverage recent advancements in geometric deep learning and
denoising diffusion probabilistic models to refine the segmentation results of
the large intestine. We begin by representing the organ as point clouds sampled
from the surface of the 3D segmentation mask. Subsequently, we employ a
hierarchical variational autoencoder to obtain global and local latent
representations of the organ's shape. We train two conditional denoising
diffusion models in the hierarchical latent space to perform shape refinement.
To further enhance our method, we incorporate a state-of-the-art surface
reconstruction model, allowing us to generate smooth meshes from the obtained
complete point clouds. Experimental results demonstrate the effectiveness of
our approach in capturing both the global distribution of the organ's shape and
its fine details. Our complete refinement pipeline demonstrates remarkable
enhancements in surface representation compared to the initial segmentation,
reducing the Chamfer distance by 70%, the Hausdorff distance by 32%, and the
Earth Mover's distance by 6%. By combining geometric deep learning, denoising
diffusion models, and advanced surface reconstruction techniques, our proposed
method offers a promising solution for accurately modeling the large
intestine's surface and can easily be extended to other anatomical structures.
</p></li>
</ul>

<h3>Title: Compositional Foundation Models for Hierarchical Planning. (arXiv:2309.08587v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08587">http://arxiv.org/abs/2309.08587</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08587]] Compositional Foundation Models for Hierarchical Planning(http://arxiv.org/abs/2309.08587)</code></li>
<li>Summary: <p>To make effective decisions in novel environments with long-horizon goals, it
is crucial to engage in hierarchical reasoning across spatial and temporal
scales. This entails planning abstract subgoal sequences, visually reasoning
about the underlying plans, and executing actions in accordance with the
devised plan through visual-motor control. We propose Compositional Foundation
Models for Hierarchical Planning (HiP), a foundation model which leverages
multiple expert foundation model trained on language, vision and action data
individually jointly together to solve long-horizon tasks. We use a large
language model to construct symbolic plans that are grounded in the environment
through a large video diffusion model. Generated video plans are then grounded
to visual-motor control, through an inverse dynamics model that infers actions
from generated videos. To enable effective reasoning within this hierarchy, we
enforce consistency between the models via iterative refinement. We illustrate
the efficacy and adaptability of our approach in three different long-horizon
table-top manipulation tasks.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Differentiable Resolution Compression and Alignment for Efficient Video Classification and Retrieval. (arXiv:2309.08167v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08167">http://arxiv.org/abs/2309.08167</a></li>
<li>Code URL: https://github.com/dun-research/drca</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08167]] Differentiable Resolution Compression and Alignment for Efficient Video Classification and Retrieval(http://arxiv.org/abs/2309.08167)</code></li>
<li>Summary: <p>Optimizing video inference efficiency has become increasingly important with
the growing demand for video analysis in various fields. Some existing methods
achieve high efficiency by explicit discard of spatial or temporal information,
which poses challenges in fast-changing and fine-grained scenarios. To address
these issues, we propose an efficient video representation network with
Differentiable Resolution Compression and Alignment mechanism, which compresses
non-essential information in the early stage of the network to reduce
computational costs while maintaining consistent temporal correlations.
Specifically, we leverage a Differentiable Context-aware Compression Module to
encode the saliency and non-saliency frame features, refining and updating the
features into a high-low resolution video sequence. To process the new
sequence, we introduce a new Resolution-Align Transformer Layer to capture
global temporal correlations among frame features with different resolutions,
while reducing spatial computation costs quadratically by utilizing fewer
spatial tokens in low-resolution non-saliency frames. The entire network can be
end-to-end optimized via the integration of the differentiable compression
module. Experimental results show that our method achieves the best trade-off
between efficiency and performance on near-duplicate video retrieval and
competitive results on dynamic video classification compared to
state-of-the-art methods. Code:https://github.com/dun-research/DRCA
</p></li>
</ul>

<h3>Title: Salient Object Detection in Optical Remote Sensing Images Driven by Transformer. (arXiv:2309.08206v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08206">http://arxiv.org/abs/2309.08206</a></li>
<li>Code URL: https://github.com/mathlee/gelenet</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08206]] Salient Object Detection in Optical Remote Sensing Images Driven by Transformer(http://arxiv.org/abs/2309.08206)</code></li>
<li>Summary: <p>Existing methods for Salient Object Detection in Optical Remote Sensing
Images (ORSI-SOD) mainly adopt Convolutional Neural Networks (CNNs) as the
backbone, such as VGG and ResNet. Since CNNs can only extract features within
certain receptive fields, most ORSI-SOD methods generally follow the
local-to-contextual paradigm. In this paper, we propose a novel Global
Extraction Local Exploration Network (GeleNet) for ORSI-SOD following the
global-to-local paradigm. Specifically, GeleNet first adopts a transformer
backbone to generate four-level feature embeddings with global long-range
dependencies. Then, GeleNet employs a Direction-aware Shuffle Weighted Spatial
Attention Module (D-SWSAM) and its simplified version (SWSAM) to enhance local
interactions, and a Knowledge Transfer Module (KTM) to further enhance
cross-level contextual interactions. D-SWSAM comprehensively perceives the
orientation information in the lowest-level features through directional
convolutions to adapt to various orientations of salient objects in ORSIs, and
effectively enhances the details of salient objects with an improved attention
mechanism. SWSAM discards the direction-aware part of D-SWSAM to focus on
localizing salient objects in the highest-level features. KTM models the
contextual correlation knowledge of two middle-level features of different
scales based on the self-attention mechanism, and transfers the knowledge to
the raw features to generate more discriminative features. Finally, a saliency
predictor is used to generate the saliency map based on the outputs of the
above three modules. Extensive experiments on three public datasets demonstrate
that the proposed GeleNet outperforms relevant state-of-the-art methods. The
code and results of our method are available at
https://github.com/MathLee/GeleNet.
</p></li>
</ul>

<h3>Title: UniST: Towards Unifying Saliency Transformer for Video Saliency Prediction and Detection. (arXiv:2309.08220v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08220">http://arxiv.org/abs/2309.08220</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08220]] UniST: Towards Unifying Saliency Transformer for Video Saliency Prediction and Detection(http://arxiv.org/abs/2309.08220)</code></li>
<li>Summary: <p>Video saliency prediction and detection are thriving research domains that
enable computers to simulate the distribution of visual attention akin to how
humans perceiving dynamic scenes. While many approaches have crafted
task-specific training paradigms for either video saliency prediction or video
salient object detection tasks, few attention has been devoted to devising a
generalized saliency modeling framework that seamlessly bridges both these
distinct tasks. In this study, we introduce the Unified Saliency Transformer
(UniST) framework, which comprehensively utilizes the essential attributes of
video saliency prediction and video salient object detection. In addition to
extracting representations of frame sequences, a saliency-aware transformer is
designed to learn the spatio-temporal representations at progressively
increased resolutions, while incorporating effective cross-scale saliency
information to produce a robust representation. Furthermore, a task-specific
decoder is proposed to perform the final prediction for each task. To the best
of our knowledge, this is the first work that explores designing a transformer
structure for both saliency modeling tasks. Convincible experiments demonstrate
that the proposed UniST achieves superior performance across seven challenging
benchmarks for two tasks, and significantly outperforms the other
state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Leveraging the Power of Data Augmentation for Transformer-based Tracking. (arXiv:2309.08264v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08264">http://arxiv.org/abs/2309.08264</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08264]] Leveraging the Power of Data Augmentation for Transformer-based Tracking(http://arxiv.org/abs/2309.08264)</code></li>
<li>Summary: <p>Due to long-distance correlation and powerful pretrained models,
transformer-based methods have initiated a breakthrough in visual object
tracking performance. Previous works focus on designing effective architectures
suited for tracking, but ignore that data augmentation is equally crucial for
training a well-performing model. In this paper, we first explore the impact of
general data augmentations on transformer-based trackers via systematic
experiments, and reveal the limited effectiveness of these common strategies.
Motivated by experimental observations, we then propose two data augmentation
methods customized for tracking. First, we optimize existing random cropping
via a dynamic search radius mechanism and simulation for boundary samples.
Second, we propose a token-level feature mixing augmentation strategy, which
enables the model against challenges like background interference. Extensive
experiments on two transformer-based trackers and six benchmarks demonstrate
the effectiveness and data efficiency of our methods, especially under
challenging settings, like one-shot tracking and small image resolutions.
</p></li>
</ul>

<h3>Title: M$^3$Net: Multilevel, Mixed and Multistage Attention Network for Salient Object Detection. (arXiv:2309.08365v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08365">http://arxiv.org/abs/2309.08365</a></li>
<li>Code URL: https://github.com/I2-Multimedia-Lab/M3Net</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08365]] M$^3$Net: Multilevel, Mixed and Multistage Attention Network for Salient Object Detection(http://arxiv.org/abs/2309.08365)</code></li>
<li>Summary: <p>Most existing salient object detection methods mostly use U-Net or feature
pyramid structure, which simply aggregates feature maps of different scales,
ignoring the uniqueness and interdependence of them and their respective
contributions to the final prediction. To overcome these, we propose the
M$^3$Net, i.e., the Multilevel, Mixed and Multistage attention network for
Salient Object Detection (SOD). Firstly, we propose Multiscale Interaction
Block which innovatively introduces the cross-attention approach to achieve the
interaction between multilevel features, allowing high-level features to guide
low-level feature learning and thus enhancing salient regions. Secondly,
considering the fact that previous Transformer based SOD methods locate salient
regions only using global self-attention while inevitably overlooking the
details of complex objects, we propose the Mixed Attention Block. This block
combines global self-attention and window self-attention, aiming at modeling
context at both global and local levels to further improve the accuracy of the
prediction map. Finally, we proposed a multilevel supervision strategy to
optimize the aggregated feature stage-by-stage. Experiments on six challenging
datasets demonstrate that the proposed M$^3$Net surpasses recent CNN and
Transformer-based SOD arts in terms of four metrics. Codes are available at
https://github.com/I2-Multimedia-Lab/M3Net.
</p></li>
</ul>

<h3>Title: SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels. (arXiv:2309.08513v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08513">http://arxiv.org/abs/2309.08513</a></li>
<li>Code URL: https://github.com/zhaohengyuan1/sct</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08513]] SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels(http://arxiv.org/abs/2309.08513)</code></li>
<li>Summary: <p>Pre-trained vision transformers have strong representation benefits to
various downstream tasks. Recently, many parameter-efficient fine-tuning (PEFT)
methods have been proposed, and their experiments demonstrate that tuning only
1% of extra parameters could surpass full fine-tuning in low-data resource
scenarios. However, these methods overlook the task-specific information when
fine-tuning diverse downstream tasks. In this paper, we propose a simple yet
effective method called "Salient Channel Tuning" (SCT) to leverage the
task-specific information by forwarding the model with the task images to
select partial channels in a feature map that enables us to tune only 1/8
channels leading to significantly lower parameter costs. Experiments outperform
full fine-tuning on 18 out of 19 tasks in the VTAB-1K benchmark by adding only
0.11M parameters of the ViT-B, which is 780$\times$ fewer than its full
fine-tuning counterpart. Furthermore, experiments on domain generalization and
few-shot learning surpass other PEFT methods with lower parameter costs,
demonstrating our proposed tuning technique's strong capability and
effectiveness in the low-data regime.
</p></li>
</ul>

<h3>Title: Replacing softmax with ReLU in Vision Transformers. (arXiv:2309.08586v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08586">http://arxiv.org/abs/2309.08586</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08586]] Replacing softmax with ReLU in Vision Transformers(http://arxiv.org/abs/2309.08586)</code></li>
<li>Summary: <p>Previous research observed accuracy degradation when replacing the attention
softmax with a point-wise activation such as ReLU. In the context of vision
transformers, we find that this degradation is mitigated when dividing by
sequence length. Our experiments training small to large vision transformers on
ImageNet-21k indicate that ReLU-attention can approach or match the performance
of softmax-attention in terms of scaling behavior as a function of compute.
</p></li>
</ul>

<h3>Title: Multilingual Sentence-Level Semantic Search using Meta-Distillation Learning. (arXiv:2309.08185v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08185">http://arxiv.org/abs/2309.08185</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08185]] Multilingual Sentence-Level Semantic Search using Meta-Distillation Learning(http://arxiv.org/abs/2309.08185)</code></li>
<li>Summary: <p>Multilingual semantic search is the task of retrieving relevant contents to a
query expressed in different language combinations. This requires a better
semantic understanding of the user's intent and its contextual meaning.
Multilingual semantic search is less explored and more challenging than its
monolingual or bilingual counterparts, due to the lack of multilingual parallel
resources for this task and the need to circumvent "language bias". In this
work, we propose an alignment approach: MAML-Align, specifically for
low-resource scenarios. Our approach leverages meta-distillation learning based
on MAML, an optimization-based Model-Agnostic Meta-Learner. MAML-Align distills
knowledge from a Teacher meta-transfer model T-MAML, specialized in
transferring from monolingual to bilingual semantic search, to a Student model
S-MAML, which meta-transfers from bilingual to multilingual semantic search. To
the best of our knowledge, we are the first to extend meta-distillation to a
multilingual search application. Our empirical results show that on top of a
strong baseline based on sentence transformers, our meta-distillation approach
boosts the gains provided by MAML and significantly outperforms naive
fine-tuning methods. Furthermore, multilingual meta-distillation learning
improves generalization even to unseen languages.
</p></li>
</ul>

<h3>Title: Structural Self-Supervised Objectives for Transformers. (arXiv:2309.08272v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08272">http://arxiv.org/abs/2309.08272</a></li>
<li>Code URL: https://github.com/lucadiliello/transformers-framework</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08272]] Structural Self-Supervised Objectives for Transformers(http://arxiv.org/abs/2309.08272)</code></li>
<li>Summary: <p>This thesis focuses on improving the pre-training of natural language models
using unsupervised raw data to make them more efficient and aligned with
downstream applications.
</p>
<p>In the first part, we introduce three alternative pre-training objectives to
BERT's Masked Language Modeling (MLM), namely Random Token Substitution (RTS),
Cluster-based Random Token Substitution (C-RTS), and Swapped Language Modeling
(SLM). These objectives involve token swapping instead of masking, with RTS and
C-RTS aiming to predict token originality and SLM predicting the original token
values. Results show that RTS and C-RTS require less pre-training time while
maintaining performance comparable to MLM. Surprisingly, SLM outperforms MLM on
certain tasks despite using the same computational budget.
</p>
<p>In the second part, we proposes self-supervised pre-training tasks that align
structurally with downstream applications, reducing the need for labeled data.
We use large corpora like Wikipedia and CC-News to train models to recognize if
text spans originate from the same paragraph or document in several ways. By
doing continuous pre-training, starting from existing models like RoBERTa,
ELECTRA, DeBERTa, BART, and T5, we demonstrate significant performance
improvements in tasks like Fact Verification, Answer Sentence Selection, and
Summarization. These improvements are especially pronounced when limited
annotation data is available. The proposed objectives also achieve
state-of-the-art results on various benchmark datasets, including FEVER (dev
set), ASNQ, WikiQA, and TREC-QA, as well as enhancing the quality of summaries.
Importantly, these techniques can be easily integrated with other methods
without altering the internal structure of Transformer models, making them
versatile for various NLP applications.
</p></li>
</ul>

<h3>Title: VulnSense: Efficient Vulnerability Detection in Ethereum Smart Contracts by Multimodal Learning with Graph Neural Network and Language Model. (arXiv:2309.08474v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08474">http://arxiv.org/abs/2309.08474</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08474]] VulnSense: Efficient Vulnerability Detection in Ethereum Smart Contracts by Multimodal Learning with Graph Neural Network and Language Model(http://arxiv.org/abs/2309.08474)</code></li>
<li>Summary: <p>This paper presents VulnSense framework, a comprehensive approach to
efficiently detect vulnerabilities in Ethereum smart contracts using a
multimodal learning approach on graph-based and natural language processing
(NLP) models. Our proposed framework combines three types of features from
smart contracts comprising source code, opcode sequences, and control flow
graph (CFG) extracted from bytecode. We employ Bidirectional Encoder
Representations from Transformers (BERT), Bidirectional Long Short-Term Memory
(BiLSTM) and Graph Neural Network (GNN) models to extract and analyze these
features. The final layer of our multimodal approach consists of a fully
connected layer used to predict vulnerabilities in Ethereum smart contracts.
Addressing limitations of existing vulnerability detection methods relying on
single-feature or single-model deep learning techniques, our method surpasses
accuracy and effectiveness constraints. We assess VulnSense using a collection
of 1.769 smart contracts derived from the combination of three datasets:
Curated, SolidiFI-Benchmark, and Smartbugs Wild. We then make a comparison with
various unimodal and multimodal learning techniques contributed by GNN, BiLSTM
and BERT architectures. The experimental outcomes demonstrate the superior
performance of our proposed approach, achieving an average accuracy of 77.96\%
across all three categories of vulnerable smart contracts.
</p></li>
</ul>

<h3>Title: A Data Source for Reasoning Embodied Agents. (arXiv:2309.07974v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07974">http://arxiv.org/abs/2309.07974</a></li>
<li>Code URL: https://github.com/facebookresearch/neuralmemory</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07974]] A Data Source for Reasoning Embodied Agents(http://arxiv.org/abs/2309.07974)</code></li>
<li>Summary: <p>Recent progress in using machine learning models for reasoning tasks has been
driven by novel model architectures, large-scale pre-training protocols, and
dedicated reasoning datasets for fine-tuning. In this work, to further pursue
these advances, we introduce a new data generator for machine reasoning that
integrates with an embodied agent. The generated data consists of templated
text queries and answers, matched with world-states encoded into a database.
The world-states are a result of both world dynamics and the actions of the
agent. We show the results of several baseline models on instantiations of
train sets. These include pre-trained language models fine-tuned on a
text-formatted representation of the database, and graph-structured
Transformers operating on a knowledge-graph representation of the database. We
find that these models can answer some questions about the world-state, but
struggle with others. These results hint at new research directions in
designing neural reasoning models and database representations. Code to
generate the data will be released at github.com/facebookresearch/neuralmemory
</p></li>
</ul>

<h3>Title: Folding Attention: Memory and Power Optimization for On-Device Transformer-based Streaming Speech Recognition. (arXiv:2309.07988v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07988">http://arxiv.org/abs/2309.07988</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07988]] Folding Attention: Memory and Power Optimization for On-Device Transformer-based Streaming Speech Recognition(http://arxiv.org/abs/2309.07988)</code></li>
<li>Summary: <p>Transformer-based models excel in speech recognition. Existing efforts to
optimize Transformer inference, typically for long-context applications, center
on simplifying attention score calculations. However, streaming speech
recognition models usually process a limited number of tokens each time, making
attention score calculation less of a bottleneck. Instead, the bottleneck lies
in the linear projection layers of multi-head attention and feedforward
networks, constituting a substantial portion of the model size and contributing
significantly to computation, memory, and power usage.
</p>
<p>To address this bottleneck, we propose folding attention, a technique
targeting these linear layers, significantly reducing model size and improving
memory and power efficiency. Experiments on on-device Transformer-based
streaming speech recognition models show that folding attention reduces model
size (and corresponding memory consumption) by up to 24% and power consumption
by up to 23%, all without compromising model accuracy or computation overhead.
</p></li>
</ul>

<h3>Title: Scaling Laws for Sparsely-Connected Foundation Models. (arXiv:2309.08520v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08520">http://arxiv.org/abs/2309.08520</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08520]] Scaling Laws for Sparsely-Connected Foundation Models(http://arxiv.org/abs/2309.08520)</code></li>
<li>Summary: <p>We explore the impact of parameter sparsity on the scaling behavior of
Transformers trained on massive datasets (i.e., "foundation models"), in both
vision and language domains. In this setting, we identify the first scaling law
describing the relationship between weight sparsity, number of non-zero
parameters, and amount of training data, which we validate empirically across
model and data scales; on ViT/JFT-4B and T5/C4. These results allow us to
characterize the "optimal sparsity", the sparsity level which yields the best
performance for a given effective model size and training budget. For a fixed
number of non-zero parameters, we identify that the optimal sparsity increases
with the amount of data used for training. We also extend our study to
different sparsity structures (such as the hardware-friendly n:m pattern) and
strategies (such as starting from a pretrained dense model). Our findings shed
light on the power and limitations of weight sparsity across various parameter
and computational settings, offering both theoretical understanding and
practical implications for leveraging sparsity towards computational efficiency
improvements.
</p></li>
</ul>

<h3>Title: Attention-Only Transformers and Implementing MLPs with Attention Heads. (arXiv:2309.08593v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08593">http://arxiv.org/abs/2309.08593</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08593]] Attention-Only Transformers and Implementing MLPs with Attention Heads(http://arxiv.org/abs/2309.08593)</code></li>
<li>Summary: <p>The transformer architecture is widely used in machine learning models and
consists of two alternating sublayers: attention heads and MLPs. We prove that
an MLP neuron can be implemented by a masked attention head with internal
dimension 1 so long as the MLP's activation function comes from a restricted
class including SiLU and close approximations of ReLU and GeLU. This allows one
to convert an MLP-and-attention transformer into an attention-only transformer
at the cost of greatly increasing the number of attention heads. We also prove
that attention heads can perform the components of an MLP (linear
transformations and activation functions) separately. Finally, we prove that
attention heads can encode arbitrary masking patterns in their weight matrices
to within arbitrarily small error.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Breathing New Life into 3D Assets with Generative Repainting. (arXiv:2309.08523v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08523">http://arxiv.org/abs/2309.08523</a></li>
<li>Code URL: https://github.com/toshas/remesh_isotropic_planar</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08523]] Breathing New Life into 3D Assets with Generative Repainting(http://arxiv.org/abs/2309.08523)</code></li>
<li>Summary: <p>Diffusion-based text-to-image models ignited immense attention from the
vision community, artists, and content creators. Broad adoption of these models
is due to significant improvement in the quality of generations and efficient
conditioning on various modalities, not just text. However, lifting the rich
generative priors of these 2D models into 3D is challenging. Recent works have
proposed various pipelines powered by the entanglement of diffusion models and
neural fields. We explore the power of pretrained 2D diffusion models and
standard 3D neural radiance fields as independent, standalone tools and
demonstrate their ability to work together in a non-learned fashion. Such
modularity has the intrinsic advantage of eased partial upgrades, which became
an important property in such a fast-paced domain. Our pipeline accepts any
legacy renderable geometry, such as textured or untextured meshes, orchestrates
the interaction between 2D generative refinement and 3D consistency enforcement
tools, and outputs a painted input geometry in several formats. We conduct a
large-scale study on a wide range of objects and categories from the
ShapeNetSem dataset and demonstrate the advantages of our approach, both
qualitatively and quantitatively. Project page:
https://www.obukhov.ai/repainting_3d_assets
</p></li>
</ul>

<h3>Title: Reward Engineering for Generating Semi-structured Explanation. (arXiv:2309.08347v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08347">http://arxiv.org/abs/2309.08347</a></li>
<li>Code URL: https://github.com/jiuzhouh/reward-engineering-for-generating-seg</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08347]] Reward Engineering for Generating Semi-structured Explanation(http://arxiv.org/abs/2309.08347)</code></li>
<li>Summary: <p>Semi-structured explanation depicts the implicit process of a reasoner with
an explicit representation. This explanation highlights how available
information in a specific query is supplemented with information a reasoner
produces from its internal weights towards generating an answer. Despite the
recent improvements in generative capabilities of language models, producing
structured explanations to verify model's true reasoning capabilities remains a
challenge. This issue is particularly pronounced for not-so-large LMs, as the
reasoner is expected to couple a sequential answer with a structured
explanation which embodies both the correct presentation and the correct
reasoning process. In this work, we first underscore the limitations of
supervised fine-tuning (SFT) in tackling this challenge, and then introduce a
carefully crafted reward engineering method in reinforcement learning (RL) to
better address this problem. We investigate multiple reward aggregation methods
and provide a detailed discussion which sheds light on the promising potential
of RL for future research. Our proposed reward on two semi-structured
explanation generation benchmarks (ExplaGraph and COPA-SSE) achieves new
state-of-the-art results.
</p></li>
</ul>

<h3>Title: Masked Generative Modeling with Enhanced Sampling Scheme. (arXiv:2309.07945v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07945">http://arxiv.org/abs/2309.07945</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07945]] Masked Generative Modeling with Enhanced Sampling Scheme(http://arxiv.org/abs/2309.07945)</code></li>
<li>Summary: <p>This paper presents a novel sampling scheme for masked non-autoregressive
generative modeling. We identify the limitations of TimeVQVAE, MaskGIT, and
Token-Critic in their sampling processes, and propose Enhanced Sampling Scheme
(ESS) to overcome these limitations. ESS explicitly ensures both sample
diversity and fidelity, and consists of three stages: Naive Iterative Decoding,
Critical Reverse Sampling, and Critical Resampling. ESS starts by sampling a
token set using the naive iterative decoding as proposed in MaskGIT, ensuring
sample diversity. Then, the token set undergoes the critical reverse sampling,
masking tokens leading to unrealistic samples. After that, critical resampling
reconstructs masked tokens until the final sampling step is reached to ensure
high fidelity. Critical resampling uses confidence scores obtained from a
self-Token-Critic to better measure the realism of sampled tokens, while
critical reverse sampling uses the structure of the quantized latent vector
space to discover unrealistic sample paths. We demonstrate significant
performance gains of ESS in both unconditional sampling and class-conditional
sampling using all the 128 datasets in the UCR Time Series archive.
</p></li>
</ul>

<h3>Title: An Automated Machine Learning Approach for Detecting Anomalous Peak Patterns in Time Series Data from a Research Watershed in the Northeastern United States Critical Zone. (arXiv:2309.07992v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07992">http://arxiv.org/abs/2309.07992</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07992]] An Automated Machine Learning Approach for Detecting Anomalous Peak Patterns in Time Series Data from a Research Watershed in the Northeastern United States Critical Zone(http://arxiv.org/abs/2309.07992)</code></li>
<li>Summary: <p>This paper presents an automated machine learning framework designed to
assist hydrologists in detecting anomalies in time series data generated by
sensors in a research watershed in the northeastern United States critical
zone. The framework specifically focuses on identifying peak-pattern anomalies,
which may arise from sensor malfunctions or natural phenomena. However, the use
of classification methods for anomaly detection poses challenges, such as the
requirement for labeled data as ground truth and the selection of the most
suitable deep learning model for the given task and dataset. To address these
challenges, our framework generates labeled datasets by injecting synthetic
peak patterns into synthetically generated time series data and incorporates an
automated hyperparameter optimization mechanism. This mechanism generates an
optimized model instance with the best architectural and training parameters
from a pool of five selected models, namely Temporal Convolutional Network
(TCN), InceptionTime, MiniRocket, Residual Networks (ResNet), and Long
Short-Term Memory (LSTM). The selection is based on the user's preferences
regarding anomaly detection accuracy and computational cost. The framework
employs Time-series Generative Adversarial Networks (TimeGAN) as the synthetic
dataset generator. The generated model instances are evaluated using a
combination of accuracy and computational cost metrics, including training time
and memory, during the anomaly detection process. Performance evaluation of the
framework was conducted using a dataset from a watershed, demonstrating
consistent selection of the most fitting model instance that satisfies the
user's preferences.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing. (arXiv:2309.08008v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08008">http://arxiv.org/abs/2309.08008</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08008]] An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing(http://arxiv.org/abs/2309.08008)</code></li>
<li>Summary: <p>Large language models (LLMs) have shown remarkable capabilities in Natural
Language Processing (NLP), especially in domains where labeled data is scarce
or expensive, such as clinical domain. However, to unlock the clinical
knowledge hidden in these LLMs, we need to design effective prompts that can
guide them to perform specific clinical NLP tasks without any task-specific
training data. This is known as in-context learning, which is an art and
science that requires understanding the strengths and weaknesses of different
LLMs and prompt engineering approaches. In this paper, we present a
comprehensive and systematic experimental study on prompt engineering for five
clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence
Extraction, Coreference Resolution, Medication Status Extraction, and
Medication Attribute Extraction. We assessed the prompts proposed in recent
literature, including simple prefix, simple cloze, chain of thought, and
anticipatory prompts, and introduced two new types of prompts, namely heuristic
prompting and ensemble prompting. We evaluated the performance of these prompts
on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted
zero-shot prompting with few-shot prompting, and provide novel insights and
guidelines for prompt engineering for LLMs in clinical NLP. To the best of our
knowledge, this is one of the first works on the empirical evaluation of
different prompt engineering approaches for clinical NLP in this era of
generative AI, and we hope that it will inspire and inform future research in
this area.
</p></li>
</ul>

<h3>Title: Investigating Gender Bias in News Summarization. (arXiv:2309.08047v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08047">http://arxiv.org/abs/2309.08047</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08047]] Investigating Gender Bias in News Summarization(http://arxiv.org/abs/2309.08047)</code></li>
<li>Summary: <p>Summarization is an important application of large language models (LLMs).
Most previous evaluation of summarization models has focused on their
performance in content selection, grammaticality and coherence. However, it is
well known that LLMs reproduce and reinforce harmful social biases. This raises
the question: Do these biases affect model outputs in a relatively constrained
setting like summarization?
</p>
<p>To help answer this question, we first motivate and introduce a number of
definitions for biased behaviours in summarization models, along with practical
measures to quantify them. Since we find biases inherent to the input document
can confound our analysis, we additionally propose a method to generate input
documents with carefully controlled demographic attributes. This allows us to
sidestep this issue, while still working with somewhat realistic input
documents.
</p>
<p>Finally, we apply our measures to summaries generated by both purpose-built
summarization models and general purpose chat models. We find that content
selection in single document summarization seems to be largely unaffected by
bias, while hallucinations exhibit evidence of biases propagating to generated
summaries.
</p></li>
</ul>

<h3>Title: Investigating the Applicability of Self-Assessment Tests for Personality Measurement of Large Language Models. (arXiv:2309.08163v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08163">http://arxiv.org/abs/2309.08163</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08163]] Investigating the Applicability of Self-Assessment Tests for Personality Measurement of Large Language Models(http://arxiv.org/abs/2309.08163)</code></li>
<li>Summary: <p>As large language models (LLM) evolve in their capabilities, various recent
studies have tried to quantify their behavior using psychological tools created
to study human behavior. One such example is the measurement of "personality"
of LLMs using personality self-assessment tests. In this paper, we take three
such studies on personality measurement of LLMs that use personality
self-assessment tests created to study human behavior. We use the prompts used
in these three different papers to measure the personality of the same LLM. We
find that all three prompts lead very different personality scores. This simple
test reveals that personality self-assessment scores in LLMs depend on the
subjective choice of the prompter. Since we don't know the ground truth value
of personality scores for LLMs as there is no correct answer to such questions,
there's no way of claiming if one prompt is more or less correct than the
other. We then introduce the property of option order symmetry for personality
measurement of LLMs. Since most of the self-assessment tests exist in the form
of multiple choice question (MCQ) questions, we argue that the scores should
also be robust to not just the prompt template but also the order in which the
options are presented. This test unsurprisingly reveals that the answers to the
self-assessment tests are not robust to the order of the options. These simple
tests, done on ChatGPT and Llama2 models show that self-assessment personality
tests created for humans are not appropriate for measuring personality in LLMs.
</p></li>
</ul>

<h3>Title: Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding. (arXiv:2309.08168v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08168">http://arxiv.org/abs/2309.08168</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08168]] Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding(http://arxiv.org/abs/2309.08168)</code></li>
<li>Summary: <p>We present a novel inference scheme, self-speculative decoding, for
accelerating Large Language Models (LLMs) without the need for an auxiliary
model. This approach is characterized by a two-stage process: drafting and
verification. The drafting stage generates draft tokens at a slightly lower
quality but more quickly, which is achieved by selectively skipping certain
intermediate layers during drafting Subsequently, the verification stage
employs the original LLM to validate those draft output tokens in one forward
pass. This process ensures the final output remains identical to that produced
by the unaltered LLM, thereby maintaining output quality. The proposed method
requires no additional neural network training and no extra memory footprint,
making it a plug-and-play and cost-effective solution for inference
acceleration. Benchmarks with LLaMA-2 and its fine-tuned models demonstrated a
speedup up to 1.73$\times$.
</p></li>
</ul>

<h3>Title: LASER: LLM Agent with State-Space Exploration for Web Navigation. (arXiv:2309.08172v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08172">http://arxiv.org/abs/2309.08172</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08172]] LASER: LLM Agent with State-Space Exploration for Web Navigation(http://arxiv.org/abs/2309.08172)</code></li>
<li>Summary: <p>Large language models (LLMs) have been successfully adapted for interactive
decision-making tasks like web navigation. While achieving decent performance,
previous methods implicitly assume a forward-only execution mode for the model,
where they only provide oracle trajectories as in-context examples to teach the
model how to reason in the interactive environment. Consequently, the model
could not handle more challenging scenarios not covered in the in-context
examples, e.g., mistakes, leading to sub-optimal performance. To address this
issue, we propose to model the interactive task as state space exploration,
where the LLM agent transitions among a pre-defined set of states by performing
actions to complete the task. This formulation enables flexible back-tracking,
allowing the model to easily recover from errors. We evaluate our proposed LLM
Agent with State-Space ExploRation (LASER) on the WebShop task. Experimental
results show that our LASER agent significantly outperforms previous methods
and closes the gap with human performance on the web navigation task.
</p></li>
</ul>

<h3>Title: Large Language Models for Failure Mode Classification: An Investigation. (arXiv:2309.08181v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08181">http://arxiv.org/abs/2309.08181</a></li>
<li>Code URL: https://github.com/nlp-tlp/chatgpt-fmc</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08181]] Large Language Models for Failure Mode Classification: An Investigation(http://arxiv.org/abs/2309.08181)</code></li>
<li>Summary: <p>In this paper we present the first investigation into the effectiveness of
Large Language Models (LLMs) for Failure Mode Classification (FMC). FMC, the
task of automatically labelling an observation with a corresponding failure
mode code, is a critical task in the maintenance domain as it reduces the need
for reliability engineers to spend their time manually analysing work orders.
We detail our approach to prompt engineering to enable an LLM to predict the
failure mode of a given observation using a restricted code list. We
demonstrate that the performance of a GPT-3.5 model (F1=0.80) fine-tuned on
annotated data is a significant improvement over a currently available text
classification model (F1=0.60) trained on the same annotated data set. The
fine-tuned model also outperforms the out-of-the box GPT-3.5 (F1=0.46). This
investigation reinforces the need for high quality fine-tuning data sets for
domain-specific tasks using LLMs.
</p></li>
</ul>

<h3>Title: Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level. (arXiv:2309.08182v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08182">http://arxiv.org/abs/2309.08182</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08182]] Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level(http://arxiv.org/abs/2309.08182)</code></li>
<li>Summary: <p>Our work demonstrates that large language model (LLM) pre-trained on texts
can not only solve pure math word problems, but also physics word
problems-problems to be solved by calculation and inference based on some prior
physical knowledge. We collect and annotate the first physics word problem
dataset-PhysQA, which contains over 1000 junior high school physics word
problems (on Kinematics, Mass&amp;Density, Mechanics, Heat, Electricity). Then we
use OpenAI' s GPT3.5 to generate the answer of these problems and found that
GPT3.5 could automatically solve 49.3% of the problems on zero-shot learning
and 73.2% on few-shot learning. This result show that by using similar problem
and its answer as prompt, LLM could solve elementary physics word problems
approaching human level. Besides automatically solving problems, GPT3.5 could
also summarize the knowledge or topic examined by the problem, generate the
relevant explanation, and synthesis new physics word problems according tothe
input problems.Our work is the first research on automatically solving,
explaining and generating physics word problems of multiple types and scenes,
and we gain an acceptable and state-of-art accuracy, which demonstrates the
potential of LLM's further application in the field of secondary education.
</p></li>
</ul>

<h3>Title: Unleashing Potential of Evidence in Knowledge-Intensive Dialogue Generation. (arXiv:2309.08380v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08380">http://arxiv.org/abs/2309.08380</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08380]] Unleashing Potential of Evidence in Knowledge-Intensive Dialogue Generation(http://arxiv.org/abs/2309.08380)</code></li>
<li>Summary: <p>Incorporating external knowledge into dialogue generation (KIDG) is crucial
for improving the correctness of response, where evidence fragments serve as
knowledgeable snippets supporting the factual dialogue replies. However,
introducing irrelevant content often adversely impacts reply quality and easily
leads to hallucinated responses. Prior work on evidence retrieval and
integration in dialogue systems falls short of fully leveraging existing
evidence since the model fails to locate useful fragments accurately and
overlooks hidden evidence labels within the KIDG dataset. To fully Unleash the
potential of evidence, we propose a framework to effectively incorporate
Evidence in knowledge-Intensive Dialogue Generation (u-EIDG). Specifically, we
introduce an automatic evidence generation framework that harnesses the power
of Large Language Models (LLMs) to mine reliable evidence veracity labels from
unlabeled data. By utilizing these evidence labels, we train a reliable
evidence indicator to effectively identify relevant evidence from retrieved
passages. Furthermore, we propose an evidence-augmented generator with an
evidence-focused attention mechanism, which allows the model to concentrate on
evidenced segments. Experimental results on MultiDoc2Dial demonstrate the
efficacy of evidential label augmentation and refined attention mechanisms in
improving model performance. Further analysis confirms that the proposed method
outperforms other baselines (+3~+5 points) regarding coherence and factual
consistency.
</p></li>
</ul>

<h3>Title: Advancing the Evaluation of Traditional Chinese Language Models: Towards a Comprehensive Benchmark Suite. (arXiv:2309.08448v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08448">http://arxiv.org/abs/2309.08448</a></li>
<li>Code URL: https://github.com/mtkresearch/mr-models</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08448]] Advancing the Evaluation of Traditional Chinese Language Models: Towards a Comprehensive Benchmark Suite(http://arxiv.org/abs/2309.08448)</code></li>
<li>Summary: <p>The evaluation of large language models is an essential task in the field of
language understanding and generation. As language models continue to advance,
the need for effective benchmarks to assess their performance has become
imperative. In the context of Traditional Chinese, there is a scarcity of
comprehensive and diverse benchmarks to evaluate the capabilities of language
models, despite the existence of certain benchmarks such as DRCD, TTQA, CMDQA,
and FGC dataset. To address this gap, we propose a novel set of benchmarks that
leverage existing English datasets and are tailored to evaluate language models
in Traditional Chinese. These benchmarks encompass a wide range of tasks,
including contextual question-answering, summarization, classification, and
table understanding. The proposed benchmarks offer a comprehensive evaluation
framework, enabling the assessment of language models' capabilities across
different tasks. In this paper, we evaluate the performance of GPT-3.5,
Taiwan-LLaMa-v1.0, and Model 7-C, our proprietary model, on these benchmarks.
The evaluation results highlight that our model, Model 7-C, achieves
performance comparable to GPT-3.5 with respect to a part of the evaluated
capabilities. In an effort to advance the evaluation of language models in
Traditional Chinese and stimulate further research in this field, we have
open-sourced our benchmark and opened the model for trial.
</p></li>
</ul>

<h3>Title: Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata. (arXiv:2309.08491v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08491">http://arxiv.org/abs/2309.08491</a></li>
<li>Code URL: https://github.com/bohuizhang/llmke</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08491]] Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata(http://arxiv.org/abs/2309.08491)</code></li>
<li>Summary: <p>In this work, we explore the use of Large Language Models (LLMs) for
knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge.
For this task, given subject and relation pairs sourced from Wikidata, we
utilize pre-trained LLMs to produce the relevant objects in string format and
link them to their respective Wikidata QIDs. We developed a pipeline using LLMs
for Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata
entity mapping. The method achieved a macro-averaged F1-score of 0.701 across
the properties, with the scores varying from 1.00 to 0.328. These results
demonstrate that the knowledge of LLMs varies significantly depending on the
domain and that further experimentation is required to determine the
circumstances under which LLMs can be used for automatic Knowledge Base (e.g.,
Wikidata) completion and correction. The investigation of the results also
suggests the promising contribution of LLMs in collaborative knowledge
engineering. LLMKE won Track 2 of the challenge. The implementation is
available at https://github.com/bohuizhang/LLMKE.
</p></li>
</ul>

<h3>Title: Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers. (arXiv:2309.08532v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08532">http://arxiv.org/abs/2309.08532</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08532]] Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers(http://arxiv.org/abs/2309.08532)</code></li>
<li>Summary: <p>Large Language Models (LLMs) excel in various tasks, but they rely on
carefully crafted prompts that often demand substantial human effort. To
automate this process, in this paper, we propose a novel framework for discrete
prompt optimization, called EvoPrompt, which borrows the idea of evolutionary
algorithms (EAs) as they exhibit good performance and fast convergence. To
enable EAs to work on discrete prompts, which are natural language expressions
that need to be coherent and human-readable, we connect LLMs with EAs. This
approach allows us to simultaneously leverage the powerful language processing
capabilities of LLMs and the efficient optimization performance of EAs.
Specifically, abstaining from any gradients or parameters, EvoPrompt starts
from a population of prompts and iteratively generates new prompts with LLMs
based on the evolutionary operators, improving the population based on the
development set. We optimize prompts for both closed- and open-source LLMs
including GPT-3.5 and Alpaca, on 9 datasets spanning language understanding and
generation tasks. EvoPrompt significantly outperforms human-engineered prompts
and existing methods for automatic prompt generation by up to 25% and 14%
respectively. Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs
creates synergies, which could inspire further research on the combination of
LLMs and conventional algorithms.
</p></li>
</ul>

<h3>Title: Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias between India and the West. (arXiv:2309.08573v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08573">http://arxiv.org/abs/2309.08573</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08573]] Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias between India and the West(http://arxiv.org/abs/2309.08573)</code></li>
<li>Summary: <p>Large Language Models (LLMs), now used daily by millions of users, can encode
societal biases, exposing their users to representational harms. A large body
of scholarship on LLM bias exists but it predominantly adopts a Western-centric
frame and attends comparatively less to bias levels and potential harms in the
Global South. In this paper, we quantify stereotypical bias in popular LLMs
according to an Indian-centric frame and compare bias levels between the Indian
and Western contexts. To do this, we develop a novel dataset which we call
Indian-BhED (Indian Bias Evaluation Dataset), containing stereotypical and
anti-stereotypical examples for caste and religion contexts. We find that the
majority of LLMs tested are strongly biased towards stereotypes in the Indian
context, especially as compared to the Western context. We finally investigate
Instruction Prompting as a simple intervention to mitigate such bias and find
that it significantly reduces both stereotypical and anti-stereotypical biases
in the majority of cases for GPT-3.5. The findings of this work highlight the
need for including more diverse voices when evaluating LLMs.
</p></li>
</ul>

<h3>Title: Chain-of-Thought Reasoning is a Policy Improvement Operator. (arXiv:2309.08589v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08589">http://arxiv.org/abs/2309.08589</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08589]] Chain-of-Thought Reasoning is a Policy Improvement Operator(http://arxiv.org/abs/2309.08589)</code></li>
<li>Summary: <p>Large language models have astounded the world with fascinating new
capabilities. However, they currently lack the ability to teach themselves new
skills, relying instead on being trained on large amounts of human-generated
data. We introduce SECToR (Self-Education via Chain-of-Thought Reasoning), a
proof-of-concept demonstration that language models can successfully teach
themselves new skills using chain-of-thought reasoning. Inspired by previous
work in both reinforcement learning (Silver et al., 2017) and human cognition
(Kahneman, 2011), SECToR first uses chain-of-thought reasoning to slowly think
its way through problems. SECToR then fine-tunes the model to generate those
same answers, this time without using chain-of-thought reasoning. Language
models trained via SECToR autonomously learn to add up to 29-digit numbers
without any access to any ground truth examples beyond an initial supervised
fine-tuning phase consisting only of numbers with 6 or fewer digits. Our
central hypothesis is that chain-of-thought reasoning can act as a policy
improvement operator, analogously to how Monte-Carlo Tree Search is used in
AlphaZero. We hope that this research can lead to new directions in which
language models can learn to teach themselves without the need for human
demonstrations.
</p></li>
</ul>

<h3>Title: Neural Machine Translation Models Can Learn to be Few-shot Learners. (arXiv:2309.08590v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08590">http://arxiv.org/abs/2309.08590</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08590]] Neural Machine Translation Models Can Learn to be Few-shot Learners(http://arxiv.org/abs/2309.08590)</code></li>
<li>Summary: <p>The emergent ability of Large Language Models to use a small number of
examples to learn to perform in novel domains and tasks, also called in-context
learning (ICL). In this work, we show that a much smaller model can be trained
to perform ICL by fine-tuning towards a specialized training objective,
exemplified on the task of domain adaptation for neural machine translation.
With this capacity for ICL, the model can take advantage of relevant few-shot
examples to adapt its output towards the domain. We compare the quality of this
domain adaptation to traditional supervised techniques and ICL with a
40B-parameter Large Language Model. Our approach allows efficient batch
inference on a mix of domains and outperforms state-of-the-art baselines in
terms of both translation quality and immediate adaptation rate, i.e. the
ability to reproduce a specific term after being shown a single example.
</p></li>
</ul>

<h3>Title: Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings. (arXiv:2309.08591v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08591">http://arxiv.org/abs/2309.08591</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08591]] Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings(http://arxiv.org/abs/2309.08591)</code></li>
<li>Summary: <p>Large language models (LLMs) are highly adept at question answering and
reasoning tasks, but when reasoning in situational context, human expectations
vary depending on the relevant cultural common ground. As human languages are
associated with diverse cultures, LLMs should also be culturally-diverse
reasoners. In this paper, we study the ability of a wide range of
state-of-the-art multilingual LLMs (mLLMs) to reason with proverbs and sayings
in a conversational context. Our experiments reveal that: (1) mLLMs 'knows'
limited proverbs and memorizing proverbs does not mean understanding them
within a conversational context; (2) mLLMs struggle to reason with figurative
proverbs and sayings, and when asked to select the wrong answer (instead of
asking it to select the correct answer); and (3) there is a "culture gap" in
mLLMs when reasoning about proverbs and sayings translated from other
languages. We construct and release our evaluation dataset MAPS (MulticultrAl
Proverbs and Sayings) for proverb understanding with conversational context for
six different languages.
</p></li>
</ul>

<h3>Title: "Merge Conflicts!" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs. (arXiv:2309.08594v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08594">http://arxiv.org/abs/2309.08594</a></li>
<li>Code URL: https://github.com/qiancheng0/ekd_impacts_pkg</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08594]] "Merge Conflicts!" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs(http://arxiv.org/abs/2309.08594)</code></li>
<li>Summary: <p>Large language models (LLMs) acquire extensive knowledge during pre-training,
known as their parametric knowledge. However, in order to remain up-to-date and
align with human instructions, LLMs inevitably require external knowledge
during their interactions with users. This raises a crucial question: How will
LLMs respond when external knowledge interferes with their parametric
knowledge? To investigate this question, we propose a framework that
systematically elicits LLM parametric knowledge and introduces external
knowledge. Specifically, we uncover the impacts by constructing a parametric
knowledge graph to reveal the different knowledge structures of LLMs, and
introduce external knowledge through distractors of varying degrees, methods,
positions, and formats. Our experiments on both black-box and open-source
models demonstrate that LLMs tend to produce responses that deviate from their
parametric knowledge, particularly when they encounter direct conflicts or
confounding changes of information within detailed contexts. We also find that
while LLMs are sensitive to the veracity of external knowledge, they can still
be distracted by unrelated information. These findings highlight the risk of
hallucination when integrating external knowledge, even indirectly, during
interactions with current LLMs. All the data and results are publicly
available.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Prompting Segmentation with Sound is Generalizable Audio-Visual Source Localizer. (arXiv:2309.07929v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07929">http://arxiv.org/abs/2309.07929</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07929]] Prompting Segmentation with Sound is Generalizable Audio-Visual Source Localizer(http://arxiv.org/abs/2309.07929)</code></li>
<li>Summary: <p>Never having seen an object and heard its sound simultaneously, can the model
still accurately localize its visual position from the input audio? In this
work, we concentrate on the Audio-Visual Localization and Segmentation tasks
but under the demanding zero-shot and few-shot scenarios. To achieve this goal,
different from existing approaches that mostly employ the
encoder-fusion-decoder paradigm to decode localization information from the
fused audio-visual feature, we introduce the encoder-prompt-decoder paradigm,
aiming to better fit the data scarcity and varying data distribution dilemmas
with the help of abundant knowledge from pre-trained models. Specifically, we
first propose to construct Semantic-aware Audio Prompt (SAP) to help the visual
foundation model focus on sounding objects, meanwhile, the semantic gap between
the visual and audio modalities is also encouraged to shrink. Then, we develop
a Correlation Adapter (ColA) to keep minimal training efforts as well as
maintain adequate knowledge of the visual foundation model. By equipping with
these means, extensive experiments demonstrate that this new paradigm
outperforms other fusion-based methods in both the unseen class and
cross-dataset settings. We hope that our work can further promote the
generalization study of Audio-Visual Localization and Segmentation in practical
application scenarios.
</p></li>
</ul>

<h3>Title: Temporal-aware Hierarchical Mask Classification for Video Semantic Segmentation. (arXiv:2309.08020v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08020">http://arxiv.org/abs/2309.08020</a></li>
<li>Code URL: https://github.com/zhaochongan/the-mask</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08020]] Temporal-aware Hierarchical Mask Classification for Video Semantic Segmentation(http://arxiv.org/abs/2309.08020)</code></li>
<li>Summary: <p>Modern approaches have proved the huge potential of addressing semantic
segmentation as a mask classification task which is widely used in
instance-level segmentation. This paradigm trains models by assigning part of
object queries to ground truths via conventional one-to-one matching. However,
we observe that the popular video semantic segmentation (VSS) dataset has
limited categories per video, meaning less than 10% of queries could be matched
to receive meaningful gradient updates during VSS training. This inefficiency
limits the full expressive potential of all queries.Thus, we present a novel
solution THE-Mask for VSS, which introduces temporal-aware hierarchical object
queries for the first time. Specifically, we propose to use a simple two-round
matching mechanism to involve more queries matched with minimal cost during
training while without any extra cost during inference. To support our
more-to-one assignment, in terms of the matching results, we further design a
hierarchical loss to train queries with their corresponding hierarchy of
primary or secondary. Moreover, to effectively capture temporal information
across frames, we propose a temporal aggregation decoder that fits seamlessly
into the mask-classification paradigm for VSS. Utilizing temporal-sensitive
multi-level queries, our method achieves state-of-the-art performance on the
latest challenging VSS benchmark VSPW without bells and whistles.
</p></li>
</ul>

<h3>Title: A Ground Segmentation Method Based on Point Cloud Map for Unstructured Roads. (arXiv:2309.08164v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08164">http://arxiv.org/abs/2309.08164</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08164]] A Ground Segmentation Method Based on Point Cloud Map for Unstructured Roads(http://arxiv.org/abs/2309.08164)</code></li>
<li>Summary: <p>Ground segmentation, as the basic task of unmanned intelligent perception,
provides an important support for the target detection task. Unstructured road
scenes represented by open-pit mines have irregular boundary lines and uneven
road surfaces, which lead to segmentation errors in current ground segmentation
methods. To solve this problem, a ground segmentation method based on point
cloud map is proposed, which involves three parts: region of interest
extraction, point cloud registration and background subtraction. Firstly,
establishing boundary semantic associations to obtain regions of interest in
unstructured roads. Secondly, establishing the location association between
point cloud map and the real-time point cloud of region of interest by
semantics information. Thirdly, establishing a background model based on
Gaussian distribution according to location association, and segments the
ground in real-time point cloud by the background substraction method.
Experimental results show that the correct segmentation rate of ground points
is 99.95%, and the running time is 26ms. Compared with state of the art ground
segmentation algorithm Patchwork++, the average accuracy of ground point
segmentation is increased by 7.43%, and the running time is increased by 17ms.
Furthermore, the proposed method is practically applied to unstructured road
scenarios represented by open pit mines.
</p></li>
</ul>

<h3>Title: One-stage Modality Distillation for Incomplete Multimodal Learning. (arXiv:2309.08204v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08204">http://arxiv.org/abs/2309.08204</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08204]] One-stage Modality Distillation for Incomplete Multimodal Learning(http://arxiv.org/abs/2309.08204)</code></li>
<li>Summary: <p>Learning based on multimodal data has attracted increasing interest recently.
While a variety of sensory modalities can be collected for training, not all of
them are always available in development scenarios, which raises the challenge
to infer with incomplete modality. To address this issue, this paper presents a
one-stage modality distillation framework that unifies the privileged knowledge
transfer and modality information fusion into a single optimization procedure
via multi-task learning. Compared with the conventional modality distillation
that performs them independently, this helps to capture the valuable
representation that can assist the final model inference directly.
Specifically, we propose the joint adaptation network for the modality transfer
task to preserve the privileged information. This addresses the representation
heterogeneity caused by input discrepancy via the joint distribution
adaptation. Then, we introduce the cross translation network for the modality
fusion task to aggregate the restored and available modality features. It
leverages the parameters-sharing strategy to capture the cross-modal cues
explicitly. Extensive experiments on RGB-D classification and segmentation
tasks demonstrate the proposed multimodal inheritance framework can overcome
the problem of incomplete modality input in various scenes and achieve
state-of-the-art performance.
</p></li>
</ul>

<h3>Title: T-UDA: Temporal Unsupervised Domain Adaptation in Sequential Point Clouds. (arXiv:2309.08302v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08302">http://arxiv.org/abs/2309.08302</a></li>
<li>Code URL: https://github.com/ctu-vras/t-uda</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08302]] T-UDA: Temporal Unsupervised Domain Adaptation in Sequential Point Clouds(http://arxiv.org/abs/2309.08302)</code></li>
<li>Summary: <p>Deep perception models have to reliably cope with an open-world setting of
domain shifts induced by different geographic regions, sensor properties,
mounting positions, and several other reasons. Since covering all domains with
annotated data is technically intractable due to the endless possible
variations, researchers focus on unsupervised domain adaptation (UDA) methods
that adapt models trained on one (source) domain with annotations available to
another (target) domain for which only unannotated data are available. Current
predominant methods either leverage semi-supervised approaches, e.g.,
teacher-student setup, or exploit privileged data, such as other sensor
modalities or temporal data consistency. We introduce a novel domain adaptation
method that leverages the best of both trends. Our approach combines input
data's temporal and cross-sensor geometric consistency with the mean teacher
method. Dubbed T-UDA for "temporal UDA", such a combination yields massive
performance gains for the task of 3D semantic segmentation of driving scenes.
Experiments are conducted on Waymo Open Dataset, nuScenes and SemanticKITTI,
for two popular 3D point cloud architectures, Cylinder3D and MinkowskiNet. Our
codes are publicly available at https://github.com/ctu-vras/T-UDA.
</p></li>
</ul>

<h3>Title: Double Domain Guided Real-Time Low-Light Image Enhancement for Ultra-High-Definition Transportation Surveillance. (arXiv:2309.08382v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08382">http://arxiv.org/abs/2309.08382</a></li>
<li>Code URL: https://github.com/qujx/ddnet</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08382]] Double Domain Guided Real-Time Low-Light Image Enhancement for Ultra-High-Definition Transportation Surveillance(http://arxiv.org/abs/2309.08382)</code></li>
<li>Summary: <p>Real-time transportation surveillance is an essential part of the intelligent
transportation system (ITS). However, images captured under low-light
conditions often suffer the poor visibility with types of degradation, such as
noise interference and vague edge features, etc. With the development of
imaging devices, the quality of the visual surveillance data is continually
increasing, like 2K and 4K, which has more strict requirements on the
efficiency of image processing. To satisfy the requirements on both enhancement
quality and computational speed, this paper proposes a double domain guided
real-time low-light image enhancement network (DDNet) for ultra-high-definition
(UHD) transportation surveillance. Specifically, we design an encoder-decoder
structure as the main architecture of the learning network. In particular, the
enhancement processing is divided into two subtasks (i.e., color enhancement
and gradient enhancement) via the proposed coarse enhancement module (CEM) and
LoG-based gradient enhancement module (GEM), which are embedded in the
encoder-decoder structure. It enables the network to enhance the color and edge
features simultaneously. Through the decomposition and reconstruction on both
color and gradient domains, our DDNet can restore the detailed feature
information concealed by the darkness with better visual quality and
efficiency. The evaluation experiments on standard and transportation-related
datasets demonstrate that our DDNet provides superior enhancement quality and
efficiency compared with the state-of-the-art methods. Besides, the object
detection and scene segmentation experiments indicate the practical benefits
for higher-level image analysis under low-light environments in ITS.
</p></li>
</ul>

<h3>Title: X-PDNet: Accurate Joint Plane Instance Segmentation and Monocular Depth Estimation with Cross-Task Distillation and Boundary Correction. (arXiv:2309.08424v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08424">http://arxiv.org/abs/2309.08424</a></li>
<li>Code URL: https://github.com/caodinhduc/x-pdnet-official</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08424]] X-PDNet: Accurate Joint Plane Instance Segmentation and Monocular Depth Estimation with Cross-Task Distillation and Boundary Correction(http://arxiv.org/abs/2309.08424)</code></li>
<li>Summary: <p>Segmentation of planar regions from a single RGB image is a particularly
important task in the perception of complex scenes. To utilize both visual and
geometric properties in images, recent approaches often formulate the problem
as a joint estimation of planar instances and dense depth through feature
fusion mechanisms and geometric constraint losses. Despite promising results,
these methods do not consider cross-task feature distillation and perform
poorly in boundary regions. To overcome these limitations, we propose X-PDNet,
a framework for the multitask learning of plane instance segmentation and depth
estimation with improvements in the following two aspects. Firstly, we
construct the cross-task distillation design which promotes early information
sharing between dual-tasks for specific task improvements. Secondly, we
highlight the current limitations of using the ground truth boundary to develop
boundary regression loss, and propose a novel method that exploits depth
information to support precise boundary region segmentation. Finally, we
manually annotate more than 3000 images from Stanford 2D-3D-Semantics dataset
and make available for evaluation of plane instance segmentation. Through the
experiments, our proposed methods prove the advantages, outperforming the
baseline with large improvement margins in the quantitative results on the
ScanNet and the Stanford 2D-3D-S dataset, demonstrating the effectiveness of
our proposals.
</p></li>
</ul>

<h3>Title: TreeLearn: A Comprehensive Deep Learning Method for Segmenting Individual Trees from Forest Point Clouds. (arXiv:2309.08471v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08471">http://arxiv.org/abs/2309.08471</a></li>
<li>Code URL: https://github.com/ecker-lab/treelearn</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08471]] TreeLearn: A Comprehensive Deep Learning Method for Segmenting Individual Trees from Forest Point Clouds(http://arxiv.org/abs/2309.08471)</code></li>
<li>Summary: <p>Laser-scanned point clouds of forests make it possible to extract valuable
information for forest management. To consider single trees, a forest point
cloud needs to be segmented into individual tree point clouds. Existing
segmentation methods are usually based on hand-crafted algorithms, such as
identifying trunks and growing trees from them, and face difficulties in dense
forests with overlapping tree crowns. In this study, we propose
\mbox{TreeLearn}, a deep learning-based approach for semantic and instance
segmentation of forest point clouds. Unlike previous methods, TreeLearn is
trained on already segmented point clouds in a data-driven manner, making it
less reliant on predefined features and algorithms. Additionally, we introduce
a new manually segmented benchmark forest dataset containing 156 full trees,
and 79 partial trees, that have been cleanly segmented by hand. This enables
the evaluation of instance segmentation performance going beyond just
evaluating the detection of individual trees. We trained TreeLearn on forest
point clouds of 6665 trees, labeled using the Lidar360 software. An evaluation
on the benchmark dataset shows that TreeLearn performs equally well or better
than the algorithm used to generate its training data. Furthermore, the
method's performance can be vastly improved by fine-tuning on the cleanly
labeled benchmark dataset. The TreeLearn code is availabe from
https://github.com/ecker-lab/TreeLearn. The data as well as trained models can
be found at https://doi.org/10.25625/VPMPID.
</p></li>
</ul>

<h3>Title: 3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images. (arXiv:2309.08481v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08481">http://arxiv.org/abs/2309.08481</a></li>
<li>Code URL: https://github.com/alinafdima/3dseg-mip-depth</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08481]] 3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images(http://arxiv.org/abs/2309.08481)</code></li>
<li>Summary: <p>Automated segmentation of the blood vessels in 3D volumes is an essential
step for the quantitative diagnosis and treatment of many vascular diseases. 3D
vessel segmentation is being actively investigated in existing works, mostly in
deep learning approaches. However, training 3D deep networks requires large
amounts of manual 3D annotations from experts, which are laborious to obtain.
This is especially the case for 3D vessel segmentation, as vessels are sparse
yet spread out over many slices and disconnected when visualized in 2D slices.
In this work, we propose a novel method to segment the 3D peripancreatic
arteries solely from one annotated 2D projection per training image with depth
supervision. We perform extensive experiments on the segmentation of
peripancreatic arteries on 3D contrast-enhanced CT images and demonstrate how
well we capture the rich depth information from 2D projections. We demonstrate
that by annotating a single, randomly chosen projection for each training
sample, we obtain comparable performance to annotating multiple 2D projections,
thereby reducing the annotation effort. Furthermore, by mapping the 2D labels
to the 3D space using depth information and incorporating this into training,
we almost close the performance gap between 3D supervision and 2D supervision.
Our code is available at: https://github.com/alinafdima/3Dseg-mip-depth.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
