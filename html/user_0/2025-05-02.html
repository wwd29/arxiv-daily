<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-05-02</h1>
<h3>Title: Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Shaun Baek, Shaun Esua-Mensah, Cyrus Tsui, Sejan Vigneswaralingam, Abdullah Alali, Michael Lu, Vasu Sharma, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00001">https://arxiv.org/abs/2505.00001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00001">https://arxiv.org/pdf/2505.00001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00001]] Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning(https://arxiv.org/abs/2505.00001)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are primarily trained on high-resource natural languages, limiting their effectiveness in low-resource settings and in tasks requiring deep logical reasoning. This research introduces Rosetta-PL, a benchmark designed to evaluate LLMs' logical reasoning and generalization capabilities in a controlled environment. We construct Rosetta-PL by translating a dataset of logical propositions from Lean into a custom logical language, which is then used to fine-tune an LLM (e.g., GPT-4o). Our experiments analyze the impact of the size of the dataset and the translation methodology on the performance of the model. Our results indicate that preserving logical relationships in the translation process significantly boosts precision, with accuracy plateauing beyond roughly 20,000 training samples. These insights provide valuable guidelines for optimizing LLM training in formal reasoning tasks and improving performance in various low-resource language applications.</li>
</ul>

<h3>Title: The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zizhou Liu, Ziwei Gong, Lin Ai, Zheng Hui, Run Chen, Colin Wayne Leach, Michelle R. Greene, Julia Hirschberg</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00003">https://arxiv.org/abs/2505.00003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00003">https://arxiv.org/pdf/2505.00003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00003]] The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs(https://arxiv.org/abs/2505.00003)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Psychological insights have long shaped pivotal NLP breakthroughs, including the cognitive underpinnings of attention mechanisms, formative reinforcement learning, and Theory of Mind-inspired social modeling. As Large Language Models (LLMs) continue to grow in scale and complexity, there is a rising consensus that psychology is essential for capturing human-like cognition, behavior, and interaction. This paper reviews how psychological theories can inform and enhance stages of LLM development, including data, pre-training, post-training, and evaluation\&application. Our survey integrates insights from cognitive, developmental, behavioral, social, personality psychology, and psycholinguistics. Our analysis highlights current trends and gaps in how psychological theories are applied. By examining both cross-domain connections and points of tension, we aim to bridge disciplinary divides and promote more thoughtful integration of psychology into future NLP research.</li>
</ul>

<h3>Title: LangVAE and LangSpace: Building and Probing for Language Model VAEs</h3>
<ul>
<li><strong>Authors: </strong>Danilo S. Carvalho, Yingji Zhang, Harriet Unsworth, Andr√© Freitas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00004">https://arxiv.org/abs/2505.00004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00004">https://arxiv.org/pdf/2505.00004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00004]] LangVAE and LangSpace: Building and Probing for Language Model VAEs(https://arxiv.org/abs/2505.00004)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present LangVAE, a novel framework for modular construction of variational autoencoders (VAEs) on top of pre-trained large language models (LLMs). Such language model VAEs can encode the knowledge of their pre-trained components into more compact and semantically disentangled representations. The representations obtained in this way can be analysed with the LangVAE companion framework: LangSpace, which implements a collection of probing methods, such as vector traversal and interpolation, disentanglement measures, and cluster visualisations. LangVAE and LangSpace offer a flexible, efficient and scalable way of building and analysing textual representations, with simple integration for models available on the HuggingFace Hub. Additionally, we conducted a set of experiments with different encoder and decoder combinations, as well as annotated inputs, revealing a wide range of interactions across architectural families and sizes w.r.t. generalisation and disentanglement. Our findings demonstrate a promising framework for systematising the experimentation and understanding of textual representations.</li>
</ul>

<h3>Title: A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyi Sun, Wen-Wai Yim, Ozlem Uzuner, Fei Xia, Meliha Yetisgen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00008">https://arxiv.org/abs/2505.00008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00008">https://arxiv.org/pdf/2505.00008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00008]] A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination(https://arxiv.org/abs/2505.00008)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Objective: This review aims to explore the potential and challenges of using Natural Language Processing (NLP) to detect, correct, and mitigate medically inaccurate information, including errors, misinformation, and hallucination. By unifying these concepts, the review emphasizes their shared methodological foundations and their distinct implications for healthcare. Our goal is to advance patient safety, improve public health communication, and support the development of more reliable and transparent NLP applications in healthcare. Methods: A scoping review was conducted following PRISMA guidelines, analyzing studies from 2020 to 2024 across five databases. Studies were selected based on their use of NLP to address medically inaccurate information and were categorized by topic, tasks, document types, datasets, models, and evaluation metrics. Results: NLP has shown potential in addressing medically inaccurate information on the following tasks: (1) error detection (2) error correction (3) misinformation detection (4) misinformation correction (5) hallucination detection (6) hallucination mitigation. However, challenges remain with data privacy, context dependency, and evaluation standards. Conclusion: This review highlights the advancements in applying NLP to tackle medically inaccurate information while underscoring the need to address persistent challenges. Future efforts should focus on developing real-world datasets, refining contextual methods, and improving hallucination management to ensure reliable and transparent healthcare applications.</li>
</ul>

<h3>Title: Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models</h3>
<ul>
<li><strong>Authors: </strong>Tri Nguyen, Lohith Srikanth Pentapalli, Magnus Sieverding, Laurah Turner, Seth Overla, Weibing Zheng, Chris Zhou, David Furniss, Danielle Weber, Michael Gharib, Matt Kelleher, Michael Shukis, Cameron Pawlik, Kelly Cohen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00010">https://arxiv.org/abs/2505.00010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00010">https://arxiv.org/pdf/2505.00010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00010]] Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models(https://arxiv.org/abs/2505.00010)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Jailbreaking in Large Language Models (LLMs) threatens their safe use in sensitive domains like education by allowing users to bypass ethical safeguards. This study focuses on detecting jailbreaks in 2-Sigma, a clinical education platform that simulates patient interactions using LLMs. We annotated over 2,300 prompts across 158 conversations using four linguistic variables shown to correlate strongly with jailbreak behavior. The extracted features were used to train several predictive models, including Decision Trees, Fuzzy Logic-based classifiers, Boosting methods, and Logistic Regression. Results show that feature-based predictive models consistently outperformed Prompt Engineering, with the Fuzzy Decision Tree achieving the best overall performance. Our findings demonstrate that linguistic-feature-based models are effective and explainable alternatives for jailbreak detection. We suggest future work explore hybrid frameworks that integrate prompt-based flexibility with rule-based robustness for real-time, spectrum-based jailbreak monitoring in educational LLMs.</li>
</ul>

<h3>Title: Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa</h3>
<ul>
<li><strong>Authors: </strong>Yoichi Takenaka</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00013">https://arxiv.org/abs/2505.00013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00013">https://arxiv.org/pdf/2505.00013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00013]] Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa(https://arxiv.org/abs/2505.00013)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Background Practical applications such as social media monitoring and customer-feedback analysis require accurate emotion detection for Japanese text, yet resource scarcity and class imbalance hinder model performance. Objective This study aims to build a high-accuracy model for predicting the presence or absence of eight Plutchik emotions in Japanese sentences. Methods Using the WRIME corpus, we transform reader-averaged intensity scores into binary labels and fine-tune four pre-trained language models (BERT, RoBERTa, DeBERTa-v3-base, DeBERTa-v3-large). For context, we also assess two large language models (TinySwallow-1.5B-Instruct and ChatGPT-4o). Accuracy and F1-score serve as evaluation metrics. Results DeBERTa-v3-large attains the best mean accuracy (0.860) and F1-score (0.662), outperforming all other models. It maintains robust F1 across both high-frequency emotions (e.g., Joy, Anticipation) and low-frequency emotions (e.g., Anger, Trust). The LLMs lag, with ChatGPT-4o and TinySwallow-1.5B-Instruct scoring 0.527 and 0.292 in mean F1, respectively. Conclusion The fine-tuned DeBERTa-v3-large model currently offers the most reliable solution for binary emotion classification in Japanese. We release this model as a pip-installable package (pip install deberta-emotion-predictor). Future work should augment data for rare emotions, reduce model size, and explore prompt engineering to improve LLM performance. This manuscript is under review for possible publication in New Generation Computing.</li>
</ul>

<h3>Title: Design and Application of Multimodal Large Language Model Based System for End to End Automation of Accident Dataset Generation</h3>
<ul>
<li><strong>Authors: </strong>MD Thamed Bin Zaman Chowdhury, Moazzem Hossain</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00015">https://arxiv.org/abs/2505.00015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00015">https://arxiv.org/pdf/2505.00015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00015]] Design and Application of Multimodal Large Language Model Based System for End to End Automation of Accident Dataset Generation(https://arxiv.org/abs/2505.00015)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>Road traffic accidents remain a major public safety and socio-economic issue in developing countries like Bangladesh. Existing accident data collection is largely manual, fragmented, and unreliable, resulting in underreporting and inconsistent records. This research proposes a fully automated system using Large Language Models (LLMs) and web scraping techniques to address these challenges. The pipeline consists of four components: automated web scraping code generation, news collection from online sources, accident news classification with structured data extraction, and duplicate removal. The system uses the multimodal generative LLM Gemini-2.0-Flash for seamless automation. The code generation module classifies webpages into pagination, dynamic, or infinite scrolling categories and generates suitable Python scripts for scraping. LLMs also classify and extract key accident information such as date, time, location, fatalities, injuries, road type, vehicle types, and pedestrian involvement. A deduplication algorithm ensures data integrity by removing duplicate reports. The system scraped 14 major Bangladeshi news sites over 111 days (Oct 1, 2024 - Jan 20, 2025), processing over 15,000 news articles and identifying 705 unique accidents. The code generation module achieved 91.3% calibration and 80% validation accuracy. Chittagong reported the highest number of accidents (80), fatalities (70), and injuries (115), followed by Dhaka, Faridpur, Gazipur, and Cox's Bazar. Peak accident times were morning (8-9 AM), noon (12-1 PM), and evening (6-7 PM). A public repository was also developed with usage instructions. This study demonstrates the viability of an LLM-powered, scalable system for accurate, low-effort accident data collection, providing a foundation for data-driven road safety policymaking in Bangladesh.</li>
</ul>

<h3>Title: Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Josefa Lia Stoisser, Marc Boubnovski Martell, Julien Fauqueur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00016">https://arxiv.org/abs/2505.00016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00016">https://arxiv.org/pdf/2505.00016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00016]] Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning(https://arxiv.org/abs/2505.00016)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>This work reframes the Text-to-SQL task as a pathway for teaching large language models (LLMs) to reason over and manipulate tabular data--moving beyond the traditional focus on query generation. We propose a two-stage framework that leverages SQL supervision to develop transferable table reasoning capabilities. First, we synthesize detailed chain-of-thought (CoT) traces from real-world SQL queries, providing step-by-step, clause-level supervision that teaches the model how to traverse, filter, and aggregate table fields. Second, we introduce a Group Relative Policy Optimization (GRPO) reinforcement learning objective that connects SQL execution accuracy to generalizable reasoning by encouraging steps that extend beyond task-specific syntax and transfer across datasets. Empirically, our approach improves performance on standard Text-to-SQL benchmarks and achieves substantial gains on reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced generalization and interpretability. Specifically, the distilled-quantized LLaMA model achieved a 20\% increase in accuracy when trained on Text-to-SQL tasks, while Qwen achieved a 5\% increase. These results suggest that SQL can serve not only as a target formalism but also as an effective scaffold for learning robust, transferable reasoning over structured data.</li>
</ul>

<h3>Title: ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation</h3>
<ul>
<li><strong>Authors: </strong>Dezheng Han, Yibin Jia, Ruxiao Chen, Wenjie Han, Shuaishuai Guo, Jianbo Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00017">https://arxiv.org/abs/2505.00017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00017">https://arxiv.org/pdf/2505.00017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00017]] ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation(https://arxiv.org/abs/2505.00017)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To enable precise and fully automated cell type annotation with large language models (LLMs), we developed a graph structured feature marker database to retrieve entities linked to differential genes for cell reconstruction. We further designed a multi task workflow to optimize the annotation process. Compared to general purpose LLMs, our method improves human evaluation scores by up to 0.21 and semantic similarity by 6.1% across 11 tissue types, while more closely aligning with the cognitive logic of manual annotation.</li>
</ul>

<h3>Title: An Empirical Study on Prompt Compression for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zheng Zhang, Jinyi Li, Yihuai Lan, Xiang Wang, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00019">https://arxiv.org/abs/2505.00019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00019">https://arxiv.org/pdf/2505.00019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00019]] An Empirical Study on Prompt Compression for Large Language Models(https://arxiv.org/abs/2505.00019)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompt engineering enables Large Language Models (LLMs) to perform a variety of tasks. However, lengthy prompts significantly increase computational complexity and economic costs. To address this issue, we study six prompt compression methods for LLMs, aiming to reduce prompt length while maintaining LLM response quality. In this paper, we present a comprehensive analysis covering aspects such as generation performance, model hallucinations, efficacy in multimodal tasks, word omission analysis, and more. We evaluate these methods across 13 datasets, including news, scientific articles, commonsense QA, math QA, long-context QA, and VQA datasets. Our experiments reveal that prompt compression has a greater impact on LLM performance in long contexts compared to short ones. In the Longbench evaluation, moderate compression even enhances LLM performance. Our code and data is available at this https URL.</li>
</ul>

<h3>Title: Beyond Public Access in LLM Pre-Training Data</h3>
<ul>
<li><strong>Authors: </strong>Sruly Rosenblat, Tim O'Reilly, Ilan Strauss</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00020">https://arxiv.org/abs/2505.00020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00020">https://arxiv.org/pdf/2505.00020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00020]] Beyond Public Access in LLM Pre-Training Data(https://arxiv.org/abs/2505.00020)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Using a legally obtained dataset of 34 copyrighted O'Reilly Media books, we apply the DE-COP membership inference attack method to investigate whether OpenAI's large language models were trained on copyrighted content without consent. Our AUROC scores show that GPT-4o, OpenAI's more recent and capable model, demonstrates strong recognition of paywalled O'Reilly book content (AUROC = 82\%), compared to OpenAI's earlier model GPT-3.5 Turbo. In contrast, GPT-3.5 Turbo shows greater relative recognition of publicly accessible O'Reilly book samples. GPT-4o Mini, as a much smaller model, shows no knowledge of public or non-public O'Reilly Media content when tested (AUROC $\approx$ 50\%). Testing multiple models, with the same cutoff date, helps us account for potential language shifts over time that might bias our findings. These results highlight the urgent need for increased corporate transparency regarding pre-training data sources as a means to develop formal licensing frameworks for AI content training</li>
</ul>

<h3>Title: Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss</h3>
<ul>
<li><strong>Authors: </strong>Zhuoang Cai, Zhenghao Li, Yang Liu, Liyuan Guo, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00021">https://arxiv.org/abs/2505.00021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00021">https://arxiv.org/pdf/2505.00021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00021]] Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss(https://arxiv.org/abs/2505.00021)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Classification tasks often suffer from imbal- anced data distribution, which presents chal- lenges in food hazard detection due to severe class imbalances, short and unstructured text, and overlapping semantic categories. In this paper, we present our system for SemEval- 2025 Task 9: Food Hazard Detection, which ad- dresses these issues by applying data augmenta- tion techniques to improve classification perfor- mance. We utilize transformer-based models, BERT and RoBERTa, as backbone classifiers and explore various data balancing strategies, including random oversampling, Easy Data Augmentation (EDA), and focal loss. Our ex- periments show that EDA effectively mitigates class imbalance, leading to significant improve- ments in accuracy and F1 scores. Furthermore, combining focal loss with oversampling and EDA further enhances model robustness, par- ticularly for hard-to-classify examples. These findings contribute to the development of more effective NLP-based classification models for food hazard detection.</li>
</ul>

<h3>Title: Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation</h3>
<ul>
<li><strong>Authors: </strong>Thomas F Burns, Letitia Parcalabescu, Stephan W√§ldchen, Michael Barlow, Gregor Ziegltrum, Volker Stampa, Bastian Harren, Bj√∂rn Deiseroth</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00022">https://arxiv.org/abs/2505.00022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00022">https://arxiv.org/pdf/2505.00022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00022]] Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation(https://arxiv.org/abs/2505.00022)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Scaling data quantity is essential for large language models (LLMs), yet recent findings show that data quality can significantly boost performance and training efficiency. We introduce a German-language dataset curation pipeline that combines heuristic and model-based filtering techniques with synthetic data generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a large-scale German pre-training dataset which draws from: (1) Common Crawl web data, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual, organic web data. We evaluate our dataset by pre-training both a 1B Llama-style model and an 8B tokenizer-free hierarchical autoregressive transformer (HAT). A comparison on German-language benchmarks, including MMMLU, shows significant performance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage holds at the 8B scale even when FineWeb2 is enriched by human-curated high-quality data sources such as Wikipedia. Our findings support the growing body of evidence that model-based data curation and synthetic data generation can significantly enhance LLM pre-training datasets.</li>
</ul>

<h3>Title: Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, Guilin Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00024">https://arxiv.org/abs/2505.00024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00024">https://arxiv.org/pdf/2505.00024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00024]] Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning(https://arxiv.org/abs/2505.00024)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Enabling large language models with external tools has become a pivotal strategy for extending their functionality beyond text generation tasks. Prior work typically enhances tool-use abilities by either applying supervised fine-tuning (SFT) to enforce tool-call correctness or distilling reasoning traces from stronger models for SFT. However, both approaches fall short, either omitting reasoning entirely or producing imitative reasoning that limits generalization. Inspired by the success of DeepSeek-R1 in eliciting reasoning through rule-based reinforcement learning, we develop the Nemotron-Research-Tool-N1 series of tool-using language models using a similar training paradigm. Instead of restrictively supervising intermediate reasoning traces distilled from stronger models, Nemotron-Research-Tool-N1 is optimized with a binary reward that evaluates only the structural validity and functional correctness of tool invocations. This lightweight supervision allows the model to autonomously internalize reasoning strategies, without the need for annotated reasoning trajectories. Experiments on the BFCL and API-Bank benchmarks show that Nemotron-Research-Tool-N1-7B and Nemotron-Research-Tool-N1-14B, built on Qwen-2.5-7B/14B-Instruct, achieve state-of-the-art results, outperforming GPT-4o on both evaluations.</li>
</ul>

<h3>Title: A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1</h3>
<ul>
<li><strong>Authors: </strong>Mingda Zhang, Jianglong Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00025">https://arxiv.org/abs/2505.00025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00025">https://arxiv.org/pdf/2505.00025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00025]] A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1(https://arxiv.org/abs/2505.00025)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, despite foundation models like DeepSeek-R1 and ChatGPT demonstrating significant capabilities in general tasks, professional knowledge barriers, computational resource requirements, and deployment environment limitations have severely hindered their application in actual medical scenarios. Addressing these challenges, this paper proposes an efficient lightweight medical vertical large language model architecture method, systematically solving the lightweight problem of medical large models from three dimensions: knowledge acquisition, model compression, and computational optimization. At the knowledge acquisition level, a knowledge transfer pipeline is designed from the fine-tuned DeepSeek-R1-Distill-70B teacher model to the DeepSeek-R1-Distill-7B student model, and Low-Rank Adaptation (LoRA) technology is adopted to precisely adjust key attention layers. At the model compression level, compression techniques including 4-bit weight quantization are implemented while preserving the core representation ability for medical reasoning. At the computational optimization level, inference optimization techniques such as Flash Attention acceleration and continuous batching are integrated, and a professional prompt template system is constructed to adapt to different types of medical problems. Experimental results on medical question-answering datasets show that the method proposed in this paper maintains professional accuracy while reducing memory consumption by 64.7\% and inference latency by 12.4\%, providing an effective solution for the application of medical large models in resource-constrained environments such as edge computing devices.</li>
</ul>

<h3>Title: Theory of Mind in Large Language Models: Assessment and Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Ruirui Chen, Weifeng Jiang, Chengwei Qin, Cheston Tan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00026">https://arxiv.org/abs/2505.00026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00026">https://arxiv.org/pdf/2505.00026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00026]] Theory of Mind in Large Language Models: Assessment and Enhancement(https://arxiv.org/abs/2505.00026)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Theory of Mind (ToM)-the ability to infer and reason about others' mental states-is fundamental to human social intelligence. As Large Language Models (LLMs) become increasingly integrated into daily life, it is crucial to assess and enhance their capacity to interpret and respond to human mental states. In this paper, we review LLMs' ToM capabilities by examining both evaluation benchmarks and the strategies designed to improve them. We focus on widely adopted story-based benchmarks and provide an in-depth analysis of methods aimed at enhancing ToM in LLMs. Furthermore, we outline promising future research directions informed by recent benchmarks and state-of-the-art approaches. Our survey serves as a valuable resource for researchers interested in advancing LLMs' ToM capabilities.</li>
</ul>

<h3>Title: Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Pengchao Feng, Ziyang Ma, Wenxi Chen, Yao Li, Sheng Wang, Kai Yu, Xie Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00028">https://arxiv.org/abs/2505.00028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00028">https://arxiv.org/pdf/2505.00028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00028]] Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation(https://arxiv.org/abs/2505.00028)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, end-to-end speech-to-speech (S2S) dialogue systems have garnered increasing research attention due to their advantages over traditional cascaded systems, including achieving lower latency and more natural integration of nonverbal cues such as emotion and speaker identity. However, these end-to-end systems face key challenges, particularly in incorporating external knowledge, a capability commonly addressed by Retrieval-Augmented Generation (RAG) in text-based large language models (LLMs). The core difficulty lies in the modality gap between input speech and retrieved textual knowledge, which hinders effective integration. To address this issue, we propose a novel end-to-end RAG framework that directly retrieves relevant textual knowledge from speech queries, eliminating the need for intermediate speech-to-text conversion via techniques like ASR. Experimental results demonstrate that our method significantly improves the performance of end-to-end S2S dialogue systems while achieving higher retrieval efficiency. Although the overall performance still lags behind cascaded models, our framework offers a promising direction for enhancing knowledge integration in end-to-end S2S systems. We will release the code and dataset to support reproducibility and promote further research in this area.</li>
</ul>

<h3>Title: Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting</h3>
<ul>
<li><strong>Authors: </strong>Yijie Hong, Xiaofei Yin, Xinzhong Wang, Yi Tu, Ya Guo, Sufeng Duan, Weiqiang Wang, Lingyong Fang, Depeng Wang, Huijia Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00029">https://arxiv.org/abs/2505.00029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00029">https://arxiv.org/pdf/2505.00029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00029]] Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting(https://arxiv.org/abs/2505.00029)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Large Vision Language Models have demonstrated impressive versatile capabilities through extensive multimodal pre-training, but face significant limitations when incorporating specialized knowledge domains beyond their training distribution. These models struggle with a fundamental dilemma: direct adaptation approaches that inject domain-specific knowledge often trigger catastrophic forgetting of foundational visual-linguistic abilities. We introduce Structured Dialogue Fine-Tuning (SDFT), an effective approach that effectively injects domain-specific knowledge while minimizing catastrophic forgetting. Drawing inspiration from supervised fine-tuning in LLMs and subject-driven personalization in text-to-image diffusion models, our method employs a three-phase dialogue structure: Foundation Preservation reinforces pre-trained visual-linguistic alignment through caption tasks; Contrastive Disambiguation introduces carefully designed counterfactual examples to maintain semantic boundaries; and Knowledge Specialization embeds specialized information through chain-of-thought reasoning. Experimental results across multiple domains confirm SDFT's effectiveness in balancing specialized knowledge acquisition with general capability retention. Our key contributions include a data-centric dialogue template that balances foundational alignment with targeted knowledge integration, a weighted multi-turn supervision framework, and comprehensive evaluation across diverse knowledge types.</li>
</ul>

<h3>Title: Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving</h3>
<ul>
<li><strong>Authors: </strong>Jin Zhang, Flood Sung, Zhilin Yang, Yang Gao, Chongjie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00031">https://arxiv.org/abs/2505.00031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00031">https://arxiv.org/pdf/2505.00031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00031]] Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving(https://arxiv.org/abs/2505.00031)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the field of large language model (LLM) post-training, the effectiveness of utilizing synthetic data generated by the LLM itself has been well-presented. However, a key question remains unaddressed: what essential information should such self-generated data encapsulate? Existing approaches only produce step-by-step problem solutions, and fail to capture the abstract meta-knowledge necessary for generalization across similar problems. Drawing insights from cognitive science, where humans employ high-level abstraction to simplify complex problems before delving into specifics, we introduce a novel self-training algorithm: LEarning to Plan before Answering (LEPA). LEPA trains the LLM to formulate anticipatory plans, which serve as abstract meta-knowledge for problem-solving, before engaging with the intricacies of problems. This approach not only outlines the solution generation path but also shields the LLM from the distraction of irrelevant details. During data generation, LEPA first crafts an anticipatory plan based on the problem, and then generates a solution that aligns with both the plan and the problem. LEPA refines the plan through self-reflection, aiming to acquire plans that are instrumental in yielding correct solutions. During model optimization, the LLM is trained to predict both the refined plans and the corresponding solutions. By efficiently extracting and utilizing the anticipatory plans, LEPA demonstrates remarkable superiority over conventional algorithms on various challenging natural language reasoning benchmarks.</li>
</ul>

<h3>Title: MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Yuyang Sha, Hongxin Pan, Wei Xu, Weiyu Meng, Gang Luo, Xinyu Du, Xiaobing Zhai, Henry H. Y. Tong, Caijuan Shi, Kefeng Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00032">https://arxiv.org/abs/2505.00032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00032">https://arxiv.org/pdf/2505.00032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00032]] MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis(https://arxiv.org/abs/2505.00032)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Major depressive disorder (MDD) impacts more than 300 million people worldwide, highlighting a significant public health issue. However, the uneven distribution of medical resources and the complexity of diagnostic methods have resulted in inadequate attention to this disorder in numerous countries and regions. This paper introduces a high-performance MDD diagnosis tool named MDD-LLM, an AI-driven framework that utilizes fine-tuned large language models (LLMs) and extensive real-world samples to tackle challenges in MDD diagnosis. Therefore, we select 274,348 individual information from the UK Biobank cohort to train and evaluate the proposed method. Specifically, we select 274,348 individual records from the UK Biobank cohort and design a tabular data transformation method to create a large corpus for training and evaluating the proposed approach. To illustrate the advantages of MDD-LLM, we perform comprehensive experiments and provide several comparative analyses against existing model-based solutions across multiple evaluation metrics. Experimental results show that MDD-LLM (70B) achieves an accuracy of 0.8378 and an AUC of 0.8919 (95% CI: 0.8799 - 0.9040), significantly outperforming existing machine learning and deep learning frameworks for MDD diagnosis. Given the limited exploration of LLMs in MDD diagnosis, we examine numerous factors that may influence the performance of our proposed method, such as tabular data transformation techniques and different fine-tuning strategies.</li>
</ul>

<h3>Title: From Attention to Atoms: Spectral Dictionary Learning for Fast, Interpretable Language Models</h3>
<ul>
<li><strong>Authors: </strong>Andrew Kiruluta</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00033">https://arxiv.org/abs/2505.00033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00033">https://arxiv.org/pdf/2505.00033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00033]] From Attention to Atoms: Spectral Dictionary Learning for Fast, Interpretable Language Models(https://arxiv.org/abs/2505.00033)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>We propose a novel spectral generative modeling framework for natural language processing that jointly learns a global time varying Fourier dictionary and per token mixing coefficients, replacing the ubiquitous self attention mechanism in transformer architectures. By enforcing reconstruction losses in both the time domain (embedding reconstruction) and the frequency domain (via Short Time Fourier Transform magnitude matching) alongside a standard language modeling objective, and fitting a Gaussian Mixture Model (GMM) prior over the learned mixing vectors, our approach achieves competitive perplexity and generation quality on standard benchmarks such as WikiText2 and Penn Treebank. In contrast to the quadratic computation complexity of self attention, our method operates with linear complexity, delivering substantial efficiency gains. We demonstrate that spectral dictionary models can achieve competitive performance compared to transformer baselines while significantly reducing inference latency and memory footprint, offering a compelling alternative for scalable language modeling.</li>
</ul>

<h3>Title: Improving Phishing Email Detection Performance of Small Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zijie Lin, Zikang Liu, Hanbo Fan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00034">https://arxiv.org/abs/2505.00034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00034">https://arxiv.org/pdf/2505.00034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00034]] Improving Phishing Email Detection Performance of Small Large Language Models(https://arxiv.org/abs/2505.00034)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models(LLMs) have demonstrated remarkable performance on many natural language processing(NLP) tasks and have been employed in phishing email detection research. However, in current studies, well-performing LLMs typically contain billions or even tens of billions of parameters, requiring enormous computational resources. To reduce computational costs, we investigated the effectiveness of small-parameter LLMs for phishing email detection. These LLMs have around 3 billion parameters and can run on consumer-grade GPUs. However, small LLMs often perform poorly in phishing email detection task. To address these issues, we designed a set of methods including Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble to improve phishing email detection capabilities of small LLMs. We validated the effectiveness of our approach through experiments, significantly improving accuracy on the SpamAssassin dataset from around 0.5 for baseline models like Qwen2.5-1.5B-Instruct to 0.976.</li>
</ul>

<h3>Title: A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies</h3>
<ul>
<li><strong>Authors: </strong>Zhongren Chen, Joshua Kalla, Quan Le, Shinpei Nakamura-Sakai, Jasjeet Sekhon, Ruixiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00036">https://arxiv.org/abs/2505.00036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00036">https://arxiv.org/pdf/2505.00036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00036]] A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies(https://arxiv.org/abs/2505.00036)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, significant concern has emerged regarding the potential threat that Large Language Models (LLMs) pose to democratic societies through their persuasive capabilities. We expand upon existing research by conducting two survey experiments and a real-world simulation exercise to determine whether it is more cost effective to persuade a large number of voters using LLM chatbots compared to standard political campaign practice, taking into account both the "receive" and "accept" steps in the persuasion process (Zaller 1992). These experiments improve upon previous work by assessing extended interactions between humans and LLMs (instead of using single-shot interactions) and by assessing both short- and long-run persuasive effects (rather than simply asking users to rate the persuasiveness of LLM-produced content). In two survey experiments (N = 10,417) across three distinct political domains, we find that while LLMs are about as persuasive as actual campaign ads once voters are exposed to them, political persuasion in the real-world depends on both exposure to a persuasive message and its impact conditional on exposure. Through simulations based on real-world parameters, we estimate that LLM-based persuasion costs between \$48-\$74 per persuaded voter compared to \$100 for traditional campaign methods, when accounting for the costs of exposure. However, it is currently much easier to scale traditional campaign persuasion methods than LLM-based persuasion. While LLMs do not currently appear to have substantially greater potential for large-scale political persuasion than existing non-LLM methods, this may change as LLM capabilities continue to improve and it becomes easier to scalably encourage exposure to persuasive LLMs.</li>
</ul>

<h3>Title: HyPerAlign: Hypotheses-driven Personalized Alignment</h3>
<ul>
<li><strong>Authors: </strong>Cristina Garbacea, Chenhao Tan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00038">https://arxiv.org/abs/2505.00038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00038">https://arxiv.org/pdf/2505.00038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00038]] HyPerAlign: Hypotheses-driven Personalized Alignment(https://arxiv.org/abs/2505.00038)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Alignment algorithms are widely used to align large language models (LLMs) to human users based on preference annotations that reflect their intended real-world use cases. Typically these (often divergent) preferences are aggregated over a diverse set of users, resulting in fine-tuned models that are aligned to the ``average-user'' preference. Nevertheless, current models are used by individual users in very specific contexts and situations, emphasizing the need for user-dependent preference control. In this work we address the problem of personalizing LLM outputs to their users, aiming to generate customized responses tailored to individual users, instead of generic outputs that emulate the collective voices of diverse populations. We propose a novel interpretable and sample-efficient hypotheses-driven personalization approach (HyPerAlign) where given few-shot examples written by a particular user, we first infer hypotheses about their communication strategies, personality and writing style, then prompt LLM models with these hypotheses and user specific attributes to generate customized outputs. We conduct experiments on two different personalization tasks, authorship attribution and deliberative alignment, with datasets from diverse domains (news articles, blog posts, emails, jailbreaking benchmarks), and demonstrate the superiority of hypotheses-driven personalization approach when compared to preference-based fine-tuning methods. For deliberative alignment, the helpfulness of LLM models is improved by up to $70\%$ on average. For authorship attribution, results indicate consistently high win-rates (commonly $>90\%$) against state-of-the-art preference fine-tuning approaches for LLM personalization across diverse user profiles and LLM models. Overall, our approach represents an interpretable and sample-efficient strategy for the personalization of LLM models to individual users.</li>
</ul>

<h3>Title: Learning to Borrow Features for Improved Detection of Small Objects in Single-Shot Detectors</h3>
<ul>
<li><strong>Authors: </strong>Richard Schmit</a></li>
<li><strong>Subjects: </strong>cs.CV, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00044">https://arxiv.org/abs/2505.00044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00044">https://arxiv.org/pdf/2505.00044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00044]] Learning to Borrow Features for Improved Detection of Small Objects in Single-Shot Detectors(https://arxiv.org/abs/2505.00044)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Detecting small objects remains a significant challenge in single-shot object detectors due to the inherent trade-off between spatial resolution and semantic richness in convolutional feature maps. To address this issue, we propose a novel framework that enables small object representations to "borrow" discriminative features from larger, semantically richer instances within the same class. Our architecture introduces three key components: the Feature Matching Block (FMB) to identify semantically similar descriptors across layers, the Feature Representing Block (FRB) to generate enhanced shallow features through weighted aggregation, and the Feature Fusion Block (FFB) to refine feature maps by integrating original, borrowed, and context information. Built upon the SSD framework, our method improves the descriptive capacity of shallow layers while maintaining real-time detection performance. Experimental results demonstrate that our approach significantly boosts small object detection accuracy over baseline methods, offering a promising direction for robust object detection in complex visual environments.</li>
</ul>

<h3>Title: A Report on the llms evaluating the high school questions</h3>
<ul>
<li><strong>Authors: </strong>Zhu Jiawei, Chen Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00057">https://arxiv.org/abs/2505.00057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00057">https://arxiv.org/pdf/2505.00057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00057]] A Report on the llms evaluating the high school questions(https://arxiv.org/abs/2505.00057)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This report aims to evaluate the performance of large language models (LLMs) in solving high school science questions and to explore their potential applications in the educational field. With the rapid development of LLMs in the field of natural language processing, their application in education has attracted widespread attention. This study selected mathematics exam questions from the college entrance examinations (2019-2023) as evaluation data and utilized at least eight LLM APIs to provide answers. A comprehensive assessment was conducted based on metrics such as accuracy, response time, logical reasoning, and creativity. Through an in-depth analysis of the evaluation results, this report reveals the strengths and weaknesses of LLMs in handling high school science questions and discusses their implications for educational practice. The findings indicate that although LLMs perform excellently in certain aspects, there is still room for improvement in logical reasoning and creative problem-solving. This report provides an empirical foundation for further research and application of LLMs in the educational field and offers suggestions for improvement.</li>
</ul>

<h3>Title: BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition</h3>
<ul>
<li><strong>Authors: </strong>Paige Tutt√∂s√≠, Mantaj Dhillon, Luna Sang, Shane Eastwood, Poorvi Bhatia, Quang Minh Dinh, Avni Kapoor, Yewon Jin, Angelica Lim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00059">https://arxiv.org/abs/2505.00059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00059">https://arxiv.org/pdf/2505.00059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00059]] BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition(https://arxiv.org/abs/2505.00059)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Some speech recognition tasks, such as automatic speech recognition (ASR), are approaching or have reached human performance in many reported metrics. Yet, they continue to struggle in complex, real-world, situations, such as with distanced speech. Previous challenges have released datasets to address the issue of distanced ASR, however, the focus remains primarily on distance, specifically relying on multi-microphone array systems. Here we present the B(asic) E(motion) R(andom phrase) S(hou)t(s) (BERSt) dataset. The dataset contains almost 4 hours of English speech from 98 actors with varying regional and non-native accents. The data was collected on smartphones in the actors homes and therefore includes at least 98 different acoustic environments. The data also includes 7 different emotion prompts and both shouted and spoken utterances. The smartphones were places in 19 different positions, including obstructions and being in a different room than the actor. This data is publicly available for use and can be used to evaluate a variety of speech recognition tasks, including: ASR, shout detection, and speech emotion recognition (SER). We provide initial benchmarks for ASR and SER tasks, and find that ASR degrades both with an increase in distance and shout level and shows varied performance depending on the intended emotion. Our results show that the BERSt dataset is challenging for both ASR and SER tasks and continued work is needed to improve the robustness of such systems for more accurate real-world use.</li>
</ul>

<h3>Title: Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5</h3>
<ul>
<li><strong>Authors: </strong>Jeho Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00060">https://arxiv.org/abs/2505.00060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00060">https://arxiv.org/pdf/2505.00060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00060]] Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5(https://arxiv.org/abs/2505.00060)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown promise in enabling natural language interfaces for structured data querying through text-to-SQL generation. However, their application in real-world Business Intelligence (BI) contexts remains limited due to semantic hallucinations, structural errors, and a lack of domain-specific evaluation frameworks. In this study, we propose a Fact-Consistency Evaluation Framework for assessing the semantic accuracy of LLM-generated SQL outputs using Exaone 3.5--an instruction-tuned, bilingual LLM optimized for enterprise tasks. We construct a domain-specific benchmark comprising 219 natural language business questions across five SQL complexity levels, derived from actual sales data in LG Electronics' internal BigQuery environment. Each question is paired with a gold-standard SQL query and a validated ground-truth answer. We evaluate model performance using answer accuracy, execution success rate, semantic error rate, and non-response rate. Experimental results show that while Exaone 3.5 performs well on simple aggregation tasks (93% accuracy in L1), it exhibits substantial degradation in arithmetic reasoning (4% accuracy in H1) and grouped ranking tasks (31% in H4), with semantic errors and non-responses concentrated in complex cases. Qualitative error analysis further identifies common failure types such as misapplied arithmetic logic, incomplete filtering, and incorrect grouping operations. Our findings highlight the current limitations of LLMs in business-critical environments and underscore the need for fact-consistency validation layers and hybrid reasoning approaches. This work contributes a reproducible benchmark and evaluation methodology for advancing reliable natural language interfaces to structured enterprise data systems.</li>
</ul>

<h3>Title: Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems</h3>
<ul>
<li><strong>Authors: </strong>Sahar Yarmohammadtoosky, Yiyun Zhou, Victoria Yaneva, Peter Baldwin, Saed Rezayi, Brian Clauser, Polina Harikeo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00061">https://arxiv.org/abs/2505.00061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00061">https://arxiv.org/pdf/2505.00061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00061]] Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems(https://arxiv.org/abs/2505.00061)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, robust, fair, transformer, large language model</a></li>
<li><strong>Abstract: </strong>This study examines vulnerabilities in transformer-based automated short-answer grading systems used in medical education, with a focus on how these systems can be manipulated through adversarial gaming strategies. Our research identifies three main types of gaming strategies that exploit the system's weaknesses, potentially leading to false positives. To counteract these vulnerabilities, we implement several adversarial training methods designed to enhance the systems' robustness. Our results indicate that these methods significantly reduce the susceptibility of grading systems to such manipulations, especially when combined with ensemble techniques like majority voting and ridge regression, which further improve the system's defense against sophisticated adversarial inputs. Additionally, employing large language models such as GPT-4 with varied prompting techniques has shown promise in recognizing and scoring gaming strategies effectively. The findings underscore the importance of continuous improvements in AI-driven educational tools to ensure their reliability and fairness in high-stakes settings.</li>
</ul>

<h3>Title: GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling</h3>
<ul>
<li><strong>Authors: </strong>Siqi Li, Yufan Shen, Xiangnan Chen, Jiayi Chen, Hengwei Ju, Haodong Duan, Song Mao, Hongbin Zhou, Bo Zhang, Pinlong Cai, Licheng Wen, Botian Shi, Yong Liu, Xinyu Cai, Yu Qiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00063">https://arxiv.org/abs/2505.00063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00063">https://arxiv.org/pdf/2505.00063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00063]] GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling(https://arxiv.org/abs/2505.00063)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of multimodal large language models (MLLMs) has profoundly impacted the document domain, creating a wide array of application scenarios. This progress highlights the need for a comprehensive benchmark to evaluate these models' capabilities across various document-specific tasks. However, existing benchmarks often fail to locate specific model weaknesses or guide systematic improvements. To bridge this gap, we introduce a General Document Intelligence Benchmark (GDI-Bench), featuring 1.9k images across 9 key scenarios and 19 document-specific tasks. By decoupling visual complexity and reasoning complexity, the GDI-Bench structures graded tasks that allow performance assessment by difficulty, aiding in model weakness identification and optimization guidance. We evaluate the GDI-Bench on various open-source and closed-source models, conducting decoupled analyses in the visual and reasoning domains. For instance, the GPT-4o model excels in reasoning tasks but exhibits limitations in visual capabilities. To address the diverse tasks and domains in the GDI-Bench, we propose a GDI Model that mitigates the issue of catastrophic forgetting during the supervised fine-tuning (SFT) process through a intelligence-preserving training strategy. Our model achieves state-of-the-art performance on previous benchmarks and the GDI-Bench. Both our benchmark and model will be open source.</li>
</ul>

<h3>Title: ConSens: Assessing context grounding in open-book question answering</h3>
<ul>
<li><strong>Authors: </strong>Ivan Vankov, Matyo Ivanov, Adriana Correia, Victor Botev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00065">https://arxiv.org/abs/2505.00065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00065">https://arxiv.org/pdf/2505.00065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00065]] ConSens: Assessing context grounding in open-book question answering(https://arxiv.org/abs/2505.00065)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated considerable success in open-book question answering (QA), where the task requires generating answers grounded in a provided external context. A critical challenge in open-book QA is to ensure that model responses are based on the provided context rather than its parametric knowledge, which can be outdated, incomplete, or incorrect. Existing evaluation methods, primarily based on the LLM-as-a-judge approach, face significant limitations, including biases, scalability issues, and dependence on costly external systems. To address these challenges, we propose a novel metric that contrasts the perplexity of the model response under two conditions: when the context is provided and when it is not. The resulting score quantifies the extent to which the model's answer relies on the provided context. The validity of this metric is demonstrated through a series of experiments that show its effectiveness in identifying whether a given answer is grounded in the provided context. Unlike existing approaches, this metric is computationally efficient, interpretable, and adaptable to various use cases, offering a scalable and practical solution to assess context utilization in open-book QA systems.</li>
</ul>

<h3>Title: From Lab to Wrist: Bridging Metabolic Monitoring and Consumer Wearables for Heart Rate and Oxygen Consumption Modeling</h3>
<ul>
<li><strong>Authors: </strong>Barak Gahtan, Sanketh Vedula, Gil Samuelly Leichtag, Einat Kodesh, Alex M. Bronstein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00101">https://arxiv.org/abs/2505.00101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00101">https://arxiv.org/pdf/2505.00101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00101]] From Lab to Wrist: Bridging Metabolic Monitoring and Consumer Wearables for Heart Rate and Oxygen Consumption Modeling(https://arxiv.org/abs/2505.00101)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Understanding physiological responses during running is critical for performance optimization, tailored training prescriptions, and athlete health management. We introduce a comprehensive framework -- what we believe to be the first capable of predicting instantaneous oxygen consumption (VO$_{2}$) trajectories exclusively from consumer-grade wearable data. Our approach employs two complementary physiological models: (1) accurate modeling of heart rate (HR) dynamics via a physiologically constrained ordinary differential equation (ODE) and neural Kalman filter, trained on over 3 million HR observations, achieving 1-second interval predictions with mean absolute errors as low as 2.81\,bpm (correlation 0.87); and (2) leveraging the principles of precise HR modeling, a novel VO$_{2}$ prediction architecture requiring only the initial second of VO$_{2}$ data for calibration, enabling robust, sequence-to-sequence metabolic demand estimation. Despite relying solely on smartwatch and chest-strap data, our method achieves mean absolute percentage errors of approximately 13\%, effectively capturing rapid physiological transitions and steady-state conditions across diverse running intensities. Our synchronized dataset, complemented by blood lactate measurements, further lays the foundation for future noninvasive metabolic zone identification. By embedding physiological constraints within modern machine learning, this framework democratizes advanced metabolic monitoring, bridging laboratory-grade accuracy and everyday accessibility, thus empowering both elite athletes and recreational fitness enthusiasts.</li>
</ul>

<h3>Title: Security-by-Design at the Telco Edge with OSS: Challenges and Lessons Learned</h3>
<ul>
<li><strong>Authors: </strong>Carmine Cesarano, Alessio Foggia, Gianluca Roscigno, Luca Andreani, Roberto Natella</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00111">https://arxiv.org/abs/2505.00111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00111">https://arxiv.org/pdf/2505.00111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00111]] Security-by-Design at the Telco Edge with OSS: Challenges and Lessons Learned(https://arxiv.org/abs/2505.00111)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>This paper presents our experience, in the context of an industrial R&D project, on securing GENIO, a platform for edge computing on Passive Optical Network (PON) infrastructures, and based on Open-Source Software (OSS). We identify threats and related mitigations through hardening, vulnerability management, digital signatures, and static and dynamic analysis. In particular, we report lessons learned in applying these mitigations using OSS, and share our findings about the maturity and limitations of these security solutions in an industrial context.</li>
</ul>

<h3>Title: Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese</h3>
<ul>
<li><strong>Authors: </strong>Silvana Yakhni, Ali Chehab</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00114">https://arxiv.org/abs/2505.00114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00114">https://arxiv.org/pdf/2505.00114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00114]] Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese(https://arxiv.org/abs/2505.00114)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper examines the effectiveness of Large Language Models (LLMs) in translating the low-resource Lebanese dialect, focusing on the impact of culturally authentic data versus larger translated datasets. We compare three fine-tuning approaches: Basic, contrastive, and grammar-hint tuning, using open-source Aya23 models. Experiments reveal that models fine-tuned on a smaller but culturally aware Lebanese dataset (LW) consistently outperform those trained on larger, non-native data. The best results were achieved through contrastive fine-tuning paired with contrastive prompting, which indicates the benefits of exposing translation models to bad examples. In addition, to ensure authentic evaluation, we introduce LebEval, a new benchmark derived from native Lebanese content, and compare it to the existing FLoRes benchmark. Our findings challenge the "More Data is Better" paradigm and emphasize the crucial role of cultural authenticity in dialectal translation. We made our datasets and code available on Github.</li>
</ul>

<h3>Title: Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jinyan Su, Jennifer Healey, Preslav Nakov, Claire Cardie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00127">https://arxiv.org/abs/2505.00127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00127">https://arxiv.org/pdf/2505.00127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00127]] Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs(https://arxiv.org/abs/2505.00127)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly optimized for long reasoning, under the assumption that more reasoning leads to better performance. However, emerging evidence suggests that longer responses can sometimes degrade accuracy rather than improve it. In this paper, we conduct a systematic empirical study of the relationship between reasoning length and answer correctness. We find that LLMs tend to overthink simple problems, generating unnecessarily long outputs, and underthink harder ones, failing to extend their reasoning when it is most needed. This indicates that models might misjudge problem difficulty and fail to calibrate their response length appropriately. Furthermore, we investigate the effects of length reduction with a preference optimization algorithm when simply preferring the shorter responses regardless of answer correctness. Experiments show that the generation length can be significantly reduced while maintaining acceptable accuracy. Our findings highlight generation length as a meaningful signal for reasoning behavior and motivate further exploration into LLMs' self-awareness in reasoning length adaptation.</li>
</ul>

<h3>Title: AdaptMI: Adaptive Skill-based In-context Math Instruction for Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yinghui He, Abhishek Panigrahi, Yong Lin, Sanjeev Arora</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00147">https://arxiv.org/abs/2505.00147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00147">https://arxiv.org/pdf/2505.00147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00147]] AdaptMI: Adaptive Skill-based In-context Math Instruction for Small Language Models(https://arxiv.org/abs/2505.00147)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) allows a language model to improve its problem-solving capability when provided with suitable information in context. Since the choice of in-context information can be determined based on the problem itself, in-context learning is analogous to human learning from teachers in a classroom. Recent works (Didolkar et al., 2024a; 2024b) show that ICL performance can be improved by leveraging a frontier large language model's (LLM) ability to predict required skills to solve a problem, popularly referred to as an LLM's metacognition, and using the recommended skills to construct necessary in-context examples. While this skill-based strategy boosts ICL performance in larger models, its gains on small language models (SLMs) have been minimal, highlighting a performance gap in ICL capabilities. We investigate this gap and show that skill-based prompting can hurt SLM performance on easy questions by introducing unnecessary information, akin to cognitive overload. To address this, we introduce AdaptMI, an adaptive approach to selecting skill-based in-context Math Instructions for SLMs. Inspired by cognitive load theory from human pedagogy, our method only introduces skill-based examples when the model performs poorly. We further propose AdaptMI+, which adds examples targeted to the specific skills missing from the model's responses. On 5-shot evaluations across popular math benchmarks and five SLMs (1B--7B; Qwen, Llama), AdaptMI+ improves accuracy by up to 6% over naive skill-based strategies.</li>
</ul>

<h3>Title: V3LMA: Visual 3D-enhanced Language Model for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Jannik L√ºbberstedt, Esteban Rivera, Nico Uhlemann, Markus Lienkamp</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00156">https://arxiv.org/abs/2505.00156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00156">https://arxiv.org/pdf/2505.00156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00156]] V3LMA: Visual 3D-enhanced Language Model for Autonomous Driving(https://arxiv.org/abs/2505.00156)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Vision Language Models (LVLMs) have shown strong capabilities in understanding and analyzing visual scenes across various domains. However, in the context of autonomous driving, their limited comprehension of 3D environments restricts their effectiveness in achieving a complete and safe understanding of dynamic surroundings. To address this, we introduce V3LMA, a novel approach that enhances 3D scene understanding by integrating Large Language Models (LLMs) with LVLMs. V3LMA leverages textual descriptions generated from object detections and video inputs, significantly boosting performance without requiring fine-tuning. Through a dedicated preprocessing pipeline that extracts 3D object data, our method improves situational awareness and decision-making in complex traffic scenarios, achieving a score of 0.56 on the LingoQA benchmark. We further explore different fusion strategies and token combinations with the goal of advancing the interpretation of traffic scenes, ultimately enabling safer autonomous driving systems.</li>
</ul>

<h3>Title: Stochastic Subspace Descent Accelerated via Bi-fidelity Line Search</h3>
<ul>
<li><strong>Authors: </strong>Nuojin Cheng, Alireza Doostan, Stephen Becker</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00162">https://arxiv.org/abs/2505.00162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00162">https://arxiv.org/pdf/2505.00162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00162]] Stochastic Subspace Descent Accelerated via Bi-fidelity Line Search(https://arxiv.org/abs/2505.00162)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, transformer</a></li>
<li><strong>Abstract: </strong>Efficient optimization remains a fundamental challenge across numerous scientific and engineering domains, especially when objective function and gradient evaluations are computationally expensive. While zeroth-order optimization methods offer effective approaches when gradients are inaccessible, their practical performance can be limited by the high cost associated with function queries. This work introduces the bi-fidelity stochastic subspace descent (BF-SSD) algorithm, a novel zeroth-order optimization method designed to reduce this computational burden. BF-SSD leverages a bi-fidelity framework, constructing a surrogate model from a combination of computationally inexpensive low-fidelity (LF) and accurate high-fidelity (HF) function evaluations. This surrogate model facilitates an efficient backtracking line search for step size selection, for which we provide theoretical convergence guarantees under standard assumptions. We perform a comprehensive empirical evaluation of BF-SSD across four distinct problems: a synthetic optimization benchmark, dual-form kernel ridge regression, black-box adversarial attacks on machine learning models, and transformer-based black-box language model fine-tuning. Numerical results demonstrate that BF-SSD consistently achieves superior optimization performance while requiring significantly fewer HF function evaluations compared to relevant baseline methods. This study highlights the efficacy of integrating bi-fidelity strategies within zeroth-order optimization, positioning BF-SSD as a promising and computationally efficient approach for tackling large-scale, high-dimensional problems encountered in various real-world applications.</li>
</ul>

<h3>Title: GEOM-Drugs Revisited: Toward More Chemically Accurate Benchmarks for 3D Molecule Generation</h3>
<ul>
<li><strong>Authors: </strong>Filipp Nikitin, Ian Dunn, David Ryan Koes, Olexandr Isayev</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00169">https://arxiv.org/abs/2505.00169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00169">https://arxiv.org/pdf/2505.00169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00169]] GEOM-Drugs Revisited: Toward More Chemically Accurate Benchmarks for 3D Molecule Generation(https://arxiv.org/abs/2505.00169)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep generative models have shown significant promise in generating valid 3D molecular structures, with the GEOM-Drugs dataset serving as a key benchmark. However, current evaluation protocols suffer from critical flaws, including incorrect valency definitions, bugs in bond order calculations, and reliance on force fields inconsistent with the reference data. In this work, we revisit GEOM-Drugs and propose a corrected evaluation framework: we identify and fix issues in data preprocessing, construct chemically accurate valency tables, and introduce a GFN2-xTB-based geometry and energy benchmark. We retrain and re-evaluate several leading models under this framework, providing updated performance metrics and practical recommendations for future benchmarking. Our results underscore the need for chemically rigorous evaluation practices in 3D molecular generation. Our recommended evaluation methods and GEOM-Drugs processing scripts are available at this https URL.</li>
</ul>

<h3>Title: Chronic Diseases Prediction using Machine Learning and Deep Learning Methods</h3>
<ul>
<li><strong>Authors: </strong>Houda Belhad, Asmae Bourbia, Salma Boughanja</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00189">https://arxiv.org/abs/2505.00189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00189">https://arxiv.org/pdf/2505.00189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00189]] Chronic Diseases Prediction using Machine Learning and Deep Learning Methods(https://arxiv.org/abs/2505.00189)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Chronic diseases, such as cardiovascular disease, diabetes, chronic kidney disease, and thyroid disorders, are the leading causes of premature mortality worldwide. Early detection and intervention are crucial for improving patient outcomes, yet traditional diagnostic methods often fail due to the complex nature of these conditions. This study explores the application of machine learning (ML) and deep learning (DL) techniques to predict chronic disease and thyroid disorders. We used a variety of models, including Logistic Regression (LR), Random Forest (RF), Gradient Boosted Trees (GBT), Neural Networks (NN), Decision Trees (DT) and Native Bayes (NB), to analyze and predict disease outcomes. Our methodology involved comprehensive data pre-processing, including handling missing values, categorical encoding, and feature aggregation, followed by model training and evaluation. Performance metrics such ad precision, recall, accuracy, F1-score, and Area Under the Curve (AUC) were used to assess the effectiveness of each model. The results demonstrated that ensemble methods like Random Forest and Gradient Boosted Trees consistently outperformed. Neutral Networks also showed superior performance, particularly in capturing complex data patterns. The findings highlight the potential of ML and DL in revolutionizing chronic disease prediction, enabling early diagnosis and personalized treatment strategies. However, challenges such as data quality, model interpretability, and the need for advanced computational techniques in healthcare to improve patient outcomes and reduce the burden of chronic diseases. This study was conducted as part of Big Data class project under the supervision of our professors Mr. Abderrahmane EZ-ZAHOUT and Mr. Abdessamad ESSAIDI.</li>
</ul>

<h3>Title: IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports</h3>
<ul>
<li><strong>Authors: </strong>Yuyan Ge, Kwan Ho Ryan Chan, Pablo Messina, Ren√© Vidal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00191">https://arxiv.org/abs/2505.00191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00191">https://arxiv.org/pdf/2505.00191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00191]] IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports(https://arxiv.org/abs/2505.00191)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The development of AI-based methods for analyzing radiology reports could lead to significant advances in medical diagnosis--from improving diagnostic accuracy to enhancing efficiency and reducing workload. However, the lack of interpretability in these methods has hindered their adoption in clinical settings. In this paper, we propose an interpretable-by-design framework for classifying radiology reports. The key idea is to extract a set of most informative queries from a large set of reports and use these queries and their corresponding answers to predict a diagnosis. Thus, the explanation for a prediction is, by construction, the set of selected queries and answers. We use the Information Pursuit framework to select informative queries, the Flan-T5 model to determine if facts are present in the report, and a classifier to predict the disease. Experiments on the MIMIC-CXR dataset demonstrate the effectiveness of the proposed method, highlighting its potential to enhance trust and usability in medical AI.</li>
</ul>

<h3>Title: Mapping minds not averages: a scalable subject-specific manifold learning framework for neuroimaging data</h3>
<ul>
<li><strong>Authors: </strong>Eloy Geenjaar, Vince Calhoun</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00196">https://arxiv.org/abs/2505.00196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00196">https://arxiv.org/pdf/2505.00196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00196]] Mapping minds not averages: a scalable subject-specific manifold learning framework for neuroimaging data(https://arxiv.org/abs/2505.00196)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Mental and cognitive representations are believed to reside on low-dimensional, non-linear manifolds embedded within high-dimensional brain activity. Uncovering these manifolds is key to understanding individual differences in brain function, yet most existing machine learning methods either rely on population-level spatial alignment or assume data that is temporally structured, either because data is aligned among subjects or because event timings are known. We introduce a manifold learning framework that can capture subject-specific spatial variations across both structured and temporally unstructured neuroimaging data. On simulated data and two naturalistic fMRI datasets (Sherlock and Forrest Gump), our framework outperforms group-based baselines by recovering more accurate and individualized representations. We further show that the framework scales efficiently to large datasets and generalizes well to new subjects. To test this, we apply the framework to temporally unstructured resting-state fMRI data from individuals with schizophrenia and healthy controls. We further apply our method to a large resting-state fMRI dataset comprising individuals with schizophrenia and controls. In this setting, we demonstrate that the framework scales efficiently to large populations and generalizes robustly to unseen subjects. The learned subject-specific spatial maps our model finds reveal clinically relevant patterns, including increased activation in the basal ganglia, visual, auditory, and somatosensory regions, and decreased activation in the insula, inferior frontal gyrus, and angular gyrus. These findings suggest that our framework can uncover clinically relevant subject-specific brain activity patterns. Our approach thus provides a scalable and individualized framework for modeling brain activity, with applications in computational neuroscience and clinical research.</li>
</ul>

<h3>Title: Direct Motion Models for Assessing Generated Videos</h3>
<ul>
<li><strong>Authors: </strong>Kelsey Allen, Carl Doersch, Guangyao Zhou, Mohammed Suhail, Danny Driess, Ignacio Rocco, Yulia Rubanova, Thomas Kipf, Mehdi S. M. Sajjadi, Kevin Murphy, Joao Carreira, Sjoerd van Steenkiste</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00209">https://arxiv.org/abs/2505.00209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00209">https://arxiv.org/pdf/2505.00209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00209]] Direct Motion Models for Assessing Generated Videos(https://arxiv.org/abs/2505.00209)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>A current limitation of video generative video models is that they generate plausible looking frames, but poor motion -- an issue that is not well captured by FVD and other popular methods for evaluating generated videos. Here we go beyond FVD by developing a metric which better measures plausible object interactions and motion. Our novel approach is based on auto-encoding point tracks and yields motion features that can be used to not only compare distributions of videos (as few as one generated and one ground truth, or as many as two datasets), but also for evaluating motion of single videos. We show that using point tracks instead of pixel reconstruction or action recognition features results in a metric which is markedly more sensitive to temporal distortions in synthetic data, and can predict human evaluations of temporal consistency and realism in generated videos obtained from open-source models better than a wide range of alternatives. We also show that by using a point track representation, we can spatiotemporally localize generative video inconsistencies, providing extra interpretability of generated video errors relative to prior work. An overview of the results and link to the code can be found on the project page: this http URL.</li>
</ul>

<h3>Title: Generative Machine Learning in Adaptive Control of Dynamic Manufacturing Processes: A Review</h3>
<ul>
<li><strong>Authors: </strong>Suk Ki Lee, Hyunwoong Ko</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00210">https://arxiv.org/abs/2505.00210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00210">https://arxiv.org/pdf/2505.00210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00210]] Generative Machine Learning in Adaptive Control of Dynamic Manufacturing Processes: A Review(https://arxiv.org/abs/2505.00210)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dynamic manufacturing processes exhibit complex characteristics defined by time-varying parameters, nonlinear behaviors, and uncertainties. These characteristics require sophisticated in-situ monitoring techniques utilizing multimodal sensor data and adaptive control systems that can respond to real-time feedback while maintaining product quality. Recently, generative machine learning (ML) has emerged as a powerful tool for modeling complex distributions and generating synthetic data while handling these manufacturing uncertainties. However, adopting these generative technologies in dynamic manufacturing systems lacks a functional control-oriented perspective to translate their probabilistic understanding into actionable process controls while respecting constraints. This review presents a functional classification of Prediction-Based, Direct Policy, Quality Inference, and Knowledge-Integrated approaches, offering a perspective for understanding existing ML-enhanced control systems and incorporating generative ML. The analysis of generative ML architectures within this framework demonstrates control-relevant properties and potential to extend current ML-enhanced approaches where conventional methods prove insufficient. We show generative ML's potential for manufacturing control through decision-making applications, process guidance, simulation, and digital twins, while identifying critical research gaps: separation between generation and control functions, insufficient physical understanding of manufacturing phenomena, and challenges adapting models from other domains. To address these challenges, we propose future research directions aimed at developing integrated frameworks that combine generative ML and control technologies to address the dynamic complexities of modern manufacturing systems.</li>
</ul>

<h3>Title: Online Federation For Mixtures of Proprietary Agents with Black-Box Encoders</h3>
<ul>
<li><strong>Authors: </strong>Xuwei Yang, Fatemeh Tavakoli, David B. Emerson, Anastasis Kratsios</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00216">https://arxiv.org/abs/2505.00216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00216">https://arxiv.org/pdf/2505.00216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00216]] Online Federation For Mixtures of Proprietary Agents with Black-Box Encoders(https://arxiv.org/abs/2505.00216)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, transformer, generative</a></li>
<li><strong>Abstract: </strong>Most industry-standard generative AIs and feature encoders are proprietary, offering only black-box access: their outputs are observable, but their internal parameters and architectures remain hidden from the end-user. This black-box access is especially limiting when constructing mixture-of-expert type ensemble models since the user cannot optimize each proprietary AI's internal parameters. Our problem naturally lends itself to a non-competitive game-theoretic lens where each proprietary AI (agent) is inherently competing against the other AI agents, with this competition arising naturally due to their obliviousness of the AI's to their internal structure. In contrast, the user acts as a central planner trying to synchronize the ensemble of competing AIs. We show the existence of the unique Nash equilibrium in the online setting, which we even compute in closed-form by eliciting a feedback mechanism between any given time series and the sequence generated by each (proprietary) AI agent. Our solution is implemented as a decentralized, federated-learning algorithm in which each agent optimizes their structure locally on their machine without ever releasing any internal structure to the others. We obtain refined expressions for pre-trained models such as transformers, random feature models, and echo-state networks. Our ``proprietary federated learning'' algorithm is implemented on a range of real-world and synthetic time-series benchmarks. It achieves orders-of-magnitude improvements in predictive accuracy over natural benchmarks, of which there are surprisingly few due to this natural problem still being largely unexplored.</li>
</ul>

<h3>Title: Towards Robust and Generalizable Gerchberg Saxton based Physics Inspired Neural Networks for Computer Generated Holography: A Sensitivity Analysis Framework</h3>
<ul>
<li><strong>Authors: </strong>Ankit Amrutkar, Bj√∂rn Kampa, Volkmar Schulz, Johannes Stegmaier, Markus Rothermel, Dorit Merhof</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.optics</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00220">https://arxiv.org/abs/2505.00220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00220">https://arxiv.org/pdf/2505.00220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00220]] Towards Robust and Generalizable Gerchberg Saxton based Physics Inspired Neural Networks for Computer Generated Holography: A Sensitivity Analysis Framework(https://arxiv.org/abs/2505.00220)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Computer-generated holography (CGH) enables applications in holographic augmented reality (AR), 3D displays, systems neuroscience, and optical trapping. The fundamental challenge in CGH is solving the inverse problem of phase retrieval from intensity measurements. Physics-inspired neural networks (PINNs), especially Gerchberg-Saxton-based PINNs (GS-PINNs), have advanced phase retrieval capabilities. However, their performance strongly depends on forward models (FMs) and their hyperparameters (FMHs), limiting generalization, complicating benchmarking, and hindering hardware optimization. We present a systematic sensitivity analysis framework based on Saltelli's extension of Sobol's method to quantify FMH impacts on GS-PINN performance. Our analysis demonstrates that SLM pixel-resolution is the primary factor affecting neural network sensitivity, followed by pixel-pitch, propagation distance, and wavelength. Free space propagation forward models demonstrate superior neural network performance compared to Fourier holography, providing enhanced parameterization and generalization. We introduce a composite evaluation metric combining performance consistency, generalization capability, and hyperparameter perturbation resilience, establishing a unified benchmarking standard across CGH configurations. Our research connects physics-inspired deep learning theory with practical CGH implementations through concrete guidelines for forward model selection, neural network architecture, and performance evaluation. Our contributions advance the development of robust, interpretable, and generalizable neural networks for diverse holographic applications, supporting evidence-based decisions in CGH research and implementation.</li>
</ul>

<h3>Title: Predicting Estimated Times of Restoration for Electrical Outages Using Longitudinal Tabular Transformers</h3>
<ul>
<li><strong>Authors: </strong>Bogireddy Sai Prasanna Teja, Valliappan Muthukaruppan, Carls Benjamin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00225">https://arxiv.org/abs/2505.00225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00225">https://arxiv.org/pdf/2505.00225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00225]] Predicting Estimated Times of Restoration for Electrical Outages Using Longitudinal Tabular Transformers(https://arxiv.org/abs/2505.00225)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>As climate variability increases, the ability of utility providers to deliver precise Estimated Times of Restoration (ETR) during natural disasters has become increasingly critical. Accurate and timely ETRs are essential for enabling customer preparedness during extended power outages, where informed decision-making can be crucial, particularly in severe weather conditions. Nonetheless, prevailing utility practices predominantly depend on manual assessments or traditional statistical methods, which often fail to achieve the level of precision required for reliable and actionable predictions. To address these limitations, we propose a Longitudinal Tabular Transformer (LTT) model that leverages historical outage event data along with sequential updates of these events to improve the accuracy of ETR predictions. The model's performance was evaluated over 34,000 storm-related outage events from three major utility companies, collectively serving over 3 million customers over a 2-year period. Results demonstrate that the LTT model improves the Customer Satisfaction Impact (CSI) metric by an average of 19.08% (p > 0.001) compared to existing methods. Additionally, we introduce customer-informed regression metrics that align model evaluation with real-world satisfaction, ensuring the outcomes resonate with customer expectations. Furthermore, we employ interpretability techniques to analyze the temporal significance of incorporating sequential updates in modeling outage events and to identify the contributions of predictive features to a given ETR. This comprehensive approach not only improves predictive accuracy but also enhances transparency, fostering greater trust in the model's capabilities.</li>
</ul>

<h3>Title: Scaling On-Device GPU Inference for Large Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jiuqiang Tang, Raman Sarokin, Ekaterina Ignasheva, Grant Jensen, Lin Chen, Juhyun Lee, Andrei Kulik, Matthias Grundmann</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00232">https://arxiv.org/abs/2505.00232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00232">https://arxiv.org/pdf/2505.00232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00232]] Scaling On-Device GPU Inference for Large Generative Models(https://arxiv.org/abs/2505.00232)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>Driven by the advancements in generative AI, large machine learning models have revolutionized domains such as image processing, audio synthesis, and speech recognition. While server-based deployments remain the locus of peak performance, the imperative for on-device inference, necessitated by privacy and efficiency considerations, persists. Recognizing GPUs as the on-device ML accelerator with the widest reach, we present ML Drift--an optimized framework that extends the capabilities of state-of-the-art GPU-accelerated inference engines. ML Drift enables on-device execution of generative AI workloads which contain 10 to 100x more parameters than existing on-device generative AI models. ML Drift addresses intricate engineering challenges associated with cross-GPU API development, and ensures broad compatibility across mobile and desktop/laptop platforms, thereby facilitating the deployment of significantly more complex models on resource-constrained devices. Our GPU-accelerated ML/AI inference engine achieves an order-of-magnitude performance improvement relative to existing open-source GPU inference engines.</li>
</ul>

<h3>Title: Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks</h3>
<ul>
<li><strong>Authors: </strong>Vishnu Sarukkai, Zhiqiang Xie, Kayvon Fatahalian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00234">https://arxiv.org/abs/2505.00234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00234">https://arxiv.org/pdf/2505.00234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00234]] Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks(https://arxiv.org/abs/2505.00234)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining a database of self-generated examples. We demonstrate that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Our results demonstrate that automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering.</li>
</ul>

<h3>Title: Node2Vec-DGI-EL: A Hierarchical Graph Representation Learning Model for Ingredient-Disease Association Prediction</h3>
<ul>
<li><strong>Authors: </strong>Leifeng Zhang, Xin Dong, Shuaibing Jia, Jianhua Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00236">https://arxiv.org/abs/2505.00236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00236">https://arxiv.org/pdf/2505.00236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00236]] Node2Vec-DGI-EL: A Hierarchical Graph Representation Learning Model for Ingredient-Disease Association Prediction(https://arxiv.org/abs/2505.00236)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Traditional Chinese medicine, as an essential component of traditional medicine, contains active ingredients that serve as a crucial source for modern drug development, holding immense therapeutic potential and development value. A multi-layered and complex network is formed from Chinese medicine to diseases and used to predict the potential associations between Chinese medicine ingredients and diseases. This study proposes an ingredient-disease association prediction model (Node2Vec-DGI-EL) based on hierarchical graph representation learning. First, the model uses the Node2Vec algorithm to extract node embedding vectors from the network as the initial features of the nodes. Next, the network nodes are deeply represented and learned using the DGI algorithm to enhance the model's expressive power. To improve prediction accuracy and robustness, an ensemble learning method is incorporated to achieve more accurate ingredient-disease association predictions. The effectiveness of the model is then evaluated through a series of theoretical verifications. The results demonstrated that the proposed model significantly outperformed existing methods, achieving an AUC of 0.9987 and an AUPR of 0.9545, thereby indicating superior predictive capability. Ablation experiments further revealed the contribution and importance of each module. Additionally, case studies explored potential associations, such as triptonide with hypertensive retinopathy and methyl ursolate with colorectal cancer. Molecular docking experiments validated these findings, showing the triptonide-PGR interaction and the methyl ursolate-NFE2L2 interaction can bind stable. In conclusion, the Node2Vec-DGI-EL model focuses on TCM datasets and effectively predicts ingredient-disease associations, overcoming the reliance on node semantic information.</li>
</ul>

<h3>Title: LLM-Based Threat Detection and Prevention Framework for IoT Ecosystems</h3>
<ul>
<li><strong>Authors: </strong>Yazan Otoum, Arghavan Asad, Amiya Nayak</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.ET, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00240">https://arxiv.org/abs/2505.00240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00240">https://arxiv.org/pdf/2505.00240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00240]] LLM-Based Threat Detection and Prevention Framework for IoT Ecosystems(https://arxiv.org/abs/2505.00240)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>The increasing complexity and scale of the Internet of Things (IoT) have made security a critical concern. This paper presents a novel Large Language Model (LLM)-based framework for comprehensive threat detection and prevention in IoT environments. The system integrates lightweight LLMs fine-tuned on IoT-specific datasets (IoT-23, TON_IoT) for real-time anomaly detection and automated, context-aware mitigation strategies optimized for resource-constrained devices. A modular Docker-based deployment enables scalable and reproducible evaluation across diverse network conditions. Experimental results in simulated IoT environments demonstrate significant improvements in detection accuracy, response latency, and resource efficiency over traditional security methods. The proposed framework highlights the potential of LLM-driven, autonomous security solutions for future IoT ecosystems.</li>
</ul>

<h3>Title: Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial Data Circulation</h3>
<ul>
<li><strong>Authors: </strong>Zhizhong Tan, Jiexin Zheng, Kevin Qi Zhang, Wenyong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00257">https://arxiv.org/abs/2505.00257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00257">https://arxiv.org/pdf/2505.00257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00257]] Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial Data Circulation(https://arxiv.org/abs/2505.00257)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>The sharing of external data has become a strong demand of financial institutions, but the privacy issue has led to the difficulty of interconnecting different platforms and the low degree of data openness. To effectively solve the privacy problem of financial data in trans-border flow and sharing, to ensure that the data is available but not visible, to realize the joint portrait of all kinds of heterogeneous data of business organizations in different industries, we propose a Heterogeneous Federated Graph Neural Network (HFGNN) approach. In this method, the distribution of heterogeneous business data of trans-border organizations is taken as subgraphs, and the sharing and circulation process among subgraphs is constructed as a statistically heterogeneous global graph through a central server. Each subgraph learns the corresponding personalized service model through local training to select and update the relevant subset of subgraphs with aggregated parameters, and effectively separates and combines topological and feature information among subgraphs. Finally, our simulation experimental results show that the proposed method has higher accuracy performance and faster convergence speed than existing methods.</li>
</ul>

<h3>Title: Enriching the Korean Learner Corpus with Multi-reference Annotations and Rubric-Based Scoring</h3>
<ul>
<li><strong>Authors: </strong>Jayoung Song, KyungTae Lim, Jungyeul Park</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00261">https://arxiv.org/abs/2505.00261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00261">https://arxiv.org/pdf/2505.00261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00261]] Enriching the Korean Learner Corpus with Multi-reference Annotations and Rubric-Based Scoring(https://arxiv.org/abs/2505.00261)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite growing global interest in Korean language education, there remains a significant lack of learner corpora tailored to Korean L2 writing. To address this gap, we enhance the KoLLA Korean learner corpus by adding multiple grammatical error correction (GEC) references, thereby enabling more nuanced and flexible evaluation of GEC systems, and reflects the variability of human language. Additionally, we enrich the corpus with rubric-based scores aligned with guidelines from the Korean National Language Institute, capturing grammatical accuracy, coherence, and lexical diversity. These enhancements make KoLLA a robust and standardized resource for research in Korean L2 education, supporting advancements in language learning, assessment, and automated error correction.</li>
</ul>

<h3>Title: Consistency in Language Models: Current Landscape, Challenges, and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Jekaterina Novikova, Carol Anderson, Borhane Blili-Hamelin, Subhabrata Majumdar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00268">https://arxiv.org/abs/2505.00268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00268">https://arxiv.org/pdf/2505.00268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00268]] Consistency in Language Models: Current Landscape, Challenges, and Future Directions(https://arxiv.org/abs/2505.00268)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The hallmark of effective language use lies in consistency -- expressing similar meanings in similar contexts and avoiding contradictions. While human communication naturally demonstrates this principle, state-of-the-art language models struggle to maintain reliable consistency across different scenarios. This paper examines the landscape of consistency research in AI language systems, exploring both formal consistency (including logical rule adherence) and informal consistency (such as moral and factual coherence). We analyze current approaches to measure aspects of consistency, identify critical research gaps in standardization of definitions, multilingual assessment, and methods to improve consistency. Our findings point to an urgent need for robust benchmarks to measure and interdisciplinary approaches to ensure consistency in the application of language models on domain-specific tasks while preserving the utility and adaptability.</li>
</ul>

<h3>Title: AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care</h3>
<ul>
<li><strong>Authors: </strong>Md Asaduzzaman Jabin, Hanqi Jiang, Yiwei Li, Patrick Kaggwa, Eugene Douglass, Juliet N. Sekandi, Tianming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00275">https://arxiv.org/abs/2505.00275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00275">https://arxiv.org/pdf/2505.00275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00275]] AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care(https://arxiv.org/abs/2505.00275)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS, epilepsy, and tuberculosis, necessitate rigorous adherence to medication to avert disease progression, manage symptoms, and decrease mortality rates. Adherence is frequently undermined by factors including patient behavior, caregiver support, elevated medical costs, and insufficient healthcare infrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based multimodal large vision language model (LVLM) aimed at visual question answering (VQA) concerning medication adherence through patient videos. We employ a private dataset comprising 806 custom-annotated tuberculosis (TB) medication monitoring videos, which have been labeled by clinical experts, to fine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a detailed medical adherence VQA dataset that encompasses positive, negative, and ambiguous adherence cases. Our method identifies correlations between visual features, such as the clear visibility of the patient's face, medication, water intake, and the act of ingestion, and their associated medical concepts in captions. This facilitates the integration of aligned visual-linguistic representations and improves multimodal interactions. Experimental results indicate that our method surpasses parameter-efficient fine-tuning (PEFT) enabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute improvements ranging from 3.1% to 3.54% across pre-trained, regular, and low-rank adaptation (LoRA) configurations. Comprehensive ablation studies and attention map visualizations substantiate our approach, enhancing interpretability.</li>
</ul>

<h3>Title: PatchFuzz: Patch Fuzzing for JavaScript Engines</h3>
<ul>
<li><strong>Authors: </strong>Junjie Wang, Yuhan Ma, Xiaofei Xie, Xiaoning Du, Xiangwei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00289">https://arxiv.org/abs/2505.00289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00289">https://arxiv.org/pdf/2505.00289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00289]] PatchFuzz: Patch Fuzzing for JavaScript Engines(https://arxiv.org/abs/2505.00289)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Patch fuzzing is a technique aimed at identifying vulnerabilities that arise from newly patched code. While researchers have made efforts to apply patch fuzzing to testing JavaScript engines with considerable success, these efforts have been limited to using ordinary test cases or publicly available vulnerability PoCs (Proof of Concepts) as seeds, and the sustainability of these approaches is hindered by the challenges associated with automating the PoC collection. To address these limitations, we propose an end-to-end sustainable approach for JavaScript engine patch fuzzing, named PatchFuzz. It automates the collection of PoCs of a broader range of historical vulnerabilities and leverages both the PoCs and their corresponding patches to uncover new vulnerabilities more effectively. PatchFuzz starts by recognizing git commits which intend to fix security bugs. Subsequently, it extracts and processes PoCs from these commits to form the seeds for fuzzing, while utilizing code revisions to focus limited fuzzing resources on the more vulnerable code areas through selective instrumentation. The mutation strategy of PatchFuzz is also optimized to maximize the potential of the PoCs. Experimental results demonstrate the effectiveness of PatchFuzz. Notably, 54 bugs across six popular JavaScript engines have been exposed and a total of $62,500 bounties has been received.</li>
</ul>

<h3>Title: Multi-Hierarchical Fine-Grained Feature Mapping Driven by Feature Contribution for Molecular Odor Prediction</h3>
<ul>
<li><strong>Authors: </strong>Hong Xin Xie, Jian De Sun, Fan Fu Xue, Zi Fei Han, Shan Shan Feng, Qi Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00290">https://arxiv.org/abs/2505.00290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00290">https://arxiv.org/pdf/2505.00290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00290]] Multi-Hierarchical Fine-Grained Feature Mapping Driven by Feature Contribution for Molecular Odor Prediction(https://arxiv.org/abs/2505.00290)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Molecular odor prediction is the process of using a molecule's structure to predict its smell. While accurate prediction remains challenging, AI models can suggest potential odors. Existing methods, however, often rely on basic descriptors or handcrafted fingerprints, which lack expressive power and hinder effective learning. Furthermore, these methods suffer from severe class imbalance, limiting the training effectiveness of AI models. To address these challenges, we propose a Feature Contribution-driven Hierarchical Multi-Feature Mapping Network (HMFNet). Specifically, we introduce a fine-grained, Local Multi-Hierarchy Feature Extraction module (LMFE) that performs deep feature extraction at the atomic level, capturing detailed features crucial for odor prediction. To enhance the extraction of discriminative atomic features, we integrate a Harmonic Modulated Feature Mapping (HMFM). This module dynamically learns feature importance and frequency modulation, improving the model's capability to capture relevant patterns. Additionally, a Global Multi-Hierarchy Feature Extraction module (GMFE) is designed to learn global features from the molecular graph topology, enabling the model to fully leverage global information and enhance its discriminative power for odor prediction. To further mitigate the issue of class imbalance, we propose a Chemically-Informed Loss (CIL). Experimental results demonstrate that our approach significantly improves performance across various deep learning models, highlighting its potential to advance molecular structure representation and accelerate the development of AI-driven technologies.</li>
</ul>

<h3>Title: Fine-grained spatial-temporal perception for gas leak segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xinlong Zhao, Shan Du</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00295">https://arxiv.org/abs/2505.00295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00295">https://arxiv.org/pdf/2505.00295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00295]] Fine-grained spatial-temporal perception for gas leak segmentation(https://arxiv.org/abs/2505.00295)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Gas leaks pose significant risks to human health and the environment. Despite long-standing concerns, there are limited methods that can efficiently and accurately detect and segment leaks due to their concealed appearance and random shapes. In this paper, we propose a Fine-grained Spatial-Temporal Perception (FGSTP) algorithm for gas leak segmentation. FGSTP captures critical motion clues across frames and integrates them with refined object features in an end-to-end network. Specifically, we first construct a correlation volume to capture motion information between consecutive frames. Then, the fine-grained perception progressively refines the object-level features using previous outputs. Finally, a decoder is employed to optimize boundary segmentation. Because there is no highly precise labeled dataset for gas leak segmentation, we manually label a gas leak video dataset, GasVid. Experimental results on GasVid demonstrate that our model excels in segmenting non-rigid objects such as gas leaks, generating the most accurate mask compared to other state-of-the-art (SOTA) models.</li>
</ul>

<h3>Title: Gateformer: Advancing Multivariate Time Series Forecasting through Temporal and Variate-Wise Attention with Gated Representations</h3>
<ul>
<li><strong>Authors: </strong>Yu-Hsiang Lan, Anton Alyakin, Eric K. Oermann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00307">https://arxiv.org/abs/2505.00307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00307">https://arxiv.org/pdf/2505.00307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00307]] Gateformer: Advancing Multivariate Time Series Forecasting through Temporal and Variate-Wise Attention with Gated Representations(https://arxiv.org/abs/2505.00307)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>There has been a recent surge of interest in time series modeling using the Transformer architecture. However, forecasting multivariate time series with Transformer presents a unique challenge as it requires modeling both temporal (cross-time) and variate (cross-variate) dependencies. While Transformer-based models have gained popularity for their flexibility in capturing both sequential and cross-variate relationships, it is unclear how to best integrate these two sources of information in the context of the Transformer architecture while optimizing for both performance and efficiency. We re-purpose the Transformer architecture to effectively model both cross-time and cross-variate dependencies. Our approach begins by embedding each variate independently into a variate-wise representation that captures its cross-time dynamics, and then models cross-variate dependencies through attention mechanisms on these learned embeddings. Gating operations in both cross-time and cross-variate modeling phases regulate information flow, allowing the model to focus on the most relevant features for accurate predictions. Our method achieves state-of-the-art performance across 13 real-world datasets and can be seamlessly integrated into other Transformer-based and LLM-based forecasters, delivering performance improvements up to 20.7\% over original models. Code is available at this repository: this https URL.</li>
</ul>

<h3>Title: AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality</h3>
<ul>
<li><strong>Authors: </strong>Biling Wang, Austen Maniscalco, Ti Bai, Siqiu Wang, Michael Dohopolski, Mu-Han Lin, Chenyang Shen, Dan Nguyen, Junzhou Huang, Steve Jiang, Xinlei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00308">https://arxiv.org/abs/2505.00308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00308">https://arxiv.org/pdf/2505.00308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00308]] AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality(https://arxiv.org/abs/2505.00308)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Purpose: This study presents a Deep Learning (DL)-based quality assessment (QA) approach for evaluating auto-generated contours (auto-contours) in radiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging Bayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds, the method enables confident QA predictions without relying on ground truth contours or extensive manual labeling. Methods: We developed a BOC model to classify auto-contour quality and quantify prediction uncertainty. A calibration step was used to optimize uncertainty thresholds that meet clinical accuracy needs. The method was validated under three data scenarios: no manual labels, limited labels, and extensive labels. For rectum contours in prostate cancer, we applied geometric surrogate labels when manual labels were absent, transfer learning when limited, and direct supervision when ample labels were available. Results: The BOC model delivered robust performance across all scenarios. Fine-tuning with just 30 manual labels and calibrating with 34 subjects yielded over 90% accuracy on test data. Using the calibrated threshold, over 93% of the auto-contours' qualities were accurately predicted in over 98% of cases, reducing unnecessary manual reviews and highlighting cases needing correction. Conclusion: The proposed QA model enhances contouring efficiency in OART by reducing manual workload and enabling fast, informed clinical decisions. Through uncertainty quantification, it ensures safer, more reliable radiotherapy workflows.</li>
</ul>

<h3>Title: AWARE-NET: Adaptive Weighted Averaging for Robust Ensemble Network in Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Salman, Iqra Tariq, Mishal Zulfiqar, Muqadas Jalal, Sami Aujla, Sumbal Fatima</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00312">https://arxiv.org/abs/2505.00312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00312">https://arxiv.org/pdf/2505.00312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00312]] AWARE-NET: Adaptive Weighted Averaging for Robust Ensemble Network in Deepfake Detection(https://arxiv.org/abs/2505.00312)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Deepfake detection has become increasingly important due to the rise of synthetic media, which poses significant risks to digital identity and cyber presence for security and trust. While multiple approaches have improved detection accuracy, challenges remain in achieving consistent performance across diverse datasets and manipulation types. In response, we propose a novel two-tier ensemble framework for deepfake detection based on deep learning that hierarchically combines multiple instances of three state-of-the-art architectures: Xception, Res2Net101, and EfficientNet-B7. Our framework employs a unique approach where each architecture is instantiated three times with different initializations to enhance model diversity, followed by a learnable weighting mechanism that dynamically combines their predictions. Unlike traditional fixed-weight ensembles, our first-tier averages predictions within each architecture family to reduce model variance, while the second tier learns optimal contribution weights through backpropagation, automatically adjusting each architecture's influence based on their detection reliability. Our experiments achieved state-of-the-art intra-dataset performance with AUC scores of 99.22% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.06% (FF++) and 99.94% (CelebDF-v2) without augmentation. With augmentation, we achieve AUC scores of 99.47% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.43% (FF++) and 99.95% (CelebDF-v2). The framework demonstrates robust cross-dataset generalization, achieving AUC scores of 88.20% and 72.52%, and F1 scores of 93.16% and 80.62% in cross-dataset evaluations.</li>
</ul>

<h3>Title: Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing</h3>
<ul>
<li><strong>Authors: </strong>Piotr Piƒôkos, R√≥bert Csord√°s, J√ºrgen Schmidhuber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00315">https://arxiv.org/abs/2505.00315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00315">https://arxiv.org/pdf/2505.00315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00315]] Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing(https://arxiv.org/abs/2505.00315)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models highlighted the excessive quadratic cost of self-attention. Despite the significant research efforts, subquadratic attention methods still suffer from inferior performance in practice. We hypothesize that dynamic, learned content-based sparsity can lead to more efficient attention mechanisms. We present Mixture of Sparse Attention (MoSA), a novel approach inspired by Mixture of Experts (MoE) with expert choice routing. MoSA dynamically selects tokens for each attention head, allowing arbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of length $T$, MoSA reduces the computational complexity of each attention head from $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same computational budget, allowing higher specialization. We show that among the tested sparse attention variants, MoSA is the only one that can outperform the dense baseline, sometimes with up to 27% better perplexity for an identical compute budget. MoSA can also reduce the resource usage compared to dense self-attention. Despite using torch implementation without an optimized kernel, perplexity-matched MoSA models are simultaneously faster in wall-clock time, require less memory for training, and drastically reduce the size of the KV-cache compared to the dense transformer baselines.</li>
</ul>

<h3>Title: Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture</h3>
<ul>
<li><strong>Authors: </strong>Tien Comlekoglu, J. Quetzalc√≥atl Toledo-Mar√≠n, Tina Comlekoglu, Douglas W. DeSimone, Shayn M. Peirce, Geoffrey Fox, James A. Glazier</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00316">https://arxiv.org/abs/2505.00316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00316">https://arxiv.org/pdf/2505.00316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00316]] Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture(https://arxiv.org/abs/2505.00316)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The Cellular-Potts model is a powerful and ubiquitous framework for developing computational models for simulating complex multicellular biological systems. Cellular-Potts models (CPMs) are often computationally expensive due to the explicit modeling of interactions among large numbers of individual model agents and diffusive fields described by partial differential equations (PDEs). In this work, we develop a convolutional neural network (CNN) surrogate model using a U-Net architecture that accounts for periodic boundary conditions. We use this model to accelerate the evaluation of a mechanistic CPM previously used to investigate \textit{in vitro} vasculogenesis. The surrogate model was trained to predict 100 computational steps ahead (Monte-Carlo steps, MCS), accelerating simulation evaluations by a factor of 590 times compared to CPM code execution. Over multiple recursive evaluations, our model effectively captures the emergent behaviors demonstrated by the original Cellular-Potts model of such as vessel sprouting, extension and anastomosis, and contraction of vascular lacunae. This approach demonstrates the potential for deep learning to serve as efficient surrogate models for CPM simulations, enabling faster evaluation of computationally expensive CPM of biological processes at greater spatial and temporal scales.</li>
</ul>

<h3>Title: Optimal Vector Compressed Sensing Using James Stein Shrinkage</h3>
<ul>
<li><strong>Authors: </strong>Apratim Dey, David Donoho</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV, eess.SP, stat.CO, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00326">https://arxiv.org/abs/2505.00326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00326">https://arxiv.org/pdf/2505.00326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00326]] Optimal Vector Compressed Sensing Using James Stein Shrinkage(https://arxiv.org/abs/2505.00326)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The trend in modern science and technology is to take vector measurements rather than scalars, ruthlessly scaling to ever higher dimensional vectors. For about two decades now, traditional scalar Compressed Sensing has been synonymous with a Convex Optimization based procedure called Basis Pursuit. In the vector recovery case, the natural tendency is to return to a straightforward vector extension of Basis Pursuit, also based on Convex Optimization. However, Convex Optimization is provably suboptimal, particularly when $B$ is large. In this paper, we propose SteinSense, a lightweight iterative algorithm, which is provably optimal when $B$ is large. It does not have any tuning parameter, does not need any training data, requires zero knowledge of sparsity, is embarrassingly simple to implement, and all of this makes it easily scalable to high vector dimensions. We conduct a massive volume of both real and synthetic experiments that confirm the efficacy of SteinSense, and also provide theoretical justification based on ideas from Approximate Message Passing. Fascinatingly, we discover that SteinSense is quite robust, delivering the same quality of performance on real data, and even under substantial departures from conditions under which existing theory holds.</li>
</ul>

<h3>Title: Communication-Efficient Wireless Federated Fine-Tuning for Large-Scale AI Models</h3>
<ul>
<li><strong>Authors: </strong>Bumjun Kim, Wan Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00333">https://arxiv.org/abs/2505.00333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00333">https://arxiv.org/pdf/2505.00333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00333]] Communication-Efficient Wireless Federated Fine-Tuning for Large-Scale AI Models(https://arxiv.org/abs/2505.00333)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based large language models (LLMs) have achieved remarkable success across various tasks. Yet, fine-tuning such massive models in federated learning (FL) settings poses significant challenges due to resource constraints and communication overhead. Low-Rank Adaptation (LoRA) addresses these issues by training compact, low-rank matrices instead of fully fine-tuning large models. This paper introduces a wireless federated LoRA fine-tuning framework that optimizes both learning performance and communication efficiency. We provide a novel convergence analysis, revealing how LoRA rank and covariance effects influence FL training dynamics. Leveraging these insights, we propose Sparsified Orthogonal Fine-Tuning (\textbf{SOFT}), an adaptive sparsification method that streamlines parameter updates without expensive matrix multiplications and singular value decomposition (SVD) operations. Additionally, we present a Two Stage Federated Algorithm (\textbf{TSFA}) algorithm that pre-determines key parameters offline and dynamically adjusts bandwidth and sparsification online, ensuring efficient training under latency constraints. Experiments on benchmark datasets show that our approach achieves accuracy comparable to ideal scenario models while significantly reducing communication overhead. Our framework thus enables scalable, resource-efficient deployment of large models in real-world wireless FL scenarios.</li>
</ul>

<h3>Title: Quaternion Wavelet-Conditioned Diffusion Models for Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Luigi Sigillo, Christian Bianchi, Danilo Comminiello</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00334">https://arxiv.org/abs/2505.00334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00334">https://arxiv.org/pdf/2505.00334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00334]] Quaternion Wavelet-Conditioned Diffusion Models for Image Super-Resolution(https://arxiv.org/abs/2505.00334)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Image Super-Resolution is a fundamental problem in computer vision with broad applications spacing from medical imaging to satellite analysis. The ability to reconstruct high-resolution images from low-resolution inputs is crucial for enhancing downstream tasks such as object detection and segmentation. While deep learning has significantly advanced SR, achieving high-quality reconstructions with fine-grained details and realistic textures remains challenging, particularly at high upscaling factors. Recent approaches leveraging diffusion models have demonstrated promising results, yet they often struggle to balance perceptual quality with structural fidelity. In this work, we introduce ResQu a novel SR framework that integrates a quaternion wavelet preprocessing framework with latent diffusion models, incorporating a new quaternion wavelet- and time-aware encoder. Unlike prior methods that simply apply wavelet transforms within diffusion models, our approach enhances the conditioning process by exploiting quaternion wavelet embeddings, which are dynamically integrated at different stages of denoising. Furthermore, we also leverage the generative priors of foundation models such as Stable Diffusion. Extensive experiments on domain-specific datasets demonstrate that our method achieves outstanding SR results, outperforming in many cases existing approaches in perceptual quality and standard evaluation metrics. The code will be available after the revision process.</li>
</ul>

<h3>Title: T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Xuyang Guo, Jiayan Huo, Zhenmei Shi, Zhao Song, Jiahao Zhang, Jiale Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00337">https://arxiv.org/abs/2505.00337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00337">https://arxiv.org/pdf/2505.00337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00337]] T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation(https://arxiv.org/abs/2505.00337)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Text-to-video generative models have made significant strides in recent years, producing high-quality videos that excel in both aesthetic appeal and accurate instruction following, and have become central to digital art creation and user engagement online. Yet, despite these advancements, their ability to respect fundamental physical laws remains largely untested: many outputs still violate basic constraints such as rigid-body collisions, energy conservation, and gravitational dynamics, resulting in unrealistic or even misleading content. Existing physical-evaluation benchmarks typically rely on automatic, pixel-level metrics applied to simplistic, life-scenario prompts, and thus overlook both human judgment and first-principles physics. To fill this gap, we introduce \textbf{T2VPhysBench}, a first-principled benchmark that systematically evaluates whether state-of-the-art text-to-video systems, both open-source and commercial, obey twelve core physical laws including Newtonian mechanics, conservation principles, and phenomenological effects. Our benchmark employs a rigorous human evaluation protocol and includes three targeted studies: (1) an overall compliance assessment showing that all models score below 0.60 on average in each law category; (2) a prompt-hint ablation revealing that even detailed, law-specific hints fail to remedy physics violations; and (3) a counterfactual robustness test demonstrating that models often generate videos that explicitly break physical rules when so instructed. The results expose persistent limitations in current architectures and offer concrete insights for guiding future research toward truly physics-aware video generation.</li>
</ul>

<h3>Title: Vehicular Communication Security: Multi-Channel and Multi-Factor Authentication</h3>
<ul>
<li><strong>Authors: </strong>Marco De Vincenzi, Shuyang Sun, Chen Bo Calvin Zhang, Manuel Garcia, Shaozu Ding, Chiara Bodei, Ilaria Matteucci, Dajiang Suo</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00340">https://arxiv.org/abs/2505.00340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00340">https://arxiv.org/pdf/2505.00340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00340]] Vehicular Communication Security: Multi-Channel and Multi-Factor Authentication(https://arxiv.org/abs/2505.00340)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust, extraction</a></li>
<li><strong>Abstract: </strong>Secure and reliable communications are crucial for Intelligent Transportation Systems (ITSs), where Vehicle-to-Infrastructure (V2I) communication plays a key role in enabling mobility-enhancing and safety-critical services. Current V2I authentication relies on credential-based methods over wireless Non-Line-of-Sight (NLOS) channels, leaving them exposed to remote impersonation and proximity attacks. To mitigate these risks, we propose a unified Multi-Channel, Multi-Factor Authentication (MFA) scheme that combines NLOS cryptographic credentials with a Line-of-Sight (LOS) visual channel. Our approach leverages a challenge-response security paradigm: the infrastructure issues challenges and the vehicle's headlights respond by flashing a structured sequence containing encoded security data. Deep learning models on the infrastructure side then decode the embedded information to authenticate the vehicle. Real-world experimental evaluations demonstrate high test accuracy, reaching an average of 95% and 96.6%, respectively, under various lighting, weather, speed, and distance conditions. Additionally, we conducted extensive experiments on three state-of-the-art deep learning models, including detailed ablation studies for decoding the flashing sequence. Our results indicate that the optimal architecture employs a dual-channel design, enabling simultaneous decoding of the flashing sequence and extraction of vehicle spatial and locational features for robust authentication.</li>
</ul>

<h3>Title: Validation of a 24-hour-ahead Prediction model for a Residential Electrical Load under diverse climate</h3>
<ul>
<li><strong>Authors: </strong>Ehtisham Asghar, Martin Hill, Ibrahim Sengor, Conor Lynch, Phan Quang An</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00348">https://arxiv.org/abs/2505.00348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00348">https://arxiv.org/pdf/2505.00348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00348]] Validation of a 24-hour-ahead Prediction model for a Residential Electrical Load under diverse climate(https://arxiv.org/abs/2505.00348)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate household electrical energy demand prediction is essential for effectively managing sustainable Energy Communities. Integrated with the Energy Management System, these communities aim to optimise operational costs. However, most existing forecasting models are region-specific and depend on large datasets, limiting their applicability across different climates and geographical areas. These models often lack flexibility and may not perform well in regions with limited historical data, leading to inaccurate predictions. This paper proposes a global model for 24-hour-ahead hourly electrical energy demand prediction that is designed to perform effectively across diverse climate conditions and datasets. The model's efficiency is demonstrated using data from two distinct regions: Ireland, with a maritime climate and Vietnam, with a tropical climate. Remarkably, the model achieves high accuracy even with a limited dataset spanning only nine months. Its robustness is further validated across different seasons in Ireland (summer and winter) and Vietnam (dry and wet). The proposed model is evaluated against state-of-the-art machine learning and deep learning methods. Simulation results indicate that the model consistently outperforms benchmark models, showcasing its capability to provide reliable forecasts globally, regardless of varying climatic conditions and data availability. This research underscores the model's potential to enhance the efficiency and sustainability of Energy Communities worldwide. The proposed model achieves a Mean Absolute Percentage Error of 8.0% and 4.0% on the full Irish and Vietnamese datasets.</li>
</ul>

<h3>Title: Optimizing Deep Neural Networks using Safety-Guided Self Compression</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Zbeeb, Mariam Salman, Mohammad Bazzi, Ammar Mohanna</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00350">https://arxiv.org/abs/2505.00350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00350">https://arxiv.org/pdf/2505.00350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00350]] Optimizing Deep Neural Networks using Safety-Guided Self Compression(https://arxiv.org/abs/2505.00350)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The deployment of deep neural networks on resource-constrained devices necessitates effective model com- pression strategies that judiciously balance the reduction of model size with the preservation of performance. This study introduces a novel safety-driven quantization framework that leverages preservation sets to systematically prune and quantize neural network weights, thereby optimizing model complexity without compromising accuracy. The proposed methodology is rigorously evaluated on both a convolutional neural network (CNN) and an attention-based language model, demonstrating its applicability across diverse architectural paradigms. Experimental results reveal that our framework achieves up to a 2.5% enhancement in test accuracy relative to the original unquantized models while maintaining 60% of the initial model size. In comparison to conventional quantization techniques, our approach not only augments generalization by eliminating parameter noise and retaining essential weights but also reduces variance, thereby ensuring the retention of critical model features. These findings underscore the efficacy of safety-driven quantization as a robust and reliable strategy for the efficient optimization of deep learn- ing models. The implementation and comprehensive experimental evaluations of our framework are publicly accessible at GitHub.</li>
</ul>

<h3>Title: From GNNs to Trees: Multi-Granular Interpretability for Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Jie Yang, Yuwen Wang, Kaixuan Chen, Tongya Zheng, Yihe Zhou, Zhenbang Xiao, Ji Cao, Mingli Song, Shunyu Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00364">https://arxiv.org/abs/2505.00364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00364">https://arxiv.org/pdf/2505.00364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00364]] From GNNs to Trees: Multi-Granular Interpretability for Graph Neural Networks(https://arxiv.org/abs/2505.00364)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Interpretable Graph Neural Networks (GNNs) aim to reveal the underlying reasoning behind model predictions, attributing their decisions to specific subgraphs that are informative. However, existing subgraph-based interpretable methods suffer from an overemphasis on local structure, potentially overlooking long-range dependencies within the entire graphs. Although recent efforts that rely on graph coarsening have proven beneficial for global interpretability, they inevitably reduce the graphs to a fixed granularity. Such an inflexible way can only capture graph connectivity at a specific level, whereas real-world graph tasks often exhibit relationships at varying granularities (e.g., relevant interactions in proteins span from functional groups, to amino acids, and up to protein domains). In this paper, we introduce a novel Tree-like Interpretable Framework (TIF) for graph classification, where plain GNNs are transformed into hierarchical trees, with each level featuring coarsened graphs of different granularity as tree nodes. Specifically, TIF iteratively adopts a graph coarsening module to compress original graphs (i.e., root nodes of trees) into increasingly coarser ones (i.e., child nodes of trees), while preserving diversity among tree nodes within different branches through a dedicated graph perturbation module. Finally, we propose an adaptive routing module to identify the most informative root-to-leaf paths, providing not only the final prediction but also the multi-granular interpretability for the decision-making process. Extensive experiments on the graph classification benchmarks with both synthetic and real-world datasets demonstrate the superiority of TIF in interpretability, while also delivering a competitive prediction performance akin to the state-of-the-art counterparts.</li>
</ul>

<h3>Title: SacFL: Self-Adaptive Federated Continual Learning for Resource-Constrained End Devices</h3>
<ul>
<li><strong>Authors: </strong>Zhengyi Zhong, Weidong Bao, Ji Wang, Jianguo Chen, Lingjuan Lyu, Wei Yang Bryan Lim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00365">https://arxiv.org/abs/2505.00365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00365">https://arxiv.org/pdf/2505.00365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00365]] SacFL: Self-Adaptive Federated Continual Learning for Resource-Constrained End Devices(https://arxiv.org/abs/2505.00365)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>The proliferation of end devices has led to a distributed computing paradigm, wherein on-device machine learning models continuously process diverse data generated by these devices. The dynamic nature of this data, characterized by continuous changes or data drift, poses significant challenges for on-device models. To address this issue, continual learning (CL) is proposed, enabling machine learning models to incrementally update their knowledge and mitigate catastrophic forgetting. However, the traditional centralized approach to CL is unsuitable for end devices due to privacy and data volume concerns. In this context, federated continual learning (FCL) emerges as a promising solution, preserving user data locally while enhancing models through collaborative updates. Aiming at the challenges of limited storage resources for CL, poor autonomy in task shift detection, and difficulty in coping with new adversarial tasks in FCL scenario, we propose a novel FCL framework named SacFL. SacFL employs an Encoder-Decoder architecture to separate task-robust and task-sensitive components, significantly reducing storage demands by retaining lightweight task-sensitive components for resource-constrained end devices. Moreover, $\rm{SacFL}$ leverages contrastive learning to introduce an autonomous data shift detection mechanism, enabling it to discern whether a new task has emerged and whether it is a benign task. This capability ultimately allows the device to autonomously trigger CL or attack defense strategy without additional information, which is more practical for end devices. Comprehensive experiments conducted on multiple text and image datasets, such as Cifar100 and THUCNews, have validated the effectiveness of $\rm{SacFL}$ in both class-incremental and domain-incremental scenarios. Furthermore, a demo system has been developed to verify its practicality.</li>
</ul>

<h3>Title: KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis</h3>
<ul>
<li><strong>Authors: </strong>JunSeo Kim, HyeHyeon Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00367">https://arxiv.org/abs/2505.00367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00367">https://arxiv.org/pdf/2505.00367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00367]] KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis(https://arxiv.org/abs/2505.00367)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Cognitive distortion refers to negative thinking patterns that can lead to mental health issues like depression and anxiety in adolescents. Previous studies using natural language processing (NLP) have focused mainly on small-scale adult datasets, with limited research on adolescents. This study introduces KoACD, the first large-scale dataset of cognitive distortions in Korean adolescents, containing 108,717 instances. We applied a multi-Large Language Model (LLM) negotiation method to refine distortion classification and generate synthetic data using two approaches: cognitive clarification for textual clarity and cognitive balancing for diverse distortion representation. Validation through LLMs and expert evaluations showed that while LLMs classified distortions with explicit markers, they struggled with context-dependent reasoning, where human evaluators demonstrated higher accuracy. KoACD aims to enhance future research on cognitive distortion detection.</li>
</ul>

<h3>Title: Automated segmenta-on of pediatric neuroblastoma on mul--modal MRI: Results of the SPPIN challenge at MICCAI 2023</h3>
<ul>
<li><strong>Authors: </strong>M.A.D. Buser, D.C. Simons, M. Fitski, M.H.W.A. Wijnen, A.S. Littooij, A.H. ter Brugge, I.N. Vos, M.H.A. Janse, M. de Boer, R. ter Maat, J. Sato, S. Kido, S. Kondo, S. Kasai, M. Wodzinski, H. Muller, J. Ye, J. He, Y. Kirchhoff, M.R. Rokkus, G. Haokai, S. Zitong, M. Fern√°ndez-Pat√≥n, D. Veiga-Canuto, D.G. Ellis, M.R. Aizenberg, B.H.M. van der Velden, H. Kuijf, A. De Luca, A.F.W. van der Steeg</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00369">https://arxiv.org/abs/2505.00369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00369">https://arxiv.org/pdf/2505.00369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00369]] Automated segmenta-on of pediatric neuroblastoma on mul--modal MRI: Results of the SPPIN challenge at MICCAI 2023(https://arxiv.org/abs/2505.00369)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Surgery plays an important role within the treatment for neuroblastoma, a common pediatric cancer. This requires careful planning, often via magnetic resonance imaging (MRI)-based anatomical 3D models. However, creating these models is often time-consuming and user dependent. We organized the Surgical Planning in Pediatric Neuroblastoma (SPPIN) challenge, to stimulate developments on this topic, and set a benchmark for fully automatic segmentation of neuroblastoma on multi-model MRI. The challenge started with a training phase, where teams received 78 sets of MRI scans from 34 patients, consisting of both diagnostic and post-chemotherapy MRI scans. The final test phase, consisting of 18 MRI sets from 9 patients, determined the ranking of the teams. Ranking was based on the Dice similarity coefficient (Dice score), the 95th percentile of the Hausdorff distance (HD95) and the volumetric similarity (VS). The SPPIN challenge was hosted at MICCAI 2023. The final leaderboard consisted of 9 teams. The highest-ranking team achieved a median Dice score 0.82, a median HD95 of 7.69 mm and a VS of 0.91, utilizing a large, pretrained network called STU-Net. A significant difference for the segmentation results between diagnostic and post-chemotherapy MRI scans was observed (Dice = 0.89 vs Dice = 0.59, P = 0.01) for the highest-ranking team. SPPIN is the first medical segmentation challenge in extracranial pediatric oncology. The highest-ranking team used a large pre-trained network, suggesting that pretraining can be of use in small, heterogenous datasets. Although the results of the highest-ranking team were high for most patients, segmentation especially in small, pre-treated tumors were insufficient. Therefore, more reliable segmentation methods are needed to create clinically applicable models to aid surgical planning in pediatric neuroblastoma.</li>
</ul>

<h3>Title: Learning to Estimate Package Delivery Time in Mixed Imbalanced Delivery and Pickup Logistics Services</h3>
<ul>
<li><strong>Authors: </strong>Jinhui Yi, Huan Yan, Haotian Wang, Jian Yuan, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00375">https://arxiv.org/abs/2505.00375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00375">https://arxiv.org/pdf/2505.00375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00375]] Learning to Estimate Package Delivery Time in Mixed Imbalanced Delivery and Pickup Logistics Services(https://arxiv.org/abs/2505.00375)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurately estimating package delivery time is essential to the logistics industry, which enables reasonable work allocation and on-time service guarantee. This becomes even more necessary in mixed logistics scenarios where couriers handle a high volume of delivery and a smaller number of pickup simultaneously. However, most of the related works treat the pickup and delivery patterns on couriers' decision behavior equally, neglecting that the pickup has a greater impact on couriers' decision-making compared to the delivery due to its tighter time constraints. In such context, we have three main challenges: 1) multiple spatiotemporal factors are intricately interconnected, significantly affecting couriers' delivery behavior; 2) pickups have stricter time requirements but are limited in number, making it challenging to model their effects on couriers' delivery process; 3) couriers' spatial mobility patterns are critical determinants of their delivery behavior, but have been insufficiently explored. To deal with these, we propose TransPDT, a Transformer-based multi-task package delivery time prediction model. We first employ the Transformer encoder architecture to capture the spatio-temporal dependencies of couriers' historical travel routes and pending package sets. Then we design the pattern memory to learn the patterns of pickup in the imbalanced dataset via attention mechanism. We also set the route prediction as an auxiliary task of delivery time prediction, and incorporate the prior courier spatial movement regularities in prediction. Extensive experiments on real industry-scale datasets demonstrate the superiority of our method. A system based on TransPDT is deployed internally in JD Logistics to track more than 2000 couriers handling hundreds of thousands of packages per day in Beijing.</li>
</ul>

<h3>Title: Cues3D: Unleashing the Power of Sole NeRF for Consistent and Unique Instances in Open-Vocabulary 3D Panoptic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Feng Xue, Wenzhuang Xu, Guofeng Zhong, Anlong Minga, Nicu Sebe</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00378">https://arxiv.org/abs/2505.00378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00378">https://arxiv.org/pdf/2505.00378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00378]] Cues3D: Unleashing the Power of Sole NeRF for Consistent and Unique Instances in Open-Vocabulary 3D Panoptic Segmentation(https://arxiv.org/abs/2505.00378)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Open-vocabulary 3D panoptic segmentation has recently emerged as a significant trend. Top-performing methods currently integrate 2D segmentation with geometry-aware 3D primitives. However, the advantage would be lost without high-fidelity 3D point clouds, such as methods based on Neural Radiance Field (NeRF). These methods are limited by the insufficient capacity to maintain consistency across partial observations. To address this, recent works have utilized contrastive loss or cross-view association pre-processing for view consensus. In contrast to them, we present Cues3D, a compact approach that relies solely on NeRF instead of pre-associations. The core idea is that NeRF's implicit 3D field inherently establishes a globally consistent geometry, enabling effective object distinction without explicit cross-view supervision. We propose a three-phase training framework for NeRF, initialization-disambiguation-refinement, whereby the instance IDs are corrected using the initially-learned knowledge. Additionally, an instance disambiguation method is proposed to match NeRF-rendered 3D masks and ensure globally unique 3D instance identities. With the aid of Cues3D, we obtain highly consistent and unique 3D instance ID for each object across views with a balanced version of NeRF. Our experiments are conducted on ScanNet v2, ScanNet200, ScanNet++, and Replica datasets for 3D instance, panoptic, and semantic segmentation tasks. Cues3D outperforms other 2D image-based methods and competes with the latest 2D-3D merging based methods, while even surpassing them when using additional 3D point clouds. The code link could be found in the appendix and will be released on \href{this https URL}{github}</li>
</ul>

<h3>Title: The Invisible Threat: Evaluating the Vulnerability of Cross-Spectral Face Recognition to Presentation Attacks</h3>
<ul>
<li><strong>Authors: </strong>Anjith George, Sebastien Marcel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00380">https://arxiv.org/abs/2505.00380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00380">https://arxiv.org/pdf/2505.00380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00380]] The Invisible Threat: Evaluating the Vulnerability of Cross-Spectral Face Recognition to Presentation Attacks(https://arxiv.org/abs/2505.00380)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Cross-spectral face recognition systems are designed to enhance the performance of facial recognition systems by enabling cross-modal matching under challenging operational conditions. A particularly relevant application is the matching of near-infrared (NIR) images to visible-spectrum (VIS) images, enabling the verification of individuals by comparing NIR facial captures acquired with VIS reference images. The use of NIR imaging offers several advantages, including greater robustness to illumination variations, better visibility through glasses and glare, and greater resistance to presentation attacks. Despite these claimed benefits, the robustness of NIR-based systems against presentation attacks has not been systematically studied in the literature. In this work, we conduct a comprehensive evaluation into the vulnerability of NIR-VIS cross-spectral face recognition systems to presentation attacks. Our empirical findings indicate that, although these systems exhibit a certain degree of reliability, they remain vulnerable to specific attacks, emphasizing the need for further research in this area.</li>
</ul>

<h3>Title: CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass</h3>
<ul>
<li><strong>Authors: </strong>Bowen Zhang, Zixin Song, Chunping Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00389">https://arxiv.org/abs/2505.00389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00389">https://arxiv.org/pdf/2505.00389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00389]] CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass(https://arxiv.org/abs/2505.00389)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>As a fundamental task in Information Retrieval and Computational Linguistics, sentence representation has profound implications for a wide range of practical applications such as text clustering, content analysis, question-answering systems, and web search. Recent advances in pre-trained language models (PLMs) have driven remarkable progress in this field, particularly through unsupervised embedding derivation methods centered on discriminative PLMs like BERT. However, due to time and computational constraints, few efforts have attempted to integrate unsupervised sentence representation with generative PLMs, which typically possess much larger parameter sizes. Given that state-of-the-art models in both academia and industry are predominantly based on generative architectures, there is a pressing need for an efficient unsupervised text representation framework tailored to decoder-only PLMs. To address this concern, we propose CSE-SFP, an innovative method that exploits the structural characteristics of generative models. Compared to existing strategies, CSE-SFP requires only a single forward pass to perform effective unsupervised contrastive learning. Rigorous experimentation demonstrates that CSE-SFP not only produces higher-quality embeddings but also significantly reduces both training time and memory consumption. Furthermore, we introduce two ratio metrics that jointly assess alignment and uniformity, thereby providing a more robust means for evaluating the semantic spatial properties of encoding models.</li>
</ul>

<h3>Title: Machine Learning Meets Transparency in Osteoporosis Risk Assessment: A Comparative Study of ML and Explainability Analysis</h3>
<ul>
<li><strong>Authors: </strong>Farhana Elias, Md Shihab Reza, Muhammad Zawad Mahmud, Samiha Islam</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00410">https://arxiv.org/abs/2505.00410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00410">https://arxiv.org/pdf/2505.00410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00410]] Machine Learning Meets Transparency in Osteoporosis Risk Assessment: A Comparative Study of ML and Explainability Analysis(https://arxiv.org/abs/2505.00410)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>The present research tackles the difficulty of predicting osteoporosis risk via machine learning (ML) approaches, emphasizing the use of explainable artificial intelligence (XAI) to improve model transparency. Osteoporosis is a significant public health concern, sometimes remaining untreated owing to its asymptomatic characteristics, and early identification is essential to avert fractures. The research assesses six machine learning classifiers: Random Forest, Logistic Regression, XGBoost, AdaBoost, LightGBM, and Gradient Boosting and utilizes a dataset based on clinical, demographic, and lifestyle variables. The models are refined using GridSearchCV to calibrate hyperparameters, with the objective of enhancing predictive efficacy. XGBoost had the greatest accuracy (91%) among the evaluated models, surpassing others in precision (0.92), recall (0.91), and F1-score (0.90). The research further integrates XAI approaches, such as SHAP, LIME, and Permutation Feature Importance, to elucidate the decision-making process of the optimal model. The study indicates that age is the primary determinant in forecasting osteoporosis risk, followed by hormonal alterations and familial history. These results corroborate clinical knowledge and affirm the models' therapeutic significance. The research underscores the significance of explainability in machine learning models for healthcare applications, guaranteeing that physicians can rely on the system's predictions. The report ultimately proposes directions for further research, such as validation across varied populations and the integration of supplementary biomarkers for enhanced predictive accuracy.</li>
</ul>

<h3>Title: CICADA: Cross-Domain Interpretable Coding for Anomaly Detection and Adaptation in Multivariate Time Series</h3>
<ul>
<li><strong>Authors: </strong>Tian Lan, Yifei Gao, Yimeng Lu, Chen Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00415">https://arxiv.org/abs/2505.00415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00415">https://arxiv.org/pdf/2505.00415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00415]] CICADA: Cross-Domain Interpretable Coding for Anomaly Detection and Adaptation in Multivariate Time Series(https://arxiv.org/abs/2505.00415)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Unsupervised Time series anomaly detection plays a crucial role in applications across industries. However, existing methods face significant challenges due to data distributional shifts across different domains, which are exacerbated by the non-stationarity of time series over time. Existing models fail to generalize under multiple heterogeneous source domains and emerging unseen new target domains. To fill the research gap, we introduce CICADA (Cross-domain Interpretable Coding for Anomaly Detection and Adaptation), with four key innovations: (1) a mixture of experts (MOE) framework that captures domain-agnostic anomaly features with high flexibility and interpretability; (2) a novel selective meta-learning mechanism to prevent negative transfer between dissimilar domains, (3) an adaptive expansion algorithm for emerging heterogeneous domain expansion, and (4) a hierarchical attention structure that quantifies expert contributions during fusion to enhance interpretability this http URL experiments on synthetic and real-world industrial datasets demonstrate that CICADA outperforms state-of-the-art methods in both cross-domain detection performance and interpretability.</li>
</ul>

<h3>Title: Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular Videos</h3>
<ul>
<li><strong>Authors: </strong>Xia Yuan, Hai Yuan, Wenyi Ge, Ying Fu, Xi Wu, Guanyu Xing</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00421">https://arxiv.org/abs/2505.00421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00421">https://arxiv.org/pdf/2505.00421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00421]] Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular Videos(https://arxiv.org/abs/2505.00421)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>High-quality, animatable 3D human avatar reconstruction from monocular videos offers significant potential for reducing reliance on complex hardware, making it highly practical for applications in game development, augmented reality, and social media. However, existing methods still face substantial challenges in capturing fine geometric details and maintaining animation stability, particularly under dynamic or complex poses. To address these issues, we propose a novel real-time framework for animatable human avatar reconstruction based on 2D Gaussian Splatting (2DGS). By leveraging 2DGS and global SMPL pose parameters, our framework not only aligns positional and rotational discrepancies but also enables robust and natural pose-driven animation of the reconstructed avatars. Furthermore, we introduce a Rotation Compensation Network (RCN) that learns rotation residuals by integrating local geometric features with global pose parameters. This network significantly improves the handling of non-rigid deformations and ensures smooth, artifact-free pose transitions during animation. Experimental results demonstrate that our method successfully reconstructs realistic and highly animatable human avatars from monocular videos, effectively preserving fine-grained details while ensuring stable and natural pose variation. Our approach surpasses current state-of-the-art methods in both reconstruction quality and animation robustness on public benchmarks.</li>
</ul>

<h3>Title: Toward Automated Regulatory Decision-Making: Trustworthy Medical Device Risk Classification with Multimodal Transformers and Self-Training</h3>
<ul>
<li><strong>Authors: </strong>Yu Han, Aaron Ceross, Jeroen H.M. Bergmann</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00422">https://arxiv.org/abs/2505.00422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00422">https://arxiv.org/pdf/2505.00422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00422]] Toward Automated Regulatory Decision-Making: Trustworthy Medical Device Risk Classification with Multimodal Transformers and Self-Training(https://arxiv.org/abs/2505.00422)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate classification of medical device risk levels is essential for regulatory oversight and clinical safety. We present a Transformer-based multimodal framework that integrates textual descriptions and visual information to predict device regulatory classification. The model incorporates a cross-attention mechanism to capture intermodal dependencies and employs a self-training strategy for improved generalization under limited supervision. Experiments on a real-world regulatory dataset demonstrate that our approach achieves up to 90.4% accuracy and 97.9% AUROC, significantly outperforming text-only (77.2%) and image-only (54.8%) baselines. Compared to standard multimodal fusion, the self-training mechanism improved SVM performance by 3.3 percentage points in accuracy (from 87.1% to 90.4%) and 1.4 points in macro-F1, suggesting that pseudo-labeling can effectively enhance generalization under limited supervision. Ablation studies further confirm the complementary benefits of both cross-modal attention and self-training.</li>
</ul>

<h3>Title: Leveraging Pretrained Diffusion Models for Zero-Shot Part Assembly</h3>
<ul>
<li><strong>Authors: </strong>Ruiyuan Zhang, Qi Wang, Jiaxiang Liu, Yu Zhang, Yuchi Huo, Chao Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00426">https://arxiv.org/abs/2505.00426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00426">https://arxiv.org/pdf/2505.00426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00426]] Leveraging Pretrained Diffusion Models for Zero-Shot Part Assembly(https://arxiv.org/abs/2505.00426)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>3D part assembly aims to understand part relationships and predict their 6-DoF poses to construct realistic 3D shapes, addressing the growing demand for autonomous assembly, which is crucial for robots. Existing methods mainly estimate the transformation of each part by training neural networks under supervision, which requires a substantial quantity of manually labeled data. However, the high cost of data collection and the immense variability of real-world shapes and parts make traditional methods impractical for large-scale applications. In this paper, we propose first a zero-shot part assembly method that utilizes pre-trained point cloud diffusion models as discriminators in the assembly process, guiding the manipulation of parts to form realistic shapes. Specifically, we theoretically demonstrate that utilizing a diffusion model for zero-shot part assembly can be transformed into an Iterative Closest Point (ICP) process. Then, we propose a novel pushing-away strategy to address the overlap parts, thereby further enhancing the robustness of the method. To verify our work, we conduct extensive experiments and quantitative comparisons to several strong baseline methods, demonstrating the effectiveness of the proposed approach, which even surpasses the supervised learning method. The code has been released on this https URL.</li>
</ul>

<h3>Title: HoneyWin: High-Interaction Windows Honeypot in Enterprise Environment</h3>
<ul>
<li><strong>Authors: </strong>Yan Lin Aung, Yee Loon Khoo, Davis Yang Zheng, Bryan Swee Duo, Sudipta Chattopadhyay, Jianying Zhou, Liming Lu, Weihan Goh</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00465">https://arxiv.org/abs/2505.00465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00465">https://arxiv.org/pdf/2505.00465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00465]] HoneyWin: High-Interaction Windows Honeypot in Enterprise Environment(https://arxiv.org/abs/2505.00465)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Windows operating systems (OS) are ubiquitous in enterprise Information Technology (IT) and operational technology (OT) environments. Due to their widespread adoption and known vulnerabilities, they are often the primary targets of malware and ransomware attacks. With 93% of the ransomware targeting Windows-based systems, there is an urgent need for advanced defensive mechanisms to detect, analyze, and mitigate threats effectively. In this paper, we propose HoneyWin a high-interaction Windows honeypot that mimics an enterprise IT environment. The HoneyWin consists of three Windows 11 endpoints and an enterprise-grade gateway provisioned with comprehensive network traffic capturing, host-based logging, deceptive tokens, endpoint security and real-time alerts capabilities. The HoneyWin has been deployed live in the wild for 34 days and receives more than 5.79 million unsolicited connections, 1.24 million login attempts, 5 and 354 successful logins via remote desktop protocol (RDP) and secure shell (SSH) respectively. The adversary interacted with the deceptive token in one of the RDP sessions and exploited the public-facing endpoint to initiate the Simple Mail Transfer Protocol (SMTP) brute-force bot attack via SSH sessions. The adversary successfully harvested 1,250 SMTP credentials after attempting 151,179 credentials during the attack.</li>
</ul>

<h3>Title: A Generalised Framework for Property-Driven Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Thomas Flinkow, Marco Casadio, Colin Kessler, Rosemary Monahan, Ekaterina Komendantskaya</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00466">https://arxiv.org/abs/2505.00466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00466">https://arxiv.org/pdf/2505.00466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00466]] A Generalised Framework for Property-Driven Machine Learning(https://arxiv.org/abs/2505.00466)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural networks have been shown to frequently fail to satisfy critical safety and correctness properties after training, highlighting the pressing need for training methods that incorporate such properties directly. While adversarial training can be used to improve robustness to small perturbations within $\epsilon$-cubes, domains other than computer vision -- such as control systems and natural language processing -- may require more flexible input region specifications via generalised hyper-rectangles. Meanwhile, differentiable logics offer a way to encode arbitrary logical constraints as additional loss terms that guide the learning process towards satisfying these constraints. In this paper, we investigate how these two complementary approaches can be unified within a single framework for property-driven machine learning. We show that well-known properties from the literature are subcases of this general approach, and we demonstrate its practical effectiveness on a case study involving a neural network controller for a drone system. Our framework is publicly available at this https URL.</li>
</ul>

<h3>Title: Red Teaming Large Language Models for Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Vahid Balazadeh, Michael Cooper, David Pellow, Atousa Assadi, Jennifer Bell, Jim Fackler, Gabriel Funingana, Spencer Gable-Cook, Anirudh Gangadhar, Abhishek Jaiswal, Sumanth Kaja, Christopher Khoury, Randy Lin, Kaden McKeen, Sara Naimimohasses, Khashayar Namdar, Aviraj Newatia, Allan Pang, Anshul Pattoo, Sameer Peesapati, Diana Prepelita, Bogdana Rakova, Saba Sadatamin, Rafael Schulman, Ajay Shah, Syed Azhar Shah, Syed Ahmar Shah, Babak Taati, Balagopal Unnikrishnan, Stephanie Williams, Rahul G Krishnan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00467">https://arxiv.org/abs/2505.00467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00467">https://arxiv.org/pdf/2505.00467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00467]] Red Teaming Large Language Models for Healthcare(https://arxiv.org/abs/2505.00467)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present the design process and findings of the pre-conference workshop at the Machine Learning for Healthcare Conference (2024) entitled Red Teaming Large Language Models for Healthcare, which took place on August 15, 2024. Conference participants, comprising a mix of computational and clinical expertise, attempted to discover vulnerabilities -- realistic clinical prompts for which a large language model (LLM) outputs a response that could cause clinical harm. Red-teaming with clinicians enables the identification of LLM vulnerabilities that may not be recognised by LLM developers lacking clinical expertise. We report the vulnerabilities found, categorise them, and present the results of a replication study assessing the vulnerabilities across all LLMs provided.</li>
</ul>

<h3>Title: Interpretable Spatial-Temporal Fusion Transformers: Multi-Output Prediction for Parametric Dynamical Systems with Time-Varying Inputs</h3>
<ul>
<li><strong>Authors: </strong>Shuwen Sun, Lihong Feng, Peter Benner</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00473">https://arxiv.org/abs/2505.00473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00473">https://arxiv.org/pdf/2505.00473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00473]] Interpretable Spatial-Temporal Fusion Transformers: Multi-Output Prediction for Parametric Dynamical Systems with Time-Varying Inputs(https://arxiv.org/abs/2505.00473)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>We explore the promising performance of a transformer model in predicting outputs of parametric dynamical systems with external time-varying input signals. The outputs of such systems vary not only with physical parameters but also with external time-varying input signals. Accurately catching the dynamics of such systems is challenging. We have adapted and extended an existing transformer model for single output prediction to a multiple-output transformer that is able to predict multiple output responses of these systems. The multiple-output transformer generalizes the interpretability of the original transformer. The generalized interpretable attention weight matrix explores not only the temporal correlations in the sequence, but also the interactions between the multiple outputs, providing explanation for the spatial correlation in the output domain. This multiple-output transformer accurately predicts the sequence of multiple outputs, regardless of the nonlinearity of the system and the dimensionality of the parameter space.</li>
</ul>

<h3>Title: Computational Identification of Regulatory Statements in EU Legislation</h3>
<ul>
<li><strong>Authors: </strong>Gijs Jan Brandsma, Jens Blom-Hansen, Christiaan Meijer, Kody Moodley</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00479">https://arxiv.org/abs/2505.00479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00479">https://arxiv.org/pdf/2505.00479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00479]] Computational Identification of Regulatory Statements in EU Legislation(https://arxiv.org/abs/2505.00479)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Identifying regulatory statements in legislation is useful for developing metrics to measure the regulatory density and strictness of legislation. A computational method is valuable for scaling the identification of such statements from a growing body of EU legislation, constituting approximately 180,000 published legal acts between 1952 and 2023. Past work on extraction of these statements varies in the permissiveness of their definitions for what constitutes a regulatory statement. In this work, we provide a specific definition for our purposes based on the institutional grammar tool. We develop and compare two contrasting approaches for automatically identifying such statements in EU legislation, one based on dependency parsing, and the other on a transformer-based machine learning model. We found both approaches performed similarly well with accuracies of 80% and 84% respectively and a K alpha of 0.58. The high accuracies and not exceedingly high agreement suggests potential for combining strengths of both approaches.</li>
</ul>

<h3>Title: Decentralized Vulnerability Disclosure via Permissioned Blockchain: A Secure, Transparent Alternative to Centralized CVE Management</h3>
<ul>
<li><strong>Authors: </strong>Novruz Amirov, Kemal Bicakci</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00480">https://arxiv.org/abs/2505.00480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00480">https://arxiv.org/pdf/2505.00480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00480]] Decentralized Vulnerability Disclosure via Permissioned Blockchain: A Secure, Transparent Alternative to Centralized CVE Management(https://arxiv.org/abs/2505.00480)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>This paper proposes a decentralized, blockchain-based system for the publication of Common Vulnerabilities and Exposures (CVEs), aiming to mitigate the limitations of the current centralized model primarily overseen by MITRE. The proposed architecture leverages a permissioned blockchain, wherein only authenticated CVE Numbering Authorities (CNAs) are authorized to submit entries. This ensures controlled write access while preserving public transparency. By incorporating smart contracts, the system supports key features such as embargoed disclosures and decentralized governance. We evaluate the proposed model in comparison with existing practices, highlighting its advantages in transparency, trust decentralization, and auditability. A prototype implementation using Hyperledger Fabric is presented to demonstrate the feasibility of the approach, along with a discussion of its implications for the future of vulnerability disclosure.</li>
</ul>

<h3>Title: JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Kwon Byung-Ki, Qi Dai, Lee Hyoseok, Chong Luo, Tae-Hyun Oh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00482">https://arxiv.org/abs/2505.00482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00482">https://arxiv.org/pdf/2505.00482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00482]] JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers(https://arxiv.org/abs/2505.00482)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We present JointDiT, a diffusion transformer that models the joint distribution of RGB and depth. By leveraging the architectural benefit and outstanding image prior of the state-of-the-art diffusion transformer, JointDiT not only generates high-fidelity images but also produces geometrically plausible and accurate depth maps. This solid joint distribution modeling is achieved through two simple yet effective techniques that we propose, i.e., adaptive scheduling weights, which depend on the noise levels of each modality, and the unbalanced timestep sampling strategy. With these techniques, we train our model across all noise levels for each modality, enabling JointDiT to naturally handle various combinatorial generation tasks, including joint generation, depth estimation, and depth-conditioned image generation by simply controlling the timestep of each branch. JointDiT demonstrates outstanding joint generation performance. Furthermore, it achieves comparable results in depth estimation and depth-conditioned image generation, suggesting that joint distribution modeling can serve as a replaceable alternative to conditional generation. The project page is available at this https URL.</li>
</ul>

<h3>Title: Analysis of the vulnerability of machine learning regression models to adversarial attacks using data from 5G wireless networks</h3>
<ul>
<li><strong>Authors: </strong>Leonid Legashev, Artur Zhigalov, Denis Parfenov</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00487">https://arxiv.org/abs/2505.00487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00487">https://arxiv.org/pdf/2505.00487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00487]] Analysis of the vulnerability of machine learning regression models to adversarial attacks using data from 5G wireless networks(https://arxiv.org/abs/2505.00487)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>This article describes the process of creating a script and conducting an analytical study of a dataset using the DeepMIMO emulator. An advertorial attack was carried out using the FGSM method to maximize the gradient. A comparison is made of the effectiveness of binary classifiers in the task of detecting distorted data. The dynamics of changes in the quality indicators of the regression model were analyzed in conditions without adversarial attacks, during an adversarial attack and when the distorted data was isolated. It is shown that an adversarial FGSM attack with gradient maximization leads to an increase in the value of the MSE metric by 33% and a decrease in the R2 indicator by 10% on average. The LightGBM binary classifier effectively identifies data with adversarial anomalies with 98% accuracy. Regression machine learning models are susceptible to adversarial attacks, but rapid analysis of network traffic and data transmitted over the network makes it possible to identify malicious activity</li>
</ul>

<h3>Title: Enhancing Tropical Cyclone Path Forecasting with an Improved Transformer Network</h3>
<ul>
<li><strong>Authors: </strong>Nguyen Van Thanh, Nguyen Dang Huynh, Nguyen Ngoc Tan, Nguyen Thai Minh, Nguyen Nam Hoang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00495">https://arxiv.org/abs/2505.00495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00495">https://arxiv.org/pdf/2505.00495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00495]] Enhancing Tropical Cyclone Path Forecasting with an Improved Transformer Network(https://arxiv.org/abs/2505.00495)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, transformer</a></li>
<li><strong>Abstract: </strong>A storm is a type of extreme weather. Therefore, forecasting the path of a storm is extremely important for protecting human life and property. However, storm forecasting is very challenging because storm trajectories frequently change. In this study, we propose an improved deep learning method using a Transformer network to predict the movement trajectory of a storm over the next 6 hours. The storm data used to train the model was obtained from the National Oceanic and Atmospheric Administration (NOAA) [1]. Simulation results show that the proposed method is more accurate than traditional methods. Moreover, the proposed method is faster and more cost-effective</li>
</ul>

<h3>Title: KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution</h3>
<ul>
<li><strong>Authors: </strong>Antoni Bigata, Rodrigo Mira, Stella Bounareli, Micha≈Ç Stypu≈Çkowski, Konstantinos Vougioukas, Stavros Petridis, Maja Pantic</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00497">https://arxiv.org/abs/2505.00497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00497">https://arxiv.org/pdf/2505.00497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00497]] KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution(https://arxiv.org/abs/2505.00497)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Lip synchronization, known as the task of aligning lip movements in an existing video with new input audio, is typically framed as a simpler variant of audio-driven facial animation. However, as well as suffering from the usual issues in talking head generation (e.g., temporal consistency), lip synchronization presents significant new challenges such as expression leakage from the input video and facial occlusions, which can severely impact real-world applications like automated dubbing, but are often neglected in existing works. To address these shortcomings, we present KeySync, a two-stage framework that succeeds in solving the issue of temporal consistency, while also incorporating solutions for leakage and occlusions using a carefully designed masking strategy. We show that KeySync achieves state-of-the-art results in lip reconstruction and cross-synchronization, improving visual quality and reducing expression leakage according to LipLeak, our novel leakage metric. Furthermore, we demonstrate the effectiveness of our new masking approach in handling occlusions and validate our architectural choices through several ablation studies. Code and model weights can be found at this https URL.</li>
</ul>

<h3>Title: HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection</h3>
<ul>
<li><strong>Authors: </strong>Deanna Emery, Michael Goitia, Freddie Vargus, Iulia Neagu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00506">https://arxiv.org/abs/2505.00506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00506">https://arxiv.org/pdf/2505.00506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00506]] HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection(https://arxiv.org/abs/2505.00506)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are increasingly deployed in high-stakes domains, detecting hallucinated content$\unicode{x2013}$text that is not grounded in supporting evidence$\unicode{x2013}$has become a critical challenge. Existing benchmarks for hallucination detection are often synthetically generated, narrowly focused on extractive question answering, and fail to capture the complexity of real-world scenarios involving multi-document contexts and full-sentence outputs. We introduce the HalluMix Benchmark, a diverse, task-agnostic dataset that includes examples from a range of domains and formats. Using this benchmark, we evaluate seven hallucination detection systems$\unicode{x2013}$both open and closed source$\unicode{x2013}$highlighting differences in performance across tasks, document lengths, and input representations. Our analysis highlights substantial performance disparities between short and long contexts, with critical implications for real-world Retrieval Augmented Generation (RAG) implementations. Quotient Detections achieves the best overall performance, with an accuracy of 0.82 and an F1 score of 0.84.</li>
</ul>

<h3>Title: Self-Ablating Transformers: More Interpretability, Less Sparsity</h3>
<ul>
<li><strong>Authors: </strong>Jeremias Ferrao, Luhan Mikaelson, Keenan Pepper, Natalia Perez-Campanero Antolin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00509">https://arxiv.org/abs/2505.00509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00509">https://arxiv.org/pdf/2505.00509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00509]] Self-Ablating Transformers: More Interpretability, Less Sparsity(https://arxiv.org/abs/2505.00509)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>A growing intuition in machine learning suggests a link between sparsity and interpretability. We introduce a novel self-ablation mechanism to investigate this connection ante-hoc in the context of language transformers. Our approach dynamically enforces a k-winner-takes-all constraint, forcing the model to demonstrate selective activation across neuron and attention units. Unlike post-hoc methods that analyze already-trained models, our approach integrates interpretability directly into model training, promoting feature localization from inception. Training small models on the TinyStories dataset and employing interpretability tests, we find that self-ablation leads to more localized circuits, concentrated feature representations, and increased neuron specialization without compromising language modelling performance. Surprisingly, our method also decreased overall sparsity, indicating that self-ablation promotes specialization rather than widespread inactivity. This reveals a complex interplay between sparsity and interpretability, where decreased global sparsity can coexist with increased local specialization, leading to enhanced interpretability. To facilitate reproducibility, we make our code available at this https URL.</li>
</ul>

<h3>Title: InterLoc: LiDAR-based Intersection Localization using Road Segmentation with Automated Evaluation Method</h3>
<ul>
<li><strong>Authors: </strong>Nguyen Hoang Khoi Tran, Julie Stephany Berrio, Mao Shan, Zhenxing Ming, Stewart Worrall</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00512">https://arxiv.org/abs/2505.00512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00512">https://arxiv.org/pdf/2505.00512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00512]] InterLoc: LiDAR-based Intersection Localization using Road Segmentation with Automated Evaluation Method(https://arxiv.org/abs/2505.00512)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Intersections are geometric and functional key points in every road network. They offer strong landmarks to correct GNSS dropouts and anchor new sensor data in up-to-date maps. Despite that importance, intersection detectors either ignore the rich semantic information already computed onboard or depend on scarce, hand-labeled intersection datasets. To close that gap, this paper presents a LiDAR-based method for intersection detection that (i) fuses semantic road segmentation with vehicle localization to detect intersection candidates in a bird's eye view (BEV) representation and (ii) refines those candidates by analyzing branch topology with a least squares formulation. To evaluate our method, we introduce an automated benchmarking pipeline that pairs detections with OpenStreetMap (OSM) intersection nodes using precise GNSS/INS ground-truth poses. Tested on eight SemanticKITTI sequences, the approach achieves a mean localization error of 1.9 m, 89% precision, and 77% recall at a 5 m tolerance, outperforming the latest learning-based baseline. Moreover, the method is robust to segmentation errors higher than those of the benchmark model, demonstrating its applicability in the real world.</li>
</ul>

<h3>Title: Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in Reinforcement Learning Frameworks</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Wang, Jinbo Bi, Minghu Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00530">https://arxiv.org/abs/2505.00530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00530">https://arxiv.org/pdf/2505.00530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00530]] Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in Reinforcement Learning Frameworks(https://arxiv.org/abs/2505.00530)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>SMILES-based molecule generation has emerged as a powerful approach in drug discovery. Deep reinforcement learning (RL) using large language model (LLM) has been incorporated into the molecule generation process to achieve high matching score in term of likelihood of desired molecule candidates. However, a critical challenge in this approach is catastrophic forgetting during the RL phase, where knowledge such as molecule validity, which often exceeds 99\% during pretraining, significantly deteriorates. Current RL algorithms applied in drug discovery, such as REINVENT, use prior models as anchors to retian pretraining knowledge, but these methods lack robust exploration mechanisms. To address these issues, we propose Partial SMILES Validation-PPO (PSV-PPO), a novel RL algorithm that incorporates real-time partial SMILES validation to prevent catastrophic forgetting while encouraging exploration. Unlike traditional RL approaches that validate molecule structures only after generating entire sequences, PSV-PPO performs stepwise validation at each auto-regressive step, evaluating not only the selected token candidate but also all potential branches stemming from the prior partial sequence. This enables early detection of invalid partial SMILES across all potential paths. As a result, PSV-PPO maintains high validity rates even during aggressive exploration of the vast chemical space. Our experiments on the PMO and GuacaMol benchmark datasets demonstrate that PSV-PPO significantly reduces the number of invalid generated structures while maintaining competitive exploration and optimization performance. While our work primarily focuses on maintaining validity, the framework of PSV-PPO can be extended in future research to incorporate additional forms of valuable domain knowledge, further enhancing reinforcement learning applications in drug discovery.</li>
</ul>

<h3>Title: Test-time Correlation Alignment</h3>
<ul>
<li><strong>Authors: </strong>Linjing You, Jiabao Lu, Xiayuan Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00533">https://arxiv.org/abs/2505.00533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00533">https://arxiv.org/pdf/2505.00533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00533]] Test-time Correlation Alignment(https://arxiv.org/abs/2505.00533)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Deep neural networks often experience performance drops due to distribution shifts between training and test data. Although domain adaptation offers a solution, privacy concerns restrict access to training data in many real-world scenarios. This restriction has spurred interest in Test-Time Adaptation (TTA), which adapts models using only unlabeled test data. However, current TTA methods still face practical challenges: (1) a primary focus on instance-wise alignment, overlooking CORrelation ALignment (CORAL) due to missing source correlations; (2) complex backpropagation operations for model updating, resulting in overhead computation and (3) domain forgetting. To address these challenges, we provide a theoretical analysis to investigate the feasibility of Test-time Correlation Alignment (TCA), demonstrating that correlation alignment between high-certainty instances and test instances can enhance test performances with a theoretical guarantee. Based on this, we propose two simple yet effective algorithms: LinearTCA and LinearTCA+. LinearTCA applies a simple linear transformation to achieve both instance and correlation alignment without additional model updates, while LinearTCA+ serves as a plug-and-play module that can easily boost existing TTA methods. Extensive experiments validate our theoretical insights and show that TCA methods significantly outperforms baselines across various tasks, benchmarks and backbones. Notably, LinearTCA improves adaptation accuracy by 5.88% on OfficeHome dataset, while using only 4% maximum GPU memory usage and 0.6% computation time compared to the best baseline TTA method.</li>
</ul>

<h3>Title: A Robust Deep Networks based Multi-Object MultiCamera Tracking System for City Scale Traffic</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Imran Zaman, Usama Ijaz Bajwa, Gulshan Saleem, Rana Hammad Raza</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00534">https://arxiv.org/abs/2505.00534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00534">https://arxiv.org/pdf/2505.00534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00534]] A Robust Deep Networks based Multi-Object MultiCamera Tracking System for City Scale Traffic(https://arxiv.org/abs/2505.00534)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Vision sensors are becoming more important in Intelligent Transportation Systems (ITS) for traffic monitoring, management, and optimization as the number of network cameras continues to rise. However, manual object tracking and matching across multiple non-overlapping cameras pose significant challenges in city-scale urban traffic scenarios. These challenges include handling diverse vehicle attributes, occlusions, illumination variations, shadows, and varying video resolutions. To address these issues, we propose an efficient and cost-effective deep learning-based framework for Multi-Object Multi-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for object detection and employs Non-Maximum Suppression (NMS) to select target objects from overlapping detections. Transfer learning is employed for re-identification, enabling the association and generation of vehicle tracklets across multiple cameras. Moreover, we leverage appropriate loss functions and distance measures to handle occlusion, illumination, and shadow challenges. The final solution identification module performs feature extraction using ResNet-152 coupled with Deep SORT based vehicle tracking. The proposed framework is evaluated on the 5th AI City Challenge dataset (Track 3), comprising 46 camera feeds. Among these 46 camera streams, 40 are used for model training and validation, while the remaining six are utilized for model testing. The proposed framework achieves competitive performance with an IDF1 score of 0.8289, and precision and recall scores of 0.9026 and 0.8527 respectively, demonstrating its effectiveness in robust and accurate vehicle tracking.</li>
</ul>

<h3>Title: KnowEEG: Explainable Knowledge Driven EEG Classification</h3>
<ul>
<li><strong>Authors: </strong>Amarpal Sahota, Navid Mohammadi Foumani, Raul Santos-Rodriguez, Zahraa S. Abdallah</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00541">https://arxiv.org/abs/2505.00541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00541">https://arxiv.org/pdf/2505.00541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00541]] KnowEEG: Explainable Knowledge Driven EEG Classification(https://arxiv.org/abs/2505.00541)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Electroencephalography (EEG) is a method of recording brain activity that shows significant promise in applications ranging from disease classification to emotion detection and brain-computer interfaces. Recent advances in deep learning have improved EEG classification performance yet model explainability remains an issue. To address this key limitation of explainability we introduce KnowEEG; a novel explainable machine learning approach for EEG classification. KnowEEG extracts a comprehensive set of per-electrode features, filters them using statistical tests, and integrates between-electrode connectivity statistics. These features are then input to our modified Random Forest model (Fusion Forest) that balances per electrode statistics with between electrode connectivity features in growing the trees of the forest. By incorporating knowledge from both the generalized time-series and EEG-specific domains, KnowEEG achieves performance comparable to or exceeding state-of-the-art deep learning models across five different classification tasks: emotion detection, mental workload classification, eyes open/closed detection, abnormal EEG classification, and event detection. In addition to high performance, KnowEEG provides inherent explainability through feature importance scores for understandable features. We demonstrate by example on the eyes closed/open classification task that this explainability can be used to discover knowledge about the classes. This discovered knowledge for eyes open/closed classification was proven to be correct by current neuroscience literature. Therefore, the impact of KnowEEG will be significant for domains where EEG explainability is critical such as healthcare.</li>
</ul>

<h3>Title: Directly Forecasting Belief for Reinforcement Learning with Delays</h3>
<ul>
<li><strong>Authors: </strong>Qingyuan Wu, Yuhui Wang, Simon Sinong Zhan, Yixuan Wang, Chung-Wei Lin, Chen Lv, Qi Zhu, J√ºrgen Schmidhuber, Chao Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00546">https://arxiv.org/abs/2505.00546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00546">https://arxiv.org/pdf/2505.00546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00546]] Directly Forecasting Belief for Reinforcement Learning with Delays(https://arxiv.org/abs/2505.00546)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) with delays is challenging as sensory perceptions lag behind the actual events: the RL agent needs to estimate the real state of its environment based on past observations. State-of-the-art (SOTA) methods typically employ recursive, step-by-step forecasting of states. This can cause the accumulation of compounding errors. To tackle this problem, our novel belief estimation method, named Directly Forecasting Belief Transformer (DFBT), directly forecasts states from observations without incrementally estimating intermediate states step-by-step. We theoretically demonstrate that DFBT greatly reduces compounding errors of existing recursively forecasting methods, yielding stronger performance guarantees. In experiments with D4RL offline datasets, DFBT reduces compounding errors with remarkable prediction accuracy. DFBT's capability to forecast state sequences also facilitates multi-step bootstrapping, thus greatly improving learning efficiency. On the MuJoCo benchmark, our DFBT-based method substantially outperforms SOTA baselines.</li>
</ul>

<h3>Title: 100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chong Zhang, Yue Deng, Xiang Lin, Bin Wang, Dianwen Ng, Hai Ye, Xingxuan Li, Yao Xiao, Zhanfeng Mo, Qi Zhang, Lidong Bing</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00551">https://arxiv.org/abs/2505.00551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00551">https://arxiv.org/pdf/2505.00551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00551]] 100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models(https://arxiv.org/abs/2505.00551)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The recent development of reasoning language models (RLMs) represents a novel evolution in large language models. In particular, the recent release of DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in the research community for exploring the explicit reasoning paradigm of language models. However, the implementation details of the released models have not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero, DeepSeek-R1, and the distilled small models. As a result, many replication studies have emerged aiming to reproduce the strong performance achieved by DeepSeek-R1, reaching comparable performance through similar training procedures and fully open-source data resources. These works have investigated feasible strategies for supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR), focusing on data preparation and method design, yielding various valuable insights. In this report, we provide a summary of recent replication studies to inspire future research. We primarily focus on SFT and RLVR as two main directions, introducing the details for data construction, method design and training procedure of current replication studies. Moreover, we conclude key findings from the implementation details and experimental results reported by these studies, anticipating to inspire future research. We also discuss additional techniques of enhancing RLMs, highlighting the potential of expanding the application scope of these models, and discussing the challenges in development. By this survey, we aim to help researchers and developers of RLMs stay updated with the latest advancements, and seek to inspire new ideas to further enhance RLMs.</li>
</ul>

<h3>Title: Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Makoto Sato</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00557">https://arxiv.org/abs/2505.00557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00557">https://arxiv.org/pdf/2505.00557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00557]] Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models(https://arxiv.org/abs/2505.00557)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hallucinations in large language models (LLMs) present a growing challenge across real-world applications, from healthcare to law, where factual reliability is essential. Despite advances in alignment and instruction tuning, LLMs can still generate outputs that are fluent yet fundamentally untrue. Understanding the cognitive dynamics that underlie these hallucinations remains an open problem. In this study, we propose a prompt-based framework to systematically trigger and quantify hallucination: a Hallucination-Inducing Prompt (HIP), which synthetically fuses semantically distant concepts (e.g., periodic table of elements and tarot divination) in a misleading way, and a Hallucination Quantifying Prompt (HQP), which scores the plausibility, confidence, and coherence of the output. Controlled experiments across multiple LLMs revealed that HIPs consistently produced less coherent and more hallucinated responses than their null-fusion controls. These effects varied across models, with reasoning-oriented LLMs showing distinct profiles from general-purpose ones. Our framework provides a reproducible testbed for studying hallucination vulnerability, and opens the door to developing safer, more introspective LLMs that can detect and self-regulate the onset of conceptual instability.</li>
</ul>

<h3>Title: X-ray illicit object detection using hybrid CNN-transformer neural network architectures</h3>
<ul>
<li><strong>Authors: </strong>Jorgen Cani, Christos Diou, Spyridon Evangelatos, Panagiotis Radoglou-Grammatikis, Vasileios Argyriou, Panagiotis Sarigiannidis, Iraklis Varlamis, Georgios Th. Papadopoulos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00564">https://arxiv.org/abs/2505.00564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00564">https://arxiv.org/pdf/2505.00564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00564]] X-ray illicit object detection using hybrid CNN-transformer neural network architectures(https://arxiv.org/abs/2505.00564)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, transformer</a></li>
<li><strong>Abstract: </strong>In the field of X-ray security applications, even the smallest details can significantly impact outcomes. Objects that are heavily occluded or intentionally concealed pose a great challenge for detection, whether by human observation or through advanced technological applications. While certain Deep Learning (DL) architectures demonstrate strong performance in processing local information, such as Convolutional Neural Networks (CNNs), others excel in handling distant information, e.g., transformers. In X-ray security imaging the literature has been dominated by the use of CNN-based methods, while the integration of the two aforementioned leading architectures has not been sufficiently explored. In this paper, various hybrid CNN-transformer architectures are evaluated against a common CNN object detection baseline, namely YOLOv8. In particular, a CNN (HGNetV2) and a hybrid CNN-transformer (Next-ViT-S) backbone are combined with different CNN/transformer detection heads (YOLOv8 and RT-DETR). The resulting architectures are comparatively evaluated on three challenging public X-ray inspection datasets, namely EDS, HiXray, and PIDray. Interestingly, while the YOLOv8 detector with its default backbone (CSP-DarkNet53) is generally shown to be advantageous on the HiXray and PIDray datasets, when a domain distribution shift is incorporated in the X-ray images (as happens in the EDS datasets), hybrid CNN-transformer architectures exhibit increased robustness. Detailed comparative evaluation results, including object-level detection performance and object-size error analysis, demonstrate the strengths and weaknesses of each architectural combination and suggest guidelines for future research. The source code and network weights of the models employed in this study are available at this https URL.</li>
</ul>

<h3>Title: FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension</h3>
<ul>
<li><strong>Authors: </strong>Jushi Kai, Boyi Zeng, Yixuan Wang, Haoli Bai, Bo Jiang, Zhouhan Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00570">https://arxiv.org/abs/2505.00570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00570">https://arxiv.org/pdf/2505.00570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00570]] FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension(https://arxiv.org/abs/2505.00570)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Extending the context window in large language models (LLMs) is essential for applications involving long-form content generation. However, the linear increase in key-value (KV) cache memory requirements and the quadratic complexity of self-attention with respect to sequence length present significant challenges during fine-tuning and inference. Existing methods suffer from performance degradation when extending to longer contexts. In this work, we introduce a novel context extension method that optimizes both fine-tuning and inference efficiency. Our method exploits a key observation: in the frequency domain, the energy distribution of the KV cache is primarily concentrated in low-frequency components. By filtering out the high-frequency components, the KV cache can be effectively compressed with minimal information loss. Building on this insight, we propose an efficient compression technique, FreqKV, that iteratively compresses the increasing KV cache to a fixed size in the frequency domain, applicable to both fine-tuning and inference. FreqKV introduces no additional parameters or architectural modifications. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window efficiently. Experiments on various long context language modeling and understanding tasks demonstrate the efficiency and efficacy of the proposed method.</li>
</ul>

<h3>Title: Block Circulant Adapter for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Ding, Meiqi Wang, Siyu Liao, Zhongfeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00582">https://arxiv.org/abs/2505.00582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00582">https://arxiv.org/pdf/2505.00582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00582]] Block Circulant Adapter for Large Language Models(https://arxiv.org/abs/2505.00582)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) is difficult due to their huge model size. Recent Fourier domain-based methods show potential for reducing fine-tuning costs. We propose a block circulant matrix-based fine-tuning method with a stable training heuristic to leverage the properties of circulant matrices and one-dimensional Fourier transforms to reduce storage and computation costs. Experiments show that our method uses $14\times$ less number of parameters than VeRA, $16\times$ smaller than LoRA and $32\times$ less FLOPs than FourierFT, while maintaining close or better task performance. Our approach presents a promising way in frequency domain to fine-tune large models on downstream tasks.</li>
</ul>

<h3>Title: Synthesizing and Identifying Noise Levels in Autonomous Vehicle Camera Radar Datasets</h3>
<ul>
<li><strong>Authors: </strong>Mathis Morales, Golnaz Habibi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00584">https://arxiv.org/abs/2505.00584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00584">https://arxiv.org/pdf/2505.00584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00584]] Synthesizing and Identifying Noise Levels in Autonomous Vehicle Camera Radar Datasets(https://arxiv.org/abs/2505.00584)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Detecting and tracking objects is a crucial component of any autonomous navigation method. For the past decades, object detection has yielded promising results using neural networks on various datasets. While many methods focus on performance metrics, few projects focus on improving the robustness of these detection and tracking pipelines, notably to sensor failures. In this paper we attempt to address this issue by creating a realistic synthetic data augmentation pipeline for camera-radar Autonomous Vehicle (AV) datasets. Our goal is to accurately simulate sensor failures and data deterioration due to real-world interferences. We also present our results of a baseline lightweight Noise Recognition neural network trained and tested on our augmented dataset, reaching an overall recognition accuracy of 54.4\% on 11 categories across 10086 images and 2145 radar point-clouds.</li>
</ul>

<h3>Title: Unlocking the Potential of Linear Networks for Irregular Multivariate Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Chengsen Wang, Qi Qi, Jingyu Wang, Haifeng Sun, Zirui Zhuang, Jianxin Liao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00590">https://arxiv.org/abs/2505.00590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00590">https://arxiv.org/pdf/2505.00590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00590]] Unlocking the Potential of Linear Networks for Irregular Multivariate Time Series Forecasting(https://arxiv.org/abs/2505.00590)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Time series forecasting holds significant importance across various industries, including finance, transportation, energy, healthcare, and climate. Despite the widespread use of linear networks due to their low computational cost and effectiveness in modeling temporal dependencies, most existing research has concentrated on regularly sampled and fully observed multivariate time series. However, in practice, we frequently encounter irregular multivariate time series characterized by variable sampling intervals and missing values. The inherent intra-series inconsistency and inter-series asynchrony in such data hinder effective modeling and forecasting with traditional linear networks relying on static weights. To tackle these challenges, this paper introduces a novel model named AiT. AiT utilizes an adaptive linear network capable of dynamically adjusting weights according to observation time points to address intra-series inconsistency, thereby enhancing the accuracy of temporal dependencies modeling. Furthermore, by incorporating the Transformer module on variable semantics embeddings, AiT efficiently captures variable correlations, avoiding the challenge of inter-series asynchrony. Comprehensive experiments across four benchmark datasets demonstrate the superiority of AiT, improving prediction accuracy by 11% and decreasing runtime by 52% compared to existing state-of-the-art methods.</li>
</ul>

<h3>Title: Uncertainty-Aware Multi-Expert Knowledge Distillation for Imbalanced Disease Grading</h3>
<ul>
<li><strong>Authors: </strong>Shuo Tong, Shangde Gao, Ke Liu, Zihang Huang, Hongxia Xu, Haochao Ying, Jian Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00592">https://arxiv.org/abs/2505.00592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00592">https://arxiv.org/pdf/2505.00592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00592]] Uncertainty-Aware Multi-Expert Knowledge Distillation for Imbalanced Disease Grading(https://arxiv.org/abs/2505.00592)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Automatic disease image grading is a significant application of artificial intelligence for healthcare, enabling faster and more accurate patient assessments. However, domain shifts, which are exacerbated by data imbalance, introduce bias into the model, posing deployment difficulties in clinical applications. To address the problem, we propose a novel \textbf{U}ncertainty-aware \textbf{M}ulti-experts \textbf{K}nowledge \textbf{D}istillation (UMKD) framework to transfer knowledge from multiple expert models to a single student model. Specifically, to extract discriminative features, UMKD decouples task-agnostic and task-specific features with shallow and compact feature alignment in the feature space. At the output space, an uncertainty-aware decoupled distillation (UDD) mechanism dynamically adjusts knowledge transfer weights based on expert model uncertainties, ensuring robust and reliable distillation. Additionally, UMKD also tackles the problems of model architecture heterogeneity and distribution discrepancies between source and target domains, which are inadequately tackled by previous KD approaches. Extensive experiments on histology prostate grading (\textit{SICAPv2}) and fundus image grading (\textit{APTOS}) demonstrate that UMKD achieves a new state-of-the-art in both source-imbalanced and target-imbalanced scenarios, offering a robust and practical solution for real-world disease image grading.</li>
</ul>

<h3>Title: A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security and Privacy in IoT and Edge Networks</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Shahbaz Khan, Ahmed Al-Dubai, Jawad Ahmad, Nikolaos Pitropakis, Baraq Ghaleb</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00593">https://arxiv.org/abs/2505.00593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00593">https://arxiv.org/pdf/2505.00593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00593]] A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security and Privacy in IoT and Edge Networks(https://arxiv.org/abs/2505.00593)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, segmentation</a></li>
<li><strong>Abstract: </strong>The security of image data in the Internet of Things (IoT) and edge networks is crucial due to the increasing deployment of intelligent systems for real-time decision-making. Traditional encryption algorithms such as AES and RSA are computationally expensive for resource-constrained IoT devices and ineffective for large-volume image data, leading to inefficiencies in privacy-preserving distributed learning applications. To address these concerns, this paper proposes a novel Feature-Aware Chaotic Image Encryption scheme that integrates Feature-Aware Pixel Segmentation (FAPS) with Chaotic Chain Permutation and Confusion mechanisms to enhance security while maintaining efficiency. The proposed scheme consists of three stages: (1) FAPS, which extracts and reorganizes pixels based on high and low edge intensity features for correlation disruption; (2) Chaotic Chain Permutation, which employs a logistic chaotic map with SHA-256-based dynamically updated keys for block-wise permutation; and (3) Chaotic chain Confusion, which utilises dynamically generated chaotic seed matrices for bitwise XOR operations. Extensive security and performance evaluations demonstrate that the proposed scheme significantly reduces pixel correlation -- almost zero, achieves high entropy values close to 8, and resists differential cryptographic attacks. The optimum design of the proposed scheme makes it suitable for real-time deployment in resource-constrained environments.</li>
</ul>

<h3>Title: Fast and Low-Cost Genomic Foundation Models via Outlier Removal</h3>
<ul>
<li><strong>Authors: </strong>Haozheng Luo, Chenghao Qiu, Maojiang Su, Zhihan Zhou, Zoe Mehta, Guo Ye, Jerry Yao-Chieh Hu, Han Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00598">https://arxiv.org/abs/2505.00598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00598">https://arxiv.org/pdf/2505.00598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00598]] Fast and Low-Cost Genomic Foundation Models via Outlier Removal(https://arxiv.org/abs/2505.00598)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>We propose the first unified adversarial attack benchmark for Genomic Foundation Models (GFMs), named GERM. Unlike existing GFM benchmarks, GERM offers the first comprehensive evaluation framework to systematically assess the vulnerability of GFMs to adversarial attacks. Methodologically, we evaluate the adversarial robustness of five state-of-the-art GFMs using four widely adopted attack algorithms and three defense strategies. Importantly, our benchmark provides an accessible and comprehensive framework to analyze GFM vulnerabilities with respect to model architecture, quantization schemes, and training datasets. Empirically, transformer-based models exhibit greater robustness to adversarial perturbations compared to HyenaDNA, highlighting the impact of architectural design on vulnerability. Moreover, adversarial attacks frequently target biologically significant genomic regions, suggesting that these models effectively capture meaningful sequence features.</li>
</ul>

<h3>Title: Visual Trajectory Prediction of Vessels for Inland Navigation</h3>
<ul>
<li><strong>Authors: </strong>Alexander Puzicha, Konstantin W√ºstefeld, Kathrin Wilms, Frank Weichert</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00599">https://arxiv.org/abs/2505.00599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00599">https://arxiv.org/pdf/2505.00599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00599]] Visual Trajectory Prediction of Vessels for Inland Navigation(https://arxiv.org/abs/2505.00599)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The future of inland navigation increasingly relies on autonomous systems and remote operations, emphasizing the need for accurate vessel trajectory prediction. This study addresses the challenges of video-based vessel tracking and prediction by integrating advanced object detection methods, Kalman filters, and spline-based interpolation. However, existing detection systems often misclassify objects in inland waterways due to complex surroundings. A comparative evaluation of tracking algorithms, including BoT-SORT, Deep OC-SORT, and ByeTrack, highlights the robustness of the Kalman filter in providing smoothed trajectories. Experimental results from diverse scenarios demonstrate improved accuracy in predicting vessel movements, which is essential for collision avoidance and situational awareness. The findings underline the necessity of customized datasets and models for inland navigation. Future work will expand the datasets and incorporate vessel classification to refine predictions, supporting both autonomous systems and human operators in complex environments.</li>
</ul>

<h3>Title: Pixel3DMM: Versatile Screen-Space Priors for Single-Image 3D Face Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Simon Giebenhain, Tobias Kirschstein, Martin R√ºnz, Lourdes Agapito, Matthias Nie√üner</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00615">https://arxiv.org/abs/2505.00615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00615">https://arxiv.org/pdf/2505.00615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00615]] Pixel3DMM: Versatile Screen-Space Priors for Single-Image 3D Face Reconstruction(https://arxiv.org/abs/2505.00615)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We address the 3D reconstruction of human faces from a single RGB image. To this end, we propose Pixel3DMM, a set of highly-generalized vision transformers which predict per-pixel geometric cues in order to constrain the optimization of a 3D morphable face model (3DMM). We exploit the latent features of the DINO foundation model, and introduce a tailored surface normal and uv-coordinate prediction head. We train our model by registering three high-quality 3D face datasets against the FLAME mesh topology, which results in a total of over 1,000 identities and 976K images. For 3D face reconstruction, we propose a FLAME fitting opitmization that solves for the 3DMM parameters from the uv-coordinate and normal estimates. To evaluate our method, we introduce a new benchmark for single-image face reconstruction, which features high diversity facial expressions, viewing angles, and ethnicities. Crucially, our benchmark is the first to evaluate both posed and neutral facial geometry. Ultimately, our method outperforms the most competitive baselines by over 15% in terms of geometric accuracy for posed facial expressions.</li>
</ul>

<h3>Title: RevealNet: Distributed Traffic Correlation for Attack Attribution on Programmable Networks</h3>
<ul>
<li><strong>Authors: </strong>Gurjot Singh, Alim Dhanani, Diogo Barradas</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00618">https://arxiv.org/abs/2505.00618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00618">https://arxiv.org/pdf/2505.00618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00618]] RevealNet: Distributed Traffic Correlation for Attack Attribution on Programmable Networks(https://arxiv.org/abs/2505.00618)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Network attackers have increasingly resorted to proxy chains, VPNs, and anonymity networks to conceal their activities. To tackle this issue, past research has explored the applicability of traffic correlation techniques to perform attack attribution, i.e., to identify an attacker's true network location. However, current traffic correlation approaches rely on well-provisioned and centralized systems that ingest flows from multiple network probes to compute correlation scores. Unfortunately, this makes correlation efforts scale poorly for large high-speed networks. In this paper, we propose RevealNet, a decentralized framework for attack attribution that orchestrates a fleet of P4-programmable switches to perform traffic correlation. RevealNet builds on a set of correlation primitives inspired by prior work on computing and comparing flow sketches -- compact summaries of flows' key characteristics -- to enable efficient, distributed, in-network traffic correlation. Our evaluation suggests that RevealNet achieves comparable accuracy to centralized attack attribution systems while significantly reducing both the computational complexity and bandwidth overheads imposed by correlation tasks.</li>
</ul>

<h3>Title: FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation</h3>
<ul>
<li><strong>Authors: </strong>Chaitali Bhattacharyya, Yeseong Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00624">https://arxiv.org/abs/2505.00624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00624">https://arxiv.org/pdf/2505.00624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00624]] FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation(https://arxiv.org/abs/2505.00624)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Training large language models (LLMs) from scratch requires significant computational resources, driving interest in developing smaller, domain-specific LLMs that maintain both efficiency and strong task performance. Medium-sized models such as LLaMA, llama} have served as starting points for domain-specific adaptation, but they often suffer from accuracy degradation when tested on specialized datasets. We introduce FineScope, a framework for deriving compact, domain-optimized LLMs from larger pretrained models. FineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its ability to produce interpretable feature representations, to extract domain-specific subsets from large datasets. We apply structured pruning with domain-specific constraints, ensuring that the resulting pruned models retain essential knowledge for the target domain. To further enhance performance, these pruned models undergo self-data distillation, leveraging SAE-curated datasets to restore key domain-specific information lost during pruning. Extensive experiments and ablation studies demonstrate that FineScope achieves highly competitive performance, outperforming several large-scale state-of-the-art LLMs in domain-specific tasks. Additionally, our results show that FineScope enables pruned models to regain a substantial portion of their original performance when fine-tuned with SAE-curated datasets. Furthermore, applying these datasets to fine-tune pretrained LLMs without pruning also improves their domain-specific accuracy, highlighting the robustness of our approach. The code will be released.</li>
</ul>

<h3>Title: The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)</h3>
<ul>
<li><strong>Authors: </strong>Zihao Wang, Yibo Jiang, Jiahao Yu, Heqing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00626">https://arxiv.org/abs/2505.00626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00626">https://arxiv.org/pdf/2505.00626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00626]] The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)(https://arxiv.org/abs/2505.00626)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role -- a concept we call \emph{role separation} -- is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine \emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens. Through a \emph{simple, controlled experimental framework}, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing \emph{invariant signals} that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, manipulating position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers.</li>
</ul>

<h3>Title: Brain Foundation Models with Hypergraph Dynamic Adapter for Brain Disease Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zhongying Deng, Haoyu Wang, Ziyan Huang, Lipei Zhang, Angelica I. Aviles-Rivero, Chaoyu Liu, Junjun He, Zoe Kourtzi, Carola-Bibiane Sch√∂nlieb</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00627">https://arxiv.org/abs/2505.00627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00627">https://arxiv.org/pdf/2505.00627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00627]] Brain Foundation Models with Hypergraph Dynamic Adapter for Brain Disease Analysis(https://arxiv.org/abs/2505.00627)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Brain diseases, such as Alzheimer's disease and brain tumors, present profound challenges due to their complexity and societal impact. Recent advancements in brain foundation models have shown significant promise in addressing a range of brain-related tasks. However, current brain foundation models are limited by task and data homogeneity, restricted generalization beyond segmentation or classification, and inefficient adaptation to diverse clinical tasks. In this work, we propose SAM-Brain3D, a brain-specific foundation model trained on over 66,000 brain image-label pairs across 14 MRI sub-modalities, and Hypergraph Dynamic Adapter (HyDA), a lightweight adapter for efficient and effective downstream adaptation. SAM-Brain3D captures detailed brain-specific anatomical and modality priors for segmenting diverse brain targets and broader downstream tasks. HyDA leverages hypergraphs to fuse complementary multi-modal data and dynamically generate patient-specific convolutional kernels for multi-scale feature fusion and personalized patient-wise adaptation. Together, our framework excels across a broad spectrum of brain disease segmentation and classification tasks. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art approaches, offering a new paradigm for brain disease analysis through multi-modal, multi-scale, and dynamic foundation modeling.</li>
</ul>

<h3>Title: Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques, Applications and Outlook</h3>
<ul>
<li><strong>Authors: </strong>Muyi Bao, Shuchang Lyu, Zhaoyang Xu, Huiyu Zhou, Jinchang Ren, Shiming Xiang, Xiangtai Li, Guangliang Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00630">https://arxiv.org/abs/2505.00630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00630">https://arxiv.org/pdf/2505.00630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00630]] Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques, Applications and Outlook(https://arxiv.org/abs/2505.00630)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Deep learning has profoundly transformed remote sensing, yet prevailing architectures like Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) remain constrained by critical trade-offs: CNNs suffer from limited receptive fields, while ViTs grapple with quadratic computational complexity, hindering their scalability for high-resolution remote sensing data. State Space Models (SSMs), particularly the recently proposed Mamba architecture, have emerged as a paradigm-shifting solution, combining linear computational scaling with global context modeling. This survey presents a comprehensive review of Mamba-based methodologies in remote sensing, systematically analyzing about 120 studies to construct a holistic taxonomy of innovations and applications. Our contributions are structured across five dimensions: (i) foundational principles of vision Mamba architectures, (ii) micro-architectural advancements such as adaptive scan strategies and hybrid SSM formulations, (iii) macro-architectural integrations, including CNN-Transformer-Mamba hybrids and frequency-domain adaptations, (iv) rigorous benchmarking against state-of-the-art methods in multiple application tasks, such as object detection, semantic segmentation, change detection, etc. and (v) critical analysis of unresolved challenges with actionable future directions. By bridging the gap between SSM theory and remote sensing practice, this survey establishes Mamba as a transformative framework for remote sensing analysis. To our knowledge, this paper is the first systematic review of Mamba architectures in remote sensing. Our work provides a structured foundation for advancing research in remote sensing systems through SSM-based methods. We curate an open-source repository (this https URL) to foster community-driven advancements.</li>
</ul>

<h3>Title: OmicsCL: Unsupervised Contrastive Learning for Cancer Subtype Discovery and Survival Stratification</h3>
<ul>
<li><strong>Authors: </strong>Atahan Karagoz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.GN, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00650">https://arxiv.org/abs/2505.00650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00650">https://arxiv.org/pdf/2505.00650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00650]] OmicsCL: Unsupervised Contrastive Learning for Cancer Subtype Discovery and Survival Stratification(https://arxiv.org/abs/2505.00650)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Unsupervised learning of disease subtypes from multi-omics data presents a significant opportunity for advancing personalized medicine. We introduce OmicsCL, a modular contrastive learning framework that jointly embeds heterogeneous omics modalities-such as gene expression, DNA methylation, and miRNA expression-into a unified latent space. Our method incorporates a survival-aware contrastive loss that encourages the model to learn representations aligned with survival-related patterns, without relying on labeled outcomes. Evaluated on the TCGA BRCA dataset, OmicsCL uncovers clinically meaningful clusters and achieves strong unsupervised concordance with patient survival. The framework demonstrates robustness across hyperparameter configurations and can be tuned to prioritize either subtype coherence or survival stratification. Ablation studies confirm that integrating survival-aware loss significantly enhances the predictive power of learned embeddings. These results highlight the promise of contrastive objectives for biological insight discovery in high-dimensional, heterogeneous omics data.</li>
</ul>

<h3>Title: Large Language Models Understanding: an Inherent Ambiguity Barrier</h3>
<ul>
<li><strong>Authors: </strong>Daniel N. Nissani (Nissensohn)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00654">https://arxiv.org/abs/2505.00654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00654">https://arxiv.org/pdf/2505.00654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00654]] Large Language Models Understanding: an Inherent Ambiguity Barrier(https://arxiv.org/abs/2505.00654)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A lively ongoing debate is taking place, since the extraordinary emergence of Large Language Models (LLMs) with regards to their capability to understand the world and capture the meaning of the dialogues in which they are involved. Arguments and counter-arguments have been proposed based upon thought experiments, anecdotal conversations between LLMs and humans, statistical linguistic analysis, philosophical considerations, and more. In this brief paper we present a counter-argument based upon a thought experiment and semi-formal considerations leading to an inherent ambiguity barrier which prevents LLMs from having any understanding of what their amazingly fluent dialogues mean.</li>
</ul>

<h3>Title: On the generalization of language models from in-context learning and finetuning: a controlled study</h3>
<ul>
<li><strong>Authors: </strong>Andrew K. Lampinen, Arslan Chaudhry, Stephanie C.Y. Chan, Cody Wild, Diane Wan, Alex Ku, J√∂rg Bornschein, Razvan Pascanu, Murray Shanahan, James L. McClelland</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00661">https://arxiv.org/abs/2505.00661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00661">https://arxiv.org/pdf/2505.00661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00661]] On the generalization of language models from in-context learning and finetuning: a controlled study(https://arxiv.org/abs/2505.00661)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning -- from failing to generalize to simple reversals of relations they are trained on, to missing logical deductions that can be made from trained information. These failures to generalize from fine-tuning can hinder practical application of these models. However, language models' in-context learning shows different inductive biases, and can generalize better in some of these cases. Here, we explore these differences in generalization between in-context- and fine-tuning-based learning. To do so, we constructed several novel datasets to evaluate and improve models' ability to generalize from finetuning data. The datasets are constructed to isolate the knowledge in the dataset from that in pretraining, to create clean tests of generalization. We expose pretrained large models to controlled subsets of the information in these datasets -- either in context, or through fine-tuning -- and evaluate their performance on test sets that require various types of generalization. We find overall that in data-matched settings, in-context learning can generalize more flexibly than fine-tuning (though we also find some qualifications of prior findings, such as cases when fine-tuning can generalize to reversals embedded in a larger structure of knowledge). We build on these findings to propose a method to enable improved generalization from fine-tuning: adding in-context inferences to finetuning data. We show that this method improves generalization across various splits of our datasets and other benchmarks. Our results have implications for understanding the inductive biases of different modes of learning in language models, and practically improving their performance.</li>
</ul>

<h3>Title: DeepCritic: Deliberate Critique with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenkai Yang, Jingwen Chen, Yankai Lin, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00662">https://arxiv.org/abs/2505.00662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00662">https://arxiv.org/pdf/2505.00662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00662]] DeepCritic: Deliberate Critique with Large Language Models(https://arxiv.org/abs/2505.00662)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback.</li>
</ul>

<h3>Title: Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Yiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai Wong, Jeff Z. Pan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00675">https://arxiv.org/abs/2505.00675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00675">https://arxiv.org/pdf/2505.00675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00675]] Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions(https://arxiv.org/abs/2505.00675)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Memory is a fundamental component of AI systems, underpinning large language models (LLMs) based agents. While prior surveys have focused on memory applications with LLMs, they often overlook the atomic operations that underlie memory dynamics. In this survey, we first categorize memory representations into parametric, contextual structured, and contextual unstructured and then introduce six fundamental memory operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and Compression. We systematically map these operations to the most relevant research topics across long-term, long-context, parametric modification, and multi-source memory. By reframing memory systems through the lens of atomic operations and representation types, this survey provides a structured and dynamic perspective on research, benchmark datasets, and tools related to memory in AI, clarifying the functional interplay in LLMs based agents while outlining promising directions for future research\footnote{The paper list, datasets, methods and tools are available at \href{this https URL}{this https URL\_Memory\_in\_AI}.}.</li>
</ul>

<h3>Title: Steering Large Language Models with Register Analysis for Arbitrary Style Transfer</h3>
<ul>
<li><strong>Authors: </strong>Xinchen Yang, Marine Carpuat</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00679">https://arxiv.org/abs/2505.00679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00679">https://arxiv.org/pdf/2505.00679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00679]] Steering Large Language Models with Register Analysis for Arbitrary Style Transfer(https://arxiv.org/abs/2505.00679)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated strong capabilities in rewriting text across various styles. However, effectively leveraging this ability for example-based arbitrary style transfer, where an input text is rewritten to match the style of a given exemplar, remains an open challenge. A key question is how to describe the style of the exemplar to guide LLMs toward high-quality rewrites. In this work, we propose a prompting method based on register analysis to guide LLMs to perform this task. Empirical evaluations across multiple style transfer tasks show that our prompting approach enhances style transfer strength while preserving meaning more effectively than existing prompting strategies.</li>
</ul>

<h3>Title: On the Importance of Gaussianizing Representations</h3>
<ul>
<li><strong>Authors: </strong>Daniel Eftekhari, Vardan Papyan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00685">https://arxiv.org/abs/2505.00685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00685">https://arxiv.org/pdf/2505.00685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00685]] On the Importance of Gaussianizing Representations(https://arxiv.org/abs/2505.00685)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The normal distribution plays a central role in information theory - it is at the same time the best-case signal and worst-case noise distribution, has the greatest representational capacity of any distribution, and offers an equivalence between uncorrelatedness and independence for joint distributions. Accounting for the mean and variance of activations throughout the layers of deep neural networks has had a significant effect on facilitating their effective training, but seldom has a prescription for precisely what distribution these activations should take, and how this might be achieved, been offered. Motivated by the information-theoretic properties of the normal distribution, we address this question and concurrently present normality normalization: a novel normalization layer which encourages normality in the feature representations of neural networks using the power transform and employs additive Gaussian noise during training. Our experiments comprehensively demonstrate the effectiveness of normality normalization, in regards to its generalization performance on an array of widely used model and dataset combinations, its strong performance across various common factors of variation such as model width, depth, and training minibatch size, its suitability for usage wherever existing normalization layers are conventionally used, and as a means to improving model robustness to random perturbations.</li>
</ul>

<h3>Title: RayZer: A Self-supervised Large View Synthesis Model</h3>
<ul>
<li><strong>Authors: </strong>Hanwen Jiang, Hao Tan, Peng Wang, Haian Jin, Yue Zhao, Sai Bi, Kai Zhang, Fujun Luan, Kalyan Sunkavalli, Qixing Huang, Georgios Pavlakos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00702">https://arxiv.org/abs/2505.00702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00702">https://arxiv.org/pdf/2505.00702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00702]] RayZer: A Self-supervised Large View Synthesis Model(https://arxiv.org/abs/2505.00702)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present RayZer, a self-supervised multi-view 3D Vision model trained without any 3D supervision, i.e., camera poses and scene geometry, while exhibiting emerging 3D awareness. Concretely, RayZer takes unposed and uncalibrated images as input, recovers camera parameters, reconstructs a scene representation, and synthesizes novel views. During training, RayZer relies solely on its self-predicted camera poses to render target views, eliminating the need for any ground-truth camera annotations and allowing RayZer to be trained with 2D image supervision. The emerging 3D awareness of RayZer is attributed to two key factors. First, we design a self-supervised framework, which achieves 3D-aware auto-encoding of input images by disentangling camera and scene representations. Second, we design a transformer-based model in which the only 3D prior is the ray structure, connecting camera, pixel, and scene simultaneously. RayZer demonstrates comparable or even superior novel view synthesis performance than ``oracle'' methods that rely on pose annotations in both training and testing. Project: this https URL</li>
</ul>

<h3>Title: T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT</h3>
<ul>
<li><strong>Authors: </strong>Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00703">https://arxiv.org/abs/2505.00703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00703">https://arxiv.org/pdf/2505.00703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00703]] T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT(https://arxiv.org/abs/2505.00703)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: this https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
