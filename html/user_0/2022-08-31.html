<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: Towards Adversarial Purification using Denoising AutoEncoders. (arXiv:2208.13838v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13838">http://arxiv.org/abs/2208.13838</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13838] Towards Adversarial Purification using Denoising AutoEncoders](http://arxiv.org/abs/2208.13838)</code></li>
<li>Summary: <p>With the rapid advancement and increased use of deep learning models in image
identification, security becomes a major concern to their deployment in
safety-critical systems. Since the accuracy and robustness of deep learning
models are primarily attributed from the purity of the training samples,
therefore the deep learning architectures are often susceptible to adversarial
attacks. Adversarial attacks are often obtained by making subtle perturbations
to normal images, which are mostly imperceptible to humans, but can seriously
confuse the state-of-the-art machine learning models. We propose a framework,
named APuDAE, leveraging Denoising AutoEncoders (DAEs) to purify these samples
by using them in an adaptive way and thus improve the classification accuracy
of the target classifier networks that have been attacked. We also show how
using DAEs adaptively instead of using them directly, improves classification
accuracy further and is more robust to the possibility of designing adaptive
attacks to fool them. We demonstrate our results over MNIST, CIFAR-10, ImageNet
dataset and show how our framework (APuDAE) provides comparable and in most
cases better performance to the baseline methods in purifying adversaries. We
also design adaptive attack specifically designed to attack our purifying model
and demonstrate how our defense is robust to that.
</p></li>
</ul>

<h3>Title: Reinforcement Learning for Hardware Security: Opportunities, Developments, and Challenges. (arXiv:2208.13885v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13885">http://arxiv.org/abs/2208.13885</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13885] Reinforcement Learning for Hardware Security: Opportunities, Developments, and Challenges](http://arxiv.org/abs/2208.13885)</code></li>
<li>Summary: <p>Reinforcement learning (RL) is a machine learning paradigm where an
autonomous agent learns to make an optimal sequence of decisions by interacting
with the underlying environment. The promise demonstrated by RL-guided
workflows in unraveling electronic design automation problems has encouraged
hardware security researchers to utilize autonomous RL agents in solving
domain-specific problems. From the perspective of hardware security, such
autonomous agents are appealing as they can generate optimal actions in an
unknown adversarial environment. On the other hand, the continued globalization
of the integrated circuit supply chain has forced chip fabrication to
off-shore, untrustworthy entities, leading to increased concerns about the
security of the hardware. Furthermore, the unknown adversarial environment and
increasing design complexity make it challenging for defenders to detect subtle
modifications made by attackers (a.k.a. hardware Trojans). In this brief, we
outline the development of RL agents in detecting hardware Trojans, one of the
most challenging hardware security problems. Additionally, we outline potential
opportunities and enlist the challenges of applying RL to solve hardware
security problems.
</p></li>
</ul>

<h3>Title: AVMiner: Expansible and Semantic-Preserving Anti-Virus Labels Mining Method. (arXiv:2208.14221v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14221">http://arxiv.org/abs/2208.14221</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14221] AVMiner: Expansible and Semantic-Preserving Anti-Virus Labels Mining Method](http://arxiv.org/abs/2208.14221)</code></li>
<li>Summary: <p>With the increase in the variety and quantity of malware, there is an urgent
need to speed up the diagnosis and the analysis of malware. Extracting the
malware family-related tokens from AV (Anti-Virus) labels, provided by online
anti-virus engines, paves the way for pre-diagnosing the malware. Automatically
extract the vital information from AV labels will greatly enhance the detection
ability of security enterprises and equip the research ability of security
analysts. Recent works like AVCLASS and AVCLASS2 try to extract the attributes
of malware from AV labels and establish the taxonomy based on expert knowledge.
However, due to the uncertain trend of complicated malicious behaviors, the
system needs the following abilities to face the challenge: preserving vital
semantics, being expansible, and free from expert knowledge. In this work, we
present AVMiner, an expansible malware tagging system that can mine the most
vital tokens from AV labels. AVMiner adopts natural language processing
techniques and clustering methods to generate a sequence of tokens without
expert knowledge ranked by importance. AVMiner can self-update when new samples
come. Finally, we evaluate AVMiner on over 8,000 samples from well-known
datasets with manually labeled ground truth, which outperforms previous works.
</p></li>
</ul>

<h3>Title: Integral Sampler and Polynomial Multiplication Architecture for Lattice-based Cryptography. (arXiv:2208.14270v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14270">http://arxiv.org/abs/2208.14270</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14270] Integral Sampler and Polynomial Multiplication Architecture for Lattice-based Cryptography](http://arxiv.org/abs/2208.14270)</code></li>
<li>Summary: <p>With the surge of the powerful quantum computer, lattice-based cryptography
proliferated the latest cryptography hardware implementation due to its
resistance against quantum computers. Among the computational blocks of
lattice-based cryptography, the random errors produced by the sampler play a
key role in ensuring the security of these schemes. This paper proposes an
integral architecture for the sampler, which can reduce the overall resource
consumption by reusing the multipliers and adders within the modular polynomial
computation. For instance, our experimental results show that the proposed
design can effectively reduce the discrete Ziggurat sampling method in DSP
usage.
</p></li>
</ul>

<h3>Title: Software Update Practices on Smart Home IoT Devices. (arXiv:2208.14367v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14367">http://arxiv.org/abs/2208.14367</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14367] Software Update Practices on Smart Home IoT Devices](http://arxiv.org/abs/2208.14367)</code></li>
<li>Summary: <p>Smart home IoT devices are known to be breeding grounds for security and
privacy vulnerabilities. Although some IoT vendors deploy updates, the update
process is mostly opaque to researchers. It is unclear what software components
are on devices, whether and when these components are updated, and how
vulnerabilities change alongside the updates. This opaqueness makes it
difficult to understand the security of software supply chains of IoT devices.
</p></li>
</ul>

<p>To understand the software update practices on IoT devices, we leverage IoT
Inspector's dataset of network traffic from real-world IoT devices. We analyze
the User Agent strings from plain-text HTTP connections. We focus on four
software components included in User Agents: cURL, Wget, OkHttp, and
python-requests. By keeping track of what kinds of devices have which of these
components at what versions, we find that many IoT devices potentially used
outdated and vulnerable versions of these components - based on the User Agents
- even though less vulnerable, more updated versions were available; and that
the rollout of updates tends to be slow for some IoT devices.
</p>

<h3>Title: Denoising Architecture for Unsupervised Anomaly Detection in Time-Series. (arXiv:2208.14337v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14337">http://arxiv.org/abs/2208.14337</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14337] Denoising Architecture for Unsupervised Anomaly Detection in Time-Series](http://arxiv.org/abs/2208.14337)</code></li>
<li>Summary: <p>Anomalies in time-series provide insights of critical scenarios across a
range of industries, from banking and aerospace to information technology,
security, and medicine. However, identifying anomalies in time-series data is
particularly challenging due to the imprecise definition of anomalies, the
frequent absence of labels, and the enormously complex temporal correlations
present in such data. The LSTM Autoencoder is an Encoder-Decoder scheme for
Anomaly Detection based on Long Short Term Memory Networks that learns to
reconstruct time-series behavior and then uses reconstruction error to identify
abnormalities. We introduce the Denoising Architecture as a complement to this
LSTM Encoder-Decoder model and investigate its effect on real-world as well as
artificially generated datasets. We demonstrate that the proposed architecture
increases both the accuracy and the training speed, thereby, making the LSTM
Autoencoder more efficient for unsupervised anomaly detection tasks.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: On the (Im)Possibility of Estimating Various Notions of Differential Privacy. (arXiv:2208.14414v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14414">http://arxiv.org/abs/2208.14414</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14414] On the (Im)Possibility of Estimating Various Notions of Differential Privacy](http://arxiv.org/abs/2208.14414)</code></li>
<li>Summary: <p>We analyze to what extent final users can infer information about the level
of protection of their data when the data obfuscation mechanism is a priori
unknown to him (the so called "black-box" scenario). In particular, we delve
into the investigation of various notions of differential privacy (DP), namely
epsilon-DP, local DP, and R\'enyi DP. On one side, we prove that, without any
assumption on the underlying distributions, it is not possible to have an
algorithm able to infer the level of data protection with provable guarantees.
On the other side, we demonstrate that, under reasonable assumptions (namely,
Lipschitzness of the involved densities on a closed interval), such guarantees
exist and can be achieved by a simple histogram-based estimator. Then, by using
one of the best known DP obfuscation mechanisms (namely, the Laplacian one), we
test in practice that the theoretical number of samples needed to prove our
bound is actually much larger than the real number needed for obtaining
satisfactory results. Furthermore, we also see that the estimated epsilon is in
practice much closer to the real one w.r.t. what our theorems foresee.
</p></li>
</ul>

<h3>Title: Modeling Spatial Trajectories using Coarse-Grained Smartphone Logs. (arXiv:2208.13775v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13775">http://arxiv.org/abs/2208.13775</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13775] Modeling Spatial Trajectories using Coarse-Grained Smartphone Logs](http://arxiv.org/abs/2208.13775)</code></li>
<li>Summary: <p>Current approaches for points-of-interest (POI) recommendation learn the
preferences of a user via the standard spatial features such as the POI
coordinates, the social network, etc. These models ignore a crucial aspect of
spatial mobility -- every user carries their smartphones wherever they go. In
addition, with growing privacy concerns, users refrain from sharing their exact
geographical coordinates and their social media activity. In this paper, we
present REVAMP, a sequential POI recommendation approach that utilizes the user
activity on smartphone applications (or apps) to identify their mobility
preferences. This work aligns with the recent psychological studies of online
urban users, which show that their spatial mobility behavior is largely
influenced by the activity of their smartphone apps. In addition, our proposal
of coarse-grained smartphone data refers to data logs collected in a
privacy-conscious manner, i.e., consisting only of (a) category of the
smartphone app and (b) category of check-in location. Thus, REVAMP is not privy
to precise geo-coordinates, social networks, or the specific application being
accessed. Buoyed by the efficacy of self-attention models, we learn the POI
preferences of a user using two forms of positional encodings -- absolute and
relative -- with each extracted from the inter-check-in dynamics in the
check-in sequence of a user. Extensive experiments across two large-scale
datasets from China show the predictive prowess of REVAMP and its ability to
predict app- and POI categories.
</p></li>
</ul>

<h3>Title: Neural Architecture Search for Improving Latency-Accuracy Trade-off in Split Computing. (arXiv:2208.13968v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13968">http://arxiv.org/abs/2208.13968</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13968] Neural Architecture Search for Improving Latency-Accuracy Trade-off in Split Computing](http://arxiv.org/abs/2208.13968)</code></li>
<li>Summary: <p>This paper proposes a neural architecture search (NAS) method for split
computing. Split computing is an emerging machine-learning inference technique
that addresses the privacy and latency challenges of deploying deep learning in
IoT systems. In split computing, neural network models are separated and
cooperatively processed using edge servers and IoT devices via networks. Thus,
the architecture of the neural network model significantly impacts the
communication payload size, model accuracy, and computational load. In this
paper, we address the challenge of optimizing neural network architecture for
split computing. To this end, we proposed NASC, which jointly explores optimal
model architecture and a split point to achieve higher accuracy while meeting
latency requirements (i.e., smaller total latency of computation and
communication than a certain threshold). NASC employs a one-shot NAS that does
not require repeating model training for a computationally efficient
architecture search. Our performance evaluation using hardware (HW)-NAS-Bench
of benchmark data demonstrates that the proposed NASC can improve the
``communication latency and model accuracy" trade-off, i.e., reduce the latency
by approximately 40-60% from the baseline, with slight accuracy degradation.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: On the Trade-Off between Actionable Explanations and the Right to be Forgotten. (arXiv:2208.14137v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14137">http://arxiv.org/abs/2208.14137</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14137] On the Trade-Off between Actionable Explanations and the Right to be Forgotten](http://arxiv.org/abs/2208.14137)</code></li>
<li>Summary: <p>As machine learning (ML) models are increasingly being deployed in
high-stakes applications, policymakers have suggested tighter data protection
regulations (e.g., GDPR, CCPA). One key principle is the <code>right to be
forgotten'' which gives users the right to have their data deleted. Another key
principle is the right to an actionable explanation, also known as algorithmic
recourse, allowing users to reverse unfavorable decisions. To date it is
unknown whether these two principles can be operationalized simultaneously.
Therefore, we introduce and study the problem of recourse invalidation in the
context of data deletion requests. More specifically, we theoretically and
empirically analyze the behavior of popular state-of-the-art algorithms and
demonstrate that the recourses generated by these algorithms are likely to be
invalidated if a small number of data deletion requests (e.g., 1 or 2) warrant
updates of the predictive model. For the setting of linear models and
overparameterized neural networks -- studied through the lens of neural tangent
kernels (NTKs) -- we suggest a framework to identify a minimal subset of
critical training points, which when removed, would lead to maximize the
fraction of invalidated recourses. Using our framework, we empirically
establish that the removal of as little as 2 data instances from the training
set can invalidate up to 95 percent of all recourses output by popular
state-of-the-art algorithms. Thus, our work raises fundamental questions about
the compatibility of</code>the right to an actionable explanation'' in the context
of the ``right to be forgotten''.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Toward a Mathematical Vulnerability Propagation and Defense Model in Smart Grid Networks. (arXiv:2208.13884v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13884">http://arxiv.org/abs/2208.13884</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13884] Toward a Mathematical Vulnerability Propagation and Defense Model in Smart Grid Networks](http://arxiv.org/abs/2208.13884)</code></li>
<li>Summary: <p>For reducing threat propagation within an inter-connected network, it is
essential to distribute the defense investment optimally. Most electric power
utilities are resource constrained, yet how to account for costs while
designing threat reduction techniques is not well understood. Hence, in this
work, a vulnerability propagation and a defense model is proposed based on an
epidemic model. The new defense mechanism is then validated through sensitivity
of the propagation parameters on the optimal investment with two-node and
N-node cases. Further, the model efficacy is evaluated with implementation in
one of the communication networks of a cyber-physical power system. Topological
impact on the optimal nodal investment is also emphasized. Optimal investment
of the neighbors with less degree were found to be highly sensitive to
fluctuation in vulnerability exploitability probability.
</p></li>
</ul>

<h3>Title: Reducing Certified Regression to Certified Classification. (arXiv:2208.13904v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13904">http://arxiv.org/abs/2208.13904</a></li>
<li>Code URL: <a href="https://github.com/ZaydH/certified-regression">https://github.com/ZaydH/certified-regression</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13904] Reducing Certified Regression to Certified Classification](http://arxiv.org/abs/2208.13904)</code></li>
<li>Summary: <p>Adversarial training instances can severely distort a model's behavior. This
work investigates certified regression defenses, which provide guaranteed
limits on how much a regressor's prediction may change under a training-set
attack. Our key insight is that certified regression reduces to certified
classification when using median as a model's primary decision function.
Coupling our reduction with existing certified classifiers, we propose six new
provably-robust regressors. To the extent of our knowledge, this is the first
work that certifies the robustness of individual regression predictions without
any assumptions about the data distribution and model architecture. We also
show that existing state-of-the-art certified classifiers often make
overly-pessimistic assumptions that can degrade their provable guarantees. We
introduce a tighter analysis of model robustness, which in many cases results
in significantly improved certified guarantees. Lastly, we empirically
demonstrate our approaches' effectiveness on both regression and classification
data, where the accuracy of up to 50% of test predictions can be guaranteed
under 1% training-set corruption and up to 30% of predictions under 4%
corruption. Our source code is available at
https://github.com/ZaydH/certified-regression.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Perfusion assessment via local remote photoplethysmography (rPPG). (arXiv:2208.13840v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13840">http://arxiv.org/abs/2208.13840</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13840] Perfusion assessment via local remote photoplethysmography (rPPG)](http://arxiv.org/abs/2208.13840)</code></li>
<li>Summary: <p>This paper presents an approach to assess the perfusion of visible human
tissue from RGB video files. We propose metrics derived from remote
photoplethysmography (rPPG) signals to detect whether a tissue is adequately
supplied with blood. The perfusion analysis is done in three different scales,
offering a flexible approach for different applications. We perform a
plane-orthogonal-to-skin rPPG independently for locally defined regions of
interest on each scale. From the extracted signals, we derive the
signal-to-noise ratio, magnitude in the frequency domain, heart rate, perfusion
index as well as correlation between specific rPPG signals in order to locally
assess the perfusion of a specific region of human tissue. We show that locally
resolved rPPG has a broad range of applications. As exemplary applications, we
present results in intraoperative perfusion analysis and visualization during
skin and organ transplantation as well as an application for liveliness
assessment for the detection of presentation attacks to authentication systems.
</p></li>
</ul>

<h3>Title: A Black-Box Attack on Optical Character Recognition Systems. (arXiv:2208.14302v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14302">http://arxiv.org/abs/2208.14302</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14302] A Black-Box Attack on Optical Character Recognition Systems](http://arxiv.org/abs/2208.14302)</code></li>
<li>Summary: <p>Adversarial machine learning is an emerging area showing the vulnerability of
deep learning models. Exploring attack methods to challenge state of the art
artificial intelligence (A.I.) models is an area of critical concern. The
reliability and robustness of such A.I. models are one of the major concerns
with an increasing number of effective adversarial attack methods.
Classification tasks are a major vulnerable area for adversarial attacks. The
majority of attack strategies are developed for colored or gray-scaled images.
Consequently, adversarial attacks on binary image recognition systems have not
been sufficiently studied. Binary images are simple two possible pixel-valued
signals with a single channel. The simplicity of binary images has a
significant advantage compared to colored and gray scaled images, namely
computation efficiency. Moreover, most optical character recognition systems
(O.C.R.s), such as handwritten character recognition, plate number
identification, and bank check recognition systems, use binary images or
binarization in their processing steps. In this paper, we propose a simple yet
efficient attack method, Efficient Combinatorial Black-box Adversarial Attack,
on binary image classifiers. We validate the efficiency of the attack technique
on two different data sets and three classification networks, demonstrating its
performance. Furthermore, we compare our proposed method with state-of-the-art
methods regarding advantages and disadvantages as well as applicability.
</p></li>
</ul>

<h3>Title: Attack detection based on machine learning algorithms for different variants of Spectre attacks and different Meltdown attack implementations. (arXiv:2208.14062v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14062">http://arxiv.org/abs/2208.14062</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14062] Attack detection based on machine learning algorithms for different variants of Spectre attacks and different Meltdown attack implementations](http://arxiv.org/abs/2208.14062)</code></li>
<li>Summary: <p>To improve the overall performance of processors, computer architects use
various performance optimization techniques in modern processors, such as
speculative execution, branch prediction, and chaotic execution. Both now and
in the future, these optimization techniques are critical for improving the
execution speed of processor instructions. However, researchers have discovered
that these techniques introduce hidden inherent security flaws, such as
meltdown and ghost attacks in recent years. They exploit techniques such as
chaotic execution or speculative execution combined with cache-based
side-channel attacks to leak protected data. The impact of these
vulnerabilities is enormous because they are prevalent in existing or future
processors. However, until today, meltdown and ghost have not been effectively
addressed, but instead, multiple attack variants and different attack
implementations have evolved from them. This paper proposes to optimize four
different hardware performance events through feature selection and use machine
learning algorithms to build a real-time detection mechanism for Spectre
v1,v2,v4, and different implementations of meltdown attacks, ultimately
achieving an accuracy rate of over 99\%. In order to verify the practicality of
the attack detection model, this paper is tested with a variety of benign
programs and different implementations of Spectre attacks different from the
modeling process, and the absolute accuracy also exceeds 99\%, showing that
this paper can cope with different attack variants and different
implementations of the same attack that may occur daily.
</p></li>
</ul>

<h3>Title: Solving the Capsulation Attack against Backdoor-based Deep Neural Network Watermarks by Reversing Triggers. (arXiv:2208.14127v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14127">http://arxiv.org/abs/2208.14127</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14127] Solving the Capsulation Attack against Backdoor-based Deep Neural Network Watermarks by Reversing Triggers](http://arxiv.org/abs/2208.14127)</code></li>
<li>Summary: <p>Backdoor-based watermarking schemes were proposed to protect the intellectual
property of artificial intelligence models, especially deep neural networks,
under the black-box setting. Compared with ordinary backdoors, backdoor-based
watermarks need to digitally incorporate the owner's identity, which fact adds
extra requirements to the trigger generation and verification programs.
Moreover, these concerns produce additional security risks after the
watermarking scheme has been published for as a forensics tool or the owner's
evidence has been eavesdropped on. This paper proposes the capsulation attack,
an efficient method that can invalidate most established backdoor-based
watermarking schemes without sacrificing the pirated model's functionality. By
encapsulating the deep neural network with a rule-based or Bayes filter, an
adversary can block ownership probing and reject the ownership verification. We
propose a metric, CAScore, to measure a backdoor-based watermarking scheme's
security against the capsulation attack. This paper also proposes a new
backdoor-based deep neural network watermarking scheme that is secure against
the capsulation attack by reversing the encoding process and randomizing the
exposure of triggers.
</p></li>
</ul>

<h3>Title: FuncFooler: A Practical Black-box Attack Against Learning-based Binary Code Similarity Detection Methods. (arXiv:2208.14191v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14191">http://arxiv.org/abs/2208.14191</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14191] FuncFooler: A Practical Black-box Attack Against Learning-based Binary Code Similarity Detection Methods](http://arxiv.org/abs/2208.14191)</code></li>
<li>Summary: <p>The binary code similarity detection (BCSD) method measures the similarity of
two binary executable codes. Recently, the learning-based BCSD methods have
achieved great success, outperforming traditional BCSD in detection accuracy
and efficiency. However, the existing studies are rather sparse on the
adversarial vulnerability of the learning-based BCSD methods, which cause
hazards in security-related applications. To evaluate the adversarial
robustness, this paper designs an efficient and black-box adversarial code
generation algorithm, namely, FuncFooler. FuncFooler constrains the adversarial
codes 1) to keep unchanged the program's control flow graph (CFG), and 2) to
preserve the same semantic meaning. Specifically, FuncFooler consecutively 1)
determines vulnerable candidates in the malicious code, 2) chooses and inserts
the adversarial instructions from the benign code, and 3) corrects the semantic
side effect of the adversarial code to meet the constraints. Empirically, our
FuncFooler can successfully attack the three learning-based BCSD models,
including SAFE, Asm2Vec, and jTrans, which calls into question whether the
learning-based BCSD is desirable.
</p></li>
</ul>

<h3>Title: Survey on Architectural Attacks: A Unified Classification and Attack Model. (arXiv:2208.14194v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14194">http://arxiv.org/abs/2208.14194</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14194] Survey on Architectural Attacks: A Unified Classification and Attack Model](http://arxiv.org/abs/2208.14194)</code></li>
<li>Summary: <p>According to the World Economic Forum, cyber attacks are considered as one of
the most important sources of risk to companies and institutions worldwide.
Attacks can target the network, software, and/or hardware. During the past
years, much knowledge has been developed to understand and mitigate
cyberattacks. However, new threats have appeared in recent years regarding
software attacks that exploit hardware vulnerabilities. We define these attacks
as architectural attacks. Today, both industry and academy have only limited
comprehension of architectural attacks, which represents a critical issue for
the design of future systems. To this end, this work proposes a new taxonomy, a
new attack model, and a complete survey of existing architectural attacks. As a
result, our study provides the tools to understand the Architectural Attacks
deeply and start building better designs as well as protection mechanisms.
</p></li>
</ul>

<h3>Title: One Year of DDoS Attacks Against a Cloud Provider: an Overview. (arXiv:2208.14205v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14205">http://arxiv.org/abs/2208.14205</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14205] One Year of DDoS Attacks Against a Cloud Provider: an Overview](http://arxiv.org/abs/2208.14205)</code></li>
<li>Summary: <p>Distributed denial of service attacks represents one of the most important
threats to cloud-providers. Over the years, volumetric DDoS attacks have become
increasingly important and complex. Due to the rapid adaptation of attackers to
the detection and mitigation methods designed to counter them, the industry
needs to constantly monitor and analyse the attacks they face. In this paper,
we present an overview of the attacks that were perpetrated against our
infrastructure in 2021. Our motivation is to give an insight of the challenge
that DDoS attacks still represent within a large European cloud provider
</p></li>
</ul>

<h3>Title: Cyberattacks on Energy Infrastructures: Modern War Weapons. (arXiv:2208.14225v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14225">http://arxiv.org/abs/2208.14225</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14225] Cyberattacks on Energy Infrastructures: Modern War Weapons](http://arxiv.org/abs/2208.14225)</code></li>
<li>Summary: <p>Recent high-profile cyberattacks on energy infrastructures, such as the
security breach of the Colonial Pipeline in 2021 and attacks that have
disrupted Ukraine's power grid from the mid-2010s till date, have pushed
cybersecurity as a top priority. As political tensions have escalated in Europe
this year, concerns about critical infrastructure security have increased.
Operators in the industrial sector face new cybersecurity threats that increase
the risk of disruptions in services, property damages, and environmental harm.
Amid rising geopolitical tensions, industrial companies, with their
network-connected systems, are now considered major targets for adversaries to
advance political, social, or military agendas. Moreover, the recent
Russian-Ukrainian conflict has set the alarm worldwide about the danger of
targeting energy grids via cyberattacks. Attack methodologies, techniques, and
procedures used successfully to hack energy grids in Ukraine can be used
elsewhere. This work aims to present a thorough analysis of the cybersecurity
of the energy infrastructure amid the increased rise of cyberwars. The article
navigates through the recent history of energy-related cyberattacks and their
reasoning, discusses the grid's vulnerability, and makes a precautionary
argument for securing the grids against them.
</p></li>
</ul>

<h3>Title: FDB: Fraud Dataset Benchmark. (arXiv:2208.14417v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14417">http://arxiv.org/abs/2208.14417</a></li>
<li>Code URL: <a href="https://github.com/amazon-research/fraud-dataset-benchmark">https://github.com/amazon-research/fraud-dataset-benchmark</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14417] FDB: Fraud Dataset Benchmark](http://arxiv.org/abs/2208.14417)</code></li>
<li>Summary: <p>Standardized datasets and benchmarks have spurred innovations in computer
vision, natural language processing, multi-modal and tabular settings. We note
that, as compared to other well researched fields fraud detection has numerous
differences. The differences include a high class imbalance, diverse feature
types, frequently changing fraud patterns, and adversarial nature of the
problem. Due to these differences, the modeling approaches that are designed
for other classification tasks may not work well for the fraud detection. We
introduce Fraud Dataset Benchmark (FDB), a compilation of publicly available
datasets catered to fraud detection. FDB comprises variety of fraud related
tasks, ranging from identifying fraudulent card-not-present transactions,
detecting bot attacks, classifying malicious URLs, predicting risk of loan to
content moderation. The Python based library from FDB provides consistent API
for data loading with standardized training and testing splits. For reference,
we also provide baseline evaluations of different modeling approaches on FDB.
Considering the increasing popularity of Automated Machine Learning (AutoML)
for various research and business problems, we used AutoML frameworks for our
baseline evaluations. For fraud prevention, the organizations that operate with
limited resources and lack ML expertise often hire a team of investigators, use
blocklists and manual rules, all of which are inefficient and do not scale
well. Such organizations can benefit from AutoML solutions that are easy to
deploy in production and pass the bar of fraud prevention requirements. We hope
that FDB helps in the development of customized fraud detection techniques
catered to different fraud modus operandi (MOs) as well as in the improvement
of AutoML systems that can work well for all datasets in the benchmark.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data. (arXiv:2208.13944v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13944">http://arxiv.org/abs/2208.13944</a></li>
<li>Code URL: <a href="https://github.com/ostadabbas/prior-aware-synthetic-data-generation-pasyn-">https://github.com/ostadabbas/prior-aware-synthetic-data-generation-pasyn-</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13944] Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data](http://arxiv.org/abs/2208.13944)</code></li>
<li>Summary: <p>Accurately annotated image datasets are essential components for studying
animal behaviors from their poses. Compared to the number of species we know
and may exist, the existing labeled pose datasets cover only a small portion of
them, while building comprehensive large-scale datasets is prohibitively
expensive. Here, we present a very data efficient strategy targeted for pose
estimation in quadrupeds that requires only a small amount of real images from
the target animal. It is confirmed that fine-tuning a backbone network with
pretrained weights on generic image datasets such as ImageNet can mitigate the
high demand for target animal pose data and shorten the training time by
learning the the prior knowledge of object segmentation and keypoint estimation
in advance. However, when faced with serious data scarcity (i.e., $<10^2$ real
images), the model performance stays unsatisfactory, particularly for limbs
with considerable flexibility and several comparable parts. We therefore
introduce a prior-aware synthetic animal data generation pipeline called PASyn
to augment the animal pose data essential for robust pose estimation. PASyn
generates a probabilistically-valid synthetic pose dataset, SynAP, through
training a variational generative model on several animated 3D animal models.
In addition, a style transfer strategy is utilized to blend the synthetic
animal image into the real backgrounds. We evaluate the improvement made by our
approach with three popular backbone networks and test their pose estimation
accuracy on publicly available animal pose images as well as collected from
real animals in a zoo.
</p></li>
</ul>

<h3>Title: SSORN: Self-Supervised Outlier Removal Network for Robust Homography Estimation. (arXiv:2208.14093v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14093">http://arxiv.org/abs/2208.14093</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14093] SSORN: Self-Supervised Outlier Removal Network for Robust Homography Estimation](http://arxiv.org/abs/2208.14093)</code></li>
<li>Summary: <p>The traditional homography estimation pipeline consists of four main steps:
feature detection, feature matching, outlier removal and transformation
estimation. Recent deep learning models intend to address the homography
estimation problem using a single convolutional network. While these models are
trained in an end-to-end fashion to simplify the homography estimation problem,
they lack the feature matching step and/or the outlier removal step, which are
important steps in the traditional homography estimation pipeline. In this
paper, we attempt to build a deep learning model that mimics all four steps in
the traditional homography estimation pipeline. In particular, the feature
matching step is implemented using the cost volume technique. To remove
outliers in the cost volume, we treat this outlier removal problem as a
denoising problem and propose a novel self-supervised loss to solve the
problem. Extensive experiments on synthetic and real datasets demonstrate that
the proposed model outperforms existing deep learning models.
</p></li>
</ul>

<h3>Title: Robust Sound-Guided Image Manipulation. (arXiv:2208.14114v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14114">http://arxiv.org/abs/2208.14114</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14114] Robust Sound-Guided Image Manipulation](http://arxiv.org/abs/2208.14114)</code></li>
<li>Summary: <p>Recent successes suggest that an image can be manipulated by a text prompt,
e.g., a landscape scene on a sunny day is manipulated into the same scene on a
rainy day driven by a text input "raining". These approaches often utilize a
StyleCLIP-based image generator, which leverages multi-modal (text and image)
embedding space. However, we observe that such text inputs are often
bottlenecked in providing and synthesizing rich semantic cues, e.g.,
differentiating heavy rain from rain with thunderstorms. To address this issue,
we advocate leveraging an additional modality, sound, which has notable
advantages in image manipulation as it can convey more diverse semantic cues
(vivid emotions or dynamic expressions of the natural world) than texts. In
this paper, we propose a novel approach that first extends the image-text joint
embedding space with sound and applies a direct latent optimization method to
manipulate a given image based on audio input, e.g., the sound of rain. Our
extensive experiments show that our sound-guided image manipulation approach
produces semantically and visually more plausible manipulation results than the
state-of-the-art text and sound-guided image manipulation methods, which are
further confirmed by our human evaluations. Our downstream task evaluations
also show that our learned image-text-sound joint embedding space effectively
encodes sound inputs.
</p></li>
</ul>

<h3>Title: ASpanFormer: Detector-Free Image Matching with Adaptive Span Transformer. (arXiv:2208.14201v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14201">http://arxiv.org/abs/2208.14201</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14201] ASpanFormer: Detector-Free Image Matching with Adaptive Span Transformer](http://arxiv.org/abs/2208.14201)</code></li>
<li>Summary: <p>Generating robust and reliable correspondences across images is a fundamental
task for a diversity of applications. To capture context at both global and
local granularity, we propose ASpanFormer, a Transformer-based detector-free
matcher that is built on hierarchical attention structure, adopting a novel
attention operation which is capable of adjusting attention span in a
self-adaptive manner. To achieve this goal, first, flow maps are regressed in
each cross attention phase to locate the center of search region. Next, a
sampling grid is generated around the center, whose size, instead of being
empirically configured as fixed, is adaptively computed from a pixel
uncertainty estimated along with the flow map. Finally, attention is computed
across two images within derived regions, referred to as attention span. By
these means, we are able to not only maintain long-range dependencies, but also
enable fine-grained attention among pixels of high relevance that compensates
essential locality and piece-wise smoothness in matching tasks.
State-of-the-art accuracy on a wide range of evaluation benchmarks validates
the strong matching capability of our method.
</p></li>
</ul>

<h3>Title: GaitFi: Robust Device-Free Human Identification via WiFi and Vision Multimodal Learning. (arXiv:2208.14326v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14326">http://arxiv.org/abs/2208.14326</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14326] GaitFi: Robust Device-Free Human Identification via WiFi and Vision Multimodal Learning](http://arxiv.org/abs/2208.14326)</code></li>
<li>Summary: <p>As an important biomarker for human identification, human gait can be
collected at a distance by passive sensors without subject cooperation, which
plays an essential role in crime prevention, security detection and other human
identification applications. At present, most research works are based on
cameras and computer vision techniques to perform gait recognition. However,
vision-based methods are not reliable when confronting poor illuminations,
leading to degrading performances. In this paper, we propose a novel multimodal
gait recognition method, namely GaitFi, which leverages WiFi signals and videos
for human identification. In GaitFi, Channel State Information (CSI) that
reflects the multi-path propagation of WiFi is collected to capture human
gaits, while videos are captured by cameras. To learn robust gait information,
we propose a Lightweight Residual Convolution Network (LRCN) as the backbone
network, and further propose the two-stream GaitFi by integrating WiFi and
vision features for the gait retrieval task. The GaitFi is trained by the
triplet loss and classification loss on different levels of features. Extensive
experiments are conducted in the real world, which demonstrates that the GaitFi
outperforms state-of-the-art gait recognition methods based on single WiFi or
camera, achieving 94.2% for human identification tasks of 12 subjects.
</p></li>
</ul>

<h3>Title: A Portable Multiscopic Camera for Novel View and Time Synthesis in Dynamic Scenes. (arXiv:2208.14433v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14433">http://arxiv.org/abs/2208.14433</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14433] A Portable Multiscopic Camera for Novel View and Time Synthesis in Dynamic Scenes](http://arxiv.org/abs/2208.14433)</code></li>
<li>Summary: <p>We present a portable multiscopic camera system with a dedicated model for
novel view and time synthesis in dynamic scenes. Our goal is to render
high-quality images for a dynamic scene from any viewpoint at any time using
our portable multiscopic camera. To achieve such novel view and time synthesis,
we develop a physical multiscopic camera equipped with five cameras to train a
neural radiance field (NeRF) in both time and spatial domains for dynamic
scenes. Our model maps a 6D coordinate (3D spatial position, 1D temporal
coordinate, and 2D viewing direction) to view-dependent and time-varying
emitted radiance and volume density. Volume rendering is applied to render a
photo-realistic image at a specified camera pose and time. To improve the
robustness of our physical camera, we propose a camera parameter optimization
module and a temporal frame interpolation module to promote information
propagation across time. We conduct experiments on both real-world and
synthetic datasets to evaluate our system, and the results show that our
approach outperforms alternative solutions qualitatively and quantitatively.
Our code and dataset are available at https://yuenfuilau.github.io.
</p></li>
</ul>

<h3>Title: MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction. (arXiv:2208.14437v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14437">http://arxiv.org/abs/2208.14437</a></li>
<li>Code URL: <a href="https://github.com/hustvl/maptr">https://github.com/hustvl/maptr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14437] MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction](http://arxiv.org/abs/2208.14437)</code></li>
<li>Summary: <p>We present MapTR, a structured end-to-end framework for efficient online
vectorized HD map construction. We propose a unified permutation-based modeling
approach, i.e., modeling map element as a point set with a group of equivalent
permutations, which avoids the definition ambiguity of map element and eases
learning. We adopt a hierarchical query embedding scheme to flexibly encode
structured map information and perform hierarchical bipartite matching for map
element learning. MapTR achieves the best performance and efficiency among
existing vectorized map construction approaches on nuScenes dataset. In
particular, MapTR-nano runs at real-time inference speed ($25.1$ FPS) on RTX
3090, $8\times$ faster than the existing state-of-the-art camera-based method
while achieving $3.3$ higher mAP. MapTR-tiny significantly outperforms the
existing state-of-the-art multi-modality method by $13.5$ mAP while being
faster. Qualitative results show that MapTR maintains stable and robust map
construction quality in complex and various driving scenes. Abundant demos are
available at \url{https://github.com/hustvl/MapTR} to prove the effectiveness
in real-world scenarios. MapTR is of great application value in autonomous
driving. Code will be released for facilitating further research and
application.
</p></li>
</ul>

<h3>Title: Data Isotopes for Data Provenance in DNNs. (arXiv:2208.13893v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13893">http://arxiv.org/abs/2208.13893</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13893] Data Isotopes for Data Provenance in DNNs](http://arxiv.org/abs/2208.13893)</code></li>
<li>Summary: <p>Today, creators of data-hungry deep neural networks (DNNs) scour the Internet
for training fodder, leaving users with little control over or knowledge of
when their data is appropriated for model training. To empower users to
counteract unwanted data use, we design, implement and evaluate a practical
system that enables users to detect if their data was used to train an DNN
model. We show how users can create special data points we call isotopes, which
introduce "spurious features" into DNNs during training. With only query access
to a trained model and no knowledge of the model training process, or control
of the data labels, a user can apply statistical hypothesis testing to detect
if a model has learned the spurious features associated with their isotopes by
training on the user's data. This effectively turns DNNs' vulnerability to
memorization and spurious correlations into a tool for data provenance. Our
results confirm efficacy in multiple settings, detecting and distinguishing
between hundreds of isotopes with high accuracy. We further show that our
system works on public ML-as-a-service platforms and larger models such as
ImageNet, can use physical objects instead of digital marks, and remains
generally robust against several adaptive countermeasures.
</p></li>
</ul>

<h3>Title: Dimension Independent Data Sets Approximation and Applications to Classification. (arXiv:2208.13781v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13781">http://arxiv.org/abs/2208.13781</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13781] Dimension Independent Data Sets Approximation and Applications to Classification](http://arxiv.org/abs/2208.13781)</code></li>
<li>Summary: <p>We revisit the classical kernel method of approximation/interpolation theory
in a very specific context motivated by the desire to obtain a robust procedure
to approximate discrete data sets by (super)level sets of functions that are
merely continuous at the data set arguments but are otherwise smooth. Special
functions, called data signals, are defined for any given data set and are used
to succesfully solve supervised classification problems in a robust way that
depends continuously on the data set. The efficacy of the method is illustrated
with a series of low dimensional examples and by its application to the
standard benchmark high dimensional problem of MNIST digit classification.
</p></li>
</ul>

<h3>Title: Expert Opinion Elicitation for Assisting Deep Learning based Lyme Disease Classifier with Patient Data. (arXiv:2208.14384v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14384">http://arxiv.org/abs/2208.14384</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14384] Expert Opinion Elicitation for Assisting Deep Learning based Lyme Disease Classifier with Patient Data](http://arxiv.org/abs/2208.14384)</code></li>
<li>Summary: <p>Diagnosing erythema migrans (EM) skin lesion, the most common early symptom
of Lyme disease using deep learning techniques can be effective to prevent
long-term complications. Existing works on deep learning based EM recognition
only utilizes lesion image due to the lack of a dataset of Lyme disease related
images with associated patient data. Physicians rely on patient information
about the background of the skin lesion to confirm their diagnosis. In order to
assist the deep learning model with a probability score calculated from patient
data, this study elicited opinion from fifteen doctors. For the elicitation
process, a questionnaire with questions and possible answers related to EM was
prepared. Doctors provided relative weights to different answers to the
questions. We converted doctors evaluations to probability scores using
Gaussian mixture based density estimation. For elicited probability model
validation, we exploited formal concept analysis and decision tree. The
elicited probability scores can be utilized to make image based deep learning
Lyme disease pre-scanners robust.
</p></li>
</ul>

<h3>Title: DR-DSGD: A Distributionally Robust Decentralized Learning Algorithm over Graphs. (arXiv:2208.13810v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13810">http://arxiv.org/abs/2208.13810</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13810] DR-DSGD: A Distributionally Robust Decentralized Learning Algorithm over Graphs](http://arxiv.org/abs/2208.13810)</code></li>
<li>Summary: <p>In this paper, we propose to solve a regularized distributionally robust
learning problem in the decentralized setting, taking into account the data
distribution shift. By adding a Kullback-Liebler regularization function to the
robust min-max optimization problem, the learning problem can be reduced to a
modified robust minimization problem and solved efficiently. Leveraging the
newly formulated optimization problem, we propose a robust version of
Decentralized Stochastic Gradient Descent (DSGD), coined Distributionally
Robust Decentralized Stochastic Gradient Descent (DR-DSGD). Under some mild
assumptions and provided that the regularization parameter is larger than one,
we theoretically prove that DR-DSGD achieves a convergence rate of
$\mathcal{O}\left(1/\sqrt{KT} + K/T\right)$, where $K$ is the number of devices
and $T$ is the number of iterations. Simulation results show that our proposed
algorithm can improve the worst distribution test accuracy by up to $10\%$.
Moreover, DR-DSGD is more communication-efficient than DSGD since it requires
fewer communication rounds (up to $20$ times less) to achieve the same worst
distribution test accuracy target. Furthermore, the conducted experiments
reveal that DR-DSGD results in a fairer performance across devices in terms of
test accuracy.
</p></li>
</ul>

<h3>Title: The case for fully Bayesian optimisation in small-sample trials. (arXiv:2208.13960v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13960">http://arxiv.org/abs/2208.13960</a></li>
<li>Code URL: <a href="https://github.com/ysaikai/case4fbo">https://github.com/ysaikai/case4fbo</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13960] The case for fully Bayesian optimisation in small-sample trials](http://arxiv.org/abs/2208.13960)</code></li>
<li>Summary: <p>While sample efficiency is the main motive for use of Bayesian optimisation
when black-box functions are expensive to evaluate, the standard approach based
on type II maximum likelihood (ML-II) may fail and result in disappointing
performance in small-sample trials. The paper provides three compelling reasons
to adopt fully Bayesian optimisation (FBO) as an alternative. First, failures
of ML-II are more commonplace than implied by the existing studies using the
contrived settings. Second, FBO is more robust than ML-II, and the price of
robustness is almost trivial. Third, FBO has become simple to implement and
fast enough to be practical. The paper supports the argument using relevant
experiments, which reflect the current practice regarding models, algorithms,
and software platforms. Since the benefits seem to outweigh the costs,
researchers should consider adopting FBO for their applications so that they
can guard against potential failures that end up wasting precious research
resources.
</p></li>
</ul>

<h3>Title: Prediction of Red Wine Quality Using One-dimensional Convolutional Neural Networks. (arXiv:2208.14008v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14008">http://arxiv.org/abs/2208.14008</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14008] Prediction of Red Wine Quality Using One-dimensional Convolutional Neural Networks](http://arxiv.org/abs/2208.14008)</code></li>
<li>Summary: <p>As an alcoholic beverage, wine has remained prevalent for thousands of years,
and the quality assessment of wines has been significant in wine production and
trade. Scholars have proposed various deep learning and machine learning
algorithms for wine quality prediction, such as Support vector machine (SVM),
Random Forest (RF), K-nearest neighbors (KNN), Deep neural network (DNN), and
Logistic regression (LR). However, these methods ignore the inner relationship
between the physical and chemical properties of the wine, for example, the
correlations between pH values, fixed acidity, citric acid, and so on. To fill
the gap, this paper conducts the Pearson correlation analysis, PCA analysis,
and Shapiro-Wilk test on those properties and incorporates 1D-CNN architecture
to capture the correlations among neighboring features. In addition, it
implemented dropout and batch normalization techniques to improve the
robustness of the proposed model. Massive experiments have shown that our
method can outperform baseline approaches in wine quality prediction. Moreover,
ablation experiments also demonstrate the effectiveness of incorporating the
1-D CNN module, Dropout, and normalization techniques.
</p></li>
</ul>

<h3>Title: Anomaly Detection using Contrastive Normalizing Flows. (arXiv:2208.14024v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14024">http://arxiv.org/abs/2208.14024</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14024] Anomaly Detection using Contrastive Normalizing Flows](http://arxiv.org/abs/2208.14024)</code></li>
<li>Summary: <p>Detecting test data deviating from training data is a central problem for
safe and robust machine learning. Likelihoods learned by a generative model,
e.g., a normalizing flow via standard log-likelihood training, perform poorly
as an anomaly score. We propose to use an unlabelled auxiliary dataset and a
probabilistic outlier score for anomaly detection. We use a self-supervised
feature extractor trained on the auxiliary dataset and train a normalizing flow
on the extracted features by maximizing the likelihood on in-distribution data
and minimizing the likelihood on the auxiliary dataset. We show that this is
equivalent to learning the normalized positive difference between the
in-distribution and the auxiliary feature density. We conduct experiments on
benchmark datasets and show a robust improvement compared to likelihood,
likelihood ratio methods and state-of-the-art anomaly detection methods.
</p></li>
</ul>

<h3>Title: A Deep Neural Networks ensemble workflow from hyperparameter search to inference leveraging GPU clusters. (arXiv:2208.14046v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14046">http://arxiv.org/abs/2208.14046</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14046] A Deep Neural Networks ensemble workflow from hyperparameter search to inference leveraging GPU clusters](http://arxiv.org/abs/2208.14046)</code></li>
<li>Summary: <p>Automated Machine Learning with ensembling (or AutoML with ensembling) seeks
to automatically build ensembles of Deep Neural Networks (DNNs) to achieve
qualitative predictions. Ensemble of DNNs are well known to avoid over-fitting
but they are memory and time consuming approaches. Therefore, an ideal AutoML
would produce in one single run time different ensembles regarding accuracy and
inference speed. While previous works on AutoML focus to search for the best
model to maximize its generalization ability, we rather propose a new AutoML to
build a larger library of accurate and diverse individual models to then
construct ensembles. First, our extensive benchmarks show asynchronous
Hyperband is an efficient and robust way to build a large number of diverse
models to combine them. Then, a new ensemble selection method based on a
multi-objective greedy algorithm is proposed to generate accurate ensembles by
controlling their computing cost. Finally, we propose a novel algorithm to
optimize the inference of the DNNs ensemble in a GPU cluster based on
allocation optimization. The produced AutoML with ensemble method shows robust
results on two datasets using efficiently GPU clusters during both the training
phase and the inference phase.
</p></li>
</ul>

<h3>Title: Effective Multi-User Delay-Constrained Scheduling with Deep Recurrent Reinforcement Learning. (arXiv:2208.14074v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14074">http://arxiv.org/abs/2208.14074</a></li>
<li>Code URL: <a href="https://github.com/hupihe/rsd4">https://github.com/hupihe/rsd4</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14074] Effective Multi-User Delay-Constrained Scheduling with Deep Recurrent Reinforcement Learning](http://arxiv.org/abs/2208.14074)</code></li>
<li>Summary: <p>Multi-user delay constrained scheduling is important in many real-world
applications including wireless communication, live streaming, and cloud
computing. Yet, it poses a critical challenge since the scheduler needs to make
real-time decisions to guarantee the delay and resource constraints
simultaneously without prior information of system dynamics, which can be
time-varying and hard to estimate. Moreover, many practical scenarios suffer
from partial observability issues, e.g., due to sensing noise or hidden
correlation. To tackle these challenges, we propose a deep reinforcement
learning (DRL) algorithm, named Recurrent Softmax Delayed Deep Double
Deterministic Policy Gradient ($\mathtt{RSD4}$), which is a data-driven method
based on a Partially Observed Markov Decision Process (POMDP) formulation.
$\mathtt{RSD4}$ guarantees resource and delay constraints by Lagrangian dual
and delay-sensitive queues, respectively. It also efficiently tackles partial
observability with a memory mechanism enabled by the recurrent neural network
(RNN) and introduces user-level decomposition and node-level merging to ensure
scalability. Extensive experiments on simulated/real-world datasets demonstrate
that $\mathtt{RSD4}$ is robust to system dynamics and partially observable
environments, and achieves superior performances over existing DRL and
non-DRL-based methods.
</p></li>
</ul>

<h3>Title: Unsupervised Representation Learning in Deep Reinforcement Learning: A Review. (arXiv:2208.14226v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14226">http://arxiv.org/abs/2208.14226</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14226] Unsupervised Representation Learning in Deep Reinforcement Learning: A Review](http://arxiv.org/abs/2208.14226)</code></li>
<li>Summary: <p>This review addresses the problem of learning abstract representations of the
measurement data in the context of Deep Reinforcement Learning (DRL). While the
data are often ambiguous, high-dimensional, and complex to interpret, many
dynamical systems can be effectively described by a low-dimensional set of
state variables. Discovering these state variables from the data is a crucial
aspect for improving the data efficiency, robustness and generalization of DRL
methods, tackling the curse of dimensionality, and bringing interpretability
and insights into black-box DRL. This review provides a comprehensive and
complete overview of unsupervised representation learning in DRL by describing
the main Deep Learning tools used for learning representations of the world,
providing a systematic view of the method and principles, summarizing
applications, benchmarks and evaluation strategies, and discussing open
challenges and future directions.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Synthetic Latent Fingerprint Generator. (arXiv:2208.13811v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13811">http://arxiv.org/abs/2208.13811</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13811] Synthetic Latent Fingerprint Generator](http://arxiv.org/abs/2208.13811)</code></li>
<li>Summary: <p>Given a full fingerprint image (rolled or slap), we present CycleGAN models
to generate multiple latent impressions of the same identity as the full print.
Our models can control the degree of distortion, noise, blurriness and
occlusion in the generated latent print images to obtain Good, Bad and Ugly
latent image categories as introduced in the NIST SD27 latent database. The
contributions of our work are twofold: (i) demonstrate the similarity of
synthetically generated latent fingerprint images to crime scene latents in
NIST SD27 and MSP databases as evaluated by the NIST NFIQ 2 quality measure and
ROC curves obtained by a SOTA fingerprint matcher, and (ii) use of synthetic
latents to augment small-size latent training databases in the public domain to
improve the performance of DeepPrint, a SOTA fingerprint matcher designed for
rolled to rolled fingerprint matching on three latent databases (NIST SD27,
NIST SD302, and IIITD-SLF). As an example, with synthetic latent data
augmentation, the Rank-1 retrieval performance of DeepPrint is improved from
15.50% to 29.07% on challenging NIST SD27 latent database. Our approach for
generating synthetic latent fingerprints can be used to improve the recognition
performance of any latent matcher and its individual components (e.g.,
enhancement, segmentation and feature extraction).
</p></li>
</ul>

<h3>Title: Video-based Cross-modal Auxiliary Network for Multimodal Sentiment Analysis. (arXiv:2208.13954v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13954">http://arxiv.org/abs/2208.13954</a></li>
<li>Code URL: <a href="https://github.com/rongfei-chen/VCAN">https://github.com/rongfei-chen/VCAN</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13954] Video-based Cross-modal Auxiliary Network for Multimodal Sentiment Analysis](http://arxiv.org/abs/2208.13954)</code></li>
<li>Summary: <p>Multimodal sentiment analysis has a wide range of applications due to its
information complementarity in multimodal interactions. Previous works focus
more on investigating efficient joint representations, but they rarely consider
the insufficient unimodal features extraction and data redundancy of multimodal
fusion. In this paper, a Video-based Cross-modal Auxiliary Network (VCAN) is
proposed, which is comprised of an audio features map module and a cross-modal
selection module. The first module is designed to substantially increase
feature diversity in audio feature extraction, aiming to improve classification
accuracy by providing more comprehensive acoustic representations. To empower
the model to handle redundant visual features, the second module is addressed
to efficiently filter the redundant visual frames during integrating
audiovisual data. Moreover, a classifier group consisting of several image
classification networks is introduced to predict sentiment polarities and
emotion categories. Extensive experimental results on RAVDESS, CMU-MOSI, and
CMU-MOSEI benchmarks indicate that VCAN is significantly superior to the
state-of-the-art methods for improving the classification accuracy of
multimodal sentiment analysis.
</p></li>
</ul>

<h3>Title: Boosting Night-time Scene Parsing with Learnable Frequency. (arXiv:2208.14241v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14241">http://arxiv.org/abs/2208.14241</a></li>
<li>Code URL: <a href="https://github.com/wangsen99/FDLNet">https://github.com/wangsen99/FDLNet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14241] Boosting Night-time Scene Parsing with Learnable Frequency](http://arxiv.org/abs/2208.14241)</code></li>
<li>Summary: <p>Night-Time Scene Parsing (NTSP) is essential to many vision applications,
especially for autonomous driving. Most of the existing methods are proposed
for day-time scene parsing. They rely on modeling pixel intensity-based spatial
contextual cues under even illumination. Hence, these methods do not perform
well in night-time scenes as such spatial contextual cues are buried in the
over-/under-exposed regions in night-time scenes. In this paper, we first
conduct an image frequency-based statistical experiment to interpret the
day-time and night-time scene discrepancies. We find that image frequency
distributions differ significantly between day-time and night-time scenes, and
understanding such frequency distributions is critical to NTSP problem. Based
on this, we propose to exploit the image frequency distributions for night-time
scene parsing. First, we propose a Learnable Frequency Encoder (LFE) to model
the relationship between different frequency coefficients to measure all
frequency components dynamically. Second, we propose a Spatial Frequency Fusion
module (SFF) that fuses both spatial and frequency information to guide the
extraction of spatial context features. Extensive experiments show that our
method performs favorably against the state-of-the-art methods on the
NightCity, NightCity+ and BDD100K-night datasets. In addition, we demonstrate
that our method can be applied to existing day-time scene parsing methods and
boost their performance on night-time scenes.
</p></li>
</ul>

<h3>Title: NEAR: Named Entity and Attribute Recognition of clinical concepts. (arXiv:2208.13949v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13949">http://arxiv.org/abs/2208.13949</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13949] NEAR: Named Entity and Attribute Recognition of clinical concepts](http://arxiv.org/abs/2208.13949)</code></li>
<li>Summary: <p>Named Entity Recognition (NER) or the extraction of concepts from clinical
text is the task of identifying entities in text and slotting them into
categories such as problems, treatments, tests, clinical departments,
occurrences (such as admission and discharge) and others. NER forms a critical
component of processing and leveraging unstructured data from Electronic Health
Records (EHR). While identifying the spans and categories of concepts is itself
a challenging task, these entities could also have attributes such as negation
that pivot their meanings implied to the consumers of the named entities. There
has been little research dedicated to identifying the entities and their
qualifying attributes together. This research hopes to contribute to the area
of detecting entities and their corresponding attributes by modelling the NER
task as a supervised, multi-label tagging problem with each of the attributes
assigned tagging sequence labels. In this paper, we propose 3 architectures to
achieve this multi-label entity tagging: BiLSTM n-CRF, BiLSTM-CRF-Smax-TF and
BiLSTM n-CRF-TF. We evaluate these methods on the 2010 i2b2/VA and the i2b2
2012 shared task datasets. Our different models obtain best NER F1 scores of 0.
894 and 0.808 on the i2b2 2010/VA and i2b2 2012 respectively. The highest span
based micro-averaged F1 polarity scores obtained were 0.832 and 0.836 on the
i2b2 2010/VA and i2b2 2012 datasets respectively, and the highest
macro-averaged F1 polarity scores obtained were 0.924 and 0.888 respectively.
The modality studies conducted on i2b2 2012 dataset revealed high scores of
0.818 and 0.501 for span based micro-averaged F1 and macro-averaged F1
respectively.
</p></li>
</ul>

<h3>Title: Combining keyphrase extraction and lexical diversity to characterize ideas in publication titles. (arXiv:2208.13978v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13978">http://arxiv.org/abs/2208.13978</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13978] Combining keyphrase extraction and lexical diversity to characterize ideas in publication titles](http://arxiv.org/abs/2208.13978)</code></li>
<li>Summary: <p>Beyond bibliometrics, there is interest in characterizing the evolution of
the number of ideas in scientific papers. A common approach for investigating
this involves analyzing the titles of publications to detect vocabulary changes
over time. With the notion that phrases, or more specifically keyphrases,
represent concepts, lexical diversity metrics are applied to phrased versions
of the titles. Thus changes in lexical diversity are treated as indicators of
shifts, and possibly expansion, of research. Therefore, optimizing detection of
keyphrases is an important aspect of this process. Rather than just one, we
propose to use multiple phrase detection models with the goal to produce a more
comprehensive set of keyphrases from the source corpora. Another potential
advantage to this approach is that the union and difference of these sets may
provide automated techniques for identifying and omitting non-specific phrases.
We compare the performance of several phrase detection models, analyze the
keyphrase sets output of each, and calculate lexical diversity of corpora
variants incorporating keyphrases from each model, using four common lexical
diversity metrics.
</p></li>
</ul>

<h3>Title: IMCI: Integrate Multi-view Contextual Information for Fact Extraction and Verification. (arXiv:2208.14001v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14001">http://arxiv.org/abs/2208.14001</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14001] IMCI: Integrate Multi-view Contextual Information for Fact Extraction and Verification](http://arxiv.org/abs/2208.14001)</code></li>
<li>Summary: <p>With the rapid development of automatic fake news detection technology, fact
extraction and verification (FEVER) has been attracting more attention. The
task aims to extract the most related fact evidences from millions of
open-domain Wikipedia documents and then verify the credibility of
corresponding claims. Although several strong models have been proposed for the
task and they have made great progress, we argue that they fail to utilize
multi-view contextual information and thus cannot obtain better performance. In
this paper, we propose to integrate multi-view contextual information (IMCI)
for fact extraction and verification. For each evidence sentence, we define two
kinds of context, i.e. intra-document context and inter-document context}.
Intra-document context consists of the document title and all the other
sentences from the same document. Inter-document context consists of all other
evidences which may come from different documents. Then we integrate the
multi-view contextual information to encode the evidence sentences to handle
the task. Our experimental results on FEVER 1.0 shared task show that our IMCI
framework makes great progress on both fact extraction and verification, and
achieves state-of-the-art performance with a winning FEVER score of 72.97% and
label accuracy of 75.84% on the online blind test set. We also conduct ablation
study to detect the impact of multi-view contextual information. Our codes will
be released at https://github.com/phoenixsecularbird/IMCI.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h2>fair</h2>
<h3>Title: Machine learning in the prediction of cardiac epicardial and mediastinal fat volumes. (arXiv:2208.14374v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14374">http://arxiv.org/abs/2208.14374</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14374] Machine learning in the prediction of cardiac epicardial and mediastinal fat volumes](http://arxiv.org/abs/2208.14374)</code></li>
<li>Summary: <p>We propose a methodology to predict the cardiac epicardial and mediastinal
fat volumes in computed tomography images using regression algorithms. The
obtained results indicate that it is feasible to predict these fats with a high
degree of correlation, thus alleviating the requirement for manual or automatic
segmentation of both fat volumes. Instead, segmenting just one of them
suffices, while the volume of the other may be predicted fairly precisely. The
correlation coefficient obtained by the Rotation Forest algorithm using MLP
Regressor for predicting the mediastinal fat based on the epicardial fat was
0.9876, with a relative absolute error of 14.4% and a root relative squared
error of 15.7%. The best correlation coefficient obtained in the prediction of
the epicardial fat based on the mediastinal was 0.9683 with a relative absolute
error of 19.6% and a relative squared error of 24.9%. Moreover, we analysed the
feasibility of using linear regressors, which provide an intuitive
interpretation of the underlying approximations. In this case, the obtained
correlation coefficient was 0.9534 for predicting the mediastinal fat based on
the epicardial, with a relative absolute error of 31.6% and a root relative
squared error of 30.1%. On the prediction of the epicardial fat based on the
mediastinal fat, the correlation coefficient was 0.8531, with a relative
absolute error of 50.43% and a root relative squared error of 52.06%. In
summary, it is possible to speed up general medical analyses and some
segmentation and quantification methods that are currently employed in the
state-of-the-art by using this prediction approach, which consequently reduces
costs and therefore enables preventive treatments that may lead to a reduction
of health problems.
</p></li>
</ul>

<h3>Title: RAGUEL: Recourse-Aware Group Unfairness Elimination. (arXiv:2208.14175v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.14175">http://arxiv.org/abs/2208.14175</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.14175] RAGUEL: Recourse-Aware Group Unfairness Elimination](http://arxiv.org/abs/2208.14175)</code></li>
<li>Summary: <p>While machine learning and ranking-based systems are in widespread use for
sensitive decision-making processes (e.g., determining job candidates,
assigning credit scores), they are rife with concerns over unintended biases in
their outcomes, which makes algorithmic fairness (e.g., demographic parity,
equal opportunity) an objective of interest. 'Algorithmic recourse' offers
feasible recovery actions to change unwanted outcomes through the modification
of attributes. We introduce the notion of ranked group-level recourse fairness,
and develop a 'recourse-aware ranking' solution that satisfies ranked recourse
fairness constraints while minimizing the cost of suggested modifications. Our
solution suggests interventions that can reorder the ranked list of database
records and mitigate group-level unfairness; specifically, disproportionate
representation of sub-groups and recourse cost imbalance. This re-ranking
identifies the minimum modifications to data points, with these attribute
modifications weighted according to their ease of recourse. We then present an
efficient block-based extension that enables re-ranking at any granularity
(e.g., multiple brackets of bank loan interest rates, multiple pages of search
engine results). Evaluation on real datasets shows that, while existing methods
may even exacerbate recourse unfairness, our solution -- RAGUEL --
significantly improves recourse-aware fairness. RAGUEL outperforms alternatives
at improving recourse fairness, through a combined process of counterfactual
generation and re-ranking, whilst remaining efficient for large-scale datasets.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: HiGNN: Hierarchical Informative Graph Neural Networks for Molecular Property Prediction Equipped with Feature-Wise Attention. (arXiv:2208.13994v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13994">http://arxiv.org/abs/2208.13994</a></li>
<li>Code URL: <a href="https://github.com/idruglab/hignn">https://github.com/idruglab/hignn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13994] HiGNN: Hierarchical Informative Graph Neural Networks for Molecular Property Prediction Equipped with Feature-Wise Attention](http://arxiv.org/abs/2208.13994)</code></li>
<li>Summary: <p>Elucidating and accurately predicting the druggability and bioactivities of
molecules plays a pivotal role in drug design and discovery and remains an open
challenge. Recently, graph neural networks (GNN) have made remarkable
advancements in graph-based molecular property prediction. However, current
graph-based deep learning methods neglect the hierarchical information of
molecules and the relationships between feature channels. In this study, we
propose a well-designed hierarchical informative graph neural networks
framework (termed HiGNN) for predicting molecular property by utilizing a
co-representation learning of molecular graphs and chemically synthesizable
BRICS fragments. Furthermore, a plug-and-play feature-wise attention block is
first designed in HiGNN architecture to adaptively recalibrate atomic features
after the message passing phase. Extensive experiments demonstrate that HiGNN
achieves state-of-the-art predictive performance on many challenging drug
discovery-associated benchmark datasets. In addition, we devise a
molecule-fragment similarity mechanism to comprehensively investigate the
interpretability of HiGNN model at the subgraph level, indicating that HiGNN as
a powerful deep learning tool can help chemists and pharmacists identify the
key components of molecules for designing better molecules with desired
properties or functions. The source code is publicly available at
https://github.com/idruglab/hignn.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
