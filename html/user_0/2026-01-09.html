<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-09</h1>
<h3>Title: MedPI: Evaluating AI Systems in Medical Patient-facing Interactions</h3>
<ul>
<li><strong>Authors: </strong>Diego Fajardo V., Oleksii Proniakin, Victoria-Elisabeth Gruber, Razvan Marinescu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04195">https://arxiv.org/abs/2601.04195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04195">https://arxiv.org/pdf/2601.04195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04195]] MedPI: Evaluating AI Systems in Medical Patient-facing Interactions(https://arxiv.org/abs/2601.04195)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present MedPI, a high-dimensional benchmark for evaluating large language models (LLMs) in patient-clinician conversations. Unlike single-turn question-answer (QA) benchmarks, MedPI evaluates the medical dialogue across 105 dimensions comprising the medical process, treatment safety, treatment outcomes and doctor-patient communication across a granular, accreditation-aligned rubric. MedPI comprises five layers: (1) Patient Packets (synthetic EHR-like ground truth); (2) an AI Patient instantiated through an LLM with memory and affect; (3) a Task Matrix spanning encounter reasons (e.g. anxiety, pregnancy, wellness checkup) x encounter objectives (e.g. diagnosis, lifestyle advice, medication advice); (4) an Evaluation Framework with 105 dimensions on a 1-4 scale mapped to the Accreditation Council for Graduate Medical Education (ACGME) competencies; and (5) AI Judges that are calibrated, committee-based LLMs providing scores, flags, and evidence-linked rationales. We evaluate 9 flagship models -- Claude Opus 4.1, Claude Sonnet 4, MedGemma, Gemini 2.5 Pro, Llama 3.3 70b Instruct, GPT-5, GPT OSS 120b, o3, Grok-4 -- across 366 AI Patients and 7,097 conversations using a standardized "vanilla clinician" prompt. For all LLMs, we observe low performance across a variety of dimensions, in particular on differential diagnosis. Our work can help guide future use of LLMs for diagnosis and treatment recommendations.</li>
</ul>

<h3>Title: Automatic Construction of Chinese Verb Collostruction Database</h3>
<ul>
<li><strong>Authors: </strong>Xuri Tang, Daohuan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04197">https://arxiv.org/abs/2601.04197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04197">https://arxiv.org/pdf/2601.04197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04197]] Automatic Construction of Chinese Verb Collostruction Database(https://arxiv.org/abs/2601.04197)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This paper proposes a fully unsupervised approach to the construction of verb collostruction database for Chinese language, aimed at complementing LLMs by providing explicit and interpretable rules for application scenarios where explanation and interpretability are indispensable. The paper formally defines a verb collostruction as a projective, rooted, ordered, and directed acyclic graph and employs a series of clustering algorithms to generate collostructions for a given verb from a list of sentences retrieved from large-scale corpus. Statistical analysis demonstrates that the generated collostructions possess the design features of functional independence and graded typicality. Evaluation with verb grammatical error correction shows that the error correction algorithm based on maximum matching with collostructions achieves better performance than LLMs.</li>
</ul>

<h3>Title: The Forgotten Shield: Safety Grafting in Parameter-Space for Medical MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiale Zhao, Xing Mou, Jinlin Wu, Hongyuan Yu, Mingrui Sun, Yang Shi, Xuanwu Yin, Zhen Chen, Zhen Lei, Yaohua Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04199">https://arxiv.org/abs/2601.04199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04199">https://arxiv.org/pdf/2601.04199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04199]] The Forgotten Shield: Safety Grafting in Parameter-Space for Medical MLLMs(https://arxiv.org/abs/2601.04199)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Medical Multimodal Large Language Models (Medical MLLMs) have achieved remarkable progress in specialized medical tasks; however, research into their safety has lagged, posing potential risks for real-world deployment. In this paper, we first establish a multidimensional evaluation framework to systematically benchmark the safety of current SOTA Medical MLLMs. Our empirical analysis reveals pervasive vulnerabilities across both general and medical-specific safety dimensions in existing models, particularly highlighting their fragility against cross-modality jailbreak attacks. Furthermore, we find that the medical fine-tuning process frequently induces catastrophic forgetting of the model's original safety alignment. To address this challenge, we propose a novel "Parameter-Space Intervention" approach for efficient safety re-alignment. This method extracts intrinsic safety knowledge representations from original base models and concurrently injects them into the target model during the construction of medical capabilities. Additionally, we design a fine-grained parameter search algorithm to achieve an optimal trade-off between safety and medical performance. Experimental results demonstrate that our approach significantly bolsters the safety guardrails of Medical MLLMs without relying on additional domain-specific safety data, while minimizing degradation to core medical performance.</li>
</ul>

<h3>Title: Attribute-Aware Controlled Product Generation with LLMs for E-commerce</h3>
<ul>
<li><strong>Authors: </strong>Virginia Negri, Víctor Martínez Gómez, Sergio A. Balanya, Subburam Rajaram</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04200">https://arxiv.org/abs/2601.04200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04200">https://arxiv.org/pdf/2601.04200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04200]] Attribute-Aware Controlled Product Generation with LLMs for E-commerce(https://arxiv.org/abs/2601.04200)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Product information extraction is crucial for e-commerce services, but obtaining high-quality labeled datasets remains challenging. We present a systematic approach for generating synthetic e-commerce product data using Large Language Models (LLMs), introducing a controlled modification framework with three strategies: attribute-preserving modification, controlled negative example generation, and systematic attribute removal. Using a state-of-the-art LLM with attribute-aware prompts, we enforce store constraints while maintaining product coherence. Human evaluation of 2000 synthetic products demonstrates high effectiveness, with 99.6% rated as natural, 96.5% containing valid attribute values, and over 90% showing consistent attribute usage. On the public MAVE dataset, our synthetic data achieves 60.5% accuracy, performing on par with real training data (60.8%) and significantly improving upon the 13.4% zero-shot baseline. Hybrid configurations combining synthetic and real data further improve performance, reaching 68.8% accuracy. Our framework provides a practical solution for augmenting e-commerce datasets, particularly valuable for low-resource scenarios.</li>
</ul>

<h3>Title: Collective Narrative Grounding: Community-Coordinated Data Contributions to Improve Local AI Systems</h3>
<ul>
<li><strong>Authors: </strong>Zihan Gao, Mohsin Y. K. Yousufi, Jacob Thebault-Spieker</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04201">https://arxiv.org/abs/2601.04201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04201">https://arxiv.org/pdf/2601.04201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04201]] Collective Narrative Grounding: Community-Coordinated Data Contributions to Improve Local AI Systems(https://arxiv.org/abs/2601.04201)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) question-answering systems often fail on community-specific queries, creating "knowledge blind spots" that marginalize local voices and reinforce epistemic injustice. We present Collective Narrative Grounding, a participatory protocol that transforms community stories into structured narrative units and integrates them into AI systems under community governance. Learning from three participatory mapping workshops with N=24 community members, we designed elicitation methods and a schema that retain narrative richness while enabling entity, time, and place extraction, validation, and provenance control. To scope the problem, we audit a county-level benchmark of 14,782 local information QA pairs, where factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments account for 76.7% of errors. On a participatory QA set derived from our workshops, a state-of-the-art LLM answered fewer than 21% of questions correctly without added context, underscoring the need for local grounding. The missing facts often appear in the collected narratives, suggesting a direct path to closing the dominant error modes for narrative items. Beyond the protocol and pilot, we articulate key design tensions, such as representation and power, governance and control, and privacy and consent, providing concrete requirements for retrieval-first, provenance-visible, locally governed QA systems. Together, our taxonomy, protocol, and participatory evaluation offer a rigorous foundation for building community-grounded AI that better answers local questions.</li>
</ul>

<h3>Title: TeleTables: A Benchmark for Large Language Models in Telecom Table Interpretation</h3>
<ul>
<li><strong>Authors: </strong>Anas Ezzakri, Nicola Piovesan, Mohamed Sana, Antonio De Domenico, Fadhel Ayed, Haozhe Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04202">https://arxiv.org/abs/2601.04202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04202">https://arxiv.org/pdf/2601.04202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04202]] TeleTables: A Benchmark for Large Language Models in Telecom Table Interpretation(https://arxiv.org/abs/2601.04202)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Language Models (LLMs) are increasingly explored in the telecom industry to support engineering tasks, accelerate troubleshooting, and assist in interpreting complex technical documents. However, recent studies show that LLMs perform poorly on telecom standards, particularly 3GPP specifications. We argue that a key reason is that these standards densely include tables to present essential information, yet the LLM knowledge and interpretation ability of such tables remains largely unexamined. To address this gap, we introduce TeleTables, a benchmark designed to evaluate both the implicit knowledge LLMs have about tables in technical specifications and their explicit ability to interpret them. TeleTables is built through a novel multi-stage data generation pipeline that extracts tables from 3GPP standards and uses multimodal and reasoning-oriented LLMs to generate and validate questions. The resulting dataset, which is publicly available, comprises 500 human-verified question-answer pairs, each associated with the corresponding table in multiple formats. Our evaluation shows that, smaller models (under 10B parameters) struggle both to recall 3GPP knowledge and to interpret tables, indicating the limited exposure to telecom standards in their pretraining and the insufficient inductive biases for navigating complex technical material. Larger models, on the other hand, show stronger reasoning on table interpretation. Overall, TeleTables highlights the need for domain-specialized fine-tuning to reliably interpret and reason over telecom standards.</li>
</ul>

<h3>Title: STDD:Spatio-Temporal Dynamics-Driven Token Refinement in Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinhao Sun, Maoliang Li, Zihao Zheng, Jiayu Chen, Hezhao Xu, Yun Liang, Xiang Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04205">https://arxiv.org/abs/2601.04205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04205">https://arxiv.org/pdf/2601.04205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04205]] STDD:Spatio-Temporal Dynamics-Driven Token Refinement in Diffusion Language Models(https://arxiv.org/abs/2601.04205)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Unlike autoregressive language models, diffusion language models (DLMs) generate text by iteratively denoising all token positions in parallel. At each timestep, the remasking strategy of a DLM selects low- priority tokens to defer their decoding, thereby improving both efficiency and output quality. However, mainstream remasking strategies rely on a single global confidence threshold, overlooking the temporal and spatial dynamics of individual tokens. Motivated by the redundant iterations and constrained parallelism introduced by fixed-threshold remasking, we propose a novel remasking approach that dynamically detects Temporal Variance and Spa- tial Deviance of each token, which reflect its convergence status and inter-token correlations. Using these signals, our method adaptively adjusts the confidence threshold for every token at every step. Empirical re- sults show that our approach significantly improves the operational efficiency of DLMs across mainstream datasets, achieving speedups of up to 8.9 times while faithfully preserving generation quality.</li>
</ul>

<h3>Title: LLMs for Explainable Business Decision-Making: A Reinforcement Learning Fine-Tuning Approach</h3>
<ul>
<li><strong>Authors: </strong>Xiang Cheng, Wen Wang, Anindya Ghose</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04208">https://arxiv.org/abs/2601.04208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04208">https://arxiv.org/pdf/2601.04208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04208]] LLMs for Explainable Business Decision-Making: A Reinforcement Learning Fine-Tuning Approach(https://arxiv.org/abs/2601.04208)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (AI) models increasingly drive high-stakes consumer interactions, yet their decision logic often remains opaque. Prevailing explainable AI techniques rely on post hoc numerical feature attributions, which fail to provide coherent narratives behind model decisions. Large language models (LLMs) present an opportunity to generate natural-language explanations, but three design challenges remain unresolved: explanations must be both decision-correct and faithful to the factors that drive the prediction; they should be able to serve multiple audiences without shifting the underlying decision rule; and they should be trained in a label-efficient way that does not depend on large corpora of human-scored explanations. To address these challenges, we introduce LEXMA (LLM-based EXplanations for Multi-Audience decisions), a reinforcement-learning-based fine-tuning framework that produces narrative-driven, audience-appropriate explanations. LEXMA combines reflection-augmented supervised fine-tuning with two stages of Group Relative Policy Optimization (GRPO). Specifically, it fine-tunes two separate parameter sets to improve decision correctness and satisfy stylistic requirements for different audiences, using reward signals that do not rely on human-annotated explanations. We instantiate LEXMA in the context of mortgage approval decisions. Results demonstrate that LEXMA yields significant improvements in predictive performance compared with other LLM baselines. Moreover, human evaluations show that expert-facing explanations generated by our approach are more risk-focused, and consumer-facing explanations are clearer, more actionable, and more polite. Our study contributes a cost-efficient, systematic LLM fine-tuning approach to enhance explanation quality for business decisions, offering strong potential for scalable deployment of transparent AI systems.</li>
</ul>

<h3>Title: Leveraging Language Models and RAG for Efficient Knowledge Discovery in Clinical Environments</h3>
<ul>
<li><strong>Authors: </strong>Seokhwan Ko, Donghyeon Lee, Jaewoo Chun, Hyungsoo Han, Junghwan Cho</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04209">https://arxiv.org/abs/2601.04209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04209">https://arxiv.org/pdf/2601.04209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04209]] Leveraging Language Models and RAG for Efficient Knowledge Discovery in Clinical Environments(https://arxiv.org/abs/2601.04209)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly recognized as valuable tools across the medical environment, supporting clinical, research, and administrative workflows. However, strict privacy and network security regulations in hospital settings require that sensitive data be processed within fully local infrastructures. Within this context, we developed and evaluated a retrieval-augmented generation (RAG) system designed to recommend research collaborators based on PubMed publications authored by members of a medical institution. The system utilizes PubMedBERT for domain-specific embedding generation and a locally deployed LLaMA3 model for generative synthesis. This study demonstrates the feasibility and utility of integrating domain-specialized encoders with lightweight LLMs to support biomedical knowledge discovery under local deployment constraints.</li>
</ul>

<h3>Title: Complexity Agnostic Recursive Decomposition of Thoughts</h3>
<ul>
<li><strong>Authors: </strong>Kaleem Ullah Qasim, Jiashu Zhang, Hafiz Saif Ur Rehman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04210">https://arxiv.org/abs/2601.04210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04210">https://arxiv.org/pdf/2601.04210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04210]] Complexity Agnostic Recursive Decomposition of Thoughts(https://arxiv.org/abs/2601.04210)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models often fail on multi-step reasoning due to fixed reasoning strategies that ignore problem specific difficulty. We introduce CARD (Complexity Agnostic Recursive Decomposition), a framework that predicts problem complexity before generation and adapts decomposition accordingly. Our system comprises MRCE (Multi-dimensional Reasoning Complexity Estimator), a 0.6B Qwen model predicting 30 fine-grained features from question text and a two-stage recursive solver: (1) hierarchical decomposition into K steps based on task profile and (2) per-step thought budget allocation (1, 5-9, or 10 thoughts) via recursive MRCE profiling. Evaluated on three reasoning models (Qwen3-0.6B, DeepSeek-R1-Distill-Qwen-1.5B, Qwen3-1.7B), CARD achieves 81.4% to 89.2% accuracy on GSM8K while reducing token cost by 1.88x to 2.40x compared to fixed decomposition baselines. On MATH-500, CARD reaches 75.1 to 86.8% accuracy using 1.71x to 5.74x fewer tokens. Our results demonstrate that preemptive complexity estimation enables both higher accuracy and significant efficiency gains.</li>
</ul>

<h3>Title: Qwerty AI: Explainable Automated Age Rating and Content Safety Assessment for Russian-Language Screenplays</h3>
<ul>
<li><strong>Authors: </strong>Nikita Zmanovskii</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04211">https://arxiv.org/abs/2601.04211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04211">https://arxiv.org/pdf/2601.04211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04211]] Qwerty AI: Explainable Automated Age Rating and Content Safety Assessment for Russian-Language Screenplays(https://arxiv.org/abs/2601.04211)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present Qwerty AI, an end-to-end system for automated age-rating and content-safety assessment of Russian-language screenplays according to Federal Law No. 436-FZ. The system processes full-length scripts (up to 700 pages in under 2 minutes), segments them into narrative units, detects content violations across five categories (violence, sexual content, profanity, substances, frightening elements), and assigns age ratings (0+, 6+, 12+, 16+, 18+) with explainable justifications. Our implementation leverages a fine-tuned Phi-3-mini model with 4-bit quantization, achieving 80% rating accuracy and 80-95% segmentation precision (format-dependent). The system was developed under strict constraints: no external API calls, 80GB VRAM limit, and <5 minute processing time for average scripts. Deployed on Yandex Cloud with CUDA acceleration, Qwerty AI demonstrates practical applicability for production workflows. We achieved these results during the Wink hackathon (November 2025), where our solution addressed real editorial challenges in the Russian media industry.</li>
</ul>

<h3>Title: TrueBrief: Faithful Summarization through Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kumud Lakara, Ruibo Shi, Fran Silavong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04212">https://arxiv.org/abs/2601.04212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04212">https://arxiv.org/pdf/2601.04212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04212]] TrueBrief: Faithful Summarization through Small Language Models(https://arxiv.org/abs/2601.04212)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have exhibited remarkable proficiency in generating high-quality text; however, their propensity for producing hallucinations poses a significant challenge for their deployment in security-critical domains. In this work, we present TrueBrief, an end-to-end framework specifically designed to enhance the faithfulness of small LLMs (SLMs) primarily for the task of text summarization through a preference-optimization paradigm. Central to our framework is a data generation module that facilitates controlled hallucination injection to generate synthetic preference data. Our work provides insights into the impact of data quality and model size on preference-based optimization, highlighting the conditions under which these methods are most effective.</li>
</ul>

<h3>Title: AnimatedLLM: Explaining LLMs with Interactive Visualizations</h3>
<ul>
<li><strong>Authors: </strong>Zdeněk Kasner, Ondřej Dušek</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04213">https://arxiv.org/abs/2601.04213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04213">https://arxiv.org/pdf/2601.04213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04213]] AnimatedLLM: Explaining LLMs with Interactive Visualizations(https://arxiv.org/abs/2601.04213)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are becoming central to natural language processing education, yet materials showing their mechanics are sparse. We present AnimatedLLM, an interactive web application that provides step-by-step visualizations of a Transformer language model. AnimatedLLM runs entirely in the browser, using pre-computed traces of open LLMs applied on manually curated inputs. The application is available at this https URL, both as a teaching aid and for self-educational purposes.</li>
</ul>

<h3>Title: Social Engineering Attacks: A Systemisation of Knowledge on People Against Humans</h3>
<ul>
<li><strong>Authors: </strong>Scott Thomson, Michael Bewong, Arash Mahboubi, Tanveer Zia</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04215">https://arxiv.org/abs/2601.04215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04215">https://arxiv.org/pdf/2601.04215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04215]] Social Engineering Attacks: A Systemisation of Knowledge on People Against Humans(https://arxiv.org/abs/2601.04215)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Our systematisation of knowledge on Social Engineering Attacks (SEAs), identifies the human, organisational, and adversarial dimensions of cyber threats. It addresses the growing risks posed by SEAs, highly relevant in the context physical cyber places, such as travellers at airports and residents in smart cities, and synthesizes findings from peer reviewed studies, industry and government reports to inform effective countermeasures that can be embedded into future smart city strategies. SEAs increasingly sidestep technical controls by weaponising leaked personal data and behavioural cues, an urgency underscored by the Optus, Medibank and now Qantas (2025) mega breaches that placed millions of personal records in criminals' hands. Our review surfaces three critical dimensions: (i) human factors of knowledge, abilities and behaviours (KAB) (ii) organisational culture and informal norms that shape those behaviours and (iii) attacker motivations, techniques and return on investment calculations. Our contributions are threefold: (1) TriLayer Systematisation: to the best of our knowledge, we are the first to unify KAB metrics, cultural drivers and attacker economics into a single analytical lens, enabling practitioners to see how vulnerabilities, norms and threat incentives coevolve. (2) Risk Weighted HAISQ Meta analysis: By normalising and ranking HAISQ scores across recent field studies, we reveal persistent high risk clusters (Internet and Social Media use) and propose impact weightings that make the instrument predictive rather than descriptive. (3) Adaptive 'Segment and Simulate' Training Blueprint: Building on clustering evidence, we outline a differentiated programme that matches low, medium, high risk user cohorts to experiential learning packages including phishing simulations, gamified challenges and realtime feedback thereby aligning effort with measured exposure.</li>
</ul>

<h3>Title: Integrating Multi-Agent Simulation, Behavioral Forensics, and Trust-Aware Machine Learning for Adaptive Insider Threat Detection</h3>
<ul>
<li><strong>Authors: </strong>Firdous Kausar, Asmah Muallem, Naw Safrin Sattar, Mohamed Zakaria Kurdi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04243">https://arxiv.org/abs/2601.04243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04243">https://arxiv.org/pdf/2601.04243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04243]] Integrating Multi-Agent Simulation, Behavioral Forensics, and Trust-Aware Machine Learning for Adaptive Insider Threat Detection(https://arxiv.org/abs/2601.04243)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>We present a hybrid framework for adaptive insider-threat detection that tightly integrates multi-agent simulation (MAS), layered Security Information and Event Management (SIEM) correlation, behavioral and communication forensics, trust-aware machine learning, and Theory-of-Mind (ToM) reasoning. Intelligent agents operate in a simulated enterprise environment, generating both behavioral events and cognitive intent signals that are ingested by a centralized SIEM. We evaluate four system variants: a Layered SIEM-Core (LSC) baseline, a Cognitive-Enriched SIEM (CE-SIEM) incorporating ToM and communication forensics, an Evidence-Gated SIEM (EG-SIEM) introducing precision-focused validation mechanisms, and an Enron-enabled EG-SIEM (EG-SIEM-Enron) that augments evidence gating with a pretrained email forensics module calibrated on Enron corpora. Across ten simulation runs involving eight malicious insiders, CE-SIEM achieves perfect recall (1.000) and improves actor-level F1 from 0.521 (LSC) to 0.774. EG-SIEM raises actor-level F1 to 0.922 and confirmed-alert precision to 0.997 while reducing false positives to 0.2 per run. EG-SIEM-Enron preserves high precision (1.000 confirmed-alert precision; 0.0 false positives per run), slightly improves actor-level F1 to 0.933, and reduces detection latency (average TTD 10.26 steps versus 15.20 for EG-SIEM). These results demonstrate that cognitive context improves sensitivity, evidence-gated validation enables high-precision, low-noise detection, and pretrained communication calibration can further accelerate high-confidence insider threat identification.</li>
</ul>

<h3>Title: Beyond Immediate Activation: Temporally Decoupled Backdoor Attacks on Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Zhixin Liu, Xuanlin Liu, Sihan Xu, Yaqiong Qiao, Ying Zhang, Xiangrui Cai</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04247">https://arxiv.org/abs/2601.04247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04247">https://arxiv.org/pdf/2601.04247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04247]] Beyond Immediate Activation: Temporally Decoupled Backdoor Attacks on Time Series Forecasting(https://arxiv.org/abs/2601.04247)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal</a></li>
<li><strong>Abstract: </strong>Existing backdoor attacks on multivariate time series (MTS) forecasting enforce strict temporal and dimensional coupling between triggers and target patterns, requiring synchronous activation at fixed positions across variables. However, realistic scenarios often demand delayed and variable-specific activation. We identify this critical unmet need and propose TDBA, a temporally decoupled backdoor attack framework for MTS forecasting. By injecting triggers that encode the expected location of the target pattern, TDBA enables the activation of the target pattern at any positions within the forecasted data, with the activation position flexibly varying across different variable dimensions. TDBA introduces two core modules: (1) a position-guided trigger generation mechanism that leverages smoothed Gaussian priors to generate triggers that are position-related to the predefined target pattern; and (2) a position-aware optimization module that assigns soft weights based on trigger completeness, pattern coverage, and temporal offset, facilitating targeted and stealthy attack optimization. Extensive experiments on real-world datasets show that TDBA consistently outperforms existing baselines in effectiveness while maintaining good stealthiness. Ablation studies confirm the controllability and robustness of its design.</li>
</ul>

<h3>Title: Inhibitory Attacks on Backdoor-based Fingerprinting for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hang Fu, Wanli Peng, Yinghan Zhou, Jiaxuan Wu, Juan Wen, Yiming Xue</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04261">https://arxiv.org/abs/2601.04261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04261">https://arxiv.org/pdf/2601.04261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04261]] Inhibitory Attacks on Backdoor-based Fingerprinting for Large Language Models(https://arxiv.org/abs/2601.04261)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The widespread adoption of Large Language Model (LLM) in commercial and research settings has intensified the need for robust intellectual property protection. Backdoor-based LLM fingerprinting has emerged as a promising solution for this challenge. In practical application, the low-cost multi-model collaborative technique, LLM ensemble, combines diverse LLMs to leverage their complementary strengths, garnering significant attention and practical adoption. Unfortunately, the vulnerability of existing LLM fingerprinting for the ensemble scenario is unexplored. In order to comprehensively assess the robustness of LLM fingerprinting, in this paper, we propose two novel fingerprinting attack methods: token filter attack (TFA) and sentence verification attack (SVA). The TFA gets the next token from a unified set of tokens created by the token filter mechanism at each decoding step. The SVA filters out fingerprint responses through a sentence verification mechanism based on perplexity and voting. Experimentally, the proposed methods effectively inhibit the fingerprint response while maintaining ensemble performance. Compared with state-of-the-art attack methods, the proposed method can achieve better performance. The findings necessitate enhanced robustness in LLM fingerprinting.</li>
</ul>

<h3>Title: Safety-Utility Conflicts Are Not Global: Surgical Alignment via Head-Level Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Wang Cai, Yilin Wen, Jinchang Hou, Du Su, Guoqiu Wang, Zhonghou Lv, Chenfu Bao, Yunfang Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04262">https://arxiv.org/abs/2601.04262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04262">https://arxiv.org/pdf/2601.04262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04262]] Safety-Utility Conflicts Are Not Global: Surgical Alignment via Head-Level Diagnosis(https://arxiv.org/abs/2601.04262)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Safety alignment in Large Language Models (LLMs) inherently presents a multi-objective optimization conflict, often accompanied by an unintended degradation of general capabilities. Existing mitigation strategies typically rely on global gradient geometry to resolve these conflicts, yet they overlook Modular Heterogeneity within Transformers, specifically that the functional sensitivity and degree of conflict vary substantially across different attention heads. Such global approaches impose uniform update rules across all parameters, often resulting in suboptimal trade-offs by indiscriminately updating utility sensitive heads that exhibit intense gradient conflicts. To address this limitation, we propose Conflict-Aware Sparse Tuning (CAST), a framework that integrates head-level diagnosis with sparse fine-tuning. CAST first constructs a pre-alignment conflict map by synthesizing Optimization Conflict and Functional Sensitivity, which then guides the selective update of parameters. Experiments reveal that alignment conflicts in LLMs are not uniformly distributed. We find that the drop in general capabilities mainly comes from updating a small group of ``high-conflict'' heads. By simply skipping these heads during training, we significantly reduce this loss without compromising safety, offering an interpretable and parameter-efficient approach to improving the safety-utility trade-off.</li>
</ul>

<h3>Title: Learning to Reason: Temporal Saliency Distillation for Interpretable Knowledge Transfer</h3>
<ul>
<li><strong>Authors: </strong>Nilushika Udayangani Hewa Dehigahawattage, Kishor Nandakishor, Marimuthu Palaniswami</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04263">https://arxiv.org/abs/2601.04263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04263">https://arxiv.org/pdf/2601.04263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04263]] Learning to Reason: Temporal Saliency Distillation for Interpretable Knowledge Transfer(https://arxiv.org/abs/2601.04263)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Knowledge distillation has proven effective for model compression by transferring knowledge from a larger network called the teacher to a smaller network called the student. Current knowledge distillation in time series is predominantly based on logit and feature aligning techniques originally developed for computer vision tasks. These methods do not explicitly account for temporal data and fall short in two key aspects. First, the mechanisms by which the transferred knowledge helps the student model learning process remain unclear due to uninterpretability of logits and features. Second, these methods transfer only limited knowledge, primarily replicating the teacher predictive accuracy. As a result, student models often produce predictive distributions that differ significantly from those of their teachers, hindering their safe substitution for teacher models. In this work, we propose transferring interpretable knowledge by extending conventional logit transfer to convey not just the right prediction but also the right reasoning of the teacher. Specifically, we induce other useful knowledge from the teacher logits termed temporal saliency which captures the importance of each input timestep to the teacher prediction. By training the student with Temporal Saliency Distillation we encourage it to make predictions based on the same input features as the teacher. Temporal Saliency Distillation requires no additional parameters or architecture specific assumptions. We demonstrate that Temporal Saliency Distillation effectively improves the performance of baseline methods while also achieving desirable properties beyond predictive accuracy. We hope our work establishes a new paradigm for interpretable knowledge distillation in time series analysis.</li>
</ul>

<h3>Title: You Only Anonymize What Is Not Intent-Relevant: Suppressing Non-Intent Privacy Evidence</h3>
<ul>
<li><strong>Authors: </strong>Weihao Shen, Yaxin Xu, Shuang Li, Wei Chen, Yuqin Lan, Meng Yuan, Fuzhen Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04265">https://arxiv.org/abs/2601.04265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04265">https://arxiv.org/pdf/2601.04265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04265]] You Only Anonymize What Is Not Intent-Relevant: Suppressing Non-Intent Privacy Evidence(https://arxiv.org/abs/2601.04265)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Anonymizing sensitive information in user text is essential for privacy, yet existing methods often apply uniform treatment across attributes, which can conflict with communicative intent and obscure necessary information. This is particularly problematic when personal attributes are integral to expressive or pragmatic goals. The central challenge lies in determining which attributes to protect, and to what extent, while preserving semantic and pragmatic functions. We propose IntentAnony, a utility-preserving anonymization approach that performs intent-conditioned exposure control. IntentAnony models pragmatic intent and constructs privacy inference evidence chains to capture how distributed cues support attribute inference. Conditioned on intent, it assigns each attribute an exposure budget and selectively suppresses non-intent inference pathways while preserving intent-relevant content, semantic structure, affective nuance, and interactional function. We evaluate IntentAnony using privacy inference success rates, text utility metrics, and human evaluation. The results show an approximately 30% improvement in the overall privacy--utility trade-off, with notably stronger usability of anonymized text compared to prior state-of-the-art methods. Our code is available at this https URL.</li>
</ul>

<h3>Title: State Backdoor: Towards Stealthy Real-world Poisoning Attack on Vision-Language-Action Model in State Space</h3>
<ul>
<li><strong>Authors: </strong>Ji Guo, Wenbo Jiang, Yansong Lin, Yijing Liu, Ruichen Zhang, Guomin Lu, Aiguo Chen, Xinshuo Han, Hongwei Li, Dusit Niyato</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04266">https://arxiv.org/abs/2601.04266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04266">https://arxiv.org/pdf/2601.04266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04266]] State Backdoor: Towards Stealthy Real-world Poisoning Attack on Vision-Language-Action Model in State Space(https://arxiv.org/abs/2601.04266)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>Vision-Language-Action (VLA) models are widely deployed in safety-critical embodied AI applications such as robotics. However, their complex multimodal interactions also expose new security vulnerabilities. In this paper, we investigate a backdoor threat in VLA models, where malicious inputs cause targeted misbehavior while preserving performance on clean data. Existing backdoor methods predominantly rely on inserting visible triggers into visual modality, which suffer from poor robustness and low insusceptibility in real-world settings due to environmental variability. To overcome these limitations, we introduce the State Backdoor, a novel and practical backdoor attack that leverages the robot arm's initial state as the trigger. To optimize trigger for insusceptibility and effectiveness, we design a Preference-guided Genetic Algorithm (PGA) that efficiently searches the state space for minimal yet potent triggers. Extensive experiments on five representative VLA models and five real-world tasks show that our method achieves over 90% attack success rate without affecting benign task performance, revealing an underexplored vulnerability in embodied AI systems.</li>
</ul>

<h3>Title: Making Tunable Parameters State-Dependent in Weather and Climate Models with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Pritthijit Nath, Sebastian Schemm, Henry Moss, Peter Haynes, Emily Shuckburgh, Mark J. Webb</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04268">https://arxiv.org/abs/2601.04268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04268">https://arxiv.org/pdf/2601.04268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04268]] Making Tunable Parameters State-Dependent in Weather and Climate Models with Reinforcement Learning(https://arxiv.org/abs/2601.04268)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Weather and climate models rely on parametrisations to represent unresolved sub-grid processes. Traditional schemes rely on fixed coefficients that are weakly constrained and tuned offline, contributing to persistent biases that limit their ability to adapt to the underlying physics. This study presents a framework that learns components of parametrisation schemes online as a function of the evolving model state using reinforcement learning (RL) and evaluates the resulting RL-driven parameter updates across a hierarchy of idealised testbeds spanning a simple climate bias correction (SCBC), a radiative-convective equilibrium (RCE), and a zonal mean energy balance model (EBM) with both single-agent and federated multi-agent settings. Across nine RL algorithms, Truncated Quantile Critics (TQC), Deep Deterministic Policy Gradient (DDPG), and Twin Delayed DDPG (TD3) achieved the highest skill and the most stable convergence across configurations, with performance assessed against a static baseline using area-weighted RMSE, temperature profile and pressure-level diagnostics. For the EBM, single-agent RL outperformed static parameter tuning with the strongest gains in tropical and mid-latitude bands, while federated RL on multi-agent setups enabled geographically specialised control and faster convergence, with a six-agent DDPG configuration using frequent aggregation yielding the lowest area-weighted RMSE across the tropics and mid-latitudes. The learnt corrections were also physically meaningful as agents modulated EBM radiative parameters to reduce meridional biases, adjusted RCE lapse rates to match vertical temperature errors, and stabilised SCBC heating increments to limit drift. Overall, results highlight RL to deliver skilful state-dependent, and regime-aware parametrisations, offering a scalable pathway for online learning within numerical models.</li>
</ul>

<h3>Title: Predictable Gradient Manifolds in Deep Learning: Temporal Path-Length and Intrinsic Rank as a Complexity Regime</h3>
<ul>
<li><strong>Authors: </strong>Anherutowa Calvo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04270">https://arxiv.org/abs/2601.04270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04270">https://arxiv.org/pdf/2601.04270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04270]] Predictable Gradient Manifolds in Deep Learning: Temporal Path-Length and Intrinsic Rank as a Complexity Regime(https://arxiv.org/abs/2601.04270)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Deep learning optimization exhibits structure that is not captured by worst-case gradient bounds. Empirically, gradients along training trajectories are often temporally predictable and evolve within a low-dimensional subspace. In this work we formalize this observation through a measurable framework for predictable gradient manifolds. We introduce two computable quantities: a prediction-based path length that measures how well gradients can be forecast from past information, and a predictable rank that quantifies the intrinsic temporal dimension of gradient increments. We show how classical online and nonconvex optimization guarantees can be restated so that convergence and regret depend explicitly on these quantities, rather than on worst-case variation. Across convolutional networks, vision transformers, language models, and synthetic control tasks, we find that gradient trajectories are locally predictable and exhibit strong low-rank structure over time. These properties are stable across architectures and optimizers, and can be diagnosed directly from logged gradients using lightweight random projections. Our results provide a unifying lens for understanding optimization dynamics in modern deep learning, reframing standard training as operating in a low-complexity temporal regime. This perspective suggests new directions for adaptive optimizers, rank-aware tracking, and prediction-based algorithm design grounded in measurable properties of real training runs.</li>
</ul>

<h3>Title: Shadow Unlearning: A Neuro-Semantic Approach to Fidelity-Preserving Faceless Forgetting in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Dinesh Srivasthav P, Ashok Urlana, Rahul Mishra, Bala Mallikarjunarao Garlapati, Ponnurangam Kumaraguru</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04275">https://arxiv.org/abs/2601.04275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04275">https://arxiv.org/pdf/2601.04275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04275]] Shadow Unlearning: A Neuro-Semantic Approach to Fidelity-Preserving Faceless Forgetting in LLMs(https://arxiv.org/abs/2601.04275)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Machine unlearning aims to selectively remove the influence of specific training samples to satisfy privacy regulations such as the GDPR's 'Right to be Forgotten'. However, many existing methods require access to the data being removed, exposing it to membership inference attacks and potential misuse of Personally Identifiable Information (PII). We address this critical challenge by proposing Shadow Unlearning, a novel paradigm of approximate unlearning, that performs machine unlearning on anonymized forget data without exposing PII. We further propose a novel privacy-preserving framework, Neuro-Semantic Projector Unlearning (NSPU) to achieve Shadow unlearning. To evaluate our method, we compile Multi-domain Fictitious Unlearning (MuFU) forget set across five diverse domains and introduce an evaluation stack to quantify the trade-off between knowledge retention and unlearning effectiveness. Experimental results on various LLMs show that NSPU achieves superior unlearning performance, preserves model utility, and enhances user privacy. Additionally, the proposed approach is at least 10 times more computationally efficient than standard unlearning approaches. Our findings foster a new direction for privacy-aware machine unlearning that balances data protection and model fidelity.</li>
</ul>

<h3>Title: Unlocking the Pre-Trained Model as a Dual-Alignment Calibrator for Post-Trained LLMs</h3>
<ul>
<li><strong>Authors: </strong>Beier Luo, Cheng Wang, Hongxin Wei, Sharon Li, Xuefeng Du</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04277">https://arxiv.org/abs/2601.04277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04277">https://arxiv.org/pdf/2601.04277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04277]] Unlocking the Pre-Trained Model as a Dual-Alignment Calibrator for Post-Trained LLMs(https://arxiv.org/abs/2601.04277)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Post-training improves large language models (LLMs) but often worsens confidence calibration, leading to systematic overconfidence. Recent unsupervised post-hoc methods for post-trained LMs (PoLMs) mitigate this by aligning PoLM confidence to that of well-calibrated pre-trained counterparts. However, framing calibration as static output-distribution matching overlooks the inference-time dynamics introduced by post-training. In particular, we show that calibration errors arise from two regimes: (i) confidence drift, where final confidence inflates despite largely consistent intermediate decision processes, and (ii) process drift, where intermediate inference pathways diverge. Guided by this diagnosis, we propose Dual-Align, an unsupervised post-hoc framework for dual alignment in confidence calibration. Dual-Align performs confidence alignment to correct confidence drift via final-distribution matching, and introduces process alignment to address process drift by locating the layer where trajectories diverge and realigning the stability of subsequent inference. This dual strategy learns a single temperature parameter that corrects both drift types without sacrificing post-training performance gains. Experiments show consistent improvements over baselines, reducing calibration errors and approaching a supervised oracle.</li>
</ul>

<h3>Title: From Domains to Instances: Dual-Granularity Data Synthesis for LLM Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Xu, Minxin Du, Zitong Li, Zi Liang, Zhibiao Guo, Shiyu Zhang, Peizhao Hu, Qingqing Ye, Haibo Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04278">https://arxiv.org/abs/2601.04278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04278">https://arxiv.org/pdf/2601.04278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04278]] From Domains to Instances: Dual-Granularity Data Synthesis for LLM Unlearning(https://arxiv.org/abs/2601.04278)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Although machine unlearning is essential for removing private, harmful, or copyrighted content from LLMs, current benchmarks often fail to faithfully represent the true "forgetting scope" learned by the model. We formalize two distinct unlearning granularities, domain-level and instance-level, and propose BiForget, an automated framework for synthesizing high-quality forget sets. Unlike prior work relying on external generators, BiForget exploits the target model per se to elicit data that matches its internal knowledge distribution through seed-guided and adversarial prompting. Our experiments across diverse benchmarks show that it achieves a superior balance of relevance, diversity, and efficiency. Quantitatively, in the Harry Potter domain, it improves relevance by ${\sim}20$ and diversity by ${\sim}$0.05 while halving the total data size compared to SOTAs. Ultimately, it facilitates more robust forgetting and better utility preservation, providing a more rigorous foundation for evaluating LLM unlearning.</li>
</ul>

<h3>Title: Generation of synthetic delay time series for air transport applications</h3>
<ul>
<li><strong>Authors: </strong>Pau Esteve, Massimiliano Zanin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04279">https://arxiv.org/abs/2601.04279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04279">https://arxiv.org/pdf/2601.04279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04279]] Generation of synthetic delay time series for air transport applications(https://arxiv.org/abs/2601.04279)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The generation of synthetic data is receiving increasing attention from the scientific community, thanks to its ability to solve problems like data scarcity and privacy, and is starting to find applications in air transport. We here tackle the problem of generating synthetic, yet realistic, time series of delays at airports, starting from large collections of operations in Europe and the US. We specifically compare three models, two of them based on state of the art Deep Learning algorithms, and one simplified Genetic Algorithm approach. We show how the latter can generate time series that are almost indistinguishable from real ones, while maintaining a high variability. We further validate the resulting time series in a problem of detecting delay propagations between airports. We finally make the synthetic data available to the scientific community.</li>
</ul>

<h3>Title: A Privacy-Preserving Localization Scheme with Node Selection in Mobile Networks</h3>
<ul>
<li><strong>Authors: </strong>Liangbo Xie, Mude Cai, Xiaolong Yang, Mu Zhou, Jiacheng Wang, Dusit Niyato</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04280">https://arxiv.org/abs/2601.04280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04280">https://arxiv.org/pdf/2601.04280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04280]] A Privacy-Preserving Localization Scheme with Node Selection in Mobile Networks(https://arxiv.org/abs/2601.04280)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Localization in mobile networks has been widely applied in many scenarios. However, an entity responsible for location estimation exposes both the target and anchors to potential location leakage at any time, creating serious security risks. Although existing studies have proposed privacy-preserving localization algorithms, they still face challenges of insufficient positioning accuracy and excessive communication overhead. In this article, we propose a privacy-preserving localization scheme, named PPLZN. PPLZN protects protects the location privacy of both the target and anchor nodes in crowdsourced localization. Simulation results validate the effectiveness of PPLZN. Evidently, it can achieve accurate position estimation without location leakage and outperform state-of-the-art approaches in both positioning accuracy and communication overhead. In addition, PPLZN significantly reduces computational and communication overhead in large-scale deployments, making it well-fitted for practical privacy-preserving localization in resource-constrained networks.</li>
</ul>

<h3>Title: LEGATO: Good Identity Unlearning Is Continuous</h3>
<ul>
<li><strong>Authors: </strong>Qiang Chen, Chun-Wun Cheng, Xiu Su, Hongyan Xu, Xi Lin, Shan You, Angelica I. Aviles-Rivero, Yi Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04282">https://arxiv.org/abs/2601.04282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04282">https://arxiv.org/pdf/2601.04282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04282]] LEGATO: Good Identity Unlearning Is Continuous(https://arxiv.org/abs/2601.04282)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, interpretability, explainability, generative</a></li>
<li><strong>Abstract: </strong>Machine unlearning has become a crucial role in enabling generative models trained on large datasets to remove sensitive, private, or copyright-protected data. However, existing machine unlearning methods face three challenges in learning to forget identity of generative models: 1) inefficient, where identity erasure requires fine-tuning all the model's parameters; 2) limited controllability, where forgetting intensity cannot be controlled and explainability is lacking; 3) catastrophic collapse, where the model's retention capability undergoes drastic degradation as forgetting progresses. Forgetting has typically been handled through discrete and unstable updates, often requiring full-model fine-tuning and leading to catastrophic collapse. In this work, we argue that identity forgetting should be modeled as a continuous trajectory, and introduce LEGATO - Learn to ForgEt Identity in GenerAtive Models via Trajectory-consistent Neural Ordinary Differential Equations. LEGATO augments pre-trained generators with fine-tunable lightweight Neural ODE adapters, enabling smooth, controllable forgetting while keeping the original model weights frozen. This formulation allows forgetting intensity to be precisely modulated via ODE step size, offering interpretability and robustness. To further ensure stability, we introduce trajectory consistency constraints that explicitly prevent catastrophic collapse during unlearning. Extensive experiments across in-domain and out-of-domain identity unlearning benchmarks show that LEGATO achieves state-of-the-art forgetting performance, avoids catastrophic collapse and reduces fine-tuned parameters.</li>
</ul>

<h3>Title: Mitigating Position-Shift Failures in Text-Based Modular Arithmetic via Position Curriculum and Template Diversity</h3>
<ul>
<li><strong>Authors: </strong>Nikolay Yudin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04283">https://arxiv.org/abs/2601.04283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04283">https://arxiv.org/pdf/2601.04283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04283]] Mitigating Position-Shift Failures in Text-Based Modular Arithmetic via Position Curriculum and Template Diversity(https://arxiv.org/abs/2601.04283)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Building on insights from the grokking literature, we study character-level Transformers trained to compute modular addition from text, and focus on robustness under input-format variation rather than only in-distribution accuracy. We identify a previously under-emphasized failure mode: models that achieve high in-distribution accuracy can fail catastrophically when the same expression is shifted to different absolute character positions ("position shift") or presented under out-of-distribution natural-language templates. Using a disjoint-pair split over all ordered pairs for p=97, we show that a baseline model reaches strong in-distribution performance yet collapses under position shift and template OOD. We then introduce a simple training recipe that combines (i) explicit expression boundary markers, (ii) position curriculum that broadens the range of absolute positions seen during training, (iii) diverse template mixtures, and (iv) consistency training across multiple variants per example. Across three seeds, this intervention substantially improves robustness to position shift and template OOD while maintaining high in-distribution accuracy, whereas an ALiBi-style ablation fails to learn the task under our setup. Our results suggest that steering procedural generalization under noisy supervision benefits from explicitly training invariances that are otherwise absent from the data distribution, and we provide a reproducible evaluation protocol and artifacts.</li>
</ul>

<h3>Title: Enhancing Robustness of Asynchronous EEG-Based Movement Prediction using Classifier Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Niklas Kueper, Kartik Chari, Elsa Andrea Kirchner</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04286">https://arxiv.org/abs/2601.04286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04286">https://arxiv.org/pdf/2601.04286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04286]] Enhancing Robustness of Asynchronous EEG-Based Movement Prediction using Classifier Ensembles(https://arxiv.org/abs/2601.04286)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Objective: Stroke is one of the leading causes of disabilities. One promising approach is to extend the rehabilitation with self-initiated robot-assisted movement therapy. To enable this, it is required to detect the patient's intention to move to trigger the assistance of a robotic device. This intention to move can be detected from human surface electroencephalography (EEG) signals; however, it is particularly challenging to decode when classifications are performed online and asynchronously. In this work, the effectiveness of classifier ensembles and a sliding-window postprocessing technique was investigated to enhance the robustness of such asynchronous classification. Approach: To investigate the effectiveness of classifier ensembles and a sliding-window postprocessing, two EEG datasets with 14 healthy subjects who performed self-initiated arm movements were analyzed. Offline and pseudo-online evaluations were conducted to compare ensemble combinations of the support vector machine (SVM), multilayer perceptron (MLP), and EEGNet classification models. Results: The results of the pseudo-online evaluation show that the two model ensembles significantly outperformed the best single model for the optimal number of postprocessing windows. In particular, for single models, an increased number of postprocessing windows significantly improved classification performances. Interestingly, we found no significant improvements between performances of the best single model and classifier ensembles in the offline evaluation. Significance: We demonstrated that classifier ensembles and appropriate postprocessing methods effectively enhance the asynchronous detection of movement intentions from EEG signals. In particular, the classifier ensemble approach yields greater improvements in online classification than in offline classification, and reduces false detections, i.e., early false positives.</li>
</ul>

<h3>Title: ArtCognition: A Multimodal AI Framework for Affective State Sensing from Visual and Kinematic Drawing Cues</h3>
<ul>
<li><strong>Authors: </strong>Behrad Binaei-Haghighi, Nafiseh Sadat Sajadi, Mehrad Liviyan, Reyhane Akhavan Kharazi, Fatemeh Amirkhani, Behnam Bahrak</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.HC, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04297">https://arxiv.org/abs/2601.04297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04297">https://arxiv.org/pdf/2601.04297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04297]] ArtCognition: A Multimodal AI Framework for Affective State Sensing from Visual and Kinematic Drawing Cues(https://arxiv.org/abs/2601.04297)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>The objective assessment of human affective and psychological states presents a significant challenge, particularly through non-verbal channels. This paper introduces digital drawing as a rich and underexplored modality for affective sensing. We present a novel multimodal framework, named ArtCognition, for the automated analysis of the House-Tree-Person (HTP) test, a widely used psychological instrument. ArtCognition uniquely fuses two distinct data streams: static visual features from the final artwork, captured by computer vision models, and dynamic behavioral kinematic cues derived from the drawing process itself, such as stroke speed, pauses, and smoothness. To bridge the gap between low-level features and high-level psychological interpretation, we employ a Retrieval-Augmented Generation (RAG) architecture. This grounds the analysis in established psychological knowledge, enhancing explainability and reducing the potential for model hallucination. Our results demonstrate that the fusion of visual and behavioral kinematic cues provides a more nuanced assessment than either modality alone. We show significant correlations between the extracted multimodal features and standardized psychological metrics, validating the framework's potential as a scalable tool to support clinicians. This work contributes a new methodology for non-intrusive affective state assessment and opens new avenues for technology-assisted mental healthcare.</li>
</ul>

<h3>Title: Privacy at Scale in Networked Healthcare</h3>
<ul>
<li><strong>Authors: </strong>M. Amin Rahimian, Benjamin Panny, James Joshi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY, cs.ET, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04298">https://arxiv.org/abs/2601.04298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04298">https://arxiv.org/pdf/2601.04298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04298]] Privacy at Scale in Networked Healthcare(https://arxiv.org/abs/2601.04298)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Digitized, networked healthcare promises earlier detection, precision therapeutics, and continuous care; yet, it also expands the surface for privacy loss and compliance risk. We argue for a shift from siloed, application-specific protections to privacy-by-design at scale, centered on decision-theoretic differential privacy (DP) across the full healthcare data lifecycle; network-aware privacy accounting for interdependence in people, sensors, and organizations; and compliance-as-code tooling that lets health systems share evidence while demonstrating regulatory due care. We synthesize the privacy-enhancing technology (PET) landscape in health (federated analytics, DP, cryptographic computation), identify practice gaps, and outline a deployable agenda involving privacy-budget ledgers, a control plane to coordinate PET components across sites, shared testbeds, and PET literacy, to make lawful, trustworthy sharing the default. We illustrate with use cases (multi-site trials, genomics, disease surveillance, mHealth) and highlight distributed inference as a workhorse for multi-institution learning under explicit privacy budgets.</li>
</ul>

<h3>Title: Transformer-Based Multi-Modal Temporal Embeddings for Explainable Metabolic Phenotyping in Type 1 Diabetes</h3>
<ul>
<li><strong>Authors: </strong>Pir Bakhsh Khokhar, Carmine Gravino, Fabio Palomba, Sule Yildrim Yayilgan, Sarang Shaikh</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04299">https://arxiv.org/abs/2601.04299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04299">https://arxiv.org/pdf/2601.04299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04299]] Transformer-Based Multi-Modal Temporal Embeddings for Explainable Metabolic Phenotyping in Type 1 Diabetes(https://arxiv.org/abs/2601.04299)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Type 1 diabetes (T1D) is a highly metabolically heterogeneous disease that cannot be adequately characterized by conventional biomarkers such as glycated hemoglobin (HbA1c). This study proposes an explainable deep learning framework that integrates continuous glucose monitoring (CGM) data with laboratory profiles to learn multimodal temporal embeddings of individual metabolic status. Temporal dependencies across modalities are modeled using a transformer encoder, while latent metabolic phenotypes are identified via Gaussian mixture modeling. Model interpretability is achieved through transformer attention visualization and SHAP-based feature attribution. Five latent metabolic phenotypes, ranging from metabolic stability to elevated cardiometabolic risk, were identified among 577 individuals with T1D. These phenotypes exhibit distinct biochemical profiles, including differences in glycemic control, lipid metabolism, renal markers, and thyrotropin (TSH) levels. Attention analysis highlights glucose variability as a dominant temporal factor, while SHAP analysis identifies HbA1c, triglycerides, cholesterol, creatinine, and TSH as key contributors to phenotype differentiation. Phenotype membership shows statistically significant, albeit modest, associations with hypertension, myocardial infarction, and heart failure. Overall, this explainable multimodal temporal embedding framework reveals physiologically coherent metabolic subgroups in T1D and supports risk stratification beyond single biomarkers.</li>
</ul>

<h3>Title: Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes</h3>
<ul>
<li><strong>Authors: </strong>Chenye Meng, Zejian Li, Zhongni Liu, Yize Li, Changle Xie, Kaixin Jia, Ling Yang, Huanghuang Deng, Shiying Ding, Shengyuan Zhang, Jiayi Li, Lingyun Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04300">https://arxiv.org/abs/2601.04300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04300">https://arxiv.org/pdf/2601.04300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04300]] Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes(https://arxiv.org/abs/2601.04300)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.</li>
</ul>

<h3>Title: Quantifying the Effect of Test Set Contamination on Generative Evaluations</h3>
<ul>
<li><strong>Authors: </strong>Rylan Schaeffer, Joshua Kazdan, Baber Abbasi, Ken Ziyu Liu, Brando Miranda, Ahmed Ahmed, Abhay Puri, Niloofar Mireshghallah, Sanmi Koyejo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04301">https://arxiv.org/abs/2601.04301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04301">https://arxiv.org/pdf/2601.04301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04301]] Quantifying the Effect of Test Set Contamination on Generative Evaluations(https://arxiv.org/abs/2601.04301)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As frontier AI systems are pretrained on web-scale data, test set contamination has become a critical concern for accurately assessing their capabilities. While research has thoroughly investigated the impact of test set contamination on discriminative evaluations like multiple-choice question-answering, comparatively little research has studied the impact of test set contamination on generative evaluations. In this work, we quantitatively assess the effect of test set contamination on generative evaluations through the language model lifecycle. We pretrain language models on mixtures of web data and the MATH benchmark, sweeping model sizes and number of test set replicas contaminating the pretraining corpus; performance improves with contamination and model size. Using scaling laws, we make a surprising discovery: including even a single test set replica enables models to achieve lower loss than the irreducible error of training on the uncontaminated corpus. We then study further training: overtraining with fresh data reduces the effects of contamination, whereas supervised finetuning on the training set can either increase or decrease performance on test data, depending on the amount of pretraining contamination. Finally, at inference, we identify factors that modulate memorization: high sampling temperatures mitigate contamination effects, and longer solutions are exponentially more difficult to memorize than shorter ones, presenting a contrast with discriminative evaluations, where solutions are only a few tokens in length. By characterizing how generation and memorization interact, we highlight a new layer of complexity for trustworthy evaluation of AI systems.</li>
</ul>

<h3>Title: Embedding Textual Information in Images Using Quinary Pixel Combinations</h3>
<ul>
<li><strong>Authors: </strong>A V Uday Kiran Kandala</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04302">https://arxiv.org/abs/2601.04302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04302">https://arxiv.org/pdf/2601.04302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04302]] Embedding Textual Information in Images Using Quinary Pixel Combinations(https://arxiv.org/abs/2601.04302)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents a novel technique for embedding textual data into images using quinary combinations of pixel intensities in RGB space. Existing methods predominantly rely on least and most significant bit (LSB & MSB) manipulation, Pixel Value Differencing (PVD), spatial perturbations in RGB channels, transform domain based methods, Quantization methods, Edge and Region based methods and more recently through deep learning methods and generative AI techniques for hiding textual information in spatial domain of images. Most of them are dependent on pixel intensity flipping over multiple pixels, such as LSB and combination of LSB based methodologies, and on transform coefficients, often resulting in the form of noise. Encoding and Decoding are deterministic in most of the existing approaches and are computationally heavy in case of higher models such as deep learning and gen AI approaches. The proposed method works on quinary pixel intensity combinations in RGB space, where five controlled different pixel intensity variations in each of the R, G, and B channels formulate up to one hundred and twenty five distinct pixel intensity combinations. These combinations are mapped to textual symbols, enabling the representation of uppercase and lowercase alphabetic characters, numeric digits, whitespace, and commonly used special characters. Different metrics such as MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison and Heatmap analysis, were evaluated for both original and encoded images resulting in no significant distortion in the images. Furthermore, the method achieves improved embedding efficiency by encoding a complete textual symbol within a single RGB pixel, in contrast to LSB and MSB based approaches that typically require multiple pixels or multi-step processes, as well as transform and learning based methods that incur higher computational overhead.</li>
</ul>

<h3>Title: ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Mohsen Ghafoorian, Amirhossein Habibian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04342">https://arxiv.org/abs/2601.04342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04342">https://arxiv.org/pdf/2601.04342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04342]] ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers(https://arxiv.org/abs/2601.04342)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at this https URL.</li>
</ul>

<h3>Title: Comparative Analysis of Custom CNN Architectures versus Pre-trained Models and Transfer Learning: A Study on Five Bangladesh Datasets</h3>
<ul>
<li><strong>Authors: </strong>Ibrahim Tanvir (University of Dhaka), Alif Ruslan (University of Dhaka), Sartaj Solaiman (University of Dhaka)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04352">https://arxiv.org/abs/2601.04352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04352">https://arxiv.org/pdf/2601.04352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04352]] Comparative Analysis of Custom CNN Architectures versus Pre-trained Models and Transfer Learning: A Study on Five Bangladesh Datasets(https://arxiv.org/abs/2601.04352)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This study presents a comprehensive comparative analysis of custom-built Convolutional Neural Networks (CNNs) against popular pre-trained architectures (ResNet-18 and VGG-16) using both feature extraction and transfer learning approaches. We evaluated these models across five diverse image classification datasets from Bangladesh: Footpath Vision, Auto Rickshaw Detection, Mango Image Classification, Paddy Variety Recognition, and Road Damage Detection. Our experimental results demonstrate that transfer learning with fine-tuning consistently outperforms both custom CNNs built from scratch and feature extraction methods, achieving accuracy improvements ranging from 3% to 76% across different datasets. Notably, ResNet-18 with fine-tuning achieved perfect 100% accuracy on the Road Damage BD dataset. While custom CNNs offer advantages in model size (3.4M parameters vs. 11-134M for pre-trained models) and training efficiency on simpler tasks, pre-trained models with transfer learning provide superior performance, particularly on complex classification tasks with limited training data. This research provides practical insights for practitioners in selecting appropriate deep learning approaches based on dataset characteristics, computational resources, and performance requirements.</li>
</ul>

<h3>Title: PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache</h3>
<ul>
<li><strong>Authors: </strong>Kunyang Li, Mubarak Shah, Yuzhang Shang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04359">https://arxiv.org/abs/2601.04359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04359">https://arxiv.org/pdf/2601.04359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04359]] PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache(https://arxiv.org/abs/2601.04359)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.</li>
</ul>

<h3>Title: Combining facial videos and biosignals for stress estimation during driving</h3>
<ul>
<li><strong>Authors: </strong>Paraskevi Valergaki, Vassilis C. Nicodemou, Iason Oikonomidis, Antonis Argyros, Anastasios Roussos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04376">https://arxiv.org/abs/2601.04376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04376">https://arxiv.org/pdf/2601.04376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04376]] Combining facial videos and biosignals for stress estimation during driving(https://arxiv.org/abs/2601.04376)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Reliable stress recognition from facial videos is challenging due to stress's subjective nature and voluntary facial control. While most methods rely on Facial Action Units, the role of disentangled 3D facial geometry remains underexplored. We address this by analyzing stress during distracted driving using EMOCA-derived 3D expression and pose coefficients. Paired hypothesis tests between baseline and stressor phases reveal that 41 of 56 coefficients show consistent, phase-specific stress responses comparable to physiological markers. Building on this, we propose a Transformer-based temporal modeling framework and assess unimodal, early-fusion, and cross-modal attention strategies. Cross-Modal Attention fusion of EMOCA and physiological signals achieves best performance (AUROC 92\%, Accuracy 86.7\%), with EMOCA-gaze fusion also competitive (AUROC 91.8\%). This highlights the effectiveness of temporal modeling and cross-modal attention for stress recognition.</li>
</ul>

<h3>Title: Disco-RAG: Discourse-Aware Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Dongqi Liu, Hang Ding, Qiming Feng, Jian Li, Xurong Xie, Zhucun Xue, Chengjie Wang, Jiangning Zhang, Yabiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04377">https://arxiv.org/abs/2601.04377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04377">https://arxiv.org/pdf/2601.04377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04377]] Disco-RAG: Discourse-Aware Retrieval-Augmented Generation(https://arxiv.org/abs/2601.04377)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has emerged as an important means of enhancing the performance of large language models (LLMs) in knowledge-intensive tasks. However, most existing RAG strategies treat retrieved passages in a flat and unstructured way, which prevents the model from capturing structural cues and constrains its ability to synthesize knowledge from dispersed evidence across documents. To overcome these limitations, we propose Disco-RAG, a discourse-aware framework that explicitly injects discourse signals into the generation process. Our method constructs intra-chunk discourse trees to capture local hierarchies and builds inter-chunk rhetorical graphs to model cross-passage coherence. These structures are jointly integrated into a planning blueprint that conditions the generation. Experiments on question answering and long-document summarization benchmarks show the efficacy of our approach. Disco-RAG achieves state-of-the-art results on the benchmarks without fine-tuning. These findings underscore the important role of discourse structure in advancing RAG systems.</li>
</ul>

<h3>Title: Aligned explanations in neural networks</h3>
<ul>
<li><strong>Authors: </strong>Corentin Lobet, Francesca Chiaromonte</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04378">https://arxiv.org/abs/2601.04378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04378">https://arxiv.org/pdf/2601.04378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04378]] Aligned explanations in neural networks(https://arxiv.org/abs/2601.04378)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Feature attribution is the dominant paradigm for explaining deep neural networks. However, most existing methods only loosely reflect the model's prediction-making process, thereby merely white-painting the black box. We argue that explanatory alignment is a key aspect of trustworthiness in prediction tasks: explanations must be directly linked to predictions, rather than serving as post-hoc rationalizations. We present model readability as a design principle enabling alignment, and PiNets as a modeling framework to pursue it in a deep learning context. PiNets are pseudo-linear networks that produce instance-wise linear predictions in an arbitrary feature space, making them linearly readable. We illustrate their use on image classification and segmentation tasks, demonstrating how PiNets produce explanations that are faithful across multiple criteria in addition to alignment.</li>
</ul>

<h3>Title: MiJaBench: Revealing Minority Biases in Large Language Models via Hate Speech Jailbreaking</h3>
<ul>
<li><strong>Authors: </strong>Iago Alves Brito, Walcy Santos Rezende Rios, Julia Soares Dollis, Diogo Fernandes Costa Silva, Arlindo Rodrigues Galvão Filho</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04389">https://arxiv.org/abs/2601.04389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04389">https://arxiv.org/pdf/2601.04389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04389]] MiJaBench: Revealing Minority Biases in Large Language Models via Hate Speech Jailbreaking(https://arxiv.org/abs/2601.04389)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, large language model</a></li>
<li><strong>Abstract: </strong>Current safety evaluations of large language models (LLMs) create a dangerous illusion of universality, aggregating "Identity Hate" into scalar scores that mask systemic vulnerabilities against specific populations. To expose this selective safety, we introduce MiJaBench, a bilingual (English and Portuguese) adversarial benchmark comprising 44,000 prompts across 16 minority groups. By generating 528,000 prompt-response pairs from 12 state-of-the-art LLMs, we curate MiJaBench-Align, revealing that safety alignment is not a generalized semantic capability but a demographic hierarchy: defense rates fluctuate by up to 33\% within the same model solely based on the target group. Crucially, we demonstrate that model scaling exacerbates these disparities, suggesting that current alignment techniques do not create principle of non-discrimination but reinforces memorized refusal boundaries only for specific groups, challenging the current scaling laws of security. We release all datasets and scripts to encourage research into granular demographic alignment at GitHub.</li>
</ul>

<h3>Title: Enhanced-FQL($λ$), an Efficient and Interpretable RL with novel Fuzzy Eligibility Traces and Segmented Experience Replay</h3>
<ul>
<li><strong>Authors: </strong>Mohsen Jalaeian-Farimani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO, eess.SY, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04392">https://arxiv.org/abs/2601.04392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04392">https://arxiv.org/pdf/2601.04392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04392]] Enhanced-FQL($λ$), an Efficient and Interpretable RL with novel Fuzzy Eligibility Traces and Segmented Experience Replay(https://arxiv.org/abs/2601.04392)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This paper introduces a fuzzy reinforcement learning framework, Enhanced-FQL($\lambda$), that integrates novel Fuzzified Eligibility Traces (FET) and Segmented Experience Replay (SER) into fuzzy Q-learning with Fuzzified Bellman Equation (FBE) for continuous control tasks. The proposed approach employs an interpretable fuzzy rule base instead of complex neural architectures, while maintaining competitive performance through two key innovations: a fuzzified Bellman equation with eligibility traces for stable multi-step credit assignment, and a memory-efficient segment-based experience replay mechanism for enhanced sample efficiency. Theoretical analysis proves the proposed method convergence under standard assumptions. Extensive evaluations in continuous control domains demonstrate that Enhanced-FQL($\lambda$) achieves superior sample efficiency and reduced variance compared to n-step fuzzy TD and fuzzy SARSA($\lambda$) baselines, while maintaining substantially lower computational complexity than deep RL alternatives such as DDPG. The framework's inherent interpretability, combined with its computational efficiency and theoretical convergence guarantees, makes it particularly suitable for safety-critical applications where transparency and resource constraints are essential.</li>
</ul>

<h3>Title: ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sharanya Dasgupta, Arkaprabha Basu, Sujoy Nath, Swagatam Das</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04394">https://arxiv.org/abs/2601.04394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04394">https://arxiv.org/pdf/2601.04394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04394]] ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models(https://arxiv.org/abs/2601.04394)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Human cognition, driven by complex neurochemical processes, oscillates between imagination and reality and learns to self-correct whenever such subtle drifts lead to hallucinations or unsafe associations. In recent years, LLMs have demonstrated remarkable performance in a wide range of tasks. However, they still lack human cognition to balance factuality and safety. Bearing the resemblance, we argue that both factual and safety failures in LLMs arise from a representational misalignment in their latent activation space, rather than addressing those as entirely separate alignment issues. We hypothesize that an external network, trained to understand the fluctuations, can selectively intervene in the model to regulate falsehood into truthfulness and unsafe output into safe output without fine-tuning the model parameters themselves. Reflecting the hypothesis, we propose ARREST (Adversarial Resilient Regulation Enhancing Safety and Truth), a unified framework that identifies and corrects drifted features, engaging both soft and hard refusals in addition to factual corrections. Our empirical results show that ARREST not only regulates misalignment but is also more versatile compared to the RLHF-aligned models in generating soft refusals due to adversarial training. We make our codebase available at this https URL.</li>
</ul>

<h3>Title: Interpreting Transformers Through Attention Head Intervention</h3>
<ul>
<li><strong>Authors: </strong>Mason Kadem, Rong Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04398">https://arxiv.org/abs/2601.04398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04398">https://arxiv.org/pdf/2601.04398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04398]] Interpreting Transformers Through Attention Head Intervention(https://arxiv.org/abs/2601.04398)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Neural networks are growing more capable on their own, but we do not understand their neural mechanisms. Understanding these mechanisms' decision-making processes, or mechanistic interpretability, enables (1) accountability and control in high-stakes domains, (2) the study of digital brains and the emergence of cognition, and (3) discovery of new knowledge when AI systems outperform humans.</li>
</ul>

<h3>Title: From Preoperative CT to Postmastoidectomy Mesh Construction:1Mastoidectomy Shape Prediction for Cochlear Implant Surgery</h3>
<ul>
<li><strong>Authors: </strong>Yike Zhang, Eduardo Davalos, Dingjie Su, Ange Lou, Jack Noble</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04405">https://arxiv.org/abs/2601.04405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04405">https://arxiv.org/pdf/2601.04405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04405]] From Preoperative CT to Postmastoidectomy Mesh Construction:1Mastoidectomy Shape Prediction for Cochlear Implant Surgery(https://arxiv.org/abs/2601.04405)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Cochlear Implant (CI) surgery treats severe hearing loss by inserting an electrode array into the cochlea to stimulate the auditory nerve. An important step in this procedure is mastoidectomy, which removes part of the mastoid region of the temporal bone to provide surgical access. Accurate mastoidectomy shape prediction from preoperative imaging improves pre-surgical planning, reduces risks, and enhances surgical outcomes. Despite its importance, there are limited deep-learning-based studies regarding this topic due to the challenges of acquiring ground-truth labels. We address this gap by investigating self-supervised and weakly-supervised learning models to predict the mastoidectomy region without human annotations. We propose a hybrid self-supervised and weakly-supervised learning framework to predict the mastoidectomy region directly from preoperative CT scans, where the mastoid remains intact. Our hybrid method achieves a mean Dice score of 0.72 when predicting the complex and boundary-less mastoidectomy shape, surpassing state-of-the-art approaches and demonstrating strong performance. The method provides groundwork for constructing 3D postmastoidectomy surfaces directly from the corresponding preoperative CT scans. To our knowledge, this is the first work that integrating self-supervised and weakly-supervised learning for mastoidectomy shape prediction, offering a robust and efficient solution for CI surgical planning while leveraging 3D T-distribution loss in weakly-supervised medical imaging.</li>
</ul>

<h3>Title: Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization</h3>
<ul>
<li><strong>Authors: </strong>Yao Dou, Wei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04424">https://arxiv.org/abs/2601.04424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04424">https://arxiv.org/pdf/2601.04424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04424]] Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization(https://arxiv.org/abs/2601.04424)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) now support contexts of up to 1M tokens, but their effectiveness on complex long-context tasks remains unclear. In this paper, we study multi-document legal case summarization, where a single case often spans many documents totaling 100K-500K tokens. We introduce Gavel-Ref, a reference-based evaluation framework with multi-value checklist evaluation over 26 items, as well as residual fact and writing-style evaluations. Using Gavel-Ref, we go beyond the single aggregate scores reported in prior work and systematically evaluate 12 frontier LLMs on 100 legal cases ranging from 32K to 512K tokens, primarily from 2025. Our results show that even the strongest model, Gemini 2.5 Pro, achieves only around 50 of $S_{\text{Gavel-Ref}}$, highlighting the difficulty of the task. Models perform well on simple checklist items (e.g., filing date) but struggle on multi-value or rare ones such as settlements and monitor reports. As LLMs continue to improve and may surpass human-written summaries -- making human references less reliable -- we develop Gavel-Agent, an efficient and autonomous agent scaffold that equips LLMs with six tools to navigate and extract checklists directly from case documents. With Qwen3, Gavel-Agent reduces token usage by 36% while resulting in only a 7% drop in $S_{\text{checklist}}$ compared to end-to-end extraction with GPT-4.1.</li>
</ul>

<h3>Title: Accommodation and Epistemic Vigilance: A Pragmatic Account of Why LLMs Fail to Challenge Harmful Beliefs</h3>
<ul>
<li><strong>Authors: </strong>Myra Cheng, Robert D. Hawkins, Dan Jurafsky</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04435">https://arxiv.org/abs/2601.04435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04435">https://arxiv.org/pdf/2601.04435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04435]] Accommodation and Epistemic Vigilance: A Pragmatic Account of Why LLMs Fail to Challenge Harmful Beliefs(https://arxiv.org/abs/2601.04435)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) frequently fail to challenge users' harmful beliefs in domains ranging from medical advice to social reasoning. We argue that these failures can be understood and addressed pragmatically as consequences of LLMs defaulting to accommodating users' assumptions and exhibiting insufficient epistemic vigilance. We show that social and linguistic factors known to influence accommodation in humans (at-issueness, linguistic encoding, and source reliability) similarly affect accommodation in LLMs, explaining performance differences across three safety benchmarks that test models' ability to challenge harmful beliefs, spanning misinformation (Cancer-Myth, SAGE-Eval) and sycophancy (ELEPHANT). We further show that simple pragmatic interventions, such as adding the phrase "wait a minute", significantly improve performance on these benchmarks while preserving low false-positive rates. Our results highlight the importance of considering pragmatics for evaluating LLM behavior and improving LLM safety.</li>
</ul>

<h3>Title: Large Language Models for Detecting Cyberattacks on Smart Grid Protective Relays</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Mohammad Saber, Saeed Jafari, Zhengmao Ouyang, Paul Budnarain, Amr Youssef, Deepa Kundur</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04443">https://arxiv.org/abs/2601.04443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04443">https://arxiv.org/pdf/2601.04443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04443]] Large Language Models for Detecting Cyberattacks on Smart Grid Protective Relays(https://arxiv.org/abs/2601.04443)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a large language model (LLM)-based framework for detecting cyberattacks on transformer current differential relays (TCDRs), which, if undetected, may trigger false tripping of critical transformers. The proposed approach adapts and fine-tunes compact LLMs such as DistilBERT to distinguish cyberattacks from actual faults using textualized multidimensional TCDR current measurements recorded before and after tripping. Our results demonstrate that DistilBERT detects 97.6% of cyberattacks without compromising TCDR dependability and achieves inference latency below 6 ms on a commercial workstation. Additional evaluations confirm the framework's robustness under combined time-synchronization and false-data-injection attacks, resilience to measurement noise, and stability across prompt formulation variants. Furthermore, GPT-2 and DistilBERT+LoRA achieve comparable performance, highlighting the potential of LLMs for enhancing smart grid cybersecurity. We provide the full dataset used in this study for reproducibility.</li>
</ul>

<h3>Title: Merging Triggers, Breaking Backdoors: Defensive Poisoning for Instruction-Tuned Language Models</h3>
<ul>
<li><strong>Authors: </strong>San Kim, Gary Geunbae Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04448">https://arxiv.org/abs/2601.04448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04448">https://arxiv.org/pdf/2601.04448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04448]] Merging Triggers, Breaking Backdoors: Defensive Poisoning for Instruction-Tuned Language Models(https://arxiv.org/abs/2601.04448)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have greatly advanced Natural Language Processing (NLP), particularly through instruction tuning, which enables broad task generalization without additional fine-tuning. However, their reliance on large-scale datasets-often collected from human or web sources-makes them vulnerable to backdoor attacks, where adversaries poison a small subset of data to implant hidden behaviors. Despite this growing risk, defenses for instruction-tuned models remain underexplored. We propose MB-Defense (Merging & Breaking Defense Framework), a novel training pipeline that immunizes instruction-tuned LLMs against diverse backdoor threats. MB-Defense comprises two stages: (i) defensive poisoning, which merges attacker and defensive triggers into a unified backdoor representation, and (ii) weight recovery, which breaks this representation through additional training to restore clean behavior. Extensive experiments across multiple LLMs show that MB-Defense substantially lowers attack success rates while preserving instruction-following ability. Our method offers a generalizable and data-efficient defense strategy, improving the robustness of instruction-tuned LLMs against unseen backdoor attacks.</li>
</ul>

<h3>Title: UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Zhexiao Xiong, Xin Ye, Burhan Yaman, Sheng Cheng, Yiren Lu, Jingru Luo, Nathan Jacobs, Liu Ren</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04453">https://arxiv.org/abs/2601.04453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04453">https://arxiv.org/pdf/2601.04453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04453]] UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving(https://arxiv.org/abs/2601.04453)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture. UniDrive-WM's trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. We further compare discrete and continuous output representations for future image prediction, analyzing their influence on downstream driving performance. Experiments on the challenging Bench2Drive benchmark show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method. These results demonstrate the advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving. The project page is available at this https URL .</li>
</ul>

<h3>Title: Using Large Language Models to Detect Socially Shared Regulation of Collaborative Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Zhang, Conrad Borchers, Clayton Cohn, Namrata Srivastava, Caitlin Snyder, Siyuan Guo, Ashwin T S, Naveeduddin Mohammed, Haley Noh, Gautam Biswas</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04458">https://arxiv.org/abs/2601.04458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04458">https://arxiv.org/pdf/2601.04458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04458]] Using Large Language Models to Detect Socially Shared Regulation of Collaborative Learning(https://arxiv.org/abs/2601.04458)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The field of learning analytics has made notable strides in automating the detection of complex learning processes in multimodal data. However, most advancements have focused on individualized problem-solving instead of collaborative, open-ended problem-solving, which may offer both affordances (richer data) and challenges (low cohesion) to behavioral prediction. Here, we extend predictive models to automatically detect socially shared regulation of learning (SSRL) behaviors in collaborative computational modeling environments using embedding-based approaches. We leverage large language models (LLMs) as summarization tools to generate task-aware representations of student dialogue aligned with system logs. These summaries, combined with text-only embeddings, context-enriched embeddings, and log-derived features, were used to train predictive models. Results show that text-only embeddings often achieve stronger performance in detecting SSRL behaviors related to enactment or group dynamics (e.g., off-task behavior or requesting assistance). In contrast, contextual and multimodal features provide complementary benefits for constructs such as planning and reflection. Overall, our findings highlight the promise of embedding-based models for extending learning analytics by enabling scalable detection of SSRL behaviors, ultimately supporting real-time feedback and adaptive scaffolding in collaborative learning environments that teachers value.</li>
</ul>

<h3>Title: Meta-probabilistic Modeling</h3>
<ul>
<li><strong>Authors: </strong>Kevin Zhang, Yixin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04462">https://arxiv.org/abs/2601.04462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04462">https://arxiv.org/pdf/2601.04462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04462]] Meta-probabilistic Modeling(https://arxiv.org/abs/2601.04462)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While probabilistic graphical models can discover latent structure in data, their effectiveness hinges on choosing well-specified models. Identifying such models is challenging in practice, often requiring iterative checking and revision through trial and error. To this end, we propose meta-probabilistic modeling (MPM), a meta-learning algorithm that learns generative model structure directly from multiple related datasets. MPM uses a hierarchical architecture where global model specifications are shared across datasets while local parameters remain dataset-specific. For learning and inference, we propose a tractable VAE-inspired surrogate objective, and optimize it through bi-level optimization: local variables are updated analytically via coordinate ascent, while global parameters are trained with gradient-based methods. We evaluate MPM on object-centric image modeling and sequential text modeling, demonstrating that it adapts generative models to data while recovering meaningful latent representations.</li>
</ul>

<h3>Title: Beyond Static Summarization: Proactive Memory Extraction for LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Chengyuan Yang, Zequn Sun, Wei Wei, Wei Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04463">https://arxiv.org/abs/2601.04463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04463">https://arxiv.org/pdf/2601.04463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04463]] Beyond Static Summarization: Proactive Memory Extraction for LLM Agents(https://arxiv.org/abs/2601.04463)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is "ahead-of-time", acting as a blind "feed-forward" process that misses important details because it doesn't know future tasks. Second, extraction is usually "one-off", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.</li>
</ul>

<h3>Title: SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers</h3>
<ul>
<li><strong>Authors: </strong>Iaroslav Chelombitko, Ekaterina Chelombitko, Aleksey Komissarov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04469">https://arxiv.org/abs/2601.04469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04469">https://arxiv.org/pdf/2601.04469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04469]] SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers(https://arxiv.org/abs/2601.04469)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons. We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings. Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the "elbow points" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: this https URL</li>
</ul>

<h3>Title: When Models Manipulate Manifolds: The Geometry of a Counting Task</h3>
<ul>
<li><strong>Authors: </strong>Wes Gurnee, Emmanuel Ameisen, Isaac Kauvar, Julius Tarng, Adam Pearce, Chris Olah, Joshua Batson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04480">https://arxiv.org/abs/2601.04480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04480">https://arxiv.org/pdf/2601.04480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04480]] When Models Manipulate Manifolds: The Geometry of a Counting Task(https://arxiv.org/abs/2601.04480)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Language models can perceive visual properties of text despite receiving only sequences of tokens-we mechanistically investigate how Claude 3.5 Haiku accomplishes one such task: linebreaking in fixed-width text. We find that character counts are represented on low-dimensional curved manifolds discretized by sparse feature families, analogous to biological place cells. Accurate predictions emerge from a sequence of geometric transformations: token lengths are accumulated into character count manifolds, attention heads twist these manifolds to estimate distance to the line boundary, and the decision to break the line is enabled by arranging estimates orthogonally to create a linear decision boundary. We validate our findings through causal interventions and discover visual illusions--character sequences that hijack the counting mechanism. Our work demonstrates the rich sensory processing of early layers, the intricacy of attention algorithms, and the importance of combining feature-based and geometric views of interpretability.</li>
</ul>

<h3>Title: Hybrid Federated Learning for Noise-Robust Training</h3>
<ul>
<li><strong>Authors: </strong>Yongjun Kim, Hyeongjun Park, Hwanjin Kim, Junil Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04483">https://arxiv.org/abs/2601.04483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04483">https://arxiv.org/pdf/2601.04483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04483]] Hybrid Federated Learning for Noise-Robust Training(https://arxiv.org/abs/2601.04483)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) and federated distillation (FD) are distributed learning paradigms that train UE models with enhanced privacy, each offering different trade-offs between noise robustness and learning speed. To mitigate their respective weaknesses, we propose a hybrid federated learning (HFL) framework in which each user equipment (UE) transmits either gradients or logits, and the base station (BS) selects the per-round weights of FL and FD updates. We derive convergence of HFL framework and introduce two methods to exploit degrees of freedom (DoF) in HFL, which are (i) adaptive UE clustering via Jenks optimization and (ii) adaptive weight selection via a damped Newton method. Numerical results show that HFL achieves superior test accuracy at low SNR when both DoF are exploited.</li>
</ul>

<h3>Title: Decision-Aware Trust Signal Alignment for SOC Alert Triage</h3>
<ul>
<li><strong>Authors: </strong>Israt Jahan Chowdhury, Md Abu Yousuf Tanvir</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04486">https://arxiv.org/abs/2601.04486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04486">https://arxiv.org/pdf/2601.04486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04486]] Decision-Aware Trust Signal Alignment for SOC Alert Triage(https://arxiv.org/abs/2601.04486)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>Detection systems that utilize machine learning are progressively implemented at Security Operations Centers (SOCs) to help an analyst to filter through high volumes of security alerts. Practically, such systems tend to reveal probabilistic results or confidence scores which are ill-calibrated and hard to read when under pressure. Qualitative and survey based studies of SOC practice done before reveal that poor alert quality and alert overload greatly augment the burden on the analyst, especially when tool outputs are not coherent with decision requirements, or signal noise. One of the most significant limitations is that model confidence is usually shown without expressing that there are asymmetric costs in decision making where false alarms are much less harmful than missed attacks. The present paper presents a decision-sensitive trust signal correspondence scheme of SOC alert triage. The framework combines confidence that has been calibrated, lightweight uncertainty cues, and cost-sensitive decision thresholds into coherent decision-support layer, instead of making changes to detection models. To enhance probabilistic consistency, the calibration is done using the known post-hoc methods and the uncertainty cues give conservative protection in situations where model certainty is low. To measure the model-independent performance of the suggested model, we apply the Logistic Regression and the Random Forest classifiers to the UNSW-NB15 intrusion detection benchmark. According to simulation findings, false negatives are greatly amplified by the presence of misaligned displays of confidence, whereas cost weighted loss decreases by orders of magnitude between models with decision aligned trust signals. Lastly, we describe a human-in-the-loop study plan that would allow empirically assessing the decision-making of the analysts with aligned and misaligned trust interfaces.</li>
</ul>

<h3>Title: Vision-Language Agents for Interactive Forest Change Analysis</h3>
<ul>
<li><strong>Authors: </strong>James Brock, Ce Zhang, Nantheera Anantrasirichai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04497">https://arxiv.org/abs/2601.04497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04497">https://arxiv.org/pdf/2601.04497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04497]] Vision-Language Agents for Interactive Forest Change Analysis(https://arxiv.org/abs/2601.04497)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Modern forest monitoring workflows increasingly benefit from the growing availability of high-resolution satellite imagery and advances in deep learning. Two persistent challenges in this context are accurate pixel-level change detection and meaningful semantic change captioning for complex forest dynamics. While large language models (LLMs) are being adapted for interactive data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored. To address this gap, we introduce an LLM-driven agent for integrated forest change analysis that supports natural language querying across multiple RSICI tasks. The proposed system builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration. To facilitate adaptation and evaluation in forest environments, we further introduce the Forest-Change dataset, which comprises bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated using a combination of human annotation and rule-based methods. Experimental results show that the proposed system achieves mIoU and BLEU-4 scores of 67.10% and 40.17% on the Forest-Change dataset, and 88.13% and 34.41% on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI benchmark for joint change detection and captioning. These results highlight the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available at this https URL.</li>
</ul>

<h3>Title: IGenBench: Benchmarking the Reliability of Text-to-Infographic Generation</h3>
<ul>
<li><strong>Authors: </strong>Yinghao Tang, Xueding Liu, Boyuan Zhang, Tingfeng Lan, Yupeng Xie, Jiale Lao, Yiyao Wang, Haoxuan Li, Tingting Gao, Bo Pan, Luoxuan Weng, Xiuqi Huang, Minfeng Zhu, Yingchaojie Feng, Yuyu Luo, Wei Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04498">https://arxiv.org/abs/2601.04498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04498">https://arxiv.org/pdf/2601.04498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04498]] IGenBench: Benchmarking the Reliability of Text-to-Infographic Generation(https://arxiv.org/abs/2601.04498)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Infographics are composite visual artifacts that combine data visualizations with textual and illustrative elements to communicate information. While recent text-to-image (T2I) models can generate aesthetically appealing images, their reliability in generating infographics remains unclear. Generated infographics may appear correct at first glance but contain easily overlooked issues, such as distorted data encoding or incorrect textual content. We present IGENBENCH, the first benchmark for evaluating the reliability of text-to-infographic generation, comprising 600 curated test cases spanning 30 infographic types. We design an automated evaluation framework that decomposes reliability verification into atomic yes/no questions based on a taxonomy of 10 question types. We employ multimodal large language models (MLLMs) to verify each question, yielding question-level accuracy (Q-ACC) and infographic-level accuracy (I-ACC). We comprehensively evaluate 10 state-of-the-art T2I models on IGENBENCH. Our systematic analysis reveals key insights for future model development: (i) a three-tier performance hierarchy with the top model achieving Q-ACC of 0.90 but I-ACC of only 0.49; (ii) data-related dimensions emerging as universal bottlenecks (e.g., Data Completeness: 0.21); and (iii) the challenge of achieving end-to-end correctness across all models. We release IGENBENCH at this https URL.</li>
</ul>

<h3>Title: Surface-based Molecular Design with Multi-modal Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Fang Wu, Zhengyuan Zhou, Shuting Jin, Xiangxiang Zeng, Jure Leskovec, Jinbo Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04506">https://arxiv.org/abs/2601.04506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04506">https://arxiv.org/pdf/2601.04506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04506]] Surface-based Molecular Design with Multi-modal Flow Matching(https://arxiv.org/abs/2601.04506)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Therapeutic peptides show promise in targeting previously undruggable binding sites, with recent advancements in deep generative models enabling full-atom peptide co-design for specific protein receptors. However, the critical role of molecular surfaces in protein-protein interactions (PPIs) has been underexplored. To bridge this gap, we propose an omni-design peptides generation paradigm, called SurfFlow, a novel surface-based generative algorithm that enables comprehensive co-design of sequence, structure, and surface for peptides. SurfFlow employs a multi-modality conditional flow matching (CFM) architecture to learn distributions of surface geometries and biochemical properties, enhancing peptide binding accuracy. Evaluated on the comprehensive PepMerge benchmark, SurfFlow consistently outperforms full-atom baselines across all metrics. These results highlight the advantages of considering molecular surfaces in de novo peptide discovery and demonstrate the potential of integrating multiple protein modalities for more effective therapeutic peptide discovery.</li>
</ul>

<h3>Title: LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuxiao Ye, Yiming Zhang, Yiran Ma, Huiyuan Xie, Huining Zhu, Zhiyuan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04516">https://arxiv.org/abs/2601.04516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04516">https://arxiv.org/pdf/2601.04516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04516]] LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation(https://arxiv.org/abs/2601.04516)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have enabled Multi-Agent Systems (MASs) where agents interact through natural language to solve complex tasks or simulate multi-party dialogues. Recent work on LLM-based MASs has mainly focused on architecture design, such as role assignment and workflow orchestration. In contrast, this paper targets the interaction process itself, aiming to improve agents' communication efficiency by helping them convey their intended meaning more effectively through language. To this end, we propose LinguaGame, a linguistically-grounded game-theoretic paradigm for multi-agent dialogue generation. Our approach models dialogue as a signalling game over communicative intents and strategies, solved with a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, whose game designs are often tightly coupled with task-specific objectives, our framework relies on linguistically informed reasoning with minimal task-specific coupling. Specifically, it treats dialogue as intentional and strategic communication, requiring agents to infer what others aim to achieve (intents) and how they pursue those goals (strategies). We evaluate our framework in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency.</li>
</ul>

<h3>Title: TokenSeg: Efficient 3D Medical Image Segmentation via Hierarchical Visual Token Compression</h3>
<ul>
<li><strong>Authors: </strong>Sen Zeng, Hong Zhou, Zheng Zhu, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04519">https://arxiv.org/abs/2601.04519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04519">https://arxiv.org/pdf/2601.04519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04519]] TokenSeg: Efficient 3D Medical Image Segmentation via Hierarchical Visual Token Compression(https://arxiv.org/abs/2601.04519)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Three-dimensional medical image segmentation is a fundamental yet computationally demanding task due to the cubic growth of voxel processing and the redundant computation on homogeneous regions. To address these limitations, we propose \textbf{TokenSeg}, a boundary-aware sparse token representation framework for efficient 3D medical volume segmentation. Specifically, (1) we design a \emph{multi-scale hierarchical encoder} that extracts 400 candidate tokens across four resolution levels to capture both global anatomical context and fine boundary details; (2) we introduce a \emph{boundary-aware tokenizer} that combines VQ-VAE quantization with importance scoring to select 100 salient tokens, over 60\% of which lie near tumor boundaries; and (3) we develop a \emph{sparse-to-dense decoder} that reconstructs full-resolution masks through token reprojection, progressive upsampling, and skip connections. Extensive experiments on a 3D breast DCE-MRI dataset comprising 960 cases demonstrate that TokenSeg achieves state-of-the-art performance with 94.49\% Dice and 89.61\% IoU, while reducing GPU memory and inference latency by 64\% and 68\%, respectively. To verify the generalization capability, our evaluations on MSD cardiac and brain MRI benchmark datasets demonstrate that TokenSeg consistently delivers optimal performance across heterogeneous anatomical structures. These results highlight the effectiveness of anatomically informed sparse representation for accurate and efficient 3D medical image segmentation.</li>
</ul>

<h3>Title: GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence</h3>
<ul>
<li><strong>Authors: </strong>Yibo Zhao, Jiapeng Zhu, Zichen Ding, Xiang Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04525">https://arxiv.org/abs/2601.04525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04525">https://arxiv.org/pdf/2601.04525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04525]] GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence(https://arxiv.org/abs/2601.04525)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) integrates external knowledge to enhance Large Language Models (LLMs), yet systems remain susceptible to two critical flaws: providing correct answers without explicit grounded evidence and producing fabricated responses when the retrieved context is insufficient. While prior research has addressed these issues independently, a unified framework that integrates evidence-based grounding and reliable abstention is currently lacking. In this paper, we propose GRACE, a reinforcement-learning framework that simultaneously mitigates both types of flaws. GRACE employs a data construction method that utilizes heterogeneous retrievers to generate diverse training samples without manual annotation. A multi-stage gated reward function is then employed to train the model to assess evidence sufficiency, extract key supporting evidence, and provide answers or explicitly abstain. Experimental results on two benchmarks demonstrate that GRACE achieves state-of-the-art overall accuracy and strikes a favorable balance between accurate response and rejection, while requiring only 10% of the annotation costs of prior methods. Our code is available at this https URL..</li>
</ul>

<h3>Title: BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Amit Bin Tariqul, A N M Zahid Hossain Milkan, Sahab-Al-Chowdhury, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04534">https://arxiv.org/abs/2601.04534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04534">https://arxiv.org/pdf/2601.04534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04534]] BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation(https://arxiv.org/abs/2601.04534)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are increasingly deployed for text generation, watermarking has become essential for authorship attribution, intellectual property protection, and misuse detection. While existing watermarking methods perform well in high-resource languages, their robustness in low-resource languages remains underexplored. This work presents the first systematic evaluation of state-of-the-art text watermarking methods: KGW, Exponential Sampling (EXP), and Waterfall, for Bangla LLM text generation under cross-lingual round-trip translation (RTT) attacks. Under benign conditions, KGW and EXP achieve high detection accuracy (>88%) with negligible perplexity and ROUGE degradation. However, RTT causes detection accuracy to collapse below RTT causes detection accuracy to collapse to 9-13%, indicating a fundamental failure of token-level watermarking. To address this, we propose a layered watermarking strategy that combines embedding-time and post-generation watermarks. Experimental results show that layered watermarking improves post-RTT detection accuracy by 25-35%, achieving 40-50% accuracy, representing a 3$\times$ to 4$\times$ relative improvement over single-layer methods, at the cost of controlled semantic degradation. Our findings quantify the robustness-quality trade-off in multilingual watermarking and establish layered watermarking as a practical, training-free solution for low-resource languages such as Bangla. Our code and data will be made public.</li>
</ul>

<h3>Title: Not All Steps are Informative: On the Linearity of LLMs' RLVR Training</h3>
<ul>
<li><strong>Authors: </strong>Tianle Wang, Zhongyuan Wu, Shenghao Jin, Hao Xu, Wei Chen, Ning Miao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04537">https://arxiv.org/abs/2601.04537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04537">https://arxiv.org/pdf/2601.04537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04537]] Not All Steps are Informative: On the Linearity of LLMs' RLVR Training(https://arxiv.org/abs/2601.04537)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with verifiable rewards (RLVR) has become a central component of large language model (LLM) post-training. Unlike supervised fine-tuning (SFT), RLVR lets an LLM generate multiple candidate solutions and reinforces those that lead to a verifiably correct final answer. However, in practice, RLVR often requires thousands of training steps to reach strong performance, incurring substantial computation largely attributed to prolonged exploration. In this work, we make a surprising observation: during RLVR, LLMs evolve in a strongly linear manner. Specifically, both model weights and model output log-probabilities exhibit strong linear correlations with RL training steps. This suggests that RLVR predominantly amplifies trends that emerge early in training, rather than continuously discovering new behaviors throughout the entire optimization trajectory. Motivated by this linearity, we investigate whether future model states can be predicted from intermediate checkpoints via extrapolation, avoiding continued expensive training. We show that Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Moreover, Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.</li>
</ul>

<h3>Title: Identifying Good and Bad Neurons for Task-Level Controllable LLMs</h3>
<ul>
<li><strong>Authors: </strong>Wenjie Li, Guansong Pang, Hezhe Qiao, Debin Gao, David Lo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04548">https://arxiv.org/abs/2601.04548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04548">https://arxiv.org/pdf/2601.04548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04548]] Identifying Good and Bad Neurons for Task-Level Controllable LLMs(https://arxiv.org/abs/2601.04548)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models have demonstrated remarkable capabilities on multiple-choice question answering benchmarks, but the complex mechanisms underlying their large-scale neurons remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons that correlate positively with task completion, while neglecting neurons with other roles-such as inhibitive roles-and misled neuron attribution due to fortuitous behaviors in LLMs (i.e., correctly answer the questions by chance rather than genuine understanding). To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification. The key insight is that task performance is jointly determined by neurons with two opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. NeuronLLM achieves a holistic modeling of neurons via contrastive learning of good and bad neurons, while leveraging augmented question sets to mitigate the fortuitous behaviors in LLMs. Comprehensive experiments on LLMs of different sizes and families show the superiority of NeuronLLM over existing methods in four NLP tasks, providing new insights into LLM functional organization.</li>
</ul>

<h3>Title: GEnSHIN: Graphical Enhanced Spatio-temporal Hierarchical Inference Network for Traffic Flow Prediction</h3>
<ul>
<li><strong>Authors: </strong>Zhiyan Zhou, Junjie Liao, Manho Zhang, Yingyi Liao, Ziai Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04550">https://arxiv.org/abs/2601.04550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04550">https://arxiv.org/pdf/2601.04550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04550]] GEnSHIN: Graphical Enhanced Spatio-temporal Hierarchical Inference Network for Traffic Flow Prediction(https://arxiv.org/abs/2601.04550)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>With the acceleration of urbanization, intelligent transportation systems have an increasing demand for accurate traffic flow prediction. This paper proposes a novel Graph Enhanced Spatio-temporal Hierarchical Inference Network (GEnSHIN) to handle the complex spatio-temporal dependencies in traffic flow prediction. The model integrates three innovative designs: 1) An attention-enhanced Graph Convolutional Recurrent Unit (GCRU), which strengthens the modeling capability for long-term temporal dependencies by introducing Transformer modules; 2) An asymmetric dual-embedding graph generation mechanism, which leverages the real road network and data-driven latent asymmetric topology to generate graph structures that better fit the characteristics of actual traffic flow; 3) A dynamic memory bank module, which utilizes learnable traffic pattern prototypes to provide personalized traffic pattern representations for each sensor node, and introduces a lightweight graph updater during the decoding phase to adapt to dynamic changes in road network states. Extensive experiments on the public dataset METR-LA show that GEnSHIN achieves or surpasses the performance of comparative models across multiple metrics such as Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE). Notably, the model demonstrates excellent prediction stability during peak morning and evening traffic hours. Ablation experiments further validate the effectiveness of each core module and its contribution to the final performance.</li>
</ul>

<h3>Title: Deep Dive into the Abuse of DL APIs To Create Malicious AI Models and How to Detect Them</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Nabeel, Oleksii Starov</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04553">https://arxiv.org/abs/2601.04553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04553">https://arxiv.org/pdf/2601.04553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04553]] Deep Dive into the Abuse of DL APIs To Create Malicious AI Models and How to Detect Them(https://arxiv.org/abs/2601.04553)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>According to Gartner, more than 70% of organizations will have integrated AI models into their workflows by the end of 2025. In order to reduce cost and foster innovation, it is often the case that pre-trained models are fetched from model hubs like Hugging Face or TensorFlow Hub. However, this introduces a security risk where attackers can inject malicious code into the models they upload to these hubs, leading to various kinds of attacks including remote code execution (RCE), sensitive data exfiltration, and system file modification when these models are loaded or executed (predict function). Since AI models play a critical role in digital transformation, this would drastically increase the number of software supply chain attacks. While there are several efforts at detecting malware when deserializing pickle based saved models (hiding malware in model parameters), the risk of abusing DL APIs (e.g. TensorFlow APIs) is understudied. Specifically, we show how one can abuse hidden functionalities of TensorFlow APIs such as file read/write and network send/receive along with their persistence APIs to launch attacks. It is concerning to note that existing scanners in model hubs like Hugging Face and TensorFlow Hub are unable to detect some of the stealthy abuse of such APIs. This is because scanning tools only have a syntactically identified set of suspicious functionality that is being analysed. They often do not have a semantic-level understanding of the functionality utilized. After demonstrating the possible attacks, we show how one may identify potentially abusable hidden API functionalities using LLMs and build scanners to detect such abuses.</li>
</ul>

<h3>Title: All Changes May Have Invariant Principles: Improving Ever-Shifting Harmful Meme Detection via Design Concept Reproduction</h3>
<ul>
<li><strong>Authors: </strong>Ziyou Jiang, Mingyang Li, Junjie Wang, Yuekai Huang, Jie Huang, Zhiyuan Chang, Zhaoyang Li, Qing Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04567">https://arxiv.org/abs/2601.04567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04567">https://arxiv.org/pdf/2601.04567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04567]] All Changes May Have Invariant Principles: Improving Ever-Shifting Harmful Meme Detection via Design Concept Reproduction(https://arxiv.org/abs/2601.04567)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Harmful memes are ever-shifting in the Internet communities, which are difficult to analyze due to their type-shifting and temporal-evolving nature. Although these memes are shifting, we find that different memes may share invariant principles, i.e., the underlying design concept of malicious users, which can help us analyze why these memes are harmful. In this paper, we propose RepMD, an ever-shifting harmful meme detection method based on the design concept reproduction. We first refer to the attack tree to define the Design Concept Graph (DCG), which describes steps that people may take to design a harmful meme. Then, we derive the DCG from historical memes with design step reproduction and graph pruning. Finally, we use DCG to guide the Multimodal Large Language Model (MLLM) to detect harmful memes. The evaluation results show that RepMD achieves the highest accuracy with 81.1% and has slight accuracy decreases when generalized to type-shifting and temporal-evolving memes. Human evaluation shows that RepMD can improve the efficiency of human discovery on harmful memes, with 15$\sim$30 seconds per meme.</li>
</ul>

<h3>Title: Spatial-Temporal Feedback Diffusion Guidance for Controlled Traffic Imputation</h3>
<ul>
<li><strong>Authors: </strong>Xiaowei Mao, Huihu Ding, Yan Lin, Tingrui Wu, Shengnan Guo, Dazhuo Qiu, Feiling Fang, Jilin Hu, Huaiyu Wan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04572">https://arxiv.org/abs/2601.04572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04572">https://arxiv.org/pdf/2601.04572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04572]] Spatial-Temporal Feedback Diffusion Guidance for Controlled Traffic Imputation(https://arxiv.org/abs/2601.04572)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Imputing missing values in spatial-temporal traffic data is essential for intelligent transportation systems. Among advanced imputation methods, score-based diffusion models have demonstrated competitive performance. These models generate data by reversing a noising process, using observed values as conditional guidance. However, existing diffusion models typically apply a uniform guidance scale across both spatial and temporal dimensions, which is inadequate for nodes with high missing data rates. Sparse observations provide insufficient conditional guidance, causing the generative process to drift toward the learned prior distribution rather than closely following the conditional observations, resulting in suboptimal imputation performance. To address this, we propose FENCE, a spatial-temporal feedback diffusion guidance method designed to adaptively control guidance scales during imputation. First, FENCE introduces a dynamic feedback mechanism that adjusts the guidance scale based on the posterior likelihood approximations. The guidance scale is increased when generated values diverge from observations and reduced when alignment improves, preventing overcorrection. Second, because alignment to observations varies across nodes and denoising steps, a global guidance scale for all nodes is suboptimal. FENCE computes guidance scales at the cluster level by grouping nodes based on their attention scores, leveraging spatial-temporal correlations to provide more accurate guidance. Experimental results on real-world traffic datasets show that FENCE significantly enhances imputation accuracy.</li>
</ul>

<h3>Title: Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization</h3>
<ul>
<li><strong>Authors: </strong>Mizanur Rahman, Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Shafiq Joty, Enamul Hoque</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04582">https://arxiv.org/abs/2601.04582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04582">https://arxiv.org/pdf/2601.04582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04582]] Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization(https://arxiv.org/abs/2601.04582)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at this https URL.</li>
</ul>

<h3>Title: FedKDX: Federated Learning with Negative Knowledge Distillation for Enhanced Healthcare AI Systems</h3>
<ul>
<li><strong>Authors: </strong>Quang-Tu Pham, Hoang-Dieu Vu, Dinh-Dat Pham, Hieu H. Pham</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04587">https://arxiv.org/abs/2601.04587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04587">https://arxiv.org/pdf/2601.04587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04587]] FedKDX: Federated Learning with Negative Knowledge Distillation for Enhanced Healthcare AI Systems(https://arxiv.org/abs/2601.04587)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>This paper introduces FedKDX, a federated learning framework that addresses limitations in healthcare AI through Negative Knowledge Distillation (NKD). Unlike existing approaches that focus solely on positive knowledge transfer, FedKDX captures both target and non-target information to improve model generalization in healthcare applications. The framework integrates multiple knowledge transfer techniques--including traditional knowledge distillation, contrastive learning, and NKD--within a unified architecture that maintains privacy while reducing communication costs. Through experiments on healthcare datasets (SLEEP, UCI-HAR, and PAMAP2), FedKDX demonstrates improved accuracy (up to 2.53% over state-of-the-art methods), faster convergence, and better performance on non-IID data distributions. Theoretical analysis supports NKD's contribution to addressing statistical heterogeneity in distributed healthcare data. The approach shows promise for privacy-sensitive medical applications under regulatory frameworks like HIPAA and GDPR, offering a balanced solution between performance and practical implementation requirements in decentralized healthcare settings. The code and model are available at this https URL.</li>
</ul>

<h3>Title: 3D Conditional Image Synthesis of Left Atrial LGE MRI from Composite Semantic Masks</h3>
<ul>
<li><strong>Authors: </strong>Yusri Al-Sanaani, Rebecca Thornhill, Sreeraman Rajan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04588">https://arxiv.org/abs/2601.04588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04588">https://arxiv.org/pdf/2601.04588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04588]] 3D Conditional Image Synthesis of Left Atrial LGE MRI from Composite Semantic Masks(https://arxiv.org/abs/2601.04588)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Segmentation of the left atrial (LA) wall and endocardium from late gadolinium-enhanced (LGE) MRI is essential for quantifying atrial fibrosis in patients with atrial fibrillation. The development of accurate machine learning-based segmentation models remains challenging due to the limited availability of data and the complexity of anatomical structures. In this work, we investigate 3D conditional generative models as potential solution for augmenting scarce LGE training data and improving LA segmentation performance. We develop a pipeline to synthesize high-fidelity 3D LGE MRI volumes from composite semantic label maps combining anatomical expert annotations with unsupervised tissue clusters, using three 3D conditional generators (Pix2Pix GAN, SPADE-GAN, and SPADE-LDM). The synthetic images are evaluated for realism and their impact on downstream LA segmentation. SPADE-LDM generates the most realistic and structurally accurate images, achieving an FID of 4.063 and surpassing GAN models, which have FIDs of 40.821 and 7.652 for Pix2Pix and SPADE-GAN, respectively. When augmented with synthetic LGE images, the Dice score for LA cavity segmentation with a 3D U-Net model improved from 0.908 to 0.936, showing a statistically significant improvement (p < 0.05) over the this http URL findings demonstrate the potential of label-conditioned 3D synthesis to enhance the segmentation of under-represented cardiac structures.</li>
</ul>

<h3>Title: THaLLE-ThaiLLM: Domain-Specialized Small LLMs for Finance and Thai -- Technical Report</h3>
<ul>
<li><strong>Authors: </strong>KBTG Labs: Anuruth Lertpiya, Danupat Khamnuansin, Kantapong Sucharitpongpan, Pornchanan Balee, Tawunrat Chalothorn, Thadpong Pongthawornkamol, Monchai Lertsutthiwong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04597">https://arxiv.org/abs/2601.04597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04597">https://arxiv.org/pdf/2601.04597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04597]] THaLLE-ThaiLLM: Domain-Specialized Small LLMs for Finance and Thai -- Technical Report(https://arxiv.org/abs/2601.04597)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated significant potential across various domains, particularly in banking and finance, where they can automate complex tasks and enhance decision-making at scale. Due to privacy, security, and regulatory concerns, organizations often prefer on-premise deployment of LLMs. The ThaiLLM initiative aims to enhance Thai language capabilities in open-LLMs, enabling Thai industry to leverage advanced language models. However, organizations often face a trade-off between deploying multiple specialized models versus the prohibitive expense of training a single multi-capability model. To address this, we explore model merging as a resource-efficient alternative for developing high-performance, multi-capability LLMs. We present results from two key experiments: first, merging Qwen-8B with ThaiLLM-8B demonstrates how ThaiLLM-8B enhances Thai general capabilities, showing an uplift of M3 and M6 O-NET exams over the general instruction-following Qwen-8B. Second, we merge Qwen-8B with both ThaiLLM-8B and THaLLE-CFA-8B. This combination results in further improvements in performance across both general and financial domains, by demonstrating an uplift in both M3 and M6 O-NET, Flare-CFA, and Thai-IC benchmarks. The report showcases the viability of model merging for efficiently creating multi-capability LLMs.</li>
</ul>

<h3>Title: On the Limitations of Rank-One Model Editing in Answering Multi-hop Questions</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan He, Binghan Chen, Tianxiang Xiong, Ziyang Sun, Mozhao Zhu, Xi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04600">https://arxiv.org/abs/2601.04600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04600">https://arxiv.org/pdf/2601.04600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04600]] On the Limitations of Rank-One Model Editing in Answering Multi-hop Questions(https://arxiv.org/abs/2601.04600)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in Knowledge Editing (KE), particularly Rank-One Model Editing (ROME), show superior efficiency over fine-tuning and in-context learning for updating single-hop facts in transformers. However, these methods face significant challenges when applied to multi-hop reasoning tasks requiring knowledge chaining. In this work, we study the effect of editing knowledge with ROME on different layer depths and identify three key failure modes. First, the "hopping-too-late" problem occurs as later layers lack access to necessary intermediate representations. Second, generalization ability deteriorates sharply when editing later layers. Third, the model overfits to edited knowledge, incorrectly prioritizing edited-hop answers regardless of context. To mitigate the issues of "hopping-too-late" and generalisation decay, we propose Redundant Editing, a simple yet effective strategy that enhances multi-hop reasoning. Our experiments demonstrate that this approach can improve accuracy on 2-hop questions by at least 15.5 percentage points, representing a 96% increase over the previous single-edit strategy, while trading off some specificity and language naturalness.</li>
</ul>

<h3>Title: Constitutional Classifiers++: Efficient Production-Grade Defenses against Universal Jailbreaks</h3>
<ul>
<li><strong>Authors: </strong>Hoagy Cunningham, Jerry Wei, Zihan Wang, Andrew Persic, Alwin Peng, Jordan Abderrachid, Raj Agarwal, Bobby Chen, Austin Cohen, Andy Dau, Alek Dimitriev, Rob Gilson, Logan Howard, Yijin Hua, Jared Kaplan, Jan Leike, Mu Lin, Christopher Liu, Vladimir Mikulik, Rohit Mittapalli, Clare O'Hara, Jin Pan, Nikhil Saxena, Alex Silverstein, Yue Song, Xunjie Yu, Giulio Zhou, Ethan Perez, Mrinank Sharma</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04603">https://arxiv.org/abs/2601.04603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04603">https://arxiv.org/pdf/2601.04603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04603]] Constitutional Classifiers++: Efficient Production-Grade Defenses against Universal Jailbreaks(https://arxiv.org/abs/2601.04603)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>We introduce enhanced Constitutional Classifiers that deliver production-grade jailbreak robustness with dramatically reduced computational costs and refusal rates compared to previous-generation defenses. Our system combines several key insights. First, we develop exchange classifiers that evaluate model responses in their full conversational context, which addresses vulnerabilities in last-generation systems that examine outputs in isolation. Second, we implement a two-stage classifier cascade where lightweight classifiers screen all traffic and escalate only suspicious exchanges to more expensive classifiers. Third, we train efficient linear probe classifiers and ensemble them with external classifiers to simultaneously improve robustness and reduce computational costs. Together, these techniques yield a production-grade system achieving a 40x computational cost reduction compared to our baseline exchange classifier, while maintaining a 0.05% refusal rate on production traffic. Through extensive red-teaming comprising over 1,700 hours, we demonstrate strong protection against universal jailbreaks -- no attack on this system successfully elicited responses to all eight target queries comparable in detail to an undefended model. Our work establishes Constitutional Classifiers as practical and efficient safeguards for large language models.</li>
</ul>

<h3>Title: Detection of Deployment Operational Deviations for Safety and Security of AI-Enabled Human-Centric Cyber Physical Systems</h3>
<ul>
<li><strong>Authors: </strong>Bernard Ngabonziza, Ayan Banerjee, Sandeep K.S. Gupta</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04605">https://arxiv.org/abs/2601.04605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04605">https://arxiv.org/pdf/2601.04605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04605]] Detection of Deployment Operational Deviations for Safety and Security of AI-Enabled Human-Centric Cyber Physical Systems(https://arxiv.org/abs/2601.04605)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction</a></li>
<li><strong>Abstract: </strong>In recent years, Human-centric cyber-physical systems have increasingly involved artificial intelligence to enable knowledge extraction from sensor-collected data. Examples include medical monitoring and control systems, as well as autonomous cars. Such systems are intended to operate according to the protocols and guidelines for regular system operations. However, in many scenarios, such as closed-loop blood glucose control for Type 1 diabetics, self-driving cars, and monitoring systems for stroke diagnosis. The operations of such AI-enabled human-centric applications can expose them to cases for which their operational mode may be uncertain, for instance, resulting from the interactions with a human with the system. Such cases, in which the system is in uncertain conditions, can violate the system's safety and security requirements.  This paper will discuss operational deviations that can lead these systems to operate in unknown conditions. We will then create a framework to evaluate different strategies for ensuring the safety and security of AI-enabled human-centric cyber-physical systems in operation deployment. Then, as an example, we show a personalized image-based novel technique for detecting the non-announcement of meals in closed-loop blood glucose control for Type 1 diabetics.</li>
</ul>

<h3>Title: HUR-MACL: High-Uncertainty Region-Guided Multi-Architecture Collaborative Learning for Head and Neck Multi-Organ Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Liu, Siwen Wei, Linhao Qu, Mingyuan Pan, Chengsheng Zhang, Yonghong Shi, Zhijian Song</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04607">https://arxiv.org/abs/2601.04607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04607">https://arxiv.org/pdf/2601.04607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04607]] HUR-MACL: High-Uncertainty Region-Guided Multi-Architecture Collaborative Learning for Head and Neck Multi-Organ Segmentation(https://arxiv.org/abs/2601.04607)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of organs at risk in the head and neck is essential for radiation therapy, yet deep learning models often fail on small, complexly shaped organs. While hybrid architectures that combine different models show promise, they typically just concatenate features without exploiting the unique strengths of each component. This results in functional overlap and limited segmentation accuracy. To address these issues, we propose a high uncertainty region-guided multi-architecture collaborative learning (HUR-MACL) model for multi-organ segmentation in the head and neck. This model adaptively identifies high uncertainty regions using a convolutional neural network, and for these regions, Vision Mamba as well as Deformable CNN are utilized to jointly improve their segmentation accuracy. Additionally, a heterogeneous feature distillation loss was proposed to promote collaborative learning between the two architectures in high uncertainty regions to further enhance performance. Our method achieves SOTA results on two public datasets and one private dataset.</li>
</ul>

<h3>Title: Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR</h3>
<ul>
<li><strong>Authors: </strong>Yihong Tang, Kehai Chen, Xuefeng Bai, Benyou Wang, Zeming Liu, Haifeng Wang, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04611">https://arxiv.org/abs/2601.04611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04611">https://arxiv.org/pdf/2601.04611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04611]] Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR(https://arxiv.org/abs/2601.04611)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Current role-playing agents (RPAs) are typically constructed by imitating surface-level behaviors, but this approach lacks internal cognitive consistency, often causing out-of-character errors in complex situations. To address this, we propose Character-R1, a framework designed to provide comprehensive verifiable reward signals for effective role-aware reasoning, which are missing in recent studies. Specifically, our framework comprises three core designs: (1) Cognitive Focus Reward, which enforces explicit label-based analysis of 10 character elements (e.g., worldview) to structure internal cognition; (2) Reference-Guided Reward, which utilizes overlap-based metrics with reference responses as optimization anchors to enhance exploration and performance; and (3) Character-Conditioned Reward Normalization, which adjusts reward distributions based on character categories to ensure robust optimization across heterogeneous roles. Extensive experiments demonstrate that Character-R1 significantly outperforms existing methods in knowledge, memory and others.</li>
</ul>

<h3>Title: DeepHalo: A Neural Choice Model with Controllable Context Effects</h3>
<ul>
<li><strong>Authors: </strong>Shuhan Zhang, Zhi Wang, Rui Gao, Shuang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04616">https://arxiv.org/abs/2601.04616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04616">https://arxiv.org/pdf/2601.04616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04616]] DeepHalo: A Neural Choice Model with Controllable Context Effects(https://arxiv.org/abs/2601.04616)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Modeling human decision-making is central to applications such as recommendation, preference learning, and human-AI alignment. While many classic models assume context-independent choice behavior, a large body of behavioral research shows that preferences are often influenced by the composition of the choice set itself -- a phenomenon known as the context effect or Halo effect. These effects can manifest as pairwise (first-order) or even higher-order interactions among the available alternatives. Recent models that attempt to capture such effects either focus on the featureless setting or, in the feature-based setting, rely on restrictive interaction structures or entangle interactions across all orders, which limits interpretability. In this work, we propose DeepHalo, a neural modeling framework that incorporates features while enabling explicit control over interaction order and principled interpretation of context effects. Our model enables systematic identification of interaction effects by order and serves as a universal approximator of context-dependent choice functions when specialized to a featureless setting. Experiments on synthetic and real-world datasets demonstrate strong predictive performance while providing greater transparency into the drivers of choice.</li>
</ul>

<h3>Title: From National Curricula to Cultural Awareness: Constructing Open-Ended Culture-Specific Question Answering Dataset</h3>
<ul>
<li><strong>Authors: </strong>Haneul Yoo, Won Ik Cho, Geunhye Kim, Jiyoon Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04632">https://arxiv.org/abs/2601.04632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04632">https://arxiv.org/pdf/2601.04632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04632]] From National Curricula to Cultural Awareness: Constructing Open-Ended Culture-Specific Question Answering Dataset(https://arxiv.org/abs/2601.04632)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve strong performance on many tasks, but their progress remains uneven across languages and cultures, often reflecting values latent in English-centric training data. To enable practical cultural alignment, we propose a scalable approach that leverages national social studies curricula as a foundation for culture-aware supervision. We introduce CuCu, an automated multi-agent LLM framework that transforms national textbook curricula into open-ended, culture-specific question-answer pairs. Applying CuCu to the Korean national social studies curriculum, we construct KCaQA, comprising 34.1k open-ended QA pairs. Our quantitative and qualitative analyses suggest that KCaQA covers culture-specific topics and produces responses grounded in local sociocultural contexts.</li>
</ul>

<h3>Title: MAGA-Bench: Machine-Augment-Generated Text via Alignment Detection Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Anyang Song, Ying Cheng, Yiqian Xu, Rui Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04633">https://arxiv.org/abs/2601.04633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04633">https://arxiv.org/pdf/2601.04633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04633]] MAGA-Bench: Machine-Augment-Generated Text via Alignment Detection Benchmark(https://arxiv.org/abs/2601.04633)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) alignment is constantly evolving. Machine-Generated Text (MGT) is becoming increasingly difficult to distinguish from Human-Written Text (HWT). This has exacerbated abuse issues such as fake news and online fraud. Fine-tuned detectors' generalization ability is highly dependent on dataset quality, and simply expanding the sources of MGT is insufficient. Further augment of generation process is required. According to HC-Var's theory, enhancing the alignment of generated text can not only facilitate attacks on existing detectors to test their robustness, but also help improve the generalization ability of detectors fine-tuned on it. Therefore, we propose \textbf{M}achine-\textbf{A}ugment-\textbf{G}enerated Text via \textbf{A}lignment (MAGA). MAGA's pipeline achieves comprehensive alignment from prompt construction to reasoning process, among which \textbf{R}einforced \textbf{L}earning from \textbf{D}etectors \textbf{F}eedback (RLDF), systematically proposed by us, serves as a key component. In our experiments, the RoBERTa detector fine-tuned on MAGA training set achieved an average improvement of 4.60\% in generalization detection AUC. MAGA Dataset caused an average decrease of 8.13\% in the AUC of the selected detectors, expecting to provide indicative significance for future research on the generalization detection ability of detectors.</li>
</ul>

<h3>Title: SpeechMedAssist: Efficiently and Effectively Adapting Speech Language Models for Medical Consultation</h3>
<ul>
<li><strong>Authors: </strong>Sirry Chen, Jieyi Wang, Wei Chen, Zhongyu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04638">https://arxiv.org/abs/2601.04638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04638">https://arxiv.org/pdf/2601.04638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04638]] SpeechMedAssist: Efficiently and Effectively Adapting Speech Language Models for Medical Consultation(https://arxiv.org/abs/2601.04638)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Medical consultations are intrinsically speech-centric. However, most prior works focus on long-text-based interactions, which are cumbersome and patient-unfriendly. Recent advances in speech language models (SpeechLMs) have enabled more natural speech-based interaction, yet the scarcity of medical speech data and the inefficiency of directly fine-tuning on speech data jointly hinder the adoption of SpeechLMs in medical consultation. In this paper, we propose SpeechMedAssist, a SpeechLM natively capable of conducting speech-based multi-turn interactions with patients. By exploiting the architectural properties of SpeechLMs, we decouple the conventional one-stage training into a two-stage paradigm consisting of (1) Knowledge & Capability Injection via Text and (2) Modality Re-alignment with Limited Speech Data, thereby reducing the requirement for medical speech data to only 10k synthesized samples. To evaluate SpeechLMs for medical consultation scenarios, we design a benchmark comprising both single-turn question answering and multi-turn simulated interactions. Experimental results show that our model outperforms all baselines in both effectiveness and robustness in most evaluation settings.</li>
</ul>

<h3>Title: DP-MGTD: Privacy-Preserving Machine-Generated Text Detection via Adaptive Differentially Private Entity Sanitization</h3>
<ul>
<li><strong>Authors: </strong>Lionel Z. Wang, Yusheng Zhao, Jiabin Luo, Xinfeng Li, Lixu Wang, Yinan Peng, Haoyang Li, XiaoFeng Wang, Wei Dong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04641">https://arxiv.org/abs/2601.04641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04641">https://arxiv.org/pdf/2601.04641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04641]] DP-MGTD: Privacy-Preserving Machine-Generated Text Detection via Adaptive Differentially Private Entity Sanitization(https://arxiv.org/abs/2601.04641)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The deployment of Machine-Generated Text (MGT) detection systems necessitates processing sensitive user data, creating a fundamental conflict between authorship verification and privacy preservation. Standard anonymization techniques often disrupt linguistic fluency, while rigorous Differential Privacy (DP) mechanisms typically degrade the statistical signals required for accurate detection. To resolve this dilemma, we propose \textbf{DP-MGTD}, a framework incorporating an Adaptive Differentially Private Entity Sanitization algorithm. Our approach utilizes a two-stage mechanism that performs noisy frequency estimation and dynamically calibrates privacy budgets, applying Laplace and Exponential mechanisms to numerical and textual entities respectively. Crucially, we identify a counter-intuitive phenomenon where the application of DP noise amplifies the distinguishability between human and machine text by exposing distinct sensitivity patterns to perturbation. Extensive experiments on the MGTBench-2.0 dataset show that our method achieves near-perfect detection accuracy, significantly outperforming non-private baselines while satisfying strict privacy guarantees.</li>
</ul>

<h3>Title: CRANE: Causal Relevance Analysis of Language-Specific Neurons in Multilingual Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yifan Le, Yunliang Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04664">https://arxiv.org/abs/2601.04664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04664">https://arxiv.org/pdf/2601.04664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04664]] CRANE: Causal Relevance Analysis of Language-Specific Neurons in Multilingual Large Language Models(https://arxiv.org/abs/2601.04664)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multilingual large language models (LLMs) achieve strong performance across languages, yet how language capabilities are organized at the neuron level remains poorly understood. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. We propose CRANE, a relevance-based analysis framework that redefines language specificity in terms of functional necessity, identifying language-specific neurons through targeted neuron-level interventions. CRANE characterizes neuron specialization by their contribution to language-conditioned predictions rather than activation magnitude. Our implementation will be made publicly available. Neuron-level interventions reveal a consistent asymmetric pattern: masking neurons relevant to a target language selectively degrades performance on that language while preserving performance on other languages to a substantial extent, indicating language-selective but non-exclusive neuron specializations. Experiments on English, Chinese, and Vietnamese across multiple benchmarks, together with a dedicated relevance-based metric and base-to-chat model transfer analysis, show that CRANE isolates language-specific components more precisely than activation-based methods.</li>
</ul>

<h3>Title: Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Wentao Zhang, Lifei Wang, Lina Lu, MingKun Xu, Shangyang Li, Yanchao Yang, Tao Fang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04672">https://arxiv.org/abs/2601.04672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04672">https://arxiv.org/pdf/2601.04672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04672]] Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning(https://arxiv.org/abs/2601.04672)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Agricultural disease diagnosis challenges VLMs, as conventional fine-tuning requires extensive labels, lacks interpretability, and generalizes poorly. While reasoning improves model robustness, existing methods rely on costly expert annotations and rarely address the open-ended, diverse nature of agricultural queries. To address these limitations, we propose \textbf{Agri-R1}, a reasoning-enhanced large model for agriculture. Our framework automates high-quality reasoning data generation via vision-language synthesis and LLM-based filtering, using only 19\% of available samples. Training employs Group Relative Policy Optimization (GRPO) with a novel proposed reward function that integrates domain-specific lexicons and fuzzy matching to assess both correctness and linguistic flexibility in open-ended responses. Evaluated on CDDMBench, our resulting 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines, showing a +23.2\% relative gain in disease recognition accuracy, +33.3\% in agricultural knowledge QA, and a +26.10-point improvement in cross-domain generalization over standard fine-tuning. Ablation studies confirm that the synergy between structured reasoning data and GRPO-driven exploration underpins these gains, with benefits scaling as question complexity increases.</li>
</ul>

<h3>Title: DB-MSMUNet:Dual Branch Multi-scale Mamba UNet for Pancreatic CT Scans Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Qiu Guan, Zhiqiang Yang, Dezhang Ye, Yang Chen, Xinli Xu, Ying Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04676">https://arxiv.org/abs/2601.04676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04676">https://arxiv.org/pdf/2601.04676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04676]] DB-MSMUNet:Dual Branch Multi-scale Mamba UNet for Pancreatic CT Scans Segmentation(https://arxiv.org/abs/2601.04676)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of the pancreas and its lesions in CT scans is crucial for the precise diagnosis and treatment of pancreatic cancer. However, it remains a highly challenging task due to several factors such as low tissue contrast with surrounding organs, blurry anatomical boundaries, irregular organ shapes, and the small size of lesions. To tackle these issues, we propose DB-MSMUNet (Dual-Branch Multi-scale Mamba UNet), a novel encoder-decoder architecture designed specifically for robust pancreatic segmentation. The encoder is constructed using a Multi-scale Mamba Module (MSMM), which combines deformable convolutions and multi-scale state space modeling to enhance both global context modeling and local deformation adaptation. The network employs a dual-decoder design: the edge decoder introduces an Edge Enhancement Path (EEP) to explicitly capture boundary cues and refine fuzzy contours, while the area decoder incorporates a Multi-layer Decoder (MLD) to preserve fine-grained details and accurately reconstruct small lesions by leveraging multi-scale deep semantic features. Furthermore, Auxiliary Deep Supervision (ADS) heads are added at multiple scales to both decoders, providing more accurate gradient feedback and further enhancing the discriminative capability of multi-scale features. We conduct extensive experiments on three datasets: the NIH Pancreas dataset, the MSD dataset, and a clinical pancreatic tumor dataset provided by collaborating hospitals. DB-MSMUNet achieves Dice Similarity Coefficients of 89.47%, 87.59%, and 89.02%, respectively, outperforming most existing state-of-the-art methods in terms of segmentation accuracy, edge preservation, and robustness across different datasets. These results demonstrate the effectiveness and generalizability of the proposed method for real-world pancreatic CT segmentation tasks.</li>
</ul>

<h3>Title: HATIR: Heat-Aware Diffusion for Turbulent Infrared Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Yang Zou, Xingyue Zhu, Kaiqi Han, Jun Ma, Xingyuan Li, Zhiying Jiang, Jinyuan Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04682">https://arxiv.org/abs/2601.04682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04682">https://arxiv.org/pdf/2601.04682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04682]] HATIR: Heat-Aware Diffusion for Turbulent Infrared Video Super-Resolution(https://arxiv.org/abs/2601.04682)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Infrared video has been of great interest in visual tasks under challenging environments, but often suffers from severe atmospheric turbulence and compression degradation. Existing video super-resolution (VSR) methods either neglect the inherent modality gap between infrared and visible images or fail to restore turbulence-induced distortions. Directly cascading turbulence mitigation (TM) algorithms with VSR methods leads to error propagation and accumulation due to the decoupled modeling of degradation between turbulence and resolution. We introduce HATIR, a Heat-Aware Diffusion for Turbulent InfraRed Video Super-Resolution, which injects heat-aware deformation priors into the diffusion sampling path to jointly model the inverse process of turbulent degradation and structural detail loss. Specifically, HATIR constructs a Phasor-Guided Flow Estimator, rooted in the physical principle that thermally active regions exhibit consistent phasor responses over time, enabling reliable turbulence-aware flow to guide the reverse diffusion process. To ensure the fidelity of structural recovery under nonuniform distortions, a Turbulence-Aware Decoder is proposed to selectively suppress unstable temporal cues and enhance edge-aware feature aggregation via turbulence gating and structure-aware attention. We built FLIR-IVSR, the first dataset for turbulent infrared VSR, comprising paired LR-HR sequences from a FLIR T1050sc camera (1024 X 768) spanning 640 diverse scenes with varying camera and object motion conditions. This encourages future research in infrared VSR. Project page: this https URL</li>
</ul>

<h3>Title: WebCryptoAgent: Agentic Crypto Trading with Web Informatics</h3>
<ul>
<li><strong>Authors: </strong>Ali Kurban, Wei Luo, Liangyu Zuo, Zeyu Zhang, Renda Han, Zhaolu Kang, Hao Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04687">https://arxiv.org/abs/2601.04687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04687">https://arxiv.org/pdf/2601.04687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04687]] WebCryptoAgent: Agentic Crypto Trading with Web Informatics(https://arxiv.org/abs/2601.04687)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust</a></li>
<li><strong>Abstract: </strong>Cryptocurrency trading increasingly depends on timely integration of heterogeneous web information and market microstructure signals to support short-horizon decision making under extreme volatility. However, existing trading systems struggle to jointly reason over noisy multi-source web evidence while maintaining robustness to rapid price shocks at sub-second timescales. The first challenge lies in synthesizing unstructured web content, social sentiment, and structured OHLCV signals into coherent and interpretable trading decisions without amplifying spurious correlations, while the second challenge concerns risk control, as slow deliberative reasoning pipelines are ill-suited for handling abrupt market shocks that require immediate defensive responses. To address these challenges, we propose WebCryptoAgent, an agentic trading framework that decomposes web-informed decision making into modality-specific agents and consolidates their outputs into a unified evidence document for confidence-calibrated reasoning. We further introduce a decoupled control architecture that separates strategic hourly reasoning from a real-time second-level risk model, enabling fast shock detection and protective intervention independent of the trading loop. Extensive experiments on real-world cryptocurrency markets demonstrate that WebCryptoAgent improves trading stability, reduces spurious activity, and enhances tail-risk handling compared to existing baselines. Code will be available at this https URL.</li>
</ul>

<h3>Title: ToolGate: Contract-Grounded and Verified Tool Execution for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yanming Liu, Xinyue Peng, Jiannan Cao, Xinyi Wang, Songhang Deng, Jintao Chen, Jianwei Yin, Xuhong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.FL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04688">https://arxiv.org/abs/2601.04688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04688">https://arxiv.org/pdf/2601.04688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04688]] ToolGate: Contract-Grounded and Verified Tool Execution for LLMs(https://arxiv.org/abs/2601.04688)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) augmented with external tools have demonstrated remarkable capabilities in complex reasoning tasks. However, existing frameworks rely heavily on natural language reasoning to determine when tools can be invoked and whether their results should be committed, lacking formal guarantees for logical safety and verifiability. We present \textbf{ToolGate}, a forward execution framework that provides logical safety guarantees and verifiable state evolution for LLM tool calling. ToolGate maintains an explicit symbolic state space as a typed key-value mapping representing trusted world information throughout the reasoning process. Each tool is formalized as a Hoare-style contract consisting of a precondition and a postcondition, where the precondition gates tool invocation by checking whether the current state satisfies the required conditions, and the postcondition determines whether the tool's result can be committed to update the state through runtime verification. Our approach guarantees that the symbolic state evolves only through verified tool executions, preventing invalid or hallucinated results from corrupting the world representation. Experimental validation demonstrates that ToolGate significantly improves the reliability and verifiability of tool-augmented LLM systems while maintaining competitive performance on complex multi-step reasoning tasks. This work establishes a foundation for building more trustworthy and debuggable AI systems that integrate language models with external tools.</li>
</ul>

<h3>Title: Do LLMs Benefit from User and Item Embeddings in Recommendation Tasks?</h3>
<ul>
<li><strong>Authors: </strong>Mir Rayat Imtiaz Hossain, Leo Feng, Leonid Sigal, Mohamed Osama Ahmed</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04690">https://arxiv.org/abs/2601.04690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04690">https://arxiv.org/pdf/2601.04690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04690]] Do LLMs Benefit from User and Item Embeddings in Recommendation Tasks?(https://arxiv.org/abs/2601.04690)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as promising recommendation systems, offering novel ways to model user preferences through generative approaches. However, many existing methods often rely solely on text semantics or incorporate collaborative signals in a limited manner, typically using only user or item embeddings. These methods struggle to handle multiple item embeddings representing user history, reverting to textual semantics and neglecting richer collaborative information. In this work, we propose a simple yet effective solution that projects user and item embeddings, learned from collaborative filtering, into the LLM token space via separate lightweight projector modules. A finetuned LLM then conditions on these projected embeddings alongside textual tokens to generate recommendations. Preliminary results show that this design effectively leverages structured user-item interaction data, improves recommendation performance over text-only LLM baselines, and offers a practical path for bridging traditional recommendation systems with modern LLMs.</li>
</ul>

<h3>Title: See, Explain, and Intervene: A Few-Shot Multimodal Agent Framework for Hateful Meme Moderation</h3>
<ul>
<li><strong>Authors: </strong>Naquee Rizwan, Subhankar Swain, Paramananda Bhaskar, Gagan Aryan, Shehryaar Shah Khan, Animesh Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04692">https://arxiv.org/abs/2601.04692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04692">https://arxiv.org/pdf/2601.04692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04692]] See, Explain, and Intervene: A Few-Shot Multimodal Agent Framework for Hateful Meme Moderation(https://arxiv.org/abs/2601.04692)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we examine hateful memes from three complementary angles - how to detect them, how to explain their content and how to intervene them prior to being posted - by applying a range of strategies built on top of generative AI models. To the best of our knowledge, explanation and intervention have typically been studied separately from detection, which does not reflect real-world conditions. Further, since curating large annotated datasets for meme moderation is prohibitively expensive, we propose a novel framework that leverages task-specific generative multimodal agents and the few-shot adaptability of large multimodal models to cater to different types of memes. We believe this is the first work focused on generalizable hateful meme moderation under limited data conditions, and has strong potential for deployment in real-world production scenarios. Warning: Contains potentially toxic contents.</li>
</ul>

<h3>Title: Thunder-KoNUBench: A Corpus-Aligned Benchmark for Korean Negation Understanding</h3>
<ul>
<li><strong>Authors: </strong>Sungmok Jung, Yeonkyoung So, Joonhak Lee, Sangho Kim, Yelim Ahn, Jaejin Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04693">https://arxiv.org/abs/2601.04693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04693">https://arxiv.org/pdf/2601.04693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04693]] Thunder-KoNUBench: A Corpus-Aligned Benchmark for Korean Negation Understanding(https://arxiv.org/abs/2601.04693)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although negation is known to challenge large language models (LLMs), benchmarks for evaluating negation understanding, especially in Korean, are scarce. We conduct a corpus-based analysis of Korean negation and show that LLM performance degrades under negation. We then introduce Thunder-KoNUBench, a sentence-level benchmark that reflects the empirical distribution of Korean negation phenomena. Evaluating 47 LLMs, we analyze the effects of model size and instruction tuning, and show that fine-tuning on Thunder-KoNUBench improves negation understanding and broader contextual comprehension in Korean.</li>
</ul>

<h3>Title: Unified Framework for Qualifying Security Boundary of PUFs Against Machine Learning Attacks</h3>
<ul>
<li><strong>Authors: </strong>Hongming Fei, Zilong Hu, Prosanta Gope, Biplab Sikdar</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04697">https://arxiv.org/abs/2601.04697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04697">https://arxiv.org/pdf/2601.04697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04697]] Unified Framework for Qualifying Security Boundary of PUFs Against Machine Learning Attacks(https://arxiv.org/abs/2601.04697)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Physical Unclonable Functions (PUFs) serve as lightweight, hardware-intrinsic entropy sources widely deployed in IoT security applications. However, delay-based PUFs are vulnerable to Machine Learning Attacks (MLAs), undermining their assumed unclonability. There are no valid metrics for evaluating PUF MLA resistance, but empirical modelling experiments, which lack theoretical guarantees and are highly sensitive to advances in machine learning techniques. To address the fundamental gap between PUF designs and security qualifications, this work proposes a novel, formal, and unified framework for evaluating PUF security against modelling attacks by providing security lower bounds, independent of specific attack models or learning algorithms. We mathematically characterise the adversary's advantage in predicting responses to unseen challenges based solely on observed challenge-response pairs (CRPs), formulating the problem as a conditional probability estimation over the space of candidate PUFs. We present our analysis on previous "broken" PUFs, e.g., Arbiter PUFs, XOR PUFs, Feed-Forward PUFs, and for the first time compare their MLA resistance in a formal way. In addition, we evaluate the currently "secure" CT PUF, and show its security boundary. We demonstrate that the proposed approach systematically quantifies PUF resilience, captures subtle security differences, and provides actionable, theoretically grounded security guarantees for the practical deployment of PUFs.</li>
</ul>

<h3>Title: PRISM: A Unified Framework for Post-Training LLMs Without Verifiable Rewards</h3>
<ul>
<li><strong>Authors: </strong>Mukesh Ghimire, Aosong Feng, Liwen You, Youzhi Luo, Fang Liu, Xuan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04700">https://arxiv.org/abs/2601.04700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04700">https://arxiv.org/pdf/2601.04700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04700]] PRISM: A Unified Framework for Post-Training LLMs Without Verifiable Rewards(https://arxiv.org/abs/2601.04700)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Current techniques for post-training Large Language Models (LLMs) rely either on costly human supervision or on external verifiers to boost performance on tasks such as mathematical reasoning and code generation. However, as LLMs improve their problem-solving, any further improvement will potentially require high-quality solutions to difficult problems that are not available to humans. As a result, learning from unlabeled data is becoming increasingly attractive in the research community. Existing methods extract learning signal from a model's consistency, either by majority voting or by converting the model's internal confidence into reward. Although internal consistency metric such as entropy or self-certainty require no human intervention, as we show in this work, these are unreliable signals for large-scale and long-term training. To address the unreliability, we propose PRISM, a unified training framework that uses a Process Reward Model (PRM) to guide learning alongside model's internal confidence in the absence of ground-truth labels. We show that effectively combining PRM with self-certainty can lead to both stable training and better test-time performance, and also keep the model's internal confidence in check.</li>
</ul>

<h3>Title: Prior-Informed Zeroth-Order Optimization with Adaptive Direction Alignment for Memory-Efficient LLM Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Feihu Jin, Shipeng Cen, Ying Tan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04710">https://arxiv.org/abs/2601.04710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04710">https://arxiv.org/pdf/2601.04710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04710]] Prior-Informed Zeroth-Order Optimization with Adaptive Direction Alignment for Memory-Efficient LLM Fine-Tuning(https://arxiv.org/abs/2601.04710)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) has achieved remarkable success across various NLP tasks, but the substantial memory overhead during backpropagation remains a critical bottleneck, especially as model scales grow. Zeroth-order (ZO) optimization alleviates this issue by estimating gradients through forward passes and Gaussian sampling, avoiding the need for backpropagation. However, conventional ZO methods suffer from high variance in gradient estimation due to their reliance on random perturbations, leading to slow convergence and suboptimal performance. We propose a simple plug-and-play method that incorporates prior-informed perturbations to refine gradient estimation. Our method dynamically computes a guiding vector from Gaussian samples, which directs perturbations toward more informative directions, significantly accelerating convergence compared to standard ZO approaches. We further investigate a greedy perturbation strategy to explore the impact of prior knowledge on gradient estimation. Theoretically, we prove that our gradient estimator achieves stronger alignment with the true gradient direction, enhancing optimization efficiency. Extensive experiments across LLMs of varying scales and architectures demonstrate that our proposed method could seamlessly integrate into existing optimization methods, delivering faster convergence and superior performance. Notably, on the OPT-13B model, our method outperforms traditional ZO optimization across all 11 benchmark tasks and surpasses gradient-based baselines on 9 out of 11 tasks, establishing a robust balance between efficiency and accuracy.</li>
</ul>

<h3>Title: DSC2025 -- ViHallu Challenge: Detecting Hallucination in Vietnamese LLMs</h3>
<ul>
<li><strong>Authors: </strong>Anh Thi-Hoang Nguyen, Khanh Quoc Tran, Tin Van Huynh, Phuoc Tan-Hoang Nguyen, Cam Tan Nguyen, Kiet Van Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04711">https://arxiv.org/abs/2601.04711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04711">https://arxiv.org/pdf/2601.04711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04711]] DSC2025 -- ViHallu Challenge: Detecting Hallucination in Vietnamese LLMs(https://arxiv.org/abs/2601.04711)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The reliability of large language models (LLMs) in production environments remains significantly constrained by their propensity to generate hallucinations--fluent, plausible-sounding outputs that contradict or fabricate information. While hallucination detection has recently emerged as a priority in English-centric benchmarks, low-to-medium resource languages such as Vietnamese remain inadequately covered by standardized evaluation frameworks. This paper introduces the DSC2025 ViHallu Challenge, the first large-scale shared task for detecting hallucinations in Vietnamese LLMs. We present the ViHallu dataset, comprising 10,000 annotated triplets of (context, prompt, response) samples systematically partitioned into three hallucination categories: no hallucination, intrinsic, and extrinsic hallucinations. The dataset incorporates three prompt types--factual, noisy, and adversarial--to stress-test model robustness. A total of 111 teams participated, with the best-performing system achieving a macro-F1 score of 84.80\%, compared to a baseline encoder-only score of 32.83\%, demonstrating that instruction-tuned LLMs with structured prompting and ensemble strategies substantially outperform generic architectures. However, the gap to perfect performance indicates that hallucination detection remains a challenging problem, particularly for intrinsic (contradiction-based) hallucinations. This work establishes a rigorous benchmark and explores a diverse range of detection methodologies, providing a foundation for future research into the trustworthiness and reliability of Vietnamese language AI systems.</li>
</ul>

<h3>Title: On the Holistic Approach for Detecting Human Image Forgery</h3>
<ul>
<li><strong>Authors: </strong>Xiao Guo, Jie Zhu, Anil Jain, Xiaoming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04715">https://arxiv.org/abs/2601.04715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04715">https://arxiv.org/pdf/2601.04715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04715]] On the Holistic Approach for Detecting Human Image Forgery(https://arxiv.org/abs/2601.04715)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of AI-generated content (AIGC) has escalated the threat of deepfakes, from facial manipulations to the synthesis of entire photorealistic human bodies. However, existing detection methods remain fragmented, specializing either in facial-region forgeries or full-body synthetic images, and consequently fail to generalize across the full spectrum of human image manipulations. We introduce HuForDet, a holistic framework for human image forgery detection, which features a dual-branch architecture comprising: (1) a face forgery detection branch that employs heterogeneous experts operating in both RGB and frequency domains, including an adaptive Laplacian-of-Gaussian (LoG) module designed to capture artifacts ranging from fine-grained blending boundaries to coarse-scale texture irregularities; and (2) a contextualized forgery detection branch that leverages a Multi-Modal Large Language Model (MLLM) to analyze full-body semantic consistency, enhanced with a confidence estimation mechanism that dynamically weights its contribution during feature fusion. We curate a human image forgery (HuFor) dataset that unifies existing face forgery data with a new corpus of full-body synthetic humans. Extensive experiments show that our HuForDet achieves state-of-the-art forgery detection performance and superior robustness across diverse human image forgeries.</li>
</ul>

<h3>Title: Fame Fades, Nature Remains: Disentangling the Character Identity of Role-Playing Agents</h3>
<ul>
<li><strong>Authors: </strong>Yonghyun Jun, Junhyuk Choi, Jihyeong Park, Hwanhee Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04716">https://arxiv.org/abs/2601.04716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04716">https://arxiv.org/pdf/2601.04716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04716]] Fame Fades, Nature Remains: Disentangling the Character Identity of Role-Playing Agents(https://arxiv.org/abs/2601.04716)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Despite the rapid proliferation of Role-Playing Agents (RPAs) based on Large Language Models (LLMs), the structural dimensions defining a character's identity remain weakly formalized, often treating characters as arbitrary text inputs. In this paper, we propose the concept of \textbf{Character Identity}, a multidimensional construct that disentangles a character into two distinct layers: \textbf{(1) Parametric Identity}, referring to character-specific knowledge encoded from the LLM's pre-training, and \textbf{(2) Attributive Identity}, capturing fine-grained behavioral properties such as personality traits and moral values. To systematically investigate these layers, we construct a unified character profile schema and generate both Famous and Synthetic characters under identical structural constraints. Our evaluation across single-turn and multi-turn interactions reveals two critical phenomena. First, we identify \textit{"Fame Fades"}: while famous characters hold a significant advantage in initial turns due to parametric knowledge, this edge rapidly vanishes as models prioritize accumulating conversational context over pre-trained priors. Second, we find that \textit{"Nature Remains"}: while models robustly portray general personality traits regardless of polarity, RPA performance is highly sensitive to the valence of morality and interpersonal relationships. Our findings pinpoint negative social natures as the primary bottleneck in RPA fidelity, guiding future character construction and evaluation.</li>
</ul>

<h3>Title: GPU-Accelerated INT8 Quantization for KV Cache Compression in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Maanas Taneja, Purab Shingvi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04719">https://arxiv.org/abs/2601.04719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04719">https://arxiv.org/pdf/2601.04719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04719]] GPU-Accelerated INT8 Quantization for KV Cache Compression in Large Language Models(https://arxiv.org/abs/2601.04719)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The key-value (KV) cache in large language models presents a significant memory bottleneck during inference, growing linearly with sequence length and often exceeding the memory footprint of model weights themselves. We implement and evaluate GPU-accelerated INT8 quantization for KV cache compression, achieving 4$\times$ memory reduction with minimal accuracy degradation. We develop four CUDA kernel variants -- naive, tiled, coarsened, and vectorized -- and benchmark them across realistic workload sizes up to 1 billion elements. Our vectorized kernel achieves up to 1,694$\times$ speedup over CPU baselines while maintaining reconstruction error below 0.004 and attention score error below 0.1 even for 8K-dimensional heads. These results demonstrate that INT8 quantization provides a practical approach for reducing memory pressure in LLM inference with negligible computational overhead (6--58ms) and minimal impact on downstream model behavior</li>
</ul>

<h3>Title: Training a Custom CNN on Five Heterogeneous Image Datasets</h3>
<ul>
<li><strong>Authors: </strong>Anika Tabassum, Tasnuva Mahazabin Tuba, Nafisa Naznin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04727">https://arxiv.org/abs/2601.04727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04727">https://arxiv.org/pdf/2601.04727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04727]] Training a Custom CNN on Five Heterogeneous Image Datasets(https://arxiv.org/abs/2601.04727)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning has transformed visual data analysis, with Convolutional Neural Networks (CNNs) becoming highly effective in learning meaningful feature representations directly from images. Unlike traditional manual feature engineering methods, CNNs automatically extract hierarchical visual patterns, enabling strong performance across diverse real-world contexts. This study investigates the effectiveness of CNN-based architectures across five heterogeneous datasets spanning agricultural and urban domains: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring. These datasets introduce varying challenges, including differences in illumination, resolution, environmental complexity, and class imbalance, necessitating adaptable and robust learning models. We evaluate a lightweight, task-specific custom CNN alongside established deep architectures, including ResNet-18 and VGG-16, trained both from scratch and using transfer learning. Through systematic preprocessing, augmentation, and controlled experimentation, we analyze how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty. The key contributions of this work are: (1) the development of an efficient custom CNN that achieves competitive performance across multiple application domains, and (2) a comprehensive comparative analysis highlighting when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments. These findings offer practical insights for deploying deep learning models in resource-limited yet high-impact real-world visual classification tasks.</li>
</ul>

<h3>Title: Automatic Classifiers Underdetect Emotions Expressed by Men</h3>
<ul>
<li><strong>Authors: </strong>Ivan Smirnov, Segun T. Aroyehun, Paul Plener, David Garcia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04730">https://arxiv.org/abs/2601.04730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04730">https://arxiv.org/pdf/2601.04730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04730]] Automatic Classifiers Underdetect Emotions Expressed by Men(https://arxiv.org/abs/2601.04730)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The widespread adoption of automatic sentiment and emotion classifiers makes it important to ensure that these tools perform reliably across different populations. Yet their reliability is typically assessed using benchmarks that rely on third-party annotators rather than the individuals experiencing the emotions themselves, potentially concealing systematic biases. In this paper, we use a unique, large-scale dataset of more than one million self-annotated posts and a pre-registered research design to investigate gender biases in emotion detection across 414 combinations of models and emotion-related classes. We find that across different types of automatic classifiers and various underlying emotions, error rates are consistently higher for texts authored by men compared to those authored by women. We quantify how this bias could affect results in downstream applications and show that current machine learning tools, including large language models, should be applied with caution when the gender composition of a sample is not known or variable. Our findings demonstrate that sentiment analysis is not yet a solved problem, especially in ensuring equitable model behaviour across demographic groups.</li>
</ul>

<h3>Title: AIVD: Adaptive Edge-Cloud Collaboration for Accurate and Efficient Industrial Visual Detection</h3>
<ul>
<li><strong>Authors: </strong>Yunqing Hu, Zheming Yang, Chang Zhao, Qi Guo, Meng Gao, Pengcheng Li, Wen Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04734">https://arxiv.org/abs/2601.04734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04734">https://arxiv.org/pdf/2601.04734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04734]] AIVD: Adaptive Edge-Cloud Collaboration for Accurate and Efficient Industrial Visual Detection(https://arxiv.org/abs/2601.04734)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) demonstrate exceptional capabilities in semantic understanding and visual reasoning, yet they still face challenges in precise object localization and resource-constrained edge-cloud deployment. To address this, this paper proposes the AIVD framework, which achieves unified precise localization and high-quality semantic generation through the collaboration between lightweight edge detectors and cloud-based MLLMs. To enhance the cloud MLLM's robustness against edge cropped-box noise and scenario variations, we design an efficient fine-tuning strategy with visual-semantic collaborative augmentation, significantly improving classification accuracy and semantic consistency. Furthermore, to maintain high throughput and low latency across heterogeneous edge devices and dynamic network conditions, we propose a heterogeneous resource-aware dynamic scheduling algorithm. Experimental results demonstrate that AIVD substantially reduces resource consumption while improving MLLM classification performance and semantic generation quality. The proposed scheduling strategy also achieves higher throughput and lower latency across diverse scenarios.</li>
</ul>

<h3>Title: AM$^3$Safety: Towards Data Efficient Alignment of Multi-modal Multi-turn Safety for MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Han Zhu, Jiale Chen, Chengkun Cai, Shengjie Sun, Haoran Li, Yujin Zhou, Chi-Min Chan, Pengcheng Wen, Lei Li, Sirui Han, Yike Guo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04736">https://arxiv.org/abs/2601.04736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04736">https://arxiv.org/pdf/2601.04736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04736]] AM$^3$Safety: Towards Data Efficient Alignment of Multi-modal Multi-turn Safety for MLLMs(https://arxiv.org/abs/2601.04736)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Multi-modal Large Language Models (MLLMs) are increasingly deployed in interactive applications. However, their safety vulnerabilities become pronounced in multi-turn multi-modal scenarios, where harmful intent can be gradually reconstructed across turns, and security protocols fade into oblivion as the conversation progresses. Existing Reinforcement Learning from Human Feedback (RLHF) alignment methods are largely developed for single-turn visual question-answer (VQA) task and often require costly manual preference annotations, limiting their effectiveness and scalability in dialogues. To address this challenge, we present InterSafe-V, an open-source multi-modal dialogue dataset containing 11,270 dialogues and 500 specially designed refusal VQA samples. This dataset, constructed through interaction between several models, is designed to more accurately reflect real-world scenarios and includes specialized VQA pairs tailored for specific domains. Building on this dataset, we propose AM$^3$Safety, a framework that combines a cold-start refusal phase with Group Relative Policy Optimization (GRPO) fine-tuning using turn-aware dual-objective rewards across entire dialogues. Experiments on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B show more than 10\% decrease in Attack Success Rate (ASR) together with an increment of at least 8\% in harmless dimension and over 13\% in helpful dimension of MLLMs on multi-modal multi-turn safety benchmarks, while preserving their general abilities.</li>
</ul>

<h3>Title: RiskAtlas: Exposing Domain-Specific Risks in LLMs through Knowledge-Graph-Guided Harmful Prompt Generation</h3>
<ul>
<li><strong>Authors: </strong>Huawei Zheng, Xinqi Jiang, Sen Yang, Shouling Ji, Yingcai Wu, Dazhen Deng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04740">https://arxiv.org/abs/2601.04740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04740">https://arxiv.org/pdf/2601.04740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04740]] RiskAtlas: Exposing Domain-Specific Risks in LLMs through Knowledge-Graph-Guided Harmful Prompt Generation(https://arxiv.org/abs/2601.04740)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly applied in specialized domains such as finance and healthcare, where they introduce unique safety risks. Domain-specific datasets of harmful prompts remain scarce and still largely rely on manual construction; public datasets mainly focus on explicit harmful prompts, which modern LLM defenses can often detect and refuse. In contrast, implicit harmful prompts-expressed through indirect domain knowledge-are harder to detect and better reflect real-world threats. We identify two challenges: transforming domain knowledge into actionable constraints and increasing the implicitness of generated harmful prompts. To address them, we propose an end-to-end framework that first performs knowledge-graph-guided harmful prompt generation to systematically produce domain-relevant prompts, and then applies dual-path obfuscation rewriting to convert explicit harmful prompts into implicit variants via direct and context-enhanced rewriting. This framework yields high-quality datasets combining strong domain relevance with implicitness, enabling more realistic red-teaming and advancing LLM safety research. We release our code and datasets at GitHub.</li>
</ul>

<h3>Title: Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Seyeon Jeong, Yeonjun Choi, JongWook Kim, Beakcheol Jang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04742">https://arxiv.org/abs/2601.04742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04742">https://arxiv.org/pdf/2601.04742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04742]] Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval(https://arxiv.org/abs/2601.04742)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.</li>
</ul>

<h3>Title: Intraday spatiotemporal PV power prediction at national scale using satellite-based solar forecast models</h3>
<ul>
<li><strong>Authors: </strong>Luca Lanzilao, Angela Meyer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04751">https://arxiv.org/abs/2601.04751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04751">https://arxiv.org/pdf/2601.04751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04751]] Intraday spatiotemporal PV power prediction at national scale using satellite-based solar forecast models(https://arxiv.org/abs/2601.04751)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a novel framework for spatiotemporal photovoltaic (PV) power forecasting and use it to evaluate the reliability, sharpness, and overall performance of seven intraday PV power nowcasting models. The model suite includes satellite-based deep learning and optical-flow approaches and physics-based numerical weather prediction models, covering both deterministic and probabilistic formulations. Forecasts are first validated against satellite-derived surface solar irradiance (SSI). Irradiance fields are then converted into PV power using station-specific machine learning models, enabling comparison with production data from 6434 PV stations across Switzerland. To our knowledge, this is the first study to investigate spatiotemporal PV forecasting at a national scale. We additionally provide the first visualizations of how mesoscale cloud systems shape national PV production on hourly and sub-hourly timescales. Our results show that satellite-based approaches outperform the Integrated Forecast System (IFS-ENS), particularly at short lead times. Among them, SolarSTEPS and SHADECast deliver the most accurate SSI and PV power predictions, with SHADECast providing the most reliable ensemble spread. The deterministic model IrradianceNet achieves the lowest root mean square error, while probabilistic forecasts of SolarSTEPS and SHADECast provide better-calibrated uncertainty. Forecast skill generally decreases with elevation. At a national scale, satellite-based models forecast the daily total PV generation with relative errors below 10% for 82% of the days in 2019-2020, demonstrating robustness and their potential for operational use.</li>
</ul>

<h3>Title: Skeletonization-Based Adversarial Perturbations on Large Vision Language Model's Mathematical Text Recognition</h3>
<ul>
<li><strong>Authors: </strong>Masatomo Yoshida, Haruto Namura, Nicola Adami, Masahiro Okuda</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04752">https://arxiv.org/abs/2601.04752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04752">https://arxiv.org/pdf/2601.04752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04752]] Skeletonization-Based Adversarial Perturbations on Large Vision Language Model's Mathematical Text Recognition(https://arxiv.org/abs/2601.04752)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>This work explores the visual capabilities and limitations of foundation models by introducing a novel adversarial attack method utilizing skeletonization to reduce the search space effectively. Our approach specifically targets images containing text, particularly mathematical formula images, which are more challenging due to their LaTeX conversion and intricate structure. We conduct a detailed evaluation of both character and semantic changes between original and adversarially perturbed outputs to provide insights into the models' visual interpretation and reasoning abilities. The effectiveness of our method is further demonstrated through its application to ChatGPT, which shows its practical implications in real-world scenarios.</li>
</ul>

<h3>Title: PILOT-Bench: A Benchmark for Legal Reasoning in the Patent Domain with IRAC-Aligned Classification Tasks</h3>
<ul>
<li><strong>Authors: </strong>Yehoon Jang, Chaewon Lee, Hyun-seok Min, Sungchul Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04758">https://arxiv.org/abs/2601.04758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04758">https://arxiv.org/pdf/2601.04758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04758]] PILOT-Bench: A Benchmark for Legal Reasoning in the Patent Domain with IRAC-Aligned Classification Tasks(https://arxiv.org/abs/2601.04758)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Patent Trial and Appeal Board (PTAB) of the USPTO adjudicates thousands of ex parte appeals each year, requiring the integration of technical understanding and legal reasoning. While large language models (LLMs) are increasingly applied in patent and legal practice, their use has remained limited to lightweight tasks, with no established means of systematically evaluating their capacity for structured legal reasoning in the patent domain. In this work, we introduce PILOT-Bench, the first PTAB-centric benchmark that aligns PTAB decisions with USPTO patent data at the case-level and formalizes three IRAC-aligned classification tasks: Issue Type, Board Authorities, and Subdecision. We evaluate a diverse set of closed-source (commercial) and open-source LLMs and conduct analyses across multiple perspectives, including input-variation settings, model families, and error tendencies. Notably, on the Issue Type task, closed-source models consistently exceed 0.75 in Micro-F1 score, whereas the strongest open-source model (Qwen-8B) achieves performance around 0.56, highlighting a substantial gap in reasoning capabilities. PILOT-Bench establishes a foundation for the systematic evaluation of patent-domain legal reasoning and points toward future directions for improving LLMs through dataset design and model alignment. All data, code, and benchmark resources are available at this https URL.</li>
</ul>

<h3>Title: Differential syntactic and semantic encoding in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Santiago Acevedo, Alessandro Laio, Marco Baroni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04765">https://arxiv.org/abs/2601.04765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04765">https://arxiv.org/pdf/2601.04765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04765]] Differential syntactic and semantic encoding in LLMs(https://arxiv.org/abs/2601.04765)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We study how syntactic and semantic information is encoded in inner layer representations of Large Language Models (LLMs), focusing on the very large DeepSeek-V3. We find that, by averaging hidden-representation vectors of sentences sharing syntactic structure or meaning, we obtain vectors that capture a significant proportion of the syntactic and semantic information contained in the representations. In particular, subtracting these syntactic and semantic ``centroids'' from sentence vectors strongly affects their similarity with syntactically and semantically matched sentences, respectively, suggesting that syntax and semantics are, at least partially, linearly encoded. We also find that the cross-layer encoding profiles of syntax and semantics are different, and that the two signals can to some extent be decoupled, suggesting differential encoding of these two types of linguistic information in LLM representations.</li>
</ul>

<h3>Title: Revisiting Judge Decoding from First Principles via Training-Free Distributional Divergence</h3>
<ul>
<li><strong>Authors: </strong>Shengyin Sun, Yiming Li, Renxi Liu, Weizhe Lin, Hui-Ling Zhen, Xianzhi Yu, Mingxuan Yuan, Chen Ma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04766">https://arxiv.org/abs/2601.04766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04766">https://arxiv.org/pdf/2601.04766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04766]] Revisiting Judge Decoding from First Principles via Training-Free Distributional Divergence(https://arxiv.org/abs/2601.04766)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Judge Decoding accelerates LLM inference by relaxing the strict verification of Speculative Decoding, yet it typically relies on expensive and noisy supervision. In this work, we revisit this paradigm from first principles, revealing that the ``criticality'' scores learned via costly supervision are intrinsically encoded in the draft-target distributional divergence. We theoretically prove a structural correspondence between learned linear judges and Kullback-Leibler (KL) divergence, demonstrating they rely on the same underlying logit primitives. Guided by this, we propose a simple, training-free verification mechanism based on KL divergence. Extensive experiments across reasoning and coding benchmarks show that our method matches or outperforms complex trained judges (e.g., AutoJudge), offering superior robustness to domain shifts and eliminating the supervision bottleneck entirely.</li>
</ul>

<h3>Title: Segmentation-Driven Monocular Shape from Polarization based on Physical Model</h3>
<ul>
<li><strong>Authors: </strong>Jinyu Zhang, Xu Ma, Weili Chen, Gonzalo R. Arce</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04776">https://arxiv.org/abs/2601.04776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04776">https://arxiv.org/pdf/2601.04776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04776]] Segmentation-Driven Monocular Shape from Polarization based on Physical Model(https://arxiv.org/abs/2601.04776)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Monocular shape-from-polarization (SfP) leverages the intrinsic relationship between light polarization properties and surface geometry to recover surface normals from single-view polarized images, providing a compact and robust approach for three-dimensional (3D) reconstruction. Despite its potential, existing monocular SfP methods suffer from azimuth angle ambiguity, an inherent limitation of polarization analysis, that severely compromises reconstruction accuracy and stability. This paper introduces a novel segmentation-driven monocular SfP (SMSfP) framework that reformulates global shape recovery into a set of local reconstructions over adaptively segmented convex sub-regions. Specifically, a polarization-aided adaptive region growing (PARG) segmentation strategy is proposed to decompose the global convexity assumption into locally convex regions, effectively suppressing azimuth ambiguities and preserving surface continuity. Furthermore, a multi-scale fusion convexity prior (MFCP) constraint is developed to ensure local surface consistency and enhance the recovery of fine textural and structural details. Extensive experiments on both synthetic and real-world datasets validate the proposed approach, showing significant improvements in disambiguation accuracy and geometric fidelity compared with existing physics-based monocular SfP techniques.</li>
</ul>

<h3>Title: GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shurong Zheng, Yousong Zhu, Hongyin Zhao, Fan Yang, Yufei Zhan, Ming Tang, Jinqiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04777">https://arxiv.org/abs/2601.04777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04777">https://arxiv.org/pdf/2601.04777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04777]] GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models(https://arxiv.org/abs/2601.04777)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model's overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.</li>
</ul>

<h3>Title: CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tobia Poppi, Burak Uzkent, Amanmeet Garg, Lucas Porto, Garin Kessler, Yezhou Yang, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara, Florian Schiffers</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04778">https://arxiv.org/abs/2601.04778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04778">https://arxiv.org/pdf/2601.04778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04778]] CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models(https://arxiv.org/abs/2601.04778)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video-language models (VLMs) achieve strong multimodal understanding but remain prone to hallucinations, especially when reasoning about actions and temporal order. Existing mitigation strategies, such as textual filtering or random video perturbations, often fail to address the root cause: over-reliance on language priors rather than fine-grained visual dynamics. We propose a scalable framework for counterfactual video generation that synthesizes videos differing only in actions or temporal structure while preserving scene context. Our pipeline combines multimodal LLMs for action proposal and editing guidance with diffusion-based image and video models to generate semantic hard negatives at scale. Using this framework, we build CounterVid, a synthetic dataset of ~26k preference pairs targeting action recognition and temporal reasoning. We further introduce MixDPO, a unified Direct Preference Optimization approach that jointly leverages textual and visual preferences. Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, notably in temporal ordering, and transfers effectively to standard video hallucination benchmarks. Code and models will be made publicly available.</li>
</ul>

<h3>Title: AgentOCR: Reimagining Agent History via Optical Self-Compression</h3>
<ul>
<li><strong>Authors: </strong>Lang Feng, Fuchao Yang, Feng Chen, Xin Cheng, Haiyang Xu, Zhenglin Wan, Ming Yan, Bo An</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04786">https://arxiv.org/abs/2601.04786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04786">https://arxiv.org/pdf/2601.04786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04786]] AgentOCR: Reimagining Agent History via Optical Self-Compression(https://arxiv.org/abs/2601.04786)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\% of text-based agent performance while substantially reducing token consumption (>50\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.</li>
</ul>

<h3>Title: NC2C: Automated Convexification of Generic Non-Convex Optimization Problems</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Peng, Yanming Liu, Yihan Cang, Yuwei Zhang, Xinyi Wang, Songhang Deng, Jiannan Cao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04789">https://arxiv.org/abs/2601.04789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04789">https://arxiv.org/pdf/2601.04789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04789]] NC2C: Automated Convexification of Generic Non-Convex Optimization Problems(https://arxiv.org/abs/2601.04789)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Non-convex optimization problems are pervasive across mathematical programming, engineering design, and scientific computing, often posing intractable challenges for traditional solvers due to their complex objective functions and constrained landscapes. To address the inefficiency of manual convexification and the over-reliance on expert knowledge, we propose NC2C, an LLM-based end-to-end automated framework designed to transform generic non-convex optimization problems into solvable convex forms using large language models. NC2C leverages LLMs' mathematical reasoning capabilities to autonomously detect non-convex components, select optimal convexification strategies, and generate rigorous convex equivalents. The framework integrates symbolic reasoning, adaptive transformation techniques, and iterative validation, equipped with error correction loops and feasibility domain correction mechanisms to ensure the robustness and validity of transformed problems. Experimental results on a diverse dataset of 100 generic non-convex problems demonstrate that NC2C achieves an 89.3\% execution rate and a 76\% success rate in producing feasible, high-quality convex transformations. This outperforms baseline methods by a significant margin, highlighting NC2C's ability to leverage LLMs for automated non-convex to convex transformation, reduce expert dependency, and enable efficient deployment of convex solvers for previously intractable optimization tasks.</li>
</ul>

<h3>Title: Belief in Authority: Impact of Authority in Multi-Agent Evaluation Framework</h3>
<ul>
<li><strong>Authors: </strong>Junhyuk Choi, Jeongyoun Kwon, Heeju Kim, Haeun Cho, Hayeong Jung, Sehee Min, Bugeun Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04790">https://arxiv.org/abs/2601.04790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04790">https://arxiv.org/pdf/2601.04790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04790]] Belief in Authority: Impact of Authority in Multi-Agent Evaluation Framework(https://arxiv.org/abs/2601.04790)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-agent systems utilizing large language models often assign authoritative roles to improve performance, yet the impact of authority bias on agent interactions remains underexplored. We present the first systematic analysis of role-based authority bias in free-form multi-agent evaluation using ChatEval. Applying French and Raven's power-based theory, we classify authoritative roles into legitimate, referent, and expert types and analyze their influence across 12-turn conversations. Experiments with GPT-4o and DeepSeek R1 reveal that Expert and Referent power roles exert stronger influence than Legitimate power roles. Crucially, authority bias emerges not through active conformity by general agents, but through authoritative roles consistently maintaining their positions while general agents demonstrate flexibility. Furthermore, authority influence requires clear position statements, as neutral responses fail to generate bias. These findings provide key insights for designing multi-agent frameworks with asymmetric interaction patterns.</li>
</ul>

<h3>Title: Measurement-Consistent Langevin Corrector: A Remedy for Latent Diffusion Inverse Solvers</h3>
<ul>
<li><strong>Authors: </strong>Lee Hyoseok, Sohwi Lim, Eunju Cha, Tae-Hyun Oh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04791">https://arxiv.org/abs/2601.04791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04791">https://arxiv.org/pdf/2601.04791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04791]] Measurement-Consistent Langevin Corrector: A Remedy for Latent Diffusion Inverse Solvers(https://arxiv.org/abs/2601.04791)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>With recent advances in generative models, diffusion models have emerged as powerful priors for solving inverse problems in each domain. Since Latent Diffusion Models (LDMs) provide generic priors, several studies have explored their potential as domain-agnostic zero-shot inverse solvers. Despite these efforts, existing latent diffusion inverse solvers suffer from their instability, exhibiting undesirable artifacts and degraded quality. In this work, we first identify the instability as a discrepancy between the solver's and true reverse diffusion dynamics, and show that reducing this gap stabilizes the solver. Building on this, we introduce Measurement-Consistent Langevin Corrector (MCLC), a theoretically grounded plug-and-play correction module that remedies the LDM-based inverse solvers through measurement-consistent Langevin updates. Compared to prior approaches that rely on linear manifold assumptions, which often do not hold in latent space, MCLC operates without this assumption, leading to more stable and reliable behavior. We experimentally demonstrate the effectiveness of MCLC and its compatibility with existing solvers across diverse image restoration tasks. Additionally, we analyze blob artifacts and offer insights into their underlying causes. We highlight that MCLC is a key step toward more robust zero-shot inverse problem solvers.</li>
</ul>

<h3>Title: PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference</h3>
<ul>
<li><strong>Authors: </strong>Denis Korzhenkov, Adil Karjauv, Animesh Karnewar, Mohsen Ghafoorian, Amirhossein Habibian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04792">https://arxiv.org/abs/2601.04792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04792">https://arxiv.org/pdf/2601.04792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04792]] PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference(https://arxiv.org/abs/2601.04792)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at this https URL.</li>
</ul>

<h3>Title: Detector-Augmented SAMURAI for Long-Duration Drone Tracking</h3>
<ul>
<li><strong>Authors: </strong>Tamara R. Lenhard, Andreas Weinmann, Hichem Snoussi, Tobias Koch</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04798">https://arxiv.org/abs/2601.04798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04798">https://arxiv.org/pdf/2601.04798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04798]] Detector-Augmented SAMURAI for Long-Duration Drone Tracking(https://arxiv.org/abs/2601.04798)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI's potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI's zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.</li>
</ul>

<h3>Title: Parallelizing Node-Level Explainability in Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Oscar Llorente, Jaime Boal, Eugenio F. Sánchez-Úbeda, Antonio Diaz-Cano, Miguel Familiar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04807">https://arxiv.org/abs/2601.04807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04807">https://arxiv.org/pdf/2601.04807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04807]] Parallelizing Node-Level Explainability in Graph Neural Networks(https://arxiv.org/abs/2601.04807)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have demonstrated remarkable performance in a wide range of tasks, such as node classification, link prediction, and graph classification, by exploiting the structural information in graph-structured data. However, in node classification, computing node-level explainability becomes extremely time-consuming as the size of the graph increases, while batching strategies often degrade explanation quality. This paper introduces a novel approach to parallelizing node-level explainability in GNNs through graph partitioning. By decomposing the graph into disjoint subgraphs, we enable parallel computation of explainability for node neighbors, significantly improving the scalability and efficiency without affecting the correctness of the results, provided sufficient memory is available. For scenarios where memory is limited, we further propose a dropout-based reconstruction mechanism that offers a controllable trade-off between memory usage and explanation fidelity. Experimental results on real-world datasets demonstrate substantial speedups, enabling scalable and transparent explainability for large-scale GNN models.</li>
</ul>

<h3>Title: SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Oriol Rabasseda, Zenjie Li, Kamal Nasrollahi, Sergio Escalera</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04824">https://arxiv.org/abs/2601.04824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04824">https://arxiv.org/pdf/2601.04824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04824]] SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models(https://arxiv.org/abs/2601.04824)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models. Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.</li>
</ul>

<h3>Title: Quantum Secure Biometric Authentication in Decentralised Systems</h3>
<ul>
<li><strong>Authors: </strong>Tooba Qasim, Vasilios A. Siris, Izak Oosthuizen, Muttukrishnan Rajarajan, Sujit Biswas</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04852">https://arxiv.org/abs/2601.04852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04852">https://arxiv.org/pdf/2601.04852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04852]] Quantum Secure Biometric Authentication in Decentralised Systems(https://arxiv.org/abs/2601.04852)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, robust, biometric</a></li>
<li><strong>Abstract: </strong>Biometric authentication has become integral to digital identity systems, particularly in smart cities where it en-ables secure access to services across governance, trans-portation, and public infrastructure. Centralised archi-tectures, though widely used, pose privacy and scalabil-ity challenges due to the aggregation of sensitive biomet-ric data. Decentralised identity frameworks offer better data sovereignty and eliminate single points of failure but introduce new security concerns, particularly around mu-tual trust among distributed devices. In such environments, biometric sensors and verification agents must authenticate one another before sharing sensitive biometric data. Ex-isting authentication schemes rely on classical public key infrastructure, which is increasingly susceptible to quan-tum attacks. This work addresses this gap by propos-ing a quantum-secure communication protocol for decen-tralised biometric systems, built upon an enhanced Quan-tum Key Distribution (QKD) system. The protocol incorpo-rates quantum-resilient authentication at both the classical and quantum layers of QKD: post-quantum cryptography (PQC) is used to secure the classical channel, while authen-tication qubits verify the integrity of the quantum channel. Once trust is established, QKD generates symmetric keys for encrypting biometric data in transit. Qiskit-based sim-ulations show a key generation rate of 15 bits/sec and 89% efficiency. This layered, quantum-resilient approach offers scalable, robust authentication for next-generation smart city infrastructures.</li>
</ul>

<h3>Title: RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Liu, Runteng Guo, Baojie Qu, Yuechen Jiang, Min Peng, Qianqian Xie, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04853">https://arxiv.org/abs/2601.04853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04853">https://arxiv.org/pdf/2601.04853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04853]] RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection(https://arxiv.org/abs/2601.04853)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at this https URL.</li>
</ul>

<h3>Title: Token Maturation: Autoregressive Language Generation via Continuous Token Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Oshri Naparstek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04854">https://arxiv.org/abs/2601.04854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04854">https://arxiv.org/pdf/2601.04854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04854]] Token Maturation: Autoregressive Language Generation via Continuous Token Dynamics(https://arxiv.org/abs/2601.04854)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Autoregressive language models are conventionally defined over discrete token sequences, committing to a specific token at every generation step. This early discretization forces uncertainty to be resolved through token-level sampling, often leading to instability, repetition, and sensitivity to decoding heuristics. In this work, we introduce a continuous autoregressive formulation of language generation in which tokens are represented as continuous vectors that \emph{mature} over multiple update steps before being discretized. Rather than sampling tokens, the model evolves continuous token representations through a deterministic dynamical process, committing to a discrete token only when the representation has sufficiently converged. Discrete text is recovered via hard decoding, while uncertainty is maintained and resolved in the continuous space. We show that this maturation process alone is sufficient to produce coherent and diverse text using deterministic decoding (argmax), without reliance on token-level sampling, diffusion-style denoising, or auxiliary stabilization mechanisms. Additional perturbations, such as stochastic dynamics or history smoothing, can be incorporated naturally but are not required for the model to function. To our knowledge, this is the first autoregressive language model that generates text by evolving continuous token representations to convergence prior to discretization, enabling stable generation without token-level sampling.</li>
</ul>

<h3>Title: Rethinking GNNs and Missing Features: Challenges, Evaluation and a Robust Solution</h3>
<ul>
<li><strong>Authors: </strong>Francesco Ferrini, Veronica Lachi, Antonio Longa, Bruno Lepri, Matono Akiyoshi, Andrea Passerini, Xin Liu, Manfred Jaeger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04855">https://arxiv.org/abs/2601.04855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04855">https://arxiv.org/pdf/2601.04855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04855]] Rethinking GNNs and Missing Features: Challenges, Evaluation and a Robust Solution(https://arxiv.org/abs/2601.04855)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Handling missing node features is a key challenge for deploying Graph Neural Networks (GNNs) in real-world domains such as healthcare and sensor networks. Existing studies mostly address relatively benign scenarios, namely benchmark datasets with (a) high-dimensional but sparse node features and (b) incomplete data generated under Missing Completely At Random (MCAR) mechanisms. For (a), we theoretically prove that high sparsity substantially limits the information loss caused by missingness, making all models appear robust and preventing a meaningful comparison of their performance. To overcome this limitation, we introduce one synthetic and three real-world datasets with dense, semantically meaningful features. For (b), we move beyond MCAR and design evaluation protocols with more realistic missingness mechanisms. Moreover, we provide a theoretical background to state explicit assumptions on the missingness process and analyze their implications for different methods. Building on this analysis, we propose GNNmim, a simple yet effective baseline for node classification with incomplete feature data. Experiments show that GNNmim is competitive with respect to specialized architectures across diverse datasets and missingness regimes.</li>
</ul>

<h3>Title: MisSpans: Fine-Grained False Span Identification in Cross-Domain Fake News</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Liu, Paul Thompson, Jiaqi Rong, Baojie Qu, Runteng Guo, Min Peng, Qianqian Xie, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04857">https://arxiv.org/abs/2601.04857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04857">https://arxiv.org/pdf/2601.04857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04857]] MisSpans: Fine-Grained False Span Identification in Cross-Domain Fake News(https://arxiv.org/abs/2601.04857)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Online misinformation is increasingly pervasive, yet most existing benchmarks and methods evaluate veracity at the level of whole claims or paragraphs using coarse binary labels, obscuring how true and false details often co-exist within single sentences. These simplifications also limit interpretability: global explanations cannot identify which specific segments are misleading or differentiate how a detail is false (e.g., distorted vs. fabricated). To address these gaps, we introduce MisSpans, the first multi-domain, human-annotated benchmark for span-level misinformation detection and analysis, consisting of paired real and fake news stories. MisSpans defines three complementary tasks: MisSpansIdentity for pinpointing false spans within sentences, MisSpansType for categorising false spans by misinformation type, and MisSpansExplanation for providing rationales grounded in identified spans. Together, these tasks enable fine-grained localisation, nuanced characterisation beyond true/false and actionable explanations. Expert annotators were guided by standardised guidelines and consistency checks, leading to high inter-annotator agreement. We evaluate 15 representative LLMs, including reasoning-enhanced and non-reasoning variants, under zero-shot and one-shot settings. Results reveal the challenging nature of fine-grained misinformation identification and analysis, and highlight the need for a deeper understanding of how performance may be influenced by multiple interacting factors, including model size and reasoning capabilities, along with domain-specific textual features. This project will be available at this https URL.</li>
</ul>

<h3>Title: DivAS: Interactive 3D Segmentation of NeRFs via Depth-Weighted Voxel Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Ayush Pande</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04860">https://arxiv.org/abs/2601.04860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04860">https://arxiv.org/pdf/2601.04860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04860]] DivAS: Interactive 3D Segmentation of NeRFs via Depth-Weighted Voxel Aggregation(https://arxiv.org/abs/2601.04860)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Existing methods for segmenting Neural Radiance Fields (NeRFs) are often optimization-based, requiring slow per-scene training that sacrifices the zero-shot capabilities of 2D foundation models. We introduce DivAS (Depth-interactive Voxel Aggregation Segmentation), an optimization-free, fully interactive framework that addresses these limitations. Our method operates via a fast GUI-based workflow where 2D SAM masks, generated from user point prompts, are refined using NeRF-derived depth priors to improve geometric accuracy and foreground-background separation. The core of our contribution is a custom CUDA kernel that aggregates these refined multi-view masks into a unified 3D voxel grid in under 200ms, enabling real-time visual feedback. This optimization-free design eliminates the need for per-scene training. Experiments on Mip-NeRF 360° and LLFF show that DivAS achieves segmentation quality comparable to optimization-based methods, while being 2-2.5x faster end-to-end, and up to an order of magnitude faster when excluding user prompting time.</li>
</ul>

<h3>Title: FibreCastML: An Open Web Platform for Predicting Electrospun Nanofibre Diameter Distributions</h3>
<ul>
<li><strong>Authors: </strong>Elisa Roldan, Kirstie Andrews, Stephen M. Richardson, Reyhaneh Fatahian, Glen Cooper, Rasool Erfani, Tasneem Sabir, Neil D. Reeves</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04873">https://arxiv.org/abs/2601.04873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04873">https://arxiv.org/pdf/2601.04873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04873]] FibreCastML: An Open Web Platform for Predicting Electrospun Nanofibre Diameter Distributions(https://arxiv.org/abs/2601.04873)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Electrospinning is a scalable technique for producing fibrous scaffolds with tunable micro- and nanoscale architectures for applications in tissue engineering, drug delivery, and wound care. While machine learning (ML) has been used to support electrospinning process optimisation, most existing approaches predict only mean fibre diameters, neglecting the full diameter distribution that governs scaffold performance. This work presents FibreCastML, an open, distribution-aware ML framework that predicts complete fibre diameter spectra from routinely reported electrospinning parameters and provides interpretable insights into process structure relationships. A meta-dataset comprising 68538 individual fibre diameter measurements extracted from 1778 studies across 16 biomedical polymers was curated. Six standard processing parameters, namely solution concentration, applied voltage, flow rate, tip to collector distance, needle diameter, and collector rotation speed, were used to train seven ML models using nested cross validation with leave one study out external folds. Model interpretability was achieved using variable importance analysis, SHapley Additive exPlanations, correlation matrices, and three dimensional parameter maps. Non linear models consistently outperformed linear baselines, achieving coefficients of determination above 0.91 for several widely used polymers. Solution concentration emerged as the dominant global driver of fibre diameter distributions. Experimental validation across different electrospinning systems demonstrated close agreement between predicted and measured distributions. FibreCastML enables more reproducible and data driven optimisation of electrospun scaffold architectures.</li>
</ul>

<h3>Title: Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Mingyue Cheng, Daoyu Wang, Qi Liu, Shuo Yu, Xiaoyu Tao, Yuqian Wang, Chengzhong Chu, Yu Duan, Mingkang Long, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04879">https://arxiv.org/abs/2601.04879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04879">https://arxiv.org/pdf/2601.04879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04879]] Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis(https://arxiv.org/abs/2601.04879)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at this https URL.</li>
</ul>

<h3>Title: CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters</h3>
<ul>
<li><strong>Authors: </strong>Ao Sun, Xiaoyu Wang, Zhe Tan, Yu Li, Jiachen Zhu, Shu Su, Yuheng Jia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04885">https://arxiv.org/abs/2601.04885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04885">https://arxiv.org/pdf/2601.04885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04885]] CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters(https://arxiv.org/abs/2601.04885)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) serve a global audience, alignment must transition from enforcing universal consensus to respecting cultural pluralism. We demonstrate that dense models, when forced to fit conflicting value distributions, suffer from \textbf{Mean Collapse}, converging to a generic average that fails to represent diverse groups. We attribute this to \textbf{Cultural Sparsity}, where gradient interference prevents dense parameters from spanning distinct cultural modes. To resolve this, we propose \textbf{\textsc{CuMA}} (\textbf{Cu}ltural \textbf{M}ixture of \textbf{A}dapters), a framework that frames alignment as a \textbf{conditional capacity separation} problem. By incorporating demographic-aware routing, \textsc{CuMA} internalizes a \textit{Latent Cultural Topology} to explicitly disentangle conflicting gradients into specialized expert subspaces. Extensive evaluations on WorldValuesBench, Community Alignment, and PRISM demonstrate that \textsc{CuMA} achieves state-of-the-art performance, significantly outperforming both dense baselines and semantic-only MoEs. Crucially, our analysis confirms that \textsc{CuMA} effectively mitigates mean collapse, preserving cultural diversity. Our code is available at this https URL.</li>
</ul>

<h3>Title: Faithful Summarisation under Disagreement via Belief-Level Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Favour Yahdii Aghaebe, Tanefa Apekey, Elizabeth Williams, Nafise Sadat Moosavi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04889">https://arxiv.org/abs/2601.04889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04889">https://arxiv.org/pdf/2601.04889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04889]] Faithful Summarisation under Disagreement via Belief-Level Aggregation(https://arxiv.org/abs/2601.04889)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Opinion and multi-document summarisation often involve genuinely conflicting viewpoints, yet many existing approaches, particularly LLM-based systems, implicitly smooth disagreement and over-represent majority opinions. This limits the faithfulness of generated summaries in opinion-heavy settings. We introduce a disagreement-aware synthesis pipeline that separates belief-level aggregation from language generation. Documents are first represented as structured belief sets and aggregated using distance-based belief merging operators that explicitly model conflict. Large language models are then used only to realise the aggregated beliefs as natural language summaries. We evaluate the approach across multiple model families and scales, comparing it to methods that perform explicit aggregation during generation. Our results show that while sufficiently large models can match belief-level aggregation when aggregation is handled at generation time, this behaviour is not stable across architectures or capacities. In contrast, belief-level aggregation combined with simple prompting yields consistently strong disagreement-aware performance across models, while maintaining fluent and grounded summaries.</li>
</ul>

<h3>Title: V-FAT: Benchmarking Visual Fidelity Against Text-bias</h3>
<ul>
<li><strong>Authors: </strong>Ziteng Wang, Yujie He, Guanliang Li, Siqi Yang, Jiaqi Xiong, Songxiang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04897">https://arxiv.org/abs/2601.04897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04897">https://arxiv.org/pdf/2601.04897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04897]] V-FAT: Benchmarking Visual Fidelity Against Text-bias(https://arxiv.org/abs/2601.04897)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize "lucky" linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.</li>
</ul>

<h3>Title: Rotation-Robust Regression with Convolutional Model Trees</h3>
<ul>
<li><strong>Authors: </strong>Hongyi Li, William Ward Armstrong, Jun Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04899">https://arxiv.org/abs/2601.04899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04899">https://arxiv.org/pdf/2601.04899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04899]] Rotation-Robust Regression with Convolutional Model Trees(https://arxiv.org/abs/2601.04899)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We study rotation-robust learning for image inputs using Convolutional Model Trees (CMTs) [1], whose split and leaf coefficients can be structured on the image grid and transformed geometrically at deployment time. In a controlled MNIST setting with a rotation-invariant regression target, we introduce three geometry-aware inductive biases for split directions -- convolutional smoothing, a tilt dominance constraint, and importance-based pruning -- and quantify their impact on robustness under in-plane rotations. We further evaluate a deployment-time orientation search that selects a discrete rotation maximizing a forest-level confidence proxy without updating model parameters. Orientation search improves robustness under severe rotations but can be harmful near the canonical orientation when confidence is misaligned with correctness. Finally, we observe consistent trends on MNIST digit recognition implemented as one-vs-rest regression, highlighting both the promise and limitations of confidence-based orientation selection for model-tree ensembles.</li>
</ul>

<h3>Title: Decentralized Privacy-Preserving Federal Learning of Computer Vision Models on Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Damian Harenčák, Lukáš Gajdošech, Martin Madaras</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04912">https://arxiv.org/abs/2601.04912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04912">https://arxiv.org/pdf/2601.04912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04912]] Decentralized Privacy-Preserving Federal Learning of Computer Vision Models on Edge Devices(https://arxiv.org/abs/2601.04912)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, segmentation</a></li>
<li><strong>Abstract: </strong>Collaborative training of a machine learning model comes with a risk of sharing sensitive or private data. Federated learning offers a way of collectively training a single global model without the need to share client data, by sharing only the updated parameters from each client's local model. A central server is then used to aggregate parameters from all clients and redistribute the aggregated model back to the clients. Recent findings have shown that even in this scenario, private data can be reconstructed only using information about model parameters. Current efforts to mitigate this are mainly focused on reducing privacy risks on the server side, assuming that other clients will not act maliciously. In this work, we analyzed various methods for improving the privacy of client data concerning both the server and other clients for neural networks. Some of these methods include homomorphic encryption, gradient compression, gradient noising, and discussion on possible usage of modified federated learning systems such as split learning, swarm learning or fully encrypted models. We have analyzed the negative effects of gradient compression and gradient noising on the accuracy of convolutional neural networks used for classification. We have shown the difficulty of data reconstruction in the case of segmentation networks. We have also implemented a proof of concept on the NVIDIA Jetson TX2 module used in edge devices and simulated a federated learning process.</li>
</ul>

<h3>Title: Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences</h3>
<ul>
<li><strong>Authors: </strong>Arkadiusz Modzelewski, Paweł Golik, Anna Kołos, Giovanni Da San Martino</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04925">https://arxiv.org/abs/2601.04925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04925">https://arxiv.org/pdf/2601.04925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04925]] Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences(https://arxiv.org/abs/2601.04925)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools.</li>
</ul>

<h3>Title: GenProve: Learning to Generate Text with Fine-Grained Provenance</h3>
<ul>
<li><strong>Authors: </strong>Jingxuan Wei, Xingyue Wang, Yanghaoyu Liao, Jie Dong, Yuchen Liu, Caijun Jia, Bihui Yu, Junnan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04932">https://arxiv.org/abs/2601.04932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04932">https://arxiv.org/pdf/2601.04932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04932]] GenProve: Learning to Generate Text with Fine-Grained Provenance(https://arxiv.org/abs/2601.04932)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation.</li>
</ul>

<h3>Title: CurricuLLM: Designing Personalized and Workforce-Aligned Cybersecurity Curricula Using Fine-Tuned LLMs</h3>
<ul>
<li><strong>Authors: </strong>Arthur Nijdam, Harri Kähkönen, Valtteri Niemi, Paul Stankovski Wagner, Sara Ramezanian</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04940">https://arxiv.org/abs/2601.04940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04940">https://arxiv.org/pdf/2601.04940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04940]] CurricuLLM: Designing Personalized and Workforce-Aligned Cybersecurity Curricula Using Fine-Tuned LLMs(https://arxiv.org/abs/2601.04940)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, transformer, large language model</a></li>
<li><strong>Abstract: </strong>The cybersecurity landscape is constantly evolving, driven by increased digitalization and new cybersecurity threats. Cybersecurity programs often fail to equip graduates with skills demanded by the workforce, particularly concerning recent developments in cybersecurity, as curriculum design is costly and labor-intensive. To address this misalignment, we present a novel Large Language Model (LLM)-based framework for automated design and analysis of cybersecurity curricula, called CurricuLLM. Our approach provides three key contributions: (1) automation of personalized curriculum design, (2) a data-driven pipeline aligned with industry demands, and (3) a comprehensive methodology for leveraging fine-tuned LLMs in curriculum development. CurricuLLM utilizes a two-tier approach consisting of PreprocessLM, which standardizes input data, and ClassifyLM, which assigns course content to nine Knowledge Areas in cybersecurity. We systematically evalu- ated multiple Natural Language Processing (NLP) architectures and fine-tuning strategies, ultimately selecting the Bidirectional Encoder Representations from Transformers (BERT) model as ClassifyLM, fine-tuned on founda- tional cybersecurity concepts and workforce competencies. We are the first to validate our method with human experts who analyzed real-world cybersecurity curricula and frameworks, motivating that CurricuLLM is an efficient solution to replace labor-intensive curriculum analysis. Moreover, once course content has been classified, it can be integrated with established cybersecurity role-based weights, enabling alignment of the educational program with specific job roles, workforce categories, or general market needs. This lays the foundation for personalized, workforce-aligned cybersecurity curricula that prepare students for the evolving demands in cybersecurity.</li>
</ul>

<h3>Title: Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics</h3>
<ul>
<li><strong>Authors: </strong>Subhadeep Roy, Gagan Bhatia, Steffen Eger</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04946">https://arxiv.org/abs/2601.04946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04946">https://arxiv.org/pdf/2601.04946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04946]] Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics(https://arxiv.org/abs/2601.04946)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \textsc{\textbf{ProtoBias}} (\textit{\textbf{Proto}typical \textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \textbf{\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges.</li>
</ul>

<h3>Title: Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following</h3>
<ul>
<li><strong>Authors: </strong>Yirong Zeng, Yufei Liu, Xiao Ding, Yutai Hou, Yuxian Wang, Haonan Song, Wu Ning, Dandan Tu, Qixun Zhang, Bibo Cai, Yuxiang He, Ting Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04954">https://arxiv.org/abs/2601.04954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04954">https://arxiv.org/pdf/2601.04954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04954]] Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following(https://arxiv.org/abs/2601.04954)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A central belief in scaling reinforcement learning with verifiable rewards for instruction following (IF) tasks is that, a diverse mixture of verifiable hard and unverifiable soft constraints is essential for generalizing to unseen instructions. In this work, we challenge this prevailing consensus through a systematic empirical investigation. Counter-intuitively, we find that models trained on hard-only constraints consistently outperform those trained on mixed datasets. Extensive experiments reveal that reward precision, rather than constraint diversity, is the primary driver of effective alignment. The LLM judge suffers from a low recall rate in detecting false response, which leads to severe reward hacking, thereby undermining the benefits of diversity. Furthermore, analysis of the attention mechanism reveals that high-precision rewards develop a transferable meta-skill for IF. Motivated by these insights, we propose a simple yet effective data-centric refinement strategy that prioritizes reward precision. Evaluated on five benchmarks, our approach outperforms competitive baselines by 13.4\% in performance while achieving a 58\% reduction in training time, maintaining strong generalization beyond instruction following. Our findings advocate for a paradigm shift: moving away from the indiscriminate pursuit of data diversity toward high-precision rewards.</li>
</ul>

<h3>Title: TEA: Temporal Adaptive Satellite Image Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Juyuan Kang, Hao Zhu, Yan Zhu, Wei Zhang, Jianing Chen, Tianxiang Xiao, Yike Ma, Hao Jiang, Feng Dai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04956">https://arxiv.org/abs/2601.04956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04956">https://arxiv.org/pdf/2601.04956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04956]] TEA: Temporal Adaptive Satellite Image Semantic Segmentation(https://arxiv.org/abs/2601.04956)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Crop mapping based on satellite images time-series (SITS) holds substantial economic value in agricultural production settings, in which parcel segmentation is an essential step. Existing approaches have achieved notable advancements in SITS segmentation with predetermined sequence lengths. However, we found that these approaches overlooked the generalization capability of models across scenarios with varying temporal length, leading to markedly poor segmentation results in such cases. To address this issue, we propose TEA, a TEmporal Adaptive SITS semantic segmentation method to enhance the model's resilience under varying sequence lengths. We introduce a teacher model that encapsulates the global sequence knowledge to guide a student model with adaptive temporal input lengths. Specifically, teacher shapes the student's feature space via intermediate embedding, prototypes and soft label perspectives to realize knowledge transfer, while dynamically aggregating student model to mitigate knowledge forgetting. Finally, we introduce full-sequence reconstruction as an auxiliary task to further enhance the quality of representations across inputs of varying temporal lengths. Through extensive experiments, we demonstrate that our method brings remarkable improvements across inputs of different temporal lengths on common benchmarks. Our code will be publicly available.</li>
</ul>

<h3>Title: Text as a Universal Interface for Transferable Personalization</h3>
<ul>
<li><strong>Authors: </strong>Yuting Liu, Jian Guan, Jia-Nan Li, Wei Wu, Jiang-Ming Yang, Jianzhe Zhao, Guibing Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04963">https://arxiv.org/abs/2601.04963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04963">https://arxiv.org/pdf/2601.04963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04963]] Text as a Universal Interface for Transferable Personalization(https://arxiv.org/abs/2601.04963)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.</li>
</ul>

<h3>Title: SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Pittner, Joel Janai, Mario Faigle, Alexandru Paul Condurache</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04968">https://arxiv.org/abs/2601.04968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04968">https://arxiv.org/pdf/2601.04968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04968]] SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection(https://arxiv.org/abs/2601.04968)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.</li>
</ul>

<h3>Title: OceanSplat: Object-aware Gaussian Splatting with Trinocular View Consistency for Underwater Scene Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Minseong Kweon, Jinsun Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04984">https://arxiv.org/abs/2601.04984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04984">https://arxiv.org/pdf/2601.04984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04984]] OceanSplat: Object-aware Gaussian Splatting with Trinocular View Consistency for Underwater Scene Reconstruction(https://arxiv.org/abs/2601.04984)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce OceanSplat, a novel 3D Gaussian Splatting-based approach for accurately representing 3D geometry in underwater scenes. To overcome multi-view inconsistencies caused by underwater optical degradation, our method enforces trinocular view consistency by rendering horizontally and vertically translated camera views relative to each input view and aligning them via inverse warping. Furthermore, these translated camera views are used to derive a synthetic epipolar depth prior through triangulation, which serves as a self-supervised depth regularizer. These geometric constraints facilitate the spatial optimization of 3D Gaussians and preserve scene structure in underwater environments. We also propose a depth-aware alpha adjustment that modulates the opacity of 3D Gaussians during early training based on their $z$-component and viewing direction, deterring the formation of medium-induced primitives. With our contributions, 3D Gaussians are disentangled from the scattering medium, enabling robust representation of object geometry and significantly reducing floating artifacts in reconstructed underwater scenes. Experiments on real-world underwater and simulated scenes demonstrate that OceanSplat substantially outperforms existing methods for both scene reconstruction and restoration in scattering media.</li>
</ul>

<h3>Title: Higher-Order Adversarial Patches for Real-Time Object Detectors</h3>
<ul>
<li><strong>Authors: </strong>Jens Bayer, Stefan Becker, David Münch, Michael Arens, Jürgen Beyerer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04991">https://arxiv.org/abs/2601.04991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04991">https://arxiv.org/pdf/2601.04991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04991]] Higher-Order Adversarial Patches for Real-Time Object Detectors(https://arxiv.org/abs/2601.04991)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Higher-order adversarial attacks can directly be considered the result of a cat-and-mouse game -- an elaborate action involving constant pursuit, near captures, and repeated escapes. This idiom describes the enduring circular training of adversarial attack patterns and adversarial training the best. The following work investigates the impact of higher-order adversarial attacks on object detectors by successively training attack patterns and hardening object detectors with adversarial training. The YOLOv10 object detector is chosen as a representative, and adversarial patches are used in an evasion attack manner. Our results indicate that higher-order adversarial patches are not only affecting the object detector directly trained on but rather provide a stronger generalization capacity compared to lower-order adversarial patches. Moreover, the results highlight that solely adversarial training is not sufficient to harden an object detector efficiently against this kind of adversarial attack. Code: this https URL</li>
</ul>

<h3>Title: Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Xueyun Tian (1 and 2), Minghua Ma (3), Bingbing Xu (1 and 4), Nuoyan Lyu (1 and 2), Wei Li, Heng Dong (4), Zheng Chu (3), Yuanzhuo Wang (1), Huawei Shen (1 and 2) ((1) CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS, Beijing, China, (2) University of Chinese Academy of Sciences, Beijing, China (3) Harbin Institute of Technology, Harbin, China, (4) Tsinghua University, Beijing, China)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04992">https://arxiv.org/abs/2601.04992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04992">https://arxiv.org/pdf/2601.04992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04992]] Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization(https://arxiv.org/abs/2601.04992)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.</li>
</ul>

<h3>Title: On the Hidden Objective Biases of Group-based Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Aleksandar Fontana, Marco Simoni, Giulio Rossolini, Andrea Saracino, Paolo Mori</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05002">https://arxiv.org/abs/2601.05002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05002">https://arxiv.org/pdf/2601.05002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05002]] On the Hidden Objective Biases of Group-based Reinforcement Learning(https://arxiv.org/abs/2601.05002)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Group-based reinforcement learning methods, like Group Relative Policy Optimization (GRPO), are widely used nowadays to post-train large language models. Despite their empirical success, they exhibit structural mismatches between reward optimization and the underlying training objective. In this paper, we present a theoretical analysis of GRPO style methods by studying them within a unified surrogate formulation. This perspective reveals recurring properties that affect all the methods under analysis: (i) non-uniform group weighting induces systematic gradient biases on shared prefix tokens; (ii) interactions with the AdamW optimizer make training dynamics largely insensitive to reward scaling; and (iii) optimizer momentum can push policy updates beyond the intended clipping region under repeated optimization steps. We believe that these findings highlight fundamental limitations of current approaches and provide principled guidance for the design of future formulations.</li>
</ul>

<h3>Title: Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei</h3>
<ul>
<li><strong>Authors: </strong>Peng Wang, Xilin Tao, Siyi Yao, Jiageng Wu, Yuntao Zou, Zhuotao Tian, Libo Qin, Dagang Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05004">https://arxiv.org/abs/2601.05004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05004">https://arxiv.org/pdf/2601.05004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05004]] Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei(https://arxiv.org/abs/2601.05004)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers.</li>
</ul>

<h3>Title: HMVI: Unifying Heterogeneous Attributes with Natural Neighbors for Missing Value Inference</h3>
<ul>
<li><strong>Authors: </strong>Xiaopeng Luo, Zexi Tan, Zhuowei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05017">https://arxiv.org/abs/2601.05017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05017">https://arxiv.org/pdf/2601.05017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05017]] HMVI: Unifying Heterogeneous Attributes with Natural Neighbors for Missing Value Inference(https://arxiv.org/abs/2601.05017)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Missing value imputation is a fundamental challenge in machine intelligence, heavily dependent on data completeness. Current imputation methods often handle numerical and categorical attributes independently, overlooking critical interdependencies among heterogeneous features. To address these limitations, we propose a novel imputation approach that explicitly models cross-type feature dependencies within a unified framework. Our method leverages both complete and incomplete instances to ensure accurate and consistent imputation in tabular data. Extensive experimental results demonstrate that the proposed approach achieves superior performance over existing techniques and significantly enhances downstream machine learning tasks, providing a robust solution for real-world systems with missing data.</li>
</ul>

<h3>Title: Hán Dān Xué Bù (Mimicry) or Qīng Chū Yú Lán (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yueqing Hu, Xinyang Peng, Shuting Peng, Hanqi Wang, Tianhong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05019">https://arxiv.org/abs/2601.05019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05019">https://arxiv.org/pdf/2601.05019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05019]] Hán Dān Xué Bù (Mimicry) or Qīng Chū Yú Lán (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models(https://arxiv.org/abs/2601.05019)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent Large Reasoning Models trained via reinforcement learning exhibit a "natural" alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation -- training student models to mimic these traces via Supervised Fine-Tuning (SFT) -- fails to transmit this cognitive structure. Testing the "Hán Dān Xué Bù" (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a "Functional Alignment Collapse": while teacher models mirror human difficulty scaling ($\bar{r}=0.64$), distilled students significantly degrade this alignment ($\bar{r}=0.34$), often underperforming their own pre-distillation baselines ("Negative Transfer"). Our analysis suggests that SFT induces a "Cargo Cult" effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher's dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.</li>
</ul>

<h3>Title: Knowledge-to-Data: LLM-Driven Synthesis of Structured Network Traffic for Testbed-Free IDS Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos E. Kampourakis, Vyron Kampourakis, Efstratios Chatzoglou, Georgios Kambourakis, Stefanos Gritzalis</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05022">https://arxiv.org/abs/2601.05022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05022">https://arxiv.org/pdf/2601.05022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05022]] Knowledge-to-Data: LLM-Driven Synthesis of Structured Network Traffic for Testbed-Free IDS Evaluation(https://arxiv.org/abs/2601.05022)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, large language model</a></li>
<li><strong>Abstract: </strong>Realistic, large-scale, and well-labeled cybersecurity datasets are essential for training and evaluating Intrusion Detection Systems (IDS). However, they remain difficult to obtain due to privacy constraints, data sensitivity, and the cost of building controlled collection environments such as testbeds and cyber ranges. This paper investigates whether Large Language Models (LLMs) can operate as controlled knowledge-to-data engines for generating structured synthetic network traffic datasets suitable for IDS research. We propose a methodology that combines protocol documentation, attack semantics, and explicit statistical rules to condition LLMs without fine-tuning or access to raw samples. Using the AWID3 IEEE~802.11 benchmark as a demanding case study, we generate labeled datasets with four state-of-the-art LLMs and assess fidelity through a multi-level validation framework including global similarity metrics, per-feature distribution testing, structural comparison, and cross-domain classification. Results show that, under explicit constraints, LLM-generated datasets can closely approximate the statistical and structural characteristics of real network traffic, enabling gradient-boosting classifiers to achieve F1-scores up to 0.956 when evaluated on real samples. Overall, the findings suggest that constrained LLM-driven generation can facilitate on-demand IDS experimentation, providing a testbed-free, privacy-preserving alternative that overcomes the traditional bottlenecks of physical traffic collection and manual labeling.</li>
</ul>

<h3>Title: DeepWeightFlow: Re-Basined Flow Matching for Generating Neural Network Weights</h3>
<ul>
<li><strong>Authors: </strong>Saumya Gupta, Scott Biggs, Moritz Laber, Zohair Shafi, Robin Walters, Ayan Paul</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05052">https://arxiv.org/abs/2601.05052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05052">https://arxiv.org/pdf/2601.05052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05052]] DeepWeightFlow: Re-Basined Flow Matching for Generating Neural Network Weights(https://arxiv.org/abs/2601.05052)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Building efficient and effective generative models for neural network weights has been a research focus of significant interest that faces challenges posed by the high-dimensional weight spaces of modern neural networks and their symmetries. Several prior generative models are limited to generating partial neural network weights, particularly for larger models, such as ResNet and ViT. Those that do generate complete weights struggle with generation speed or require finetuning of the generated models. In this work, we present DeepWeightFlow, a Flow Matching model that operates directly in weight space to generate diverse and high-accuracy neural network weights for a variety of architectures, neural network sizes, and data modalities. The neural networks generated by DeepWeightFlow do not require fine-tuning to perform well and can scale to large networks. We apply Git Re-Basin and TransFusion for neural network canonicalization in the context of generative weight models to account for the impact of neural network permutation symmetries and to improve generation efficiency for larger model sizes. The generated networks excel at transfer learning, and ensembles of hundreds of neural networks can be generated in minutes, far exceeding the efficiency of diffusion-based methods. DeepWeightFlow models pave the way for more efficient and scalable generation of diverse sets of neural networks.</li>
</ul>

<h3>Title: Supporting Secured Integration of Microarchitectural Defenses</h3>
<ul>
<li><strong>Authors: </strong>Kartik Ramkrishnan, Stephen McCamant, Antonia Zhai, Pen-Chung Yew</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05057">https://arxiv.org/abs/2601.05057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05057">https://arxiv.org/pdf/2601.05057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05057]] Supporting Secured Integration of Microarchitectural Defenses(https://arxiv.org/abs/2601.05057)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack</a></li>
<li><strong>Abstract: </strong>There has been a plethora of microarchitectural-level attacks leading to many proposed countermeasures. This has created an unexpected and unaddressed security issue where naive integration of those defenses can potentially lead to security vulnerabilities. This occurs when one defense changes an aspect of a microarchitecture that is crucial for the security of another defense. We refer to this problem as a microarchitectural defense assumption violation} (MDAV). We propose a two-step methodology to screen for potential MDAVs in the early-stage of integration. The first step is to design and integrate a composed model, guided by bounded model checking of security properties. The second step is to implement the model concretely on a simulator and to evaluate with simulated attacks. As a contribution supporting the first step, we propose an event-based modeling framework, called Maestro, for testing and evaluating microarchitectural models with integrated defenses. In our evaluation, Maestro reveals MDAVs (8), supports compact expression (~15x Alloy LoC ratio), enables semantic composability and eliminates performance degradations (>100x). As a contribution supporting the second step, we use an event-based simulator (GEM5) for investigating integrated microarchitectural defenses. We show that a covert channel attack is possible on a naively integrated implementation of some state-of-the-art defenses, and a repaired implementation using our integration methodology is resilient to the attack.</li>
</ul>

<h3>Title: Compositional Steering of Large Language Models with Steering Tokens</h3>
<ul>
<li><strong>Authors: </strong>Gorjan Radevski, Kiril Gashteovski, Giwon Hong, Carolin Lawrence, Goran Glavaš</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05062">https://arxiv.org/abs/2601.05062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05062">https://arxiv.org/pdf/2601.05062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05062]] Compositional Steering of Large Language Models with Steering Tokens(https://arxiv.org/abs/2601.05062)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.</li>
</ul>

<h3>Title: Milestones over Outcome: Unlocking Geometric Reasoning with Sub-Goal Verifiable Reward</h3>
<ul>
<li><strong>Authors: </strong>Jianlong Chen, Daocheng Fu, Shengze Xu, Jiawei Chen, Yuan Feng, Yue Yang, Junchi Yan, Hongyuan Zha, Renqiu Xia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05073">https://arxiv.org/abs/2601.05073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05073">https://arxiv.org/pdf/2601.05073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05073]] Milestones over Outcome: Unlocking Geometric Reasoning with Sub-Goal Verifiable Reward(https://arxiv.org/abs/2601.05073)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) struggle with complex geometric reasoning, largely because "black box" outcome-based supervision fails to distinguish between lucky guesses and rigorous deduction. To address this, we introduce a paradigm shift towards subgoal-level evaluation and learning. We first construct GeoGoal, a benchmark synthesized via a rigorous formal verification data engine, which converts abstract proofs into verifiable numeric subgoals. This structure reveals a critical divergence between reasoning quality and outcome accuracy. Leveraging this, we propose the Sub-Goal Verifiable Reward (SGVR) framework, which replaces sparse signals with dense rewards based on the Skeleton Rate. Experiments demonstrate that SGVR not only enhances geometric performance (+9.7%) but also exhibits strong generalization, transferring gains to general math (+8.0%) and other general reasoning tasks (+2.8%), demonstrating broad applicability across diverse domains.</li>
</ul>

<h3>Title: SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Chen, Zhenxuan Huang, Yile Wang, Weiqin Wang, Lu Yin, Hui Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05075">https://arxiv.org/abs/2601.05075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05075">https://arxiv.org/pdf/2601.05075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05075]] SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment(https://arxiv.org/abs/2601.05075)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.</li>
</ul>

<h3>Title: Exploring Student Expectations and Confidence in Learning Analytics</h3>
<ul>
<li><strong>Authors: </strong>Hayk Asatryan, Basile Tousside, Janis Mohr, Malte Neugebauer, Hildo Bijl, Paul Spiegelberg, Claudia Frohn-Schauf, Jörg Frochte</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05082">https://arxiv.org/abs/2601.05082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05082">https://arxiv.org/pdf/2601.05082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05082]] Exploring Student Expectations and Confidence in Learning Analytics(https://arxiv.org/abs/2601.05082)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Learning Analytics (LA) is nowadays ubiquitous in many educational systems, providing the ability to collect and analyze student data in order to understand and optimize learning and the environments in which it occurs. On the other hand, the collection of data requires to comply with the growing demand regarding privacy legislation. In this paper, we use the Student Expectation of Learning Analytics Questionnaire (SELAQ) to analyze the expectations and confidence of students from different faculties regarding the processing of their data for Learning Analytics purposes. This allows us to identify four clusters of students through clustering algorithms: Enthusiasts, Realists, Cautious and Indifferents. This structured analysis provides valuable insights into the acceptance and criticism of Learning Analytics among students.</li>
</ul>

<h3>Title: Driving on Registers</h3>
<ul>
<li><strong>Authors: </strong>Ellington Kirby, Alexandre Boulch, Yihong Xu, Yuan Yin, Gilles Puy, Éloi Zablocki, Andrei Bursuc, Spyros Gidaris, Renaud Marlet, Florent Bartoccioni, Anh-Quan Cao, Nermin Samet, Tuan-Hung VU, Matthieu Cord</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05083">https://arxiv.org/abs/2601.05083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05083">https://arxiv.org/pdf/2601.05083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05083]] Driving on Registers(https://arxiv.org/abs/2601.05083)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving. Our approach builds on pretrained Vision Transformers (ViTs) and introduces camera-aware register tokens that compress multi-camera features into a compact scene representation, significantly reducing downstream computation without sacrificing accuracy. These tokens drive two lightweight transformer decoders that generate and then score candidate trajectories. The scoring decoder learns to mimic an oracle and predicts interpretable sub-scores representing aspects such as safety, comfort, and efficiency, enabling behavior-conditioned driving at inference. Despite its minimal design, DrivoR outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and the photorealistic closed-loop HUGSIM benchmark. Our results show that a pure-transformer architecture, combined with targeted token compression, is sufficient for accurate, efficient, and adaptive end-to-end driving. Code and checkpoints will be made available via the project page.</li>
</ul>

<h3>Title: How Human is AI? Examining the Impact of Emotional Prompts on Artificial and Human and Responsiveness</h3>
<ul>
<li><strong>Authors: </strong>Florence Bernays, Marco Henriques Pereira, Jochen Menges (University of Zurich)</a></li>
<li><strong>Subjects: </strong>cs.CL, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05104">https://arxiv.org/abs/2601.05104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05104">https://arxiv.org/pdf/2601.05104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05104]] How Human is AI? Examining the Impact of Emotional Prompts on Artificial and Human and Responsiveness(https://arxiv.org/abs/2601.05104)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>This research examines how the emotional tone of human-AI interactions shapes ChatGPT and human behavior. In a between-subject experiment, we asked participants to express a specific emotion while working with ChatGPT (GPT-4.0) on two tasks, including writing a public response and addressing an ethical dilemma. We found that compared to interactions where participants maintained a neutral tone, ChatGPT showed greater improvement in its answers when participants praised ChatGPT for its responses. Expressing anger towards ChatGPT also led to a higher albeit smaller improvement relative to the neutral condition, whereas blaming ChatGPT did not improve its answers. When addressing an ethical dilemma, ChatGPT prioritized corporate interests less when participants expressed anger towards it, while blaming increases its emphasis on protecting the public interest. Additionally, we found that people used more negative, hostile, and disappointing expressions in human-human communication after interactions during which participants blamed rather than praised for their responses. Together, our findings demonstrate that the emotional tone people apply in human-AI interactions not only shape ChatGPT's outputs but also carry over into subsequent human-human communication.</li>
</ul>

<h3>Title: UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Filippo Ghilotti, Samuel Brucker, Nahku Saidy, Matteo Matteucci, Mario Bijelic, Felix Heide</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05105">https://arxiv.org/abs/2601.05105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05105">https://arxiv.org/pdf/2601.05105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05105]] UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition(https://arxiv.org/abs/2601.05105)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research. In this work we tackle this bottleneck by leveraging temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models directly into 3D, without any manual input. We introduce an unsupervised multi-modal pseudo-labeling method relying on strong geometric priors learned from temporally accumulated LiDAR maps, alongside with a novel iterative update rule that enforces joint geometric-semantic consistency, and vice-versa detecting moving objects from inconsistencies. Our method simultaneously produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans, demonstrating robust generalization across three datasets. We experimentally validate that our method compares favorably to existing semantic segmentation and object detection pseudo-labeling methods, which often require additional manual supervision. We confirm that even a small fraction of our geometrically consistent, densified LiDAR improves depth prediction by 51.5% and 22.0% MAE in the 80-150 and 150-250 meters range, respectively.</li>
</ul>

<h3>Title: Agent-as-a-Judge</h3>
<ul>
<li><strong>Authors: </strong>Runyang You, Hongru Cai, Caiqi Zhang, Qiancheng Xu, Meng Liu, Tiezheng Yu, Yongqi Li, Wenjie Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05111">https://arxiv.org/abs/2601.05111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05111">https://arxiv.org/pdf/2601.05111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05111]] Agent-as-a-Judge(https://arxiv.org/abs/2601.05111)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.</li>
</ul>

<h3>Title: From Rays to Projections: Better Inputs for Feed-Forward View Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Zirui Wu, Zeren Jiang, Martin R. Oswald, Jie Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05116">https://arxiv.org/abs/2601.05116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05116">https://arxiv.org/pdf/2601.05116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05116]] From Rays to Projections: Better Inputs for Feed-Forward View Synthesis(https://arxiv.org/abs/2601.05116)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.</li>
</ul>

<h3>Title: Sequential Subspace Noise Injection Prevents Accuracy Collapse in Certified Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Polina Dolgova, Sebastian U. Stich</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05134">https://arxiv.org/abs/2601.05134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05134">https://arxiv.org/pdf/2601.05134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05134]] Sequential Subspace Noise Injection Prevents Accuracy Collapse in Certified Unlearning(https://arxiv.org/abs/2601.05134)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, membership infer</a></li>
<li><strong>Abstract: </strong>Certified unlearning based on differential privacy offers strong guarantees but remains largely impractical: the noisy fine-tuning approaches proposed so far achieve these guarantees but severely reduce model accuracy. We propose sequential noise scheduling, which distributes the noise budget across orthogonal subspaces of the parameter space, rather than injecting it all at once. This simple modification mitigates the destructive effect of noise while preserving the original certification guarantees. We extend the analysis of noisy fine-tuning to the subspace setting, proving that the same $(\varepsilon,\delta)$ privacy budget is retained. Empirical results on image classification benchmarks show that our approach substantially improves accuracy after unlearning while remaining robust to membership inference attacks. These results show that certified unlearning can achieve both rigorous guarantees and practical utility.</li>
</ul>

<h3>Title: VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control</h3>
<ul>
<li><strong>Authors: </strong>Sixiao Zheng, Minghao Yin, Wenbo Hu, Xiaoyu Li, Ying Shan, Yanwei Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05138">https://arxiv.org/abs/2601.05138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05138">https://arxiv.org/pdf/2601.05138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05138]] VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control(https://arxiv.org/abs/2601.05138)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.</li>
</ul>

<h3>Title: A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Rakibul Islam, Md. Siam Ansary</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05143">https://arxiv.org/abs/2601.05143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05143">https://arxiv.org/pdf/2601.05143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05143]] A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering(https://arxiv.org/abs/2601.05143)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability, transformer</a></li>
<li><strong>Abstract: </strong>Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.</li>
</ul>

<h3>Title: Atlas 2 - Foundation models for clinical deployment</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Alber, Timo Milbich, Alexandra Carpen-Amarie, Stephan Tietz, Jonas Dippel, Lukas Muttenthaler, Beatriz Perez Cancer, Alessandro Benetti, Panos Korfiatis, Elias Eulig, Jérôme Lüscher, Jiasen Wu, Sayed Abid Hashimi, Gabriel Dernbach, Simon Schallenberg, Neelay Shah, Moritz Krügener, Aniruddh Jammoria, Jake Matras, Patrick Duffy, Matt Redlon, Philipp Jurmeister, David Horst, Lukas Ruff, Klaus-Robert Müller, Frederick Klauschen, Andrew Norgan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05148">https://arxiv.org/abs/2601.05148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05148">https://arxiv.org/pdf/2601.05148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05148]] Atlas 2 - Foundation models for clinical deployment(https://arxiv.org/abs/2601.05148)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Pathology foundation models substantially advanced the possibilities in computational pathology -- yet tradeoffs in terms of performance, robustness, and computational requirements remained, which limited their clinical deployment. In this report, we present Atlas 2, Atlas 2-B, and Atlas 2-S, three pathology vision foundation models which bridge these shortcomings by showing state-of-the-art performance in prediction performance, robustness, and resource efficiency in a comprehensive evaluation across eighty public benchmarks. Our models were trained on the largest pathology foundation model dataset to date comprising 5.5 million histopathology whole slide images, collected from three medical institutions Charité - Universtätsmedizin Berlin, LMU Munich, and Mayo Clinic.</li>
</ul>

<h3>Title: $PC^2$: Politically Controversial Content Generation via Jailbreaking Attacks on GPT-based Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Wonwoo Choi, Minjae Seo, Minkyoo Song, Hwanjo Heo, Seungwon Shin, Myoungsung You</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05150">https://arxiv.org/abs/2601.05150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05150">https://arxiv.org/pdf/2601.05150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05150]] $PC^2$: Politically Controversial Content Generation via Jailbreaking Attacks on GPT-based Text-to-Image Models(https://arxiv.org/abs/2601.05150)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>The rapid evolution of text-to-image (T2I) models has enabled high-fidelity visual synthesis on a global scale. However, these advancements have introduced significant security risks, particularly regarding the generation of harmful content. Politically harmful content, such as fabricated depictions of public figures, poses severe threats when weaponized for fake news or propaganda. Despite its criticality, the robustness of current T2I safety filters against such politically motivated adversarial prompting remains underexplored. In response, we propose $PC^2$, the first black-box political jailbreaking framework for T2I models. It exploits a novel vulnerability where safety filters evaluate political sensitivity based on linguistic context. $PC^2$ operates through: (1) Identity-Preserving Descriptive Mapping to obfuscate sensitive keywords into neutral descriptions, and (2) Geopolitically Distal Translation to map these descriptions into fragmented, low-sensitivity languages. This strategy prevents filters from constructing toxic relationships between political entities within prompts, effectively bypassing detection. We construct a benchmark of 240 politically sensitive prompts involving 36 public figures. Evaluation on commercial T2I models, specifically GPT-series, shows that while all original prompts are blocked, $PC^2$ achieves attack success rates of up to 86%.</li>
</ul>

<h3>Title: Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering</h3>
<ul>
<li><strong>Authors: </strong>Shuliang Liu, Songbo Yang, Dong Fang, Sihang Jia, Yuqi Tang, Lingfeng Su, Ruoshui Peng, Yibo Yan, Xin Zou, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05159">https://arxiv.org/abs/2601.05159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05159">https://arxiv.org/pdf/2601.05159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05159]] Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering(https://arxiv.org/abs/2601.05159)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.</li>
</ul>

<h3>Title: RelayLLM: Efficient Reasoning via Collaborative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Chengsong Huang, Tong Zheng, Langlin Huang, Jinyuan Li, Haolin Liu, Jiaxin Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05167">https://arxiv.org/abs/2601.05167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05167">https://arxiv.org/pdf/2601.05167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05167]] RelayLLM: Efficient Reasoning via Collaborative Decoding(https://arxiv.org/abs/2601.05167)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively "relaying" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.</li>
</ul>

<h3>Title: VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice</h3>
<ul>
<li><strong>Authors: </strong>Shuming Liu, Mingchen Zhuge, Changsheng Zhao, Jun Chen, Lemeng Wu, Zechun Liu, Chenchen Zhu, Zhipeng Cai, Chong Zhou, Haozhe Liu, Ernie Chang, Saksham Suri, Hongyu Xu, Qi Qian, Wei Wen, Balakrishnan Varadarajan, Zhuang Liu, Hu Xu, Florian Bordes, Raghuraman Krishnamoorthi, Bernard Ghanem, Vikas Chandra, Yunyang Xiong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05175">https://arxiv.org/abs/2601.05175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05175">https://arxiv.org/pdf/2601.05175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05175]] VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice(https://arxiv.org/abs/2601.05175)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.</li>
</ul>

<h3>Title: The Adverse Effects of Omitting Records in Differential Privacy: How Sampling and Suppression Degrade the Privacy--Utility Tradeoff (Long Version)</h3>
<ul>
<li><strong>Authors: </strong>Àlex Miranda-Pascual, Javier Parra-Arnau, Thorsten Strufe</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05180">https://arxiv.org/abs/2601.05180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05180">https://arxiv.org/pdf/2601.05180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05180]] The Adverse Effects of Omitting Records in Differential Privacy: How Sampling and Suppression Degrade the Privacy--Utility Tradeoff (Long Version)(https://arxiv.org/abs/2601.05180)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Sampling is renowned for its privacy amplification in differential privacy (DP), and is often assumed to improve the utility of a DP mechanism by allowing a noise reduction. In this paper, we further show that this last assumption is flawed: When measuring utility at equal privacy levels, sampling as preprocessing consistently yields penalties due to utility loss from omitting records over all canonical DP mechanisms -- Laplace, Gaussian, exponential, and report noisy max -- , as well as recent applications of sampling, such as clustering. Extending this analysis, we investigate suppression as a generalized method of choosing, or omitting, records. Developing a theoretical analysis of this technique, we derive privacy bounds for arbitrary suppression strategies under unbounded approximate DP. We find that our tested suppression strategy also fails to improve the privacy--utility tradeoff. Surprisingly, uniform sampling emerges as one of the best suppression methods -- despite its still degrading effect. Our results call into question common preprocessing assumptions in DP practice.</li>
</ul>

<h3>Title: Cutting AI Research Costs: How Task-Aware Compression Makes Large Language Model Agents Affordable</h3>
<ul>
<li><strong>Authors: </strong>Zuhair Ahmed Khan Taha, Mohammed Mudassir Uddin, Shahnawaz Alam</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05191">https://arxiv.org/abs/2601.05191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05191">https://arxiv.org/pdf/2601.05191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05191]] Cutting AI Research Costs: How Task-Aware Compression Makes Large Language Model Agents Affordable(https://arxiv.org/abs/2601.05191)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>When researchers deploy large language models for autonomous tasks like reviewing literature or generating hypotheses, the computational bills add up quickly. A single research session using a 70-billion parameter model can cost around $127 in cloud fees, putting these tools out of reach for many academic labs. We developed AgentCompress to tackle this problem head-on. The core idea came from a simple observation during our own work: writing a novel hypothesis clearly demands more from the model than reformatting a bibliography. Why should both tasks run at full precision? Our system uses a small neural network to gauge how hard each incoming task will be, based only on its opening words, then routes it to a suitably compressed model variant. The decision happens in under a millisecond. Testing across 500 research workflows in four scientific fields, we cut compute costs by 68.3% while keeping 96.2% of the original success rate. For labs watching their budgets, this could mean the difference between running experiments and sitting on the sidelines</li>
</ul>

<h3>Title: LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Samy Haffoudhi, Fabian M. Suchanek, Nils Holzenberger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05192">https://arxiv.org/abs/2601.05192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05192">https://arxiv.org/pdf/2601.05192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05192]] LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation(https://arxiv.org/abs/2601.05192)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.</li>
</ul>

<h3>Title: An interpretable data-driven approach to optimizing clinical fall risk assessment</h3>
<ul>
<li><strong>Authors: </strong>Fardin Ganjkhanloo, Emmett Springer, Erik H. Hoyer, Daniel L. Young, Holley Farley, Kimia Ghobadi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05194">https://arxiv.org/abs/2601.05194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05194">https://arxiv.org/pdf/2601.05194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05194]] An interpretable data-driven approach to optimizing clinical fall risk assessment(https://arxiv.org/abs/2601.05194)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, interpretability</a></li>
<li><strong>Abstract: </strong>In this study, we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective cohort analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters. To incorporate clinical knowledge and maintain interpretability, we employed constrained score optimization (CSO) models to reweight the JHFRAT scoring weights, while preserving its additive structure and clinical thresholds. Recalibration refers to adjusting item weights so that the resulting score can order encounters more consistently by the study's risk labels, and without changing the tool's form factor or deployment workflow. The model demonstrated significant improvements in predictive performance over the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). This performance improvement translates to protecting an additional 35 high-risk patients per week across the Johns Hopkins Health System. The constrained score optimization models performed similarly with and without the EHR variables. Although the benchmark black-box model (XGBoost), improves upon the performance metrics of the knowledge-based constrained logistic regression (AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk labeling. This evidence-based approach provides a robust foundation for health systems to systematically enhance inpatient fall prevention protocols and patient safety using data-driven optimization techniques, contributing to improved risk assessment and resource allocation in healthcare settings.</li>
</ul>

<h3>Title: FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Danilo Danese, Angela Lombardi, Matteo Attimonelli, Giuseppe Fasano, Tommaso Di Noia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05212">https://arxiv.org/abs/2601.05212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05212">https://arxiv.org/pdf/2601.05212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05212]] FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching(https://arxiv.org/abs/2601.05212)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual's biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.</li>
</ul>

<h3>Title: Measuring and Fostering Peace through Machine Learning and Artificial Intelligence</h3>
<ul>
<li><strong>Authors: </strong>P. Gilda (1), P. Dungarwal (1), A. Thongkham (1), E. T. Ajayi (2), S. Choudhary (1), T. M. Terol (1), C. Lam (1), J. P. Araujo (1), M. McFadyen-Mungalln (1), L. S. Liebovitch (1), P. T. Coleman (1), H. West (1), K. Sieck (3), S. Carter (3) ((1) Columbia University, (2) St John's University, (3) Toyota Research Institute)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05232">https://arxiv.org/abs/2601.05232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05232">https://arxiv.org/pdf/2601.05232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05232]] Measuring and Fostering Peace through Machine Learning and Artificial Intelligence(https://arxiv.org/abs/2601.05232)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.</li>
</ul>

<h3>Title: ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos</h3>
<ul>
<li><strong>Authors: </strong>Rustin Soraki, Homanga Bharadhwaj, Ali Farhadi, Roozbeh Mottaghi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05237">https://arxiv.org/abs/2601.05237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05237">https://arxiv.org/pdf/2601.05237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05237]] ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos(https://arxiv.org/abs/2601.05237)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Humans can effortlessly anticipate how objects might move or change through interaction--imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. this http URL</li>
</ul>

<h3>Title: Plenoptic Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiao Fu, Shitao Tang, Min Shi, Xian Liu, Jinwei Gu, Ming-Yu Liu, Dahua Lin, Chen-Hsuan Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05239">https://arxiv.org/abs/2601.05239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05239">https://arxiv.org/pdf/2601.05239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05239]] Plenoptic Video Generation(https://arxiv.org/abs/2601.05239)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: this https URL</li>
</ul>

<h3>Title: Robust Reasoning as a Symmetry-Protected Topological Phase</h3>
<ul>
<li><strong>Authors: </strong>Ilmo Sung</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn, cs.AI, hep-th</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05240">https://arxiv.org/abs/2601.05240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05240">https://arxiv.org/pdf/2601.05240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05240]] Robust Reasoning as a Symmetry-Protected Topological Phase(https://arxiv.org/abs/2601.05240)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models suffer from "hallucinations"-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a "Metric Phase," where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic "mass gap," maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\times$ beyond training ($L=50 \to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold.</li>
</ul>

<h3>Title: RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Boyang Wang, Haoran Zhang, Shujie Zhang, Jinkun Hao, Mingda Jia, Qi Lv, Yucheng Mao, Zhaoyang Lyu, Jia Zeng, Xudong Xu, Jiangmiao Pang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05241">https://arxiv.org/abs/2601.05241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05241">https://arxiv.org/pdf/2601.05241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05241]] RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation(https://arxiv.org/abs/2601.05241)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.</li>
</ul>

<h3>Title: GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation</h3>
<ul>
<li><strong>Authors: </strong>Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05244">https://arxiv.org/abs/2601.05244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05244">https://arxiv.org/pdf/2601.05244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05244]] GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation(https://arxiv.org/abs/2601.05244)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies. The proposed ReLA achieves the state-of-the-art results on the both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at this https URL.</li>
</ul>

<h3>Title: Pixel-Perfect Visual Geometry Estimation</h3>
<ul>
<li><strong>Authors: </strong>Gangwei Xu, Haotong Lin, Hongcheng Luo, Haiyang Sun, Bing Wang, Guang Chen, Sida Peng, Hangjun Ye, Xin Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05246">https://arxiv.org/abs/2601.05246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05246">https://arxiv.org/pdf/2601.05246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05246]] Pixel-Perfect Visual Geometry Estimation(https://arxiv.org/abs/2601.05246)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.</li>
</ul>

<h3>Title: Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video</h3>
<ul>
<li><strong>Authors: </strong>Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, Andrea Vedaldi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05251">https://arxiv.org/abs/2601.05251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05251">https://arxiv.org/pdf/2601.05251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05251]] Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video(https://arxiv.org/abs/2601.05251)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
