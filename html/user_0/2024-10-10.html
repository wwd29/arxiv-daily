<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-10</h1>
<h3>Title: HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jianke Zhang, Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, Jianyu Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05273">https://arxiv.org/abs/2410.05273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05273">https://arxiv.org/pdf/2410.05273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05273]] HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers(https://arxiv.org/abs/2410.05273)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Large Vision-Language-Action (VLA) models, leveraging powerful pre trained Vision-Language Models (VLMs) backends, have shown promise in robotic control due to their impressive generalization ability. However, the success comes at a cost. Their reliance on VLM backends with billions of parameters leads to high computational costs and inference latency, limiting the testing scenarios to mainly quasi-static tasks and hindering performance in dynamic tasks requiring rapid interactions. To address these limitations, this paper proposes HiRT, a Hierarchical Robot Transformer framework that enables flexible frequency and performance trade-off. HiRT keeps VLMs running at low frequencies to capture temporarily invariant features while enabling real-time interaction through a high-frequency vision-based policy guided by the slowly updated features. Experiment results in both simulation and real-world settings demonstrate significant improvements over baseline methods. Empirically, in static tasks, we double the control frequency and achieve comparable success rates. Additionally, on novel real-world dynamic ma nipulation tasks which are challenging for previous VLA models, HiRT improves the success rate from 48% to 75%.</li>
</ul>

<h3>Title: Psychometrics for Hypnopaedia-Aware Machinery via Chaotic Projection of Artificial Mental Imagery</h3>
<ul>
<li><strong>Authors: </strong>Ching-Chun Chang, Kai Gao, Shuying Xu, Anastasia Kordoni, Christopher Leckie, Isao Echizen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05284">https://arxiv.org/abs/2410.05284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05284">https://arxiv.org/pdf/2410.05284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05284]] Psychometrics for Hypnopaedia-Aware Machinery via Chaotic Projection of Artificial Mental Imagery(https://arxiv.org/abs/2410.05284)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Neural backdoors represent insidious cybersecurity loopholes that render learning machinery vulnerable to unauthorised manipulations, potentially enabling the weaponisation of artificial intelligence with catastrophic consequences. A backdoor attack involves the clandestine infiltration of a trigger during the learning process, metaphorically analogous to hypnopaedia, where ideas are implanted into a subject's subconscious mind under the state of hypnosis or unconsciousness. When activated by a sensory stimulus, the trigger evokes conditioned reflex that directs a machine to mount a predetermined response. In this study, we propose a cybernetic framework for constant surveillance of backdoors threats, driven by the dynamic nature of untrustworthy data sources. We develop a self-aware unlearning mechanism to autonomously detach a machine's behaviour from the backdoor trigger. Through reverse engineering and statistical inference, we detect deceptive patterns and estimate the likelihood of backdoor infection. We employ model inversion to elicit artificial mental imagery, using stochastic processes to disrupt optimisation pathways and avoid convergent but potentially flawed patterns. This is followed by hypothesis analysis, which estimates the likelihood of each potentially malicious pattern being the true trigger and infers the probability of infection. The primary objective of this study is to maintain a stable state of equilibrium between knowledge fidelity and backdoor vulnerability.</li>
</ul>

<h3>Title: CaLMFlow: Volterra Flow Matching using Causal Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sizhuang He, Daniel Levine, Ivan Vrkic, Marco Francesco Bressana, David Zhang, Syed Asad Rizvi, Yangtian Zhang, Emanuele Zappala, David van Dijk</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05292">https://arxiv.org/abs/2410.05292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05292">https://arxiv.org/pdf/2410.05292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05292]] CaLMFlow: Volterra Flow Matching using Causal Language Models(https://arxiv.org/abs/2410.05292)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>We introduce CaLMFlow (Causal Language Models for Flow Matching), a novel framework that casts flow matching as a Volterra integral equation (VIE), leveraging the power of large language models (LLMs) for continuous data generation. CaLMFlow enables the direct application of LLMs to learn complex flows by formulating flow matching as a sequence modeling task, bridging discrete language modeling and continuous generative modeling. Our method implements tokenization across space and time, thereby solving a VIE over these domains. This approach enables efficient handling of high-dimensional data and outperforms ODE solver-dependent methods like conditional flow matching (CFM). We demonstrate CaLMFlow's effectiveness on synthetic and real-world data, including single-cell perturbation response prediction, showcasing its ability to incorporate textual context and generalize to unseen conditions. Our results highlight LLM-driven flow matching as a promising paradigm in generative modeling, offering improved scalability, flexibility, and context-awareness.</li>
</ul>

<h3>Title: AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiaogeng Liu, Peiran Li, Edward Suh, Yevgeniy Vorobeychik, Zhuoqing Mao, Somesh Jha, Patrick McDaniel, Huan Sun, Bo Li, Chaowei Xiao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05295">https://arxiv.org/abs/2410.05295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05295">https://arxiv.org/pdf/2410.05295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05295]] AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs(https://arxiv.org/abs/2410.05295)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that can automatically discover as many jailbreak strategies as possible from scratch, without any human intervention or predefined scopes (e.g., specified candidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo can significantly outperform baseline methods, achieving a 74.3% higher average attack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an 88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a unified framework that can incorporate existing human-designed jailbreak strategies in a plug-and-play manner. By integrating human-designed strategies, AutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on GPT-4-1106-turbo.</li>
</ul>

<h3>Title: Cyber Risk Taxonomies: Statistical Analysis of Cybersecurity Risk Classifications</h3>
<ul>
<li><strong>Authors: </strong>Matteo Malavasi (1), Gareth W. Peters (2), Stefan Treuck (3), Pavel V. Shevchenko (3), Jiwook Jang (3), Georgy Sofronov (4) ((1) School of Risk and Actuarial Studies, UNSW Business School, University of New South Wales, Australia, (2) Department of Statistics and Applied Probability, University of California Santa Barbara, USA, (3) Department of Actuarial Studies and Business Analytics, Macquarie University, Australia, (4) School of Mathematical and Physical Sciences, Macquarie University, Australia)</a></li>
<li><strong>Subjects: </strong>cs.CR, q-fin.RM, q-fin.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05297">https://arxiv.org/abs/2410.05297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05297">https://arxiv.org/pdf/2410.05297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05297]] Cyber Risk Taxonomies: Statistical Analysis of Cybersecurity Risk Classifications(https://arxiv.org/abs/2410.05297)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Cyber risk classifications are widely used in the modeling of cyber event distributions, yet their effectiveness in out of sample forecasting performance remains underexplored. In this paper, we analyse the most commonly used classifications and argue in favour of switching the attention from goodness-of-fit and in-sample predictive performance, to focusing on the out-of sample forecasting performance. We use a rolling window analysis, to compare cyber risk distribution forecasts via threshold weighted scoring functions. Our results indicate that business motivated cyber risk classifications appear to be too restrictive and not flexible enough to capture the heterogeneity of cyber risk events. We investigate how dynamic and impact-based cyber risk classifiers seem to be better suited in forecasting future cyber risk losses than the other considered classifications. These findings suggest that cyber risk types provide limited forecasting ability concerning cyber event severity distribution, and cyber insurance ratemakers should utilize cyber risk types only when modeling the cyber event frequency distribution. Our study offers valuable insights for decision-makers and policymakers alike, contributing to the advancement of scientific knowledge in the field of cyber risk management.</li>
</ul>

<h3>Title: How Do Large Language Models Understand Graph Patterns? A Benchmark for Graph Pattern Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Xinnan Dai, Haohao Qu, Yifen Shen, Bohang Zhang, Qihao Wen, Wenqi Fan, Dongsheng Li, Jiliang Tang, Caihua Shan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05298">https://arxiv.org/abs/2410.05298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05298">https://arxiv.org/pdf/2410.05298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05298]] How Do Large Language Models Understand Graph Patterns? A Benchmark for Graph Pattern Comprehension(https://arxiv.org/abs/2410.05298)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Benchmarking the capabilities and limitations of large language models (LLMs) in graph-related tasks is becoming an increasingly popular and crucial area of research. Recent studies have shown that LLMs exhibit a preliminary ability to understand graph structures and node features. However, the potential of LLMs in graph pattern mining remains largely unexplored. This is a key component in fields such as computational chemistry, biology, and social network analysis. To bridge this gap, this work introduces a comprehensive benchmark to assess LLMs' capabilities in graph pattern tasks. We have developed a benchmark that evaluates whether LLMs can understand graph patterns based on either terminological or topological descriptions. Additionally, our benchmark tests the LLMs' capacity to autonomously discover graph patterns from data. The benchmark encompasses both synthetic and real datasets, and a variety of models, with a total of 11 tasks and 7 models. Our experimental framework is designed for easy expansion to accommodate new models and datasets. Our findings reveal that: (1) LLMs have preliminary abilities to understand graph patterns, with O1-mini outperforming in the majority of tasks; (2) Formatting input data to align with the knowledge acquired during pretraining can enhance performance; (3) The strategies employed by LLMs may differ from those used in conventional algorithms.</li>
</ul>

<h3>Title: Developing Assurance Cases for Adversarial Robustness and Regulatory Compliance in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tomas Bueno Momcilovic, Dian Balta, Beat Buesser, Giulio Zizzo, Mark Purcell</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05304">https://arxiv.org/abs/2410.05304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05304">https://arxiv.org/pdf/2410.05304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05304]] Developing Assurance Cases for Adversarial Robustness and Regulatory Compliance in LLMs(https://arxiv.org/abs/2410.05304)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents an approach to developing assurance cases for adversarial robustness and regulatory compliance in large language models (LLMs). Focusing on both natural and code language tasks, we explore the vulnerabilities these models face, including adversarial attacks based on jailbreaking, heuristics, and randomization. We propose a layered framework incorporating guardrails at various stages of LLM deployment, aimed at mitigating these attacks and ensuring compliance with the EU AI Act. Our approach includes a meta-layer for dynamic risk management and reasoning, crucial for addressing the evolving nature of LLM vulnerabilities. We illustrate our method with two exemplary assurance cases, highlighting how different contexts demand tailored strategies to ensure robust and compliant AI systems.</li>
</ul>

<h3>Title: Output Scouting: Auditing Large Language Models for Catastrophic Responses</h3>
<ul>
<li><strong>Authors: </strong>Andrew Bell, Joao Fonseca</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05305">https://arxiv.org/abs/2410.05305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05305">https://arxiv.org/pdf/2410.05305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05305]] Output Scouting: Auditing Large Language Models for Catastrophic Responses(https://arxiv.org/abs/2410.05305)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recent high profile incidents in which the use of Large Language Models (LLMs) resulted in significant harm to individuals have brought about a growing interest in AI safety. One reason LLM safety issues occur is that models often have at least some non-zero probability of producing harmful outputs. In this work, we explore the following scenario: imagine an AI safety auditor is searching for catastrophic responses from an LLM (e.g. a "yes" responses to "can I fire an employee for being pregnant?"), and is able to query the model a limited number times (e.g. 1000 times). What is a strategy for querying the model that would efficiently find those failure responses? To this end, we propose output scouting: an approach that aims to generate semantically fluent outputs to a given prompt matching any target probability distribution. We then run experiments using two LLMs and find numerous examples of catastrophic responses. We conclude with a discussion that includes advice for practitioners who are looking to implement LLM auditing for catastrophic responses. We also release an open-source toolkit (this https URL) that implements our auditing framework using the Hugging Face transformers library.</li>
</ul>

<h3>Title: Towards Assuring EU AI Act Compliance and Adversarial Robustness of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tomas Bueno Momcilovic, Beat Buesser, Giulio Zizzo, Mark Purcell, Dian Balta</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05306">https://arxiv.org/abs/2410.05306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05306">https://arxiv.org/pdf/2410.05306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05306]] Towards Assuring EU AI Act Compliance and Adversarial Robustness of LLMs(https://arxiv.org/abs/2410.05306)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models are prone to misuse and vulnerable to security threats, raising significant safety and security concerns. The European Union's Artificial Intelligence Act seeks to enforce AI robustness in certain contexts, but faces implementation challenges due to the lack of standards, complexity of LLMs and emerging security vulnerabilities. Our research introduces a framework using ontologies, assurance cases, and factsheets to support engineers and stakeholders in understanding and documenting AI system compliance and security regarding adversarial robustness. This approach aims to ensure that LLMs adhere to regulatory standards and are equipped to counter potential threats.</li>
</ul>

<h3>Title: Comparative Survey of Cyber-Threat and Attack Trends and Prediction of Future Cyber-Attack Patterns</h3>
<ul>
<li><strong>Authors: </strong>Uwazie Emmanuel Chinanu, Oluyemi Amujo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05308">https://arxiv.org/abs/2410.05308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05308">https://arxiv.org/pdf/2410.05308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05308]] Comparative Survey of Cyber-Threat and Attack Trends and Prediction of Future Cyber-Attack Patterns(https://arxiv.org/abs/2410.05308)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>This paper presents a comparative survey of cyberthreat and attack trends starting from 2010 till date Cyber security breaches are constantly on the rise with huge uncertainty and risks The trend is causing rife globally because of its consequences to national security and economy With diverse interests and motivations for various categories of threats and attacks we carried out a comparative survey and analysis of security breaches to unravel the patterns and predict what will shape future security challenges The diversity of attacks and growing state actors involvement without any sort of regulation is making cyber weapons attractive to the states States are leveraging the anonymity and attribution flaws to hit hard on perceived adversaries thereby complicating the cyber security equation</li>
</ul>

<h3>Title: ShieldDiff: Suppressing Sexual Content Generation from Diffusion Models through Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Dong Han, Salaheldin Mohamed, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05309">https://arxiv.org/abs/2410.05309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05309">https://arxiv.org/pdf/2410.05309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05309]] ShieldDiff: Suppressing Sexual Content Generation from Diffusion Models through Reinforcement Learning(https://arxiv.org/abs/2410.05309)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>With the advance of generative AI, the text-to-image (T2I) model has the ability to generate various contents. However, the generated contents cannot be fully controlled. There is a potential risk that T2I model can generate unsafe images with uncomfortable contents. In our work, we focus on eliminating the NSFW (not safe for work) content generation from T2I model while maintaining the high quality of generated images by fine-tuning the pre-trained diffusion model via reinforcement learning by optimizing the well-designed content-safe reward function. The proposed method leverages a customized reward function consisting of the CLIP (Contrastive Language-Image Pre-training) and nudity rewards to prune the nudity contents that adhere to the pret-rained model and keep the corresponding semantic meaning on the safe side. In this way, the T2I model is robust to unsafe adversarial prompts since unsafe visual representations are mitigated from latent space. Extensive experiments conducted on different datasets demonstrate the effectiveness of the proposed method in alleviating unsafe content generation while preserving the high-fidelity of benign images as well as images generated by unsafe prompts. We compare with five existing state-of-the-art (SOTA) methods and achieve competitive performance on sexual content removal and image quality retention. In terms of robustness, our method outperforms counterparts under the SOTA black-box attacking model. Furthermore, our constructed method can be a benchmark for anti-NSFW generation with semantically-relevant safe alignment.</li>
</ul>

<h3>Title: An Approach To Enhance IoT Security In 6G Networks Through Explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Navneet Kaur, Lav Gupta</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05310">https://arxiv.org/abs/2410.05310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05310">https://arxiv.org/pdf/2410.05310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05310]] An Approach To Enhance IoT Security In 6G Networks Through Explainable AI(https://arxiv.org/abs/2410.05310)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, fair</a></li>
<li><strong>Abstract: </strong>Wireless communication has evolved significantly, with 6G offering groundbreaking capabilities, particularly for IoT. However, the integration of IoT into 6G presents new security challenges, expanding the attack surface due to vulnerabilities introduced by advanced technologies such as open RAN, terahertz (THz) communication, IRS, massive MIMO, and AI. Emerging threats like AI exploitation, virtualization risks, and evolving attacks, including data manipulation and signal interference, further complicate security efforts. As 6G standards are set to be finalized by 2030, work continues to align security measures with technological advances. However, substantial gaps remain in frameworks designed to secure integrated IoT and 6G systems. Our research addresses these challenges by utilizing tree-based machine learning algorithms to manage complex datasets and evaluate feature importance. We apply data balancing techniques to ensure fair attack representation and use SHAP and LIME to improve model transparency. By aligning feature importance with XAI methods and cross-validating for consistency, we boost model accuracy and enhance IoT security within the 6G ecosystem.</li>
</ul>

<h3>Title: ConceptLens: from Pixels to Understanding</h3>
<ul>
<li><strong>Authors: </strong>Abhilekha Dalal, Pascal Hitzler</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05311">https://arxiv.org/abs/2410.05311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05311">https://arxiv.org/pdf/2410.05311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05311]] ConceptLens: from Pixels to Understanding(https://arxiv.org/abs/2410.05311)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>ConceptLens is an innovative tool designed to illuminate the intricate workings of deep neural networks (DNNs) by visualizing hidden neuron activations. By integrating deep learning with symbolic methods, ConceptLens offers users a unique way to understand what triggers neuron activations and how they respond to various stimuli. The tool uses error-margin analysis to provide insights into the confidence levels of neuron activations, thereby enhancing the interpretability of DNNs. This paper presents an overview of ConceptLens, its implementation, and its application in real-time visualization of neuron activations and error margins through bar charts.</li>
</ul>

<h3>Title: An Intelligent Native Network Slicing Security Architecture Empowered by Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Rodrigo Moreira, Rodolfo S. Villaca, Moises R. N. Ribeiro, Joberto S. B. Martins, Joao Henrique Correa, Tereza C. Carvalho, Flavio de Oliveira Silva</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.ET, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05312">https://arxiv.org/abs/2410.05312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05312">https://arxiv.org/pdf/2410.05312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05312]] An Intelligent Native Network Slicing Security Architecture Empowered by Federated Learning(https://arxiv.org/abs/2410.05312)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, federate</a></li>
<li><strong>Abstract: </strong>Network Slicing (NS) has transformed the landscape of resource sharing in networks, offering flexibility to support services and applications with highly variable requirements in areas such as the next-generation 5G/6G mobile networks (NGMN), vehicular networks, industrial Internet of Things (IoT), and verticals. Although significant research and experimentation have driven the development of network slicing, existing architectures often fall short in intrinsic architectural intelligent security capabilities. This paper proposes an architecture-intelligent security mechanism to improve the NS solutions. We idealized a security-native architecture that deploys intelligent microservices as federated agents based on machine learning, providing intra-slice and architectural operation security for the Slicing Future Internet Infrastructures (SFI2) reference architecture. It is noteworthy that federated learning approaches match the highly distributed modern microservice-based architectures, thus providing a unifying and scalable design choice for NS platforms addressing both service and security. Using ML-Agents and Security Agents, our approach identified Distributed Denial-of-Service (DDoS) and intrusion attacks within the slice using generic and non-intrusive telemetry records, achieving an average accuracy of approximately $95.60\%$ in the network slicing architecture and $99.99\%$ for the deployed slice -- intra-slice. This result demonstrates the potential for leveraging architectural operational security and introduces a promising new research direction for network slicing architectures.</li>
</ul>

<h3>Title: PalmBench: A Comprehensive Benchmark of Compressed Large Language Models on Mobile Platforms</h3>
<ul>
<li><strong>Authors: </strong>Yilong Li, Jingyu Liu, Hao Zhang, M Badri Narayanan, Utkarsh Sharma, Shuai Zhang, Pan Hu, Yijing Zeng, Jayaram Raghuram, Suman Banerjee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05315">https://arxiv.org/abs/2410.05315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05315">https://arxiv.org/pdf/2410.05315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05315]] PalmBench: A Comprehensive Benchmark of Compressed Large Language Models on Mobile Platforms(https://arxiv.org/abs/2410.05315)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative, large language model</a></li>
<li><strong>Abstract: </strong>Deploying large language models (LLMs) locally on mobile devices is advantageous in scenarios where transmitting data to remote cloud servers is either undesirable due to privacy concerns or impractical due to network connection. Recent advancements (MLC, 2023a; Gerganov, 2023) have facilitated the local deployment of LLMs. However, local deployment also presents challenges, particularly in balancing quality (generative performance), latency, and throughput within the hardware constraints of mobile devices. In this paper, we introduce our lightweight, all-in-one automated benchmarking framework that allows users to evaluate LLMs on mobile devices. We provide a comprehensive benchmark of various popular LLMs with different quantization configurations (both weights and activations) across multiple mobile platforms with varying hardware capabilities. Unlike traditional benchmarks that assess full-scale models on high-end GPU clusters, we focus on evaluating resource efficiency (memory and power consumption) and harmful output for compressed models on mobile devices. Our key observations include i) differences in energy efficiency and throughput across mobile platforms; ii) the impact of quantization on memory usage, GPU execution time, and power consumption; and iii) accuracy and performance degradation of quantized models compared to their non-quantized counterparts; and iv) the frequency of hallucinations and toxic content generated by compressed LLMs on mobile devices.</li>
</ul>

<h3>Title: Accelerating Diffusion Transformers with Token-wise Feature Caching</h3>
<ul>
<li><strong>Authors: </strong>Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05317">https://arxiv.org/abs/2410.05317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05317">https://arxiv.org/pdf/2410.05317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05317]] Accelerating Diffusion Transformers with Token-wise Feature Caching(https://arxiv.org/abs/2410.05317)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion transformers have shown significant effectiveness in both image and video synthesis at the expense of huge computation costs. To address this problem, feature caching methods have been introduced to accelerate diffusion transformers by caching the features in previous timesteps and reusing them in the following timesteps. However, previous caching methods ignore that different tokens exhibit different sensitivities to feature caching, and feature caching on some tokens may lead to 10$\times$ more destruction to the overall generation quality compared with other tokens. In this paper, we introduce token-wise feature caching, allowing us to adaptively select the most suitable tokens for caching, and further enable us to apply different caching ratios to neural layers in different types and depths. Extensive experiments on PixArt-$\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image and video generation with no requirements for training. For instance, 2.36$\times$ and 1.93$\times$ acceleration are achieved on OpenSora and PixArt-$\alpha$ with almost no drop in generation quality.</li>
</ul>

<h3>Title: Improving LLM Reasoning through Scaling Inference Computation with Collaborative Verification</h3>
<ul>
<li><strong>Authors: </strong>Zhenwen Liang, Ye Liu, Tong Niu, Xiangliang Zhang, Yingbo Zhou, Semih Yavuz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05318">https://arxiv.org/abs/2410.05318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05318">https://arxiv.org/pdf/2410.05318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05318]] Improving LLM Reasoning through Scaling Inference Computation with Collaborative Verification(https://arxiv.org/abs/2410.05318)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Despite significant advancements in the general capability of large language models (LLMs), they continue to struggle with consistent and accurate reasoning, especially in complex tasks such as mathematical and code reasoning. One key limitation is that LLMs are trained primarily on correct solutions, reducing their ability to detect and learn from errors, which hampers their ability to reliably verify and rank outputs. To address this, we scale up the inference-time computation by generating multiple reasoning paths and employing verifiers to assess and rank the generated outputs by correctness. To facilitate this, we introduce a comprehensive dataset consisting of correct and incorrect solutions for math and code tasks, generated by multiple LLMs. This diverse set of solutions enables verifiers to more effectively distinguish and rank correct answers from erroneous outputs. The training methods for building verifiers were selected based on an extensive comparison of existing approaches. Moreover, to leverage the unique strengths of different reasoning strategies, we propose a novel collaborative method integrating Chain-of-Thought (CoT) and Program-of-Thought (PoT) solutions for verification. CoT provides a clear, step-by-step reasoning process that enhances interpretability, while PoT, being executable, offers a precise and error-sensitive validation mechanism. By taking both of their strengths, our approach significantly improves the accuracy and reliability of reasoning verification. Our verifiers, Math-Rev and Code-Rev, demonstrate substantial performance gains to existing LLMs, achieving state-of-the-art results on benchmarks such as GSM8k and MATH and even outperforming GPT-4o with Qwen-72B-Instruct as the reasoner.</li>
</ul>

<h3>Title: Noise Crystallization and Liquid Noise: Zero-shot Video Generation using Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Haaris Khan, Hadrien Reynaud, Bernhard Kainz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05322">https://arxiv.org/abs/2410.05322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05322">https://arxiv.org/pdf/2410.05322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05322]] Noise Crystallization and Liquid Noise: Zero-shot Video Generation using Image Diffusion Models(https://arxiv.org/abs/2410.05322)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Although powerful for image generation, consistent and controllable video is a longstanding problem for diffusion models. Video models require extensive training and computational resources, leading to high costs and large environmental impacts. Moreover, video models currently offer limited control of the output motion. This paper introduces a novel approach to video generation by augmenting image diffusion models to create sequential animation frames while maintaining fine detail. These techniques can be applied to existing image models without training any video parameters (zero-shot) by altering the input noise in a latent diffusion model. Two complementary methods are presented. Noise crystallization ensures consistency but is limited to large movements due to reduced latent embedding sizes. Liquid noise trades consistency for greater flexibility without resolution limitations. The core concepts also allow other applications such as relighting, seamless upscaling, and improved video style transfer. Furthermore, an exploration of the VAE embedding used for latent diffusion models is performed, resulting in interesting theoretical insights such as a method for human-interpretable latent spaces.</li>
</ul>

<h3>Title: From Incomplete Coarse-Grained to Complete Fine-Grained: A Two-Stage Framework for Spatiotemporal Data Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Sun, Haoyang Su, En Wang, Funing Yang, Yongjian Yang, Wenbin Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05323">https://arxiv.org/abs/2410.05323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05323">https://arxiv.org/pdf/2410.05323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05323]] From Incomplete Coarse-Grained to Complete Fine-Grained: A Two-Stage Framework for Spatiotemporal Data Reconstruction(https://arxiv.org/abs/2410.05323)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>With the rapid development of various sensing devices, spatiotemporal data is becoming increasingly important nowadays. However, due to sensing costs and privacy concerns, the collected data is often incomplete and coarse-grained, limiting its application to specific tasks. To address this, we propose a new task called spatiotemporal data reconstruction, which aims to infer complete and fine-grained data from sparse and coarse-grained observations. To achieve this, we introduce a two-stage data inference framework, DiffRecon, grounded in the Denoising Diffusion Probabilistic Model (DDPM). In the first stage, we present Diffusion-C, a diffusion model augmented by ST-PointFormer, a powerful encoder designed to leverage the spatial correlations between sparse data points. Following this, the second stage introduces Diffusion-F, which incorporates the proposed T-PatternNet to capture the temporal pattern within sequential data. Together, these two stages form an end-to-end framework capable of inferring complete, fine-grained data from incomplete and coarse-grained observations. We conducted experiments on multiple real-world datasets to demonstrate the superiority of our method.</li>
</ul>

<h3>Title: Early-Cycle Internal Impedance Enables ML-Based Battery Cycle Life Predictions Across Manufacturers</h3>
<ul>
<li><strong>Authors: </strong>Tyler Sours (1), Shivang Agarwal (1), Marc Cormier (2), Jordan Crivelli-Decker (1), Steffen Ridderbusch (1), Stephen L. Glazier (2), Connor P. Aiken (2), Aayush R. Singh (1), Ang Xiao (1), Omar Allam (1)</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05326">https://arxiv.org/abs/2410.05326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05326">https://arxiv.org/pdf/2410.05326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05326]] Early-Cycle Internal Impedance Enables ML-Based Battery Cycle Life Predictions Across Manufacturers(https://arxiv.org/abs/2410.05326)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Predicting the end-of-life (EOL) of lithium-ion batteries across different manufacturers presents significant challenges due to variations in electrode materials, manufacturing processes, cell formats, and a lack of generally available data. Methods that construct features solely on voltage-capacity profile data typically fail to generalize across cell chemistries. This study introduces a methodology that combines traditional voltage-capacity features with Direct Current Internal Resistance (DCIR) measurements, enabling more accurate and generalizable EOL predictions. The use of early-cycle DCIR data captures critical degradation mechanisms related to internal resistance growth, enhancing model robustness. Models are shown to successfully predict the number of cycles to EOL for unseen manufacturers of varied electrode composition with a mean absolute error (MAE) of 150 cycles. This cross-manufacturer generalizability reduces the need for extensive new data collection and retraining, enabling manufacturers to optimize new battery designs using existing datasets. Additionally, a novel DCIR-compatible dataset is released as part of ongoing efforts to enrich the growing ecosystem of cycling data and accelerate battery materials development.</li>
</ul>

<h3>Title: Taylor Unswift: Secured Weight Release for Large Language Models via Taylor Expansion</h3>
<ul>
<li><strong>Authors: </strong>Guanchu Wang, Yu-Neng Chuang, Ruixiang Tang, Shaochen Zhong, Jiayi Yuan, Hongye Jin, Zirui Liu, Vipin Chaudhary, Shuai Xu, James Caverlee, Xia Hu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05331">https://arxiv.org/abs/2410.05331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05331">https://arxiv.org/pdf/2410.05331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05331]] Taylor Unswift: Secured Weight Release for Large Language Models via Taylor Expansion(https://arxiv.org/abs/2410.05331)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>Ensuring the security of released large language models (LLMs) poses a significant dilemma, as existing mechanisms either compromise ownership rights or raise data privacy concerns. To address this dilemma, we introduce TaylorMLP to protect the ownership of released LLMs and prevent their abuse. Specifically, TaylorMLP preserves the ownership of LLMs by transforming the weights of LLMs into parameters of Taylor-series. Instead of releasing the original weights, developers can release the Taylor-series parameters with users, thereby ensuring the security of LLMs. Moreover, TaylorMLP can prevent abuse of LLMs by adjusting the generation speed. It can induce low-speed token generation for the protected LLMs by increasing the terms in the Taylor-series. This intentional delay helps LLM developers prevent potential large-scale unauthorized uses of their models. Empirical experiments across five datasets and three LLM architectures demonstrate that TaylorMLP induces over 4x increase in latency, producing the tokens precisely matched with original LLMs. Subsequent defensive experiments further confirm that TaylorMLP effectively prevents users from reconstructing the weight values based on downstream datasets.</li>
</ul>

<h3>Title: A Global Cybersecurity Standardization Framework for Healthcare Informatics</h3>
<ul>
<li><strong>Authors: </strong>Kishu Gupta, Vinaytosh Mishra, Aaisha Makkar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05333">https://arxiv.org/abs/2410.05333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05333">https://arxiv.org/pdf/2410.05333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05333]] A Global Cybersecurity Standardization Framework for Healthcare Informatics(https://arxiv.org/abs/2410.05333)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Healthcare has witnessed an increased digitalization in the post-COVID world. Technologies such as the medical internet of things and wearable devices are generating a plethora of data available on the cloud anytime from anywhere. This data can be analyzed using advanced artificial intelligence techniques for diagnosis, prognosis, or even treatment of disease. This advancement comes with a major risk to protecting and securing protected health information (PHI). The prevailing regulations for preserving PHI are neither comprehensive nor easy to implement. The study first identifies twenty activities crucial for privacy and security, then categorizes them into five homogeneous categories namely: $\complement_1$ (Policy and Compliance Management), $\complement_2$ (Employee Training and Awareness), $\complement_3$ (Data Protection and Privacy Control), $\complement_4$ (Monitoring and Response), and $\complement_5$ (Technology and Infrastructure Security) and prioritizes these categories to provide a framework for the implementation of privacy and security in a wise manner. The framework utilized the Delphi Method to identify activities, criteria for categorization, and prioritization. Categorization is based on the Density-Based Spatial Clustering of Applications with Noise (DBSCAN), and prioritization is performed using a Technique for Order of Preference by Similarity to the Ideal Solution (TOPSIS). The outcomes conclude that $\complement_3$ activities should be given first preference in implementation and followed by $\complement_1$ and $\complement_2$ activities. Finally, $\complement_4$ and $\complement_5$ should be implemented. The prioritized view of identified clustered healthcare activities related to security and privacy, are useful for healthcare policymakers and healthcare informatics professionals.</li>
</ul>

<h3>Title: TA3: Testing Against Adversarial Attacks on Machine Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Yuanzhe Jin, Min Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05334">https://arxiv.org/abs/2410.05334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05334">https://arxiv.org/pdf/2410.05334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05334]] TA3: Testing Against Adversarial Attacks on Machine Learning Models(https://arxiv.org/abs/2410.05334)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Adversarial attacks are major threats to the deployment of machine learning (ML) models in many applications. Testing ML models against such attacks is becoming an essential step for evaluating and improving ML models. In this paper, we report the design and development of an interactive system for aiding the workflow of Testing Against Adversarial Attacks (TA3). In particular, with TA3, human-in-the-loop (HITL) enables human-steered attack simulation and visualization-assisted attack impact evaluation. While the current version of TA3 focuses on testing decision tree models against adversarial attacks based on the One Pixel Attack Method, it demonstrates the importance of HITL in ML testing and the potential application of HITL to the ML testing workflows for other types of ML models and other types of adversarial attacks.</li>
</ul>

<h3>Title: Generating CAD Code with Vision-Language Models for 3D Designs</h3>
<ul>
<li><strong>Authors: </strong>Kamel Alrashedy, Pradyumna Tambwekar, Zulfiqar Zaidi, Megan Langwasser, Wei Xu, Matthew Gombolay</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05340">https://arxiv.org/abs/2410.05340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05340">https://arxiv.org/pdf/2410.05340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05340]] Generating CAD Code with Vision-Language Models for 3D Designs(https://arxiv.org/abs/2410.05340)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative AI has transformed the fields of Design and Manufacturing by providing efficient and automated methods for generating and modifying 3D objects. One approach involves using Large Language Models (LLMs) to generate Computer- Aided Design (CAD) scripting code, which can then be executed to render a 3D object; however, the resulting 3D object may not meet the specified requirements. Testing the correctness of CAD generated code is challenging due to the complexity and structure of 3D objects (e.g., shapes, surfaces, and dimensions) that are not feasible in code. In this paper, we introduce CADCodeVerify, a novel approach to iteratively verify and improve 3D objects generated from CAD code. Our approach works by producing ameliorative feedback by prompting a Vision-Language Model (VLM) to generate and answer a set of validation questions to verify the generated object and prompt the VLM to correct deviations. To evaluate CADCodeVerify, we introduce, CADPrompt, the first benchmark for CAD code generation, consisting of 200 natural language prompts paired with expert-annotated scripting code for 3D objects to benchmark progress. Our findings show that CADCodeVerify improves VLM performance by providing visual feedback, enhancing the structure of the 3D objects, and increasing the success rate of the compiled program. When applied to GPT-4, CADCodeVerify achieved a 7.30% reduction in Point Cloud distance and a 5.0% improvement in success rate compared to prior work</li>
</ul>

<h3>Title: Trained Models Tell Us How to Make Them Robust to Spurious Correlation without Group Annotation</h3>
<ul>
<li><strong>Authors: </strong>Mahdi Ghaznavi, Hesam Asadollahzadeh, Fahimeh Hosseini Noohdani, Soroush Vafaie Tabar, Hosein Hasani, Taha Akbari Alvanagh, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05345">https://arxiv.org/abs/2410.05345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05345">https://arxiv.org/pdf/2410.05345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05345]] Trained Models Tell Us How to Make Them Robust to Spurious Correlation without Group Annotation(https://arxiv.org/abs/2410.05345)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Classifiers trained with Empirical Risk Minimization (ERM) tend to rely on attributes that have high spurious correlation with the target. This can degrade the performance on underrepresented (or 'minority') groups that lack these attributes, posing significant challenges for both out-of-distribution generalization and fairness objectives. Many studies aim to enhance robustness to spurious correlation, but they sometimes depend on group annotations for training. Additionally, a common limitation in previous research is the reliance on group-annotated validation datasets for model selection. This constrains their applicability in situations where the nature of the spurious correlation is not known, or when group labels for certain spurious attributes are not available. To enhance model robustness with minimal group annotation assumptions, we propose Environment-based Validation and Loss-based Sampling (EVaLS). It uses the losses from an ERM-trained model to construct a balanced dataset of high-loss and low-loss samples, mitigating group imbalance in data. This significantly enhances robustness to group shifts when equipped with a simple post-training last layer retraining. By using environment inference methods to create diverse environments with correlation shifts, EVaLS can potentially eliminate the need for group annotation in validation data. In this context, the worst environment accuracy acts as a reliable surrogate throughout the retraining process for tuning hyperparameters and finding a model that performs well across diverse group shifts. EVaLS effectively achieves group robustness, showing that group annotation is not necessary even for validation. It is a fast, straightforward, and effective approach that reaches near-optimal worst group accuracy without needing group annotations, marking a new chapter in the robustness of trained models against spurious correlation.</li>
</ul>

<h3>Title: AnyAttack: Towards Large-scale Self-supervised Generation of Targeted Adversarial Examples for Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Zhang, Junhong Ye, Xingjun Ma, Yige Li, Yunfan Yang, Jitao Sang, Dit-Yan Yeung</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05346">https://arxiv.org/abs/2410.05346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05346">https://arxiv.org/pdf/2410.05346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05346]] AnyAttack: Towards Large-scale Self-supervised Generation of Targeted Adversarial Examples for Vision-Language Models(https://arxiv.org/abs/2410.05346)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Due to their multimodal capabilities, Vision-Language Models (VLMs) have found numerous impactful applications in real-world scenarios. However, recent studies have revealed that VLMs are vulnerable to image-based adversarial attacks, particularly targeted adversarial images that manipulate the model to generate harmful content specified by the adversary. Current attack methods rely on predefined target labels to create targeted adversarial attacks, which limits their scalability and applicability for large-scale robustness evaluations. In this paper, we propose AnyAttack, a self-supervised framework that generates targeted adversarial images for VLMs without label supervision, allowing any image to serve as a target for the attack. To address the limitation of existing methods that require label supervision, we introduce a contrastive loss that trains a generator on a large-scale unlabeled image dataset, LAION-400M dataset, for generating targeted adversarial noise. This large-scale pre-training endows our method with powerful transferability across a wide range of VLMs. Extensive experiments on five mainstream open-source VLMs (CLIP, BLIP, BLIP2, InstructBLIP, and MiniGPT-4) across three multimodal tasks (image-text retrieval, multimodal classification, and image captioning) demonstrate the effectiveness of our attack. Additionally, we successfully transfer AnyAttack to multiple commercial VLMs, including Google's Gemini, Claude's Sonnet, and Microsoft's Copilot. These results reveal an unprecedented risk to VLMs, highlighting the need for effective countermeasures.</li>
</ul>

<h3>Title: ResTNet: Defense against Adversarial Policies via Transformer in Computer Go</h3>
<ul>
<li><strong>Authors: </strong>Tai-Lin Wu, Ti-Rong Wu, Chung-Chin Shih, Yan-Ru Ju, I-Chen Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05347">https://arxiv.org/abs/2410.05347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05347">https://arxiv.org/pdf/2410.05347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05347]] ResTNet: Defense against Adversarial Policies via Transformer in Computer Go(https://arxiv.org/abs/2410.05347)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, transformer</a></li>
<li><strong>Abstract: </strong>Although AlphaZero has achieved superhuman levels in Go, recent research has highlighted its vulnerability in particular situations requiring a more comprehensive understanding of the entire board. To address this challenge, this paper introduces ResTNet, a network that interleaves residual networks and Transformer. Our empirical experiments demonstrate several advantages of using ResTNet. First, it not only improves playing strength but also enhances the ability of global information. Second, it defends against an adversary Go program, called cyclic-adversary, tailor-made for attacking AlphaZero algorithms, significantly reducing the average probability of being attacked rate from 70.44% to 23.91%. Third, it improves the accuracy from 59.15% to 80.01% in correctly recognizing ladder patterns, which are one of the challenging patterns for Go AIs. Finally, ResTNet offers a potential explanation of the decision-making process and can also be applied to other games like Hex. To the best of our knowledge, ResTNet is the first to integrate residual networks and Transformer in the context of AlphaZero for board games, suggesting a promising direction for enhancing AlphaZero's global understanding.</li>
</ul>

<h3>Title: SoK: Towards Security and Safety of Edge AI</h3>
<ul>
<li><strong>Authors: </strong>Tatjana Wingarz, Anne Lauscher, Janick Edinger, Dominik Kaaser, Stefan Schulte, Mathias Fischer</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05349">https://arxiv.org/abs/2410.05349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05349">https://arxiv.org/pdf/2410.05349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05349]] SoK: Towards Security and Safety of Edge AI(https://arxiv.org/abs/2410.05349)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Advanced AI applications have become increasingly available to a broad audience, e.g., as centrally managed large language models (LLMs). Such centralization is both a risk and a performance bottleneck - Edge AI promises to be a solution to these problems. However, its decentralized approach raises additional challenges regarding security and safety. In this paper, we argue that both of these aspects are critical for Edge AI, and even more so, their integration. Concretely, we survey security and safety threats, summarize existing countermeasures, and collect open challenges as a call for more research in this area.</li>
</ul>

<h3>Title: Towards the generation of hierarchical attack models from cybersecurity vulnerabilities using language models</h3>
<ul>
<li><strong>Authors: </strong>Kacper Sowka, Vasile Palade, Xiaorui Jiang, Hesam Jadidbonab</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05351">https://arxiv.org/abs/2410.05351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05351">https://arxiv.org/pdf/2410.05351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05351]] Towards the generation of hierarchical attack models from cybersecurity vulnerabilities using language models(https://arxiv.org/abs/2410.05351)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>This paper investigates the use of a pre-trained language model and siamese network to discern sibling relationships between text-based cybersecurity vulnerability data. The ultimate purpose of the approach presented in this paper is towards the construction of hierarchical attack models based on a set of text descriptions characterising potential/observed vulnerabilities in a given system. Due to the nature of the data, and the uncertainty sensitive environment in which the problem is presented, a practically oriented soft computing approach is necessary. Therefore, a key focus of this work is to investigate practical questions surrounding the reliability of predicted links towards the construction of such models, to which end conceptual and practical challenges and solutions associated with the proposed approach are outlined, such as dataset complexity and stability of predictions. Accordingly, the contributions of this paper focus on producing neural networks using a pre-trained language model for predicting sibling relationships between cybersecurity vulnerabilities, then outlining how to apply this capability towards the generation of hierarchical attack models. In addition, two data sampling mechanisms for tackling data complexity, and a consensus mechanism for reducing the amount of false positive predictions are outlined. Each of these approaches is compared and contrasted using empirical results from three sets of cybersecurity data to determine their effectiveness.</li>
</ul>

<h3>Title: Over-the-Air Federated Learning in Cell-Free MIMO with Long-term Power Constraint</h3>
<ul>
<li><strong>Authors: </strong>Yifan Wang, Cheng Zhang, Yuanndong Zhuang, Yongming Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05354">https://arxiv.org/abs/2410.05354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05354">https://arxiv.org/pdf/2410.05354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05354]] Over-the-Air Federated Learning in Cell-Free MIMO with Long-term Power Constraint(https://arxiv.org/abs/2410.05354)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Wireless networks supporting artificial intelligence have gained significant attention, with Over-the-Air Federated Learning emerging as a key application due to its unique transmission and distributed computing characteristics. This paper derives error bounds for Over-the-Air Federated Learning in a Cell-free MIMO system and formulates an optimization problem to minimize optimality gap via joint optimization of power control and beamforming. We introduce the MOP-LOFPC algorithm, which employs Lyapunov optimization to decouple long-term constraints across rounds while requiring only causal channel state information. Experimental results demonstrate that MOP-LOFPC achieves a better and more flexible trade-off between the model's training loss and adherence to long-term power constraints compared to existing baselines.</li>
</ul>

<h3>Title: Falcon Mamba: The First Competitive Attention-free 7B Language Model</h3>
<ul>
<li><strong>Authors: </strong>Jingwei Zuo, Maksim Velikanov, Dhia Eddine Rhaiem, Ilyas Chahed, Younes Belkada, Guillaume Kunsch, Hakim Hacid</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05355">https://arxiv.org/abs/2410.05355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05355">https://arxiv.org/pdf/2410.05355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05355]] Falcon Mamba: The First Competitive Attention-free 7B Language Model(https://arxiv.org/abs/2410.05355)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>In this technical report, we present Falcon Mamba 7B, a new base large language model based on the novel Mamba architecture. Falcon Mamba 7B is trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure Mamba-based model, Falcon Mamba 7B surpasses leading open-weight models based on Transformers, such as Mistral 7B, Llama3.1 8B, and Falcon2 11B. It is on par with Gemma 7B and outperforms models with different architecture designs, such as RecurrentGemma 9B and RWKV-v6 Finch 7B/14B. Currently, Falcon Mamba 7B is the best-performing Mamba model in the literature at this scale, surpassing both existing Mamba and hybrid Mamba-Transformer models, according to the Open LLM Leaderboard. Due to its architecture, Falcon Mamba 7B is significantly faster at inference and requires substantially less memory for long sequence generation. Despite recent studies suggesting that hybrid Mamba-Transformer models outperform pure architecture designs, we demonstrate that even the pure Mamba design can achieve similar, or even superior results compared to the Transformer and hybrid designs. We make the weights of our implementation of Falcon Mamba 7B publicly available on this https URL, under a permissive license.</li>
</ul>

<h3>Title: Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Zhao, Guoheng Sun, Ruisi Cai, Yukun Zhou, Pingzhi Li, Peihao Wang, Bowen Tan, Yexiao He, Li Chen, Yi Liang, Beidi Chen, Binhang Yuan, Hongyi Wang, Ang Li, Zhangyang Wang, Tianlong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05357">https://arxiv.org/abs/2410.05357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05357">https://arxiv.org/pdf/2410.05357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05357]] Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild(https://arxiv.org/abs/2410.05357)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) excel across tasks and specialized domains, scaling LLMs based on existing models has garnered significant attention, which faces the challenge of decreasing performance when combining disparate models. Various techniques have been proposed for the aggregation of pre-trained LLMs, including model merging, Mixture-of-Experts, and stacking. Despite their merits, a comprehensive comparison and synergistic application of them to a diverse model zoo is yet to be adequately addressed. In light of this research gap, this paper introduces Model-GLUE, a holistic LLM scaling guideline. First, our work starts with a benchmarking of existing LLM scaling techniques, especially selective merging, and variants of mixture. Utilizing the insights from the benchmark results, we formulate an strategy for the selection and aggregation of a heterogeneous model zoo characterizing different architectures and initialization. Our methodology involves the clustering of mergeable models and optimal merging strategy selection, and the integration of clusters through a model mixture. Finally, evidenced by our experiments on a diverse Llama-2-based model zoo, Model-GLUE shows an average performance enhancement of 5.61%, achieved without additional training. Codes are available at: this https URL.</li>
</ul>

<h3>Title: RespLLM: Unifying Audio and Text with Multimodal LLMs for Generalized Respiratory Health Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yuwei Zhang, Tong Xia, Aaqib Saeed, Cecilia Mascolo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05361">https://arxiv.org/abs/2410.05361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05361">https://arxiv.org/pdf/2410.05361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05361]] RespLLM: Unifying Audio and Text with Multimodal LLMs for Generalized Respiratory Health Prediction(https://arxiv.org/abs/2410.05361)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The high incidence and mortality rates associated with respiratory diseases underscores the importance of early screening. Machine learning models can automate clinical consultations and auscultation, offering vital support in this area. However, the data involved, spanning demographics, medical history, symptoms, and respiratory audio, are heterogeneous and complex. Existing approaches are insufficient and lack generalizability, as they typically rely on limited training data, basic fusion techniques, and task-specific models. In this paper, we propose RespLLM, a novel multimodal large language model (LLM) framework that unifies text and audio representations for respiratory health prediction. RespLLM leverages the extensive prior knowledge of pretrained LLMs and enables effective audio-text fusion through cross-modal attentions. Instruction tuning is employed to integrate diverse data from multiple sources, ensuring generalizability and versatility of the model. Experiments on five real-world datasets demonstrate that RespLLM outperforms leading baselines by an average of 4.6% on trained tasks, 7.9% on unseen datasets, and facilitates zero-shot predictions for new tasks. Our work lays the foundation for multimodal models that can perceive, listen to, and understand heterogeneous data, paving the way for scalable respiratory health diagnosis.</li>
</ul>

<h3>Title: LLMs Are In-Context Reinforcement Learners</h3>
<ul>
<li><strong>Authors: </strong>Giovanni Monea, Antoine Bosselut, Kianté Brantley, Yoav Artzi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05362">https://arxiv.org/abs/2410.05362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05362">https://arxiv.org/pdf/2410.05362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05362]] LLMs Are In-Context Reinforcement Learners(https://arxiv.org/abs/2410.05362)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can learn new tasks through in-context supervised learning (i.e., ICL). This work studies if this ability extends to in-context reinforcement learning (ICRL), where models are not given gold labels in context, but only their past predictions and rewards. We show that a naive application of ICRL fails miserably, and identify the root cause as a fundamental deficiency at exploration, which leads to quick model degeneration. We propose an algorithm to address this deficiency by increasing test-time compute, as well as a compute-bound approximation. We use several challenging classification tasks to empirically show that our ICRL algorithms lead to effective learning from rewards alone, and analyze the characteristics of this ability and our methods. Overall, our results reveal remarkable ICRL abilities in LLMs.</li>
</ul>

<h3>Title: Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, Ping Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05363">https://arxiv.org/abs/2410.05363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05363">https://arxiv.org/pdf/2410.05363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05363]] Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation(https://arxiv.org/abs/2410.05363)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text-to-video (T2V) models like Sora have made significant strides in visualizing complex prompts, which is increasingly viewed as a promising path towards constructing the universal world simulator. Cognitive psychologists believe that the foundation for achieving this goal is the ability to understand intuitive physics. However, the capacity of these models to accurately represent intuitive physics remains largely unexplored. To bridge this gap, we introduce PhyGenBench, a comprehensive \textbf{Phy}sics \textbf{Gen}eration \textbf{Ben}chmark designed to evaluate physical commonsense correctness in T2V generation. PhyGenBench comprises 160 carefully crafted prompts across 27 distinct physical laws, spanning four fundamental domains, which could comprehensively assesses models' understanding of physical commonsense. Alongside PhyGenBench, we propose a novel evaluation framework called PhyGenEval. This framework employs a hierarchical evaluation structure utilizing appropriate advanced vision-language models and large language models to assess physical commonsense. Through PhyGenBench and PhyGenEval, we can conduct large-scale automated assessments of T2V models' understanding of physical commonsense, which align closely with human feedback. Our evaluation results and in-depth analysis demonstrate that current models struggle to generate videos that comply with physical commonsense. Moreover, simply scaling up models or employing prompt engineering techniques is insufficient to fully address the challenges presented by PhyGenBench (e.g., dynamic scenarios). We hope this study will inspire the community to prioritize the learning of physical commonsense in these models beyond entertainment applications. We will release the data and codes at this https URL</li>
</ul>

<h3>Title: Diffusion Model Predictive Control</h3>
<ul>
<li><strong>Authors: </strong>Guangyao Zhou, Sivaramakrishnan Swaminathan, Rajkumar Vasudeva Raju, J. Swaroop Guntupalli, Wolfgang Lehrach, Joseph Ortiz, Antoine Dedieu, Miguel Lázaro-Gredilla, Kevin Murphy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05364">https://arxiv.org/abs/2410.05364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05364">https://arxiv.org/pdf/2410.05364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05364]] Diffusion Model Predictive Control(https://arxiv.org/abs/2410.05364)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose Diffusion Model Predictive Control (D-MPC), a novel MPC approach that learns a multi-step action proposal and a multi-step dynamics model, both using diffusion models, and combines them for use in online MPC. On the popular D4RL benchmark, we show performance that is significantly better than existing model-based offline planning methods using MPC and competitive with state-of-the-art (SOTA) model-based and model-free reinforcement learning methods. We additionally illustrate D-MPC's ability to optimize novel reward functions at run time and adapt to novel dynamics, and highlight its advantages compared to existing diffusion-based planning baselines.</li>
</ul>

<h3>Title: Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Tunazzina Islam, Dan Goldwasser</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05401">https://arxiv.org/abs/2410.05401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05401">https://arxiv.org/pdf/2410.05401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05401]] Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation(https://arxiv.org/abs/2410.05401)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Climate change communication on social media increasingly employs microtargeting strategies to effectively reach and influence specific demographic groups. This study presents a post-hoc analysis of microtargeting practices within climate campaigns by leveraging large language models (LLMs) to examine Facebook advertisements. Our analysis focuses on two key aspects: demographic targeting and fairness. We evaluate the ability of LLMs to accurately predict the intended demographic targets, such as gender and age group, achieving an overall accuracy of 88.55%. Furthermore, we instruct the LLMs to generate explanations for their classifications, providing transparent reasoning behind each decision. These explanations reveal the specific thematic elements used to engage different demographic segments, highlighting distinct strategies tailored to various audiences. Our findings show that young adults are primarily targeted through messages emphasizing activism and environmental consciousness, while women are engaged through themes related to caregiving roles and social advocacy. In addition to evaluating the effectiveness of LLMs in detecting microtargeted messaging, we conduct a comprehensive fairness analysis to identify potential biases in model predictions. Our findings indicate that while LLMs perform well overall, certain biases exist, particularly in the classification of senior citizens and male audiences. By showcasing the efficacy of LLMs in dissecting and explaining targeted communication strategies and by highlighting fairness concerns, this study provides a valuable framework for future research aimed at enhancing transparency, accountability, and inclusivity in social media-driven climate campaigns.</li>
</ul>

<h3>Title: Deep learning-based Visual Measurement Extraction within an Adaptive Digital Twin Framework from Limited Data Using Transfer Learning</h3>
<ul>
<li><strong>Authors: </strong>Mehrdad Shafiei Dizaji</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05403">https://arxiv.org/abs/2410.05403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05403">https://arxiv.org/pdf/2410.05403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05403]] Deep learning-based Visual Measurement Extraction within an Adaptive Digital Twin Framework from Limited Data Using Transfer Learning(https://arxiv.org/abs/2410.05403)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Digital Twins technology is revolutionizing decision-making in scientific research by integrating models and simulations with real-time data. Unlike traditional Structural Health Monitoring methods, which rely on computationally intensive Digital Image Correlation and have limitations in real-time data integration, this research proposes a novel approach using Artificial Intelligence. Specifically, Convolutional Neural Networks are employed to analyze structural behaviors in real-time by correlating Digital Image Correlation speckle pattern images with deformation fields. Initially focusing on two-dimensional speckle patterns, the research extends to three-dimensional applications using stereo-paired images for comprehensive deformation analysis. This method overcomes computational challenges by utilizing a mix of synthetically generated and authentic speckle pattern images for training the Convolutional Neural Networks. The models are designed to be robust and versatile, offering a promising alternative to traditional measurement techniques and paving the way for advanced applications in three-dimensional modeling. This advancement signifies a shift towards more efficient and dynamic structural health monitoring by leveraging the power of Artificial Intelligence for real-time simulation and analysis.</li>
</ul>

<h3>Title: Enhanced Super-Resolution Training via Mimicked Alignment for Real-World Scenes</h3>
<ul>
<li><strong>Authors: </strong>Omar Elezabi, Zongwei Wu, Radu Timofte</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05410">https://arxiv.org/abs/2410.05410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05410">https://arxiv.org/pdf/2410.05410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05410]] Enhanced Super-Resolution Training via Mimicked Alignment for Real-World Scenes(https://arxiv.org/abs/2410.05410)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Image super-resolution methods have made significant strides with deep learning techniques and ample training data. However, they face challenges due to inherent misalignment between low-resolution (LR) and high-resolution (HR) pairs in real-world datasets. In this study, we propose a novel plug-and-play module designed to mitigate these misalignment issues by aligning LR inputs with HR images during training. Specifically, our approach involves mimicking a novel LR sample that aligns with HR while preserving the degradation characteristics of the original LR samples. This module seamlessly integrates with any SR model, enhancing robustness against misalignment. Importantly, it can be easily removed during inference, therefore without introducing any parameters on the conventional SR models. We comprehensively evaluate our method on synthetic and real-world datasets, demonstrating its effectiveness across a spectrum of SR models, including traditional CNNs and state-of-the-art Transformers. The source codes will be publicly made available at this https URL .</li>
</ul>

<h3>Title: STOP! Camera Spoofing via the in-Vehicle IP Network</h3>
<ul>
<li><strong>Authors: </strong>Dror Peri, Avishai Wool</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05417">https://arxiv.org/abs/2410.05417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05417">https://arxiv.org/pdf/2410.05417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05417]] STOP! Camera Spoofing via the in-Vehicle IP Network(https://arxiv.org/abs/2410.05417)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack</a></li>
<li><strong>Abstract: </strong>Autonomous driving and advanced driver assistance systems (ADAS) rely on cameras to control the driving. In many prior approaches an attacker aiming to stop the vehicle had to send messages on the specialized and better-defended CAN bus. We suggest an easier alternative: manipulate the IP-based network communication between the camera and the ADAS logic, inject fake images of stop signs or red lights into the video stream, and let the ADAS stop the car safely. We created an attack tool that successfully exploits the GigE Vision protocol. Then we analyze two classes of passive anomaly detectors to identify such attacks: protocol-based detectors and video-based detectors. We implemented multiple detectors of both classes and evaluated them on data collected from our test vehicle and also on data from the public BDD corpus. Our results show that such detectors are effective against naive adversaries, but sophisticated adversaries can evade detection. Finally, we propose a novel class of active defense mechanisms that randomly adjust camera parameters during the video transmission, and verify that the received images obey the requested adjustments. Within this class we focus on a specific implementation, the width-varying defense, which randomly modifies the width of every frame. Beyond its function as an anomaly detector, this defense is also a protective measure against certain attacks: by distorting injected image patches it prevents their recognition by the ADAS logic. We demonstrate the effectiveness of the width-varying defense through theoretical analysis and by an extensive evaluation of several types of attack in a wide range of realistic road driving conditions. The best the attack was able to achieve against this defense was injecting a stop sign for a duration of 0.2 seconds, with a success probability of 0.2%, whereas stopping a vehicle requires about 2.5 seconds.</li>
</ul>

<h3>Title: Diffusion Imitation from Observation</h3>
<ul>
<li><strong>Authors: </strong>Bo-Ruei Huang, Chun-Kai Yang, Chun-Mao Lai, Dai-Jie Wu, Shao-Hua Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05429">https://arxiv.org/abs/2410.05429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05429">https://arxiv.org/pdf/2410.05429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05429]] Diffusion Imitation from Observation(https://arxiv.org/abs/2410.05429)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Learning from observation (LfO) aims to imitate experts by learning from state-only demonstrations without requiring action labels. Existing adversarial imitation learning approaches learn a generator agent policy to produce state transitions that are indistinguishable to a discriminator that learns to classify agent and expert state transitions. Despite its simplicity in formulation, these methods are often sensitive to hyperparameters and brittle to train. Motivated by the recent success of diffusion models in generative modeling, we propose to integrate a diffusion model into the adversarial imitation learning from observation framework. Specifically, we employ a diffusion model to capture expert and agent transitions by generating the next state, given the current state. Then, we reformulate the learning objective to train the diffusion model as a binary classifier and use it to provide "realness" rewards for policy learning. Our proposed framework, Diffusion Imitation from Observation (DIFO), demonstrates superior performance in various continuous control domains, including navigation, locomotion, manipulation, and games. Project page: this https URL</li>
</ul>

<h3>Title: A Functional Extension of Semi-Structured Networks</h3>
<ul>
<li><strong>Authors: </strong>David Rügamer, and Bernard X.W. Liew, Zainab Altai, Almond Stöcker</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP, stat.CO, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05430">https://arxiv.org/abs/2410.05430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05430">https://arxiv.org/pdf/2410.05430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05430]] A Functional Extension of Semi-Structured Networks(https://arxiv.org/abs/2410.05430)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Semi-structured networks (SSNs) merge the structures familiar from additive models with deep neural networks, allowing the modeling of interpretable partial feature effects while capturing higher-order non-linearities at the same time. A significant challenge in this integration is maintaining the interpretability of the additive model component. Inspired by large-scale biomechanics datasets, this paper explores extending SSNs to functional data. Existing methods in functional data analysis are promising but often not expressive enough to account for all interactions and non-linearities and do not scale well to large datasets. Although the SSN approach presents a compelling potential solution, its adaptation to functional data remains complex. In this work, we propose a functional SSN method that retains the advantageous properties of classical functional regression approaches while also improving scalability. Our numerical experiments demonstrate that this approach accurately recovers underlying signals, enhances predictive performance, and performs favorably compared to competing methods.</li>
</ul>

<h3>Title: Continuous Ensemble Weather Forecasting with Diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Martin Andrae, Tomas Landelius, Joel Oskarsson, Fredrik Lindsten</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05431">https://arxiv.org/abs/2410.05431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05431">https://arxiv.org/pdf/2410.05431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05431]] Continuous Ensemble Weather Forecasting with Diffusion models(https://arxiv.org/abs/2410.05431)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Weather forecasting has seen a shift in methods from numerical simulations to data-driven systems. While initial research in the area focused on deterministic forecasting, recent works have used diffusion models to produce skillful ensemble forecasts. These models are trained on a single forecasting step and rolled out autoregressively. However, they are computationally expensive and accumulate errors for high temporal resolution due to the many rollout steps. We address these limitations with Continuous Ensemble Forecasting, a novel and flexible method for sampling ensemble forecasts in diffusion models. The method can generate temporally consistent ensemble trajectories completely in parallel, with no autoregressive steps. Continuous Ensemble Forecasting can also be combined with autoregressive rollouts to yield forecasts at an arbitrary fine temporal resolution without sacrificing accuracy. We demonstrate that the method achieves competitive results for global weather forecasting with good probabilistic properties.</li>
</ul>

<h3>Title: Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback</h3>
<ul>
<li><strong>Authors: </strong>Sanjiban Choudhury, Paloma Sodhi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05434">https://arxiv.org/abs/2410.05434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05434">https://arxiv.org/pdf/2410.05434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05434]] Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback(https://arxiv.org/abs/2410.05434)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) show impressive decision-making abilities, current methods lack a mechanism for automatic self-improvement from errors during task execution. We propose LEAP, an iterative fine-tuning framework that continually improves LLM agents using feedback from AI expert teachers. Our key insight is to equip the expert teachers with a privileged state -- information that is available during training but hidden at test time. This allows even weak experts to provide precise guidance, significantly improving the student agent's performance without access to privileged information at test time. We evaluate LEAP on diverse decision-making benchmarks, including text-based games (ALFWorld), web navigation (WebShop), and interactive coding (Intercode Bash). Our experiments show that LEAP (1) outperforms behavior cloning and ReAct baselines (2) enables weak student models (e.g., Llama3-8B) to exceed the performance of strong teacher models (GPT4-o), and (3) allows weak models to self-improve using privileged versions of themselves. We also provide a theoretical analysis showing that LEAP's success hinges on balancing privileged information with the student's realizability, which we empirically validate. Our code is available at this https URL</li>
</ul>

<h3>Title: Discovering distinctive elements of biomedical datasets for high-performance exploration</h3>
<ul>
<li><strong>Authors: </strong>Md Tauhidul Islam, Lei Xing</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05436">https://arxiv.org/abs/2410.05436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05436">https://arxiv.org/pdf/2410.05436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05436]] Discovering distinctive elements of biomedical datasets for high-performance exploration(https://arxiv.org/abs/2410.05436)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability</a></li>
<li><strong>Abstract: </strong>The human brain represents an object by small elements and distinguishes two objects based on the difference in elements. Discovering the distinctive elements of high-dimensional datasets is therefore critical in numerous perception-driven biomedical and clinical studies. However, currently there is no available method for reliable extraction of distinctive elements of high-dimensional biomedical and clinical datasets. Here we present an unsupervised deep learning technique namely distinctive element analysis (DEA), which extracts the distinctive data elements using high-dimensional correlative information of the datasets. DEA at first computes a large number of distinctive parts of the data, then filters and condenses the parts into DEA elements by employing a unique kernel-driven triple-optimization network. DEA has been found to improve the accuracy by up to 45% in comparison to the traditional techniques in applications such as disease detection from medical images, gene ranking and cell recognition from single cell RNA sequence (scRNA-seq) datasets. Moreover, DEA allows user-guided manipulation of the intermediate calculation process and thus offers intermediate results with better interpretability.</li>
</ul>

<h3>Title: DAAL: Density-Aware Adaptive Line Margin Loss for Multi-Modal Deep Metric Learning</h3>
<ul>
<li><strong>Authors: </strong>Hadush Hailu Gebrerufael, Anil Kumar Tiwari, Gaurav Neupane</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05438">https://arxiv.org/abs/2410.05438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05438">https://arxiv.org/pdf/2410.05438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05438]] DAAL: Density-Aware Adaptive Line Margin Loss for Multi-Modal Deep Metric Learning(https://arxiv.org/abs/2410.05438)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-modal deep metric learning is crucial for effectively capturing diverse representations in tasks such as face verification, fine-grained object recognition, and product search. Traditional approaches to metric learning, whether based on distance or margin metrics, primarily emphasize class separation, often overlooking the intra-class distribution essential for multi-modal feature learning. In this context, we propose a novel loss function called Density-Aware Adaptive Margin Loss(DAAL), which preserves the density distribution of embeddings while encouraging the formation of adaptive sub-clusters within each class. By employing an adaptive line strategy, DAAL not only enhances intra-class variance but also ensures robust inter-class separation, facilitating effective multi-modal representation. Comprehensive experiments on benchmark fine-grained datasets demonstrate the superior performance of DAAL, underscoring its potential in advancing retrieval applications and multi-modal deep metric learning.</li>
</ul>

<h3>Title: Can LLMs Understand Time Series Anomalies?</h3>
<ul>
<li><strong>Authors: </strong>Zihao Zhou, Rose Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05440">https://arxiv.org/abs/2410.05440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05440">https://arxiv.org/pdf/2410.05440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05440]] Can LLMs Understand Time Series Anomalies?(https://arxiv.org/abs/2410.05440)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have gained popularity in time series forecasting, but their potential for anomaly detection remains largely unexplored. Our study investigates whether LLMs can understand and detect anomalies in time series data, focusing on zero-shot and few-shot scenarios. Inspired by conjectures about LLMs' behavior from time series forecasting research, we formulate key hypotheses about LLMs' capabilities in time series anomaly detection. We design and conduct principled experiments to test each of these hypotheses. Our investigation reveals several surprising findings about LLMs for time series: 1. LLMs understand time series better as *images* rather than as text 2. LLMs did not demonstrate enhanced performance when prompted to engage in *explicit reasoning* about time series analysis 3. Contrary to common beliefs, LLM's understanding of time series *do not* stem from their repetition biases or arithmetic abilities 4. LLMs' behaviors and performance in time series analysis *vary significantly* across different model architectures This study provides the first comprehensive analysis of contemporary LLM capabilities in time series anomaly detection. Our results suggest that while LLMs can understand time series anomalies, many common conjectures based on their reasoning capabilities do not hold. These insights pave the way for more effective LLM-based approaches in time series analysis, bridging the gap between forecasting and anomaly detection applications.</li>
</ul>

<h3>Title: A Deep Learning-Based Approach for Mangrove Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Lucas José Velôso de Souza, Ingrid Valverde Reis Zreik, Adrien Salem-Sermanet, Nacéra Seghouani, Lionel Pourchier</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05443">https://arxiv.org/abs/2410.05443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05443">https://arxiv.org/pdf/2410.05443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05443]] A Deep Learning-Based Approach for Mangrove Monitoring(https://arxiv.org/abs/2410.05443)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Mangroves are dynamic coastal ecosystems that are crucial to environmental health, economic stability, and climate resilience. The monitoring and preservation of mangroves are of global importance, with remote sensing technologies playing a pivotal role in these efforts. The integration of cutting-edge artificial intelligence with satellite data opens new avenues for ecological monitoring, potentially revolutionizing conservation strategies at a time when the protection of natural resources is more crucial than ever. The objective of this work is to provide a comprehensive evaluation of recent deep-learning models on the task of mangrove segmentation. We first introduce and make available a novel open-source dataset, MagSet-2, incorporating mangrove annotations from the Global Mangrove Watch and satellite images from Sentinel-2, from mangrove positions all over the world. We then benchmark three architectural groups, namely convolutional, transformer, and mamba models, using the created dataset. The experimental outcomes further validate the deep learning community's interest in the Mamba model, which surpasses other architectures in all metrics.</li>
</ul>

<h3>Title: Aligning LLMs to Be Robust Against Prompt Injection</h3>
<ul>
<li><strong>Authors: </strong>Sizhe Chen, Arman Zharmagambetov, Saeed Mahloujifar, Kamalika Chaudhuri, Chuan Guo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05451">https://arxiv.org/abs/2410.05451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05451">https://arxiv.org/pdf/2410.05451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05451]] Aligning LLMs to Be Robust Against Prompt Injection(https://arxiv.org/abs/2410.05451)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are becoming increasingly prevalent in modern software systems, interfacing between the user and the internet to assist with tasks that require advanced language understanding. To accomplish these tasks, the LLM often uses external data sources such as user documents, web retrieval, results from API calls, etc. This opens up new avenues for attackers to manipulate the LLM via prompt injection. Adversarial prompts can be carefully crafted and injected into external data sources to override the user's intended instruction and instead execute a malicious instruction. Prompt injection attacks constitute a major threat to LLM security, making the design and implementation of practical countermeasures of paramount importance. To this end, we show that alignment can be a powerful tool to make LLMs more robust against prompt injection. Our method -- SecAlign -- first builds an alignment dataset by simulating prompt injection attacks and constructing pairs of desirable and undesirable responses. Then, we apply existing alignment techniques to fine-tune the LLM to be robust against these simulated attacks. Our experiments show that SecAlign robustifies the LLM substantially with a negligible hurt on model utility. Moreover, SecAlign's protection generalizes to strong attacks unseen in training. Specifically, the success rate of state-of-the-art GCG-based prompt injections drops from 56% to 2% in Mistral-7B after our alignment process. Our code is released at this https URL</li>
</ul>

<h3>Title: Automatic Identification and Visualization of Group Training Activities Using Wearable Data</h3>
<ul>
<li><strong>Authors: </strong>Barak Gahtan, Shany Funk, Einat Kodesh, Itay Ketko, Tsvi Kuflik, Alex M. Bronstein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05452">https://arxiv.org/abs/2410.05452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05452">https://arxiv.org/pdf/2410.05452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05452]] Automatic Identification and Visualization of Group Training Activities Using Wearable Data(https://arxiv.org/abs/2410.05452)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>Human Activity Recognition (HAR) identifies daily activities from time-series data collected by wearable devices like smartwatches. Recent advancements in Internet of Things (IoT), cloud computing, and low-cost sensors have broadened HAR applications across fields like healthcare, biometrics, sports, and personal fitness. However, challenges remain in efficiently processing the vast amounts of data generated by these devices and developing models that can accurately recognize a wide range of activities from continuous recordings, without relying on predefined activity training sessions. This paper presents a comprehensive framework for imputing, analyzing, and identifying activities from wearable data, specifically targeting group training scenarios without explicit activity sessions. Our approach is based on data collected from 135 soldiers wearing Garmin 55 smartwatches over six months. The framework integrates multiple data streams, handles missing data through cross-domain statistical methods, and identifies activities with high accuracy using machine learning (ML). Additionally, we utilized statistical analysis techniques to evaluate the performance of each individual within the group, providing valuable insights into their respective positions in the group in an easy-to-understand visualization. These visualizations facilitate easy understanding of performance metrics, enhancing group interactions and informing individualized training programs. We evaluate our framework through traditional train-test splits and out-of-sample scenarios, focusing on the model's generalization capabilities. Additionally, we address sleep data imputation without relying on ML, improving recovery analysis. Our findings demonstrate the potential of wearable data for accurately identifying group activities, paving the way for intelligent, data-driven training solutions.</li>
</ul>

<h3>Title: Testing Credibility of Public and Private Surveys through the Lens of Regression</h3>
<ul>
<li><strong>Authors: </strong>Debabrota Basu, Sourav Chakraborty, Debarshi Chanda, Buddha Dev Das, Arijit Ghosh, Arnab Ray</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05458">https://arxiv.org/abs/2410.05458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05458">https://arxiv.org/pdf/2410.05458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05458]] Testing Credibility of Public and Private Surveys through the Lens of Regression(https://arxiv.org/abs/2410.05458)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Testing whether a sample survey is a credible representation of the population is an important question to ensure the validity of any downstream research. While this problem, in general, does not have an efficient solution, one might take a task-based approach and aim to understand whether a certain data analysis tool, like linear regression, would yield similar answers both on the population and the sample survey. In this paper, we design an algorithm to test the credibility of a sample survey in terms of linear regression. In other words, we design an algorithm that can certify if a sample survey is good enough to guarantee the correctness of data analysis done using linear regression tools. Nowadays, one is naturally concerned about data privacy in surveys. Thus, we further test the credibility of surveys published in a differentially private manner. Specifically, we focus on Local Differential Privacy (LDP), which is a standard technique to ensure privacy in surveys where the survey participants might not trust the aggregator. We extend our algorithm to work even when the data analysis has been done using surveys with LDP. In the process, we also propose an algorithm that learns with high probability the guarantees a linear regression model on a survey published with LDP. Our algorithm also serves as a mechanism to learn linear regression models from data corrupted with noise coming from any subexponential distribution. We prove that it achieves the optimal estimation error bound for $\ell_1$ linear regression, which might be of broader interest. We prove the theoretical correctness of our algorithms while trying to reduce the sample complexity for both public and private surveys. We also numerically demonstrate the performance of our algorithms on real and synthetic datasets.</li>
</ul>

<h3>Title: From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency</h3>
<ul>
<li><strong>Authors: </strong>Kaiyue Wen, Huaqing Zhang, Hongzhou Lin, Jingzhao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05459">https://arxiv.org/abs/2410.05459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05459">https://arxiv.org/pdf/2410.05459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05459]] From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency(https://arxiv.org/abs/2410.05459)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-thought (CoT) significantly enhances the reasoning performance of large language models (LLM). While current theoretical studies often attribute this improvement to increased expressiveness and computational capacity, we argue that expressiveness is not the primary limitation in the LLM regime, as current large models will fail on simple tasks. Using a parity-learning setup, we demonstrate that CoT can substantially improve sample efficiency even when the representation power is sufficient. Specifically, with CoT, a transformer can learn the function within polynomial samples, whereas without CoT, the required sample size is exponential. Additionally, we show that CoT simplifies the learning process by introducing sparse sequential dependencies among input tokens, and leads to a sparse and interpretable attention. We validate our theoretical analysis with both synthetic and real-world experiments, confirming that sparsity in attention layers is a key factor of the improvement induced by CoT.</li>
</ul>

<h3>Title: LevAttention: Time, Space, and Streaming Efficient Algorithm for Heavy Attentions</h3>
<ul>
<li><strong>Authors: </strong>Ravindran Kannan, Chiranjib Bhattacharyya, Praneeth Kacham, David P. Woodruff</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05462">https://arxiv.org/abs/2410.05462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05462">https://arxiv.org/pdf/2410.05462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05462]] LevAttention: Time, Space, and Streaming Efficient Algorithm for Heavy Attentions(https://arxiv.org/abs/2410.05462)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>A central problem related to transformers can be stated as follows: given two $n \times d$ matrices $Q$ and $K$, and a non-negative function $f$, define the matrix $A$ as follows: (1) apply the function $f$ to each entry of the $n \times n$ matrix $Q K^T$, and then (2) normalize each of the row sums of $A$ to be equal to $1$. The matrix $A$ can be computed in $O(n^2 d)$ time assuming $f$ can be applied to a number in constant time, but the quadratic dependence on $n$ is prohibitive in applications where it corresponds to long context lengths. For a large class of functions $f$, we show how to find all the ``large attention scores", i.e., entries of $A$ which are at least a positive value $\varepsilon$, in time with linear dependence on $n$ (i.e., $n \cdot \textrm{poly}(d/\varepsilon)$) for a positive parameter $\varepsilon > 0$. Our class of functions include all functions $f$ of the form $f(x) = |x|^p$, as explored recently in transformer models. Using recently developed tools from randomized numerical linear algebra, we prove that for any $K$, there is a ``universal set" $U \subset [n]$ of size independent of $n$, such that for any $Q$ and any row $i$, the large attention scores $A_{i,j}$ in row $i$ of $A$ all have $j \in U$. We also find $U$ in $n \cdot \textrm{poly}(d/\varepsilon)$ time. Notably, we (1) make no assumptions on the data, (2) our workspace does not grow with $n$, and (3) our algorithms can be computed in streaming and parallel settings. We call the attention mechanism that uses only the subset of keys in the universal set as LevAttention since our algorithm to identify the universal set $U$ is based on leverage scores. We empirically show the benefits of our scheme for vision transformers, showing how to train new models that use our universal set while training as well, showing that our model is able to consistently select ``important keys'' during training.</li>
</ul>

<h3>Title: Progressive distillation induces an implicit curriculum</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Panigrahi, Bingbin Liu, Sadhika Malladi, Andrej Risteski, Surbhi Goel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05464">https://arxiv.org/abs/2410.05464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05464">https://arxiv.org/pdf/2410.05464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05464]] Progressive distillation induces an implicit curriculum(https://arxiv.org/abs/2410.05464)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Knowledge distillation leverages a teacher model to improve the training of a student model. A persistent challenge is that a better teacher does not always yield a better student, to which a common mitigation is to use additional supervision from several ``intermediate'' teachers. One empirically validated variant of this principle is progressive distillation, where the student learns from successive intermediate checkpoints of the teacher. Using sparse parity as a sandbox, we identify an implicit curriculum as one mechanism through which progressive distillation accelerates the student's learning. This curriculum is available only through the intermediate checkpoints but not the final converged one, and imparts both empirical acceleration and a provable sample complexity benefit to the student. We then extend our investigation to Transformers trained on probabilistic context-free grammars (PCFGs) and real-world pre-training datasets (Wikipedia and Books). Through probing the teacher model, we identify an analogous implicit curriculum where the model progressively learns features that capture longer context. Our theoretical and empirical findings on sparse parity, complemented by empirical observations on more complex tasks, highlight the benefit of progressive distillation via implicit curriculum across setups.</li>
</ul>

<h3>Title: Herd Mentality in Augmentation -- Not a Good Idea! A Robust Multi-stage Approach towards Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Monu, Rohan Raju Dhanakshirur</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05466">https://arxiv.org/abs/2410.05466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05466">https://arxiv.org/pdf/2410.05466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05466]] Herd Mentality in Augmentation -- Not a Good Idea! A Robust Multi-stage Approach towards Deepfake Detection(https://arxiv.org/abs/2410.05466)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rapid increase in deepfake technology has raised significant concerns about digital media integrity. Detecting deepfakes is crucial for safeguarding digital media. However, most standard image classifiers fail to distinguish between fake and real faces. Our analysis reveals that this failure is due to the model's inability to explicitly focus on the artefacts typically in deepfakes. We propose an enhanced architecture based on the GenConViT model, which incorporates weighted loss and update augmentation techniques and includes masked eye pretraining. This proposed model improves the F1 score by 1.71% and the accuracy by 4.34% on the Celeb-DF v2 dataset. The source code for our model is available at this https URL</li>
</ul>

<h3>Title: PH-Dropout: Prctical Epistemic Uncertainty Quantification for View Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Chuanhao Sun, Thanos Triantafyllou, Anthos Makris, Maja Drmač, Kai Xu, Luo Mai, Mahesh K. Marina</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05468">https://arxiv.org/abs/2410.05468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05468">https://arxiv.org/pdf/2410.05468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05468]] PH-Dropout: Prctical Epistemic Uncertainty Quantification for View Synthesis(https://arxiv.org/abs/2410.05468)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>View synthesis using Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) has demonstrated impressive fidelity in rendering real-world scenarios. However, practical methods for accurate and efficient epistemic Uncertainty Quantification (UQ) in view synthesis are lacking. Existing approaches for NeRF either introduce significant computational overhead (e.g., ``10x increase in training time" or ``10x repeated training") or are limited to specific uncertainty conditions or models. Notably, GS models lack any systematic approach for comprehensive epistemic UQ. This capability is crucial for improving the robustness and scalability of neural view synthesis, enabling active model updates, error estimation, and scalable ensemble modeling based on uncertainty. In this paper, we revisit NeRF and GS-based methods from a function approximation perspective, identifying key differences and connections in 3D representation learning. Building on these insights, we introduce PH-Dropout (Post hoc Dropout), the first real-time and accurate method for epistemic uncertainty estimation that operates directly on pre-trained NeRF and GS models. Extensive evaluations validate our theoretical findings and demonstrate the effectiveness of PH-Dropout.</li>
</ul>

<h3>Title: Image Watermarks are Removable Using Controllable Regeneration from Clean Noise</h3>
<ul>
<li><strong>Authors: </strong>Yepeng Liu, Yiren Song, Hai Ci, Yu Zhang, Haofan Wang, Mike Zheng Shou, Yuheng Bu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05470">https://arxiv.org/abs/2410.05470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05470">https://arxiv.org/pdf/2410.05470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05470]] Image Watermarks are Removable Using Controllable Regeneration from Clean Noise(https://arxiv.org/abs/2410.05470)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Image watermark techniques provide an effective way to assert ownership, deter misuse, and trace content sources, which has become increasingly essential in the era of large generative models. A critical attribute of watermark techniques is their robustness against various manipulations. In this paper, we introduce a watermark removal approach capable of effectively nullifying the state of the art watermarking techniques. Our primary insight involves regenerating the watermarked image starting from a clean Gaussian noise via a controllable diffusion model, utilizing the extracted semantic and spatial features from the watermarked image. The semantic control adapter and the spatial control network are specifically trained to control the denoising process towards ensuring image quality and enhancing consistency between the cleaned image and the original watermarked image. To achieve a smooth trade-off between watermark removal performance and image consistency, we further propose an adjustable and controllable regeneration scheme. This scheme adds varying numbers of noise steps to the latent representation of the watermarked image, followed by a controlled denoising process starting from this noisy latent representation. As the number of noise steps increases, the latent representation progressively approaches clean Gaussian noise, facilitating the desired trade-off. We apply our watermark removal methods across various watermarking techniques, and the results demonstrate that our methods offer superior visual consistency/quality and enhanced watermark removal performance compared to existing regeneration approaches.</li>
</ul>

<h3>Title: Neural machine translation system for Lezgian, Russian and Azerbaijani languages</h3>
<ul>
<li><strong>Authors: </strong>Alidar Asvarov, Andrey Grabovoy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05472">https://arxiv.org/abs/2410.05472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05472">https://arxiv.org/pdf/2410.05472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05472]] Neural machine translation system for Lezgian, Russian and Azerbaijani languages(https://arxiv.org/abs/2410.05472)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We release the first neural machine translation system for translation between Russian, Azerbaijani and the endangered Lezgian languages, as well as monolingual and parallel datasets collected and aligned for training and evaluating the system. Multiple experiments are conducted to identify how different sets of training language pairs and data domains can influence the resulting translation quality. We achieve BLEU scores of 26.14 for Lezgian-Azerbaijani, 22.89 for Azerbaijani-Lezgian, 29.48 for Lezgian-Russian and 24.25 for Russian-Lezgian pairs. The quality of zero-shot translation is assessed on a Large Language Model, showing its high level of fluency in Lezgian. However, the model often refuses to translate, justifying itself with its incompetence. We contribute our translation model along with the collected parallel and monolingual corpora and sentence encoder for the Lezgian language.</li>
</ul>

<h3>Title: R-Bench: Are your Large Multimodal Model Robust to Real-world Corruptions?</h3>
<ul>
<li><strong>Authors: </strong>Chunyi Li, Jianbo Zhang, Zicheng Zhang, Haoning Wu, Yuan Tian, Wei Sun, Guo Lu, Xiaohong Liu, Xiongkuo Min, Weisi Lin, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05474">https://arxiv.org/abs/2410.05474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05474">https://arxiv.org/pdf/2410.05474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05474]] R-Bench: Are your Large Multimodal Model Robust to Real-world Corruptions?(https://arxiv.org/abs/2410.05474)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The outstanding performance of Large Multimodal Models (LMMs) has made them widely applied in vision-related tasks. However, various corruptions in the real world mean that images will not be as ideal as in simulations, presenting significant challenges for the practical application of LMMs. To address this issue, we introduce R-Bench, a benchmark focused on the **Real-world Robustness of LMMs**. Specifically, we: (a) model the complete link from user capture to LMMs reception, comprising 33 corruption dimensions, including 7 steps according to the corruption sequence, and 7 groups based on low-level attributes; (b) collect reference/distorted image dataset before/after corruption, including 2,970 question-answer pairs with human labeling; (c) propose comprehensive evaluation for absolute/relative robustness and benchmark 20 mainstream LMMs. Results show that while LMMs can correctly handle the original reference images, their performance is not stable when faced with distorted images, and there is a significant gap in robustness compared to the human visual system. We hope that R-Bench will inspire improving the robustness of LMMs, **extending them from experimental simulations to the real-world application**. Check this https URL for details.</li>
</ul>

<h3>Title: Neural Networks Decoded: Targeted and Robust Analysis of Neural Network Decisions via Causal Explanations and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Alec F. Diallo, Vaishak Belle, Paul Patras</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05484">https://arxiv.org/abs/2410.05484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05484">https://arxiv.org/pdf/2410.05484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05484]] Neural Networks Decoded: Targeted and Robust Analysis of Neural Network Decisions via Causal Explanations and Reasoning(https://arxiv.org/abs/2410.05484)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Despite their success and widespread adoption, the opaque nature of deep neural networks (DNNs) continues to hinder trust, especially in critical applications. Current interpretability solutions often yield inconsistent or oversimplified explanations, or require model changes that compromise performance. In this work, we introduce TRACER, a novel method grounded in causal inference theory designed to estimate the causal dynamics underpinning DNN decisions without altering their architecture or compromising their performance. Our approach systematically intervenes on input features to observe how specific changes propagate through the network, affecting internal activations and final outputs. Based on this analysis, we determine the importance of individual features, and construct a high-level causal map by grouping functionally similar layers into cohesive causal nodes, providing a structured and interpretable view of how different parts of the network influence the decisions. TRACER further enhances explainability by generating counterfactuals that reveal possible model biases and offer contrastive explanations for misclassifications. Through comprehensive evaluations across diverse datasets, we demonstrate TRACER's effectiveness over existing methods and show its potential for creating highly compressed yet accurate models, illustrating its dual versatility in both understanding and optimizing DNNs.</li>
</ul>

<h3>Title: Transformers learn variable-order Markov chains in-context</h3>
<ul>
<li><strong>Authors: </strong>Ruida Zhou, Chao Tian, Suhas Diggavi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05493">https://arxiv.org/abs/2410.05493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05493">https://arxiv.org/pdf/2410.05493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05493]] Transformers learn variable-order Markov chains in-context(https://arxiv.org/abs/2410.05493)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models have demonstrated impressive in-context learning (ICL) capability. However, it is still unclear how the underlying transformers accomplish it, especially in more complex scenarios. Toward this goal, several recent works studied how transformers learn fixed-order Markov chains (FOMC) in context, yet natural languages are more suitably modeled by variable-order Markov chains (VOMC), i.e., context trees (CTs). In this work, we study the ICL of VOMC by viewing language modeling as a form of data compression and focus on small alphabets and low-order VOMCs. This perspective allows us to leverage mature compression algorithms, such as context-tree weighting (CTW) and prediction by partial matching (PPM) algorithms as baselines, the former of which is Bayesian optimal for a class of CTW priors. We empirically observe a few phenomena: 1) Transformers can indeed learn to compress VOMC in-context, while PPM suffers significantly; 2) The performance of transformers is not very sensitive to the number of layers, and even a two-layer transformer can learn in-context quite well; and 3) Transformers trained and tested on non-CTW priors can significantly outperform the CTW algorithm. To explain these phenomena, we analyze the attention map of the transformers and extract two mechanisms, on which we provide two transformer constructions: 1) A construction with $D+2$ layers that can mimic the CTW algorithm accurately for CTs of maximum order $D$, 2) A 2-layer transformer that utilizes the feed-forward network for probability blending. One distinction from the FOMC setting is that a counting mechanism appears to play an important role. We implement these synthetic transformer layers and show that such hybrid transformers can match the ICL performance of transformers, and more interestingly, some of them can perform even better despite the much-reduced parameter sets.</li>
</ul>

<h3>Title: Privacy Vulnerabilities in Marginals-based Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Steven Golob, Sikha Pentyala, Anuar Maratkhan, Martine De Cock</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05506">https://arxiv.org/abs/2410.05506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05506">https://arxiv.org/pdf/2410.05506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05506]] Privacy Vulnerabilities in Marginals-based Synthetic Data(https://arxiv.org/abs/2410.05506)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, membership infer</a></li>
<li><strong>Abstract: </strong>When acting as a privacy-enhancing technology, synthetic data generation (SDG) aims to maintain a resemblance to the real data while excluding personally-identifiable information. Many SDG algorithms provide robust differential privacy (DP) guarantees to this end. However, we show that the strongest class of SDG algorithms--those that preserve \textit{marginal probabilities}, or similar statistics, from the underlying data--leak information about individuals that can be recovered more efficiently than previously understood. We demonstrate this by presenting a novel membership inference attack, MAMA-MIA, and evaluate it against three seminal DP SDG algorithms: MST, PrivBayes, and Private-GSD. MAMA-MIA leverages knowledge of which SDG algorithm was used, allowing it to learn information about the hidden data more accurately, and orders-of-magnitude faster, than other leading attacks. We use MAMA-MIA to lend insight into existing SDG vulnerabilities. Our approach went on to win the first SNAKE (SaNitization Algorithm under attacK ... $\varepsilon$) competition.</li>
</ul>

<h3>Title: Toward General Object-level Mapping from Sparse Views with 3D Diffusion Priors</h3>
<ul>
<li><strong>Authors: </strong>Ziwei Liao, Binbin Xu, Steven L. Waslander</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05514">https://arxiv.org/abs/2410.05514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05514">https://arxiv.org/pdf/2410.05514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05514]] Toward General Object-level Mapping from Sparse Views with 3D Diffusion Priors(https://arxiv.org/abs/2410.05514)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Object-level mapping builds a 3D map of objects in a scene with detailed shapes and poses from multi-view sensor observations. Conventional methods struggle to build complete shapes and estimate accurate poses due to partial occlusions and sensor noise. They require dense observations to cover all objects, which is challenging to achieve in robotics trajectories. Recent work introduces generative shape priors for object-level mapping from sparse views, but is limited to single-category objects. In this work, we propose a General Object-level Mapping system, GOM, which leverages a 3D diffusion model as shape prior with multi-category support and outputs Neural Radiance Fields (NeRFs) for both texture and geometry for all objects in a scene. GOM includes an effective formulation to guide a pre-trained diffusion model with extra nonlinear constraints from sensor measurements without finetuning. We also develop a probabilistic optimization formulation to fuse multi-view sensor observations and diffusion priors for joint 3D object pose and shape estimation. Our GOM system demonstrates superior multi-category mapping performance from sparse views, and achieves more accurate mapping results compared to state-of-the-art methods on the real-world benchmarks. We will release our code: this https URL.</li>
</ul>

<h3>Title: Generative Portrait Shadow Removal</h3>
<ul>
<li><strong>Authors: </strong>Jae Shin Yoon, Zhixin Shu, Mengwei Ren, Xuaner Zhang, Yannick Hold-Geoffroy, Krishna Kumar Singh, He Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05525">https://arxiv.org/abs/2410.05525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05525">https://arxiv.org/pdf/2410.05525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05525]] Generative Portrait Shadow Removal(https://arxiv.org/abs/2410.05525)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce a high-fidelity portrait shadow removal model that can effectively enhance the image of a portrait by predicting its appearance under disturbing shadows and highlights. Portrait shadow removal is a highly ill-posed problem where multiple plausible solutions can be found based on a single image. While existing works have solved this problem by predicting the appearance residuals that can propagate local shadow distribution, such methods are often incomplete and lead to unnatural predictions, especially for portraits with hard shadows. We overcome the limitations of existing local propagation methods by formulating the removal problem as a generation task where a diffusion model learns to globally rebuild the human appearance from scratch as a condition of an input portrait image. For robust and natural shadow removal, we propose to train the diffusion model with a compositional repurposing framework: a pre-trained text-guided image generation model is first fine-tuned to harmonize the lighting and color of the foreground with a background scene by using a background harmonization dataset; and then the model is further fine-tuned to generate a shadow-free portrait image via a shadow-paired dataset. To overcome the limitation of losing fine details in the latent diffusion model, we propose a guided-upsampling network to restore the original high-frequency details (wrinkles and dots) from the input image. To enable our compositional training framework, we construct a high-fidelity and large-scale dataset using a lightstage capturing system and synthetic graphics simulation. Our generative framework effectively removes shadows caused by both self and external occlusions while maintaining original lighting distribution and high-frequency details. Our method also demonstrates robustness to diverse subjects captured in real environments.</li>
</ul>

<h3>Title: Optimizing Tensor Computation Graphs with Equality Saturation and Monte Carlo Tree Search</h3>
<ul>
<li><strong>Authors: </strong>Jakob Hartmann, Guoliang He, Eiko Yoneki</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05534">https://arxiv.org/abs/2410.05534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05534">https://arxiv.org/pdf/2410.05534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05534]] Optimizing Tensor Computation Graphs with Equality Saturation and Monte Carlo Tree Search(https://arxiv.org/abs/2410.05534)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The real-world effectiveness of deep neural networks often depends on their latency, thereby necessitating optimization techniques that can reduce a model's inference time while preserving its performance. One popular approach is to sequentially rewrite the input computation graph into an equivalent but faster one by replacing individual subgraphs. This approach gives rise to the so-called phase-ordering problem in which the application of one rewrite rule can eliminate the possibility to apply an even better one later on. Recent work has shown that equality saturation, a technique from compiler optimization, can mitigate this issue by first building an intermediate representation (IR) that efficiently stores multiple optimized versions of the input program before extracting the best solution in a second step. In practice, however, memory constraints prevent the IR from capturing all optimized versions and thus reintroduce the phase-ordering problem in the construction phase. In this paper, we present a tensor graph rewriting approach that uses Monte Carlo tree search to build superior IRs by identifying the most promising rewrite rules. We also introduce a novel extraction algorithm that can provide fast and accurate runtime estimates of tensor programs represented in an IR. Our approach improves the inference speedup of neural networks by up to 11% compared to existing methods.</li>
</ul>

<h3>Title: Aiding Global Convergence in Federated Learning via Local Perturbation and Mutual Similarity Information</h3>
<ul>
<li><strong>Authors: </strong>Emanuel Buttaci, Giuseppe Carlo Calafiore</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05545">https://arxiv.org/abs/2410.05545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05545">https://arxiv.org/pdf/2410.05545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05545]] Aiding Global Convergence in Federated Learning via Local Perturbation and Mutual Similarity Information(https://arxiv.org/abs/2410.05545)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated learning has emerged in the last decade as a distributed optimization paradigm due to the rapidly increasing number of portable devices able to support the heavy computational needs related to the training of machine learning models. Federated learning utilizes gradient-based optimization to minimize a loss objective shared across participating agents. To the best of our knowledge, the literature mostly lacks elegant solutions that naturally harness the reciprocal statistical similarity between clients to redesign the optimization procedure. To address this gap, by conceiving the federated network as a similarity graph, we propose a novel modified framework wherein each client locally performs a perturbed gradient step leveraging prior information about other statistically affine clients. We theoretically prove that our procedure, due to a suitably introduced adaptation in the update rule, achieves a quantifiable speedup concerning the exponential contraction factor in the strongly convex case compared with popular algorithms FedAvg and FedProx, here analyzed as baselines. Lastly, we legitimize our conclusions through experimental results on the CIFAR10 and FEMNIST datasets, where we show that our algorithm speeds convergence up to a margin of 30 global rounds compared with FedAvg while modestly improving generalization on unseen data in heterogeneous settings.</li>
</ul>

<h3>Title: On Instruction-Finetuning Neural Machine Translation Models</h3>
<ul>
<li><strong>Authors: </strong>Vikas Raunak, Roman Grundkiewicz, Marcin Junczys-Dowmunt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05553">https://arxiv.org/abs/2410.05553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05553">https://arxiv.org/pdf/2410.05553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05553]] On Instruction-Finetuning Neural Machine Translation Models(https://arxiv.org/abs/2410.05553)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this work, we introduce instruction finetuning for Neural Machine Translation (NMT) models, which distills instruction following capabilities from Large Language Models (LLMs) into orders-of-magnitude smaller NMT models. Our instruction-finetuning recipe for NMT models enables customization of translations for a limited but disparate set of translation-specific tasks. We show that NMT models are capable of following multiple instructions simultaneously and demonstrate capabilities of zero-shot composition of instructions. We also show that through instruction finetuning, traditionally disparate tasks such as formality-controlled machine translation, multi-domain adaptation as well as multi-modal translations can be tackled jointly by a single instruction finetuned NMT model, at a performance level comparable to LLMs such as GPT-3.5-Turbo. To the best of our knowledge, our work is among the first to demonstrate the instruction-following capabilities of traditional NMT models, which allows for faster, cheaper and more efficient serving of customized translations.</li>
</ul>

<h3>Title: Rethinking Weak-to-Strong Augmentation in Source-Free Domain Adaptive Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiuzheng Yang, Song Tang, Yangkuiyi Zhang, Shuaifeng Li, Mao Ye, Jianwei Zhang, Xiatian Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05557">https://arxiv.org/abs/2410.05557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05557">https://arxiv.org/pdf/2410.05557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05557]] Rethinking Weak-to-Strong Augmentation in Source-Free Domain Adaptive Object Detection(https://arxiv.org/abs/2410.05557)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Source-Free domain adaptive Object Detection (SFOD) aims to transfer a detector (pre-trained on source domain) to new unlabelled target domains. Current SFOD methods typically follow the Mean Teacher framework, where weak-to-strong augmentation provides diverse and sharp contrast for self-supervised learning. However, this augmentation strategy suffers from an inherent problem called crucial semantics loss: Due to random, strong disturbance, strong augmentation is prone to losing typical visual components, hindering cross-domain feature extraction. To address this thus-far ignored limitation, this paper introduces a novel Weak-to-Strong Contrastive Learning (WSCoL) approach. The core idea is to distill semantics lossless knowledge in the weak features (from the weak/teacher branch) to guide the representation learning upon the strong features (from the strong/student branch). To achieve this, we project the original features into a shared space using a mapping network, thereby reducing the bias between the weak and strong features. Meanwhile, a weak features-guided contrastive learning is performed in a weak-to-strong manner alternatively. Specifically, we first conduct an adaptation-aware prototype-guided clustering on the weak features to generate pseudo labels for corresponding strong features matched through proposals. Sequentially, we identify positive-negative samples based on the pseudo labels and perform cross-category contrastive learning on the strong features where an uncertainty estimator encourages adaptive background contrast. Extensive experiments demonstrate that WSCoL yields new state-of-the-art performance, offering a built-in mechanism mitigating crucial semantics loss for traditional Mean Teacher framework. The code and data will be released soon.</li>
</ul>

<h3>Title: Narrative-of-Thought: Improving Temporal Reasoning of Large Language Models via Recounted Narratives</h3>
<ul>
<li><strong>Authors: </strong>Xinliang Frederick Zhang, Nick Beauchamp, Lu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05558">https://arxiv.org/abs/2410.05558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05558">https://arxiv.org/pdf/2410.05558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05558]] Narrative-of-Thought: Improving Temporal Reasoning of Large Language Models via Recounted Narratives(https://arxiv.org/abs/2410.05558)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning about time and temporal relations is an integral aspect of human cognition, essential for perceiving the world and navigating our experiences. Though large language models (LLMs) have demonstrated impressive performance in many reasoning tasks, temporal reasoning remains challenging due to its intrinsic complexity. In this work, we first study an essential task of temporal reasoning -- temporal graph generation, to unveil LLMs' inherent, global reasoning capabilities. We show that this task presents great challenges even for the most powerful LLMs, such as GPT-3.5/4. We also notice a significant performance gap by small models (<10B) that lag behind LLMs by 50%. Next, we study how to close this gap with a budget constraint, e.g., not using model finetuning. We propose a new prompting technique tailored for temporal reasoning, Narrative-of-Thought (NoT), that first converts the events set to a Python class, then prompts a small model to generate a temporally grounded narrative, guiding the final generation of a temporal graph. Extensive experiments showcase the efficacy of NoT in improving various metrics. Notably, NoT attains the highest F1 on the Schema-11 evaluation set, while securing an overall F1 on par with GPT-3.5. NoT also achieves the best structural similarity across the board, even compared with GPT-3.5/4. Our code is available at this https URL.</li>
</ul>

<h3>Title: Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification</h3>
<ul>
<li><strong>Authors: </strong>Tao Meng, Ninareh Mehrabi, Palash Goyal, Anil Ramakrishna, Aram Galstyan, Richard Zemel, Kai-Wei Chang, Rahul Gupta, Charith Peris</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05559">https://arxiv.org/abs/2410.05559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05559">https://arxiv.org/pdf/2410.05559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05559]] Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification(https://arxiv.org/abs/2410.05559)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose a constraint learning schema for fine-tuning Large Language Models (LLMs) with attribute control. Given a training corpus and control criteria formulated as a sequence-level constraint on model outputs, our method fine-tunes the LLM on the training corpus while enhancing constraint satisfaction with minimal impact on its utility and generation quality. Specifically, our approach regularizes the LLM training by penalizing the KL divergence between the desired output distribution, which satisfies the constraints, and the LLM's posterior. This regularization term can be approximated by an auxiliary model trained to decompose the sequence-level constraints into token-level guidance, allowing the term to be measured by a closed-form formulation. To further improve efficiency, we design a parallel scheme for concurrently updating both the LLM and the auxiliary model. We evaluate the empirical performance of our approach by controlling the toxicity when training an LLM. We show that our approach leads to an LLM that produces fewer inappropriate responses while achieving competitive performance on benchmarks and a toxicity detection task.</li>
</ul>

<h3>Title: Cyber Threats to Canadian Federal Election: Emerging Threats, Assessment, and Mitigation Strategies</h3>
<ul>
<li><strong>Authors: </strong>Nazmul Islam, Soomin Kim, Mohammad Pirooz, Sasha Shvetsov</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05560">https://arxiv.org/abs/2410.05560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05560">https://arxiv.org/pdf/2410.05560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05560]] Cyber Threats to Canadian Federal Election: Emerging Threats, Assessment, and Mitigation Strategies(https://arxiv.org/abs/2410.05560)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>As Canada prepares for the 2025 federal election, ensuring the integrity and security of the electoral process against cyber threats is crucial. Recent foreign interference in elections globally highlight the increasing sophistication of adversaries in exploiting technical and human vulnerabilities. Such vulnerabilities also exist in Canada's electoral system that relies on a complex network of IT systems, vendors, and personnel. To mitigate these vulnerabilities, a threat assessment is crucial to identify emerging threats, develop incident response capabilities, and build public trust and resilience against cyber threats. Therefore, this paper presents a comprehensive national cyber threat assessment, following the NIST Special Publication 800-30 framework, focusing on identifying and mitigating cybersecurity risks to the upcoming 2025 Canadian federal election. The research identifies three major threats: misinformation, disinformation, and malinformation (MDM) campaigns; attacks on critical infrastructure and election support systems; and espionage by malicious actors. Through detailed analysis, the assessment offers insights into the capabilities, intent, and potential impact of these threats. The paper also discusses emerging technologies and their influence on election security and proposes a multi-faceted approach to risk mitigation ahead of the election.</li>
</ul>

<h3>Title: Rational Metareasoning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>C. Nicolò De Sabbata, Theodore R. Sumers, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05563">https://arxiv.org/abs/2410.05563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05563">https://arxiv.org/pdf/2410.05563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05563]] Rational Metareasoning for Large Language Models(https://arxiv.org/abs/2410.05563)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Being prompted to engage in reasoning has emerged as a core technique for using large language models (LLMs), deploying additional inference-time compute to improve task performance. However, as LLMs increase in both size and adoption, inference costs are correspondingly becoming increasingly burdensome. How, then, might we optimize reasoning's cost-performance tradeoff? This work introduces a novel approach based on computational models of metareasoning used in cognitive science, training LLMs to selectively use intermediate reasoning steps only when necessary. We first develop a reward function that incorporates the Value of Computation by penalizing unnecessary reasoning, then use this reward function with Expert Iteration to train the LLM. Compared to few-shot chain-of-thought prompting and STaR, our method significantly reduces inference costs (20-37\% fewer tokens generated across three models) while maintaining task performance across diverse datasets.</li>
</ul>

<h3>Title: Chain and Causal Attention for Efficient Entity Tracking</h3>
<ul>
<li><strong>Authors: </strong>Erwan Fagnou, Paul Caillon, Blaise Delattre, Alexandre Allauzen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05565">https://arxiv.org/abs/2410.05565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05565">https://arxiv.org/pdf/2410.05565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05565]] Chain and Causal Attention for Efficient Entity Tracking(https://arxiv.org/abs/2410.05565)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates the limitations of transformers for entity-tracking tasks in large language models. We identify a theoretical constraint, showing that transformers require at least $\log_2 (n+1)$ layers to handle entity tracking with $n$ state changes. To address this issue, we propose an efficient and frugal enhancement to the standard attention mechanism, enabling it to manage long-term dependencies more efficiently. By considering attention as an adjacency matrix, our model can track entity states with a single layer. Empirical results demonstrate significant improvements in entity tracking datasets while keeping competitive performance on standard natural language modeling. Our modified attention allows us to achieve the same performance with drastically fewer layers. Additionally, our enhanced mechanism reveals structured internal representations of attention. Extensive experiments on both toy and complex datasets validate our approach. Our contributions include theoretical insights, an improved attention mechanism, and empirical validation.</li>
</ul>

<h3>Title: TaeBench: Improving Quality of Toxic Adversarial Examples</h3>
<ul>
<li><strong>Authors: </strong>Xuan Zhu, Dmitriy Bespalov, Liwen You, Ninad Kulkarni, Yanjun Qi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05573">https://arxiv.org/abs/2410.05573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05573">https://arxiv.org/pdf/2410.05573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05573]] TaeBench: Improving Quality of Toxic Adversarial Examples(https://arxiv.org/abs/2410.05573)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Toxicity text detectors can be vulnerable to adversarial examples - small perturbations to input text that fool the systems into wrong detection. Existing attack algorithms are time-consuming and often produce invalid or ambiguous adversarial examples, making them less useful for evaluating or improving real-world toxicity content moderators. This paper proposes an annotation pipeline for quality control of generated toxic adversarial examples (TAE). We design model-based automated annotation and human-based quality verification to assess the quality requirements of TAE. Successful TAE should fool a target toxicity model into making benign predictions, be grammatically reasonable, appear natural like human-generated text, and exhibit semantic toxicity. When applying these requirements to more than 20 state-of-the-art (SOTA) TAE attack recipes, we find many invalid samples from a total of 940k raw TAE attack generations. We then utilize the proposed pipeline to filter and curate a high-quality TAE dataset we call TaeBench (of size 264k). Empirically, we demonstrate that TaeBench can effectively transfer-attack SOTA toxicity content moderation models and services. Our experiments also show that TaeBench with adversarial training achieve significant improvements of the robustness of two toxicity detectors.</li>
</ul>

<h3>Title: ClaimBrush: A Novel Framework for Automated Patent Claim Refinement Based on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Seiya Kawano, Hirofumi Nonaka, Koichiro Yoshino</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05575">https://arxiv.org/abs/2410.05575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05575">https://arxiv.org/pdf/2410.05575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05575]] ClaimBrush: A Novel Framework for Automated Patent Claim Refinement Based on Large Language Models(https://arxiv.org/abs/2410.05575)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatic refinement of patent claims in patent applications is crucial from the perspective of intellectual property strategy. In this paper, we propose ClaimBrush, a novel framework for automated patent claim refinement that includes a dataset and a rewriting model. We constructed a dataset for training and evaluating patent claim rewriting models by collecting a large number of actual patent claim rewriting cases from the patent examination process. Using the constructed dataset, we built an automatic patent claim rewriting model by fine-tuning a large language model. Furthermore, we enhanced the performance of the automatic patent claim rewriting model by applying preference optimization based on a prediction model of patent examiners' Office Actions. The experimental results showed that our proposed rewriting model outperformed heuristic baselines and zero-shot learning in state-of-the-art large language models. Moreover, preference optimization based on patent examiners' preferences boosted the performance of patent claim refinement.</li>
</ul>

<h3>Title: Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?</h3>
<ul>
<li><strong>Authors: </strong>Fırat Öncel, Matthias Bethge, Beyza Ermis, Mirco Ravanelli, Cem Subakan, Çağatay Yıldız</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05581">https://arxiv.org/abs/2410.05581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05581">https://arxiv.org/pdf/2410.05581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05581]] Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?(https://arxiv.org/abs/2410.05581)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the last decade, the generalization and adaptation abilities of deep learning models were typically evaluated on fixed training and test distributions. Contrary to traditional deep learning, large language models (LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text corpora curated from the Internet with minimal human intervention, and (iii) trained in an online fashion. These stark contrasts prevent researchers from transferring lessons learned on model generalization and adaptation in deep learning contexts to LLMs. To this end, our short paper introduces empirical observations that aim to shed light on further training of already pretrained language models. Specifically, we demonstrate that training a model on a text domain could degrade its perplexity on the test portion of the same domain. We observe with our subsequent analysis that the performance degradation is positively correlated with the similarity between the additional and the original pretraining dataset of the LLM. Our further token-level perplexity observations reveals that the perplexity degradation is due to a handful of tokens that are not informative about the domain. We hope these findings will guide us in determining when to adapt a model vs when to rely on its foundational capabilities.</li>
</ul>

<h3>Title: TeaserGen: Generating Teasers for Long Documentaries</h3>
<ul>
<li><strong>Authors: </strong>Weihan Xu, Paul Pu Liang, Haven Kim, Julian McAuley, Taylor Berg-Kirkpatrick, Hao-Wen Dong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05586">https://arxiv.org/abs/2410.05586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05586">https://arxiv.org/pdf/2410.05586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05586]] TeaserGen: Generating Teasers for Long Documentaries(https://arxiv.org/abs/2410.05586)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Teasers are an effective tool for promoting content in entertainment, commercial and educational fields. However, creating an effective teaser for long videos is challenging for it requires long-range multimodal modeling on the input videos, while necessitating maintaining audiovisual alignments, managing scene changes and preserving factual accuracy for the output teasers. Due to the lack of a publicly-available dataset, progress along this research direction has been hindered. In this work, we present DocumentaryNet, a collection of 1,269 documentaries paired with their teasers, featuring multimodal data streams of video, speech, music, sound effects and narrations. With DocumentaryNet, we propose a new two-stage system for generating teasers from long documentaries. The proposed TeaserGen system first generates the teaser narration from the transcribed narration of the documentary using a pretrained large language model, and then selects the most relevant visual content to accompany the generated narration through language-vision models. For narration-video matching, we explore two approaches: a pretraining-based model using pretrained contrastive language-vision models and a deep sequential model that learns the mapping between the narrations and visuals. Our experimental results show that the pretraining-based approach is more effective at identifying relevant visual content than directly trained deep autoregressive models.</li>
</ul>

<h3>Title: ParallelSpec: Parallel Drafter for Efficient Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Zilin Xiao, Hongming Zhang, Tao Ge, Siru Ouyang, Vicente Ordonez, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05589">https://arxiv.org/abs/2410.05589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05589">https://arxiv.org/pdf/2410.05589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05589]] ParallelSpec: Parallel Drafter for Efficient Speculative Decoding(https://arxiv.org/abs/2410.05589)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding has proven to be an efficient solution to large language model (LLM) inference, where the small drafter predicts future tokens at a low cost, and the target model is leveraged to verify them in parallel. However, most existing works still draft tokens auto-regressively to maintain sequential dependency in language modeling, which we consider a huge computational burden in speculative decoding. We present ParallelSpec, an alternative to auto-regressive drafting strategies in state-of-the-art speculative decoding approaches. In contrast to auto-regressive drafting in the speculative stage, we train a parallel drafter to serve as an efficient speculative model. ParallelSpec learns to efficiently predict multiple future tokens in parallel using a single model, and it can be integrated into any speculative decoding framework that requires aligning the output distributions of the drafter and the target model with minimal training cost. Experimental results show that ParallelSpec accelerates baseline methods in latency up to 62% on text generation benchmarks from different domains, and it achieves 2.84X overall speedup on the Llama-2-13B model using third-party evaluation criteria.</li>
</ul>

<h3>Title: TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Gihyun Kwon, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05591">https://arxiv.org/abs/2410.05591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05591">https://arxiv.org/pdf/2410.05591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05591]] TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation(https://arxiv.org/abs/2410.05591)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite significant advancements in customizing text-to-image and video generation models, generating images and videos that effectively integrate multiple personalized concepts remains a challenging task. To address this, we present TweedieMix, a novel method for composing customized diffusion models during the inference phase. By analyzing the properties of reverse diffusion sampling, our approach divides the sampling process into two stages. During the initial steps, we apply a multiple object-aware sampling technique to ensure the inclusion of the desired target objects. In the later steps, we blend the appearances of the custom concepts in the de-noised image space using Tweedie's formula. Our results demonstrate that TweedieMix can generate multiple personalized concepts with higher fidelity than existing methods. Moreover, our framework can be effortlessly extended to image-to-video diffusion models, enabling the generation of videos that feature multiple personalized concepts. Results and source code are in our anonymous project page.</li>
</ul>

<h3>Title: When Graph Neural Networks Meet Dynamic Mode Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Dai Shi, Lequan Lin, Andi Han, Zhiyong Wang, Yi Guo, Junbin Gao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05593">https://arxiv.org/abs/2410.05593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05593">https://arxiv.org/pdf/2410.05593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05593]] When Graph Neural Networks Meet Dynamic Mode Decomposition(https://arxiv.org/abs/2410.05593)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have emerged as fundamental tools for a wide range of prediction tasks on graph-structured data. Recent studies have drawn analogies between GNN feature propagation and diffusion processes, which can be interpreted as dynamical systems. In this paper, we delve deeper into this perspective by connecting the dynamics in GNNs to modern Koopman theory and its numerical method, Dynamic Mode Decomposition (DMD). We illustrate how DMD can estimate a low-rank, finite-dimensional linear operator based on multiple states of the system, effectively approximating potential nonlinear interactions between nodes in the graph. This approach allows us to capture complex dynamics within the graph accurately and efficiently. We theoretically establish a connection between the DMD-estimated operator and the original dynamic operator between system states. Building upon this foundation, we introduce a family of DMD-GNN models that effectively leverage the low-rank eigenfunctions provided by the DMD algorithm. We further discuss the potential of enhancing our approach by incorporating domain-specific constraints such as symmetry into the DMD computation, allowing the corresponding GNN models to respect known physical properties of the underlying system. Our work paves the path for applying advanced dynamical system analysis tools via GNNs. We validate our approach through extensive experiments on various learning tasks, including directed graphs, large-scale graphs, long-range interactions, and spatial-temporal graphs. We also empirically verify that our proposed models can serve as powerful encoders for link prediction tasks. The results demonstrate that our DMD-enhanced GNNs achieve state-of-the-art performance, highlighting the effectiveness of integrating DMD into GNN frameworks.</li>
</ul>

<h3>Title: Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Ming Shan Hee, Aditi Kumaresan, Roy Ka-Wei Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05600">https://arxiv.org/abs/2410.05600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05600">https://arxiv.org/pdf/2410.05600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05600]] Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning(https://arxiv.org/abs/2410.05600)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The widespread presence of hate speech on the internet, including formats such as text-based tweets and vision-language memes, poses a significant challenge to digital platform safety. Recent research has developed detection models tailored to specific modalities; however, there is a notable gap in transferring detection capabilities across different formats. This study conducts extensive experiments using few-shot in-context learning with large language models to explore the transferability of hate speech detection between modalities. Our findings demonstrate that text-based hate speech examples can significantly enhance the classification accuracy of vision-language hate speech. Moreover, text-based demonstrations outperform vision-language demonstrations in few-shot learning settings. These results highlight the effectiveness of cross-modality knowledge transfer and offer valuable insights for improving hate speech detection systems.</li>
</ul>

<h3>Title: ReFIR: Grounding Large Restoration Models with Retrieval Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Hang Guo, Tao Dai, Zhihao Ouyang, Taolin Zhang, Yaohua Zha, Bin Chen, Shu-tao Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05601">https://arxiv.org/abs/2410.05601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05601">https://arxiv.org/pdf/2410.05601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05601]] ReFIR: Grounding Large Restoration Models with Retrieval Augmentation(https://arxiv.org/abs/2410.05601)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion-based Large Restoration Models (LRMs) have significantly improved photo-realistic image restoration by leveraging the internal knowledge embedded within model weights. However, existing LRMs often suffer from the hallucination dilemma, i.e., producing incorrect contents or textures when dealing with severe degradations, due to their heavy reliance on limited internal knowledge. In this paper, we propose an orthogonal solution called the Retrieval-augmented Framework for Image Restoration (ReFIR), which incorporates retrieved images as external knowledge to extend the knowledge boundary of existing LRMs in generating details faithful to the original scene. Specifically, we first introduce the nearest neighbor lookup to retrieve content-relevant high-quality images as reference, after which we propose the cross-image injection to modify existing LRMs to utilize high-quality textures from retrieved images. Thanks to the additional external knowledge, our ReFIR can well handle the hallucination challenge and facilitate faithfully results. Extensive experiments demonstrate that ReFIR can achieve not only high-fidelity but also realistic restoration results. Importantly, our ReFIR requires no training and is adaptable to various LRMs.</li>
</ul>

<h3>Title: Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition</h3>
<ul>
<li><strong>Authors: </strong>Zheyang Xiong, Ziyang Cai, John Cooper, Albert Ge, Vasilis Papageorgiou, Zack Sifakis, Angeliki Giannou, Ziqian Lin, Liu Yang, Saurabh Agarwal, Grigorios G Chrysos, Samet Oymak, Kangwook Lee, Dimitris Papailiopoulos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05603">https://arxiv.org/abs/2410.05603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05603">https://arxiv.org/pdf/2410.05603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05603]] Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition(https://arxiv.org/abs/2410.05603)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable in-context learning (ICL) capabilities. In this study, we explore a surprising phenomenon related to ICL: LLMs can perform multiple, computationally distinct ICL tasks simultaneously, during a single inference call, a capability we term "task superposition". We provide empirical evidence of this phenomenon across various LLM families and scales and show that this phenomenon emerges even if we train the model to in-context learn one task at a time. We offer theoretical explanations that this capability is well within the expressive power of transformers. We also explore how LLMs internally compose task vectors during superposition. Furthermore, we show that larger models can solve more ICL tasks in parallel, and better calibrate their output distribution. Our findings offer insights into the latent capabilities of LLMs, further substantiate the perspective of "LLMs as superposition of simulators", and raise questions about the mechanisms enabling simultaneous task execution.</li>
</ul>

<h3>Title: Multimodal Large Language Models and Tunings: Vision, Language, Sensors, Audio, and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Soyeon Caren Han, Feiqi Cao, Josiah Poon, Roberto Navigli</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05608">https://arxiv.org/abs/2410.05608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05608">https://arxiv.org/pdf/2410.05608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05608]] Multimodal Large Language Models and Tunings: Vision, Language, Sensors, Audio, and Beyond(https://arxiv.org/abs/2410.05608)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This tutorial explores recent advancements in multimodal pretrained and large models, capable of integrating and processing diverse data forms such as text, images, audio, and video. Participants will gain an understanding of the foundational concepts of multimodality, the evolution of multimodal research, and the key technical challenges addressed by these models. We will cover the latest multimodal datasets and pretrained models, including those beyond vision and language. Additionally, the tutorial will delve into the intricacies of multimodal large models and instruction tuning strategies to optimise performance for specific tasks. Hands-on laboratories will offer practical experience with state-of-the-art multimodal models, demonstrating real-world applications like visual storytelling and visual question answering. This tutorial aims to equip researchers, practitioners, and newcomers with the knowledge and skills to leverage multimodal AI. ACM Multimedia 2024 is the ideal venue for this tutorial, aligning perfectly with our goal of understanding multimodal pretrained and large language models, and their tuning mechanisms.</li>
</ul>

<h3>Title: Chain-of-Thoughts for Molecular Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yunhui Jang, Jaehyung Kim, Sungsoo Ahn</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05610">https://arxiv.org/abs/2410.05610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05610">https://arxiv.org/pdf/2410.05610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05610]] Chain-of-Thoughts for Molecular Understanding(https://arxiv.org/abs/2410.05610)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The adaptation of large language models (LLMs) to chemistry has shown promising performance in molecular understanding tasks, such as generating a text description from a molecule. However, proper reasoning based on molecular structural information remains a significant challenge, e.g., even advanced LLMs such as GPT-4o struggle to identify functional groups which are crucial for inferring the molecular property of interest. To address this limitation, we propose StructCoT, a structure-aware chain-of-thought (CoT) that enhances LLMs' understanding of molecular structures by explicitly injecting the key structural features of molecules. Moreover, we introduce two fine-tuning frameworks for adapting the existing LLMs to use our StructCoT. Our experiments demonstrate that incorporating StructCoT with our fine-tuning frameworks leads to consistent improvements in both molecular understanding tasks.</li>
</ul>

<h3>Title: Leveraging free energy in pretraining model selection for improved fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Michael Munn, Susan Wei</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05612">https://arxiv.org/abs/2410.05612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05612">https://arxiv.org/pdf/2410.05612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05612]] Leveraging free energy in pretraining model selection for improved fine-tuning(https://arxiv.org/abs/2410.05612)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in artificial intelligence have been fueled by the development of foundation models such as BERT, GPT, T5, and Vision Transformers. These models are first pretrained on vast and diverse datasets and then adapted to specific downstream tasks, often with significantly less data. However, the mechanisms behind the success of this ubiquitous pretrain-then-adapt paradigm remain underexplored, particularly the characteristics of pretraining checkpoints that lend themselves to good downstream adaptation. We introduce a Bayesian model selection criterion, called the downstream free energy, which quantifies a checkpoint's adaptability by measuring the concentration of nearby favorable parameters for the downstream task. We demonstrate that this free energy criterion can be effectively implemented without access to the downstream data or prior knowledge of the downstream task. Furthermore, we provide empirical evidence that the free energy criterion reliably correlates with improved fine-tuning performance, offering a principled approach to predicting model adaptability.</li>
</ul>

<h3>Title: Stereotype or Personalization? User Identity Biases Chatbot Recommendations</h3>
<ul>
<li><strong>Authors: </strong>Anjali Kantharuban, Jeremiah Milbauer, Emma Strubell, Graham Neubig</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05613">https://arxiv.org/abs/2410.05613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05613">https://arxiv.org/pdf/2410.05613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05613]] Stereotype or Personalization? User Identity Biases Chatbot Recommendations(https://arxiv.org/abs/2410.05613)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We demonstrate that when people use large language models (LLMs) to generate recommendations, the LLMs produce responses that reflect both what the user wants and who the user is. While personalized recommendations are often desired by users, it can be difficult in practice to distinguish cases of bias from cases of personalization: we find that models generate racially stereotypical recommendations regardless of whether the user revealed their identity intentionally through explicit indications or unintentionally through implicit cues. We argue that chatbots ought to transparently indicate when recommendations are influenced by a user's revealed identity characteristics, but observe that they currently fail to do so. Our experiments show that even though a user's revealed identity significantly influences model recommendations (p < 0.001), model responses obfuscate this fact in response to user queries. This bias and lack of transparency occurs consistently across multiple popular consumer LLMs (gpt-4o-mini, gpt-4-turbo, llama-3-70B, and claude-3.5) and for four American racial groups.</li>
</ul>

<h3>Title: Remote Sensing Image Segmentation Using Vision Mamba and Multi-Scale Multi-Frequency Feature Fusion</h3>
<ul>
<li><strong>Authors: </strong>Yice Cao, Chenchen Liu, Zhenhua Wu, Wenxin Yao, Liu Xiong, Jie Chen, Zhixiang Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05624">https://arxiv.org/abs/2410.05624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05624">https://arxiv.org/pdf/2410.05624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05624]] Remote Sensing Image Segmentation Using Vision Mamba and Multi-Scale Multi-Frequency Feature Fusion(https://arxiv.org/abs/2410.05624)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>As remote sensing imaging technology continues to advance and evolve, processing high-resolution and diversified satellite imagery to improve segmentation accuracy and enhance interpretation efficiency emerg as a pivotal area of investigation within the realm of remote sensing. Although segmentation algorithms based on CNNs and Transformers achieve significant progress in performance, balancing segmentation accuracy and computational complexity remains challenging, limiting their wide application in practical tasks. To address this, this paper introduces state space model (SSM) and proposes a novel hybrid semantic segmentation network based on vision Mamba (CVMH-UNet). This method designs a cross-scanning visual state space block (CVSSBlock) that uses cross 2D scanning (CS2D) to fully capture global information from multiple directions, while by incorporating convolutional neural network branches to overcome the constraints of Vision Mamba (VMamba) in acquiring local information, this approach facilitates a comprehensive analysis of both global and local features. Furthermore, to address the issue of limited discriminative power and the difficulty in achieving detailed fusion with direct skip connections, a multi-frequency multi-scale feature fusion block (MFMSBlock) is designed. This module introduces multi-frequency information through 2D discrete cosine transform (2D DCT) to enhance information utilization and provides additional scale local detail information through point-wise convolution branches. Finally, it aggregates multi-scale information along the channel dimension, achieving refined feature fusion. Findings from experiments conducted on renowned datasets of remote sensing imagery demonstrate that proposed CVMH-UNet achieves superior segmentation performance while maintaining low computational complexity, outperforming surpassing current leading-edge segmentation algorithms.</li>
</ul>

<h3>Title: CLOSER: Towards Better Representation Learning for Few-Shot Class-Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Junghun Oh, Sungyong Baik, Kyoung Mu Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05627">https://arxiv.org/abs/2410.05627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05627">https://arxiv.org/pdf/2410.05627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05627]] CLOSER: Towards Better Representation Learning for Few-Shot Class-Incremental Learning(https://arxiv.org/abs/2410.05627)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Aiming to incrementally learn new classes with only few samples while preserving the knowledge of base (old) classes, few-shot class-incremental learning (FSCIL) faces several challenges, such as overfitting and catastrophic forgetting. Such a challenging problem is often tackled by fixing a feature extractor trained on base classes to reduce the adverse effects of overfitting and forgetting. Under such formulation, our primary focus is representation learning on base classes to tackle the unique challenge of FSCIL: simultaneously achieving the transferability and the discriminability of the learned representation. Building upon the recent efforts for enhancing transferability, such as promoting the spread of features, we find that trying to secure the spread of features within a more confined feature space enables the learned representation to strike a better balance between transferability and discriminability. Thus, in stark contrast to prior beliefs that the inter-class distance should be maximized, we claim that the closer different classes are, the better for FSCIL. The empirical results and analysis from the perspective of information bottleneck theory justify our simple yet seemingly counter-intuitive representation learning method, raising research questions and suggesting alternative research directions. The code is available at this https URL.</li>
</ul>

<h3>Title: Vector-ICL: In-context Learning with Continuous Vector Representations</h3>
<ul>
<li><strong>Authors: </strong>Yufan Zhuang, Chandan Singh, Liyuan Liu, Jingbo Shang, Jianfeng Gao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05629">https://arxiv.org/abs/2410.05629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05629">https://arxiv.org/pdf/2410.05629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05629]] Vector-ICL: In-context Learning with Continuous Vector Representations(https://arxiv.org/abs/2410.05629)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable in-context learning (ICL) capabilities on textual data. We explore whether these capabilities can be extended to continuous vectors from diverse domains, obtained from black-box pretrained encoders. By aligning input data with an LLM's embedding space through lightweight projectors, we observe that LLMs can effectively process and learn from these projected vectors, which we term Vector-ICL. In particular, we find that pretraining projectors with general language modeling objectives enables Vector-ICL, while task-specific finetuning further enhances performance. In our experiments across various tasks and modalities, including text reconstruction, numerical function regression, text classification, summarization, molecule captioning, time-series classification, graph classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL and domain-specific model or tuning. We further conduct analyses and case studies, indicating the potential of LLMs to process vector representations beyond traditional token-based paradigms.</li>
</ul>

<h3>Title: Federated Neural Nonparametric Point Processes</h3>
<ul>
<li><strong>Authors: </strong>Hui Chen, Hengyu Liu, Yaqiong Li, Xuhui Fan, Zhilin Zhao, Feng Zhou, Christopher John Quinn, Longbing Cao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05637">https://arxiv.org/abs/2410.05637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05637">https://arxiv.org/pdf/2410.05637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05637]] Federated Neural Nonparametric Point Processes(https://arxiv.org/abs/2410.05637)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Temporal point processes (TPPs) are effective for modeling event occurrences over time, but they struggle with sparse and uncertain events in federated systems, where privacy is a major concern. To address this, we propose \textit{FedPP}, a Federated neural nonparametric Point Process model. FedPP integrates neural embeddings into Sigmoidal Gaussian Cox Processes (SGCPs) on the client side, which is a flexible and expressive class of TPPs, allowing it to generate highly flexible intensity functions that capture client-specific event dynamics and uncertainties while efficiently summarizing historical records. For global aggregation, FedPP introduces a divergence-based mechanism that communicates the distributions of SGCPs' kernel hyperparameters between the server and clients, while keeping client-specific parameters local to ensure privacy and personalization. FedPP effectively captures event uncertainty and sparsity, and extensive experiments demonstrate its superior performance in federated settings, particularly with KL divergence and Wasserstein distance-based global aggregation.</li>
</ul>

<h3>Title: Time Series Classification of Supraglacial Lakes Evolution over Greenland Ice Sheet</h3>
<ul>
<li><strong>Authors: </strong>Emam Hossain, Md Osman Gani, Devon Dunmire, Aneesh Subramanian, Hammad Younas</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05638">https://arxiv.org/abs/2410.05638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05638">https://arxiv.org/pdf/2410.05638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05638]] Time Series Classification of Supraglacial Lakes Evolution over Greenland Ice Sheet(https://arxiv.org/abs/2410.05638)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Greenland Ice Sheet (GrIS) has emerged as a significant contributor to global sea level rise, primarily due to increased meltwater runoff. Supraglacial lakes, which form on the ice sheet surface during the summer months, can impact ice sheet dynamics and mass loss; thus, better understanding these lakes' seasonal evolution and dynamics is an important task. This study presents a computationally efficient time series classification approach that uses Gaussian Mixture Models (GMMs) of the Reconstructed Phase Spaces (RPSs) to identify supraglacial lakes based on their seasonal evolution: 1) those that refreeze at the end of the melt season, 2) those that drain during the melt season, and 3) those that become buried, remaining liquid insulated a few meters beneath the surface. Our approach uses time series data from the Sentinel-1 and Sentinel-2 satellites, which utilize microwave and visible radiation, respectively. Evaluated on a GrIS-wide dataset, the RPS-GMM model, trained on a single representative sample per class, achieves 85.46% accuracy with Sentinel-1 data alone and 89.70% with combined Sentinel-1 and Sentinel-2 data. This performance significantly surpasses existing machine learning and deep learning models which require a large training data. The results demonstrate the robustness of the RPS-GMM model in capturing the complex temporal dynamics of supraglacial lakes with minimal training data.</li>
</ul>

<h3>Title: DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ranchi Zhao, Zhen Leng Thai, Yifan Zhang, Shengding Hu, Yunqi Ba, Jie Zhou, Jie Cai, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05639">https://arxiv.org/abs/2410.05639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05639">https://arxiv.org/pdf/2410.05639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05639]] DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models(https://arxiv.org/abs/2410.05639)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the quality of this data is challenging due to its sheer volume and the absence of sample-level quality annotations and enhancements. In this paper, we introduce DecorateLM, a data engineering method designed to refine the pretraining corpus through data rating, tagging and editing. Specifically, DecorateLM rates texts against quality criteria, tags texts with hierarchical labels, and edits texts into a more formalized format. Due to the massive size of the pretraining corpus, adopting an LLM for decorating the entire corpus is less efficient. Therefore, to balance performance with efficiency, we curate a meticulously annotated training corpus for DecorateLM using a large language model and distill data engineering expertise into a compact 1.2 billion parameter small language model (SLM). We then apply DecorateLM to enhance 100 billion tokens of the training corpus, selecting 45 billion tokens that exemplify high quality and diversity for the further training of another 1.2 billion parameter LLM. Our results demonstrate that employing such high-quality data can significantly boost model performance, showcasing a powerful approach to enhance the quality of the pretraining corpus.</li>
</ul>

<h3>Title: Score-Based Variational Inference for Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Xue, Penghao Cai, Xiaojun Yuan, Xiqi Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05646">https://arxiv.org/abs/2410.05646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05646">https://arxiv.org/pdf/2410.05646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05646]] Score-Based Variational Inference for Inverse Problems(https://arxiv.org/abs/2410.05646)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing diffusion-based methods for inverse problems sample from the posterior using score functions and accept the generated random samples as solutions. In applications that posterior mean is preferred, we have to generate multiple samples from the posterior which is time-consuming. In this work, by analyzing the probability density evolution of the conditional reverse diffusion process, we prove that the posterior mean can be achieved by tracking the mean of each reverse diffusion step. Based on that, we establish a framework termed reverse mean propagation (RMP) that targets the posterior mean directly. We show that RMP can be implemented by solving a variational inference problem, which can be further decomposed as minimizing a reverse KL divergence at each reverse step. We further develop an algorithm that optimizes the reverse KL divergence with natural gradient descent using score functions and propagates the mean at each reverse step. Experiments demonstrate the validity of the theory of our framework and show that our algorithm outperforms state-of-the-art algorithms on reconstruction performance with lower computational complexity in various inverse problems.</li>
</ul>

<h3>Title: SIA-OVD: Shape-Invariant Adapter for Bridging the Image-Region Gap in Open-Vocabulary Detection</h3>
<ul>
<li><strong>Authors: </strong>Zishuo Wang, Wenhao Zhou, Jinglin Xu, Yuxin Peng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05650">https://arxiv.org/abs/2410.05650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05650">https://arxiv.org/pdf/2410.05650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05650]] SIA-OVD: Shape-Invariant Adapter for Bridging the Image-Region Gap in Open-Vocabulary Detection(https://arxiv.org/abs/2410.05650)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Open-vocabulary detection (OVD) aims to detect novel objects without instance-level annotations to achieve open-world object detection at a lower cost. Existing OVD methods mainly rely on the powerful open-vocabulary image-text alignment capability of Vision-Language Pretrained Models (VLM) such as CLIP. However, CLIP is trained on image-text pairs and lacks the perceptual ability for local regions within an image, resulting in the gap between image and region representations. Directly using CLIP for OVD causes inaccurate region classification. We find the image-region gap is primarily caused by the deformation of region feature maps during region of interest (RoI) extraction. To mitigate the inaccurate region classification in OVD, we propose a new Shape-Invariant Adapter named SIA-OVD to bridge the image-region gap in the OVD task. SIA-OVD learns a set of feature adapters for regions with different shapes and designs a new adapter allocation mechanism to select the optimal adapter for each region. The adapted region representations can align better with text representations learned by CLIP. Extensive experiments demonstrate that SIA-OVD effectively improves the classification accuracy for regions by addressing the gap between images and regions caused by shape deformation. SIA-OVD achieves substantial improvements over representative methods on the COCO-OVD benchmark. The code is available at this https URL.</li>
</ul>

<h3>Title: ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler</h3>
<ul>
<li><strong>Authors: </strong>Serin Yang, Taesung Kwon, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05651">https://arxiv.org/abs/2410.05651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05651">https://arxiv.org/pdf/2410.05651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05651]] ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler(https://arxiv.org/abs/2410.05651)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V) diffusion models has greatly enhanced video generation, especially in terms of keyframe interpolation. However, current image-to-video diffusion models, while powerful in generating videos from a single conditioning frame, need adaptation for two-frame (start & end) conditioned generation, which is essential for effective bounded interpolation. Unfortunately, existing approaches that fuse temporally forward and backward paths in parallel often suffer from off-manifold issues, leading to artifacts or requiring multiple iterative re-noising steps. In this work, we introduce a novel, bidirectional sampling strategy to address these off-manifold issues without requiring extensive re-noising or fine-tuning. Our method employs sequential sampling along both forward and backward paths, conditioned on the start and end frames, respectively, ensuring more coherent and on-manifold generation of intermediate frames. Additionally, we incorporate advanced guidance techniques, CFG++ and DDS, to further enhance the interpolation process. By integrating these, our method achieves state-of-the-art performance, efficiently generating high-quality, smooth videos between keyframes. On a single 3090 GPU, our method can interpolate 25 frames at 1024 x 576 resolution in just 195 seconds, establishing it as a leading solution for keyframe interpolation.</li>
</ul>

<h3>Title: A Blockchain-Enhanced Framework for Privacy and Data Integrity in Crowdsourced Drone Services</h3>
<ul>
<li><strong>Authors: </strong>Junaid Akram, Ali Anaissi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05653">https://arxiv.org/abs/2410.05653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05653">https://arxiv.org/pdf/2410.05653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05653]] A Blockchain-Enhanced Framework for Privacy and Data Integrity in Crowdsourced Drone Services(https://arxiv.org/abs/2410.05653)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, fair</a></li>
<li><strong>Abstract: </strong>We present an innovative framework that integrates consumer-grade drones into bushfire management, addressing both service improvement and data privacy concerns under Australia's Privacy Act 1988. This system establishes a marketplace where bushfire management authorities, as data consumers, access critical information from drone operators, who serve as data providers. The framework employs local differential privacy to safeguard the privacy of data providers from all system entities, ensuring compliance with privacy standards. Additionally, a blockchain-based solution facilitates fair data and fee exchanges while maintaining immutable records for enhanced accountability. Validated through a proof-of-concept implementation, the framework's scalability and adaptability make it well-suited for large-scale, real-world applications in bushfire management.</li>
</ul>

<h3>Title: Robust Transfer Learning for Active Level Set Estimation with Locally Adaptive Gaussian Process Prior</h3>
<ul>
<li><strong>Authors: </strong>Giang Ngo, Dang Nguyen, Sunil Gupta</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05660">https://arxiv.org/abs/2410.05660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05660">https://arxiv.org/pdf/2410.05660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05660]] Robust Transfer Learning for Active Level Set Estimation with Locally Adaptive Gaussian Process Prior(https://arxiv.org/abs/2410.05660)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The objective of active level set estimation for a black-box function is to precisely identify regions where the function values exceed or fall below a specified threshold by iteratively performing function evaluations to gather more information about the function. This becomes particularly important when function evaluations are costly, drastically limiting our ability to acquire large datasets. A promising way to sample-efficiently model the black-box function is by incorporating prior knowledge from a related function. However, this approach risks slowing down the estimation task if the prior knowledge is irrelevant or misleading. In this paper, we present a novel transfer learning method for active level set estimation that safely integrates a given prior knowledge while constantly adjusting it to guarantee a robust performance of a level set estimation algorithm even when the prior knowledge is irrelevant. We theoretically analyze this algorithm to show that it has a better level set convergence compared to standard transfer learning approaches that do not make any adjustment to the prior. Additionally, extensive experiments across multiple datasets confirm the effectiveness of our method when applied to various different level set estimation algorithms as well as different transfer learning scenarios.</li>
</ul>

<h3>Title: Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Siqi Wang, Zhengyu Chen, Bei Li, Keqing He, Min Zhang, Jingang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05661">https://arxiv.org/abs/2410.05661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05661">https://arxiv.org/pdf/2410.05661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05661]] Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models(https://arxiv.org/abs/2410.05661)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The scaling of large language models (LLMs) is a critical research area for the efficiency and effectiveness of model training and deployment. Our work investigates the transferability and discrepancies of scaling laws between Dense Models and Mixture of Experts (MoE) models. Through a combination of theoretical analysis and extensive experiments, including consistent loss scaling, optimal batch size and learning rate scaling, and resource allocation strategies scaling, our findings reveal that the power-law scaling framework also applies to MoE Models, indicating that the fundamental principles governing the scaling behavior of these models are preserved, even though the architecture differs. Additionally, MoE Models demonstrate superior generalization, resulting in lower testing losses with the same training compute budget compared to Dense Models. These findings indicate the scaling consistency and transfer generalization capabilities of MoE Models, providing new insights for optimizing MoE Model training and deployment strategies.</li>
</ul>

<h3>Title: Federated Learning with Dynamic Client Arrival and Departure: Convergence and Rapid Adaptation via Initial Model Construction</h3>
<ul>
<li><strong>Authors: </strong>Zhan-Lun Chang, Dong-Jun Han, Rohit Parasnis, Seyyedali Hosseinalipour, Christopher G. Brinton</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05662">https://arxiv.org/abs/2410.05662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05662">https://arxiv.org/pdf/2410.05662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05662]] Federated Learning with Dynamic Client Arrival and Departure: Convergence and Rapid Adaptation via Initial Model Construction(https://arxiv.org/abs/2410.05662)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>While most existing federated learning (FL) approaches assume a fixed set of clients in the system, in practice, clients can dynamically leave or join the system depending on their needs or interest in the specific task. This dynamic FL setting introduces several key challenges: (1) the objective function dynamically changes depending on the current set of clients, unlike traditional FL approaches that maintain a static optimization goal; (2) the current global model may not serve as the best initial point for the next FL rounds and could potentially lead to slow adaptation, given the possibility of clients leaving or joining the system. In this paper, we consider a dynamic optimization objective in FL that seeks the optimal model tailored to the currently active set of clients. Building on our probabilistic framework that provides direct insights into how the arrival and departure of different types of clients influence the shifts in optimal points, we establish an upper bound on the optimality gap, accounting for factors such as stochastic gradient noise, local training iterations, non-IIDness of data distribution, and deviations between optimal points caused by dynamic client pattern. We also propose an adaptive initial model construction strategy that employs weighted averaging guided by gradient similarity, prioritizing models trained on clients whose data characteristics align closely with the current one, thereby enhancing adaptability to the current clients. The proposed approach is validated on various datasets and FL algorithms, demonstrating robust performance across diverse client arrival and departure patterns, underscoring its effectiveness in dynamic FL environments.</li>
</ul>

<h3>Title: Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Saemi Moon, Minjong Lee, Sangdon Park, Dongwoo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05664">https://arxiv.org/abs/2410.05664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05664">https://arxiv.org/pdf/2410.05664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05664]] Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning(https://arxiv.org/abs/2410.05664)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>As text-to-image diffusion models become advanced enough for commercial applications, there is also increasing concern about their potential for malicious and harmful use. Model unlearning has been proposed to mitigate the concerns by removing undesired and potentially harmful information from the pre-trained model. So far, the success of unlearning is mainly measured by whether the unlearned model can generate a target concept while maintaining image quality. However, unlearning is typically tested under limited scenarios, and the side effects of unlearning have barely been studied in the current literature. In this work, we thoroughly analyze unlearning under various scenarios with five key aspects. Our investigation reveals that every method has side effects or limitations, especially in more complex and realistic situations. By releasing our comprehensive evaluation framework with the source codes and artifacts, we hope to inspire further research in this area, leading to more reliable and effective unlearning methods.</li>
</ul>

<h3>Title: T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design</h3>
<ul>
<li><strong>Authors: </strong>Jiachen Li, Qian Long, Jian Zheng, Xiaofeng Gao, Robinson Piramuthu, Wenhu Chen, William Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05677">https://arxiv.org/abs/2410.05677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05677">https://arxiv.org/pdf/2410.05677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05677]] T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design(https://arxiv.org/abs/2410.05677)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we focus on enhancing a diffusion-based text-to-video (T2V) model during the post-training phase by distilling a highly capable consistency model from a pretrained T2V model. Our proposed method, T2V-Turbo-v2, introduces a significant advancement by integrating various supervision signals, including high-quality training data, reward model feedback, and conditional guidance, into the consistency distillation process. Through comprehensive ablation studies, we highlight the crucial importance of tailoring datasets to specific learning objectives and the effectiveness of learning from diverse reward models for enhancing both the visual quality and text-video alignment. Additionally, we highlight the vast design space of conditional guidance strategies, which centers on designing an effective energy function to augment the teacher ODE solver. We demonstrate the potential of this approach by extracting motion guidance from the training datasets and incorporating it into the ODE solver, showcasing its effectiveness in improving the motion quality of the generated videos with the improved motion-related metrics from VBench and T2V-CompBench. Empirically, our T2V-Turbo-v2 establishes a new state-of-the-art result on VBench, with a Total score of 85.13, surpassing proprietary systems such as Gen-3 and Kling.</li>
</ul>

<h3>Title: Extreme Value Modelling of Feature Residuals for Anomaly Detection in Dynamic Graphs</h3>
<ul>
<li><strong>Authors: </strong>Sevvandi Kandanaarachchi, Conrad Sanderson, Rob J. Hyndman</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05687">https://arxiv.org/abs/2410.05687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05687">https://arxiv.org/pdf/2410.05687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05687]] Extreme Value Modelling of Feature Residuals for Anomaly Detection in Dynamic Graphs(https://arxiv.org/abs/2410.05687)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Detecting anomalies in a temporal sequence of graphs can be applied is areas such as the detection of accidents in transport networks and cyber attacks in computer networks. Existing methods for detecting abnormal graphs can suffer from multiple limitations, such as high false positive rates as well as difficulties with handling variable-sized graphs and non-trivial temporal dynamics. To address this, we propose a technique where temporal dependencies are explicitly modelled via time series analysis of a large set of pertinent graph features, followed by using residuals to remove the dependencies. Extreme Value Theory is then used to robustly model and classify any remaining extremes, aiming to produce low false positives rates. Comparative evaluations on a multitude of graph instances show that the proposed approach obtains considerably better accuracy than TensorSplat and Laplacian Anomaly Detection.</li>
</ul>

<h3>Title: DiffusionGuard: A Robust Defense Against Malicious Diffusion-based Image Editing</h3>
<ul>
<li><strong>Authors: </strong>June Suk Choi, Kyungmin Lee, Jongheon Jeong, Saining Xie, Jinwoo Shin, Kimin Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05694">https://arxiv.org/abs/2410.05694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05694">https://arxiv.org/pdf/2410.05694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05694]] DiffusionGuard: A Robust Defense Against Malicious Diffusion-based Image Editing(https://arxiv.org/abs/2410.05694)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have introduced a new era of text-guided image manipulation, enabling users to create realistic edited images with simple textual prompts. However, there is significant concern about the potential misuse of these methods, especially in creating misleading or harmful content. Although recent defense strategies, which introduce imperceptible adversarial noise to induce model failure, have shown promise, they remain ineffective against more sophisticated manipulations, such as editing with a mask. In this work, we propose DiffusionGuard, a robust and effective defense method against unauthorized edits by diffusion-based image editing models, even in challenging setups. Through a detailed analysis of these models, we introduce a novel objective that generates adversarial noise targeting the early stage of the diffusion process. This approach significantly improves the efficiency and effectiveness of adversarial noises. We also introduce a mask-augmentation technique to enhance robustness against various masks during test time. Finally, we introduce a comprehensive benchmark designed to evaluate the effectiveness and robustness of methods in protecting against privacy threats in realistic scenarios. Through extensive experiments, we show that our method achieves stronger protection and improved mask robustness with lower computational costs compared to the strongest baseline. Additionally, our method exhibits superior transferability and better resilience to noise removal techniques compared to all baseline methods. Our source code is publicly available at this https URL.</li>
</ul>

<h3>Title: Unlocking the Boundaries of Thought: A Reasoning Granularity Framework to Quantify and Optimize Chain-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Qiguang Chen, Libo Qin, Jiaqi Wang, Jinxuan Zhou, Wanxiang Che</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05695">https://arxiv.org/abs/2410.05695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05695">https://arxiv.org/pdf/2410.05695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05695]] Unlocking the Boundaries of Thought: A Reasoning Granularity Framework to Quantify and Optimize Chain-of-Thought(https://arxiv.org/abs/2410.05695)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) reasoning has emerged as a promising approach for enhancing the performance of large language models (LLMs) on complex reasoning tasks. Recently, a series of studies attempt to explain the mechanisms underlying CoT, aiming to deepen the understanding of its efficacy. Nevertheless, the existing research faces two major challenges: (1) a lack of quantitative metrics to assess CoT capabilities and (2) a dearth of guidance on optimizing CoT performance. Motivated by this, in this work, we introduce a novel reasoning granularity framework (RGF) to address these challenges. To solve the lack of quantification, we first define a reasoning granularity (RG) to quantify the upper bound of CoT and establish a combination law for RG, enabling a practical quantitative approach applicable to various real-world CoT tasks. To address the lack of optimization, we propose three categories of RGs. We further optimize these categories with combination laws focused on RG promotion and reasoning path optimization for CoT improvement. Through extensive experiments on 25 models and 4 tasks, the study validates the existence and rationality of the proposed framework. Furthermore, it explains the effectiveness of 10 CoT strategies and guides optimization from two perspectives. We hope this work can provide a comprehensive understanding of the boundaries and optimization strategies for reasoning in LLMs. Our code and data are available at this https URL.</li>
</ul>

<h3>Title: Diffusing to the Top: Boost Graph Neural Networks with Minimal Hyperparameter Tuning</h3>
<ul>
<li><strong>Authors: </strong>Lequan Lin, Dai Shi, Andi Han, Zhiyong Wang, Junbin Gao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05697">https://arxiv.org/abs/2410.05697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05697">https://arxiv.org/pdf/2410.05697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05697]] Diffusing to the Top: Boost Graph Neural Networks with Minimal Hyperparameter Tuning(https://arxiv.org/abs/2410.05697)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) are proficient in graph representation learning and achieve promising performance on versatile tasks such as node classification and link prediction. Usually, a comprehensive hyperparameter tuning is essential for fully unlocking GNN's top performance, especially for complicated tasks such as node classification on large graphs and long-range graphs. This is usually associated with high computational and time costs and careful design of appropriate search spaces. This work introduces a graph-conditioned latent diffusion framework (GNN-Diff) to generate high-performing GNNs based on the model checkpoints of sub-optimal hyperparameters selected by a light-tuning coarse search. We validate our method through 166 experiments across four graph tasks: node classification on small, large, and long-range graphs, as well as link prediction. Our experiments involve 10 classic and state-of-the-art target models and 20 publicly available datasets. The results consistently demonstrate that GNN-Diff: (1) boosts the performance of GNNs with efficient hyperparameter tuning; and (2) presents high stability and generalizability on unseen data across multiple generation runs. The code is available at this https URL.</li>
</ul>

<h3>Title: PixLens: A Novel Framework for Disentangled Evaluation in Diffusion-Based Image Editing with Object Detection + SAM</h3>
<ul>
<li><strong>Authors: </strong>Stefan Stefanache, Lluís Pastor Pérez, Julen Costa Watanabe, Ernesto Sanchez Tejedor, Thomas Hofmann, Enis Simsar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05710">https://arxiv.org/abs/2410.05710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05710">https://arxiv.org/pdf/2410.05710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05710]] PixLens: A Novel Framework for Disentangled Evaluation in Diffusion-Based Image Editing with Object Detection + SAM(https://arxiv.org/abs/2410.05710)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Evaluating diffusion-based image-editing models is a crucial task in the field of Generative AI. Specifically, it is imperative to assess their capacity to execute diverse editing tasks while preserving the image content and realism. While recent developments in generative models have opened up previously unheard-of possibilities for image editing, conducting a thorough evaluation of these models remains a challenging and open task. The absence of a standardized evaluation benchmark, primarily due to the inherent need for a post-edit reference image for evaluation, further complicates this issue. Currently, evaluations often rely on established models such as CLIP or require human intervention for a comprehensive understanding of the performance of these image editing models. Our benchmark, PixLens, provides a comprehensive evaluation of both edit quality and latent representation disentanglement, contributing to the advancement and refinement of existing methodologies in the field.</li>
</ul>

<h3>Title: Diffusion Auto-regressive Transformer for Effective Self-supervised Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Daoyu Wang, Mingyue Cheng, Zhiding Liu, Qi Liu, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05711">https://arxiv.org/abs/2410.05711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05711">https://arxiv.org/pdf/2410.05711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05711]] Diffusion Auto-regressive Transformer for Effective Self-supervised Time Series Forecasting(https://arxiv.org/abs/2410.05711)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Self-supervised learning has become a popular and effective approach for enhancing time series forecasting, enabling models to learn universal representations from unlabeled data. However, effectively capturing both the global sequence dependence and local detail features within time series data remains challenging. To address this, we propose a novel generative self-supervised method called TimeDART, denoting Diffusion Auto-regressive Transformer for Time series forecasting. In TimeDART, we treat time series patches as basic modeling units. Specifically, we employ an self-attention based Transformer encoder to model the dependencies of inter-patches. Additionally, we introduce diffusion and denoising mechanisms to capture the detail locality features of intra-patch. Notably, we design a cross-attention-based denoising decoder that allows for adjustable optimization difficulty in the self-supervised task, facilitating more effective self-supervised pre-training. Furthermore, the entire model is optimized in an auto-regressive manner to obtain transferable representations. Extensive experiments demonstrate that TimeDART achieves state-of-the-art fine-tuning performance compared to the most advanced competitive methods in forecasting tasks. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Enhancing Temporal Modeling of Video LLMs via Time Gating</h3>
<ul>
<li><strong>Authors: </strong>Zi-Yuan Hu, Yiwu Zhong, Shijia Huang, Michael R. Lyu, Liwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05714">https://arxiv.org/abs/2410.05714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05714">https://arxiv.org/pdf/2410.05714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05714]] Enhancing Temporal Modeling of Video LLMs via Time Gating(https://arxiv.org/abs/2410.05714)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Video Large Language Models (Video LLMs) have achieved impressive performance on video-and-language tasks, such as video question answering. However, most existing Video LLMs neglect temporal information in video data, leading to struggles with temporal-aware video understanding. To address this gap, we propose a Time Gating Video LLM (TG-Vid) designed to enhance temporal modeling through a novel Time Gating module (TG). The TG module employs a time gating mechanism on its sub-modules, comprising gating spatial attention, gating temporal attention, and gating MLP. This architecture enables our model to achieve a robust understanding of temporal information within videos. Extensive evaluation of temporal-sensitive video benchmarks (i.e., MVBench, TempCompass, and NExT-QA) demonstrates that our TG-Vid model significantly outperforms the existing Video LLMs. Further, comprehensive ablation studies validate that the performance gains are attributed to the designs of our TG module. Our code is available at this https URL.</li>
</ul>

<h3>Title: Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery</h3>
<ul>
<li><strong>Authors: </strong>Xuanchen (Willow)Liu, Shuxin Qiao, Kyle Gao, Hongjie He, Michael A. Chapman, Linlin Xu, Jonathan Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05717">https://arxiv.org/abs/2410.05717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05717">https://arxiv.org/pdf/2410.05717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05717]] Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery(https://arxiv.org/abs/2410.05717)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>This research addresses the need for high-definition (HD) maps for autonomous vehicles (AVs), focusing on road lane information derived from aerial imagery. While Earth observation data offers valuable resources for map creation, specialized models for road lane extraction are still underdeveloped in remote sensing. In this study, we perform an extensive comparison of twelve foundational deep learning-based semantic segmentation models for road lane marking extraction from high-definition remote sensing images, assessing their performance under transfer learning with partially labeled datasets. These models were fine-tuned on the partially labeled Waterloo Urban Scene dataset, and pre-trained on the SkyScapes dataset, simulating a likely scenario of real-life model deployment under partial labeling. We observed and assessed the fine-tuning performance and overall performance. Models showed significant performance improvements after fine-tuning, with mean IoU scores ranging from 33.56% to 76.11%, and recall ranging from 66.0% to 98.96%. Transformer-based models outperformed convolutional neural networks, emphasizing the importance of model pre-training and fine-tuning in enhancing HD map development for AV navigation.</li>
</ul>

<h3>Title: Mero Nagarikta: Advanced Nepali Citizenship Data Extractor with Deep Learning-Powered Text Detection and OCR</h3>
<ul>
<li><strong>Authors: </strong>Sisir Dhakal, Sujan Sigdel, Sandesh Prasad Paudel, Sharad Kumar Ranabhat, Nabin Lamichhane</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05721">https://arxiv.org/abs/2410.05721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05721">https://arxiv.org/pdf/2410.05721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05721]] Mero Nagarikta: Advanced Nepali Citizenship Data Extractor with Deep Learning-Powered Text Detection and OCR(https://arxiv.org/abs/2410.05721)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Transforming text-based identity documents, such as Nepali citizenship cards, into a structured digital format poses several challenges due to the distinct characteristics of the Nepali script and minor variations in print alignment and contrast across different cards. This work proposes a robust system using YOLOv8 for accurate text object detection and an OCR algorithm based on Optimized PyTesseract. The system, implemented within the context of a mobile application, allows for the automated extraction of important textual information from both the front and the back side of Nepali citizenship cards, including names, citizenship numbers, and dates of birth. The final YOLOv8 model was accurate, with a mean average precision of 99.1% for text detection on the front and 96.1% on the back. The tested PyTesseract optimized for Nepali characters outperformed the standard OCR regarding flexibility and accuracy, extracting text from images with clean and noisy backgrounds and various contrasts. Using preprocessing steps such as converting the images into grayscale, removing noise from the images, and detecting edges further improved the system's OCR accuracy, even for low-quality photos. This work expands the current body of research in multilingual OCR and document analysis, especially for low-resource languages such as Nepali. It emphasizes the effectiveness of combining the latest object detection framework with OCR models that have been fine-tuned for practical applications.</li>
</ul>

<h3>Title: KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Wang, Xiaoyu Liang, Rui Ye, Jingyi Chai, Siheng Chen, Yanfeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05725">https://arxiv.org/abs/2410.05725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05725">https://arxiv.org/pdf/2410.05725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05725]] KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server(https://arxiv.org/abs/2410.05725)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>The success of large language models (LLMs) facilitate many parties to fine-tune LLMs on their own private data. However, this practice raises privacy concerns due to the memorization of LLMs. Existing solutions, such as utilizing synthetic data for substitution, struggle to simultaneously improve performance and preserve privacy. They either rely on a local model for generation, resulting in a performance decline, or take advantage of APIs, directly exposing the data to API servers. To address this issue, we propose \textit{KnowledgeSG}, a novel client-server framework which enhances synthetic data quality and improves model performance while ensuring privacy. We achieve this by learning local knowledge from the private data with differential privacy (DP) and distilling professional knowledge from the server. Additionally, inspired by federated learning, we transmit models rather than data between the client and server to prevent privacy leakage. Extensive experiments in medical and financial domains demonstrate the effectiveness of KnowledgeSG. Our code is now publicly available at this https URL.</li>
</ul>

<h3>Title: Less is more: Embracing sparsity and interpolation with Esiformer for time series forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yangyang Guo, Yanjun Zhao, Sizhe Dang, Tian Zhou, Liang Sun, Yi Qian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05726">https://arxiv.org/abs/2410.05726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05726">https://arxiv.org/pdf/2410.05726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05726]] Less is more: Embracing sparsity and interpolation with Esiformer for time series forecasting(https://arxiv.org/abs/2410.05726)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Time series forecasting has played a significant role in many practical fields. But time series data generated from real-world applications always exhibits high variance and lots of noise, which makes it difficult to capture the inherent periodic patterns of the data, hurting the prediction accuracy significantly. To address this issue, we propose the Esiformer, which apply interpolation on the original data, decreasing the overall variance of the data and alleviating the influence of noise. What's more, we enhanced the vanilla transformer with a robust Sparse FFN. It can enhance the representation ability of the model effectively, and maintain the excellent robustness, avoiding the risk of overfitting compared with the vanilla implementation. Through evaluations on challenging real-world datasets, our method outperforms leading model PatchTST, reducing MSE by 6.5% and MAE by 5.8% in multivariate time series forecasting. Code is available at: this https URL.</li>
</ul>

<h3>Title: Equi-GSPR: Equivariant SE(3) Graph Network Model for Sparse Point Cloud Registration</h3>
<ul>
<li><strong>Authors: </strong>Xueyang Kang, Zhaoliang Luan, Kourosh Khoshelham, Bing Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05729">https://arxiv.org/abs/2410.05729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05729">https://arxiv.org/pdf/2410.05729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05729]] Equi-GSPR: Equivariant SE(3) Graph Network Model for Sparse Point Cloud Registration(https://arxiv.org/abs/2410.05729)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Point cloud registration is a foundational task for 3D alignment and reconstruction applications. While both traditional and learning-based registration approaches have succeeded, leveraging the intrinsic symmetry of point cloud data, including rotation equivariance, has received insufficient attention. This prohibits the model from learning effectively, resulting in a requirement for more training data and increased model complexity. To address these challenges, we propose a graph neural network model embedded with a local Spherical Euclidean 3D equivariance property through SE(3) message passing based propagation. Our model is composed mainly of a descriptor module, equivariant graph layers, match similarity, and the final regression layers. Such modular design enables us to utilize sparsely sampled input points and initialize the descriptor by self-trained or pre-trained geometric feature descriptors easily. Experiments conducted on the 3DMatch and KITTI datasets exhibit the compelling and robust performance of our model compared to state-of-the-art approaches, while the model complexity remains relatively low at the same time.</li>
</ul>

<h3>Title: Private and Communication-Efficient Federated Learning based on Differentially Private Sketches</h3>
<ul>
<li><strong>Authors: </strong>Meifan Zhang, Zhanhong Xie, Lihua Yin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05733">https://arxiv.org/abs/2410.05733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05733">https://arxiv.org/pdf/2410.05733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05733]] Private and Communication-Efficient Federated Learning based on Differentially Private Sketches(https://arxiv.org/abs/2410.05733)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) faces two primary challenges: the risk of privacy leakage due to parameter sharing and communication inefficiencies. To address these challenges, we propose DPSFL, a federated learning method that utilizes differentially private sketches. DPSFL compresses the local gradients of each client using a count sketch, thereby improving communication efficiency, while adding noise to the sketches to ensure differential privacy (DP). We provide a theoretical analysis of privacy and convergence for the proposed method. Gradient clipping is essential in DP learning to limit sensitivity and constrain the addition of noise. However, clipping introduces bias into the gradients, negatively impacting FL performance. To mitigate the impact of clipping, we propose an enhanced method, DPSFL-AC, which employs an adaptive clipping strategy. Experimental comparisons with existing techniques demonstrate the superiority of our methods concerning privacy preservation, communication efficiency, and model accuracy.</li>
</ul>

<h3>Title: Polynomial Time Cryptanalytic Extraction of Deep Neural Networks in the Hard-Label Setting</h3>
<ul>
<li><strong>Authors: </strong>Nicholas Carlini, Jorge Chávez-Saab, Anna Hambitzer, Francisco Rodríguez-Henríquez, Adi Shamir</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05750">https://arxiv.org/abs/2410.05750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05750">https://arxiv.org/pdf/2410.05750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05750]] Polynomial Time Cryptanalytic Extraction of Deep Neural Networks in the Hard-Label Setting(https://arxiv.org/abs/2410.05750)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, extraction</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) are valuable assets, yet their public accessibility raises security concerns about parameter extraction by malicious actors. Recent work by Carlini et al. (crypto'20) and Canales-Martínez et al. (eurocrypt'24) has drawn parallels between this issue and block cipher key extraction via chosen plaintext attacks. Leveraging differential cryptanalysis, they demonstrated that all the weights and biases of black-box ReLU-based DNNs could be inferred using a polynomial number of queries and computational time. However, their attacks relied on the availability of the exact numeric value of output logits, which allowed the calculation of their derivatives. To overcome this limitation, Chen et al. (asiacrypt'24) tackled the more realistic hard-label scenario, where only the final classification label (e.g., "dog" or "car") is accessible to the attacker. They proposed an extraction method requiring a polynomial number of queries but an exponential execution time. In addition, their approach was applicable only to a restricted set of architectures, could deal only with binary classifiers, and was demonstrated only on tiny neural networks with up to four neurons split among up to two hidden layers. This paper introduces new techniques that, for the first time, achieve cryptanalytic extraction of DNN parameters in the most challenging hard-label setting, using both a polynomial number of queries and polynomial time. We validate our approach by extracting nearly one million parameters from a DNN trained on the CIFAR-10 dataset, comprising 832 neurons in four hidden layers. Our results reveal the surprising fact that all the weights of a ReLU-based DNN can be efficiently determined by analyzing only the geometric shape of its decision boundaries.</li>
</ul>

<h3>Title: Exploring the Meaningfulness of Nearest Neighbor Search in High-Dimensional Space</h3>
<ul>
<li><strong>Authors: </strong>Zhonghan Chen, Ruiyuan Zhang, Xi Zhao, Xiaojun Cheng, Xiaofang Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05752">https://arxiv.org/abs/2410.05752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05752">https://arxiv.org/pdf/2410.05752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05752]] Exploring the Meaningfulness of Nearest Neighbor Search in High-Dimensional Space(https://arxiv.org/abs/2410.05752)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Dense high dimensional vectors are becoming increasingly vital in fields such as computer vision, machine learning, and large language models (LLMs), serving as standard representations for multimodal data. Now the dimensionality of these vector can exceed several thousands easily. Despite the nearest neighbor search (NNS) over these dense high dimensional vectors have been widely used for retrieval augmented generation (RAG) and many other applications, the effectiveness of NNS in such a high-dimensional space remains uncertain, given the possible challenge caused by the "curse of dimensionality." To address above question, in this paper, we conduct extensive NNS studies with different distance functions, such as $L_1$ distance, $L_2$ distance and angular-distance, across diverse embedding datasets, of varied types, dimensionality and modality. Our aim is to investigate factors influencing the meaningfulness of NNS. Our experiments reveal that high-dimensional text embeddings exhibit increased resilience as dimensionality rises to higher levels when compared to random vectors. This resilience suggests that text embeddings are less affected to the "curse of dimensionality," resulting in more meaningful NNS outcomes for practical use. Additionally, the choice of distance function has minimal impact on the relevance of NNS. Our study shows the effectiveness of the embedding-based data representation method and can offer opportunity for further optimization of dense vector-related applications.</li>
</ul>

<h3>Title: Training-free Diffusion Model Alignment with Sampling Demons</h3>
<ul>
<li><strong>Authors: </strong>Po-Hung Yeh, Kuang-Huei Lee, Jun-Cheng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05760">https://arxiv.org/abs/2410.05760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05760">https://arxiv.org/pdf/2410.05760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05760]] Training-free Diffusion Model Alignment with Sampling Demons(https://arxiv.org/abs/2410.05760)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Aligning diffusion models with user preferences has been a key challenge. Existing methods for aligning diffusion models either require retraining or are limited to differentiable reward functions. To address these limitations, we propose a stochastic optimization approach, dubbed Demon, to guide the denoising process at inference time without backpropagation through reward functions or model retraining. Our approach works by controlling noise distribution in denoising steps to concentrate density on regions corresponding to high rewards through stochastic optimization. We provide comprehensive theoretical and empirical evidence to support and validate our approach, including experiments that use non-differentiable sources of rewards such as Visual-Language Model (VLM) APIs and human judgements. To the best of our knowledge, the proposed approach is the first inference-time, backpropagation-free preference alignment method for diffusion models. Our method can be easily integrated with existing diffusion models without further training. Our experiments show that the proposed approach significantly improves the average aesthetics scores for text-to-image generation.</li>
</ul>

<h3>Title: Guided Self-attention: Find the Generalized Necessarily Distinct Vectors for Grain Size Grading</h3>
<ul>
<li><strong>Authors: </strong>Fang Gao, Xuetao Li, Jiabao Wang, Shengheng Ma, Jun Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05762">https://arxiv.org/abs/2410.05762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05762">https://arxiv.org/pdf/2410.05762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05762]] Guided Self-attention: Find the Generalized Necessarily Distinct Vectors for Grain Size Grading(https://arxiv.org/abs/2410.05762)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>With the development of steel materials, metallographic analysis has become increasingly important. Unfortunately, grain size analysis is a manual process that requires experts to evaluate metallographic photographs, which is unreliable and time-consuming. To resolve this problem, we propose a novel classifi-cation method based on deep learning, namely GSNets, a family of hybrid models which can effectively introduce guided self-attention for classifying grain size. Concretely, we build our models from three insights:(1) Introducing our novel guided self-attention module can assist the model in finding the generalized necessarily distinct vectors capable of retaining intricate rela-tional connections and rich local feature information; (2) By improving the pixel-wise linear independence of the feature map, the highly condensed semantic representation will be captured by the model; (3) Our novel triple-stream merging module can significantly improve the generalization capability and efficiency of the model. Experiments show that our GSNet yields a classifi-cation accuracy of 90.1%, surpassing the state-of-the-art Swin Transformer V2 by 1.9% on the steel grain size dataset, which comprises 3,599 images with 14 grain size levels. Furthermore, we intuitively believe our approach is applicable to broader ap-plications like object detection and semantic segmentation.</li>
</ul>

<h3>Title: Grounding is All You Need? Dual Temporal Grounding for Video Dialog</h3>
<ul>
<li><strong>Authors: </strong>You Qin, Wei Ji, Xinze Lan, Hao Fei, Xun Yang, Dan Guo, Roger Zimmermann, Lizi Liao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05767">https://arxiv.org/abs/2410.05767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05767">https://arxiv.org/pdf/2410.05767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05767]] Grounding is All You Need? Dual Temporal Grounding for Video Dialog(https://arxiv.org/abs/2410.05767)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In the realm of video dialog response generation, the understanding of video content and the temporal nuances of conversation history are paramount. While a segment of current research leans heavily on large-scale pretrained visual-language models and often overlooks temporal dynamics, another delves deep into spatial-temporal relationships within videos but demands intricate object trajectory pre-extractions and sidelines dialog temporal dynamics. This paper introduces the Dual Temporal Grounding-enhanced Video Dialog model (DTGVD), strategically designed to merge the strengths of both dominant approaches. It emphasizes dual temporal relationships by predicting dialog turn-specific temporal regions, filtering video content accordingly, and grounding responses in both video and dialog contexts. One standout feature of DTGVD is its heightened attention to chronological interplay. By recognizing and acting upon the dependencies between different dialog turns, it captures more nuanced conversational dynamics. To further bolster the alignment between video and dialog temporal dynamics, we've implemented a list-wise contrastive learning strategy. Within this framework, accurately grounded turn-clip pairings are designated as positive samples, while less precise pairings are categorized as negative. This refined classification is then funneled into our holistic end-to-end response generation mechanism. Evaluations using AVSD@DSTC-7 and AVSD@DSTC-8 datasets underscore the superiority of our methodology.</li>
</ul>

<h3>Title: Efficient Few-shot Learning for Multi-label Classification of Scientific Documents with Many Classes</h3>
<ul>
<li><strong>Authors: </strong>Tim Schopf, Alexander Blatzheim, Nektarios Machner, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05770">https://arxiv.org/abs/2410.05770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05770">https://arxiv.org/pdf/2410.05770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05770]] Efficient Few-shot Learning for Multi-label Classification of Scientific Documents with Many Classes(https://arxiv.org/abs/2410.05770)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Scientific document classification is a critical task and often involves many classes. However, collecting human-labeled data for many classes is expensive and usually leads to label-scarce scenarios. Moreover, recent work has shown that sentence embedding model fine-tuning for few-shot classification is efficient, robust, and effective. In this work, we propose FusionSent (Fusion-based Sentence Embedding Fine-tuning), an efficient and prompt-free approach for few-shot classification of scientific documents with many classes. FusionSent uses available training examples and their respective label texts to contrastively fine-tune two different sentence embedding models. Afterward, the parameters of both fine-tuned models are fused to combine the complementary knowledge from the separate fine-tuning steps into a single model. Finally, the resulting sentence embedding model is frozen to embed the training instances, which are then used as input features to train a classification head. Our experiments show that FusionSent significantly outperforms strong baselines by an average of $6.0$ $F_{1}$ points across multiple scientific document classification datasets. In addition, we introduce a new dataset for multi-label classification of scientific documents, which contains 183,565 scientific articles and 130 classes from the arXiv category taxonomy. Code and data are available at this https URL.</li>
</ul>

<h3>Title: Comparative Analysis of Novel View Synthesis and Photogrammetry for 3D Forest Stand Reconstruction and extraction of individual tree parameters</h3>
<ul>
<li><strong>Authors: </strong>Guoji Tian, Chongcheng Chen, Hongyu Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05772">https://arxiv.org/abs/2410.05772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05772">https://arxiv.org/pdf/2410.05772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05772]] Comparative Analysis of Novel View Synthesis and Photogrammetry for 3D Forest Stand Reconstruction and extraction of individual tree parameters(https://arxiv.org/abs/2410.05772)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Accurate and efficient 3D reconstruction of trees is crucial for forest resource assessments and management. Close-Range Photogrammetry (CRP) is commonly used for reconstructing forest scenes but faces challenges like low efficiency and poor quality. Recently, Novel View Synthesis (NVS) technologies, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have shown promise for 3D plant reconstruction with limited images. However, existing research mainly focuses on small plants in orchards or individual trees, leaving uncertainty regarding their application in larger, complex forest stands. In this study, we collected sequential images of forest plots with varying complexity and performed dense reconstruction using NeRF and 3DGS. The resulting point clouds were compared with those from photogrammetry and laser scanning. Results indicate that NVS methods significantly enhance reconstruction efficiency. Photogrammetry struggles with complex stands, leading to point clouds with excessive canopy noise and incorrectly reconstructed trees, such as duplicated trunks. NeRF, while better for canopy regions, may produce errors in ground areas with limited views. The 3DGS method generates sparser point clouds, particularly in trunk areas, affecting diameter at breast height (DBH) accuracy. All three methods can extract tree height information, with NeRF yielding the highest accuracy; however, photogrammetry remains superior for DBH accuracy. These findings suggest that NVS methods have significant potential for 3D reconstruction of forest stands, offering valuable support for complex forest resource inventory and visualization tasks.</li>
</ul>

<h3>Title: Enhanced Feature Based Granular Ball Twin Support Vector Machine</h3>
<ul>
<li><strong>Authors: </strong>A. Quadir, M. Sajid, Mushir Akhtar, M. Tanveer, P. N. Suganthan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05786">https://arxiv.org/abs/2410.05786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05786">https://arxiv.org/pdf/2410.05786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05786]] Enhanced Feature Based Granular Ball Twin Support Vector Machine(https://arxiv.org/abs/2410.05786)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we propose enhanced feature based granular ball twin support vector machine (EF-GBTSVM). EF-GBTSVM employs the coarse granularity of granular balls (GBs) as input rather than individual data samples. The GBs are mapped to the feature space of the hidden layer using random projection followed by the utilization of a non-linear activation function. The concatenation of original and hidden features derived from the centers of GBs gives rise to an enhanced feature space, commonly referred to as the random vector functional link (RVFL) space. This space encapsulates nuanced feature information to GBs. Further, we employ twin support vector machine (TSVM) in the RVFL space for classification. TSVM generates the two non-parallel hyperplanes in the enhanced feature space, which improves the generalization performance of the proposed EF-GBTSVM model. Moreover, the coarser granularity of the GBs enables the proposed EF-GBTSVM model to exhibit robustness to resampling, showcasing reduced susceptibility to the impact of noise and outliers. We undertake a thorough evaluation of the proposed EF-GBTSVM model on benchmark UCI and KEEL datasets. This evaluation encompasses scenarios with and without the inclusion of label noise. Moreover, experiments using NDC datasets further emphasize the proposed model's ability to handle large datasets. Experimental results, supported by thorough statistical analyses, demonstrate that the proposed EF-GBTSVM model significantly outperforms the baseline models in terms of generalization capabilities, scalability, and robustness. The source code for the proposed EF-GBTSVM model, along with additional results and further details, can be accessed at this https URL.</li>
</ul>

<h3>Title: CodeCipher: Learning to Obfuscate Source Code Against LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yalan Lin, Chengcheng Wan, Yixiong Fang, Xiaodong Gu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05797">https://arxiv.org/abs/2410.05797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05797">https://arxiv.org/pdf/2410.05797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05797]] CodeCipher: Learning to Obfuscate Source Code Against LLMs(https://arxiv.org/abs/2410.05797)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>While large code language models have made significant strides in AI-assisted coding tasks, there are growing concerns about privacy challenges. The user code is transparent to the cloud LLM service provider, inducing risks of unauthorized training, reading, and execution of the user code. In this paper, we propose CodeCipher, a novel method that perturbs privacy from code while preserving the original response from LLMs. CodeCipher transforms the LLM's embedding matrix so that each row corresponds to a different word in the original matrix, forming a token-to-token confusion mapping for obfuscating source code. The new embedding matrix is optimized by minimizing the task-specific loss function. To tackle the challenge of the discrete and sparse nature of word vector spaces, CodeCipher adopts a discrete optimization strategy that aligns the updated vector to the nearest valid token in the vocabulary before each gradient update. We demonstrate the effectiveness of our approach on three AI-assisted coding tasks including code completion, summarization, and translation. Results show that our model successfully confuses the privacy in source code while preserving the original LLM's performance.</li>
</ul>

<h3>Title: SeeClear: Semantic Distillation Enhances Pixel Condensation for Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Qi Tang, Yao Zhao, Meiqin Liu, Chao Yao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05799">https://arxiv.org/abs/2410.05799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05799">https://arxiv.org/pdf/2410.05799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05799]] SeeClear: Semantic Distillation Enhances Pixel Condensation for Video Super-Resolution(https://arxiv.org/abs/2410.05799)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based Video Super-Resolution (VSR) is renowned for generating perceptually realistic videos, yet it grapples with maintaining detail consistency across frames due to stochastic fluctuations. The traditional approach of pixel-level alignment is ineffective for diffusion-processed frames because of iterative disruptions. To overcome this, we introduce SeeClear--a novel VSR framework leveraging conditional video generation, orchestrated by instance-centric and channel-wise semantic controls. This framework integrates a Semantic Distiller and a Pixel Condenser, which synergize to extract and upscale semantic details from low-resolution frames. The Instance-Centric Alignment Module (InCAM) utilizes video-clip-wise tokens to dynamically relate pixels within and across frames, enhancing coherency. Additionally, the Channel-wise Texture Aggregation Memory (CaTeGory) infuses extrinsic knowledge, capitalizing on long-standing semantic textures. Our method also innovates the blurring diffusion process with the ResShift mechanism, finely balancing between sharpness and diffusion effects. Comprehensive experiments confirm our framework's advantage over state-of-the-art diffusion-based VSR techniques. The code is available: this https URL.</li>
</ul>

<h3>Title: Core Tokensets for Data-efficient Sequential Training of Transformers</h3>
<ul>
<li><strong>Authors: </strong>Subarnaduti Paul, Manuel Brack, Patrick Schramowski, Kristian Kersting, Martin Mundt</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05800">https://arxiv.org/abs/2410.05800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05800">https://arxiv.org/pdf/2410.05800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05800]] Core Tokensets for Data-efficient Sequential Training of Transformers(https://arxiv.org/abs/2410.05800)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Deep networks are frequently tuned to novel tasks and continue learning from ongoing data streams. Such sequential training requires consolidation of new and past information, a challenge predominantly addressed by retaining the most important data points - formally known as coresets. Traditionally, these coresets consist of entire samples, such as images or sentences. However, recent transformer architectures operate on tokens, leading to the famous assertion that an image is worth 16x16 words. Intuitively, not all of these tokens are equally informative or memorable. Going beyond coresets, we thus propose to construct a deeper-level data summary on the level of tokens. Our respectively named core tokensets both select the most informative data points and leverage feature attribution to store only their most relevant features. We demonstrate that core tokensets yield significant performance retention in incremental image classification, open-ended visual question answering, and continual image captioning with significantly reduced memory. In fact, we empirically find that a core tokenset of 1\% of the data performs comparably to at least a twice as large and up to 10 times larger coreset.</li>
</ul>

<h3>Title: Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Bolei He, Nuo Chen, Xinran He, Lingyong Yan, Zhenkai Wei, Jinchang Luo, Zhen-Hua Ling</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05801">https://arxiv.org/abs/2410.05801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05801">https://arxiv.org/pdf/2410.05801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05801]] Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation(https://arxiv.org/abs/2410.05801)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent Retrieval Augmented Generation (RAG) aims to enhance Large Language Models (LLMs) by incorporating extensive knowledge retrieved from external sources. However, such approach encounters some challenges: Firstly, the original queries may not be suitable for precise retrieval, resulting in erroneous contextual knowledge; Secondly, the language model can easily generate inconsistent answer with external references due to their knowledge boundary limitation. To address these issues, we propose the chain-of-verification (CoV-RAG) to enhance the external retrieval correctness and internal generation consistency. Specifically, we integrate the verification module into the RAG, engaging in scoring, judgment, and rewriting. To correct external retrieval errors, CoV-RAG retrieves new knowledge using a revised query. To correct internal generation errors, we unify QA and verification tasks with a Chain-of-Thought (CoT) reasoning during training. Our comprehensive experiments across various LLMs demonstrate the effectiveness and adaptability compared with other strong baselines. Especially, our CoV-RAG can significantly surpass the state-of-the-art baselines using different LLM backbones.</li>
</ul>

<h3>Title: Gradual Learning: Optimizing Fine-Tuning with Partially Mastered Knowledge in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bozhou Li, Hao Liang, Yang Li, Fangcheng Fu, Hongzhi Yin, Conghui He, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05802">https://arxiv.org/abs/2410.05802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05802">https://arxiv.org/pdf/2410.05802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05802]] Gradual Learning: Optimizing Fine-Tuning with Partially Mastered Knowledge in Large Language Models(https://arxiv.org/abs/2410.05802)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>During the pretraining phase, large language models (LLMs) acquire vast amounts of knowledge from extensive text corpora. Nevertheless, in later stages such as fine-tuning and inference, the model may encounter knowledge not covered in the initial training, which can lead to hallucinations and degraded performance. This issue has a profound impact on the model's capabilities, as it will inevitably face out-of-scope knowledge after pretraining. Furthermore, fine-tuning is often required to adapt LLMs to domain-specific tasks. However, this phenomenon limits the model's ability to learn and integrate new information during fine-tuning. The effectiveness of fine-tuning largely depends on the type of knowledge involved. Existing research suggests that fine-tuning the model on partially mastered knowledge-for instance, question-answer pairs where the model has a chance of providing correct responses under non-greedy decoding-can enable the model to acquire new knowledge while mitigating hallucination. Notably, this approach can still lead to the forgetting of fully mastered knowledge, constraining the fine-tuning dataset to a narrower range and limiting the model's overall potential for improvement. Given the model's intrinsic reasoning abilities and the interconnectedness of different knowledge areas, it is likely that as the model's capacity to utilize existing knowledge improves during fine-tuning, previously unmastered knowledge may become more understandable. To explore this hypothesis, we conducted experiments and, based on the results, proposed a two-stage fine-tuning strategy. This approach not only improves the model's overall test accuracy and knowledge retention but also preserves its accuracy on previously mastered content. When fine-tuning on the WikiQA dataset, our method increases the amount of knowledge acquired by the model in this stage by 24%.</li>
</ul>

<h3>Title: CASA: Class-Agnostic Shared Attributes in Vision-Language Models for Efficient Incremental Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Mingyi Guo, Yuyang Liu, Zongying Lin, Peixi Peng, Yonghong Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05804">https://arxiv.org/abs/2410.05804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05804">https://arxiv.org/pdf/2410.05804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05804]] CASA: Class-Agnostic Shared Attributes in Vision-Language Models for Efficient Incremental Object Detection(https://arxiv.org/abs/2410.05804)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Incremental object detection (IOD) is challenged by background shift, where background categories in sequential data may include previously learned or future classes. Inspired by the vision-language foundation models such as CLIP, these models capture shared attributes from extensive image-text paired data during pre-training. We propose a novel method utilizing attributes in vision-language foundation models for incremental object detection. Our method constructs a Class-Agnostic Shared Attribute base (CASA) to capture common semantic information among incremental classes. Specifically, we utilize large language models to generate candidate textual attributes and select the most relevant ones based on current training data, recording their significance in an attribute assignment matrix. For subsequent tasks, we freeze the retained attributes and continue selecting from the remaining candidates while updating the attribute assignment matrix accordingly. Furthermore, we employ OWL-ViT as our baseline, preserving the original parameters of the pre-trained foundation model. Our method adds only 0.7% to parameter storage through parameter-efficient fine-tuning to significantly enhance the scalability and adaptability of IOD. Extensive two-phase and multi-phase experiments on the COCO dataset demonstrate the state-of-the-art performance of our proposed method.</li>
</ul>

<h3>Title: PostCast: Generalizable Postprocessing for Precipitation Nowcasting via Unsupervised Blurriness Modeling</h3>
<ul>
<li><strong>Authors: </strong>Junchao Gong, Siwei Tu, Weidong Yang, Ben Fei, Kun Chen, Wenlong Zhang, Xiaokang Yang, Wanli Ouyang, Lei Bai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05805">https://arxiv.org/abs/2410.05805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05805">https://arxiv.org/pdf/2410.05805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05805]] PostCast: Generalizable Postprocessing for Precipitation Nowcasting via Unsupervised Blurriness Modeling(https://arxiv.org/abs/2410.05805)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Precipitation nowcasting plays a pivotal role in socioeconomic sectors, especially in severe convective weather warnings. Although notable progress has been achieved by approaches mining the spatiotemporal correlations with deep learning, these methods still suffer severe blurriness as the lead time increases, which hampers accurate predictions for extreme precipitation. To alleviate blurriness, researchers explore generative methods conditioned on blurry predictions. However, the pairs of blurry predictions and corresponding ground truth need to be generated in advance, making the training pipeline cumbersome and limiting the generality of generative models within blur modes that appear in training data. By rethinking the blurriness in precipitation nowcasting as a blur kernel acting on predictions, we propose an unsupervised postprocessing method to eliminate the blurriness without the requirement of training with the pairs of blurry predictions and corresponding ground truth. Specifically, we utilize blurry predictions to guide the generation process of a pre-trained unconditional denoising diffusion probabilistic model (DDPM) to obtain high-fidelity predictions with eliminated blurriness. A zero-shot blur kernel estimation mechanism and an auto-scale denoise guidance strategy are introduced to adapt the unconditional DDPM to any blurriness modes varying from datasets and lead times in precipitation nowcasting. Extensive experiments are conducted on 7 precipitation radar datasets, demonstrating the generality and superiority of our method.</li>
</ul>

<h3>Title: Vision Transformer based Random Walk for Group Re-Identification</h3>
<ul>
<li><strong>Authors: </strong>Guoqing Zhang, Tianqi Liu, Wenxuan Fang, Yuhui Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05808">https://arxiv.org/abs/2410.05808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05808">https://arxiv.org/pdf/2410.05808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05808]] Vision Transformer based Random Walk for Group Re-Identification(https://arxiv.org/abs/2410.05808)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Group re-identification (re-ID) aims to match groups with the same people under different cameras, mainly involves the challenges of group members and layout changes well. Most existing methods usually use the k-nearest neighbor algorithm to update node features to consider changes in group membership, but these methods cannot solve the problem of group layout changes. To this end, we propose a novel vision transformer based random walk framework for group re-ID. Specifically, we design a vision transformer based on a monocular depth estimation algorithm to construct a graph through the average depth value of pedestrian features to fully consider the impact of camera distance on group members relationships. In addition, we propose a random walk module to reconstruct the graph by calculating affinity scores between target and gallery images to remove pedestrians who do not belong to the current group. Experimental results show that our framework is superior to most methods.</li>
</ul>

<h3>Title: CALoR: Towards Comprehensive Model Inversion Defense</h3>
<ul>
<li><strong>Authors: </strong>Hongyao Yu, Yixiang Qiu, Hao Fang, Bin Chen, Sijin Yu, Bin Wang, Shu-Tao Xia, Ke Xu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05814">https://arxiv.org/abs/2410.05814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05814">https://arxiv.org/pdf/2410.05814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05814]] CALoR: Towards Comprehensive Model Inversion Defense(https://arxiv.org/abs/2410.05814)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Model Inversion Attacks (MIAs) aim at recovering privacy-sensitive training data from the knowledge encoded in the released machine learning models. Recent advances in the MIA field have significantly enhanced the attack performance under multiple scenarios, posing serious privacy risks of Deep Neural Networks (DNNs). However, the development of defense strategies against MIAs is relatively backward to resist the latest MIAs and existing defenses fail to achieve further trade-off between model utility and model robustness. In this paper, we provide an in-depth analysis from the perspective of intrinsic vulnerabilities of MIAs, comprehensively uncovering the weaknesses inherent in the basic pipeline, which are partially investigated in the previous defenses. Building upon these new insights, we propose a robust defense mechanism, integrating Confidence Adaptation and Low-Rank compression(CALoR). Our method includes a novel robustness-enhanced classification loss specially-designed for model inversion defenses and reveals the extraordinary effectiveness of compressing the classification header. With CALoR, we can mislead the optimization objective, reduce the leaked information and impede the backpropagation of MIAs, thus mitigating the risk of privacy leakage. Extensive experimental results demonstrate that our method achieves state-of-the-art (SOTA) defense performance against MIAs and exhibits superior generalization to existing defenses across various scenarios.</li>
</ul>

<h3>Title: Probing Language Models on Their Knowledge Source</h3>
<ul>
<li><strong>Authors: </strong>Zineddine Tighidet, Andrea Mogini, Jiali Mei, Benjamin Piwowarski, Patrick Gallinari</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05817">https://arxiv.org/abs/2410.05817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05817">https://arxiv.org/pdf/2410.05817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05817]] Probing Language Models on Their Knowledge Source(https://arxiv.org/abs/2410.05817)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often encounter conflicts between their learned, internal (parametric knowledge, PK) and external knowledge provided during inference (contextual knowledge, CK). Understanding how LLMs models prioritize one knowledge source over the other remains a challenge. In this paper, we propose a novel probing framework to explore the mechanisms governing the selection between PK and CK in LLMs. Using controlled prompts designed to contradict the model's PK, we demonstrate that specific model activations are indicative of the knowledge source employed. We evaluate this framework on various LLMs of different sizes and demonstrate that mid-layer activations, particularly those related to relations in the input, are crucial in predicting knowledge source selection, paving the way for more reliable models capable of handling knowledge conflicts effectively.</li>
</ul>

<h3>Title: CAP: Detecting Unauthorized Data Usage in Generative Models via Prompt Generation</h3>
<ul>
<li><strong>Authors: </strong>Daniela Gallo, Angelica Liguori, Ettore Ritacco, Luca Caviglione, Fabrizio Durante, Giuseppe Manco</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05819">https://arxiv.org/abs/2410.05819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05819">https://arxiv.org/pdf/2410.05819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05819]] CAP: Detecting Unauthorized Data Usage in Generative Models via Prompt Generation(https://arxiv.org/abs/2410.05819)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To achieve accurate and unbiased predictions, Machine Learning (ML) models rely on large, heterogeneous, and high-quality datasets. However, this could raise ethical and legal concerns regarding copyright and authorization aspects, especially when information is gathered from the Internet. With the rise of generative models, being able to track data has become of particular importance, especially since they may (un)intentionally replicate copyrighted contents. Therefore, this work proposes Copyright Audit via Prompts generation (CAP), a framework for automatically testing whether an ML model has been trained with unauthorized data. Specifically, we devise an approach to generate suitable keys inducing the model to reveal copyrighted contents. To prove its effectiveness, we conducted an extensive evaluation campaign on measurements collected in four IoT scenarios. The obtained results showcase the effectiveness of CAP, when used against both realistic and synthetic datasets.</li>
</ul>

<h3>Title: IncSAR: A Dual Fusion Incremental Learning Framework for SAR Target Recognition</h3>
<ul>
<li><strong>Authors: </strong>George Karantaidis, Athanasios Pantsios, Yiannis Kompatsiaris, Symeon Papadopoulos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05820">https://arxiv.org/abs/2410.05820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05820">https://arxiv.org/pdf/2410.05820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05820]] IncSAR: A Dual Fusion Incremental Learning Framework for SAR Target Recognition(https://arxiv.org/abs/2410.05820)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Deep learning techniques have been successfully applied in Synthetic Aperture Radar (SAR) target recognition in static scenarios relying on predefined datasets. However, in real-world scenarios, models must incrementally learn new information without forgetting previously learned knowledge. Models' tendency to forget old knowledge when learning new tasks, known as catastrophic forgetting, remains an open challenge. In this paper, an incremental learning framework, called IncSAR, is proposed to mitigate catastrophic forgetting in SAR target recognition. IncSAR comprises a Vision Transformer (ViT) and a custom-designed Convolutional Neural Network (CNN) in individual branches combined through a late-fusion strategy. A denoising module, utilizing the properties of Robust Principal Component Analysis (RPCA), is introduced to alleviate the speckle noise present in SAR images. Moreover, a random projection layer is employed to enhance the linear separability of features, and a Linear Discriminant Analysis (LDA) approach is proposed to decorrelate the extracted class prototypes. Experimental results on the MSTAR and OpenSARShip benchmark datasets demonstrate that IncSAR outperforms state-of-the-art approaches, leading to an improvement from $98.05\%$ to $99.63\%$ in average accuracy and from $3.05\%$ to $0.33\%$ in performance dropping rate.</li>
</ul>

<h3>Title: Multi-Session Client-Centered Treatment Outcome Evaluation in Psychotherapy</h3>
<ul>
<li><strong>Authors: </strong>Hongbin Na, Tao Shen, Shumao Yu, Ling Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05824">https://arxiv.org/abs/2410.05824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05824">https://arxiv.org/pdf/2410.05824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05824]] Multi-Session Client-Centered Treatment Outcome Evaluation in Psychotherapy(https://arxiv.org/abs/2410.05824)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In psychotherapy, therapeutic outcome assessment, or treatment outcome evaluation, is essential for enhancing mental health care by systematically evaluating therapeutic processes and outcomes. Existing large language model approaches often focus on therapist-centered, single-session evaluations, neglecting the client's subjective experience and longitudinal progress across multiple sessions. To address these limitations, we propose IPAEval, a client-Informed Psychological Assessment-based Evaluation framework that automates treatment outcome evaluations from the client's perspective using clinical interviews. IPAEval integrates cross-session client-contextual assessment and session-focused client-dynamics assessment to provide a comprehensive understanding of therapeutic progress. Experiments on our newly developed TheraPhase dataset demonstrate that IPAEval effectively tracks symptom severity and treatment outcomes over multiple sessions, outperforming previous single-session models and validating the benefits of items-aware reasoning mechanisms.</li>
</ul>

<h3>Title: A noise-corrected Langevin algorithm and sampling by half-denoising</h3>
<ul>
<li><strong>Authors: </strong>Aapo Hyvärinen</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05837">https://arxiv.org/abs/2410.05837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05837">https://arxiv.org/pdf/2410.05837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05837]] A noise-corrected Langevin algorithm and sampling by half-denoising(https://arxiv.org/abs/2410.05837)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The Langevin algorithm is a classic method for sampling from a given pdf in a real space. In its basic version, it only requires knowledge of the gradient of the log-density, also called the score function. However, in deep learning, it is often easier to learn the so-called "noisy score function", i.e. the gradient of the log-density of noisy data, more precisely when Gaussian noise is added to the data. Such an estimate is biased and complicates the use of the Langevin method. Here, we propose a noise-corrected version of the Langevin algorithm, where the bias due to noisy data is removed, at least regarding first-order terms. Unlike diffusion models, our algorithm needs to know the noisy score function for one single noise level only. We further propose a simple special case which has an interesting intuitive interpretation of iteratively adding noise the data and then attempting to remove half of that noise.</li>
</ul>

<h3>Title: Time Transfer: On Optimal Learning Rate and Batch Size In The Infinite Data Limit</h3>
<ul>
<li><strong>Authors: </strong>Oleg Filatov, Jan Ebert, Jiangtao Wang, Stefan Kesselheim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05838">https://arxiv.org/abs/2410.05838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05838">https://arxiv.org/pdf/2410.05838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05838]] Time Transfer: On Optimal Learning Rate and Batch Size In The Infinite Data Limit(https://arxiv.org/abs/2410.05838)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>One of the main challenges in optimal scaling of large language models (LLMs) is the prohibitive cost of hyperparameter tuning, particularly learning rate $\eta$ and batch size $B$. While techniques like $\mu$P (Yang et al., 2022) provide scaling rules for optimal $\eta$ transfer in the infinite model size limit, the optimal scaling behavior in the infinite data size limit ($T \to \infty$) remains unknown. We fill in this gap by observing for the first time an interplay of three optimal $\eta$ scaling regimes: $\eta \propto \sqrt{T}$, $\eta \propto 1$, and $\eta \propto 1/\sqrt{T}$ with transitions controlled by $B$ and its relation to the time-evolving critical batch size $B_\mathrm{crit} \propto T$. Furthermore, we show that the optimal batch size is positively correlated with $B_\mathrm{crit}$: keeping it fixed becomes suboptimal over time even if learning rate is scaled optimally. Surprisingly, our results demonstrate that the observed optimal $\eta$ and $B$ dynamics are preserved with $\mu$P model scaling, challenging the conventional view of $B_\mathrm{crit}$ dependence solely on loss value. Complementing optimality, we examine the sensitivity of loss to changes in learning rate, where we find the sensitivity to decrease with $T \to \infty$ and to remain constant with $\mu$P model scaling. We hope our results make the first step towards a unified picture of the joint optimal data and model scaling.</li>
</ul>

<h3>Title: From Tokens to Words: on the inner lexicon of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Guy Kaplan, Matanel Oren, Yuval Reif, Roy Schwartz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05864">https://arxiv.org/abs/2410.05864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05864">https://arxiv.org/pdf/2410.05864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05864]] From Tokens to Words: on the inner lexicon of LLMs(https://arxiv.org/abs/2410.05864)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Natural language is composed of words, but modern LLMs process sub-words as input. A natural question raised by this discrepancy is whether LLMs encode words internally, and if so how. We present evidence that LLMs engage in an intrinsic detokenization process, where sub-word sequences are combined into coherent word representations. Our experiments show that this process takes place primarily within the early and middle layers of the model. They also show that it is robust to non-morphemic splits, typos and perhaps importantly-to out-of-vocabulary words: when feeding the inner representation of such words to the model as input vectors, it can "understand" them despite never seeing them during training. Our findings suggest that LLMs maintain a latent vocabulary beyond the tokenizer's scope. These insights provide a practical, finetuning-free application for expanding the vocabulary of pre-trained models. By enabling the addition of new vocabulary words, we reduce input length and inference iterations, which reduces both space and model latency, with little to no loss in model accuracy.</li>
</ul>

<h3>Title: Unobserved Object Detection using Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Subhransu S. Bhattacharjee, Dylan Campbell, Rahul Shome</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05869">https://arxiv.org/abs/2410.05869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05869">https://arxiv.org/pdf/2410.05869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05869]] Unobserved Object Detection using Generative Models(https://arxiv.org/abs/2410.05869)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Can we detect an object that is not visible in an image? This study introduces the novel task of 2D and 3D unobserved object detection for predicting the location of objects that are occluded or lie outside the image frame. We adapt several state-of-the-art pre-trained generative models to solve this task, including 2D and 3D diffusion models and vision--language models, and show that they can be used to infer the presence of objects that are not directly observed. To benchmark this task, we propose a suite of metrics that captures different aspects of performance. Our empirical evaluations on indoor scenes from the RealEstate10k dataset with COCO object categories demonstrate results that motivate the use of generative models for the unobserved object detection task. The current work presents a promising step towards compelling applications like visual search and probabilistic planning that can leverage object detection beyond what can be directly observed.</li>
</ul>

<h3>Title: MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment</h3>
<ul>
<li><strong>Authors: </strong>Amir Hossein Kargaran, Ali Modarressi, Nafiseh Nikeghbal, Jana Diesner, François Yvon, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05873">https://arxiv.org/abs/2410.05873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05873">https://arxiv.org/pdf/2410.05873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05873]] MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment(https://arxiv.org/abs/2410.05873)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>English-centric large language models (LLMs) often show strong multilingual capabilities. However, the multilingual performance of these models remains unclear and is not thoroughly evaluated for many languages. Most benchmarks for multilinguality focus on classic NLP tasks, or cover a minimal number of languages. We introduce MEXA, a method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. MEXA leverages the fact that English-centric LLMs use English as a kind of pivot language in their intermediate layers. It computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate model performance in other languages. We conduct studies using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models. Our results show that MEXA, in its default settings, achieves a statistically significant average Pearson correlation of 0.90 with three established downstream tasks across nine models and two parallel datasets. This suggests that MEXA is a reliable method for estimating the multilingual capabilities of English-centric LLMs, providing a clearer understanding of their multilingual potential and the inner workings of LLMs. Leaderboard: this https URL, Code: this https URL.</li>
</ul>

<h3>Title: DimOL: Dimensional Awareness as A New 'Dimension' in Operator Learning</h3>
<ul>
<li><strong>Authors: </strong>Yichen Song, Yunbo Wang, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05894">https://arxiv.org/abs/2410.05894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05894">https://arxiv.org/pdf/2410.05894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05894]] DimOL: Dimensional Awareness as A New 'Dimension' in Operator Learning(https://arxiv.org/abs/2410.05894)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In the realm of computational physics, an enduring topic is the numerical solutions to partial differential equations (PDEs). Recently, the attention of researchers has shifted towards Neural Operator methods, renowned for their capability to approximate ``operators'' -- mappings from functions to functions. Despite the universal approximation theorem within neural operators, ensuring error bounds often requires employing numerous Fourier layers. However, what about lightweight models? In response to this question, we introduce DimOL (Dimension-aware Operator Learning), drawing insights from dimensional analysis. To implement DimOL, we propose the ProdLayer, which can be seamlessly integrated into FNO-based and Transformer-based PDE solvers, enhancing their ability to handle sum-of-products structures inherent in many physical systems. Empirically, DimOL models achieve up to 48% performance gain within the PDE datasets. Furthermore, by analyzing Fourier components' weights, we can symbolically discern the physical significance of each term. This sheds light on the opaque nature of neural networks, unveiling underlying physical principles.</li>
</ul>

<h3>Title: Brain-inspired continual pre-trained learner via silent synaptic consolidation</h3>
<ul>
<li><strong>Authors: </strong>Xuming Ran, Juntao Yao, Yusong Wang, Mingkun Xu, Dianbo Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05899">https://arxiv.org/abs/2410.05899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05899">https://arxiv.org/pdf/2410.05899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05899]] Brain-inspired continual pre-trained learner via silent synaptic consolidation(https://arxiv.org/abs/2410.05899)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Pre-trained models have demonstrated impressive generalization capabilities, yet they remain vulnerable to catastrophic forgetting when incrementally trained on new tasks. Existing architecture-based strategies encounter two primary challenges: 1) Integrating a pre-trained network with a trainable sub-network complicates the delicate balance between learning plasticity and memory stability across evolving tasks during learning. 2) The absence of robust interconnections between pre-trained networks and various sub-networks limits the effective retrieval of pertinent information during inference. In this study, we introduce the Artsy, inspired by the activation mechanisms of silent synapses via spike-timing-dependent plasticity observed in mature brains, to enhance the continual learning capabilities of pre-trained models. The Artsy integrates two key components: During training, the Artsy mimics mature brain dynamics by maintaining memory stability for previously learned knowledge within the pre-trained network while simultaneously promoting learning plasticity in task-specific sub-networks. During inference, artificial silent and functional synapses are utilized to establish precise connections between the pre-synaptic neurons in the pre-trained network and the post-synaptic neurons in the sub-networks, facilitated through synaptic consolidation, thereby enabling effective extraction of relevant information from test samples. Comprehensive experimental evaluations reveal that our model significantly outperforms conventional methods on class-incremental learning tasks, while also providing enhanced biological interpretability for architecture-based approaches. Moreover, we propose that the Artsy offers a promising avenue for simulating biological synaptic mechanisms, potentially advancing our understanding of neural plasticity in both artificial and biological systems.</li>
</ul>

<h3>Title: MTFL: Multi-Timescale Feature Learning for Weakly-Supervised Anomaly Detection in Surveillance Videos</h3>
<ul>
<li><strong>Authors: </strong>Yiling Zhang, Erkut Akdag, Egor Bondarev, Peter H. N. De With</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05900">https://arxiv.org/abs/2410.05900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05900">https://arxiv.org/pdf/2410.05900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05900]] MTFL: Multi-Timescale Feature Learning for Weakly-Supervised Anomaly Detection in Surveillance Videos(https://arxiv.org/abs/2410.05900)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Detection of anomaly events is relevant for public safety and requires a combination of fine-grained motion information and contextual events at variable time-scales. To this end, we propose a Multi-Timescale Feature Learning (MTFL) method to enhance the representation of anomaly features. Short, medium, and long temporal tubelets are employed to extract spatio-temporal video features using a Video Swin Transformer. Experimental results demonstrate that MTFL outperforms state-of-the-art methods on the UCF-Crime dataset, achieving an anomaly detection performance 89.78% AUC. Moreover, it performs complementary to SotA with 95.32% AUC on the ShanghaiTech and 84.57% AP on the XD-Violence dataset. Furthermore, we generate an extended dataset of the UCF-Crime for development and evaluation on a wider range of anomalies, namely Video Anomaly Detection Dataset (VADD), involving 2,591 videos in 18 classes with extensive coverage of realistic anomalies.</li>
</ul>

<h3>Title: Automatic Summarization of Long Documents</h3>
<ul>
<li><strong>Authors: </strong>Naman Chhibbar, Jugal Kalita</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05903">https://arxiv.org/abs/2410.05903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05903">https://arxiv.org/pdf/2410.05903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05903]] Automatic Summarization of Long Documents(https://arxiv.org/abs/2410.05903)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>A vast amount of textual data is added to the internet daily, making utilization and interpretation of such data difficult and cumbersome. As a result, automatic text summarization is crucial for extracting relevant information, saving precious reading time. Although many transformer-based models excel in summarization, they are constrained by their input size, preventing them from processing texts longer than their context size. This study introduces three novel algorithms that allow any LLM to efficiently overcome its input size limitation, effectively utilizing its full potential without any architectural modifications. We test our algorithms on texts with more than 70,000 words, and our experiments show a significant increase in BERTScore with competitive ROUGE scores.</li>
</ul>

<h3>Title: MedUniSeg: 2D and 3D Medical Image Segmentation via a Prompt-driven Universal Model</h3>
<ul>
<li><strong>Authors: </strong>Yiwen Ye, Ziyang Chen, Jianpeng Zhang, Yutong Xie, Yong Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05905">https://arxiv.org/abs/2410.05905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05905">https://arxiv.org/pdf/2410.05905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05905]] MedUniSeg: 2D and 3D Medical Image Segmentation via a Prompt-driven Universal Model(https://arxiv.org/abs/2410.05905)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Universal segmentation models offer significant potential in addressing a wide range of tasks by effectively leveraging discrete annotations. As the scope of tasks and modalities expands, it becomes increasingly important to generate and strategically position task- and modal-specific priors within the universal model. However, existing universal models often overlook the correlations between different priors, and the optimal placement and frequency of these priors remain underexplored. In this paper, we introduce MedUniSeg, a prompt-driven universal segmentation model designed for 2D and 3D multi-task segmentation across diverse modalities and domains. MedUniSeg employs multiple modal-specific prompts alongside a universal task prompt to accurately characterize the modalities and tasks. To generate the related priors, we propose the modal map (MMap) and the fusion and selection (FUSE) modules, which transform modal and task prompts into corresponding priors. These modal and task priors are systematically introduced at the start and end of the encoding process. We evaluate MedUniSeg on a comprehensive multi-modal upstream dataset consisting of 17 sub-datasets. The results demonstrate that MedUniSeg achieves superior multi-task segmentation performance, attaining a 1.2% improvement in the mean Dice score across the 17 upstream tasks compared to nnUNet baselines, while using less than 1/10 of the parameters. For tasks that underperform during the initial multi-task joint training, we freeze MedUniSeg and introduce new modules to re-learn these tasks. This approach yields an enhanced version, MedUniSeg*, which consistently outperforms MedUniSeg across all tasks. Moreover, MedUniSeg surpasses advanced self-supervised and supervised pre-trained models on six downstream tasks, establishing itself as a high-quality, highly generalizable pre-trained segmentation model.</li>
</ul>

<h3>Title: Accelerating Error Correction Code Transformers</h3>
<ul>
<li><strong>Authors: </strong>Matan Levy, Yoni Choukroun, Lior Wolf</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05911">https://arxiv.org/abs/2410.05911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05911">https://arxiv.org/pdf/2410.05911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05911]] Accelerating Error Correction Code Transformers(https://arxiv.org/abs/2410.05911)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Error correction codes (ECC) are crucial for ensuring reliable information transmission in communication systems. Choukroun & Wolf (2022b) recently introduced the Error Correction Code Transformer (ECCT), which has demonstrated promising performance across various transmission channels and families of codes. However, its high computational and memory demands limit its practical applications compared to traditional decoding algorithms. Achieving effective quantization of the ECCT presents significant challenges due to its inherently small architecture, since existing, very low-precision quantization techniques often lead to performance degradation in compact neural networks. In this paper, we introduce a novel acceleration method for transformer-based decoders. We first propose a ternary weight quantization method specifically designed for the ECCT, inducing a decoder with multiplication-free linear layers. We present an optimized self-attention mechanism to reduce computational complexity via codeaware multi-heads processing. Finally, we provide positional encoding via the Tanner graph eigendecomposition, enabling a richer representation of the graph connectivity. The approach not only matches or surpasses ECCT's performance but also significantly reduces energy consumption, memory footprint, and computational complexity. Our method brings transformer-based error correction closer to practical implementation in resource-constrained environments, achieving a 90% compression ratio and reducing arithmetic operation energy consumption by at least 224 times on modern hardware.</li>
</ul>

<h3>Title: Give me a hint: Can LLMs take a hint to solve math problems?</h3>
<ul>
<li><strong>Authors: </strong>Vansh Agrawal, Pratham Singla, Amitoj Singh Miglani, Shivank Garg, Ayush Mangal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05915">https://arxiv.org/abs/2410.05915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05915">https://arxiv.org/pdf/2410.05915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05915]] Give me a hint: Can LLMs take a hint to solve math problems?(https://arxiv.org/abs/2410.05915)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While many state-of-the-art LLMs have shown poor logical and basic mathematical reasoning, recent works try to improve their problem-solving abilities using prompting techniques. We propose giving "hints" to improve the language model's performance on advanced mathematical problems, taking inspiration from how humans approach math pedagogically. We also test the model's adversarial robustness to wrong hints. We demonstrate the effectiveness of our approach by evaluating various LLMs, presenting them with a diverse set of problems of different difficulties and topics from the MATH dataset and comparing against techniques such as one-shot, few-shot, and chain of thought prompting.</li>
</ul>

<h3>Title: TIMBA: Time series Imputation with Bi-directional Mamba Blocks and Diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Javier Solís-García, Belén Vega-Márquez, Juan A. Nepomuceno, Isabel A. Nepomuceno-Chamorro</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05916">https://arxiv.org/abs/2410.05916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05916">https://arxiv.org/pdf/2410.05916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05916]] TIMBA: Time series Imputation with Bi-directional Mamba Blocks and Diffusion models(https://arxiv.org/abs/2410.05916)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>The problem of imputing multivariate time series spans a wide range of fields, from clinical healthcare to multi-sensor systems. Initially, Recurrent Neural Networks (RNNs) were employed for this task; however, their error accumulation issues led to the adoption of Transformers, leveraging attention mechanisms to mitigate these problems. Concurrently, the promising results of diffusion models in capturing original distributions have positioned them at the forefront of current research, often in conjunction with Transformers. In this paper, we propose replacing time-oriented Transformers with State-Space Models (SSM), which are better suited for temporal data modeling. Specifically, we utilize the latest SSM variant, S6, which incorporates attention-like mechanisms. By embedding S6 within Mamba blocks, we develop a model that integrates SSM, Graph Neural Networks, and node-oriented Transformers to achieve enhanced spatiotemporal representations. Implementing these architectural modifications, previously unexplored in this field, we present Time series Imputation with Bi-directional mamba blocks and diffusion models (TIMBA). TIMBA achieves superior performance in almost all benchmark scenarios and performs comparably in others across a diverse range of missing value situations and three real-world datasets. We also evaluate how the performance of our model varies with different amounts of missing values and analyse its performance on downstream tasks. In addition, we provide the original code to replicate the results.</li>
</ul>

<h3>Title: Fortify Your Foundations: Practical Privacy and Security for Foundation Model Deployments In The Cloud</h3>
<ul>
<li><strong>Authors: </strong>Marcin Chrapek, Anjo Vahldiek-Oberwagner, Marcin Spoczynski, Scott Constable, Mona Vij, Torsten Hoefler</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05930">https://arxiv.org/abs/2410.05930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05930">https://arxiv.org/pdf/2410.05930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05930]] Fortify Your Foundations: Practical Privacy and Security for Foundation Model Deployments In The Cloud(https://arxiv.org/abs/2410.05930)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Foundation Models (FMs) display exceptional performance in tasks such as natural language processing and are being applied across a growing range of disciplines. Although typically trained on large public datasets, FMs are often fine-tuned or integrated into Retrieval-Augmented Generation (RAG) systems, which rely on private data. This access, along with their size and costly training, heightens the risk of intellectual property theft. Moreover, multimodal FMs may expose sensitive information. In this work, we examine the FM threat model and discuss the practicality and comprehensiveness of various approaches for securing against them, such as ML-based methods and trusted execution environments (TEEs). We demonstrate that TEEs offer an effective balance between strong security properties, usability, and performance. Specifically, we present a solution achieving less than 10\% overhead versus bare metal for the full Llama2 7B and 13B inference pipelines running inside \intel\ SGX and \intel\ TDX. We also share our configuration files and insights from our implementation. To our knowledge, our work is the first to show the practicality of TEEs for securing FMs.</li>
</ul>

<h3>Title: Chameleon: An Efficient FHE Scheme Switching Acceleration on GPUs</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Wang, Haoqi He, Lutan Zhao, Peinan Li, Zhihao Li, Dan Meng, Rui Hou</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05934">https://arxiv.org/abs/2410.05934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05934">https://arxiv.org/pdf/2410.05934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05934]] Chameleon: An Efficient FHE Scheme Switching Acceleration on GPUs(https://arxiv.org/abs/2410.05934)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Fully homomorphic encryption (FHE) enables direct computation on encrypted data, making it a crucial technology for privacy protection. However, FHE suffers from significant performance bottlenecks. In this context, GPU acceleration offers a promising solution to bridge the performance gap. Existing efforts primarily focus on single-class FHE schemes, which fail to meet the diverse requirements of data types and functions, prompting the development of hybrid multi-class FHE schemes. However, studies have yet to thoroughly investigate specific GPU optimizations for hybrid FHE schemes. In this paper, we present an efficient GPU-based FHE scheme switching acceleration named Chameleon. First, we propose a scalable NTT acceleration design that adapts to larger CKKS polynomials and smaller TFHE polynomials. Specifically, Chameleon tackles synchronization issues by fusing stages to reduce synchronization, employing polynomial coefficient shuffling to minimize synchronization scale, and utilizing an SM-aware combination strategy to identify the optimal switching point. Second, Chameleon is the first to comprehensively analyze and optimize critical switching operations. It introduces CMux-level parallelization to accelerate LUT evaluation and a homomorphic rotation-free matrix-vector multiplication to improve repacking efficiency. Finally, Chameleon outperforms the state-of-the-art GPU implementations by 1.23x in CKKS HMUL and 1.15x in bootstrapping. It also achieves up to 4.87x and 1.51x speedups for TFHE gate bootstrapping compared to CPU and GPU versions, respectively, and delivers a 67.3x average speedup for scheme switching over CPU-based implementation.</li>
</ul>

<h3>Title: EMMA: Empowering Multi-modal Mamba with Structural and Hierarchical Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yifei Xing, Xiangyuan Lan, Ruiping Wang, Dongmei Jiang, Wenjun Huang, Qingfang Zheng, Yaowei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05938">https://arxiv.org/abs/2410.05938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05938">https://arxiv.org/pdf/2410.05938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05938]] EMMA: Empowering Multi-modal Mamba with Structural and Hierarchical Alignment(https://arxiv.org/abs/2410.05938)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Mamba-based architectures have shown to be a promising new direction for deep learning models owing to their competitive performance and sub-quadratic deployment speed. However, current Mamba multi-modal large language models (MLLM) are insufficient in extracting visual features, leading to imbalanced cross-modal alignment between visual and textural latents, negatively impacting performance on multi-modal tasks. In this work, we propose Empowering Multi-modal Mamba with Structural and Hierarchical Alignment (EMMA), which enables the MLLM to extract fine-grained visual information. Specifically, we propose a pixel-wise alignment module to autoregressively optimize the learning and processing of spatial image-level features along with textual tokens, enabling structural alignment at the image level. In addition, to prevent the degradation of visual information during the cross-model alignment process, we propose a multi-scale feature fusion (MFF) module to combine multi-scale visual features from intermediate layers, enabling hierarchical alignment at the feature level. Extensive experiments are conducted across a variety of multi-modal benchmarks. Our model shows lower latency than other Mamba-based MLLMs and is nearly four times faster than transformer-based MLLMs of similar scale during inference. Due to better cross-modal alignment, our model exhibits lower degrees of hallucination and enhanced sensitivity to visual details, which manifests in superior performance across diverse multi-modal benchmarks. Code will be provided.</li>
</ul>

<h3>Title: TouchInsight: Uncertainty-aware Rapid Touch and Text Input for Mixed Reality from Egocentric Vision</h3>
<ul>
<li><strong>Authors: </strong>Paul Streli, Mark Richardson, Fadi Botros, Shugao Ma, Robert Wang, Christian Holz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05940">https://arxiv.org/abs/2410.05940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05940">https://arxiv.org/pdf/2410.05940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05940]] TouchInsight: Uncertainty-aware Rapid Touch and Text Input for Mixed Reality from Egocentric Vision(https://arxiv.org/abs/2410.05940)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While passive surfaces offer numerous benefits for interaction in mixed reality, reliably detecting touch input solely from head-mounted cameras has been a long-standing challenge. Camera specifics, hand self-occlusion, and rapid movements of both head and fingers introduce considerable uncertainty about the exact location of touch events. Existing methods have thus not been capable of achieving the performance needed for robust interaction. In this paper, we present a real-time pipeline that detects touch input from all ten fingers on any physical surface, purely based on egocentric hand tracking. Our method TouchInsight comprises a neural network to predict the moment of a touch event, the finger making contact, and the touch location. TouchInsight represents locations through a bivariate Gaussian distribution to account for uncertainties due to sensing inaccuracies, which we resolve through contextual priors to accurately infer intended user input. We first evaluated our method offline and found that it locates input events with a mean error of 6.3 mm, and accurately detects touch events (F1=0.99) and identifies the finger used (F1=0.96). In an online evaluation, we then demonstrate the effectiveness of our approach for a core application of dexterous touch input: two-handed text entry. In our study, participants typed 37.0 words per minute with an uncorrected error rate of 2.9% on average.</li>
</ul>

<h3>Title: Hyper Adversarial Tuning for Boosting Adversarial Robustness of Pretrained Large Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Kangtao Lv, Huangsen Cao, Kainan Tu, Yihuai Xu, Zhimeng Zhang, Xin Ding, Yongwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05951">https://arxiv.org/abs/2410.05951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05951">https://arxiv.org/pdf/2410.05951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05951]] Hyper Adversarial Tuning for Boosting Adversarial Robustness of Pretrained Large Vision Models(https://arxiv.org/abs/2410.05951)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust</a></li>
<li><strong>Abstract: </strong>Large vision models have been found vulnerable to adversarial examples, emphasizing the need for enhancing their adversarial robustness. While adversarial training is an effective defense for deep convolutional models, it often faces scalability issues with large vision models due to high computational costs. Recent approaches propose robust fine-tuning methods, such as adversarial tuning of low-rank adaptation (LoRA) in large vision models, but they still struggle to match the accuracy of full parameter adversarial fine-tuning. The integration of various defense mechanisms offers a promising approach to enhancing the robustness of large vision models, yet this paradigm remains underexplored. To address this, we propose hyper adversarial tuning (HyperAT), which leverages shared defensive knowledge among different methods to improve model robustness efficiently and effectively simultaneously. Specifically, adversarial tuning of each defense method is formulated as a learning task, and a hypernetwork generates LoRA specific to this defense. Then, a random sampling and tuning strategy is proposed to extract and facilitate the defensive knowledge transfer between different defenses. Finally, diverse LoRAs are merged to enhance the adversarial robustness. Experiments on various datasets and model architectures demonstrate that HyperAT significantly enhances the adversarial robustness of pretrained large vision models without excessive computational overhead, establishing a new state-of-the-art benchmark.</li>
</ul>

<h3>Title: Active Evaluation Acquisition for Efficient LLM Benchmarking</h3>
<ul>
<li><strong>Authors: </strong>Yang Li, Jie Ma, Miguel Ballesteros, Yassine Benajiba, Graham Horwood</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05952">https://arxiv.org/abs/2410.05952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05952">https://arxiv.org/pdf/2410.05952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05952]] Active Evaluation Acquisition for Efficient LLM Benchmarking(https://arxiv.org/abs/2410.05952)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly versatile, numerous large scale benchmarks have been developed to thoroughly assess their capabilities. These benchmarks typically consist of diverse datasets and prompts to evaluate different aspects of LLM performance. However, comprehensive evaluations on hundreds or thousands of prompts incur tremendous costs in terms of computation, money, and time. In this work, we investigate strategies to improve evaluation efficiency by selecting a subset of examples from each benchmark using a learned policy. Our approach models the dependencies across test examples, allowing accurate prediction of the evaluation outcomes for the remaining examples based on the outcomes of the selected ones. Consequently, we only need to acquire the actual evaluation outcomes for the selected subset. We rigorously explore various subset selection policies and introduce a novel RL-based policy that leverages the captured dependencies. Empirical results demonstrate that our approach significantly reduces the number of evaluation prompts required while maintaining accurate performance estimates compared to previous methods.</li>
</ul>

<h3>Title: Pyramidal Flow Matching for Efficient Video Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, Zhouchen Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05954">https://arxiv.org/abs/2410.05954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05954">https://arxiv.org/pdf/2410.05954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05954]] Pyramidal Flow Matching for Efficient Video Generative Modeling(https://arxiv.org/abs/2410.05954)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing approaches employ a cascaded architecture to avoid direct training with full resolution. Despite reducing computational demands, the separate optimization of each sub-stage hinders knowledge sharing and sacrifices flexibility. This work introduces a unified pyramidal flow matching algorithm. It reinterprets the original denoising trajectory as a series of pyramid stages, where only the final stage operates at the full resolution, thereby enabling more efficient video generative modeling. Through our sophisticated design, the flows of different pyramid stages can be interlinked to maintain continuity. Moreover, we craft autoregressive video generation with a temporal pyramid to compress the full-resolution history. The entire framework can be optimized in an end-to-end manner and with a single unified Diffusion Transformer (DiT). Extensive experiments demonstrate that our method supports generating high-quality 5-second (up to 10-second) videos at 768p resolution and 24 FPS within 20.7k A100 GPU training hours. All code and models will be open-sourced at this https URL.</li>
</ul>

<h3>Title: Training-Free Open-Ended Object Detection and Segmentation via Attention as Prompts</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Lin, Yongtao Wang, Zhi Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05963">https://arxiv.org/abs/2410.05963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05963">https://arxiv.org/pdf/2410.05963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05963]] Training-Free Open-Ended Object Detection and Segmentation via Attention as Prompts(https://arxiv.org/abs/2410.05963)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Existing perception models achieve great success by learning from large amounts of labeled data, but they still struggle with open-world scenarios. To alleviate this issue, researchers introduce open-set perception tasks to detect or segment unseen objects in the training set. However, these models require predefined object categories as inputs during inference, which are not available in real-world scenarios. Recently, researchers pose a new and more practical problem, \textit{i.e.}, open-ended object detection, which discovers unseen objects without any object categories as inputs. In this paper, we present VL-SAM, a training-free framework that combines the generalized object recognition model (\textit{i.e.,} Vision-Language Model) with the generalized object localization model (\textit{i.e.,} Segment-Anything Model), to address the open-ended object detection and segmentation task. Without additional training, we connect these two generalized models with attention maps as the prompts. Specifically, we design an attention map generation module by employing head aggregation and a regularized attention flow to aggregate and propagate attention maps across all heads and layers in VLM, yielding high-quality attention maps. Then, we iteratively sample positive and negative points from the attention maps with a prompt generation module and send the sampled points to SAM to segment corresponding objects. Experimental results on the long-tail instance segmentation dataset (LVIS) show that our method surpasses the previous open-ended method on the object detection task and can provide additional instance segmentation masks. Besides, VL-SAM achieves favorable performance on the corner case object detection dataset (CODA), demonstrating the effectiveness of VL-SAM in real-world applications. Moreover, VL-SAM exhibits good model generalization that can incorporate various VLMs and SAMs.</li>
</ul>

<h3>Title: STNet: Deep Audio-Visual Fusion Network for Robust Speaker Tracking</h3>
<ul>
<li><strong>Authors: </strong>Yidi Li, Hong Liu, Bing Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05964">https://arxiv.org/abs/2410.05964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05964">https://arxiv.org/pdf/2410.05964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05964]] STNet: Deep Audio-Visual Fusion Network for Robust Speaker Tracking(https://arxiv.org/abs/2410.05964)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Audio-visual speaker tracking aims to determine the location of human targets in a scene using signals captured by a multi-sensor platform, whose accuracy and robustness can be improved by multi-modal fusion methods. Recently, several fusion methods have been proposed to model the correlation in multiple modalities. However, for the speaker tracking problem, the cross-modal interaction between audio and visual signals hasn't been well exploited. To this end, we present a novel Speaker Tracking Network (STNet) with a deep audio-visual fusion model in this work. We design a visual-guided acoustic measurement method to fuse heterogeneous cues in a unified localization space, which employs visual observations via a camera model to construct the enhanced acoustic map. For feature fusion, a cross-modal attention module is adopted to jointly model multi-modal contexts and interactions. The correlated information between audio and visual features is further interacted in the fusion model. Moreover, the STNet-based tracker is applied to multi-speaker cases by a quality-aware module, which evaluates the reliability of multi-modal observations to achieve robust tracking in complex scenarios. Experiments on the AV16.3 and CAV3D datasets show that the proposed STNet-based tracker outperforms uni-modal methods and state-of-the-art audio-visual speaker trackers.</li>
</ul>

<h3>Title: FLOPS: Forward Learning with OPtimal Sampling</h3>
<ul>
<li><strong>Authors: </strong>Tao Ren, Zishi Zhang, Jinyang Jiang, Guanghao Li, Zeliang Zhang, Mingqian Feng, Yijie Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05966">https://arxiv.org/abs/2410.05966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05966">https://arxiv.org/pdf/2410.05966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05966]] FLOPS: Forward Learning with OPtimal Sampling(https://arxiv.org/abs/2410.05966)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Given the limitations of backpropagation, perturbation-based gradient computation methods have recently gained focus for learning with only forward passes, also referred to as queries. Conventional forward learning consumes enormous queries on each data point for accurate gradient estimation through Monte Carlo sampling, which hinders the scalability of those algorithms. However, not all data points deserve equal queries for gradient estimation. In this paper, we study the problem of improving the forward learning efficiency from a novel perspective: how to reduce the gradient estimation variance with minimum cost? For this, we propose to allocate the optimal number of queries over each data in one batch during training to achieve a good balance between estimation accuracy and computational efficiency. Specifically, with a simplified proxy objective and a reparameterization technique, we derive a novel plug-and-play query allocator with minimal parameters. Theoretical results are carried out to verify its optimality. We conduct extensive experiments for fine-tuning Vision Transformers on various datasets and further deploy the allocator to two black-box applications: prompt tuning and multimodal alignment for foundation models. All findings demonstrate that our proposed allocator significantly enhances the scalability of forward-learning algorithms, paving the way for real-world applications.</li>
</ul>

<h3>Title: Deep neural network-based detection of counterfeit products from smartphone images</h3>
<ul>
<li><strong>Authors: </strong>Hugo Garcia-Cotte, Dorra Mellouli, Abdul Rehman, Li Wang, David G. Stork</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05969">https://arxiv.org/abs/2410.05969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05969">https://arxiv.org/pdf/2410.05969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05969]] Deep neural network-based detection of counterfeit products from smartphone images(https://arxiv.org/abs/2410.05969)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Counterfeit products such as drugs and vaccines as well as luxury items such as high-fashion handbags, watches, jewelry, garments, and cosmetics, represent significant direct losses of revenue to legitimate manufacturers and vendors, as well as indirect costs to societies at large. We present the world's first purely computer-vision-based system to combat such counterfeiting-one that does not require special security tags or other alterations to the products or modifications to supply chain tracking. Our deep neural network system shows high accuracy on branded garments from our first manufacturer tested (99.71% after 3.06% rejections) using images captured under natural, weakly controlled conditions, such as in retail stores, customs checkpoints, warehouses, and outdoors. Our system, suitably transfer trained on a small number of fake and genuine articles, should find application in additional product categories as well, for example fashion accessories, perfume boxes, medicines, and more.</li>
</ul>

<h3>Title: PDF-WuKong: A Large Multimodal Model for Efficient Long PDF Reading with End-to-End Sparse Sampling</h3>
<ul>
<li><strong>Authors: </strong>Xudong Xie, Liang Yin, Hao Yan, Yang Liu, Jing Ding, Minghui Liao, Yuliang Liu, Wei Chen, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05970">https://arxiv.org/abs/2410.05970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05970">https://arxiv.org/pdf/2410.05970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05970]] PDF-WuKong: A Large Multimodal Model for Efficient Long PDF Reading with End-to-End Sparse Sampling(https://arxiv.org/abs/2410.05970)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Document understanding is a challenging task to process and comprehend large amounts of textual and visual information. Recent advances in Large Language Models (LLMs) have significantly improved the performance of this task. However, existing methods typically focus on either plain text or a limited number of document images, struggling to handle long PDF documents with interleaved text and images, especially in academic papers. In this paper, we introduce PDF-WuKong, a multimodal large language model (MLLM) which is designed to enhance multimodal question-answering (QA) for long PDF documents. PDF-WuKong incorporates a sparse sampler that operates on both text and image representations, significantly improving the efficiency and capability of the MLLM. The sparse sampler is integrated with the MLLM's image encoder and selects the paragraphs or diagrams most pertinent to user queries for processing by the language model. To effectively train and evaluate our model, we construct PaperPDF, a dataset consisting of a broad collection of academic papers sourced from arXiv, multiple strategies are proposed to generate automatically 1M QA pairs along with their corresponding evidence sources. Experimental results demonstrate the superiority and high efficiency of our approach over other models on the task of long multimodal PDF understanding, surpassing proprietary products by an average of 8.6% on F1. Our code and dataset will be released at this https URL.</li>
</ul>

<h3>Title: Generalizing to any diverse distribution: uniformity, gentle finetuning and rebalancing</h3>
<ul>
<li><strong>Authors: </strong>Andreas Loukas, Karolis Martinkus, Ed Wagstaff, Kyunghyun Cho</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05980">https://arxiv.org/abs/2410.05980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05980">https://arxiv.org/pdf/2410.05980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05980]] Generalizing to any diverse distribution: uniformity, gentle finetuning and rebalancing(https://arxiv.org/abs/2410.05980)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As training datasets grow larger, we aspire to develop models that generalize well to any diverse test distribution, even if the latter deviates significantly from the training data. Various approaches like domain adaptation, domain generalization, and robust optimization attempt to address the out-of-distribution challenge by posing assumptions about the relation between training and test distribution. Differently, we adopt a more conservative perspective by accounting for the worst-case error across all sufficiently diverse test distributions within a known domain. Our first finding is that training on a uniform distribution over this domain is optimal. We also interrogate practical remedies when uniform samples are unavailable by considering methods for mitigating non-uniformity through finetuning and rebalancing. Our theory provides a mathematical grounding for previous observations on the role of entropy and rebalancing for o.o.d. generalization and foundation model training. We also provide new empirical evidence across tasks involving o.o.d. shifts which illustrate the broad applicability of our perspective.</li>
</ul>

<h3>Title: Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG</h3>
<ul>
<li><strong>Authors: </strong>Bowen Jin, Jinsung Yoon, Jiawei Han, Sercan O. Arik</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05983">https://arxiv.org/abs/2410.05983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05983">https://arxiv.org/pdf/2410.05983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05983]] Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG(https://arxiv.org/abs/2410.05983)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) empowers large language models (LLMs) to utilize external knowledge sources. The increasing capacity of LLMs to process longer input sequences opens up avenues for providing more retrieved information, to potentially enhance the quality of generated outputs. It is plausible to assume that a larger retrieval set would contain more relevant information (higher recall), that might result in improved performance. However, our empirical findings demonstrate that for many long-context LLMs, the quality of generated output initially improves first, but then subsequently declines as the number of retrieved passages increases. This paper investigates this phenomenon, identifying the detrimental impact of retrieved "hard negatives" as a key contributor. To mitigate this and enhance the robustness of long-context LLM-based RAG, we propose both training-free and training-based approaches. We first showcase the effectiveness of retrieval reordering as a simple yet powerful training-free optimization. Furthermore, we explore training-based methods, specifically RAG-specific implicit LLM fine-tuning and RAG-oriented fine-tuning with intermediate reasoning, demonstrating their capacity for substantial performance gains. Finally, we conduct a systematic analysis of design choices for these training-based methods, including data distribution, retriever selection, and training context length.</li>
</ul>

<h3>Title: Asynchronous Stochastic Gradient Descent with Decoupled Backpropagation and Layer-Wise Updates</h3>
<ul>
<li><strong>Authors: </strong>Cabrel Teguemne Fokam, Khaleelulla Khan Nazeer, Lukas König, David Kappel, Anand Subramoney</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05985">https://arxiv.org/abs/2410.05985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05985">https://arxiv.org/pdf/2410.05985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05985]] Asynchronous Stochastic Gradient Descent with Decoupled Backpropagation and Layer-Wise Updates(https://arxiv.org/abs/2410.05985)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The increasing size of deep learning models has created the need for more efficient alternatives to the standard error backpropagation algorithm, that make better use of asynchronous, parallel and distributed computing. One major shortcoming of backpropagation is the interlocking between the forward phase of the algorithm, which computes a global loss, and the backward phase where the loss is backpropagated through all layers to compute the gradients, which are used to update the network parameters. To address this problem, we propose a method that parallelises SGD updates across the layers of a model by asynchronously updating them from multiple threads. Furthermore, since we observe that the forward pass is often much faster than the backward pass, we use separate threads for the forward and backward pass calculations, which allows us to use a higher ratio of forward to backward threads than the usual 1:1 ratio, reducing the overall staleness of the parameters. Thus, our approach performs asynchronous stochastic gradient descent using separate threads for the loss (forward) and gradient (backward) computations and performs layer-wise partial updates to parameters in a distributed way. We show that this approach yields close to state-of-the-art results while running up to 2.97x faster than Hogwild! scaled on multiple devices (Locally-Partitioned-Asynchronous-Parallel SGD). We theoretically prove the convergence of the algorithm using a novel theoretical framework based on stochastic differential equations and the drift diffusion process, by modeling the asynchronous parameter updates as a stochastic process.</li>
</ul>

<h3>Title: Vector Grimoire: Codebook-based Shape Generation under Raster Image Supervision</h3>
<ul>
<li><strong>Authors: </strong>Moritz Feuerpfeil, Marco Cipriano, Gerard de Melo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05991">https://arxiv.org/abs/2410.05991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05991">https://arxiv.org/pdf/2410.05991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05991]] Vector Grimoire: Codebook-based Shape Generation under Raster Image Supervision(https://arxiv.org/abs/2410.05991)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Scalable Vector Graphics (SVG) is a popular format on the web and in the design industry. However, despite the great strides made in generative modeling, SVG has remained underexplored due to the discrete and complex nature of such data. We introduce GRIMOIRE, a text-guided SVG generative model that is comprised of two modules: A Visual Shape Quantizer (VSQ) learns to map raster images onto a discrete codebook by reconstructing them as vector shapes, and an Auto-Regressive Transformer (ART) models the joint probability distribution over shape tokens, positions and textual descriptions, allowing us to generate vector graphics from natural language. Unlike existing models that require direct supervision from SVG data, GRIMOIRE learns shape image patches using only raster image supervision which opens up vector generative modeling to significantly more data. We demonstrate the effectiveness of our method by fitting GRIMOIRE for closed filled shapes on the MNIST and for outline strokes on icon and font data, surpassing previous image-supervised methods in generative quality and vector-supervised approach in flexibility.</li>
</ul>

<h3>Title: Is the MMI Criterion Necessary for Interpretability? Degenerating Non-causal Features to Plain Noise for Self-Rationalization</h3>
<ul>
<li><strong>Authors: </strong>Wei Liu, Zhiying Deng, Zhongyu Niu, Jun Wang, Haozhao Wang, YuanKai Zhang, Ruixuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06003">https://arxiv.org/abs/2410.06003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06003">https://arxiv.org/pdf/2410.06003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06003]] Is the MMI Criterion Necessary for Interpretability? Degenerating Non-causal Features to Plain Noise for Self-Rationalization(https://arxiv.org/abs/2410.06003)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>An important line of research in the field of explainability is to extract a small subset of crucial rationales from the full input. The most widely used criterion for rationale extraction is the maximum mutual information (MMI) criterion. However, in certain datasets, there are spurious features non-causally correlated with the label and also get high mutual information, complicating the loss landscape of MMI. Although some penalty-based methods have been developed to penalize the spurious features (e.g., invariance penalty, intervention penalty, etc) to help MMI work better, these are merely remedial measures. In the optimization objectives of these methods, spurious features are still distinguished from plain noise, which hinders the discovery of causal rationales. This paper aims to develop a new criterion that treats spurious features as plain noise, allowing the model to work on datasets rich in spurious features as if it were working on clean datasets, thereby making rationale extraction easier. We theoretically observe that removing either plain noise or spurious features from the input does not alter the conditional distribution of the remaining components relative to the task label. However, significant changes in the conditional distribution occur only when causal features are eliminated. Based on this discovery, the paper proposes a criterion for \textbf{M}aximizing the \textbf{R}emaining \textbf{D}iscrepancy (MRD). Experiments on six widely used datasets show that our MRD criterion improves rationale quality (measured by the overlap with human-annotated rationales) by up to $10.4\%$ as compared to several recent competitive MMI variants. Code: \url{this https URL}.</li>
</ul>

<h3>Title: Unveiling Transformer Perception by Exploring Input Manifolds</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Benfenati, Alfio Ferrara, Alessio Marta, Davide Riva, Elisabetta Rocchetti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06019">https://arxiv.org/abs/2410.06019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06019">https://arxiv.org/pdf/2410.06019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06019]] Unveiling Transformer Perception by Exploring Input Manifolds(https://arxiv.org/abs/2410.06019)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, transformer</a></li>
<li><strong>Abstract: </strong>This paper introduces a general method for the exploration of equivalence classes in the input space of Transformer models. The proposed approach is based on sound mathematical theory which describes the internal layers of a Transformer architecture as sequential deformations of the input manifold. Using eigendecomposition of the pullback of the distance metric defined on the output space through the Jacobian of the model, we are able to reconstruct equivalence classes in the input space and navigate across them. We illustrate how this method can be used as a powerful tool for investigating how a Transformer sees the input space, facilitating local and task-agnostic explainability in Computer Vision and Natural Language Processing tasks.</li>
</ul>

<h3>Title: QT-DoG: Quantization-aware Training for Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Saqib Javed, Hieu Le, Mathieu Salzmann</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06020">https://arxiv.org/abs/2410.06020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06020">https://arxiv.org/pdf/2410.06020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06020]] QT-DoG: Quantization-aware Training for Domain Generalization(https://arxiv.org/abs/2410.06020)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Domain Generalization (DG) aims to train models that perform well not only on the training (source) domains but also on novel, unseen target data distributions. A key challenge in DG is preventing overfitting to source domains, which can be mitigated by finding flatter minima in the loss landscape. In this work, we propose Quantization-aware Training for Domain Generalization (QT-DoG) and demonstrate that weight quantization effectively leads to flatter minima in the loss landscape, thereby enhancing domain generalization. Unlike traditional quantization methods focused on model compression, QT-DoG exploits quantization as an implicit regularizer by inducing noise in model weights, guiding the optimization process toward flatter minima that are less sensitive to perturbations and overfitting. We provide both theoretical insights and empirical evidence demonstrating that quantization inherently encourages flatter minima, leading to better generalization across domains. Moreover, with the benefit of reducing the model size through quantization, we demonstrate that an ensemble of multiple quantized models further yields superior accuracy than the state-of-the-art DG approaches with no computational or memory overheads. Our extensive experiments demonstrate that QT-DoG generalizes across various datasets, architectures, and quantization algorithms, and can be combined with other DG methods, establishing its versatility and robustness.</li>
</ul>

<h3>Title: Jet Expansions of Residual Computation</h3>
<ul>
<li><strong>Authors: </strong>Yihong Chen, Xiangxiang Xu, Yao Lu, Pontus Stenetorp, Luca Franceschi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.SC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06024">https://arxiv.org/abs/2410.06024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06024">https://arxiv.org/pdf/2410.06024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06024]] Jet Expansions of Residual Computation(https://arxiv.org/abs/2410.06024)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, data-free, transformer, large language model</a></li>
<li><strong>Abstract: </strong>We introduce a framework for expanding residual computational graphs using jets, operators that generalize truncated Taylor series. Our method provides a systematic approach to disentangle contributions of different computational paths to model predictions. In contrast to existing techniques such as distillation, probing, or early decoding, our expansions rely solely on the model itself and requires no data, training, or sampling from the model. We demonstrate how our framework grounds and subsumes logit lens, reveals a (super-)exponential path structure in the recursive residual depth and opens up several applications. These include sketching a transformer large language model with $n$-gram statistics extracted from its computations, and indexing the models' levels of toxicity knowledge. Our approach enables data-free analysis of residual computation for model interpretability, development, and evaluation.</li>
</ul>

<h3>Title: Sparse Repellency for Shielded Generation in Text-to-image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Michael Kirchhof, James Thornton, Pierre Ablin, Louis Béthune, Eugene Ndiaye, Marco Cuturi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06025">https://arxiv.org/abs/2410.06025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06025">https://arxiv.org/pdf/2410.06025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06025]] Sparse Repellency for Shielded Generation in Text-to-image Diffusion Models(https://arxiv.org/abs/2410.06025)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, fair, diffusion</a></li>
<li><strong>Abstract: </strong>The increased adoption of diffusion models in text-to-image generation has triggered concerns on their reliability. Such models are now closely scrutinized under the lens of various metrics, notably calibration, fairness, or compute efficiency. We focus in this work on two issues that arise when deploying these models: a lack of diversity when prompting images, and a tendency to recreate images from the training set. To solve both problems, we propose a method that coaxes the sampled trajectories of pretrained diffusion models to land on images that fall outside of a reference set. We achieve this by adding repellency terms to the diffusion SDE throughout the generation trajectory, which are triggered whenever the path is expected to land too closely to an image in the shielded reference set. Our method is sparse in the sense that these repellency terms are zero and inactive most of the time, and even more so towards the end of the generation trajectory. Our method, named SPELL for sparse repellency, can be used either with a static reference set that contains protected images, or dynamically, by updating the set at each timestep with the expected images concurrently generated within a batch. We show that adding SPELL to popular diffusion models improves their diversity while impacting their FID only marginally, and performs comparatively better than other recent training-free diversity methods. We also demonstrate how SPELL can ensure a shielded generation away from a very large set of protected images by considering all 1.2M images from ImageNet as the protected set.</li>
</ul>

<h3>Title: Data Quality Issues in Vulnerability Detection Datasets</h3>
<ul>
<li><strong>Authors: </strong>Yuejun Guo, Seifeddine Bettaieb</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06030">https://arxiv.org/abs/2410.06030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06030">https://arxiv.org/pdf/2410.06030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06030]] Data Quality Issues in Vulnerability Detection Datasets(https://arxiv.org/abs/2410.06030)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Vulnerability detection is a crucial yet challenging task to identify potential weaknesses in software for cyber security. Recently, deep learning (DL) has made great progress in automating the detection process. Due to the complex multi-layer structure and a large number of parameters, a DL model requires massive labeled (vulnerable or secure) source code to gain knowledge to effectively distinguish between vulnerable and secure code. In the literature, many datasets have been created to train DL models for this purpose. However, these datasets suffer from several issues that will lead to low detection accuracy of DL models. In this paper, we define three critical issues (i.e., data imbalance, low vulnerability coverage, biased vulnerability distribution) that can significantly affect the model performance and three secondary issues (i.e., errors in source code, mislabeling, noisy historical data) that also affect the performance but can be addressed through a dedicated pre-processing procedure. In addition, we conduct a study of 14 papers along with 54 datasets for vulnerability detection to confirm these defined issues. Furthermore, we discuss good practices to use existing datasets and to create new ones.</li>
</ul>

<h3>Title: QERA: an Analytical Framework for Quantization Error Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Cheng Zhang, Jeffrey T. H. Wong, Can Xiao, George A. Constantinides, Yiren Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06040">https://arxiv.org/abs/2410.06040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06040">https://arxiv.org/pdf/2410.06040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06040]] QERA: an Analytical Framework for Quantization Error Reconstruction(https://arxiv.org/abs/2410.06040)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>he growing number of parameters and computational demands of large language models (LLMs) present significant challenges for their efficient deployment. Recently, there is an increasing interest in quantizing weights to extremely low precision while offsetting the resulting error with low-rank, high-precision error reconstruction terms. The combination of quantization and low-rank approximation is now popular in both adapter-based, parameter-efficient fine-tuning methods such as LoftQ and low-precision inference techniques including ZeroQuant-V2. Usually, the low-rank terms are calculated via the singular value decomposition (SVD) of the weight quantization error, minimizing the Frobenius and spectral norms of the weight approximation error. Recent methods like LQ-LoRA and LQER introduced hand-crafted heuristics to minimize errors in layer outputs (activations) rather than weights, resulting improved quantization results. However, these heuristic methods lack an analytical solution to guide the design of quantization error reconstruction terms. In this paper, we revisit this problem and formulate an analytical framework, named Quantization Error Reconstruction Analysis (QERA), and offer a closed-form solution to the problem. We show QERA benefits both existing low-precision fine-tuning and inference methods -- QERA achieves a fine-tuned accuracy gain of $\Delta_{\text{acc}}$ = 6.05% of 2-bit RoBERTa-base on GLUE compared to LoftQ; and obtains $\Delta_{\text{acc}}$ = 2.97% higher post-training quantization accuracy of 4-bit Llama-3.1-70B on average than ZeroQuant-V2 and $\Delta_{\text{ppl}}$ = - 0.28 lower perplexity on WikiText2 than LQER.</li>
</ul>

<h3>Title: Block Induced Signature Generative Adversarial Network (BISGAN): Signature Spoofing Using GANs and Their Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Haadia Amjad, Kilian Goeller, Steffen Seitz, Carsten Knoll, Naseer Bajwa, Muhammad Imran Malik, Ronald Tetzlaff</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06041">https://arxiv.org/abs/2410.06041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06041">https://arxiv.org/pdf/2410.06041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06041]] Block Induced Signature Generative Adversarial Network (BISGAN): Signature Spoofing Using GANs and Their Evaluation(https://arxiv.org/abs/2410.06041)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric, generative</a></li>
<li><strong>Abstract: </strong>Deep learning is actively being used in biometrics to develop efficient identification and verification systems. Handwritten signatures are a common subset of biometric data for authentication purposes. Generative adversarial networks (GANs) learn from original and forged signatures to generate forged signatures. While most GAN techniques create a strong signature verifier, which is the discriminator, there is a need to focus more on the quality of forgeries generated by the generator model. This work focuses on creating a generator that produces forged samples that achieve a benchmark in spoofing signature verification systems. We use CycleGANs infused with Inception model-like blocks with attention heads as the generator and a variation of the SigCNN model as the base Discriminator. We train our model with a new technique that results in 80% to 100% success in signature spoofing. Additionally, we create a custom evaluation technique to act as a goodness measure of the generated forgeries. Our work advocates generator-focused GAN architectures for spoofing data quality that aid in a better understanding of biometric data generation and evaluation.</li>
</ul>

<h3>Title: HyperDet: Generalizable Detection of Synthesized Images by Generating and Merging A Mixture of Hyper LoRAs</h3>
<ul>
<li><strong>Authors: </strong>Huangsen Cao, Yongwei Wang, Yinfeng Liu, Sixian Zheng, Kangtao Lv, Zhimeng Zhang, Bo Zhang, Xin Ding, Fei Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06044">https://arxiv.org/abs/2410.06044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06044">https://arxiv.org/pdf/2410.06044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06044]] HyperDet: Generalizable Detection of Synthesized Images by Generating and Merging A Mixture of Hyper LoRAs(https://arxiv.org/abs/2410.06044)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The emergence of diverse generative vision models has recently enabled the synthesis of visually realistic images, underscoring the critical need for effectively detecting these generated images from real photos. Despite advances in this field, existing detection approaches often struggle to accurately identify synthesized images generated by different generative models. In this work, we introduce a novel and generalizable detection framework termed HyperDet, which innovatively captures and integrates shared knowledge from a collection of functionally distinct and lightweight expert detectors. HyperDet leverages a large pretrained vision model to extract general detection features while simultaneously capturing and enhancing task-specific features. To achieve this, HyperDet first groups SRM filters into five distinct groups to efficiently capture varying levels of pixel artifacts based on their different functionality and complexity. Then, HyperDet utilizes a hypernetwork to generate LoRA model weights with distinct embedding parameters. Finally, we merge the LoRA networks to form an efficient model ensemble. Also, we propose a novel objective function that balances the pixel and semantic artifacts effectively. Extensive experiments on the UnivFD and Fake2M datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance. Moreover, our work paves a new way to establish generalizable domain-specific fake image detectors based on pretrained large vision models.</li>
</ul>

<h3>Title: Extracting Finite State Machines from Transformers</h3>
<ul>
<li><strong>Authors: </strong>Rik Adriaensen, Jaron Maene</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06045">https://arxiv.org/abs/2410.06045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06045">https://arxiv.org/pdf/2410.06045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06045]] Extracting Finite State Machines from Transformers(https://arxiv.org/abs/2410.06045)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Fueled by the popularity of the transformer architecture in deep learning, several works have investigated what formal languages a transformer can learn. Nonetheless, existing results remain hard to compare and a fine-grained understanding of the trainability of transformers on regular languages is still lacking. We investigate transformers trained on regular languages from a mechanistic interpretability perspective. Using an extension of the $L^*$ algorithm, we extract Moore machines from transformers. We empirically find tighter lower bounds on the trainability of transformers, when a finite number of symbols determine the state. Additionally, our mechanistic insight allows us to characterise the regular languages a one-layer transformer can learn with good length generalisation. However, we also identify failure cases where the determining symbols get misrecognised due to saturation of the attention mechanism.</li>
</ul>

<h3>Title: AP-LDM: Attentive and Progressive Latent Diffusion Model for Training-Free High-Resolution Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Boyuan Cao, Jiaxin Ye, Yujie Wei, Hongming Shan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06055">https://arxiv.org/abs/2410.06055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06055">https://arxiv.org/pdf/2410.06055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06055]] AP-LDM: Attentive and Progressive Latent Diffusion Model for Training-Free High-Resolution Image Generation(https://arxiv.org/abs/2410.06055)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Latent diffusion models (LDMs), such as Stable Diffusion, often experience significant structural distortions when directly generating high-resolution (HR) images that exceed their original training resolutions. A straightforward and cost-effective solution is to adapt pre-trained LDMs for HR image generation; however, existing methods often suffer from poor image quality and long inference time. In this paper, we propose an Attentive and Progressive LDM (AP-LDM), a novel, training-free framework aimed at enhancing HR image quality while accelerating the generation process. AP-LDM decomposes the denoising process of LDMs into two stages: (i) attentive training-resolution denoising, and (ii) progressive high-resolution denoising. The first stage generates a latent representation of a higher-quality training-resolution image through the proposed attentive guidance, which utilizes a novel parameter-free self-attention mechanism to enhance the structural consistency. The second stage progressively performs upsampling in pixel space, alleviating the severe artifacts caused by latent space upsampling. Leveraging the effective initialization from the first stage enables denoising at higher resolutions with significantly fewer steps, enhancing overall efficiency. Extensive experimental results demonstrate that AP-LDM significantly outperforms state-of-the-art methods, delivering up to a 5x speedup in HR image generation, thereby highlighting its substantial advantages for real-world applications. Code is available at this https URL.</li>
</ul>

<h3>Title: Contrastive Learning to Fine-Tune Feature Extraction Models for the Visual Cortex</h3>
<ul>
<li><strong>Authors: </strong>Alex Mulrooney, Austin J. Brockmeier</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06067">https://arxiv.org/abs/2410.06067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06067">https://arxiv.org/pdf/2410.06067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06067]] Contrastive Learning to Fine-Tune Feature Extraction Models for the Visual Cortex(https://arxiv.org/abs/2410.06067)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Predicting the neural response to natural images in the visual cortex requires extracting relevant features from the images and relating those feature to the observed responses. In this work, we optimize the feature extraction in order to maximize the information shared between the image features and the neural response across voxels in a given region of interest (ROI) extracted from the BOLD signal measured by fMRI. We adapt contrastive learning (CL) to fine-tune a convolutional neural network, which was pretrained for image classification, such that a mapping of a given image's features are more similar to the corresponding fMRI response than to the responses to other images. We exploit the recently released Natural Scenes Dataset (Allen et al., 2022) as organized for the Algonauts Project (Gifford et al., 2023), which contains the high-resolution fMRI responses of eight subjects to tens of thousands of naturalistic images. We show that CL fine-tuning creates feature extraction models that enable higher encoding accuracy in early visual ROIs as compared to both the pretrained network and a baseline approach that uses a regression loss at the output of the network to tune it for fMRI response encoding. We investigate inter-subject transfer of the CL fine-tuned models, including subjects from another, lower-resolution dataset (Gong et al., 2023). We also pool subjects for fine-tuning to further improve the encoding performance. Finally, we examine the performance of the fine-tuned models on common image classification tasks, explore the landscape of ROI-specific models by applying dimensionality reduction on the Bhattacharya dissimilarity matrix created using the predictions on those tasks (Mao et al., 2024), and investigate lateralization of the processing for early visual ROIs using salience maps of the classifiers built on the CL-tuned models.</li>
</ul>

<h3>Title: Enforcing Interpretability in Time Series Transformers: A Concept Bottleneck Framework</h3>
<ul>
<li><strong>Authors: </strong>Angela van Sprang, Erman Acar, Willem Zuidema</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06070">https://arxiv.org/abs/2410.06070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06070">https://arxiv.org/pdf/2410.06070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06070]] Enforcing Interpretability in Time Series Transformers: A Concept Bottleneck Framework(https://arxiv.org/abs/2410.06070)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>There has been a recent push of research on Transformer-based models for long-term time series forecasting, even though they are inherently difficult to interpret and explain. While there is a large body of work on interpretability methods for various domains and architectures, the interpretability of Transformer-based forecasting models remains largely unexplored. To address this gap, we develop a framework based on Concept Bottleneck Models to enforce interpretability of time series Transformers. We modify the training objective to encourage a model to develop representations similar to predefined interpretable concepts. In our experiments, we enforce similarity using Centered Kernel Alignment, and the predefined concepts include time features and an interpretable, autoregressive surrogate model (AR). We apply the framework to the Autoformer model, and present an in-depth analysis for a variety of benchmark tasks. We find that the model performance remains mostly unaffected, while the model shows much improved interpretability. Additionally, interpretable concepts become local, which makes the trained model easily intervenable. As a proof of concept, we demonstrate a successful intervention in the scenario of a time shift in the data, which eliminates the need to retrain.</li>
</ul>

<h3>Title: Training-free LLM-generated Text Detection by Mining Token Probability Sequences</h3>
<ul>
<li><strong>Authors: </strong>Yihuai Xu, Yongwei Wang, Yifei Bi, Huangsen Cao, Zhouhan Lin, Yu Zhao, Fei Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06072">https://arxiv.org/abs/2410.06072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06072">https://arxiv.org/pdf/2410.06072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06072]] Training-free LLM-generated Text Detection by Mining Token Probability Sequences(https://arxiv.org/abs/2410.06072)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities in generating high-quality texts across diverse domains. However, the potential misuse of LLMs has raised significant concerns, underscoring the urgent need for reliable detection of LLM-generated texts. Conventional training-based detectors often struggle with generalization, particularly in cross-domain and cross-model scenarios. In contrast, training-free methods, which focus on inherent discrepancies through carefully designed statistical features, offer improved generalization and interpretability. Despite this, existing training-free detection methods typically rely on global text sequence statistics, neglecting the modeling of local discriminative features, thereby limiting their detection efficacy. In this work, we introduce a novel training-free detector, termed \textbf{Lastde} that synergizes local and global statistics for enhanced detection. For the first time, we introduce time series analysis to LLM-generated text detection, capturing the temporal dynamics of token probability sequences. By integrating these local statistics with global ones, our detector reveals significant disparities between human and LLM-generated texts. We also propose an efficient alternative, \textbf{Lastde++} to enable real-time detection. Extensive experiments on six datasets involving cross-domain, cross-model, and cross-lingual detection scenarios, under both white-box and black-box settings, demonstrated that our method consistently achieves state-of-the-art performance. Furthermore, our approach exhibits greater robustness against paraphrasing attacks compared to existing baseline methods.</li>
</ul>

<h3>Title: Scalable Mechanistic Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Jiale Chen, Dingling Yao, Adeel Pervez, Dan Alistarh, Francesco Locatello</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06074">https://arxiv.org/abs/2410.06074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06074">https://arxiv.org/pdf/2410.06074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06074]] Scalable Mechanistic Neural Networks(https://arxiv.org/abs/2410.06074)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We propose Scalable Mechanistic Neural Network (S-MNN), an enhanced neural network framework designed for scientific machine learning applications involving long temporal sequences. By reformulating the original Mechanistic Neural Network (MNN) (Pervez et al., 2024), we reduce the computational time and space complexities from cubic and quadratic with respect to the sequence length, respectively, to linear. This significant improvement enables efficient modeling of long-term dynamics without sacrificing accuracy or interpretability. Extensive experiments demonstrate that S-MNN matches the original MNN in precision while substantially reducing computational resources. Consequently, S-MNN can drop-in replace the original MNN in applications, providing a practical and efficient tool for integrating mechanistic bottlenecks into neural network models of complex dynamical systems.</li>
</ul>

<h3>Title: Diversity-Rewarded CFG Distillation</h3>
<ul>
<li><strong>Authors: </strong>Geoffrey Cideron, Andrea Agostinelli, Johan Ferret, Sertan Girgin, Romuald Elie, Olivier Bachem, Sarah Perrin, Alexandre Ramé</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06084">https://arxiv.org/abs/2410.06084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06084">https://arxiv.org/pdf/2410.06084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06084]] Diversity-Rewarded CFG Distillation(https://arxiv.org/abs/2410.06084)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models are transforming creative domains such as music generation, with inference-time strategies like Classifier-Free Guidance (CFG) playing a crucial role. However, CFG doubles inference cost while limiting originality and diversity across generated contents. In this paper, we introduce diversity-rewarded CFG distillation, a novel finetuning procedure that distills the strengths of CFG while addressing its limitations. Our approach optimises two training objectives: (1) a distillation objective, encouraging the model alone (without CFG) to imitate the CFG-augmented predictions, and (2) an RL objective with a diversity reward, promoting the generation of diverse outputs for a given prompt. By finetuning, we learn model weights with the ability to generate high-quality and diverse outputs, without any inference overhead. This also unlocks the potential of weight-based model merging strategies: by interpolating between the weights of two models (the first focusing on quality, the second on diversity), we can control the quality-diversity trade-off at deployment time, and even further boost performance. We conduct extensive experiments on the MusicLM (Agostinelli et al., 2023) text-to-music generative model, where our approach surpasses CFG in terms of quality-diversity Pareto optimality. According to human evaluators, our finetuned-then-merged model generates samples with higher quality-diversity than the base model augmented with CFG. Explore our generations at this https URL.</li>
</ul>

<h3>Title: The GDPR's Rules on Data Breaches: Analysing Their Rationales and Effects</h3>
<ul>
<li><strong>Authors: </strong>Frederik Zuiderveen Borgesius, Hadi Asghari, Noël Bangma, Jaap-Henk Hoepman</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06086">https://arxiv.org/abs/2410.06086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06086">https://arxiv.org/pdf/2410.06086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06086]] The GDPR's Rules on Data Breaches: Analysing Their Rationales and Effects(https://arxiv.org/abs/2410.06086)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>The General Data Protection Regulation (GDPR) requires an organisation that suffers a data breach to notify the competent Data Protection Authority. The organisation must also inform the relevant individuals, when a data breach threatens their rights and freedoms. This paper focuses on the following question: given the goals of the GDPR's data breach notification obligation, and we assess the obligation in the light of those goals. We refer to insights from information security and economics, and present them in a reader-friendly way for lawyers. Our main conclusion is that the GDPR's data breach rules are likely to contribute to the goals. For instance, the data breach notification obligation can nudge organisations towards better security; such an obligation enables regulators to perform their duties; and such an obligation improves transparency and accountability. However, the paper also warns that we should not have unrealistic expectations of the possibilities for people to protect their interests after a data breach notice. Likewise, we should not have high expectations of people switching to other service providers after receiving a data breach notification. Lastly, the paper calls for Data Protection Authorities to publish more information about reported data breaches. Such information can help to analyse security threats.</li>
</ul>

<h3>Title: TOWER: Tree Organized Weighting for Evaluating Complex Instructions</h3>
<ul>
<li><strong>Authors: </strong>Noah Ziems, Zhihan Zhang, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06089">https://arxiv.org/abs/2410.06089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06089">https://arxiv.org/pdf/2410.06089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06089]] TOWER: Tree Organized Weighting for Evaluating Complex Instructions(https://arxiv.org/abs/2410.06089)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluating the ability of large language models (LLMs) to follow complex human-written instructions is essential for their deployment in real-world applications. While benchmarks like Chatbot Arena use human judges to assess model performance, they are resource-intensive and time-consuming. Alternative methods using LLMs as judges, such as AlpacaEval, MT Bench, WildBench, and InFoBench offer improvements but still do not capture that certain complex instruction aspects are more important than others to follow. To address this gap, we propose a novel evaluation metric, \textsc{TOWER}, that incorporates human-judged importance into the assessment of complex instruction following. We show that human annotators agree with tree-based representations of these complex instructions nearly as much as they agree with other human annotators. We release tree-based annotations of the InFoBench dataset and the corresponding evaluation code to facilitate future research.</li>
</ul>

<h3>Title: Decoding Decoded: Understanding Hyperparameter Effects in Open-Ended Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Esteban Garces Arias, Meimingwei Li, Christian Heumann, Matthias Aßenmacher</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06097">https://arxiv.org/abs/2410.06097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06097">https://arxiv.org/pdf/2410.06097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06097]] Decoding Decoded: Understanding Hyperparameter Effects in Open-Ended Text Generation(https://arxiv.org/abs/2410.06097)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Decoding strategies for large language models (LLMs) are a critical but often underexplored aspect of text generation tasks. Since LLMs produce probability distributions over the entire vocabulary, various decoding methods have been developed to transform these probabilities into coherent and fluent text, each with its own set of hyperparameters. In this study, we present a large-scale, comprehensive analysis of how hyperparameter selection affects text quality in open-ended text generation across multiple LLMs, datasets, and evaluation metrics. Through an extensive sensitivity analysis, we provide practical guidelines for hyperparameter tuning and demonstrate the substantial influence of these choices on text quality. Using three established datasets, spanning factual domains (e.g., news) and creative domains (e.g., fiction), we show that hyperparameter tuning significantly impacts generation quality, though its effects vary across models and tasks. We offer in-depth insights into these effects, supported by both human evaluations and a synthesis of widely-used automatic evaluation metrics.</li>
</ul>

<h3>Title: RefineStyle: Dynamic Convolution Refinement for StyleGAN</h3>
<ul>
<li><strong>Authors: </strong>Siwei Xia, Xueqi Hu, Li Sun, Qingli Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06104">https://arxiv.org/abs/2410.06104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06104">https://arxiv.org/pdf/2410.06104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06104]] RefineStyle: Dynamic Convolution Refinement for StyleGAN(https://arxiv.org/abs/2410.06104)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In StyleGAN, convolution kernels are shaped by both static parameters shared across images and dynamic modulation factors $w^+\in\mathcal{W}^+$ specific to each image. Therefore, $\mathcal{W}^+$ space is often used for image inversion and editing. However, pre-trained model struggles with synthesizing out-of-domain images due to the limited capabilities of $\mathcal{W}^+$ and its resultant kernels, necessitating full fine-tuning or adaptation through a complex hypernetwork. This paper proposes an efficient refining strategy for dynamic kernels. The key idea is to modify kernels by low-rank residuals, learned from input image or domain guidance. These residuals are generated by matrix multiplication between two sets of tokens with the same number, which controls the complexity. We validate the refining scheme in image inversion and domain adaptation. In the former task, we design grouped transformer blocks to learn these token sets by one- or two-stage training. In the latter task, token sets are directly optimized to support synthesis in the target domain while preserving original content. Extensive experiments show that our method achieves low distortions for image inversion and high quality for out-of-domain editing.</li>
</ul>

<h3>Title: UnSeGArmaNet: Unsupervised Image Segmentation using Graph Neural Networks with Convolutional ARMA Filters</h3>
<ul>
<li><strong>Authors: </strong>Kovvuri Sai Gopal Reddy, Bodduluri Saran, A. Mudit Adityaja, Saurabh J. Shigwan, Nitin Kumar, Snehasis Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06114">https://arxiv.org/abs/2410.06114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06114">https://arxiv.org/pdf/2410.06114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06114]] UnSeGArmaNet: Unsupervised Image Segmentation using Graph Neural Networks with Convolutional ARMA Filters(https://arxiv.org/abs/2410.06114)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The data-hungry approach of supervised classification drives the interest of the researchers toward unsupervised approaches, especially for problems such as medical image segmentation, where labeled data are difficult to get. Motivated by the recent success of Vision transformers (ViT) in various computer vision tasks, we propose an unsupervised segmentation framework with a pre-trained ViT. Moreover, by harnessing the graph structure inherent within the image, the proposed method achieves a notable performance in segmentation, especially in medical images. We further introduce a modularity-based loss function coupled with an Auto-Regressive Moving Average (ARMA) filter to capture the inherent graph topology within the image. Finally, we observe that employing Scaled Exponential Linear Unit (SELU) and SILU (Swish) activation functions within the proposed Graph Neural Network (GNN) architecture enhances the performance of segmentation. The proposed method provides state-of-the-art performance (even comparable to supervised methods) on benchmark image segmentation datasets such as ECSSD, DUTS, and CUB, as well as challenging medical image segmentation datasets such as KVASIR, CVC-ClinicDB, ISIC-2018. The github repository of the code is available on \url{this https URL}.</li>
</ul>

<h3>Title: Uncertainty estimation via ensembles of deep learning models and dropout layers for seismic traces</h3>
<ul>
<li><strong>Authors: </strong>Giovanni Messuti, ortensia Amoroso, Ferdinando Napolitano, Mariarosaria Falanga, Paolo Capuano, Silvia Scarpetta</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06120">https://arxiv.org/abs/2410.06120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06120">https://arxiv.org/pdf/2410.06120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06120]] Uncertainty estimation via ensembles of deep learning models and dropout layers for seismic traces(https://arxiv.org/abs/2410.06120)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning models have demonstrated remarkable success in various fields, including seismology. However, one major challenge in deep learning is the presence of mislabeled examples. Additionally, accurately estimating model uncertainty is another challenge in machine learning. In this study, we develop Convolutional Neural Networks (CNNs) to classify seismic waveforms based on first-motion polarity. We trained multiple CNN models with different settings. We also constructed ensembles of networks to estimate uncertainty. The results showed that each training setting achieved satisfactory performances, with the ensemble method outperforming individual networks in uncertainty estimation. We observe that the uncertainty estimation ability of the ensembles of networks can be enhanced using dropout layers. In addition, comparisons among different training settings revealed that the use of dropout improved the robustness of networks to mislabeled examples.</li>
</ul>

<h3>Title: Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA</h3>
<ul>
<li><strong>Authors: </strong>Wenyu Huang, Guancheng Zhou, Hongru Wang, Pavlos Vougiouklis, Mirella Lapata, Jeff Z. Pan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06121">https://arxiv.org/abs/2410.06121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06121">https://arxiv.org/pdf/2410.06121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06121]] Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA(https://arxiv.org/abs/2410.06121)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) is widely used to inject external non-parametric knowledge into large language models (LLMs). Recent works suggest that Knowledge Graphs (KGs) contain valuable external knowledge for LLMs. Retrieving information from KGs differs from extracting it from document sets. Most existing approaches seek to directly retrieve relevant subgraphs, thereby eliminating the need for extensive SPARQL annotations, traditionally required by semantic parsing methods. In this paper, we model the subgraph retrieval task as a conditional generation task handled by small language models. Specifically, we define a subgraph identifier as a sequence of relations, each represented as a special token stored in the language models. Our base generative subgraph retrieval model, consisting of only 220M parameters, achieves competitive retrieval performance compared to state-of-the-art models relying on 7B parameters, demonstrating that small language models are capable of performing the subgraph retrieval task. Furthermore, our largest 3B model, when plugged with an LLM reader, sets new SOTA end-to-end performance on both the WebQSP and CWQ benchmarks. Our model and data will be made available online: this https URL.</li>
</ul>

<h3>Title: $\textit{X}^2$-DFD: A framework for e${X}$plainable and e${X}$tendable Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Yize Chen, Zhiyuan Yan, Siwei Lyu, Baoyuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06126">https://arxiv.org/abs/2410.06126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06126">https://arxiv.org/pdf/2410.06126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06126]] $\textit{X}^2$-DFD: A framework for e${X}$plainable and e${X}$tendable Deepfake Detection(https://arxiv.org/abs/2410.06126)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Detecting deepfakes has become an important task. Most existing detection methods provide only real/fake predictions without offering human-comprehensible explanations. Recent studies leveraging MLLMs for deepfake detection have shown improvements in explainability. However, the performance of pre-trained MLLMs (e.g., LLaVA) remains limited due to a lack of understanding of their capabilities for this task and strategies to enhance them. In this work, we empirically assess the strengths and weaknesses of MLLMs specifically in deepfake detection via forgery features analysis. Building on these assessments, we propose a novel framework called ${X}^2$-DFD, consisting of three core modules. The first module, Model Feature Assessment (MFA), measures the detection capabilities of forgery features intrinsic to MLLMs, and gives a descending ranking of these features. The second module, Strong Feature Strengthening (SFS), enhances the detection and explanation capabilities by fine-tuning the MLLM on a dataset constructed based on the top-ranked features. The third module, Weak Feature Supplementing (WFS), improves the fine-tuned MLLM's capabilities on lower-ranked features by integrating external dedicated deepfake detectors. To verify the effectiveness of this framework, we further present a practical implementation, where an automated forgery features generation, evaluation, and ranking procedure is designed for MFA module; an automated generation procedure of the fine-tuning dataset containing real and fake images with explanations based on top-ranked features is developed for SFS model; an external conventional deepfake detector focusing on blending artifact, which corresponds to a low detection capability in the pre-trained MLLM, is integrated for WFS module. Experiments show that our approach enhances both detection and explanation performance.</li>
</ul>

<h3>Title: De-VertiFL: A Solution for Decentralized Vertical Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Alberto Huertas Celdrán, Chao Feng, Sabyasachi Banik, Gerome Bovet, Gregorio Martinez Perez, Burkhard Stiller</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06127">https://arxiv.org/abs/2410.06127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06127">https://arxiv.org/pdf/2410.06127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06127]] De-VertiFL: A Solution for Decentralized Vertical Federated Learning(https://arxiv.org/abs/2410.06127)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL), introduced in 2016, was designed to enhance data privacy in collaborative model training environments. Among the FL paradigm, horizontal FL, where clients share the same set of features but different data samples, has been extensively studied in both centralized and decentralized settings. In contrast, Vertical Federated Learning (VFL), which is crucial in real-world decentralized scenarios where clients possess different, yet sensitive, data about the same entity, remains underexplored. Thus, this work introduces De-VertiFL, a novel solution for training models in a decentralized VFL setting. De-VertiFL contributes by introducing a new network architecture distribution, an innovative knowledge exchange scheme, and a distributed federated training process. Specifically, De-VertiFL enables the sharing of hidden layer outputs among federation clients, allowing participants to benefit from intermediate computations, thereby improving learning efficiency. De-VertiFL has been evaluated using a variety of well-known datasets, including both image and tabular data, across binary and multiclass classification tasks. The results demonstrate that De-VertiFL generally surpasses state-of-the-art methods in F1-score performance, while maintaining a decentralized and privacy-preserving framework.</li>
</ul>

<h3>Title: Zero-Shot Learning of Causal Models</h3>
<ul>
<li><strong>Authors: </strong>Divyat Mahajan, Jannes Gladrow, Agrin Hilmkil, Cheng Zhang, Meyer Scetbon</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06128">https://arxiv.org/abs/2410.06128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06128">https://arxiv.org/pdf/2410.06128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06128]] Zero-Shot Learning of Causal Models(https://arxiv.org/abs/2410.06128)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the increasing acquisition of datasets over time, we now have access to precise and varied descriptions of the world, capturing all sorts of phenomena. These datasets can be seen as empirical observations of unknown causal generative processes, which can commonly be described by Structural Causal Models (SCMs). Recovering these causal generative processes from observations poses formidable challenges, and often require to learn a specific generative model for each dataset. In this work, we propose to learn a \emph{single} model capable of inferring in a zero-shot manner the causal generative processes of datasets. Rather than learning a specific SCM for each dataset, we enable the Fixed-Point Approach (FiP) proposed in~\cite{scetbon2024fip}, to infer the generative SCMs conditionally on their empirical representations. More specifically, we propose to amortize the learning of a conditional version of FiP to infer generative SCMs from observations and causal structures on synthetically generated datasets. We show that our model is capable of predicting in zero-shot the true generative SCMs, and as a by-product, of (i) generating new dataset samples, and (ii) inferring intervened ones. Our experiments demonstrate that our amortized procedure achieves performances on par with SoTA methods trained specifically for each dataset on both in and out-of-distribution problems. To the best of our knowledge, this is the first time that SCMs are inferred in a zero-shot manner from observations, paving the way for a paradigmatic shift towards the assimilation of causal knowledge across datasets.</li>
</ul>

<h3>Title: Towards Unsupervised Eye-Region Segmentation for Eye Tracking</h3>
<ul>
<li><strong>Authors: </strong>Jiangfan Deng, Zhuang Jia, Zhaoxue Wang, Xiang Long, Daniel K. Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06131">https://arxiv.org/abs/2410.06131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06131">https://arxiv.org/pdf/2410.06131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06131]] Towards Unsupervised Eye-Region Segmentation for Eye Tracking(https://arxiv.org/abs/2410.06131)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Finding the eye and parsing out the parts (e.g. pupil and iris) is a key prerequisite for image-based eye tracking, which has become an indispensable module in today's head-mounted VR/AR devices. However, a typical route for training a segmenter requires tedious handlabeling. In this work, we explore an unsupervised way. First, we utilize priors of human eye and extract signals from the image to establish rough clues indicating the eye-region structure. Upon these sparse and noisy clues, a segmentation network is trained to gradually identify the precise area for each part. To achieve accurate parsing of the eye-region, we first leverage the pretrained foundation model Segment Anything (SAM) in an automatic way to refine the eye indications. Then, the learning process is designed in an end-to-end manner following progressive and prior-aware principle. Experiments show that our unsupervised approach can easily achieve 90% (the pupil and iris) and 85% (the whole eye-region) of the performances under supervised learning.</li>
</ul>

<h3>Title: Estimating the Number of HTTP/3 Responses in QUIC Using Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Barak Gahtan, Robert J. Shahla, Reuven Cohen, Alex M. Bronstein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06140">https://arxiv.org/abs/2410.06140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06140">https://arxiv.org/pdf/2410.06140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06140]] Estimating the Number of HTTP/3 Responses in QUIC Using Deep Learning(https://arxiv.org/abs/2410.06140)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>QUIC, a new and increasingly used transport protocol, enhances TCP by providing better security, performance, and features like stream multiplexing. These features, however, also impose challenges for network middle-boxes that need to monitor and analyze web traffic. This paper proposes a novel solution for estimating the number of HTTP/3 responses in a given QUIC connection by an observer. This estimation reveals server behavior, client-server interactions, and data transmission efficiency, which is crucial for various applications such as designing a load balancing solution and detecting HTTP/3 flood attacks. The proposed scheme transforms QUIC connection traces into a sequence of images and trains machine learning (ML) models to predict the number of responses. Then, by aggregating images of a QUIC connection, an observer can estimate the total number of responses. As the problem is formulated as a discrete regression problem, we introduce a dedicated loss function. The proposed scheme is evaluated on a dataset of over seven million images, generated from $100,000$ traces collected from over $44,000$ websites over a four-month period, from various vantage points. The scheme achieves up to 97\% cumulative accuracy in both known and unknown web server settings and 92\% accuracy in estimating the total number of responses in unseen QUIC traces.</li>
</ul>

<h3>Title: blockLAW: Blockchain Technology for Legal Automation and Workflow -- Cyber Ethics and Cybersecurity Platforms</h3>
<ul>
<li><strong>Authors: </strong>Bishwo Prakash Pokharel, Naresh Kshetri</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06143">https://arxiv.org/abs/2410.06143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06143">https://arxiv.org/pdf/2410.06143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06143]] blockLAW: Blockchain Technology for Legal Automation and Workflow -- Cyber Ethics and Cybersecurity Platforms(https://arxiv.org/abs/2410.06143)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, fair</a></li>
<li><strong>Abstract: </strong>In the current legal environment, it is essential to prioritize the protection and reliability of data to promote trust and effectiveness. This study examines how blockchain technology in the form of blockLAW can be applicable to investigate its effects on legal automation, cybersecurity, and ethical concerns. The decentralized ledger and unchangeable characteristics of Blockchain provide opportunities to simplify legal procedures, automate contract execution with smart contracts, and improve transparency in legal transactions. Blockchain is seen as a crucial instrument for updating legal processes while maintaining ethical standards, tackling issues like scalability, regulatory adherence, and ethical dilemmas such as privacy and fairness. The study examines recent developments and evaluates blockchain impact on legal structures, offering perspectives on its potential to enhance legal procedures and guarantee transparency in legal systems. It further emphasizes blockchain ability to redefine how legal professionals handle and protect sensitive information, leading to stronger, more effective, and reliable legal procedures. We have also discussed the technological considerations when it comes to blockchain integration into legal systems like integration planning, implementation strategies, innovations, advancements, trends with Blockchain Integration Framework for legal systems.</li>
</ul>

<h3>Title: Toward Scalable Image Feature Compression: A Content-Adaptive and Diffusion-Based Approach</h3>
<ul>
<li><strong>Authors: </strong>Sha Guo, Zhuo Chen, Yang Zhao, Ning Zhang, Xiaotong Li, Lingyu Duan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06149">https://arxiv.org/abs/2410.06149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06149">https://arxiv.org/pdf/2410.06149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06149]] Toward Scalable Image Feature Compression: A Content-Adaptive and Diffusion-Based Approach(https://arxiv.org/abs/2410.06149)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Traditional image codecs emphasize signal fidelity and human perception, often at the expense of machine vision tasks. Deep learning methods have demonstrated promising coding performance by utilizing rich semantic embeddings optimized for both human and machine vision. However, these compact embeddings struggle to capture fine details such as contours and textures, resulting in imperfect reconstructions. Furthermore, existing learning-based codecs lack scalability. To address these limitations, this paper introduces a content-adaptive diffusion model for scalable image compression. The proposed method encodes fine textures through a diffusion process, enhancing perceptual quality while preserving essential features for machine vision tasks. The approach employs a Markov palette diffusion model combined with widely used feature extractors and image generators, enabling efficient data compression. By leveraging collaborative texture-semantic feature extraction and pseudo-label generation, the method accurately captures texture information. A content-adaptive Markov palette diffusion model is then applied to represent both low-level textures and high-level semantic content in a scalable manner. This framework offers flexible control over compression ratios by selecting intermediate diffusion states, eliminating the need for retraining deep learning models at different operating points. Extensive experiments demonstrate the effectiveness of the proposed framework in both image reconstruction and downstream machine vision tasks such as object detection, segmentation, and facial landmark detection, achieving superior perceptual quality compared to state-of-the-art methods.</li>
</ul>

<h3>Title: AgentSquare: Automatic LLM Agent Search in Modular Design Space</h3>
<ul>
<li><strong>Authors: </strong>Yu Shang, Yu Li, Keyu Zhao, Likai Ma, Jiahe Liu, Fengli Xu, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06153">https://arxiv.org/abs/2410.06153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06153">https://arxiv.org/pdf/2410.06153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06153]] AgentSquare: Automatic LLM Agent Search in Modular Design Space(https://arxiv.org/abs/2410.06153)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have led to a rapid growth of agentic systems capable of handling a wide range of complex tasks. However, current research largely relies on manual, task-specific design, limiting their adaptability to novel tasks. In this paper, we introduce a new research problem: Modularized LLM Agent Search (MoLAS). We propose a modular design space that abstracts existing LLM agent designs into four fundamental modules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory. Building on this design space, we present a novel LLM agent search framework called AgentSquare, which introduces two core mechanisms, i.e., module evolution and recombination, to efficiently search for optimized LLM agents. To further accelerate the process, we design a performance predictor that uses in-context surrogate models to skip unpromising agent designs. Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Moreover, AgentSquare can generate interpretable design insights, enabling a deeper understanding of agentic architecture and its impact on task performance. We believe that the modular design space and AgentSquare search framework offer a platform for fully exploiting the potential of prior successful designs and consolidating the collective efforts of research community. Code repo is available at this https URL.</li>
</ul>

<h3>Title: GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>M. Jehanzeb Mirza, Mengjie Zhao, Zhuoyuan Mao, Sivan Doveh, Wei Lin, Paul Gavrikov, Michael Dorkenwald, Shiqi Yang, Saurav Jha, Hiromi Wakaki, Yuki Mitsufuji, Horst Possegger, Rogerio Feris, Leonid Karlinsky, James Glass</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06154">https://arxiv.org/abs/2410.06154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06154">https://arxiv.org/pdf/2410.06154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06154]] GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models(https://arxiv.org/abs/2410.06154)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the downstream task description, querying it for suitable VLM prompts (e.g., for zero-shot classification with CLIP). These prompts are ranked according to a purity measure obtained through a fitness function. In each respective optimization step, the ranked prompts are fed as in-context examples (with their accuracies) to equip the LLM with the knowledge of the type of text prompts preferred by the downstream VLM. Furthermore, we also explicitly steer the LLM generation process in each optimization step by specifically adding an offset difference vector of the embeddings from the positive and negative solutions found by the LLM, in previous optimization steps, to the intermediate layer of the network for the next generation step. This offset vector steers the LLM generation toward the type of language preferred by the downstream VLM, resulting in enhanced performance on the downstream vision tasks. We comprehensively evaluate our GLOV on 16 diverse datasets using two families of VLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models -- showing that the discovered solutions can enhance the recognition performance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these models.</li>
</ul>

<h3>Title: Detecting Android Malware by Visualizing App Behaviors from Multiple Complementary Views</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyi Meng, Jiale Zhang, Jiaqi Guo, Wansen Wang, Wenchao Huang, Jie Cui, Hong Zhong, Yan Xiong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06157">https://arxiv.org/abs/2410.06157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06157">https://arxiv.org/pdf/2410.06157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06157]] Detecting Android Malware by Visualizing App Behaviors from Multiple Complementary Views(https://arxiv.org/abs/2410.06157)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Deep learning has emerged as a promising technology for achieving Android malware detection. To further unleash its detection potentials, software visualization can be integrated for analyzing the details of app behaviors clearly. However, facing increasingly sophisticated malware, existing visualization-based methods, analyzing from one or randomly-selected few views, can only detect limited attack types. We propose and implement LensDroid, a novel technique that detects Android malware by visualizing app behaviors from multiple complementary views. Our goal is to harness the power of combining deep learning and software visualization to automatically capture and aggregate high-level features that are not inherently linked, thereby revealing hidden maliciousness of Android app behaviors. To thoroughly comprehend the details of apps, we visualize app behaviors from three related but distinct views of behavioral sensitivities, operational contexts and supported environments. We then extract high-order semantics based on the views accordingly. To exploit semantic complementarity of the views, we design a deep neural network based model for fusing the visualized features from local to global based on their contributions to downstream tasks. A comprehensive comparison with five baseline techniques is performed on datasets of more than 51K apps in three real-world typical scenarios, including overall threats, app evolution and zero-day malware. The experimental results show that the overall performance of LensDroid is better than the baseline techniques. We also validate the complementarity of the views and demonstrate that the multi-view fusion in LensDroid enhances Android malware detection.</li>
</ul>

<h3>Title: Temporal Reasoning Transfer from Text to Video</h3>
<ul>
<li><strong>Authors: </strong>Lei Li, Yuanxin Liu, Linli Yao, Peiyuan Zhang, Chenxin An, Lean Wang, Xu Sun, Lingpeng Kong, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06166">https://arxiv.org/abs/2410.06166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06166">https://arxiv.org/pdf/2410.06166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06166]] Temporal Reasoning Transfer from Text to Video(https://arxiv.org/abs/2410.06166)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video Large Language Models (Video LLMs) have shown promising capabilities in video comprehension, yet they struggle with tracking temporal changes and reasoning about temporal relationships. While previous research attributed this limitation to the ineffective temporal encoding of visual inputs, our diagnostic study reveals that video representations contain sufficient information for even small probing classifiers to achieve perfect accuracy. Surprisingly, we find that the key bottleneck in Video LLMs' temporal reasoning capability stems from the underlying LLM's inherent difficulty with temporal concepts, as evidenced by poor performance on textual temporal question-answering tasks. Building on this discovery, we introduce the Textual Temporal reasoning Transfer (T3). T3 synthesizes diverse temporal reasoning tasks in pure text format from existing image-text datasets, addressing the scarcity of video samples with complex temporal scenarios. Remarkably, without using any video data, T3 enhances LongVA-7B's temporal understanding, yielding a 5.3 absolute accuracy improvement on the challenging TempCompass benchmark, which enables our model to outperform ShareGPT4Video-8B trained on 28,000 video samples. Additionally, the enhanced LongVA-7B model achieves competitive performance on comprehensive video benchmarks. For example, it achieves a 49.7 accuracy on the Temporal Reasoning task of Video-MME, surpassing powerful large-scale models such as InternVL-Chat-V1.5-20B and VILA1.5-40B. Further analysis reveals a strong correlation between textual and video temporal task performance, validating the efficacy of transferring temporal reasoning abilities from text to video domains.</li>
</ul>

<h3>Title: Quadratic Is Not What You Need For Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Phu Pham, Wentian Zhao, Kun Wan, Yu-Jhe Li, Zeliang Zhang, Daniel Miranda, Ajinkya Kale, Chenliang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06169">https://arxiv.org/abs/2410.06169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06169">https://arxiv.org/pdf/2410.06169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06169]] Quadratic Is Not What You Need For Multimodal Large Language Models(https://arxiv.org/abs/2410.06169)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the past year, the capabilities of Multimodal Large Language Models (MLLMs) have significantly improved across various aspects. However, constrained by the quadratic growth of computation in LLMs as the number of tokens increases, efficiency has become a bottleneck for further scaling MLLMs. Although recent efforts have been made to prune visual tokens or use more lightweight LLMs to reduce computation, the problem of quadratic growth in computation with the increase of visual tokens still persists. To address this, we propose a novel approach: instead of reducing the input visual tokens for LLMs, we focus on pruning vision-related computations within the LLMs. After pruning, the computation growth in the LLM is no longer quadratic with the increase of visual tokens, but linear. Surprisingly, we found that after applying such extensive pruning, the capabilities of MLLMs are comparable with the original one and even superior on some benchmarks with only 25% of the computation. This finding opens up the possibility for MLLMs to incorporate much denser visual tokens. Additionally, based on this finding, we further analyzed some architectural design deficiencies in existing MLLMs and proposed promising improvements. To the best of our knowledge, this is the first study to investigate the computational redundancy in the LLM's vision component of MLLMs. Code and checkpoints will be released soon.</li>
</ul>

<h3>Title: The Last Iterate Advantage: Empirical Auditing and Principled Heuristic Analysis of Differentially Private SGD</h3>
<ul>
<li><strong>Authors: </strong>Thomas Steinke, Milad Nasr, Arun Ganesh, Borja Balle, Christopher A. Choquette-Choo, Matthew Jagielski, Jamie Hayes, Abhradeep Guha Thakurta, Adam Smith, Andreas Terzis</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06186">https://arxiv.org/abs/2410.06186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06186">https://arxiv.org/pdf/2410.06186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06186]] The Last Iterate Advantage: Empirical Auditing and Principled Heuristic Analysis of Differentially Private SGD(https://arxiv.org/abs/2410.06186)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack</a></li>
<li><strong>Abstract: </strong>We propose a simple heuristic privacy analysis of noisy clipped stochastic gradient descent (DP-SGD) in the setting where only the last iterate is released and the intermediate iterates remain hidden. Namely, our heuristic assumes a linear structure for the model. We show experimentally that our heuristic is predictive of the outcome of privacy auditing applied to various training procedures. Thus it can be used prior to training as a rough estimate of the final privacy leakage. We also probe the limitations of our heuristic by providing some artificial counterexamples where it underestimates the privacy leakage. The standard composition-based privacy analysis of DP-SGD effectively assumes that the adversary has access to all intermediate iterates, which is often unrealistic. However, this analysis remains the state of the art in practice. While our heuristic does not replace a rigorous privacy analysis, it illustrates the large gap between the best theoretical upper bounds and the privacy auditing lower bounds and sets a target for further work to improve the theoretical privacy analyses. We also empirically support our heuristic and show existing privacy auditing attacks are bounded by our heuristic analysis in both vision and language tasks.</li>
</ul>

<h3>Title: Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images</h3>
<ul>
<li><strong>Authors: </strong>Shiyu Miao, Delong Chen, Fan Liu, Chuanyi Zhang, Yanhui Gu, Shengjie Guo, Jun Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06194">https://arxiv.org/abs/2410.06194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06194">https://arxiv.org/pdf/2410.06194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06194]] Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images(https://arxiv.org/abs/2410.06194)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>The Direct Segment Anything Model (DirectSAM) excels in class-agnostic contour extraction. In this paper, we explore its use by applying it to optical remote sensing imagery, where semantic contour extraction-such as identifying buildings, road networks, and coastlines-holds significant practical value. Those applications are currently handled via training specialized small models separately on small datasets in each domain. We introduce a foundation model derived from DirectSAM, termed DirectSAM-RS, which not only inherits the strong segmentation capability acquired from natural images, but also benefits from a large-scale dataset we created for remote sensing semantic contour extraction. This dataset comprises over 34k image-text-contour triplets, making it at least 30 times larger than individual dataset. DirectSAM-RS integrates a prompter module: a text encoder and cross-attention layers attached to the DirectSAM architecture, which allows flexible conditioning on target class labels or referring expressions. We evaluate the DirectSAM-RS in both zero-shot and fine-tuning setting, and demonstrate that it achieves state-of-the-art performance across several downstream benchmarks.</li>
</ul>

<h3>Title: Entering Real Social World! Benchmarking the Theory of Mind and Socialization Capabilities of LLMs from a First-person Perspective</h3>
<ul>
<li><strong>Authors: </strong>Guiyang Hou, Wenqi Zhang, Yongliang Shen, Zeqi Tan, Sihao Shen, Weiming Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06195">https://arxiv.org/abs/2410.06195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06195">https://arxiv.org/pdf/2410.06195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06195]] Entering Real Social World! Benchmarking the Theory of Mind and Socialization Capabilities of LLMs from a First-person Perspective(https://arxiv.org/abs/2410.06195)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the social world, humans possess the capability to infer and reason about others mental states (such as emotions, beliefs, and intentions), known as the Theory of Mind (ToM). Simultaneously, humans own mental states evolve in response to social situations, a capability we refer to as socialization. Together, these capabilities form the foundation of human social interaction. In the era of artificial intelligence (AI), especially with the development of large language models (LLMs), we raise an intriguing question: How do LLMs perform in terms of ToM and socialization capabilities? And more broadly, can these AI models truly enter and navigate the real social world? Existing research evaluating LLMs ToM and socialization capabilities by positioning LLMs as passive observers from a third person perspective, rather than as active participants. However, compared to the third-person perspective, observing and understanding the world from an egocentric first person perspective is a natural approach for both humans and AI agents. The ToM and socialization capabilities of LLMs from a first person perspective, a crucial attribute for advancing embodied AI agents, remain unexplored. To answer the aforementioned questions and bridge the research gap, we introduce EgoSocialArena, a novel framework designed to evaluate and investigate the ToM and socialization capabilities of LLMs from a first person perspective. It encompasses two evaluation environments: static environment and interactive environment, with seven scenarios: Daily Life, Counterfactual, New World, Blackjack, Number Guessing, and Limit Texas Hold em, totaling 2,195 data entries. With EgoSocialArena, we have conducted a comprehensive evaluation of nine advanced LLMs and observed some key insights regarding the future development of LLMs as well as the capabilities levels of the most advanced LLMs currently available.</li>
</ul>

<h3>Title: Integrating Planning into Single-Turn Long-Form Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Yi Liang, You Wu, Honglei Zhuang, Li Chen, Jiaming Shen, Yiling Jia, Zhen Qin, Sumit Sanghai, Xuanhui Wang, Carl Yang, Michael Bendersky</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06203">https://arxiv.org/abs/2410.06203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06203">https://arxiv.org/pdf/2410.06203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06203]] Integrating Planning into Single-Turn Long-Form Text Generation(https://arxiv.org/abs/2410.06203)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Generating high-quality, in-depth textual documents, such as academic papers, news articles, Wikipedia entries, and books, remains a significant challenge for Large Language Models (LLMs). In this paper, we propose to use planning to generate long form content. To achieve our goal, we generate intermediate steps via an auxiliary task that teaches the LLM to plan, reason and structure before generating the final text. Our main novelty lies in a single auxiliary task that does not require multiple rounds of prompting or planning. To overcome the scarcity of training data for these intermediate steps, we leverage LLMs to generate synthetic intermediate writing data such as outlines, key information and summaries from existing full articles. Our experiments demonstrate on two datasets from different domains, namely the scientific news dataset SciNews and Wikipedia datasets in KILT-Wiki and FreshWiki, that LLMs fine-tuned with the auxiliary task generate higher quality documents. We observed +2.5% improvement in ROUGE-Lsum, and a strong 3.60 overall win/loss ratio via human SxS evaluation, with clear wins in organization, relevance, and verifiability.</li>
</ul>

<h3>Title: Round and Round We Go! What makes Rotary Positional Encodings useful?</h3>
<ul>
<li><strong>Authors: </strong>Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, Petar Veličković</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06205">https://arxiv.org/abs/2410.06205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06205">https://arxiv.org/pdf/2410.06205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06205]] Round and Round We Go! What makes Rotary Positional Encodings useful?(https://arxiv.org/abs/2410.06205)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Positional Encodings (PEs) are a critical component of Transformer-based Large Language Models (LLMs), providing the attention mechanism with important sequence-position information. One of the most popular types of encoding used today in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries and keys based on their relative distance. A common belief is that RoPE is useful because it helps to decay token dependency as relative distance increases. In this work, we argue that this is unlikely to be the core reason. We study the internals of a trained Gemma 7B model to understand how RoPE is being used at a mechanical level. We find that Gemma learns to use RoPE to construct robust "positional" attention patterns by exploiting the highest frequencies. We also find that, in general, Gemma greatly prefers to use the lowest frequencies of RoPE, which we suspect are used to carry semantic information. We mathematically prove interesting behaviours of RoPE and conduct experiments to verify our findings, proposing a modification of RoPE that fixes some highlighted issues and improves performance. We believe that this work represents an interesting step in better understanding PEs in LLMs, which we believe holds crucial value for scaling LLMs to large sizes and context lengths.</li>
</ul>

<h3>Title: LeanAgent: Lifelong Learning for Formal Theorem Proving</h3>
<ul>
<li><strong>Authors: </strong>Adarsh Kumarappan, Mo Tiwari, Peiyang Song, Robert Joseph George, Chaowei Xiao, Anima Anandkumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06209">https://arxiv.org/abs/2410.06209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06209">https://arxiv.org/pdf/2410.06209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06209]] LeanAgent: Lifelong Learning for Formal Theorem Proving(https://arxiv.org/abs/2410.06209)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been successful in mathematical reasoning tasks such as formal theorem proving when integrated with interactive proof assistants like Lean. Existing approaches involve training or fine-tuning an LLM on a specific dataset to perform well on particular domains, such as undergraduate-level mathematics. These methods struggle with generalizability to advanced mathematics. A fundamental limitation is that these approaches operate on static domains, failing to capture how mathematicians often work across multiple domains and projects simultaneously or cyclically. We present LeanAgent, a novel lifelong learning framework for theorem proving that continuously generalizes to and improves on ever-expanding mathematical knowledge without forgetting previously learned knowledge. LeanAgent introduces several key innovations, including a curriculum learning strategy that optimizes the learning trajectory in terms of mathematical difficulty, a dynamic database for efficient management of evolving mathematical knowledge, and progressive training to balance stability and plasticity. LeanAgent successfully proves 162 theorems previously unproved by humans across 23 diverse Lean repositories, many from advanced mathematics. It performs up to 11$\times$ better than the static LLM baseline, proving challenging theorems in domains like abstract algebra and algebraic topology while showcasing a clear progression of learning from basic concepts to advanced topics. In addition, we analyze LeanAgent's superior performance on key lifelong learning metrics. LeanAgent achieves exceptional scores in stability and backward transfer, where learning new tasks improves performance on previously learned tasks. This emphasizes LeanAgent's continuous generalizability and improvement, explaining its superior theorem proving performance.</li>
</ul>

<h3>Title: Solving robust MDPs as a sequence of static RL problems</h3>
<ul>
<li><strong>Authors: </strong>Adil Zouitine, Matthieu Geist, Emmanuel Rachelson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06212">https://arxiv.org/abs/2410.06212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06212">https://arxiv.org/pdf/2410.06212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06212]] Solving robust MDPs as a sequence of static RL problems(https://arxiv.org/abs/2410.06212)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Designing control policies whose performance level is guaranteed to remain above a given threshold in a span of environments is a critical feature for the adoption of reinforcement learning (RL) in real-world applications. The search for such robust policies is a notoriously difficult problem, related to the so-called dynamic model of transition function uncertainty, where the environment dynamics are allowed to change at each time step. But in practical cases, one is rather interested in robustness to a span of static transition models throughout interaction episodes. The static model is known to be harder to solve than the dynamic one, and seminal algorithms, such as robust value iteration, as well as most recent works on deep robust RL, build upon the dynamic model. In this work, we propose to revisit the static model. We suggest an analysis of why solving the static model under some mild hypotheses is a reasonable endeavor, based on an equivalence with the dynamic model, and formalize the general intuition that robust MDPs can be solved by tackling a series of static problems. We introduce a generic meta-algorithm called IWOCS, which incrementally identifies worst-case transition models so as to guide the search for a robust policy. Discussion on IWOCS sheds light on new ways to decouple policy optimization and adversarial transition functions and opens new perspectives for analysis. We derive a deep RL version of IWOCS and demonstrate it is competitive with state-of-the-art algorithms on classical benchmarks.</li>
</ul>

<h3>Title: Fair-OBNC: Correcting Label Noise for Fairer Datasets</h3>
<ul>
<li><strong>Authors: </strong>Inês Oliveira e Silva, Sérgio Jesus, Hugo Ferreira, Pedro Saleiro, Inês Sousa, Pedro Bizarro, Carlos Soares</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06214">https://arxiv.org/abs/2410.06214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06214">https://arxiv.org/pdf/2410.06214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06214]] Fair-OBNC: Correcting Label Noise for Fairer Datasets(https://arxiv.org/abs/2410.06214)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Data used by automated decision-making systems, such as Machine Learning models, often reflects discriminatory behavior that occurred in the past. These biases in the training data are sometimes related to label noise, such as in COMPAS, where more African-American offenders are wrongly labeled as having a higher risk of recidivism when compared to their White counterparts. Models trained on such biased data may perpetuate or even aggravate the biases with respect to sensitive information, such as gender, race, or age. However, while multiple label noise correction approaches are available in the literature, these focus on model performance exclusively. In this work, we propose Fair-OBNC, a label noise correction method with fairness considerations, to produce training datasets with measurable demographic parity. The presented method adapts Ordering-Based Noise Correction, with an adjusted criterion of ordering, based both on the margin of error of an ensemble, and the potential increase in the observed demographic parity of the dataset. We evaluate Fair-OBNC against other different pre-processing techniques, under different scenarios of controlled label noise. Our results show that the proposed method is the overall better alternative within the pool of label correction methods, being capable of attaining better reconstructions of the original labels. Models trained in the corrected data have an increase, on average, of 150% in demographic parity, when compared to models trained in data with noisy labels, across the considered levels of label noise.</li>
</ul>

<h3>Title: DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback</h3>
<ul>
<li><strong>Authors: </strong>Zaid Khan, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06215">https://arxiv.org/abs/2410.06215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06215">https://arxiv.org/pdf/2410.06215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06215]] DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback(https://arxiv.org/abs/2410.06215)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Recent approaches using LLMs as annotators reduce human effort, but still require humans to interpret feedback from evaluations and control the LLM to produce data the student needs. Automating this labor-intensive process by creating autonomous data generation agents - or teachers - is desirable, but requires environments that can simulate the feedback-driven, iterative, closed loop of data creation. To enable rapid and scalable testing for such agents and their modules, we introduce DataEnvGym, a testbed of teacher environments for data generation agents. DataEnvGym frames data generation as a sequential decision-making task, involving an agent consisting of a data generation policy (which generates a plan for creating training data) and a data generation engine (which transforms the plan into data), inside an environment that provides student feedback. The agent's goal is to improve student performance. Students are iteratively trained and evaluated on generated data, with their feedback (in the form of errors or weak skills) being reported to the agent after each iteration. DataEnvGym includes multiple teacher environment instantiations across 3 levels of structure in the state representation and action space. More structured environments are based on inferred skills and offer more interpretability and curriculum control. We support 3 diverse tasks (math, code, and VQA) and test multiple students and teachers. Example agents in our teaching environments can iteratively improve students across tasks and settings. Moreover, we show that environments teach different skill levels and test variants of key modules, pointing to future work in improving data generation agents, engines, and feedback mechanisms.</li>
</ul>

<h3>Title: A Timeline and Analysis for Representation Plasticity in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Akshat Kannan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06225">https://arxiv.org/abs/2410.06225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06225">https://arxiv.org/pdf/2410.06225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06225]] A Timeline and Analysis for Representation Plasticity in Large Language Models(https://arxiv.org/abs/2410.06225)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The ability to steer AI behavior is crucial to preventing its long term dangerous and catastrophic potential. Representation Engineering (RepE) has emerged as a novel, powerful method to steer internal model behaviors, such as "honesty", at a top-down level. Understanding the steering of representations should thus be placed at the forefront of alignment initiatives. Unfortunately, current efforts to understand plasticity at this level are highly neglected. This paper aims to bridge the knowledge gap and understand how LLM representation stability, specifically for the concept of "honesty", and model plasticity evolve by applying steering vectors extracted at different fine-tuning stages, revealing differing magnitudes of shifts in model behavior. The findings are pivotal, showing that while early steering exhibits high plasticity, later stages have a surprisingly responsive critical window. This pattern is observed across different model architectures, signaling that there is a general pattern of model plasticity that can be used for effective intervention. These insights greatly contribute to the field of AI transparency, addressing a pressing lack of efficiency limiting our ability to effectively steer model behavior.</li>
</ul>

<h3>Title: RelitLRM: Generative Relightable Radiance for Large Reconstruction Models</h3>
<ul>
<li><strong>Authors: </strong>Tianyuan Zhang, Zhengfei Kuang, Haian Jin, Zexiang Xu, Sai Bi, Hao Tan, He Zhang, Yiwei Hu, Milos Hasan, William T. Freeman, Kai Zhang, Fujun Luan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06231">https://arxiv.org/abs/2410.06231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06231">https://arxiv.org/pdf/2410.06231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06231]] RelitLRM: Generative Relightable Radiance for Large Reconstruction Models(https://arxiv.org/abs/2410.06231)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>We propose RelitLRM, a Large Reconstruction Model (LRM) for generating high-quality Gaussian splatting representations of 3D objects under novel illuminations from sparse (4-8) posed images captured under unknown static lighting. Unlike prior inverse rendering methods requiring dense captures and slow optimization, often causing artifacts like incorrect highlights or shadow baking, RelitLRM adopts a feed-forward transformer-based model with a novel combination of a geometry reconstructor and a relightable appearance generator based on diffusion. The model is trained end-to-end on synthetic multi-view renderings of objects under varying known illuminations. This architecture design enables to effectively decompose geometry and appearance, resolve the ambiguity between material and lighting, and capture the multi-modal distribution of shadows and specularity in the relit appearance. We show our sparse-view feed-forward RelitLRM offers competitive relighting results to state-of-the-art dense-view optimization-based baselines while being significantly faster. Our project page is available at: this https URL.</li>
</ul>

<h3>Title: Parameter Choice and Neuro-Symbolic Approaches for Deep Domain-Invariant Learning</h3>
<ul>
<li><strong>Authors: </strong>Marius-Constantin Dinu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06235">https://arxiv.org/abs/2410.06235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06235">https://arxiv.org/pdf/2410.06235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06235]] Parameter Choice and Neuro-Symbolic Approaches for Deep Domain-Invariant Learning(https://arxiv.org/abs/2410.06235)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As artificial intelligence (AI) systems advance, we move towards broad AI: systems capable of performing well on diverse tasks, understanding context, and adapting rapidly to new scenarios. A central challenge for broad AI systems is to generalize over tasks in related domains and being robust to distribution shifts. Neuro-symbolic (NeSy) AI bridges the gap between symbolic and sub-symbolic paradigms to address these challenges, enabling adaptable, generalizable, and more interpretable systems. The development of broad AI requires advancements in domain adaptation (DA), enabling models trained on source domains to effectively generalize to unseen target domains. Traditional approaches often rely on parameter optimization and fine-tuning, which can be impractical due to high costs and risks of catastrophic forgetting. NeSy AI systems use multiple models and methods to generalize to unseen domains and maintain performance across varying conditions. We analyze common DA and NeSy approaches with a focus on deep domain-invariant learning, extending to real-world challenges such as adapting to continuously changing domains and handling large domain gaps. We showcase state-of-the-art model-selection methods for scenarios with limited samples and introduce domain-specific adaptations without gradient-based updates for cases where model tuning is infeasible. This work establishes a framework for scalable and generalizable broad AI systems applicable across various problem settings, demonstrating how symbolic reasoning and large language models can build universal computational graphs that generalize across domains and problems, contributing to more adaptable AI approaches for real-world applications.</li>
</ul>

<h3>Title: EVOLvE: Evaluating and Optimizing LLMs For Exploration</h3>
<ul>
<li><strong>Authors: </strong>Allen Nie, Yi Su, Bo Chang, Jonathan N. Lee, Ed H. Chi, Quoc V. Le, Minmin Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06238">https://arxiv.org/abs/2410.06238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06238">https://arxiv.org/pdf/2410.06238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06238]] EVOLvE: Evaluating and Optimizing LLMs For Exploration(https://arxiv.org/abs/2410.06238)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite their success in many domains, large language models (LLMs) remain under-studied in scenarios requiring optimal decision-making under uncertainty. This is crucial as many real-world applications, ranging from personalized recommendations to healthcare interventions, demand that LLMs not only predict but also actively learn to make optimal decisions through exploration. In this work, we measure LLMs' (in)ability to make optimal decisions in bandits, a state-less reinforcement learning setting relevant to many applications. We develop a comprehensive suite of environments, including both context-free and contextual bandits with varying task difficulties, to benchmark LLMs' performance. Motivated by the existence of optimal exploration algorithms, we propose efficient ways to integrate this algorithmic knowledge into LLMs: by providing explicit algorithm-guided support during inference; and through algorithm distillation via in-context demonstrations and fine-tuning, using synthetic data generated from these algorithms. Impressively, these techniques allow us to achieve superior exploration performance with smaller models, surpassing larger models on various tasks. We conducted an extensive ablation study to shed light on various factors, such as task difficulty and data representation, that influence the efficiency of LLM exploration. Additionally, we conduct a rigorous analysis of the LLM's exploration efficiency using the concept of regret, linking its ability to explore to the model size and underlying algorithm.</li>
</ul>

<h3>Title: Unsupervised Model Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Yinong Oliver Wang, Eileen Li, Jinqi Luo, Zhaoning Wang, Fernando De la Torre</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06243">https://arxiv.org/abs/2410.06243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06243">https://arxiv.org/pdf/2410.06243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06243]] Unsupervised Model Diagnosis(https://arxiv.org/abs/2410.06243)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Ensuring model explainability and robustness is essential for reliable deployment of deep vision systems. Current methods for evaluating robustness rely on collecting and annotating extensive test sets. While this is common practice, the process is labor-intensive and expensive with no guarantee of sufficient coverage across attributes of interest. Recently, model diagnosis frameworks have emerged leveraging user inputs (e.g., text) to assess the vulnerability of the model. However, such dependence on human can introduce bias and limitation given the domain knowledge of particular users. This paper proposes Unsupervised Model Diagnosis (UMO), that leverages generative models to produce semantic counterfactual explanations without any user guidance. Given a differentiable computer vision model (i.e., the target model), UMO optimizes for the most counterfactual directions in a generative latent space. Our approach identifies and visualizes changes in semantics, and then matches these changes to attributes from wide-ranging text sources, such as dictionaries or language models. We validate the framework on multiple vision tasks (e.g., classification, segmentation, keypoint detection). Extensive experiments show that our unsupervised discovery of semantic directions can correctly highlight spurious correlations and visualize the failure mode of target models without any human intervention.</li>
</ul>

<h3>Title: Story-Adapter: A Training-free Iterative Framework for Long Story Visualization</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Mao, Xiaoke Huang, Yunfei Xie, Yuanqi Chang, Mude Hui, Bingjie Xu, Yuyin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06244">https://arxiv.org/abs/2410.06244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06244">https://arxiv.org/pdf/2410.06244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06244]] Story-Adapter: A Training-free Iterative Framework for Long Story Visualization(https://arxiv.org/abs/2410.06244)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Story visualization, the task of generating coherent images based on a narrative, has seen significant advancements with the emergence of text-to-image models, particularly diffusion models. However, maintaining semantic consistency, generating high-quality fine-grained interactions, and ensuring computational feasibility remain challenging, especially in long story visualization (i.e., up to 100 frames). In this work, we propose a training-free and computationally efficient framework, termed Story-Adapter, to enhance the generative capability of long stories. Specifically, we propose an iterative paradigm to refine each generated image, leveraging both the text prompt and all generated images from the previous iteration. Central to our framework is a training-free global reference cross-attention module, which aggregates all generated images from the previous iteration to preserve semantic consistency across the entire story, while minimizing computational costs with global embeddings. This iterative process progressively optimizes image generation by repeatedly incorporating text constraints, resulting in more precise and fine-grained interactions. Extensive experiments validate the superiority of Story-Adapter in improving both semantic consistency and generative capability for fine-grained interactions, particularly in long story scenarios. The project page and associated code can be accessed via this https URL .</li>
</ul>

<h3>Title: SymDiff: Equivariant Diffusion via Stochastic Symmetrisation</h3>
<ul>
<li><strong>Authors: </strong>Leo Zhang, Kianoosh Ashouritaklimi, Yee Whye Teh, Rob Cornish</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06262">https://arxiv.org/abs/2410.06262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06262">https://arxiv.org/pdf/2410.06262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06262]] SymDiff: Equivariant Diffusion via Stochastic Symmetrisation(https://arxiv.org/abs/2410.06262)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We propose SymDiff, a novel method for constructing equivariant diffusion models using the recently introduced framework of stochastic symmetrisation. SymDiff resembles a learned data augmentation that is deployed at sampling time, and is lightweight, computationally efficient, and easy to implement on top of arbitrary off-the-shelf models. Notably, in contrast to previous work, SymDiff typically does not require any neural network components that are intrinsically equivariant, avoiding the need for complex parameterizations and the use of higher-order geometric features. Instead, our method can leverage highly scalable modern architectures as drop-in replacements for these more constrained alternatives. We show that this additional flexibility yields significant empirical benefit on $\mathrm{E}(3)$-equivariant molecular generation. To the best of our knowledge, this is the first application of symmetrisation to generative modelling, suggesting its potential in this domain more generally.</li>
</ul>

<h3>Title: Think While You Generate: Discrete Diffusion with Planned Denoising</h3>
<ul>
<li><strong>Authors: </strong>Sulin Liu, Juno Nam, Andrew Campbell, Hannes Stärk, Yilun Xu, Tommi Jaakkola, Rafael Gómez-Bombarelli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06264">https://arxiv.org/abs/2410.06264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06264">https://arxiv.org/pdf/2410.06264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06264]] Think While You Generate: Discrete Diffusion with Planned Denoising(https://arxiv.org/abs/2410.06264)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Discrete diffusion has achieved state-of-the-art performance, outperforming or approaching autoregressive models on standard benchmarks. In this work, we introduce Discrete Diffusion with Planned Denoising (DDPD), a novel framework that separates the generation process into two models: a planner and a denoiser. At inference time, the planner selects which positions to denoise next by identifying the most corrupted positions in need of denoising, including both initially corrupted and those requiring additional refinement. This plan-and-denoise approach enables more efficient reconstruction during generation by iteratively identifying and denoising corruptions in the optimal order. DDPD outperforms traditional denoiser-only mask diffusion methods, achieving superior results on language modeling benchmarks such as text8, OpenWebText, and token-based generation on ImageNet $256 \times 256$. Notably, in language modeling, DDPD significantly reduces the performance gap between diffusion-based and autoregressive methods in terms of generative perplexity. Code is available at this https URL.</li>
</ul>

<h3>Title: Near Exact Privacy Amplification for Matrix Mechanisms</h3>
<ul>
<li><strong>Authors: </strong>Christopher A. Choquette-Choo, Arun Ganesh, Saminul Haque, Thomas Steinke, Abhradeep Thakurta</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06266">https://arxiv.org/abs/2410.06266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06266">https://arxiv.org/pdf/2410.06266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06266]] Near Exact Privacy Amplification for Matrix Mechanisms(https://arxiv.org/abs/2410.06266)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>We study the problem of computing the privacy parameters for DP machine learning when using privacy amplification via random batching and noise correlated across rounds via a correlation matrix $\textbf{C}$ (i.e., the matrix mechanism). Past work on this problem either only applied to banded $\textbf{C}$, or gave loose privacy parameters. In this work, we give a framework for computing near-exact privacy parameters for any lower-triangular, non-negative $\textbf{C}$. Our framework allows us to optimize the correlation matrix $\textbf{C}$ while accounting for amplification, whereas past work could not. Empirically, we show this lets us achieve smaller RMSE on prefix sums than the previous state-of-the-art (SOTA). We also show that we can improve on the SOTA performance on deep learning tasks. Our two main technical tools are (i) using Monte Carlo accounting to bypass composition, which was the main technical challenge for past work, and (ii) a "balls-in-bins" batching scheme that enables easy privacy analysis and is closer to practical random batching than Poisson sampling.</li>
</ul>

<h3>Title: MC-MoE: Mixture Compressor for Mixture-of-Experts LLMs Gains More</h3>
<ul>
<li><strong>Authors: </strong>Wei Huang, Yue Liao, Jianhui Liu, Ruifei He, Haoru Tan, Shiming Zhang, Hongsheng Li, Si Liu, Xiaojuan Qi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06270">https://arxiv.org/abs/2410.06270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06270">https://arxiv.org/pdf/2410.06270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06270]] MC-MoE: Mixture Compressor for Mixture-of-Experts LLMs Gains More(https://arxiv.org/abs/2410.06270)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts large language models (MoE-LLMs) marks a significant step forward of language models, however, they encounter two critical challenges in practice: 1) expert parameters lead to considerable memory consumption and loading latency; and 2) the current activated experts are redundant, as many tokens may only require a single expert. Motivated by these issues, we investigate the MoE-LLMs and make two key observations: a) different experts exhibit varying behaviors on activation reconstruction error, routing scores, and activated frequencies, highlighting their differing importance, and b) not all tokens are equally important -- only a small subset is critical. Building on these insights, we propose MC-MoE, a training-free Mixture-Compressor for MoE-LLMs, which leverages the significance of both experts and tokens to achieve an extreme compression. First, to mitigate storage and loading overheads, we introduce Pre-Loading Mixed-Precision Quantization, which formulates the adaptive bit-width allocation as a Linear Programming problem, where the objective function balances multi-factors reflecting the importance of each expert. Additionally, we develop Online Dynamic Pruning, which identifies important tokens to retain and dynamically select activated experts for other tokens during inference to optimize efficiency while maintaining performance. Our MC-MoE integrates static quantization and dynamic pruning to collaboratively achieve extreme compression for MoE-LLMs with less accuracy loss, ensuring an optimal trade-off between performance and efficiency. Extensive experiments confirm the effectiveness of our approach. For instance, at 2.54 bits, MC-MoE compresses 76.6% of the model, with only a 3.8% average accuracy loss. During dynamic inference, we further reduce activated parameters by 15%, with a performance drop of less than 0.6%.</li>
</ul>

<h3>Title: Probing the Robustness of Theory of Mind in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Christian Nickel, Laura Schrewe, Lucie Flek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06271">https://arxiv.org/abs/2410.06271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06271">https://arxiv.org/pdf/2410.06271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06271]] Probing the Robustness of Theory of Mind in Large Language Models(https://arxiv.org/abs/2410.06271)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>With the success of ChatGPT and other similarly sized SotA LLMs, claims of emergent human like social reasoning capabilities, especially Theory of Mind (ToM), in these models have appeared in the scientific literature. On the one hand those ToM-capabilities have been successfully tested using tasks styled similar to those used in psychology (Kosinski, 2023). On the other hand, follow up studies showed that those capabilities vanished when the tasks were slightly altered (Ullman, 2023). In this work we introduce a novel dataset of 68 tasks for probing ToM in LLMs, including potentially challenging variations which are assigned to 10 complexity classes. This way it is providing novel insights into the challenges LLMs face with those task variations. We evaluate the ToM performance of four SotA open source LLMs on our dataset and the dataset introduced by (Kosinski, 2023). The overall low goal accuracy across all evaluated models indicates only a limited degree of ToM capabilities. The LLMs' performance on simple complexity class tasks from both datasets are similar. Whereas we find a consistent tendency in all tested LLMs to perform poorly on tasks that require the realization that an agent has knowledge of automatic state changes in its environment, even when those are spelled out to the model. For task complications that change the relationship between objects by replacing prepositions, we notice a performance drop in all models, with the strongest impact on the mixture-of-experts model. With our dataset of tasks grouped by complexity we offer directions for further research on how to stabilize and advance ToM capabilities in LLM.</li>
</ul>

<h3>Title: The Mystery of Compositional Generalization in Graph-based Generative Commonsense Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xiyan Fu, Anette Frank</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06272">https://arxiv.org/abs/2410.06272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06272">https://arxiv.org/pdf/2410.06272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06272]] The Mystery of Compositional Generalization in Graph-based Generative Commonsense Reasoning(https://arxiv.org/abs/2410.06272)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While LLMs have emerged as performant architectures for reasoning tasks, their compositional generalization capabilities have been questioned. In this work, we introduce a Compositional Generalization Challenge for Graph-based Commonsense Reasoning (CGGC) that goes beyond previous evaluations that are based on sequences or tree structures - and instead involves a reasoning graph: It requires models to generate a natural sentence based on given concepts and a corresponding reasoning graph, where the presented graph involves a previously unseen combination of relation types. To master this challenge, models need to learn how to reason over relation tupels within the graph, and how to compose them when conceptualizing a verbalization. We evaluate seven well-known LLMs using in-context learning and find that performant LLMs still struggle in compositional generalization. We investigate potential causes of this gap by analyzing the structures of reasoning graphs, and find that different structures present varying levels of difficulty for compositional generalization. Arranging the order of demonstrations according to the structures' difficulty shows that organizing samples in an easy-to-hard schema enhances the compositional generalization ability of LLMs.</li>
</ul>

<h3>Title: Is Pontryagin's Maximum Principle all you need? Solving optimal control problems with PMP-inspired neural networks</h3>
<ul>
<li><strong>Authors: </strong>Kawisorn Kamtue, Jose M.F. Moura, Orathai Sangpetch</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06277">https://arxiv.org/abs/2410.06277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06277">https://arxiv.org/pdf/2410.06277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06277]] Is Pontryagin's Maximum Principle all you need? Solving optimal control problems with PMP-inspired neural networks(https://arxiv.org/abs/2410.06277)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Calculus of Variations is the mathematics of functional optimization, i.e., when the solutions are functions over a time interval. This is particularly important when the time interval is unknown like in minimum-time control problems, so that forward in time solutions are not possible. Calculus of Variations offers a robust framework for learning optimal control and inference. How can this framework be leveraged to design neural networks to solve challenges in control and inference? We propose the Pontryagin's Maximum Principle Neural Network (PMP-net) that is tailored to estimate control and inference solutions, in accordance with the necessary conditions outlined by Pontryagin's Maximum Principle. We assess PMP-net on two classic optimal control and inference problems: optimal linear filtering and minimum-time control. Our findings indicate that PMP-net can be effectively trained in an unsupervised manner to solve these problems without the need for ground-truth data, successfully deriving the classical "Kalman filter" and "bang-bang" control solution. This establishes a new approach for addressing general, possibly yet unsolved, optimal control problems.</li>
</ul>

<h3>Title: Accelerated Preference Optimization for Large Language Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Jiafan He, Huizhuo Yuan, Quanquan Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06293">https://arxiv.org/abs/2410.06293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06293">https://arxiv.org/pdf/2410.06293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06293]] Accelerated Preference Optimization for Large Language Model Alignment(https://arxiv.org/abs/2410.06293)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal tool for aligning large language models (LLMs) with human preferences. Direct Preference Optimization (DPO), one of the most popular approaches, formulates RLHF as a policy optimization problem without explicitly estimating the reward function. It overcomes the stability and efficiency issues of two-step approaches, which typically involve first estimating the reward function and then optimizing the policy via proximal policy optimization (PPO). Since RLHF is essentially an optimization problem, and it is well-known that momentum techniques can accelerate optimization both theoretically and empirically, a natural question arises: Can RLHF be accelerated by momentum? This paper answers this question in the affirmative. In detail, we first show that the iterative preference optimization method can be viewed as a proximal point method. Based on this observation, we propose a general Accelerated Preference Optimization (APO) framework, which unifies many existing preference optimization algorithms and employs Nesterov's momentum technique to speed up the alignment of LLMs. Theoretically, we demonstrate that APO can achieve a faster convergence rate than the standard iterative preference optimization methods, including DPO and Self-Play Preference Optimization (SPPO). Empirically, we show the superiority of APO over DPO, iterative DPO, and other strong baselines for RLHF on the AlpacaEval 2.0 benchmark.</li>
</ul>

<h3>Title: Compositional Risk Minimization</h3>
<ul>
<li><strong>Authors: </strong>Divyat Mahajan, Mohammad Pezeshki, Ioannis Mitliagkas, Kartik Ahuja, Pascal Vincent</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06303">https://arxiv.org/abs/2410.06303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06303">https://arxiv.org/pdf/2410.06303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06303]] Compositional Risk Minimization(https://arxiv.org/abs/2410.06303)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this work, we tackle a challenging and extreme form of subpopulation shift, which is termed compositional shift. Under compositional shifts, some combinations of attributes are totally absent from the training distribution but present in the test distribution. We model the data with flexible additive energy distributions, where each energy term represents an attribute, and derive a simple alternative to empirical risk minimization termed compositional risk minimization (CRM). We first train an additive energy classifier to predict the multiple attributes and then adjust this classifier to tackle compositional shifts. We provide an extensive theoretical analysis of CRM, where we show that our proposal extrapolates to special affine hulls of seen attribute combinations. Empirical evaluations on benchmark datasets confirms the improved robustness of CRM compared to other methods from the literature designed to tackle various forms of subpopulation shifts.</li>
</ul>

<h3>Title: Fine-grained Hallucination Detection and Mitigation in Language Model Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Ruosen Li, Ziming Luo, Xinya Du</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06304">https://arxiv.org/abs/2410.06304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06304">https://arxiv.org/pdf/2410.06304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06304]] Fine-grained Hallucination Detection and Mitigation in Language Model Mathematical Reasoning(https://arxiv.org/abs/2410.06304)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hallucinations in large language models (LLMs) pose significant challenges in tasks requiring complex multi-step reasoning, such as mathematical problem-solving. Existing approaches primarily detect the presence of hallucinations but lack a nuanced understanding of their types and manifestations. In this paper, we first introduce a comprehensive taxonomy that categorizes the common hallucinations in mathematical reasoning task into six types: fabrication, factual inconsistency, context inconsistency, instruction inconsistency, logical inconsistency, and logical error. We then propose FG-PRM (Fine-Grained Process Reward Model), an augmented model designed to detect and mitigate hallucinations in a fine-grained, step-level manner. To address the limitations of manually labeling training data, we propose an automated method for generating fine-grained hallucination data using LLMs. By injecting hallucinations into reasoning steps of correct solutions, we create a diverse and balanced synthetic dataset for training FG-PRM, which consists of six specialized Process Reward Models (PRMs), each tailored to detect a specific hallucination type. Our FG-PRM demonstrates superior performance across two key tasks: 1) Fine-grained hallucination detection: classifying hallucination types for each reasoning step; and 2) Verification: ranking multiple LLM-generated outputs to select the most accurate solution, mitigating reasoning hallucinations. Our experiments show that FG-PRM outperforms ChatGPT-3.5 and Claude-3 on fine-grained hallucination detection and substantially boosts the performance of LLMs on GSM8K and MATH benchmarks.</li>
</ul>

<h3>Title: Auto-Evolve: Enhancing Large Language Model's Performance via Self-Reasoning Framework</h3>
<ul>
<li><strong>Authors: </strong>Krishna Aswani, Huilin Lu, Pranav Patankar, Priya Dhalwani, Iris Tan, Jayant Ganeshmohan, Simon Lacasse</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06328">https://arxiv.org/abs/2410.06328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06328">https://arxiv.org/pdf/2410.06328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06328]] Auto-Evolve: Enhancing Large Language Model's Performance via Self-Reasoning Framework(https://arxiv.org/abs/2410.06328)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in prompt engineering strategies, such as Chain-of-Thought (CoT) and Self-Discover, have demonstrated significant potential in improving the reasoning abilities of Large Language Models (LLMs). However, these state-of-the-art (SOTA) prompting strategies rely on single or fixed set of static seed reasoning modules like \emph{"think step by step"} or \emph{"break down this problem"} intended to simulate human approach to problem-solving. This constraint limits the flexibility of models in tackling diverse problems effectively. In this paper, we introduce Auto-Evolve, a novel framework that enables LLMs to self-create dynamic reasoning modules and downstream action plan, resulting in significant improvements over current SOTA methods. We evaluate Auto-Evolve on the challenging BigBench-Hard (BBH) dataset with Claude 2.0, Claude 3 Sonnet, Mistral Large, and GPT 4, where it consistently outperforms the SOTA prompt strategies. Auto-Evolve outperforms CoT by up to 10.4\% and on an average by 7\% across these four models. Our framework introduces two innovations: a) Auto-Evolve dynamically generates reasoning modules for each task while aligning with human reasoning paradigm, thus eliminating the need for predefined templates. b) We introduce an iterative refinement component, that incrementally refines instruction guidance for LLMs and helps boost performance by average 2.8\% compared to doing it in a single step.</li>
</ul>

<h3>Title: Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Zhuoran Zhang, Yongxiang Li, Zijian Kan, Keyuan Cheng, Lijie Hu, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06331">https://arxiv.org/abs/2410.06331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06331">https://arxiv.org/pdf/2410.06331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06331]] Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing(https://arxiv.org/abs/2410.06331)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The locate-then-edit paradigm has shown significant promise for knowledge editing (KE) in Large Language Models (LLMs). While previous methods perform well on single-hop fact recall tasks, they consistently struggle with multi-hop factual recall tasks involving newly edited knowledge. In this paper, leveraging tools in mechanistic interpretability, we first identify that in multi-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper MLP layers, unlike single-hop tasks, which rely on earlier layers. This distinction explains the poor performance of current methods in multi-hop queries, as they primarily focus on editing shallow layers, leaving deeper layers unchanged. To address this, we propose IFMET, a novel locate-then-edit KE approach designed to edit both shallow and deep MLP layers. IFMET employs multi-hop editing prompts and supplementary sets to locate and modify knowledge across different reasoning stages. Experimental results demonstrate that IFMET significantly improves performance on multi-hop factual recall tasks, effectively overcoming the limitations of previous locate-then-edit methods.</li>
</ul>

<h3>Title: Are Large Language Models State-of-the-art Quality Estimators for Machine Translation of User-generated Content?</h3>
<ul>
<li><strong>Authors: </strong>Shenbin Qian, Constantin Orăsan, Diptesh Kanojia, Félix do Carmo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06338">https://arxiv.org/abs/2410.06338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06338">https://arxiv.org/pdf/2410.06338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06338]] Are Large Language Models State-of-the-art Quality Estimators for Machine Translation of User-generated Content?(https://arxiv.org/abs/2410.06338)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates whether large language models (LLMs) are state-of-the-art quality estimators for machine translation of user-generated content (UGC) that contains emotional expressions, without the use of reference translations. To achieve this, we employ an existing emotion-related dataset with human-annotated errors and calculate quality evaluation scores based on the Multi-dimensional Quality Metrics. We compare the accuracy of several LLMs with that of our fine-tuned baseline models, under in-context learning and parameter-efficient fine-tuning (PEFT) scenarios. We find that PEFT of LLMs leads to better performance in score prediction with human interpretable explanations than fine-tuned models. However, a manual analysis of LLM outputs reveals that they still have problems such as refusal to reply to a prompt and unstable output while evaluating machine translation of UGC.</li>
</ul>

<h3>Title: Filtered Randomized Smoothing: A New Defense for Robust Modulation Classification</h3>
<ul>
<li><strong>Authors: </strong>Wenhan Zhang, Meiyu Zhong, Ravi Tandon, Marwan Krunz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.IT, cs.NI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06339">https://arxiv.org/abs/2410.06339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06339">https://arxiv.org/pdf/2410.06339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06339]] Filtered Randomized Smoothing: A New Defense for Robust Modulation Classification(https://arxiv.org/abs/2410.06339)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep Neural Network (DNN) based classifiers have recently been used for the modulation classification of RF signals. These classifiers have shown impressive performance gains relative to conventional methods, however, they are vulnerable to imperceptible (low-power) adversarial attacks. Some of the prominent defense approaches include adversarial training (AT) and randomized smoothing (RS). While AT increases robustness in general, it fails to provide resilience against previously unseen adaptive attacks. Other approaches, such as Randomized Smoothing (RS), which injects noise into the input, address this shortcoming by providing provable certified guarantees against arbitrary attacks, however, they tend to sacrifice accuracy. In this paper, we study the problem of designing robust DNN-based modulation classifiers that can provide provable defense against arbitrary attacks without significantly sacrificing accuracy. To this end, we first analyze the spectral content of commonly studied attacks on modulation classifiers for the benchmark RadioML dataset. We observe that spectral signatures of un-perturbed RF signals are highly localized, whereas attack signals tend to be spread out in frequency. To exploit this spectral heterogeneity, we propose Filtered Randomized Smoothing (FRS), a novel defense which combines spectral filtering together with randomized smoothing. FRS can be viewed as a strengthening of RS by leveraging the specificity (spectral Heterogeneity) inherent to the modulation classification problem. In addition to providing an approach to compute the certified accuracy of FRS, we also provide a comprehensive set of simulations on the RadioML dataset to show the effectiveness of FRS and show that it significantly outperforms existing defenses including AT and RS in terms of accuracy on both attacked and benign signals.</li>
</ul>

<h3>Title: FedGraph: A Research Library and Benchmark for Federated Graph Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Yao, Yuan Li, Xinyi Fan, Junhao Li, Kay Liu, Weizhao Jin, Srivatsan Ravi, Philip S. Yu, Carlee Joe-Wong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06340">https://arxiv.org/abs/2410.06340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06340">https://arxiv.org/pdf/2410.06340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06340]] FedGraph: A Research Library and Benchmark for Federated Graph Learning(https://arxiv.org/abs/2410.06340)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated graph learning is an emerging field with significant practical challenges. While many algorithms have been proposed to enhance model accuracy, their system performance, crucial for real-world deployment, is often overlooked. To address this gap, we present FedGraph, a research library designed for practical distributed deployment and benchmarking in federated graph learning. FedGraph supports a range of state-of-the-art methods and includes profiling tools for system performance evaluation, focusing on communication and computation costs during training. FedGraph can then facilitate the development of practical applications and guide the design of future algorithms.</li>
</ul>

<h3>Title: Robust Domain Generalisation with Causal Invariant Bayesian Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Gaël Gendron, Michael Witbrock, Gillian Dobbie</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06349">https://arxiv.org/abs/2410.06349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06349">https://arxiv.org/pdf/2410.06349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06349]] Robust Domain Generalisation with Causal Invariant Bayesian Neural Networks(https://arxiv.org/abs/2410.06349)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks can obtain impressive performance on various tasks under the assumption that their training domain is identical to their target domain. Performance can drop dramatically when this assumption does not hold. One explanation for this discrepancy is the presence of spurious domain-specific correlations in the training data that the network exploits. Causal mechanisms, in the other hand, can be made invariant under distribution changes as they allow disentangling the factors of distribution underlying the data generation. Yet, learning causal mechanisms to improve out-of-distribution generalisation remains an under-explored area. We propose a Bayesian neural architecture that disentangles the learning of the the data distribution from the inference process mechanisms. We show theoretically and experimentally that our model approximates reasoning under causal interventions. We demonstrate the performance of our method, outperforming point estimate-counterparts, on out-of-distribution image recognition tasks where the data distribution acts as strong adversarial confounders.</li>
</ul>

<h3>Title: Tree-Based Leakage Inspection and Control in Concept Bottleneck Models</h3>
<ul>
<li><strong>Authors: </strong>Angelos Ragkousis, Sonali Parbhoo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06352">https://arxiv.org/abs/2410.06352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06352">https://arxiv.org/pdf/2410.06352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06352]] Tree-Based Leakage Inspection and Control in Concept Bottleneck Models(https://arxiv.org/abs/2410.06352)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>As AI models grow larger, the demand for accountability and interpretability has become increasingly critical for understanding their decision-making processes. Concept Bottleneck Models (CBMs) have gained attention for enhancing interpretability by mapping inputs to intermediate concepts before making final predictions. However, CBMs often suffer from information leakage, where additional input data, not captured by the concepts, is used to improve task performance, complicating the interpretation of downstream predictions. In this paper, we introduce a novel approach for training both joint and sequential CBMs that allows us to identify and control leakage using decision trees. Our method quantifies leakage by comparing the decision paths of hard CBMs with their soft, leaky counterparts. Specifically, we show that soft leaky CBMs extend the decision paths of hard CBMs, particularly in cases where concept information is incomplete. Using this insight, we develop a technique to better inspect and manage leakage, isolating the subsets of data most affected by this. Through synthetic and real-world experiments, we demonstrate that controlling leakage in this way not only improves task accuracy but also yields more informative and transparent explanations.</li>
</ul>

<h3>Title: Language-Assisted Human Part Motion Learning for Skeleton-Based Temporal Action Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Bowen Chen, Haoyu Ji, Zhiyong Wang, Benjamin Filtjens, Chunzhuo Wang, Weihong Ren, Bart Vanrumste, Honghai Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06353">https://arxiv.org/abs/2410.06353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06353">https://arxiv.org/pdf/2410.06353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06353]] Language-Assisted Human Part Motion Learning for Skeleton-Based Temporal Action Segmentation(https://arxiv.org/abs/2410.06353)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Skeleton-based Temporal Action Segmentation involves the dense action classification of variable-length skeleton sequences. Current approaches primarily apply graph-based networks to extract framewise, whole-body-level motion representations, and use one-hot encoded labels for model optimization. However, whole-body motion representations do not capture fine-grained part-level motion representations and the one-hot encoded labels neglect the intrinsic semantic relationships within the language-based action definitions. To address these limitations, we propose a novel method named Language-assisted Human Part Motion Representation Learning (LPL), which contains a Disentangled Part Motion Encoder (DPE) to extract dual-level (i.e., part and whole-body) motion representations and a Language-assisted Distribution Alignment (LDA) strategy for optimizing spatial relations within representations. Specifically, after part-aware skeleton encoding via DPE, LDA generates dual-level action descriptions to construct a textual embedding space with the help of a large-scale language model. Then, LDA motivates the alignment of the embedding space between text descriptions and motions. This alignment allows LDA not only to enhance intra-class compactness but also to transfer the language-encoded semantic correlations among actions to skeleton-based motion learning. Moreover, we propose a simple yet efficient Semantic Offset Adapter to smooth the cross-domain misalignment. Our experiments indicate that LPL achieves state-of-the-art performance across various datasets (e.g., +4.4\% Accuracy, +5.6\% F1 on the PKU-MMD dataset). Moreover, LDA is compatible with existing methods and improves their performance (e.g., +4.8\% Accuracy, +4.3\% F1 on the LARa dataset) without additional inference costs.</li>
</ul>

<h3>Title: SpaLLM: Unified Compressive Adaptation of Large Language Models with Sketching</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Zhang, Junda Su, Oscar Wu, Zhaozhuo Xu, Anshumali Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06364">https://arxiv.org/abs/2410.06364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06364">https://arxiv.org/pdf/2410.06364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06364]] SpaLLM: Unified Compressive Adaptation of Large Language Models with Sketching(https://arxiv.org/abs/2410.06364)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Compressive adaptation approaches, such as QLoRA, are widely popular alternatives for reducing memory requirements during fine-tuning of large language models (LLMs) while producing models capable of handling various downstream tasks. The key idea is to employ a "two-tower" architecture: compressing pre-trained LLM parameters into compact representations and fine-tuning the additive full-precision adapter, which typically has few tunable parameters in low-rank format. However, the strict algebraic assumptions, such as low-rank assumption, and the complexity of composing two-tower architectures are some of the known shortcomings, resulting in a poor accuracy-efficiency trade-off. In response to these known limitations, we propose SpaLLM (Sketched Parameter Adaptation of LLMs), a novel compressive adaptation approach for LLMs. This method is also the first to illustrate parameter-sharing compression methods for LLM fine-tuning, which, unlike QLoRA, are free from strict low-rank algebraic assumptions on adapters. Furthermore, our proposal unifies model compression and adaptation into a single, streamlined process, eliminating the need for two-tower architectures. SpaLLM sketches pre-trained LLM weights into lookup tables and directly fine-tunes the values in these tables. This approach simplifies LLMs' compressive adaptation workflow, potentially improves multi-user serving efficiency, and delivers significantly better accuracy for both natural language understanding and generation tasks. Moreover, by avoiding the "two-tower" architecture, our framework only requires one compressed matrix multiplication per layer during inference, demonstrating superior inference efficiency compared to previous methods.</li>
</ul>

<h3>Title: Communication-Efficient Federated Group Distributionally Robust Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zhishuai Guo, Tianbao Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06369">https://arxiv.org/abs/2410.06369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06369">https://arxiv.org/pdf/2410.06369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06369]] Communication-Efficient Federated Group Distributionally Robust Optimization(https://arxiv.org/abs/2410.06369)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning faces challenges due to the heterogeneity in data volumes and distributions at different clients, which can compromise model generalization ability to various distributions. Existing approaches to address this issue based on group distributionally robust optimization (GDRO) often lead to high communication and sample complexity. To this end, this work introduces algorithms tailored for communication-efficient Federated Group Distributionally Robust Optimization (FGDRO). Our contributions are threefold: Firstly, we introduce the FGDRO-CVaR algorithm, which optimizes the average top-K losses while reducing communication complexity to $O(1/\epsilon^4)$, where $\epsilon$ denotes the desired precision level. Secondly, our FGDRO-KL algorithm is crafted to optimize KL regularized FGDRO, cutting communication complexity to $O(1/\epsilon^3)$. Lastly, we propose FGDRO-KL-Adam to utilize Adam-type local updates in FGDRO-KL, which not only maintains a communication cost of $O(1/\epsilon^3)$ but also shows potential to surpass SGD-type local steps in practical applications. The effectiveness of our algorithms has been demonstrated on a variety of real-world tasks, including natural language processing and computer vision.</li>
</ul>

<h3>Title: HumVI: A Multilingual Dataset for Detecting Violent Incidents Impacting Humanitarian Aid</h3>
<ul>
<li><strong>Authors: </strong>Hemank Lamba, Anton Abilov, Ke Zhang, Elizabeth M. Olson, Henry k. Dambanemuya, João c. Bárcia, David S. Batista, Christina Wille, Aoife Cahill, Joel Tetreault, Alex Jaimes</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06370">https://arxiv.org/abs/2410.06370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06370">https://arxiv.org/pdf/2410.06370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06370]] HumVI: A Multilingual Dataset for Detecting Violent Incidents Impacting Humanitarian Aid(https://arxiv.org/abs/2410.06370)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>Humanitarian organizations can enhance their effectiveness by analyzing data to discover trends, gather aggregated insights, manage their security risks, support decision-making, and inform advocacy and funding proposals. However, data about violent incidents with direct impact and relevance for humanitarian aid operations is not readily available. An automatic data collection and NLP-backed classification framework aligned with humanitarian perspectives can help bridge this gap. In this paper, we present HumVI - a dataset comprising news articles in three languages (English, French, Arabic) containing instances of different types of violent incidents categorized by the humanitarian sector they impact, e.g., aid security, education, food security, health, and protection. Reliable labels were obtained for the dataset by partnering with a data-backed humanitarian organization, Insecurity Insight. We provide multiple benchmarks for the dataset, employing various deep learning architectures and techniques, including data augmentation and mask loss, to address different task-related challenges, e.g., domain expansion. The dataset is publicly available at this https URL.</li>
</ul>

<h3>Title: Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Li, Juanxi Tian, Zedong Wang, Luyuan Zhang, Zicheng Liu, Weiyang Jin, Yang Liu, Baigui Sun, Stan Z. Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06373">https://arxiv.org/abs/2410.06373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06373">https://arxiv.org/pdf/2410.06373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06373]] Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning(https://arxiv.org/abs/2410.06373)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper delves into the interplay between vision backbones and optimizers, unvealing an inter-dependent phenomenon termed \textit{\textbf{b}ackbone-\textbf{o}ptimizer \textbf{c}oupling \textbf{b}ias} (BOCB). We observe that canonical CNNs, such as VGG and ResNet, exhibit a marked co-dependency with SGD families, while recent architectures like ViTs and ConvNeXt share a tight coupling with the adaptive learning rate ones. We further show that BOCB can be introduced by both optimizers and certain backbone designs and may significantly impact the pre-training and downstream fine-tuning of vision models. Through in-depth empirical analysis, we summarize takeaways on recommended optimizers and insights into robust vision backbone architectures. We hope this work can inspire the community to question long-held assumptions on backbones and optimizers, stimulate further explorations, and thereby contribute to more robust vision systems. The source code and models are publicly available at this https URL.</li>
</ul>

<h3>Title: Adver-City: Open-Source Multi-Modal Dataset for Collaborative Perception Under Adverse Weather Conditions</h3>
<ul>
<li><strong>Authors: </strong>Mateus Karvat, Sidney Givigi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06380">https://arxiv.org/abs/2410.06380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06380">https://arxiv.org/pdf/2410.06380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06380]] Adver-City: Open-Source Multi-Modal Dataset for Collaborative Perception Under Adverse Weather Conditions(https://arxiv.org/abs/2410.06380)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Adverse weather conditions pose a significant challenge to the widespread adoption of Autonomous Vehicles (AVs) by impacting sensors like LiDARs and cameras. Even though Collaborative Perception (CP) improves AV perception in difficult conditions, existing CP datasets lack adverse weather conditions. To address this, we introduce Adver-City, the first open-source synthetic CP dataset focused on adverse weather conditions. Simulated in CARLA with OpenCDA, it contains over 24 thousand frames, over 890 thousand annotations, and 110 unique scenarios across six different weather conditions: clear weather, soft rain, heavy rain, fog, foggy heavy rain and, for the first time in a synthetic CP dataset, glare. It has six object categories including pedestrians and cyclists, and uses data from vehicles and roadside units featuring LiDARs, RGB and semantic segmentation cameras, GNSS, and IMUs. Its scenarios, based on real crash reports, depict the most relevant road configurations for adverse weather and poor visibility conditions, varying in object density, with both dense and sparse scenes, allowing for novel testing conditions of CP models. Benchmarks run on the dataset show that weather conditions created challenging conditions for perception models, reducing multi-modal object detection performance by up to 19%, while object density affected LiDAR-based detection by up to 29%. The dataset, code and documentation are available at this https URL.</li>
</ul>

<h3>Title: Counterfactual Causal Inference in Natural Language with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gaël Gendron, Jože M. Rožanec, Michael Witbrock, Gillian Dobbie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06392">https://arxiv.org/abs/2410.06392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06392">https://arxiv.org/pdf/2410.06392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06392]] Counterfactual Causal Inference in Natural Language with Large Language Models(https://arxiv.org/abs/2410.06392)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Causal structure discovery methods are commonly applied to structured data where the causal variables are known and where statistical testing can be used to assess the causal relationships. By contrast, recovering a causal structure from unstructured natural language data such as news articles contains numerous challenges due to the absence of known variables or counterfactual data to estimate the causal links. Large Language Models (LLMs) have shown promising results in this direction but also exhibit limitations. This work investigates LLM's abilities to build causal graphs from text documents and perform counterfactual causal inference. We propose an end-to-end causal structure discovery and causal inference method from natural language: we first use an LLM to extract the instantiated causal variables from text data and build a causal graph. We merge causal graphs from multiple data sources to represent the most exhaustive set of causes possible. We then conduct counterfactual inference on the estimated graph. The causal graph conditioning allows reduction of LLM biases and better represents the causal estimands. We use our method to show that the limitations of LLMs in counterfactual causal reasoning come from prediction errors and propose directions to mitigate them. We demonstrate the applicability of our method on real-world news articles.</li>
</ul>

<h3>Title: Provable Accuracy Bounds for Hybrid Dynamical Optimization and Sampling</h3>
<ul>
<li><strong>Authors: </strong>Matthew X. Burns, Qingyuan Hou, Michael C. Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06397">https://arxiv.org/abs/2410.06397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06397">https://arxiv.org/pdf/2410.06397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06397]] Provable Accuracy Bounds for Hybrid Dynamical Optimization and Sampling(https://arxiv.org/abs/2410.06397)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Analog dynamical accelerators (DXs) are a growing sub-field in computer architecture research, offering order-of-magnitude gains in power efficiency and latency over traditional digital methods in several machine learning, optimization, and sampling tasks. However, limited-capacity accelerators require hybrid analog/digital algorithms to solve real-world problems, commonly using large-neighborhood local search (LNLS) frameworks. Unlike fully digital algorithms, hybrid LNLS has no non-asymptotic convergence guarantees and no principled hyperparameter selection schemes, particularly limiting cross-device training and inference. In this work, we provide non-asymptotic convergence guarantees for hybrid LNLS by reducing to block Langevin Diffusion (BLD) algorithms. Adapting tools from classical sampling theory, we prove exponential KL-divergence convergence for randomized and cyclic block selection strategies using ideal DXs. With finite device variation, we provide explicit bounds on the 2-Wasserstein bias in terms of step duration, noise strength, and function parameters. Our BLD model provides a key link between established theory and novel computing platforms, and our theoretical results provide a closed-form expression linking device variation, algorithm hyperparameters, and performance.</li>
</ul>

<h3>Title: Tackling the Abstraction and Reasoning Corpus with Vision Transformers: the Importance of 2D Representation, Positions, and Objects</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Li, Yudong Xu, Scott Sanner, Elias Boutros Khalil</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06405">https://arxiv.org/abs/2410.06405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06405">https://arxiv.org/pdf/2410.06405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06405]] Tackling the Abstraction and Reasoning Corpus with Vision Transformers: the Importance of 2D Representation, Positions, and Objects(https://arxiv.org/abs/2410.06405)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The Abstraction and Reasoning Corpus (ARC) is a popular benchmark focused on visual reasoning in the evaluation of Artificial Intelligence systems. In its original framing, an ARC task requires solving a program synthesis problem over small 2D images using a few input-output training pairs. In this work, we adopt the recently popular data-driven approach to the ARC and ask whether a Vision Transformer (ViT) can learn the implicit mapping, from input image to output image, that underlies the task. We show that a ViT -- otherwise a state-of-the-art model for images -- fails dramatically on most ARC tasks even when trained on one million examples per task. This points to an inherent representational deficiency of the ViT architecture that makes it incapable of uncovering the simple structured mappings underlying the ARC tasks. Building on these insights, we propose ViTARC, a ViT-style architecture that unlocks some of the visual reasoning capabilities required by the ARC. Specifically, we use a pixel-level input representation, design a spatially-aware tokenization scheme, and introduce a novel object-based positional encoding that leverages automatic segmentation, among other enhancements. Our task-specific ViTARC models achieve a test solve rate close to 100% on more than half of the 400 public ARC tasks strictly through supervised learning from input-output grids. This calls attention to the importance of imbuing the powerful (Vision) Transformer with the correct inductive biases for abstract visual reasoning that are critical even when the training data is plentiful and the mapping is noise-free. Hence, ViTARC provides a strong foundation for future research in visual reasoning using transformer-based architectures.</li>
</ul>

<h3>Title: A Skewness-Based Criterion for Addressing Heteroscedastic Noise in Causal Discovery</h3>
<ul>
<li><strong>Authors: </strong>Yingyu Lin, Yuxing Huang, Wenqin Liu, Haoran Deng, Ignavier Ng, Kun Zhang, Mingming Gong, Yi-An Ma, Biwei Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06407">https://arxiv.org/abs/2410.06407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06407">https://arxiv.org/pdf/2410.06407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06407]] A Skewness-Based Criterion for Addressing Heteroscedastic Noise in Causal Discovery(https://arxiv.org/abs/2410.06407)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Real-world data often violates the equal-variance assumption (homoscedasticity), making it essential to account for heteroscedastic noise in causal discovery. In this work, we explore heteroscedastic symmetric noise models (HSNMs), where the effect $Y$ is modeled as $Y = f(X) + \sigma(X)N$, with $X$ as the cause and $N$ as independent noise following a symmetric distribution. We introduce a novel criterion for identifying HSNMs based on the skewness of the score (i.e., the gradient of the log density) of the data distribution. This criterion establishes a computationally tractable measurement that is zero in the causal direction but nonzero in the anticausal direction, enabling the causal direction discovery. We extend this skewness-based criterion to the multivariate setting and propose SkewScore, an algorithm that handles heteroscedastic noise without requiring the extraction of exogenous noise. We also conduct a case study on the robustness of SkewScore in a bivariate model with a latent confounder, providing theoretical insights into its performance. Empirical studies further validate the effectiveness of the proposed method.</li>
</ul>

<h3>Title: MIRACLE 3D: Memory-efficient Integrated Robust Approach for Continual Learning on Point Clouds via Shape Model construction</h3>
<ul>
<li><strong>Authors: </strong>Hossein Resani, Behrooz Nasihatkon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06418">https://arxiv.org/abs/2410.06418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06418">https://arxiv.org/pdf/2410.06418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06418]] MIRACLE 3D: Memory-efficient Integrated Robust Approach for Continual Learning on Point Clouds via Shape Model construction(https://arxiv.org/abs/2410.06418)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce a novel framework for memory-efficient and privacy-preserving continual learning in 3D object classification. Unlike conventional memory-based approaches in continual learning that require storing numerous exemplars, our method constructs a compact shape model for each class, retaining only the mean shape along with a few key modes of variation. This strategy not only enables the generation of diverse training samples while drastically reducing memory usage but also enhances privacy by eliminating the need to store original data. To further improve model robustness against input variations, an issue common in 3D domains due to the absence of strong backbones and limited training data, we incorporate Gradient Mode Regularization. This technique enhances model stability and broadens classification margins, resulting in accuracy improvements. We validate our approach through extensive experiments on the ModelNet40, ShapeNet, and ScanNet datasets, where we achieve state-of-the-art performance. Notably, our method consumes only 15% of the memory required by competing methods on the ModelNet40 and ShapeNet, while achieving comparable performance on the challenging ScanNet dataset with just 8.5% of the memory. These results underscore the scalability, effectiveness, and privacy-preserving strengths of our framework for 3D object classification.</li>
</ul>

<h3>Title: FAIREDU: A Multiple Regression-Based Method for Enhancing Fairness in Machine Learning Models for Educational Applications</h3>
<ul>
<li><strong>Authors: </strong>Nga Pham, Minh Kha Do, Tran Vu Dai, Pham Ngoc Hung, Anh Nguyen-Duc</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06423">https://arxiv.org/abs/2410.06423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06423">https://arxiv.org/pdf/2410.06423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06423]] FAIREDU: A Multiple Regression-Based Method for Enhancing Fairness in Machine Learning Models for Educational Applications(https://arxiv.org/abs/2410.06423)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Fairness in artificial intelligence and machine learning (AI/ML) models is becoming critically important, especially as decisions made by these systems impact diverse groups. In education, a vital sector for all countries, the widespread application of AI/ML systems raises specific concerns regarding fairness. Current research predominantly focuses on fairness for individual sensitive features, which limits the comprehensiveness of fairness assessments. This paper introduces FAIREDU, a novel and effective method designed to improve fairness across multiple sensitive features. Through extensive experiments, we evaluate FAIREDU effectiveness in enhancing fairness without compromising model performance. The results demonstrate that FAIREDU addresses intersectionality across features such as gender, race, age, and other sensitive features, outperforming state-of-the-art methods with minimal effect on model accuracy. The paper also explores potential future research directions to enhance further the method robustness and applicability to various machine-learning models and datasets.</li>
</ul>

<h3>Title: Stress Detection on Code-Mixed Texts in Dravidian Languages using Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>L. Ramos, M. Shahiki-Tash, Z. Ahani, A. Eponon, O. Kolesnikova, H. Calvo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06428">https://arxiv.org/abs/2410.06428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06428">https://arxiv.org/pdf/2410.06428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06428]] Stress Detection on Code-Mixed Texts in Dravidian Languages using Machine Learning(https://arxiv.org/abs/2410.06428)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Stress is a common feeling in daily life, but it can affect mental well-being in some situations, the development of robust detection models is imperative. This study introduces a methodical approach to the stress identification in code-mixed texts for Dravidian languages. The challenge encompassed two datasets, targeting Tamil and Telugu languages respectively. This proposal underscores the importance of using uncleaned text as a benchmark to refine future classification methodologies, incorporating diverse preprocessing techniques. Random Forest algorithm was used, featuring three textual representations: TF-IDF, Uni-grams of words, and a composite of (1+2+3)-Grams of characters. The approach achieved a good performance for both linguistic categories, achieving a Macro F1-score of 0.734 in Tamil and 0.727 in Telugu, overpassing results achieved with different complex techniques such as FastText and Transformer models. The results underscore the value of uncleaned data for mental state detection and the challenges classifying code-mixed texts for stress, indicating the potential for improved performance through cleaning data, other preprocessing techniques, or more complex models.</li>
</ul>

<h3>Title: Functional-level Uncertainty Quantification for Calibrated Fine-tuning on LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ruijia Niu, Dongxia Wu, Rose Yu, Yi-An Ma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06431">https://arxiv.org/abs/2410.06431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06431">https://arxiv.org/pdf/2410.06431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06431]] Functional-level Uncertainty Quantification for Calibrated Fine-tuning on LLMs(https://arxiv.org/abs/2410.06431)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>From common-sense reasoning to domain-specific tasks, parameter-efficient fine tuning (PEFT) methods for large language models (LLMs) have showcased significant performance improvements on downstream tasks. However, fine-tuned LLMs often struggle with overconfidence in uncertain predictions, particularly due to sparse training data. This overconfidence reflects poor epistemic uncertainty calibration, which arises from limitations in the model's ability to generalize with limited data. Existing PEFT uncertainty quantification methods for LLMs focus on the post fine-tuning stage and thus have limited capability in calibrating epistemic uncertainty. To address these limitations, we propose Functional-Level Uncertainty Quantification for Calibrated Fine-Tuning (UQ4CT), which captures and calibrates functional-level epistemic uncertainty during the fine-tuning stage via a mixture-of-expert framework. We show that UQ4CT reduces Expected Calibration Error (ECE) by more than $25\%$ while maintaining high accuracy across $5$ benchmarks. Furthermore, UQ4CT maintains superior ECE performance with high accuracy under distribution shift, showcasing improved generalizability.</li>
</ul>

<h3>Title: MaD-Scientist: AI-based Scientist solving Convection-Diffusion-Reaction Equations Using Massive PINN-Based Prior Data</h3>
<ul>
<li><strong>Authors: </strong>Mingu Kang, Dongseok Lee, Woojin Cho, Jaehyeon Park, Kookjin Lee, Anthony Gruber, Youngjoon Hong, Noseong Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06442">https://arxiv.org/abs/2410.06442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06442">https://arxiv.org/pdf/2410.06442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06442]] MaD-Scientist: AI-based Scientist solving Convection-Diffusion-Reaction Equations Using Massive PINN-Based Prior Data(https://arxiv.org/abs/2410.06442)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), like ChatGPT, have shown that even trained with noisy prior data, they can generalize effectively to new tasks through in-context learning (ICL) and pre-training techniques. Motivated by this, we explore whether a similar approach can be applied to scientific foundation models (SFMs). Our methodology is structured as follows: (i) we collect low-cost physics-informed neural network (PINN)-based approximated prior data in the form of solutions to partial differential equations (PDEs) constructed through an arbitrary linear combination of mathematical dictionaries; (ii) we utilize Transformer architectures with self and cross-attention mechanisms to predict PDE solutions without knowledge of the governing equations in a zero-shot setting; (iii) we provide experimental evidence on the one-dimensional convection-diffusion-reaction equation, which demonstrate that pre-training remains robust even with approximated prior data, with only marginal impacts on test accuracy. Notably, this finding opens the path to pre-training SFMs with realistic, low-cost data instead of (or in conjunction with) numerical high-cost data. These results support the conjecture that SFMs can improve in a manner similar to LLMs, where fully cleaning the vast set of sentences crawled from the Internet is nearly impossible.</li>
</ul>

<h3>Title: Multi-label Classification for Android Malware Based on Active Learning</h3>
<ul>
<li><strong>Authors: </strong>Qijing Qiao, Ruitao Feng, Sen Chen, Fei Zhang, Xiaohong Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06444">https://arxiv.org/abs/2410.06444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06444">https://arxiv.org/pdf/2410.06444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06444]] Multi-label Classification for Android Malware Based on Active Learning(https://arxiv.org/abs/2410.06444)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The existing malware classification approaches (i.e., binary and family classification) can barely benefit subsequent analysis with their outputs. Even the family classification approaches suffer from lacking a formal naming standard and an incomplete definition of malicious behaviors. More importantly, the existing approaches are powerless for one malware with multiple malicious behaviors, while this is a very common phenomenon for Android malware in the wild. So, neither of them can provide researchers with a direct and comprehensive enough understanding of malware. In this paper, we propose MLCDroid, an ML-based multi-label classification approach that can directly indicate the existence of pre-defined malicious behaviors. With an in-depth analysis, we summarize six basic malicious behaviors from real-world malware with security reports and construct a labeled dataset. We compare the results of 70 algorithm combinations to evaluate the effectiveness (best at 73.3%). Faced with the challenge of the expensive cost of data annotation, we further propose an active learning approach based on data augmentation, which can improve the overall accuracy to 86.7% with a data augmentation of 5,000+ high-quality samples from an unlabeled malware dataset. This is the first multi-label Android malware classification approach intending to provide more information on fine-grained malicious behaviors.</li>
</ul>

<h3>Title: Machine Unlearning in Forgettability Sequence</h3>
<ul>
<li><strong>Authors: </strong>Junjie Chen, Qian Chen, Jian Lou, Xiaoyu Zhang, Kai Wu, Zilong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06446">https://arxiv.org/abs/2410.06446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06446">https://arxiv.org/pdf/2410.06446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06446]] Machine Unlearning in Forgettability Sequence(https://arxiv.org/abs/2410.06446)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Machine unlearning (MU) is becoming a promising paradigm to achieve the "right to be forgotten", where the training trace of any chosen data points could be eliminated, while maintaining the model utility on general testing samples after unlearning. With the advancement of forgetting research, many fundamental open questions remain unanswered: do different samples exhibit varying levels of difficulty in being forgotten? Further, does the sequence in which samples are forgotten, determined by their respective difficulty levels, influence the performance of forgetting algorithms? In this paper, we identify key factor affecting unlearning difficulty and the performance of unlearning algorithms. We find that samples with higher privacy risks are more likely to be unlearning, indicating that the unlearning difficulty varies among different samples which motives a more precise unlearning mode. Built upon this insight, we propose a general unlearning framework, dubbed RSU, which consists of Ranking module and SeqUnlearn module.</li>
</ul>

<h3>Title: Modeling chaotic Lorenz ODE System using Scientific Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Sameera S Kashyap, Raj Abhijit Dandekar, Rajat Dandekar, Sreedath Panat</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06452">https://arxiv.org/abs/2410.06452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06452">https://arxiv.org/pdf/2410.06452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06452]] Modeling chaotic Lorenz ODE System using Scientific Machine Learning(https://arxiv.org/abs/2410.06452)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>In climate science, models for global warming and weather prediction face significant challenges due to the limited availability of high-quality data and the difficulty in obtaining it, making data efficiency crucial. In the past few years, Scientific Machine Learning (SciML) models have gained tremendous traction as they can be trained in a data-efficient manner, making them highly suitable for real-world climate applications. Despite this, very little attention has been paid to chaotic climate system modeling utilizing SciML methods. In this paper, we have integrated SciML methods into foundational weather models, where we have enhanced large-scale climate predictions with a physics-informed approach that achieves high accuracy with reduced data. We successfully demonstrate that by combining the interpretability of physical climate models with the computational power of neural networks, SciML models can prove to be a reliable tool for modeling climate. This indicates a shift from the traditional black box-based machine learning modeling of climate systems to physics-informed decision-making, leading to effective climate policy implementation.</li>
</ul>

<h3>Title: From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yang Bai, Yang Zhou, Jun Zhou, Rick Siow Mong Goh, Daniel Shu Wei Ting, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06456">https://arxiv.org/abs/2410.06456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06456">https://arxiv.org/pdf/2410.06456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06456]] From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning(https://arxiv.org/abs/2410.06456)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large vision language models (VLMs) combine large language models with vision encoders, demonstrating promise across various tasks. However, they often underperform in task-specific applications due to domain gaps between pre-training and fine-tuning. We introduce VITask, a novel framework that enhances task-specific adaptability of VLMs by integrating task-specific models (TSMs). VITask employs three key strategies: exemplar prompting (EP), response distribution alignment (RDA), and contrastive response tuning (CRT) to improve the task-specific performance of VLMs by adjusting their response distributions. EP allows TSM features to guide VLMs, while RDA enables VLMs to adapt without TSMs during inference by learning from exemplar-prompted models. CRT further optimizes the ranking of correct image-response pairs, thereby reducing the risk of generating undesired responses. Experiments on 12 medical diagnosis datasets across 9 imaging modalities show that VITask outperforms both vanilla instruction-tuned VLMs and TSMs, showcasing its ability to integrate complementary features from both models effectively. Additionally, VITask offers practical advantages such as flexible TSM integration and robustness to incomplete instructions, making it a versatile and efficient solution for task-specific VLM tuning. Our code are available at this https URL.</li>
</ul>

<h3>Title: A Benchmark on Directed Graph Representation Learning in Hardware Designs</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Wang, Yinan Huang, Nan Wu, Pan Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06460">https://arxiv.org/abs/2410.06460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06460">https://arxiv.org/pdf/2410.06460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06460]] A Benchmark on Directed Graph Representation Learning in Hardware Designs(https://arxiv.org/abs/2410.06460)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>To keep pace with the rapid advancements in design complexity within modern computing systems, directed graph representation learning (DGRL) has become crucial, particularly for encoding circuit netlists, computational graphs, and developing surrogate models for hardware performance prediction. However, DGRL remains relatively unexplored, especially in the hardware domain, mainly due to the lack of comprehensive and user-friendly benchmarks. This study presents a novel benchmark comprising five hardware design datasets and 13 prediction tasks spanning various levels of circuit abstraction. We evaluate 21 DGRL models, employing diverse graph neural networks and graph transformers (GTs) as backbones, enhanced by positional encodings (PEs) tailored for directed graphs. Our results highlight that bidirected (BI) message passing neural networks (MPNNs) and robust PEs significantly enhance model performance. Notably, the top-performing models include PE-enhanced GTs interleaved with BI-MPNN layers and BI-Graph Isomorphism Network, both surpassing baselines across the 13 tasks. Additionally, our investigation into out-of-distribution (OOD) performance emphasizes the urgent need to improve OOD generalization in DGRL models. This benchmark, implemented with a modular codebase, streamlines the evaluation of DGRL models for both hardware and ML practitioners</li>
</ul>

<h3>Title: Hallucinating AI Hijacking Attack: Large Language Models and Malicious Code Recommenders</h3>
<ul>
<li><strong>Authors: </strong>David Noever, Forrest McKee</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06462">https://arxiv.org/abs/2410.06462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06462">https://arxiv.org/pdf/2410.06462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06462]] Hallucinating AI Hijacking Attack: Large Language Models and Malicious Code Recommenders(https://arxiv.org/abs/2410.06462)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>The research builds and evaluates the adversarial potential to introduce copied code or hallucinated AI recommendations for malicious code in popular code repositories. While foundational large language models (LLMs) from OpenAI, Google, and Anthropic guard against both harmful behaviors and toxic strings, previous work on math solutions that embed harmful prompts demonstrate that the guardrails may differ between expert contexts. These loopholes would appear in mixture of expert's models when the context of the question changes and may offer fewer malicious training examples to filter toxic comments or recommended offensive actions. The present work demonstrates that foundational models may refuse to propose destructive actions correctly when prompted overtly but may unfortunately drop their guard when presented with a sudden change of context, like solving a computer programming challenge. We show empirical examples with trojan-hosting repositories like GitHub, NPM, NuGet, and popular content delivery networks (CDN) like jsDelivr which amplify the attack surface. In the LLM's directives to be helpful, example recommendations propose application programming interface (API) endpoints which a determined domain-squatter could acquire and setup attack mobile infrastructure that triggers from the naively copied code. We compare this attack to previous work on context-shifting and contrast the attack surface as a novel version of "living off the land" attacks in the malware literature. In the latter case, foundational language models can hijack otherwise innocent user prompts to recommend actions that violate their owners' safety policies when posed directly without the accompanying coding support request.</li>
</ul>

<h3>Title: WAPITI: A Watermark for Finetuned Open-Source LLMs</h3>
<ul>
<li><strong>Authors: </strong>Lingjie Chen, Ruizhong Qiu, Siyu Yuan, Zhining Liu, Tianxin Wei, Hyunsik Yoo, Zhichen Zeng, Deqing Yang, Hanghang Tong</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06467">https://arxiv.org/abs/2410.06467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06467">https://arxiv.org/pdf/2410.06467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06467]] WAPITI: A Watermark for Finetuned Open-Source LLMs(https://arxiv.org/abs/2410.06467)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Watermarking of large language models (LLMs) generation embeds an imperceptible statistical pattern within texts, making it algorithmically detectable. Watermarking is a promising method for addressing potential harm and biases from LLMs, as it enables traceability, accountability, and detection of manipulated content, helping to mitigate unintended consequences. However, for open-source models, watermarking faces two major challenges: (i) incompatibility with fine-tuned models, and (ii) vulnerability to fine-tuning attacks. In this work, we propose WAPITI, a new method that transfers watermarking from base models to fine-tuned models through parameter integration. To the best of our knowledge, we propose the first watermark for fine-tuned open-source LLMs that preserves their fine-tuned capabilities. Furthermore, our approach offers an effective defense against fine-tuning attacks. We test our method on various model architectures and watermarking strategies. Results demonstrate that our method can successfully inject watermarks and is highly compatible with fine-tuned models. Additionally, we offer an in-depth analysis of how parameter editing influences the watermark strength and overall capabilities of the resulting models.</li>
</ul>

<h3>Title: LLM Compression with Neural Architecture Search</h3>
<ul>
<li><strong>Authors: </strong>Rhea Sanjay Sukthanker, Benedikt Staffler, Frank Hutter, Aaron Klein</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06479">https://arxiv.org/abs/2410.06479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06479">https://arxiv.org/pdf/2410.06479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06479]] LLM Compression with Neural Architecture Search(https://arxiv.org/abs/2410.06479)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit remarkable reasoning abilities, allowing them to generalize across a wide range of downstream tasks, such as commonsense reasoning or instruction following. However, as LLMs scale, inference costs become increasingly prohibitive, accumulating significantly over their life cycle. This poses the question: Can we compress pre-trained LLMs to meet diverse size and latency requirements? We leverage Neural Architecture Search (NAS) to compress LLMs by pruning structural components, such as attention heads, neurons, and layers, aiming to achieve a Pareto-optimal balance between performance and efficiency. While NAS already achieved promising results on small language models in previous work, in this paper we propose various extensions that allow us to scale to LLMs. Compared to structural pruning baselines, we show that NAS improves performance up to 3.4% on MMLU with an on-device latency speedup.</li>
</ul>

<h3>Title: TCGU: Data-centric Graph Unlearning based on Transferable Condensation</h3>
<ul>
<li><strong>Authors: </strong>Fan Li, Xiaoyang Wang, Dawei Cheng, Wenjie Zhang, Ying Zhang, Xuemin Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06480">https://arxiv.org/abs/2410.06480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06480">https://arxiv.org/pdf/2410.06480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06480]] TCGU: Data-centric Graph Unlearning based on Transferable Condensation(https://arxiv.org/abs/2410.06480)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>With growing demands for data privacy and model robustness, graph unlearning (GU), which erases the influence of specific data on trained GNN models, has gained significant attention. However, existing exact unlearning methods suffer from either low efficiency or poor model performance. While being more utility-preserving and efficient, current approximate unlearning methods are not applicable in the zero-glance privacy setting, where the deleted samples cannot be accessed during unlearning due to immediate deletion requested by regulations. Besides, these approximate methods, which try to directly perturb model parameters still involve high privacy concerns in practice. To fill the gap, we propose Transferable Condensation Graph Unlearning (TCGU), a data-centric solution to zero-glance graph unlearning. Specifically, we first design a two-level alignment strategy to pre-condense the original graph into a small yet utility-preserving dataset. Upon receiving an unlearning request, we fine-tune the pre-condensed data with a low-rank plugin, to directly align its distribution with the remaining graph, thus efficiently revoking the information of deleted data without accessing them. A novel similarity distribution matching approach and a discrimination regularizer are proposed to effectively transfer condensed data and preserve its utility in GNN training, respectively. Finally, we retrain the GNN on the transferred condensed data. Extensive experiments on 6 benchmark datasets demonstrate that TCGU can achieve superior performance in terms of model utility, unlearning efficiency, and unlearning efficacy than existing GU methods.</li>
</ul>

<h3>Title: OledFL: Unleashing the Potential of Decentralized Federated Learning via Opposite Lookahead Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Qinglun Li, Miao Zhang, Mengzhu Wang, Quanjun Yin, Li Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06482">https://arxiv.org/abs/2410.06482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06482">https://arxiv.org/pdf/2410.06482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06482]] OledFL: Unleashing the Potential of Decentralized Federated Learning via Opposite Lookahead Enhancement(https://arxiv.org/abs/2410.06482)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Decentralized Federated Learning (DFL) surpasses Centralized Federated Learning (CFL) in terms of faster training, privacy preservation, and light communication, making it a promising alternative in the field of federated learning. However, DFL still exhibits significant disparities with CFL in terms of generalization ability such as rarely theoretical understanding and degraded empirical performance due to severe inconsistency. In this paper, we enhance the consistency of DFL by developing an opposite lookahead enhancement technique (Ole), yielding OledFL to optimize the initialization of each client in each communication round, thus significantly improving both the generalization and convergence speed. Moreover, we rigorously establish its convergence rate in non-convex setting and characterize its generalization bound through uniform stability, which provides concrete reasons why OledFL can achieve both the fast convergence speed and high generalization ability. Extensive experiments conducted on the CIFAR10 and CIFAR100 datasets with Dirichlet and Pathological distributions illustrate that our OledFL can achieve up to 5\% performance improvement and 8$\times$ speedup, compared to the most popular DFedAvg optimizer in DFL.</li>
</ul>

<h3>Title: HFH-Font: Few-shot Chinese Font Synthesis with Higher Quality, Faster Speed, and Higher Resolution</h3>
<ul>
<li><strong>Authors: </strong>Hua Li, Zhouhui Lian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06488">https://arxiv.org/abs/2410.06488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06488">https://arxiv.org/pdf/2410.06488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06488]] HFH-Font: Few-shot Chinese Font Synthesis with Higher Quality, Faster Speed, and Higher Resolution(https://arxiv.org/abs/2410.06488)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The challenge of automatically synthesizing high-quality vector fonts, particularly for writing systems (e.g., Chinese) consisting of huge amounts of complex glyphs, remains unsolved. Existing font synthesis techniques fall into two categories: 1) methods that directly generate vector glyphs, and 2) methods that initially synthesize glyph images and then vectorize them. However, the first category often fails to construct complete and correct shapes for complex glyphs, while the latter struggles to efficiently synthesize high-resolution (i.e., 1024 $\times$ 1024 or higher) glyph images while preserving local details. In this paper, we introduce HFH-Font, a few-shot font synthesis method capable of efficiently generating high-resolution glyph images that can be converted into high-quality vector glyphs. More specifically, our method employs a diffusion model-based generative framework with component-aware conditioning to learn different levels of style information adaptable to varying input reference sizes. We also design a distillation module based on Score Distillation Sampling for 1-step fast inference, and a style-guided super-resolution module to refine and upscale low-resolution synthesis results. Extensive experiments, including a user study with professional font designers, have been conducted to demonstrate that our method significantly outperforms existing font synthesis approaches. Experimental results show that our method produces high-fidelity, high-resolution raster images which can be vectorized into high-quality vector fonts. Using our method, for the first time, large-scale Chinese vector fonts of a quality comparable to those manually created by professional font designers can be automatically generated.</li>
</ul>

<h3>Title: FedL2G: Learning to Guide Local Training in Heterogeneous Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Jianqing Zhang, Yang Liu, Yang Hua, Jian Cao, Qiang Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06490">https://arxiv.org/abs/2410.06490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06490">https://arxiv.org/pdf/2410.06490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06490]] FedL2G: Learning to Guide Local Training in Heterogeneous Federated Learning(https://arxiv.org/abs/2410.06490)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Data and model heterogeneity are two core issues in Heterogeneous Federated Learning (HtFL). In scenarios with heterogeneous model architectures, aggregating model parameters becomes infeasible, leading to the use of prototypes (i.e., class representative feature vectors) for aggregation and guidance. However, they still experience a mismatch between the extra guiding objective and the client's original local objective when aligned with global prototypes. Thus, we propose a Federated Learning-to-Guide (FedL2G) method that adaptively learns to guide local training in a federated manner and ensures the extra guidance is beneficial to clients' original tasks. With theoretical guarantees, FedL2G efficiently implements the learning-to-guide process using only first-order derivatives w.r.t. model parameters and achieves a non-convex convergence rate of O(1/T). We conduct extensive experiments on two data heterogeneity and six model heterogeneity settings using 14 heterogeneous model architectures (e.g., CNNs and ViTs) to demonstrate FedL2G's superior performance compared to six counterparts.</li>
</ul>

<h3>Title: Chemistry-Inspired Diffusion with Non-Differentiable Guidance</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Shen, Chenhao Zhang, Sijie Fu, Chenghui Zhou, Newell Washburn, Barnabás Póczos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06502">https://arxiv.org/abs/2410.06502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06502">https://arxiv.org/pdf/2410.06502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06502]] Chemistry-Inspired Diffusion with Non-Differentiable Guidance(https://arxiv.org/abs/2410.06502)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have shown remarkable potential in the conditional generation of novel molecules. These models can be guided in two ways: (i) explicitly, through additional features representing the condition, or (ii) implicitly, using a property predictor. However, training property predictors or conditional diffusion models requires an abundance of labeled data and is inherently challenging in real-world applications. We propose a novel approach that attenuates the limitations of acquiring large labeled datasets by leveraging domain knowledge from quantum chemistry as a non-differentiable oracle to guide an unconditional diffusion model. Instead of relying on neural networks, the oracle provides accurate guidance in the form of estimated gradients, allowing the diffusion process to sample from a conditional distribution specified by quantum chemistry. We show that this results in more precise conditional generation of novel and stable molecular structures. Our experiments demonstrate that our method: (1) significantly reduces atomic forces, enhancing the validity of generated molecules when used for stability optimization; (2) is compatible with both explicit and implicit guidance in diffusion models, enabling joint optimization of molecular properties and stability; and (3) generalizes effectively to molecular optimization tasks beyond stability optimization.</li>
</ul>

<h3>Title: PFAttack: Stealthy Attack Bypassing Group Fairness in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiashi Gao, Ziwei Wang, Xiangyu Zhao, Xin Yao, Xuetao Wei</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06509">https://arxiv.org/abs/2410.06509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06509">https://arxiv.org/pdf/2410.06509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06509]] PFAttack: Stealthy Attack Bypassing Group Fairness in Federated Learning(https://arxiv.org/abs/2410.06509)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal, federate, fair</a></li>
<li><strong>Abstract: </strong>Federated learning (FL), integrating group fairness mechanisms, allows multiple clients to collaboratively train a global model that makes unbiased decisions for different populations grouped by sensitive attributes (e.g., gender and race). Due to its distributed nature, previous studies have demonstrated that FL systems are vulnerable to model poisoning attacks. However, these studies primarily focus on perturbing accuracy, leaving a critical question unexplored: Can an attacker bypass the group fairness mechanisms in FL and manipulate the global model to be biased? The motivations for such an attack vary; an attacker might seek higher accuracy, yet fairness considerations typically limit the accuracy of the global model or aim to cause ethical disruption. To address this question, we design a novel form of attack in FL, termed Profit-driven Fairness Attack (PFATTACK), which aims not to degrade global model accuracy but to bypass fairness mechanisms. Our fundamental insight is that group fairness seeks to weaken the dependence of outputs on input attributes related to sensitive information. In the proposed PFATTACK, an attacker can recover this dependence through local fine-tuning across various sensitive groups, thereby creating a biased yet accuracy-preserving malicious model and injecting it into FL through model replacement. Compared to attacks targeting accuracy, PFATTACK is more stealthy. The malicious model in PFATTACK exhibits subtle parameter variations relative to the original global model, making it robust against detection and filtering by Byzantine-resilient aggregations. Extensive experiments on benchmark datasets are conducted for four fair FL frameworks and three Byzantine-resilient aggregations against model poisoning, demonstrating the effectiveness and stealth of PFATTACK in bypassing group fairness mechanisms in FL.</li>
</ul>

<h3>Title: TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training</h3>
<ul>
<li><strong>Authors: </strong>Wanchao Liang, Tianyu Liu, Less Wright, Will Constable, Andrew Gu, Chien-Chin Huang, Iris Zhang, Wei Feng, Howard Huang, Junjie Wang, Sanket Purandare, Gokul Nadathur, Stratos Idreos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06511">https://arxiv.org/abs/2410.06511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06511">https://arxiv.org/pdf/2410.06511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06511]] TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training(https://arxiv.org/abs/2410.06511)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The development of large language models (LLMs) has been instrumental in advancing state-of-the-art natural language processing applications. Training LLMs with billions of parameters and trillions of tokens require sophisticated distributed systems that enable composing and comparing several state-of-the-art techniques in order to efficiently scale across thousands of accelerators. However, existing solutions are complex, scattered across multiple libraries/repositories, lack interoperability, and are cumbersome to maintain. Thus, curating and empirically comparing training recipes require non-trivial engineering effort. This paper introduces TorchTitan, an open-source, PyTorch-native distributed training system that unifies state-of-the-art techniques, streamlining integration and reducing overhead. TorchTitan enables 3D parallelism in a modular manner with elastic scaling, providing comprehensive logging, checkpointing, and debugging tools for production-ready training. It also incorporates hardware-software co-designed solutions, leveraging features like Float8 training and SymmetricMemory. As a flexible test bed, TorchTitan facilitates custom recipe curation and comparison, allowing us to develop optimized training recipes for Llama 3.1 and provide guidance on selecting techniques for maximum efficiency based on our experiences. We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8 billion to 405 billion parameters, and showcase its exceptional performance, modular composability, and elastic scalability. By stacking training optimizations, we demonstrate accelerations of 65.08% with 1D parallelism at the 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at the 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at the 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized baselines.</li>
</ul>

<h3>Title: MORSE: An Efficient Homomorphic Secret Sharing Scheme Enabling Non-Linear Operation</h3>
<ul>
<li><strong>Authors: </strong>Weiquan Deng, Bowen Zhao, Yang Xiao, Yantao Zhong, Qingqi Pei, Ximeng Liu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06514">https://arxiv.org/abs/2410.06514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06514">https://arxiv.org/pdf/2410.06514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06514]] MORSE: An Efficient Homomorphic Secret Sharing Scheme Enabling Non-Linear Operation(https://arxiv.org/abs/2410.06514)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Homomorphic secret sharing (HSS) enables two servers to locally perform functions on encrypted data directly and obtain the results in the form of shares. A Paillier-based HSS solution seamlessly achieves multiplicative homomorphism and consumes less communication costs. Unfortunately, existing Paillier-based HSS schemes suffer from a large private key size, potential calculation error, expensive computation and storage overhead, and only valid on linear operations (e.g., addition and multiplication). To this end, inspired by the Paillier cryptosystem with fast encryption and decryption, we propose MORSE, an efficient homomorphic secret sharing scheme enabling non-linear operation, which enjoys a small key size, no calculation error and low overhead. In terms of functions, MORSE supports addition, subtraction, multiplication, scalar-multiplication, and comparison. Particularly, we carefully design two conversion protocols achieving the mutual conversion between one Paillier ciphertext and two secret shares, which allows MORSE to continuously perform the above operations. Rigorous analyses demonstrate that MORSE securely outputs correct results. Experimental results show that MORSE makes a runtime improvement of up to 9.3 times in terms of secure multiplication, and a communication costs reduction of up to 16.6% in secure comparison, compared to the state-of-the-art.</li>
</ul>

<h3>Title: SEGMENT+: Long Text Processing with Short-Context Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wei Shi, Shuang Li, Kerun Yu, Jinglei Chen, Zujie Liang, Xinhui Wu, Yuxi Qian, Feng Wei, Bo Zheng, Jiaqing Liang, Jiangjie Chen, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06519">https://arxiv.org/abs/2410.06519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06519">https://arxiv.org/pdf/2410.06519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06519]] SEGMENT+: Long Text Processing with Short-Context Language Models(https://arxiv.org/abs/2410.06519)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>There is a growing interest in expanding the input capacity of language models (LMs) across various domains. However, simply increasing the context window does not guarantee robust performance across diverse long-input processing tasks, such as understanding extensive documents and extracting detailed information from lengthy and noisy data. In response, we introduce SEGMENT+, a general framework that enables LMs to handle extended inputs within limited context windows efficiently. SEGMENT+ utilizes structured notes and a filtering module to manage information flow, resulting in a system that is both controllable and interpretable. Our extensive experiments across various model sizes, focusing on long-document question-answering and Needle-in-a-Haystack tasks, demonstrate the effectiveness of SEGMENT+ in improving performance.</li>
</ul>

<h3>Title: A Novel LLM-based Two-stage Summarization Approach for Long Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Yuan-Jhe Yin, Bo-Yu Chen, Berlin Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06520">https://arxiv.org/abs/2410.06520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06520">https://arxiv.org/pdf/2410.06520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06520]] A Novel LLM-based Two-stage Summarization Approach for Long Dialogues(https://arxiv.org/abs/2410.06520)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Long document summarization poses a significant challenge in natural language processing due to input lengths that exceed the capacity of most state-of-the-art pre-trained language models. This study proposes a hierarchical framework that segments and condenses information from long documents, subsequently fine-tuning the processed text with an abstractive summarization model. Unsupervised topic segmentation methods identify semantically appropriate breakpoints. The condensation stage utilizes an unsupervised generation model to generate condensed data, and our current experiments employ ChatGPT(v3.5). The summarization stage fine-tunes the abstractive summarization model on the condensed data to generate the final results. This framework enables long documents to be processed on models even when the document length exceeds the model's maximum input size. The exclusion of the entire document from the summarization model reduces the time and computational resources required for training, making the framework suitable for contexts with constrained local computational resources.</li>
</ul>

<h3>Title: On the Security of Bitstream-level JPEG Encryption with Restart Markers</h3>
<ul>
<li><strong>Authors: </strong>Mare Hirose, Shoko Imaizumi, Hitoshi Kiya</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06522">https://arxiv.org/abs/2410.06522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06522">https://arxiv.org/pdf/2410.06522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06522]] On the Security of Bitstream-level JPEG Encryption with Restart Markers(https://arxiv.org/abs/2410.06522)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>This paper aims to evaluate the security of a bitstream-level JPEG encryption method using restart (RST) markers, where encrypted image can keep the JPEG file format with the same file size as non-encrypted image. Data encrypted using this method can be decoded without altering header information by employing a standard JPEG decoder. Moreover, the use of RST markers enables the definition of extended blocks divided by the markers, so spatially partial encryption and block-permutation-based encryption can be carried out. However, the security of the method was evaluated only with respect to the key space analysis for brute-force attacks and other limited attacks. Accordingly, in this paper, we evaluated the security of the method with respect to robustness against ciphertext-only attacks including state-of-the-art attacks. In experiments, the method is compared with conventional encryption methods, and it is confirmed to be robust against ciphertext-only attacks if parameters used for image encryption are carefully chosen.</li>
</ul>

<h3>Title: Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA</h3>
<ul>
<li><strong>Authors: </strong>Maharshi Gor, Hal Daumé III, Tianyi Zhou, Jordan Boyd-Graber</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06524">https://arxiv.org/abs/2410.06524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06524">https://arxiv.org/pdf/2410.06524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06524]] Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA(https://arxiv.org/abs/2410.06524)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements of large language models (LLMs) have led to claims of AI surpassing humans in natural language processing (NLP) tasks such as textual understanding and reasoning. This work investigates these assertions by introducing CAIMIRA, a novel framework rooted in item response theory (IRT) that enables quantitative assessment and comparison of problem-solving abilities of question-answering (QA) agents: humans and AI systems. Through analysis of over 300,000 responses from ~70 AI systems and 155 humans across thousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in knowledge domains and reasoning skills. Humans outperform AI systems in knowledge-grounded abductive and conceptual reasoning, while state-of-the-art LLMs like GPT-4 and LLaMA show superior performance on targeted information retrieval and fact-based reasoning, particularly when information gaps are well-defined and addressable through pattern matching or data retrieval. These findings highlight the need for future QA tasks to focus on questions that challenge not only higher-order reasoning and scientific thinking, but also demand nuanced linguistic interpretation and cross-contextual knowledge application, helping advance AI developments that better emulate or complement human cognitive abilities in real-world problem-solving.</li>
</ul>

<h3>Title: TopoTune : A Framework for Generalized Combinatorial Complex Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Mathilde Papillon, Guillermo Bernárdez, Claudio Battiloro, Nina Miolane</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06530">https://arxiv.org/abs/2410.06530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06530">https://arxiv.org/pdf/2410.06530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06530]] TopoTune : A Framework for Generalized Combinatorial Complex Neural Networks(https://arxiv.org/abs/2410.06530)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) excel in learning from relational datasets, processing node and edge features in a way that preserves the symmetries of the graph domain. However, many complex systems--such as biological or social networks--involve multiway complex interactions that are more naturally represented by higher-order topological spaces. The emerging field of Topological Deep Learning (TDL) aims to accommodate and leverage these higher-order structures. Combinatorial Complex Neural Networks (CCNNs), fairly general TDL models, have been shown to be more expressive and better performing than GNNs. However, differently from the graph deep learning ecosystem, TDL lacks a principled and standardized framework for easily defining new architectures, restricting its accessibility and applicability. To address this issue, we introduce Generalized CCNNs (GCCNs), a novel simple yet powerful family of TDL models that can be used to systematically transform any (graph) neural network into its TDL counterpart. We prove that GCCNs generalize and subsume CCNNs, while extensive experiments on a diverse class of GCCNs show that these architectures consistently match or outperform CCNNs, often with less model complexity. In an effort to accelerate and democratize TDL, we introduce TopoTune, a lightweight software that allows practitioners to define, build, and train GCCNs with unprecedented flexibility and ease.</li>
</ul>

<h3>Title: Happy: A Debiased Learning Framework for Continual Generalized Category Discovery</h3>
<ul>
<li><strong>Authors: </strong>Shijie Ma, Fei Zhu, Zhun Zhong, Wenzhuo Liu, Xu-Yao Zhang, Cheng-Lin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06535">https://arxiv.org/abs/2410.06535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06535">https://arxiv.org/pdf/2410.06535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06535]] Happy: A Debiased Learning Framework for Continual Generalized Category Discovery(https://arxiv.org/abs/2410.06535)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Constantly discovering novel concepts is crucial in evolving environments. This paper explores the underexplored task of Continual Generalized Category Discovery (C-GCD), which aims to incrementally discover new classes from unlabeled data while maintaining the ability to recognize previously learned classes. Although several settings are proposed to study the C-GCD task, they have limitations that do not reflect real-world scenarios. We thus study a more practical C-GCD setting, which includes more new classes to be discovered over a longer period, without storing samples of past classes. In C-GCD, the model is initially trained on labeled data of known classes, followed by multiple incremental stages where the model is fed with unlabeled data containing both old and new classes. The core challenge involves two conflicting objectives: discover new classes and prevent forgetting old ones. We delve into the conflicts and identify that models are susceptible to prediction bias and hardness bias. To address these issues, we introduce a debiased learning framework namely Happy. For the prediction bias, we first introduce clustering-guided initialization to provide robust features. In addition, we propose soft entropy regularization to assign appropriate probabilities to new classes, which can significantly enhance the clustering performance of new classes. For the harness bias, we present the hardness-aware prototype sampling, which can effectively reduce the forgetting issue for previously seen classes, especially for difficult classes. Experimental results demonstrate our method proficiently manages the conflicts of C-GCD and achieves remarkable performance across various datasets, e.g., 7.5% overall gains on ImageNet-100. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Chip-Tuning: Classify Before Language Models Say</h3>
<ul>
<li><strong>Authors: </strong>Fangwei Zhu, Dian Li, Jiajun Huang, Gang Liu, Hui Wang, Zhifang Sui</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06541">https://arxiv.org/abs/2410.06541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06541">https://arxiv.org/pdf/2410.06541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06541]] Chip-Tuning: Classify Before Language Models Say(https://arxiv.org/abs/2410.06541)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid development in the performance of large language models (LLMs) is accompanied by the escalation of model size, leading to the increasing cost of model training and inference. Previous research has discovered that certain layers in LLMs exhibit redundancy, and removing these layers brings only marginal loss in model performance. In this paper, we adopt the probing technique to explain the layer redundancy in LLMs and demonstrate that language models can be effectively pruned with probing classifiers. We propose chip-tuning, a simple and effective structured pruning framework specialized for classification problems. Chip-tuning attaches tiny probing classifiers named chips to different layers of LLMs, and trains chips with the backbone model frozen. After selecting a chip for classification, all layers subsequent to the attached layer could be removed with marginal performance loss. Experimental results on various LLMs and datasets demonstrate that chip-tuning significantly outperforms previous state-of-the-art baselines in both accuracy and pruning ratio, achieving a pruning ratio of up to 50%. We also find that chip-tuning could be applied on multimodal models, and could be combined with model finetuning, proving its excellent compatibility.</li>
</ul>

<h3>Title: Gumbel Rao Monte Carlo based Bi-Modal Neural Architecture Search for Audio-Visual Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Aravinda Reddy PN, Raghavendra Ramachandra, Krothapalli Sreenivasa Rao, Pabitra Mitra Vinod Rathod</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06543">https://arxiv.org/abs/2410.06543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06543">https://arxiv.org/pdf/2410.06543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06543]] Gumbel Rao Monte Carlo based Bi-Modal Neural Architecture Search for Audio-Visual Deepfake Detection(https://arxiv.org/abs/2410.06543)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>Deepfakes pose a critical threat to biometric authentication systems by generating highly realistic synthetic media. Existing multimodal deepfake detectors often struggle to adapt to diverse data and rely on simple fusion methods. To address these challenges, we propose Gumbel-Rao Monte Carlo Bi-modal Neural Architecture Search (GRMC-BMNAS), a novel architecture search framework that employs Gumbel-Rao Monte Carlo sampling to optimize multimodal fusion. It refines the Straight through Gumbel Softmax (STGS) method by reducing variance with Rao-Blackwellization, stabilizing network training. Using a two-level search approach, the framework optimizes the network architecture, parameters, and performance. Crucial features are efficiently identified from backbone networks, while within the cell structure, a weighted fusion operation integrates information from various sources. By varying parameters such as temperature and number of Monte carlo samples yields an architecture that maximizes classification performance and better generalisation capability. Experimental results on the FakeAVCeleb and SWAN-DF datasets demonstrate an impressive AUC percentage of 95.4\%, achieved with minimal model parameters.</li>
</ul>

<h3>Title: Signal Watermark on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Xu, Victor S. Sheng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06545">https://arxiv.org/abs/2410.06545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06545">https://arxiv.org/pdf/2410.06545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06545]] Signal Watermark on Large Language Models(https://arxiv.org/abs/2410.06545)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) become increasingly sophisticated, they raise significant security concerns, including the creation of fake news and academic misuse. Most detectors for identifying model-generated text are limited by their reliance on variance in perplexity and burstiness, and they require substantial computational resources. In this paper, we proposed a watermarking method embedding a specific watermark into the text during its generation by LLMs, based on a pre-defined signal pattern. This technique not only ensures the watermark's invisibility to humans but also maintains the quality and grammatical integrity of model-generated text. We utilize LLMs and Fast Fourier Transform (FFT) for token probability computation and detection of the signal watermark. The unique application of signal processing principles within the realm of text generation by LLMs allows for subtle yet effective embedding of watermarks, which do not compromise the quality or coherence of the generated text. Our method has been empirically validated across multiple LLMs, consistently maintaining high detection accuracy, even with variations in temperature settings during text generation. In the experiment of distinguishing between human-written and watermarked text, our method achieved an AUROC score of 0.97, significantly outperforming existing methods like GPTZero, which scored 0.64. The watermark's resilience to various attacking scenarios further confirms its robustness, addressing significant challenges in model-generated text authentication.</li>
</ul>

<h3>Title: TuringQ: Benchmarking AI Comprehension in Theory of Computation</h3>
<ul>
<li><strong>Authors: </strong>Pardis Sadat Zahraei, Ehsaneddin Asgari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.FL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06547">https://arxiv.org/abs/2410.06547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06547">https://arxiv.org/pdf/2410.06547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06547]] TuringQ: Benchmarking AI Comprehension in Theory of Computation(https://arxiv.org/abs/2410.06547)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present TuringQ, the first benchmark designed to evaluate the reasoning capabilities of large language models (LLMs) in the theory of computation. TuringQ consists of 4,006 undergraduate and graduate-level question-answer pairs, categorized into four difficulty levels and covering seven core theoretical areas. We evaluate several open-source LLMs, as well as GPT-4, using Chain of Thought prompting and expert human assessment. Additionally, we propose an automated LLM-based evaluation system that demonstrates competitive accuracy when compared to human evaluation. Fine-tuning a Llama3-8B model on TuringQ shows measurable improvements in reasoning ability and out-of-domain tasks such as algebra. TuringQ serves as both a benchmark and a resource for enhancing LLM performance in complex computational reasoning tasks. Our analysis offers insights into LLM capabilities and advances in AI comprehension of theoretical computer science.</li>
</ul>

<h3>Title: DiffGAD: A Diffusion-based Unsupervised Graph Anomaly Detector</h3>
<ul>
<li><strong>Authors: </strong>Jinghan Li, Yuan Gao, Jinda Lu, Junfeng Fang, Congcong Wen, Hui Lin, Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06549">https://arxiv.org/abs/2410.06549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06549">https://arxiv.org/pdf/2410.06549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06549]] DiffGAD: A Diffusion-based Unsupervised Graph Anomaly Detector(https://arxiv.org/abs/2410.06549)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph Anomaly Detection (GAD) is crucial for identifying abnormal entities within networks, garnering significant attention across various fields. Traditional unsupervised methods, which decode encoded latent representations of unlabeled data with a reconstruction focus, often fail to capture critical discriminative content, leading to suboptimal anomaly detection. To address these challenges, we present a Diffusion-based Graph Anomaly Detector (DiffGAD). At the heart of DiffGAD is a novel latent space learning paradigm, meticulously designed to enhance its proficiency by guiding it with discriminative content. This innovative approach leverages diffusion sampling to infuse the latent space with discriminative content and introduces a content-preservation mechanism that retains valuable information across different scales, significantly improving its adeptness at identifying anomalies with limited time and space complexity. Our comprehensive evaluation of DiffGAD, conducted on six real-world and large-scale datasets with various metrics, demonstrated its exceptional performance.</li>
</ul>

<h3>Title: InstantIR: Blind Image Restoration with Instant Generative Reference</h3>
<ul>
<li><strong>Authors: </strong>Jen-Yuan Huang, Haofan Wang, Qixun Wang, Xu Bai, Hao Ai, Peng Xing, Jen-Tse Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06551">https://arxiv.org/abs/2410.06551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06551">https://arxiv.org/pdf/2410.06551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06551]] InstantIR: Blind Image Restoration with Instant Generative Reference(https://arxiv.org/abs/2410.06551)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Handling test-time unknown degradation is the major challenge in Blind Image Restoration (BIR), necessitating high model generalization. An effective strategy is to incorporate prior knowledge, either from human input or generative model. In this paper, we introduce Instant-reference Image Restoration (InstantIR), a novel diffusion-based BIR method which dynamically adjusts generation condition during inference. We first extract a compact representation of the input via a pre-trained vision encoder. At each generation step, this representation is used to decode current diffusion latent and instantiate it in the generative prior. The degraded image is then encoded with this reference, providing robust generation condition. We observe the variance of generative references fluctuate with degradation intensity, which we further leverage as an indicator for developing a sampling algorithm adaptive to input quality. Extensive experiments demonstrate InstantIR achieves state-of-the-art performance and offering outstanding visual quality. Through modulating generative references with textual description, InstantIR can restore extreme degradation and additionally feature creative restoration.</li>
</ul>

<h3>Title: ING-VP: MLLMs cannot Play Easy Vision-based Games Yet</h3>
<ul>
<li><strong>Authors: </strong>Haoran Zhang, Hangyu Guo, Shuyue Guo, Meng Cao, Wenhao Huang, Jiaheng Liu, Ge Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06555">https://arxiv.org/abs/2410.06555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06555">https://arxiv.org/pdf/2410.06555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06555]] ING-VP: MLLMs cannot Play Easy Vision-based Games Yet(https://arxiv.org/abs/2410.06555)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As multimodal large language models (MLLMs) continue to demonstrate increasingly competitive performance across a broad spectrum of tasks, more intricate and comprehensive benchmarks have been developed to assess these cutting-edge models. These benchmarks introduce new challenges to core capabilities such as perception, reasoning, and planning. However, existing multimodal benchmarks fall short in providing a focused evaluation of multi-step planning based on spatial relationships in images. To bridge this gap, we present ING-VP, the first INteractive Game-based Vision Planning benchmark, specifically designed to evaluate the spatial imagination and multi-step reasoning abilities of MLLMs. ING-VP features 6 distinct games, encompassing 300 levels, each with 6 unique configurations. A single model engages in over 60,000 rounds of interaction. The benchmark framework allows for multiple comparison settings, including image-text vs. text-only inputs, single-step vs. multi-step reasoning, and with-history vs. without-history conditions, offering valuable insights into the model's capabilities. We evaluated numerous state-of-the-art MLLMs, with the highest-performing model, Claude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the anticipated standard. This work aims to provide a specialized evaluation framework to drive advancements in MLLMs' capacity for complex spatial reasoning and planning. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: Deep Correlated Prompting for Visual Recognition with Missing Modalities</h3>
<ul>
<li><strong>Authors: </strong>Lianyu Hu, Tongkai Shi, Wei Feng, Fanhua Shang, Liang Wan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06558">https://arxiv.org/abs/2410.06558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06558">https://arxiv.org/pdf/2410.06558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06558]] Deep Correlated Prompting for Visual Recognition with Missing Modalities(https://arxiv.org/abs/2410.06558)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Large-scale multimodal models have shown excellent performance over a series of tasks powered by the large corpus of paired multimodal training data. Generally, they are always assumed to receive modality-complete inputs. However, this simple assumption may not always hold in the real world due to privacy constraints or collection difficulty, where models pretrained on modality-complete data easily demonstrate degraded performance on missing-modality cases. To handle this issue, we refer to prompt learning to adapt large pretrained multimodal models to handle missing-modality scenarios by regarding different missing cases as different types of input. Instead of only prepending independent prompts to the intermediate layers, we present to leverage the correlations between prompts and input features and excavate the relationships between different layers of prompts to carefully design the instructions. We also incorporate the complementary semantics of different modalities to guide the prompting design for each modality. Extensive experiments on three commonly-used datasets consistently demonstrate the superiority of our method compared to the previous approaches upon different missing scenarios. Plentiful ablations are further given to show the generalizability and reliability of our method upon different modality-missing ratios and types.</li>
</ul>

<h3>Title: Efficient and Robust Knowledge Distillation from A Stronger Teacher Based on Correlation Matching</h3>
<ul>
<li><strong>Authors: </strong>Wenqi Niu, Yingchao Wang, Guohui Cai, Hanpo Hou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06561">https://arxiv.org/abs/2410.06561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06561">https://arxiv.org/pdf/2410.06561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06561]] Efficient and Robust Knowledge Distillation from A Stronger Teacher Based on Correlation Matching(https://arxiv.org/abs/2410.06561)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Knowledge Distillation (KD) has emerged as a pivotal technique for neural network compression and performance enhancement. Most KD methods aim to transfer dark knowledge from a cumbersome teacher model to a lightweight student model based on Kullback-Leibler (KL) divergence loss. However, the student performance improvements achieved through KD exhibit diminishing marginal returns, where a stronger teacher model does not necessarily lead to a proportionally stronger student model. To address this issue, we empirically find that the KL-based KD method may implicitly change the inter-class relationships learned by the student model, resulting in a more complex and ambiguous decision boundary, which in turn reduces the model's accuracy and generalization ability. Therefore, this study argues that the student model should learn not only the probability values from the teacher's output but also the relative ranking of classes, and proposes a novel Correlation Matching Knowledge Distillation (CMKD) method that combines the Pearson and Spearman correlation coefficients-based KD loss to achieve more efficient and robust distillation from a stronger teacher model. Moreover, considering that samples vary in difficulty, CMKD dynamically adjusts the weights of the Pearson-based loss and Spearman-based loss. CMKD is simple yet practical, and extensive experiments demonstrate that it can consistently achieve state-of-the-art performance on CIRAR-100 and ImageNet, and adapts well to various teacher architectures, sizes, and other KD methods.</li>
</ul>

<h3>Title: Detecting Bias and Enhancing Diagnostic Accuracy in Large Language Models for Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Pardis Sadat Zahraei, Zahra Shakeri</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06566">https://arxiv.org/abs/2410.06566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06566">https://arxiv.org/pdf/2410.06566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06566]] Detecting Bias and Enhancing Diagnostic Accuracy in Large Language Models for Healthcare(https://arxiv.org/abs/2410.06566)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Biased AI-generated medical advice and misdiagnoses can jeopardize patient safety, making the integrity of AI in healthcare more critical than ever. As Large Language Models (LLMs) take on a growing role in medical decision-making, addressing their biases and enhancing their accuracy is key to delivering safe, reliable care. This study addresses these challenges head-on by introducing new resources designed to promote ethical and precise AI in healthcare. We present two datasets: BiasMD, featuring 6,007 question-answer pairs crafted to evaluate and mitigate biases in health-related LLM outputs, and DiseaseMatcher, with 32,000 clinical question-answer pairs spanning 700 diseases, aimed at assessing symptom-based diagnostic accuracy. Using these datasets, we developed the EthiClinician, a fine-tuned model built on the ChatDoctor framework, which outperforms GPT-4 in both ethical reasoning and clinical judgment. By exposing and correcting hidden biases in existing models for healthcare, our work sets a new benchmark for safer, more reliable patient outcomes.</li>
</ul>

<h3>Title: Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions</h3>
<ul>
<li><strong>Authors: </strong>Zhihao He, Hang Yu, Zi Gong, Shizhan Liu, Jianguo Li, Weiyao Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06577">https://arxiv.org/abs/2410.06577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06577">https://arxiv.org/pdf/2410.06577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06577]] Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions(https://arxiv.org/abs/2410.06577)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Transformer-based large language models (LLMs) have set new standards in natural language processing. However, the classical softmax attention incurs significant computational costs, leading to a $O(T)$ complexity for per-token generation, where $T$ represents the context length. This work explores reducing LLMs' complexity while maintaining performance by introducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an innovative data-dependent tempered selection (DDTS) mechanism within a linear attention-based, purely recurrent framework, achieving significant accuracy while drastically reducing the memory usage typically associated with recurrent models. This method exemplifies semantic compression by maintaining essential input information with fixed-size hidden states. Building on this, Rodimus$+$ combines Rodimus with the innovative Sliding Window Shared-Key Attention (SW-SKA) in a hybrid approach, effectively leveraging the complementary semantic, token, and head compression techniques. Our experiments demonstrate that Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior downstream performance against models trained on more tokens, including Qwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the accuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints will be available soon.</li>
</ul>

<h3>Title: Bots can Snoop: Uncovering and Mitigating Privacy Risks of Bots in Group Chats</h3>
<ul>
<li><strong>Authors: </strong>Kai-Hsiang Chou, Yi-Min Lin, Yi-An Wang, Jonathan Weiping Li, Tiffany Hyun-Jin Kim, Hsu-Chun Hsiao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06587">https://arxiv.org/abs/2410.06587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06587">https://arxiv.org/pdf/2410.06587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06587]] Bots can Snoop: Uncovering and Mitigating Privacy Risks of Bots in Group Chats(https://arxiv.org/abs/2410.06587)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, robust</a></li>
<li><strong>Abstract: </strong>New privacy concerns arise with chatbots on group messaging platforms. Chatbots may access information beyond their intended functionalities, such as messages unintended for chatbots or sender's identities. Chatbot operators may exploit such information to infer personal information and link users across groups, potentially leading to personal data breaches, pervasive tracking, and targeted advertising. Our analysis of conversation datasets shows that (1) chatbots often access far more messages than needed, and (2) when a user joins a new group with chatbots, there is a 3.4% chance that at least one of the chatbots can recognize and associate the user with their previous interactions in other groups. Although state-of-the-art group messaging protocols provide robust end-to-end security and some platforms have implemented policies to limit chatbot access, no platforms successfully combine these features. This paper introduces SnoopGuard, a secure group messaging protocol that ensures user privacy against chatbots while maintaining strong end-to-end security. Our method offers selective message access, preventing chatbots from accessing unrelated messages, and ensures sender anonymity within the group. SnoopGuard achieves $O(\log n + m)$ message-sending complexity for a group of $n$ users and $m$ chatbots, compared to $O(\log(n + m))$ in state-of-the-art protocols, with acceptable overhead for enhanced privacy. Our prototype implementation shows that sending a message in a group of 50 users and 10 chatbots takes about 30 milliseconds when integrated with Message Layer Security (MLS).</li>
</ul>

<h3>Title: Towards Natural Image Matting in the Wild via Real-Scenario Prior</h3>
<ul>
<li><strong>Authors: </strong>Ruihao Xia, Yu Liang, Peng-Tao Jiang, Hao Zhang, Qianru Sun, Yang Tang, Bo Li, Pan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06593">https://arxiv.org/abs/2410.06593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06593">https://arxiv.org/pdf/2410.06593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06593]] Towards Natural Image Matting in the Wild via Real-Scenario Prior(https://arxiv.org/abs/2410.06593)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Recent approaches attempt to adapt powerful interactive segmentation models, such as SAM, to interactive matting and fine-tune the models based on synthetic matting datasets. However, models trained on synthetic data fail to generalize to complex and occlusion scenes. We address this challenge by proposing a new matting dataset based on the COCO dataset, namely COCO-Matting. Specifically, the construction of our COCO-Matting includes accessory fusion and mask-to-matte, which selects real-world complex images from COCO and converts semantic segmentation masks to matting labels. The built COCO-Matting comprises an extensive collection of 38,251 human instance-level alpha mattes in complex natural scenarios. Furthermore, existing SAM-based matting methods extract intermediate features and masks from a frozen SAM and only train a lightweight matting decoder by end-to-end matting losses, which do not fully exploit the potential of the pre-trained SAM. Thus, we propose SEMat which revamps the network architecture and training objectives. For network architecture, the proposed feature-aligned transformer learns to extract fine-grained edge and transparency features. The proposed matte-aligned decoder aims to segment matting-specific objects and convert coarse masks into high-precision mattes. For training objectives, the proposed regularization and trimap loss aim to retain the prior from the pre-trained model and push the matting logits extracted from the mask decoder to contain trimap-based semantic information. Extensive experiments across seven diverse datasets demonstrate the superior performance of our method, proving its efficacy in interactive natural image matting. We open-source our code, models, and dataset at this https URL.</li>
</ul>

<h3>Title: DDRN:a Data Distribution Reconstruction Network for Occluded Person Re-Identification</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyong Wang, Yujie Liu, Mingyue Li, Wenxin Zhang, Zongmin Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06600">https://arxiv.org/abs/2410.06600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06600">https://arxiv.org/pdf/2410.06600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06600]] DDRN:a Data Distribution Reconstruction Network for Occluded Person Re-Identification(https://arxiv.org/abs/2410.06600)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In occluded person re-identification(ReID), severe occlusions lead to a significant amount of irrelevant information that hinders the accurate identification of individuals. These irrelevant cues primarily stem from background interference and occluding interference, adversely affecting the final retrieval results. Traditional discriminative models, which rely on the specific content and positions of the images, often misclassify in cases of occlusion. To address these limitations, we propose the Data Distribution Reconstruction Network (DDRN), a generative model that leverages data distribution to filter out irrelevant details, enhancing overall feature perception ability and reducing irrelevant feature interference. Additionally, severe occlusions lead to the complexity of the feature space. To effectively handle this, we design a multi-center approach through the proposed Hierarchical SubcenterArcface (HS-Arcface) loss function, which can better approximate complex feature spaces. On the Occluded-Duke dataset, we achieved a mAP of 62.4\% (+1.1\%) and a rank-1 accuracy of 71.3\% (+0.6\%), surpassing the latest state-of-the-art methods(FRT) significantly.</li>
</ul>

<h3>Title: Dissecting Fine-Tuning Unlearning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yihuai Hong, Yuelin Zou, Lijie Hu, Ziqian Zeng, Di Wang, Haiqin Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06606">https://arxiv.org/abs/2410.06606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06606">https://arxiv.org/pdf/2410.06606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06606]] Dissecting Fine-Tuning Unlearning in Large Language Models(https://arxiv.org/abs/2410.06606)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning-based unlearning methods prevail for preventing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of these methods is unclear. In this paper, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model's knowledge retrieval process, rather than genuinely erasing the problematic knowledge embedded in the model parameters. Furthermore, behavioral tests demonstrate that the unlearning mechanisms inevitably impact the global behavior of the models, affecting unrelated knowledge or capabilities. Our work advocates the development of more resilient unlearning techniques for truly erasing knowledge. Our code is released at this https URL.</li>
</ul>

<h3>Title: $\beta$-calibration of Language Model Confidence Scores for Generative QA</h3>
<ul>
<li><strong>Authors: </strong>Putra Manggala, Atalanti Mastakouri, Elke Kirschbaum, Shiva Prasad Kasiviswanathan, Aaditya Ramdas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06615">https://arxiv.org/abs/2410.06615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06615">https://arxiv.org/pdf/2410.06615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06615]] $\beta$-calibration of Language Model Confidence Scores for Generative QA(https://arxiv.org/abs/2410.06615)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim to ensure that the confidence score is on average indicative of the likelihood that the answer is correct. We argue, however, that this standard (average-case) notion of calibration is difficult to interpret for decision-making in generative QA. To address this, we generalize the standard notion of average calibration and introduce $\beta$-calibration, which ensures calibration holds across different question-and-answer groups. We then propose discretized posthoc calibration schemes for achieving $\beta$-calibration.</li>
</ul>

<h3>Title: Learning Evolving Tools for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guoxin Chen, Zhong Zhang, Xin Cong, Fangda Guo, Yesai Wu, Yankai Lin, Wenzheng Feng, Yasheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06617">https://arxiv.org/abs/2410.06617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06617">https://arxiv.org/pdf/2410.06617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06617]] Learning Evolving Tools for Large Language Models(https://arxiv.org/abs/2410.06617)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tool learning enables large language models (LLMs) to interact with external tools and APIs, greatly expanding the application scope of LLMs. However, due to the dynamic nature of external environments, these tools and APIs may become outdated over time, preventing LLMs from correctly invoking tools. Existing research primarily focuses on static environments and overlooks this issue, limiting the adaptability of LLMs in real-world applications. In this paper, we propose ToolEVO, a novel framework designed to enhance the adaptive and reflective capabilities of LLMs against tool variability. By leveraging Monte Carlo Tree Search, ToolEVO facilitates active exploration and interaction of LLMs within dynamic environments, allowing for autonomous self-reflection and self-updating of tool usage based on environmental feedback. Additionally, we introduce ToolQA-D, a benchmark specifically designed to evaluate the impact of tool variability. Extensive experiments demonstrate the effectiveness and stability of our approach, highlighting the importance of adaptability to tool variability for effective tool learning.</li>
</ul>

<h3>Title: Decomposing Relationship from 1-to-N into N 1-to-1 for Text-Video Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Jian Xiao, Zhenzhen Hu, Jia Li, Richang Hong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06618">https://arxiv.org/abs/2410.06618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06618">https://arxiv.org/pdf/2410.06618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06618]] Decomposing Relationship from 1-to-N into N 1-to-1 for Text-Video Retrieval(https://arxiv.org/abs/2410.06618)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text-video retrieval (TVR) has seen substantial advancements in recent years, fueled by the utilization of pre-trained models and large language models (LLMs). Despite these advancements, achieving accurate matching in TVR remains challenging due to inherent disparities between video and textual modalities and irregularities in data representation. In this paper, we propose Text-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the conventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships. By replacing a single text query with a series of text proxies, TV-ProxyNet not only broadens the query scope but also achieves a more precise expansion. Each text proxy is crafted through a refined iterative process, controlled by mechanisms we term as the director and dash, which regulate the proxy's direction and distance relative to the original text query. This setup not only facilitates more precise semantic alignment but also effectively manages the disparities and noise inherent in multimodal data. Our experiments on three representative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet Captions, demonstrate the effectiveness of TV-ProxyNet. The results show an improvement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved state-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0% improvement on DiDeMo compared to existing methods, validating our approach's ability to enhance semantic mapping and reduce error propensity.</li>
</ul>

<h3>Title: ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time</h3>
<ul>
<li><strong>Authors: </strong>Yi Ding, Bolian Li, Ruqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06625">https://arxiv.org/abs/2410.06625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06625">https://arxiv.org/pdf/2410.06625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06625]] ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time(https://arxiv.org/abs/2410.06625)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, generative</a></li>
<li><strong>Abstract: </strong>Vision Language Models (VLMs) have become essential backbones for multimodal intelligence, yet significant safety challenges limit their real-world application. While textual inputs are often effectively safeguarded, adversarial visual inputs can easily bypass VLM defense mechanisms. Existing defense methods are either resource-intensive, requiring substantial data and compute, or fail to simultaneously ensure safety and usefulness in responses. To address these limitations, we propose a novel two-phase inference-time alignment framework, Evaluating Then Aligning (ETA): 1) Evaluating input visual contents and output responses to establish a robust safety awareness in multimodal settings, and 2) Aligning unsafe behaviors at both shallow and deep levels by conditioning the VLMs' generative distribution with an interference prefix and performing sentence-level best-of-N to search the most harmless and helpful generation paths. Extensive experiments show that ETA outperforms baseline methods in terms of harmlessness, helpfulness, and efficiency, reducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6% win-ties in GPT-4 helpfulness evaluation. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: Open-RGBT: Open-vocabulary RGB-T Zero-shot Semantic Segmentation in Open-world Environments</h3>
<ul>
<li><strong>Authors: </strong>Meng Yu, Luojie Yang, Xunjie He, Yi Yang, Yufeng Yue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06626">https://arxiv.org/abs/2410.06626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06626">https://arxiv.org/pdf/2410.06626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06626]] Open-RGBT: Open-vocabulary RGB-T Zero-shot Semantic Segmentation in Open-world Environments(https://arxiv.org/abs/2410.06626)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation is a critical technique for effective scene understanding. Traditional RGB-T semantic segmentation models often struggle to generalize across diverse scenarios due to their reliance on pretrained models and predefined categories. Recent advancements in Visual Language Models (VLMs) have facilitated a shift from closed-set to open-vocabulary semantic segmentation methods. However, these models face challenges in dealing with intricate scenes, primarily due to the heterogeneity between RGB and thermal modalities. To address this gap, we present Open-RGBT, a novel open-vocabulary RGB-T semantic segmentation model. Specifically, we obtain instance-level detection proposals by incorporating visual prompts to enhance category understanding. Additionally, we employ the CLIP model to assess image-text similarity, which helps correct semantic consistency and mitigates ambiguities in category identification. Empirical evaluations demonstrate that Open-RGBT achieves superior performance in diverse and challenging real-world scenarios, even in the wild, significantly advancing the field of RGB-T semantic segmentation.</li>
</ul>

<h3>Title: Tree of Problems: Improving structured problem solving with compositionality</h3>
<ul>
<li><strong>Authors: </strong>Armel Zebaze, Benoît Sagot, Rachel Bawden</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06634">https://arxiv.org/abs/2410.06634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06634">https://arxiv.org/pdf/2410.06634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06634]] Tree of Problems: Improving structured problem solving with compositionality(https://arxiv.org/abs/2410.06634)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable performance across multiple tasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing the complex problem into paths of subproblems. In this paper, we propose Tree of Problems (ToP), a simpler version of ToT, which we hypothesise can work better for complex tasks that can be divided into identical subtasks. Our empirical results show that our approach outperforms ToT and GoT, and in addition performs better than CoT on complex reasoning tasks. All code for this paper is publicly available here: this https URL.</li>
</ul>

<h3>Title: Subtle Errors Matter: Preference Learning via Error-injected Self-editing</h3>
<ul>
<li><strong>Authors: </strong>Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Chak Tou Leong, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06638">https://arxiv.org/abs/2410.06638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06638">https://arxiv.org/pdf/2410.06638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06638]] Subtle Errors Matter: Preference Learning via Error-injected Self-editing(https://arxiv.org/abs/2410.06638)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have exhibited strong mathematical reasoning and computational prowess, tackling tasks ranging from basic arithmetic to advanced competition-level problems. However, frequently occurring subtle errors, such as miscalculations or incorrect substitutions, limit the models' full mathematical potential. Existing studies to improve mathematical ability typically involve distilling reasoning skills from stronger LLMs or applying preference learning to step-wise response pairs. Although these methods leverage samples of varying granularity to mitigate reasoning errors, they overlook the frequently occurring subtle errors. A major reason is that sampled preference pairs involve differences unrelated to the errors, which may distract the model from focusing on subtle errors. In this work, we propose a novel preference learning framework called eRror-Injected Self-Editing (RISE), which injects predefined subtle errors into partial tokens of correct solutions to construct hard pairs for error mitigation. In detail, RISE uses the model itself to edit a small number of tokens in the solution, injecting designed subtle errors. Then, pairs composed of self-edited solutions and their corresponding correct ones, along with pairs of correct and incorrect solutions obtained through sampling, are used together for subtle error-aware DPO training. Compared with other preference learning methods, RISE further refines the training objective to focus on predefined errors and their tokens, without requiring fine-grained sampling or preference annotation. Extensive experiments validate the effectiveness of RISE, with preference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0% on GSM8K and 7.9% on MATH.</li>
</ul>

<h3>Title: Q-WSL:Leveraging Dynamic Programming for Weighted Supervised Learning in Goal-conditioned RL</h3>
<ul>
<li><strong>Authors: </strong>Xing Lei, Xuetao Zhang, Zifeng Zhuang, Donglin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06648">https://arxiv.org/abs/2410.06648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06648">https://arxiv.org/pdf/2410.06648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06648]] Q-WSL:Leveraging Dynamic Programming for Weighted Supervised Learning in Goal-conditioned RL(https://arxiv.org/abs/2410.06648)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A novel class of advanced algorithms, termed Goal-Conditioned Weighted Supervised Learning (GCWSL), has recently emerged to tackle the challenges posed by sparse rewards in goal-conditioned reinforcement learning (RL). GCWSL consistently delivers strong performance across a diverse set of goal-reaching tasks due to its simplicity, effectiveness, and stability. However, GCWSL methods lack a crucial capability known as trajectory stitching, which is essential for learning optimal policies when faced with unseen skills during testing. This limitation becomes particularly pronounced when the replay buffer is predominantly filled with sub-optimal trajectories. In contrast, traditional TD-based RL methods, such as Q-learning, which utilize Dynamic Programming, do not face this issue but often experience instability due to the inherent difficulties in value function approximation. In this paper, we propose Q-learning Weighted Supervised Learning (Q-WSL), a novel framework designed to overcome the limitations of GCWSL by incorporating the strengths of Dynamic Programming found in Q-learning. Q-WSL leverages Dynamic Programming results to output the optimal action of (state, goal) pairs across different trajectories within the replay buffer. This approach synergizes the strengths of both Q-learning and GCWSL, effectively mitigating their respective weaknesses and enhancing overall performance. Empirical evaluations on challenging goal-reaching tasks demonstrate that Q-WSL surpasses other goal-conditioned approaches in terms of both performance and sample efficiency. Additionally, Q-WSL exhibits notable robustness in environments characterized by binary reward structures and environmental stochasticity.</li>
</ul>

<h3>Title: Decouple-Then-Merge: Towards Better Training for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Qianli Ma, Xuefei Ning, Dongrui Liu, Li Niu, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06664">https://arxiv.org/abs/2410.06664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06664">https://arxiv.org/pdf/2410.06664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06664]] Decouple-Then-Merge: Towards Better Training for Diffusion Models(https://arxiv.org/abs/2410.06664)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models are trained by learning a sequence of models that reverse each step of noise corruption. Typically, the model parameters are fully shared across multiple timesteps to enhance training efficiency. However, since the denoising tasks differ at each timestep, the gradients computed at different timesteps may conflict, potentially degrading the overall performance of image generation. To solve this issue, this work proposes a Decouple-then-Merge (DeMe) framework, which begins with a pretrained model and finetunes separate models tailored to specific timesteps. We introduce several improved techniques during the finetuning stage to promote effective knowledge sharing while minimizing training interference across timesteps. Finally, after finetuning, these separate models can be merged into a single model in the parameter space, ensuring efficient and practical inference. Experimental results show significant generation quality improvements upon 6 benchmarks including Stable Diffusion on COCO30K, ImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN Bedroom, and CIFAR10.</li>
</ul>

<h3>Title: Large Language Models as Code Executors: An Exploratory Study</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Lyu, Lecheng Yan, Rui Xing, Wenxi Li, Younes Samih, Tianbo Ji, Longyue Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06667">https://arxiv.org/abs/2410.06667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06667">https://arxiv.org/pdf/2410.06667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06667]] Large Language Models as Code Executors: An Exploratory Study(https://arxiv.org/abs/2410.06667)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The capabilities of Large Language Models (LLMs) have significantly evolved, extending from natural language processing to complex tasks like code understanding and generation. We expand the scope of LLMs' capabilities to a broader context, using LLMs to execute code snippets to obtain the output. This paper pioneers the exploration of LLMs as code executors, where code snippets are directly fed to the models for execution, and outputs are returned. We are the first to comprehensively examine this feasibility across various LLMs, including OpenAI's o1, GPT-4o, GPT-3.5, DeepSeek, and Qwen-Coder. Notably, the o1 model achieved over 90% accuracy in code execution, while others demonstrated lower accuracy levels. Furthermore, we introduce an Iterative Instruction Prompting (IIP) technique that processes code snippets line by line, enhancing the accuracy of weaker models by an average of 7.22% (with the highest improvement of 18.96%) and an absolute average improvement of 3.86% against CoT prompting (with the highest improvement of 19.46%). Our study not only highlights the transformative potential of LLMs in coding but also lays the groundwork for future advancements in automated programming and the completion of complex tasks.</li>
</ul>

<h3>Title: Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures</h3>
<ul>
<li><strong>Authors: </strong>Junxuan Wang, Xuyang Ge, Wentao Shu, Qiong Tang, Yunhua Zhou, Zhengfu He, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06672">https://arxiv.org/abs/2410.06672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06672">https://arxiv.org/pdf/2410.06672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06672]] Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures(https://arxiv.org/abs/2410.06672)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>The hypothesis of Universality in interpretability suggests that different neural networks may converge to implement similar algorithms on similar tasks. In this work, we investigate two mainstream architectures for language modeling, namely Transformers and Mambas, to explore the extent of their mechanistic similarity. We propose to use Sparse Autoencoders (SAEs) to isolate interpretable features from these models and show that most features are similar in these two models. We also validate the correlation between feature similarity and Universality. We then delve into the circuit-level analysis of Mamba models and find that the induction circuits in Mamba are structurally analogous to those in Transformers. We also identify a nuanced difference we call \emph{Off-by-One motif}: The information of one token is written into the SSM state in its next position. Whilst interaction between tokens in Transformers does not exhibit such trend.</li>
</ul>

<h3>Title: Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Changli Tang, Yixuan Li, Yudong Yang, Jimin Zhuang, Guangzhi Sun, Wei Li, Zujun Ma, Chao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06682">https://arxiv.org/abs/2410.06682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06682">https://arxiv.org/pdf/2410.06682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06682]] Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round Preference Optimization(https://arxiv.org/abs/2410.06682)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Videos contain a wealth of information, and generating detailed and accurate descriptions in natural language is a key aspect of video understanding. In this paper, we present video-SALMONN 2, an advanced audio-visual large language model (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with paired audio) captioning through directed preference optimization (DPO). We propose new metrics to evaluate the completeness and accuracy of video descriptions, which are optimized using DPO. To further improve training, we introduce a novel multi-round DPO (mrDPO) approach, which involves periodically updating the DPO reference model, merging and re-initializing the LoRA module as a proxy for parameter updates after each training round (1,000 steps), and incorporating guidance from ground-truth video captions to stabilize the process. To address potential catastrophic forgetting of non-captioning abilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO LLM by using the captions generated by the mrDPO-trained model as supervised labels. Experiments show that mrDPO significantly enhances video-SALMONN 2's captioning accuracy, reducing global and local error rates by 40\% and 20\%, respectively, while decreasing the repetition rate by 35\%. The final video-SALMONN 2 model, with just 7 billion parameters, surpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining competitive performance to the state-of-the-art on widely used video question-answering benchmark among models of similar size. Upon acceptance, we will release the code, model checkpoints, and training and test data. Demos are available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Perceptual Quality Assessment of Trisoup-Lifting Encoded 3D Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Juncheng Long, Honglei Su, Qi Liu, Hui Yuan, Wei Gao, Jiarun Song, Zhou Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06689">https://arxiv.org/abs/2410.06689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06689">https://arxiv.org/pdf/2410.06689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06689]] Perceptual Quality Assessment of Trisoup-Lifting Encoded 3D Point Clouds(https://arxiv.org/abs/2410.06689)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>No-reference bitstream-layer point cloud quality assessment (PCQA) can be deployed without full decoding at any network node to achieve real-time quality monitoring. In this work, we develop the first PCQA model dedicated to Trisoup-Lifting encoded 3D point clouds by analyzing bitstreams without full decoding. Specifically, we investigate the relationship among texture bitrate per point (TBPP), texture complexity (TC) and texture quantization parameter (TQP) while geometry encoding is lossless. Subsequently, we estimate TC by utilizing TQP and TBPP. Then, we establish a texture distortion evaluation model based on TC, TBPP and TQP. Ultimately, by integrating this texture distortion model with a geometry attenuation factor, a function of trisoupNodeSizeLog2 (tNSL), we acquire a comprehensive NR bitstream-layer PCQA model named streamPCQ-TL. In addition, this work establishes a database named WPC6.0, the first and largest PCQA database dedicated to Trisoup-Lifting encoding mode, encompassing 400 distorted point clouds with both 4 geometric multiplied by 5 texture distortion levels. Experiment results on M-PCCD, ICIP2020 and the proposed WPC6.0 database suggest that the proposed streamPCQ-TL model exhibits robust and notable performance in contrast to existing advanced PCQA metrics, particularly in terms of computational cost. The dataset and source code will be publicly released at \href{this https URL}{\textit{this https URL}}</li>
</ul>

<h3>Title: How hard can it be? Quantifying MITRE attack campaigns with attack trees and cATM logic</h3>
<ul>
<li><strong>Authors: </strong>Stefano M. Nicoletti, Milan Lopuhaä-Zwakenberg, Mariëlle Stoelinga, Fabio Massacci, Carlos E. Budde</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06692">https://arxiv.org/abs/2410.06692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06692">https://arxiv.org/pdf/2410.06692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06692]] How hard can it be? Quantifying MITRE attack campaigns with attack trees and cATM logic(https://arxiv.org/abs/2410.06692)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The landscape of cyber threats grows more complex by the day. Advanced Persistent Threats carry out systematic attack campaigns against which cybersecurity practitioners must defend. Examples of such organized attacks are operations Dream Job, Wocao, WannaCry or the SolarWinds Compromise. To evaluate which risks are most threatening, and which campaigns to prioritize against when defending, cybersecurity experts must be equipped with the right toolbox. In particular, they must be able to (a) obtain likelihood values for each attack campaign recorded in the wild and (b) reliably and transparently operationalize these values to carry out quantitative comparisons among campaigns. This will allow security experts to perform quantitatively-informed decision making that is transparent and accountable. In this paper we construct such a framework by: (1) quantifying the likelihood of attack campaigns via data-driven procedures on the MITRE knowledge base and (2) introducing a methodology for automatic modelling of MITRE intelligence data: this is complete in the sense that it captures any attack campaign via template attack tree models. (3) We further propose a computational framework to carry out this comparisons based on the cATM formal logic, and implement this into an open-source Python tool. Finally, we validate our approach by quantifying the likelihood of all MITRE campaigns, and comparing the likelihood of the Wocao and Dream Job MITRE campaigns -- generated with our proposed approach -- against "ad hoc" traditionally-built attack tree models, demonstrating how our methodology is substantially lighter in modelling effort, and still capable of capturing all the quantitative relevant data.</li>
</ul>

<h3>Title: Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens of Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yubo Wang, Chaohu Liu, Yanqiu Qu, Haoyu Cao, Deqiang Jiang, Linli Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06699">https://arxiv.org/abs/2410.06699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06699">https://arxiv.org/pdf/2410.06699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06699]] Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens of Large Vision-Language Models(https://arxiv.org/abs/2410.06699)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large vision-language models (LVLMs) integrate visual information into large language models, showcasing remarkable multi-modal conversational capabilities. However, the visual modules introduces new challenges in terms of robustness for LVLMs, as attackers can craft adversarial images that are visually clean but may mislead the model to generate incorrect answers. In general, LVLMs rely on vision encoders to transform images into visual tokens, which are crucial for the language models to perceive image contents effectively. Therefore, we are curious about one question: Can LVLMs still generate correct responses when the encoded visual tokens are attacked and disrupting the visual information? To this end, we propose a non-targeted attack method referred to as VT-Attack (Visual Tokens Attack), which constructs adversarial examples from multiple perspectives, with the goal of comprehensively disrupting feature representations and inherent relationships as well as the semantic properties of visual tokens output by image encoders. Using only access to the image encoder in the proposed attack, the generated adversarial examples exhibit transferability across diverse LVLMs utilizing the same image encoder and generality across different tasks. Extensive experiments validate the superior attack performance of the VT-Attack over baseline methods, demonstrating its effectiveness in attacking LVLMs with image encoders, which in turn can provide guidance on the robustness of LVLMs, particularly in terms of the stability of the visual feature space.</li>
</ul>

<h3>Title: PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Krishna Kanth Nakka, Ahmed Frikha, Ricardo Mendes, Xue Jiang, Xuebing Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06704">https://arxiv.org/abs/2410.06704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06704">https://arxiv.org/pdf/2410.06704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06704]] PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs(https://arxiv.org/abs/2410.06704)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, extraction</a></li>
<li><strong>Abstract: </strong>In this work, we introduce PII-Scope, a comprehensive benchmark designed to evaluate state-of-the-art methodologies for PII extraction attacks targeting LLMs across diverse threat settings. Our study provides a deeper understanding of these attacks by uncovering several hyperparameters (e.g., demonstration selection) crucial to their effectiveness. Building on this understanding, we extend our study to more realistic attack scenarios, exploring PII attacks that employ advanced adversarial strategies, including repeated and diverse querying, and leveraging iterative learning for continual PII extraction. Through extensive experimentation, our results reveal a notable underestimation of PII leakage in existing single-query attacks. In fact, we show that with sophisticated adversarial capabilities and a limited query budget, PII extraction rates can increase by up to fivefold when targeting the pretrained model. Moreover, we evaluate PII leakage on finetuned models, showing that they are more vulnerable to leakage than pretrained models. Overall, our work establishes a rigorous empirical benchmark for PII extraction attacks in realistic threat scenarios and provides a strong foundation for developing effective mitigation strategies.</li>
</ul>

<h3>Title: MERGE: Matching Electronic Results with Genuine Evidence for verifiable voting in person at remote locations</h3>
<ul>
<li><strong>Authors: </strong>Ben Adida, John Caron, Arash Mirzaei, Vanessa Teague</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06705">https://arxiv.org/abs/2410.06705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06705">https://arxiv.org/pdf/2410.06705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06705]] MERGE: Matching Electronic Results with Genuine Evidence for verifiable voting in person at remote locations(https://arxiv.org/abs/2410.06705)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Overseas military personnel often face significant challenges in participating in elections due to the slow pace of traditional mail systems, which can result in ballots missing crucial deadlines. While internet-based voting offers a faster alternative, it introduces serious risks to the integrity and privacy of the voting process. We introduce the MERGE protocol to address these issues by combining the speed of electronic ballot delivery with the reliability of paper returns. This protocol allows voters to submit an electronic record of their vote quickly while simultaneously mailing a paper ballot for verification. The electronic record can be used for preliminary results, but the paper ballot is used in a Risk Limiting Audit (RLA) if received in time, ensuring the integrity of the election. This approach extends the time window for ballot arrival without undermining the security and accuracy of the vote count.</li>
</ul>

<h3>Title: Calibrating Verbalized Probabilities for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Cheng Wang, Gyuri Szarvas, Georges Balazs, Pavel Danchenko, Patrick Ernst</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06707">https://arxiv.org/abs/2410.06707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06707">https://arxiv.org/pdf/2410.06707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06707]] Calibrating Verbalized Probabilities for Large Language Models(https://arxiv.org/abs/2410.06707)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Calibrating verbalized probabilities presents a novel approach for reliably assessing and leveraging outputs from black-box Large Language Models (LLMs). Recent methods have demonstrated improved calibration by applying techniques like Platt scaling or temperature scaling to the confidence scores generated by LLMs. In this paper, we explore the calibration of verbalized probability distributions for discriminative tasks. First, we investigate the capability of LLMs to generate probability distributions over categorical labels. We theoretically and empirically identify the issue of re-softmax arising from the scaling of verbalized probabilities, and propose using the invert softmax trick to approximate the "logit" by inverting verbalized probabilities. Through extensive evaluation on three public datasets, we demonstrate: (1) the robust capability of LLMs in generating class distributions, and (2) the effectiveness of the invert softmax trick in estimating logits, which, in turn, facilitates post-calibration adjustments.</li>
</ul>

<h3>Title: Guaranteed Generation from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Minbeom Kim, Thibaut Thonet, Jos Rozen, Hwaran Lee, Kyomin Jung, Marc Dymetman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06716">https://arxiv.org/abs/2410.06716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06716">https://arxiv.org/pdf/2410.06716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06716]] Guaranteed Generation from Large Language Models(https://arxiv.org/abs/2410.06716)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are increasingly used across various applications, there is a growing need to control text generation to satisfy specific constraints or requirements. This raises a crucial question: Is it possible to guarantee strict constraint satisfaction in generated outputs while preserving the distribution of the original model as much as possible? We first define the ideal distribution - the one closest to the original model, which also always satisfies the expressed constraint - as the ultimate goal of guaranteed generation. We then state a fundamental limitation, namely that it is impossible to reach that goal through autoregressive training alone. This motivates the necessity of combining training-time and inference-time methods to enforce such guarantees. Based on this insight, we propose GUARD, a simple yet effective approach that combines an autoregressive proposal distribution with rejection sampling. Through GUARD's theoretical properties, we show how controlling the KL divergence between a specific proposal and the target ideal distribution simultaneously optimizes inference speed and distributional closeness. To validate these theoretical concepts, we conduct extensive experiments on two text generation settings with hard-to-satisfy constraints: a lexical constraint scenario and a sentiment reversal scenario. These experiments show that GUARD achieves perfect constraint satisfaction while almost preserving the ideal distribution with highly improved inference efficiency. GUARD provides a principled approach to enforcing strict guarantees for LLMs without compromising their generative capabilities.</li>
</ul>

<h3>Title: MatMamba: A Matryoshka State Space Model</h3>
<ul>
<li><strong>Authors: </strong>Abhinav Shukla, Sai Vemprala, Aditya Kusupati, Ashish Kapoor</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06718">https://arxiv.org/abs/2410.06718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06718">https://arxiv.org/pdf/2410.06718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06718]] MatMamba: A Matryoshka State Space Model(https://arxiv.org/abs/2410.06718)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>State Space Models (SSMs) like Mamba2 are a promising alternative to Transformers, with faster theoretical training and inference times -- especially for long context lengths. Recent work on Matryoshka Representation Learning -- and its application to Transformer backbones in works like MatFormer -- showed how to introduce nested granularities of smaller submodels in one universal elastic model. In this work, we present MatMamba: a state space model which combines Matryoshka-style learning with Mamba2, by modifying the block to contain nested dimensions to enable joint training and adaptive inference. MatMamba allows for efficient and adaptive deployment across various model sizes. We train a single large MatMamba model and are able to get a number of smaller nested models for free -- while maintaining or improving upon the performance of a baseline smaller model trained from scratch. We train language and image models at a variety of parameter sizes from 35M to 1.4B. Our results on ImageNet and FineWeb show that MatMamba models scale comparably to Transformers, while having more efficient inference characteristics. This makes MatMamba a practically viable option for deploying large-scale models in an elastic way based on the available inference compute. Code and models are open sourced at \url{this https URL}</li>
</ul>

<h3>Title: Suppress Content Shift: Better Diffusion Features via Off-the-Shelf Generation Techniques</h3>
<ul>
<li><strong>Authors: </strong>Benyuan Meng, Qianqian Xu, Zitai Wang, Zhiyong Yang, Xiaochun Cao, Qingming Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06719">https://arxiv.org/abs/2410.06719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06719">https://arxiv.org/pdf/2410.06719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06719]] Suppress Content Shift: Better Diffusion Features via Off-the-Shelf Generation Techniques(https://arxiv.org/abs/2410.06719)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are powerful generative models, and this capability can also be applied to discrimination. The inner activations of a pre-trained diffusion model can serve as features for discriminative tasks, namely, diffusion feature. We discover that diffusion feature has been hindered by a hidden yet universal phenomenon that we call content shift. To be specific, there are content differences between features and the input image, such as the exact shape of a certain object. We locate the cause of content shift as one inherent characteristic of diffusion models, which suggests the broad existence of this phenomenon in diffusion feature. Further empirical study also indicates that its negative impact is not negligible even when content shift is not visually perceivable. Hence, we propose to suppress content shift to enhance the overall quality of diffusion features. Specifically, content shift is related to the information drift during the process of recovering an image from the noisy input, pointing out the possibility of turning off-the-shelf generation techniques into tools for content shift suppression. We further propose a practical guideline named GATE to efficiently evaluate the potential benefit of a technique and provide an implementation of our methodology. Despite the simplicity, the proposed approach has achieved superior results on various tasks and datasets, validating its potential as a generic booster for diffusion features. Our code is available at this https URL.</li>
</ul>

<h3>Title: Scaling Laws for Mixed quantization in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Cao, Cheng Zhang, Pedro Gimenes, Jianqiao Lu, Jianyi Cheng, Yiren Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06722">https://arxiv.org/abs/2410.06722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06722">https://arxiv.org/pdf/2410.06722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06722]] Scaling Laws for Mixed quantization in Large Language Models(https://arxiv.org/abs/2410.06722)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Post-training quantization of Large Language Models (LLMs) has proven effective in reducing the computational requirements for running inference on these models. In this study, we focus on a straightforward question: When aiming for a specific accuracy or perplexity target for low-precision quantization, how many high-precision numbers or calculations are required to preserve as we scale LLMs to larger sizes? We first introduce a critical metric named the quantization ratio, which compares the number of parameters quantized to low-precision arithmetic against the total parameter count. Through extensive and carefully controlled experiments across different model families, arithmetic types, and quantization granularities (e.g. layer-wise, matmul-wise), we identify two central phenomenons. 1) The larger the models, the better they can preserve performance with an increased quantization ratio, as measured by perplexity in pre-training tasks or accuracy in downstream tasks. 2) The finer the granularity of mixed-precision quantization (e.g., matmul-wise), the more the model can increase the quantization ratio. We believe these observed phenomena offer valuable insights for future AI hardware design and the development of advanced Efficient AI algorithms.</li>
</ul>

<h3>Title: Evaluating the Impact of Point Cloud Colorization on Semantic Segmentation Accuracy</h3>
<ul>
<li><strong>Authors: </strong>Qinfeng Zhu, Jiaze Cao, Yuanzhi Cai, Lei Fan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06725">https://arxiv.org/abs/2410.06725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06725">https://arxiv.org/pdf/2410.06725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06725]] Evaluating the Impact of Point Cloud Colorization on Semantic Segmentation Accuracy(https://arxiv.org/abs/2410.06725)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Point cloud semantic segmentation, the process of classifying each point into predefined categories, is essential for 3D scene understanding. While image-based segmentation is widely adopted due to its maturity, methods relying solely on RGB information often suffer from degraded performance due to color inaccuracies. Recent advancements have incorporated additional features such as intensity and geometric information, yet RGB channels continue to negatively impact segmentation accuracy when errors in colorization occur. Despite this, previous studies have not rigorously quantified the effects of erroneous colorization on segmentation performance. In this paper, we propose a novel statistical approach to evaluate the impact of inaccurate RGB information on image-based point cloud segmentation. We categorize RGB inaccuracies into two types: incorrect color information and similar color information. Our results demonstrate that both types of color inaccuracies significantly degrade segmentation accuracy, with similar color errors particularly affecting the extraction of geometric features. These findings highlight the critical need to reassess the role of RGB information in point cloud segmentation and its implications for future algorithm design.</li>
</ul>

<h3>Title: Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles</h3>
<ul>
<li><strong>Authors: </strong>Qi Chen, Bowen Zhang, Gang Wang, Qi Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06733">https://arxiv.org/abs/2410.06733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06733">https://arxiv.org/pdf/2410.06733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06733]] Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles(https://arxiv.org/abs/2410.06733)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>While advancements in NLP have significantly improved the performance of Large Language Models (LLMs) on tasks requiring vertical thinking, their lateral thinking capabilities remain under-explored and challenging to measure due to the complexity of assessing creative thought processes and the scarcity of relevant data. To address these challenges, we introduce SPLAT, a benchmark leveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs. This benchmark, containing 975 graded situation puzzles across three difficulty levels, employs a new multi-turn player-judge framework instead of the traditional model-based evaluation, which often necessitates a stronger evaluation model. This framework simulates an interactive game where the model (player) asks the evaluation model (judge) questions about an incomplete story to infer the full scenario. The judge answers based on a detailed reference scenario or evaluates if the player's predictions align with the reference one. This approach lessens dependence on more robust evaluation models, enabling the assessment of state-of-the-art LLMs. The experiments demonstrate that a robust evaluation model, such as WizardLM-2, closely matches human judgements in both intermediate question-answering and final scenario accuracy, achieving over 80% agreement-similar to the agreement levels among humans. Furthermore, applying data and reasoning processes from our benchmark to other lateral thinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to performance enhancements. This suggests that our benchmark effectively evaluates and elicits the lateral thinking abilities of LLMs. Code is available at: this https URL.</li>
</ul>

<h3>Title: MimicTalk: Mimicking a personalized and expressive 3D talking face in minutes</h3>
<ul>
<li><strong>Authors: </strong>Zhenhui Ye, Tianyun Zhong, Yi Ren, Ziyue Jiang, Jiawei Huang, Rongjie Huang, Jinglin Liu, Jinzheng He, Chen Zhang, Zehan Wang, Xize Chen, Xiang Yin, Zhou Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06734">https://arxiv.org/abs/2410.06734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06734">https://arxiv.org/pdf/2410.06734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06734]] MimicTalk: Mimicking a personalized and expressive 3D talking face in minutes(https://arxiv.org/abs/2410.06734)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Talking face generation (TFG) aims to animate a target identity's face to create realistic talking videos. Personalized TFG is a variant that emphasizes the perceptual identity similarity of the synthesized result (from the perspective of appearance and talking style). While previous works typically solve this problem by learning an individual neural radiance field (NeRF) for each identity to implicitly store its static and dynamic information, we find it inefficient and non-generalized due to the per-identity-per-training framework and the limited training data. To this end, we propose MimicTalk, the first attempt that exploits the rich knowledge from a NeRF-based person-agnostic generic model for improving the efficiency and robustness of personalized TFG. To be specific, (1) we first come up with a person-agnostic 3D TFG model as the base model and propose to adapt it into a specific identity; (2) we propose a static-dynamic-hybrid adaptation pipeline to help the model learn the personalized static appearance and facial dynamic features; (3) To generate the facial motion of the personalized talking style, we propose an in-context stylized audio-to-motion model that mimics the implicit talking style provided in the reference video without information loss by an explicit style representation. The adaptation process to an unseen identity can be performed in 15 minutes, which is 47 times faster than previous person-dependent methods. Experiments show that our MimicTalk surpasses previous baselines regarding video quality, efficiency, and expressiveness. Source code and video samples are available at this https URL .</li>
</ul>

<h3>Title: Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?</h3>
<ul>
<li><strong>Authors: </strong>Fumiya Uchiyama, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06735">https://arxiv.org/abs/2410.06735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06735">https://arxiv.org/pdf/2410.06735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06735]] Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?(https://arxiv.org/abs/2410.06735)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent large language models (LLMs) have demonstrated remarkable generalization abilities in mathematics and logical reasoning tasks. Prior research indicates that LLMs pre-trained with programming language data exhibit high mathematical and reasoning abilities; however, this causal relationship has not been rigorously tested. Our research aims to verify which programming languages and features during pre-training affect logical inference performance. Specifically, we pre-trained decoder-based language models from scratch using datasets from ten programming languages (e.g., Python, C, Java) and three natural language datasets (Wikipedia, Fineweb, C4) under identical conditions. Thereafter, we evaluated the trained models in a few-shot in-context learning setting on logical reasoning tasks: FLD and bAbi, which do not require commonsense or world knowledge. The results demonstrate that nearly all models trained with programming languages consistently outperform those trained with natural languages, indicating that programming languages contain factors that elicit logic inference performance. In addition, we found that models trained with programming languages exhibit a better ability to follow instructions compared to those trained with natural languages. Further analysis reveals that the depth of Abstract Syntax Trees representing parsed results of programs also affects logical reasoning performance. These findings will offer insights into the essential elements of pre-training for acquiring the foundational abilities of LLMs.</li>
</ul>

<h3>Title: CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zi Gong, Hang Yu, Cong Liao, Bingchang Liu, Chaoyu Chen, Jianguo Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06741">https://arxiv.org/abs/2410.06741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06741">https://arxiv.org/pdf/2410.06741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06741]] CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models(https://arxiv.org/abs/2410.06741)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-task learning (MTL) benefits the fine-tuning of large language models (LLMs) by providing a single model with improved performance and generalization ability across tasks, presenting a resource-efficient alternative to developing separate models for each task. Yet, existing MTL strategies for LLMs often fall short by either being computationally intensive or failing to ensure simultaneous task convergence. This paper presents CoBa, a new MTL approach designed to effectively manage task convergence balance with minimal computational overhead. Utilizing Relative Convergence Scores (RCS), Absolute Convergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically adjusts task weights during the training process, ensuring that the validation loss of all tasks progress towards convergence at an even pace while mitigating the issue of individual task divergence. The results of our experiments involving three disparate datasets underscore that this approach not only fosters equilibrium in task improvement but enhances the LLMs' performance by up to 13% relative to the second-best baselines. Code is open-sourced at this https URL.</li>
</ul>

<h3>Title: Cluster-wise Graph Transformer with Dual-granularity Kernelized Attention</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Huang, Yunchong Song, Jiayue Zhou, Zhouhan Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06746">https://arxiv.org/abs/2410.06746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06746">https://arxiv.org/pdf/2410.06746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06746]] Cluster-wise Graph Transformer with Dual-granularity Kernelized Attention(https://arxiv.org/abs/2410.06746)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In the realm of graph learning, there is a category of methods that conceptualize graphs as hierarchical structures, utilizing node clustering to capture broader structural information. While generally effective, these methods often rely on a fixed graph coarsening routine, leading to overly homogeneous cluster representations and loss of node-level information. In this paper, we envision the graph as a network of interconnected node sets without compressing each cluster into a single embedding. To enable effective information transfer among these node sets, we propose the Node-to-Cluster Attention (N2C-Attn) mechanism. N2C-Attn incorporates techniques from Multiple Kernel Learning into the kernelized attention framework, effectively capturing information at both node and cluster levels. We then devise an efficient form for N2C-Attn using the cluster-wise message-passing framework, achieving linear time complexity. We further analyze how N2C-Attn combines bi-level feature maps of queries and keys, demonstrating its capability to merge dual-granularity information. The resulting architecture, Cluster-wise Graph Transformer (Cluster-GT), which uses node clusters as tokens and employs our proposed N2C-Attn module, shows superior performance on various graph-level tasks. Code is available at this https URL.</li>
</ul>

<h3>Title: DreamMesh4D: Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh Hybrid Representation</h3>
<ul>
<li><strong>Authors: </strong>Zhiqi Li, Yiming Chen, Peidong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06756">https://arxiv.org/abs/2410.06756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06756">https://arxiv.org/pdf/2410.06756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06756]] DreamMesh4D: Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh Hybrid Representation(https://arxiv.org/abs/2410.06756)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in 2D/3D generative techniques have facilitated the generation of dynamic 3D objects from monocular videos. Previous methods mainly rely on the implicit neural radiance fields (NeRF) or explicit Gaussian Splatting as the underlying representation, and struggle to achieve satisfactory spatial-temporal consistency and surface appearance. Drawing inspiration from modern 3D animation pipelines, we introduce DreamMesh4D, a novel framework combining mesh representation with geometric skinning technique to generate high-quality 4D object from a monocular video. Instead of utilizing classical texture map for appearance, we bind Gaussian splats to triangle face of mesh for differentiable optimization of both the texture and mesh vertices. In particular, DreamMesh4D begins with a coarse mesh obtained through an image-to-3D generation procedure. Sparse points are then uniformly sampled across the mesh surface, and are used to build a deformation graph to drive the motion of the 3D object for the sake of computational efficiency and providing additional constraint. For each step, transformations of sparse control points are predicted using a deformation network, and the mesh vertices as well as the surface Gaussians are deformed via a novel geometric skinning algorithm, which is a hybrid approach combining LBS (linear blending skinning) and DQS (dual-quaternion skinning), mitigating drawbacks associated with both approaches. The static surface Gaussians and mesh vertices as well as the deformation network are learned via reference view photometric loss, score distillation loss as well as other regularizers in a two-stage manner. Extensive experiments demonstrate superior performance of our method. Furthermore, our method is compatible with modern graphic pipelines, showcasing its potential in the 3D gaming and film industry.</li>
</ul>

<h3>Title: To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Junyan Lin, Haoran Chen, Dawei Zhu, Xiaoyu Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06765">https://arxiv.org/abs/2410.06765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06765">https://arxiv.org/pdf/2410.06765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06765]] To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models(https://arxiv.org/abs/2410.06765)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, multimodal large language models (MLLMs) have garnered significant attention from both industry and academia. However, there is still considerable debate on constructing MLLM architectures, particularly regarding the selection of appropriate connectors for perception tasks of varying granularities. This paper systematically investigates the impact of connectors on MLLM performance. Specifically, we classify connectors into feature-preserving and feature-compressing types. Utilizing a unified classification standard, we categorize sub-tasks from three comprehensive benchmarks, MMBench, MME, and SEED-Bench, into three task types: coarse-grained perception, fine-grained perception, and reasoning, and evaluate the performance. Our findings reveal that feature-preserving connectors excel in \emph{fine-grained perception} tasks due to their ability to retain detailed visual information. In contrast, feature-compressing connectors, while less effective in fine-grained perception tasks, offer significant speed advantages and perform comparably in \emph{coarse-grained perception} and \emph{reasoning} tasks. These insights are crucial for guiding MLLM architecture design and advancing the optimization of MLLM architectures.</li>
</ul>

<h3>Title: HERM: Benchmarking and Enhancing Multimodal LLMs for Human-Centric Understanding</h3>
<ul>
<li><strong>Authors: </strong>Keliang Li, Zaifei Yang, Jiahe Zhao, Hongze Shen, Ruibing Hou, Hong Chang, Shiguang Shan, Xilin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06777">https://arxiv.org/abs/2410.06777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06777">https://arxiv.org/pdf/2410.06777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06777]] HERM: Benchmarking and Enhancing Multimodal LLMs for Human-Centric Understanding(https://arxiv.org/abs/2410.06777)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The significant advancements in visual understanding and instruction following from Multimodal Large Language Models (MLLMs) have opened up more possibilities for broader applications in diverse and universal human-centric scenarios. However, existing image-text data may not support the precise modality alignment and integration of multi-grained information, which is crucial for human-centric visual understanding. In this paper, we introduce HERM-Bench, a benchmark for evaluating the human-centric understanding capabilities of MLLMs. Our work reveals the limitations of existing MLLMs in understanding complex human-centric scenarios. To address these challenges, we present HERM-100K, a comprehensive dataset with multi-level human-centric annotations, aimed at enhancing MLLMs' training. Furthermore, we develop HERM-7B, a MLLM that leverages enhanced training data from HERM-100K. Evaluations on HERM-Bench demonstrate that HERM-7B significantly outperforms existing MLLMs across various human-centric dimensions, reflecting the current inadequacy of data annotations used in MLLM training for human-centric visual understanding. This research emphasizes the importance of specialized datasets and benchmarks in advancing the MLLMs' capabilities for human-centric understanding.</li>
</ul>

<h3>Title: Mind Your Questions Towards Backdoor Attacks on Text-to-Visualization Models</h3>
<ul>
<li><strong>Authors: </strong>Shuaimin Li, Yuanfeng Song, Xuanang Chen, Anni Peng, Zhuoyue Wan, Chen Jason Zhang, Raymond Chi-Wing Wong</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06782">https://arxiv.org/abs/2410.06782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06782">https://arxiv.org/pdf/2410.06782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06782]] Mind Your Questions Towards Backdoor Attacks on Text-to-Visualization Models(https://arxiv.org/abs/2410.06782)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Text-to-visualization (text-to-vis) models have become valuable tools in the era of big data, enabling users to generate data visualizations and make informed decisions through natural language queries (NLQs). Despite their widespread application, the security vulnerabilities of these models have been largely overlooked. To address this gap, we propose VisPoison, a novel framework designed to identify these vulnerabilities of current text-to-vis models systematically. VisPoison introduces two types of triggers that activate three distinct backdoor attacks, potentially leading to data exposure, misleading visualizations, or denial-of-service (DoS) incidents. The framework features both proactive and passive attack mechanisms: proactive attacks leverage rare-word triggers to access confidential data, while passive attacks, triggered unintentionally by users, exploit a first-word trigger method, causing errors or DoS events in visualizations. Through extensive experiments on both trainable and in-context learning (ICL)-based text-to-vis models, \textit{VisPoison} achieves attack success rates of over 90\%, highlighting the security problem of current text-to-vis models. Additionally, we explore two types of defense mechanisms against these attacks, but the results show that existing countermeasures are insufficient, underscoring the pressing need for more robust security solutions in text-to-vis systems.</li>
</ul>

<h3>Title: From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuying Shang, Xinyi Zeng, Yutao Zhu, Xiao Yang, Zhengwei Fang, Jingyuan Zhang, Jiawei Chen, Zinan Liu, Yu Tian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06795">https://arxiv.org/abs/2410.06795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06795">https://arxiv.org/pdf/2410.06795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06795]] From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models(https://arxiv.org/abs/2410.06795)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Hallucinations in large vision-language models (LVLMs) are a significant challenge, i.e., generating objects that are not presented in the visual input, which impairs their reliability. Recent studies often attribute hallucinations to a lack of understanding of visual input, yet ignore a more fundamental issue: the model's inability to effectively extract or decouple visual features. In this paper, we revisit the hallucinations in LVLMs from an architectural perspective, investigating whether the primary cause lies in the visual encoder (feature extraction) or the modal alignment module (feature decoupling). Motivated by our findings on the preliminary investigation, we propose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs. This plug-and-play method can be integrated into various LVLMs, utilizing adaptive virtual tokens to extract object features from bounding boxes, thereby addressing hallucinations caused by insufficient decoupling of visual features. PATCH achieves state-of-the-art performance on multiple multi-modal hallucination datasets. We hope this approach provides researchers with deeper insights into the underlying causes of hallucinations in LVLMs, fostering further advancements and innovation in this field.</li>
</ul>

<h3>Title: Diffuse or Confuse: A Diffusion Deepfake Speech Dataset</h3>
<ul>
<li><strong>Authors: </strong>Anton Firc, Kamil Malinka, Petr Hanáček</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06796">https://arxiv.org/abs/2410.06796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06796">https://arxiv.org/pdf/2410.06796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06796]] Diffuse or Confuse: A Diffusion Deepfake Speech Dataset(https://arxiv.org/abs/2410.06796)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Advancements in artificial intelligence and machine learning have significantly improved synthetic speech generation. This paper explores diffusion models, a novel method for creating realistic synthetic speech. We create a diffusion dataset using available tools and pretrained models. Additionally, this study assesses the quality of diffusion-generated deepfakes versus non-diffusion ones and their potential threat to current deepfake detection systems. Findings indicate that the detection of diffusion-based deepfakes is generally comparable to non-diffusion deepfakes, with some variability based on detector architecture. Re-vocoding with diffusion vocoders shows minimal impact, and the overall speech quality is comparable to non-diffusion methods.</li>
</ul>

<h3>Title: Seg2Act: Global Context-aware Action Generation for Document Logical Structuring</h3>
<ul>
<li><strong>Authors: </strong>Zichao Li, Shaojie He, Meng Liao, Xuanang Chen, Yaojie Lu, Hongyu Lin, Yanxiong Lu, Xianpei Han, Le Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06802">https://arxiv.org/abs/2410.06802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06802">https://arxiv.org/pdf/2410.06802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06802]] Seg2Act: Global Context-aware Action Generation for Document Logical Structuring(https://arxiv.org/abs/2410.06802)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>Document logical structuring aims to extract the underlying hierarchical structure of documents, which is crucial for document intelligence. Traditional approaches often fall short in handling the complexity and the variability of lengthy documents. To address these issues, we introduce Seg2Act, an end-to-end, generation-based method for document logical structuring, revisiting logical structure extraction as an action generation task. Specifically, given the text segments of a document, Seg2Act iteratively generates the action sequence via a global context-aware generative model, and simultaneously updates its global context and current logical structure based on the generated actions. Experiments on ChCatExt and HierDoc datasets demonstrate the superior performance of Seg2Act in both supervised and transfer learning settings.</li>
</ul>

<h3>Title: QuadMamba: Learning Quadtree-based Selective Scan for Visual State Space Model</h3>
<ul>
<li><strong>Authors: </strong>Fei Xie, Weijia Zhang, Zhongdao Wang, Chao Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06806">https://arxiv.org/abs/2410.06806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06806">https://arxiv.org/pdf/2410.06806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06806]] QuadMamba: Learning Quadtree-based Selective Scan for Visual State Space Model(https://arxiv.org/abs/2410.06806)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Recent advancements in State Space Models, notably Mamba, have demonstrated superior performance over the dominant Transformer models, particularly in reducing the computational complexity from quadratic to linear. Yet, difficulties in adapting Mamba from language to vision tasks arise due to the distinct characteristics of visual data, such as the spatial locality and adjacency within images and large variations in information granularity across visual tokens. Existing vision Mamba approaches either flatten tokens into sequences in a raster scan fashion, which breaks the local adjacency of images, or manually partition tokens into windows, which limits their long-range modeling and generalization capabilities. To address these limitations, we present a new vision Mamba model, coined QuadMamba, that effectively captures local dependencies of varying granularities via quadtree-based image partition and scan. Concretely, our lightweight quadtree-based scan module learns to preserve the 2D locality of spatial regions within learned window quadrants. The module estimates the locality score of each token from their features, before adaptively partitioning tokens into window quadrants. An omnidirectional window shifting scheme is also introduced to capture more intact and informative features across different local regions. To make the discretized quadtree partition end-to-end trainable, we further devise a sequence masking strategy based on Gumbel-Softmax and its straight-through gradient estimator. Extensive experiments demonstrate that QuadMamba achieves state-of-the-art performance in various vision tasks, including image classification, object detection, instance segmentation, and semantic segmentation. The code is in this https URL.</li>
</ul>

<h3>Title: Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Zeng, Yuying Shang, Yutao Zhu, Jiawei Chen, Yu Tian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06809">https://arxiv.org/abs/2410.06809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06809">https://arxiv.org/pdf/2410.06809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06809]] Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level(https://arxiv.org/abs/2410.06809)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated immense utility across various industries. However, as LLMs advance, the risk of harmful outputs increases due to incorrect or malicious instruction prompts. While current methods effectively address jailbreak risks, they share common limitations: 1) Judging harmful responses from the prefill-level lacks utilization of the model's decoding outputs, leading to relatively lower effectiveness and robustness. 2) Rejecting potentially harmful responses based on a single evaluation can significantly impair the model's this http URL paper examines the LLMs' capability to recognize harmful outputs, revealing and quantifying their proficiency in assessing the danger of previous tokens. Motivated by pilot experiment results, we design a robust defense mechanism at the decoding level. Our novel decoder-oriented, step-by-step defense architecture corrects harmful queries directly rather than rejecting them outright. We introduce speculative decoding to enhance usability and facilitate deployment to boost secure decoding speed. Extensive experiments demonstrate that our approach improves model security without compromising reasoning speed. Notably, our method leverages the model's ability to discern hazardous information, maintaining its helpfulness compared to existing methods.</li>
</ul>

<h3>Title: Rethinking the Evaluation of Visible and Infrared Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Dayan Guan, Yixuan Wu, Tianzhu Liu, Alex C. Kot, Yanfeng Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06811">https://arxiv.org/abs/2410.06811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06811">https://arxiv.org/pdf/2410.06811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06811]] Rethinking the Evaluation of Visible and Infrared Image Fusion(https://arxiv.org/abs/2410.06811)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Visible and Infrared Image Fusion (VIF) has garnered significant interest across a wide range of high-level vision tasks, such as object detection and semantic segmentation. However, the evaluation of VIF methods remains challenging due to the absence of ground truth. This paper proposes a Segmentation-oriented Evaluation Approach (SEA) to assess VIF methods by incorporating the semantic segmentation task and leveraging segmentation labels available in latest VIF datasets. Specifically, SEA utilizes universal segmentation models, capable of handling diverse images and classes, to predict segmentation outputs from fused images and compare these outputs with segmentation labels. Our evaluation of recent VIF methods using SEA reveals that their performance is comparable or even inferior to using visible images only, despite nearly half of the infrared images demonstrating better performance than visible images. Further analysis indicates that the two metrics most correlated to our SEA are the gradient-based fusion metric $Q_{\text{ABF}}$ and the visual information fidelity metric $Q_{\text{VIFF}}$ in conventional VIF evaluation metrics, which can serve as proxies when segmentation labels are unavailable. We hope that our evaluation will guide the development of novel and practical VIF methods. The code has been released in \url{this https URL}.</li>
</ul>

<h3>Title: Defending Membership Inference Attacks via Privacy-aware Sparsity Tuning</h3>
<ul>
<li><strong>Authors: </strong>Qiang Hu, Hengxiang Zhang, Hongxin Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06814">https://arxiv.org/abs/2410.06814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06814">https://arxiv.org/pdf/2410.06814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06814]] Defending Membership Inference Attacks via Privacy-aware Sparsity Tuning(https://arxiv.org/abs/2410.06814)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Over-parameterized models are typically vulnerable to membership inference attacks, which aim to determine whether a specific sample is included in the training of a given model. Previous Weight regularizations (e.g., L1 regularization) typically impose uniform penalties on all parameters, leading to a suboptimal tradeoff between model utility and privacy. In this work, we first show that only a small fraction of parameters substantially impact the privacy risk. In light of this, we propose Privacy-aware Sparsity Tuning (PAST), a simple fix to the L1 Regularization, by employing adaptive penalties to different parameters. Our key idea behind PAST is to promote sparsity in parameters that significantly contribute to privacy leakage. In particular, we construct the adaptive weight for each parameter based on its privacy sensitivity, i.e., the gradient of the loss gap with respect to the parameter. Using PAST, the network shrinks the loss gap between members and non-members, leading to strong resistance to privacy attacks. Extensive experiments demonstrate the superiority of PAST, achieving a state-of-the-art balance in the privacy-utility trade-off.</li>
</ul>

<h3>Title: Shap-Select: Lightweight Feature Selection Using SHAP Values and Regression</h3>
<ul>
<li><strong>Authors: </strong>Egor Kraev, Baran Koseoglu, Luca Traverso, Mohammed Topiwalla</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06815">https://arxiv.org/abs/2410.06815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06815">https://arxiv.org/pdf/2410.06815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06815]] Shap-Select: Lightweight Feature Selection Using SHAP Values and Regression(https://arxiv.org/abs/2410.06815)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Feature selection is an essential process in machine learning, especially when dealing with high-dimensional datasets. It helps reduce the complexity of machine learning models, improve performance, mitigate overfitting, and decrease computation time. This paper presents a novel feature selection framework, shap-select. The framework conducts a linear or logistic regression of the target on the Shapley values of the features, on the validation set, and uses the signs and significance levels of the regression coefficients to implement an efficient heuristic for feature selection in tabular regression and classification tasks. We evaluate shap-select on the Kaggle credit card fraud dataset, demonstrating its effectiveness compared to established methods such as Recursive Feature Elimination (RFE), HISEL (a mutual information-based feature selection method), Boruta and a simpler Shapley value-based method. Our findings show that shap-select combines interpretability, computational efficiency, and performance, offering a robust solution for feature selection.</li>
</ul>

<h3>Title: Multi-Neuron Unleashes Expressivity of ReLU Networks Under Convex Relaxation</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Mao, Yani Zhang, Martin Vechev</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06816">https://arxiv.org/abs/2410.06816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06816">https://arxiv.org/pdf/2410.06816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06816]] Multi-Neuron Unleashes Expressivity of ReLU Networks Under Convex Relaxation(https://arxiv.org/abs/2410.06816)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural work certification has established itself as a crucial tool for ensuring the robustness of neural networks. Certification methods typically rely on convex relaxations of the feasible output set to provide sound bounds. However, complete certification requires exact bounds, which strongly limits the expressivity of ReLU networks: even for the simple ``$\max$'' function in $\mathbb{R}^2$, there does not exist a ReLU network that expresses this function and can be exactly bounded by single-neuron relaxation methods. This raises the question whether there exists a convex relaxation that can provide exact bounds for general continuous piecewise linear functions in $\mathbb{R}^n$. In this work, we answer this question affirmatively by showing that (layer-wise) multi-neuron relaxation provides complete certification for general ReLU networks. Based on this novel result, we show that the expressivity of ReLU networks is no longer limited under multi-neuron relaxation. To the best of our knowledge, this is the first positive result on the completeness of convex relaxations, shedding light on the practice of certified robustness.</li>
</ul>

<h3>Title: An Improved Approach for Cardiac MRI Segmentation based on 3D UNet Combined with Papillary Muscle Exclusion</h3>
<ul>
<li><strong>Authors: </strong>Narjes Benameur, Ramzi Mahmoudi, Mohamed Deriche, Amira fayouka, Imene Masmoudi, Nessrine Zoghlami</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06818">https://arxiv.org/abs/2410.06818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06818">https://arxiv.org/pdf/2410.06818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06818]] An Improved Approach for Cardiac MRI Segmentation based on 3D UNet Combined with Papillary Muscle Exclusion(https://arxiv.org/abs/2410.06818)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Left ventricular ejection fraction (LVEF) is the most important clinical parameter of cardiovascular function. The accuracy in estimating this parameter is highly dependent upon the precise segmentation of the left ventricle (LV) structure at the end diastole and systole phases. Therefore, it is crucial to develop robust algorithms for the precise segmentation of the heart structure during different phases. Methodology: In this work, an improved 3D UNet model is introduced to segment the myocardium and LV, while excluding papillary muscles, as per the recommendation of the Society for Cardiovascular Magnetic Resonance. For the practical testing of the proposed framework, a total of 8,400 cardiac MRI images were collected and analysed from the military hospital in Tunis (HMPIT), as well as the popular ACDC public dataset. As performance metrics, we used the Dice coefficient and the F1 score for validation/testing of the LV and the myocardium segmentation. Results: The data was split into 70%, 10%, and 20% for training, validation, and testing, respectively. It is worth noting that the proposed segmentation model was tested across three axis views: basal, medio basal and apical at two different cardiac phases: end diastole and end systole instances. The experimental results showed a Dice index of 0.965 and 0.945, and an F1 score of 0.801 and 0.799, at the end diastolic and systolic phases, respectively. Additionally, clinical evaluation outcomes revealed a significant difference in the LVEF and other clinical parameters when the papillary muscles were included or excluded.</li>
</ul>

<h3>Title: Dynamic metastability in the self-attention model</h3>
<ul>
<li><strong>Authors: </strong>Borjan Geshkovski, Hugo Koubbi, Yury Polyanskiy, Philippe Rigollet</a></li>
<li><strong>Subjects: </strong>cs.LG, math.AP, math.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06833">https://arxiv.org/abs/2410.06833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06833">https://arxiv.org/pdf/2410.06833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06833]] Dynamic metastability in the self-attention model(https://arxiv.org/abs/2410.06833)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>We consider the self-attention model - an interacting particle system on the unit sphere, which serves as a toy model for Transformers, the deep neural network architecture behind the recent successes of large language models. We prove the appearance of dynamic metastability conjectured in [GLPR23] - although particles collapse to a single cluster in infinite time, they remain trapped near a configuration of several clusters for an exponentially long period of time. By leveraging a gradient flow interpretation of the system, we also connect our result to an overarching framework of slow motion of gradient flows proposed by Otto and Reznikoff [OR07] in the context of coarsening and the Allen-Cahn equation. We finally probe the dynamics beyond the exponentially long period of metastability, and illustrate that, under an appropriate time-rescaling, the energy reaches its global maximum in finite time and has a staircase profile, with trajectories manifesting saddle-to-saddle-like behavior, reminiscent of recent works in the analysis of training dynamics via gradient descent for two-layer neural networks.</li>
</ul>

<h3>Title: Boosting Few-Shot Detection with Large Language Models and Layout-to-Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Abdullah, Nikolas Ebert, Oliver Wasenmüller</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06841">https://arxiv.org/abs/2410.06841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06841">https://arxiv.org/pdf/2410.06841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06841]] Boosting Few-Shot Detection with Large Language Models and Layout-to-Image Synthesis(https://arxiv.org/abs/2410.06841)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion models have enabled a wide range of works exploiting their ability to generate high-volume, high-quality data for use in various downstream tasks. One subclass of such models, dubbed Layout-to-Image Synthesis (LIS), learns to generate images conditioned on a spatial layout (bounding boxes, masks, poses, etc.) and has shown a promising ability to generate realistic images, albeit with limited layout-adherence. Moreover, the question of how to effectively transfer those models for scalable augmentation of few-shot detection data remains unanswered. Thus, we propose a collaborative framework employing a Large Language Model (LLM) and an LIS model for enhancing few-shot detection beyond state-of-the-art generative augmentation approaches. We leverage LLM's reasoning ability to extrapolate the spatial prior of the annotation space by generating new bounding boxes given only a few example annotations. Additionally, we introduce our novel layout-aware CLIP score for sample ranking, enabling tight coupling between generated layouts and images. Significant improvements on COCO few-shot benchmarks are observed. With our approach, a YOLOX-S baseline is boosted by more than 140%, 50%, 35% in mAP on the COCO 5-,10-, and 30-shot settings, respectively.</li>
</ul>

<h3>Title: SurANet: Surrounding-Aware Network for Concealed Object Detection via Highly-Efficient Interactive Contrastive Learning Strategy</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Kang, Qingpeng Li, Leyuan Fang, Jian Zhao, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06842">https://arxiv.org/abs/2410.06842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06842">https://arxiv.org/pdf/2410.06842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06842]] SurANet: Surrounding-Aware Network for Concealed Object Detection via Highly-Efficient Interactive Contrastive Learning Strategy(https://arxiv.org/abs/2410.06842)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Concealed object detection (COD) in cluttered scenes is significant for various image processing applications. However, due to that concealed objects are always similar to their background, it is extremely hard to distinguish them. Here, the major obstacle is the tiny feature differences between the inside and outside object boundary region, which makes it trouble for existing COD methods to achieve accurate results. In this paper, considering that the surrounding environment information can be well utilized to identify the concealed objects, and thus, we propose a novel deep Surrounding-Aware Network, namely SurANet, for COD tasks, which introduces surrounding information into feature extraction and loss function to improve the discrimination. First, we enhance the semantics of feature maps using differential fusion of surrounding features to highlight concealed objects. Next, a Surrounding-Aware Contrastive Loss is applied to identify the concealed object via learning surrounding feature maps contrastively. Then, SurANet can be trained end-to-end with high efficiency via our proposed Spatial-Compressed Correlation Transmission strategy after our investigation of feature dynamics, and extensive experiments improve that such features can be well reserved respectively. Finally, experimental results demonstrate that the proposed SurANet outperforms state-of-the-art COD methods on multiple real datasets. Our source code will be available at this https URL.</li>
</ul>

<h3>Title: MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders</h3>
<ul>
<li><strong>Authors: </strong>Cheng Li, May Fung, Qingyun Wang, Chi Han, Manling Li, Jindong Wang, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06845">https://arxiv.org/abs/2410.06845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06845">https://arxiv.org/pdf/2410.06845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06845]] MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders(https://arxiv.org/abs/2410.06845)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Mental health disorders are one of the most serious diseases in the world. Most people with such a disease lack access to adequate care, which highlights the importance of training models for the diagnosis and treatment of mental health disorders. However, in the mental health domain, privacy concerns limit the accessibility of personalized treatment data, making it challenging to build powerful models. In this paper, we introduce MentalArena, a self-play framework to train language models by generating domain-specific personalized data, where we obtain a better model capable of making a personalized diagnosis and treatment (as a therapist) and providing information (as a patient). To accurately model human-like mental health patients, we devise Symptom Encoder, which simulates a real patient from both cognition and behavior perspectives. To address intent bias during patient-therapist interactions, we propose Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and dynamically manage the dialogue between patient and therapist according to the identified deviations. We evaluated MentalArena against 6 benchmarks, including biomedicalQA and mental health tasks, compared to 6 advanced models. Our models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform their counterparts, including GPT-4o. We hope that our work can inspire future research on personalized care. Code is available in this https URL</li>
</ul>

<h3>Title: Joint Fine-tuning and Conversion of Pretrained Speech and Language Models towards Linear Complexity</h3>
<ul>
<li><strong>Authors: </strong>Mutian He, Philip N. Garner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06846">https://arxiv.org/abs/2410.06846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06846">https://arxiv.org/pdf/2410.06846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06846]] Joint Fine-tuning and Conversion of Pretrained Speech and Language Models towards Linear Complexity(https://arxiv.org/abs/2410.06846)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Architectures such as Linformer and Mamba have recently emerged as competitive linear time replacements for transformers. However, corresponding large pretrained models are often unavailable, especially in non-text domains. To remedy this, we present a Cross-Architecture Layerwise Distillation (CALD) approach that jointly converts a transformer model to a linear time substitute and fine-tunes it to a target task. We also compare several means to guide the fine-tuning to optimally retain the desired inference capability from the original model. The methods differ in their use of the target model and the trajectory of the parameters. In a series of empirical studies on language processing, language modeling, and speech processing, we show that CALD can effectively recover the result of the original model, and that the guiding strategy contributes to the result. Some reasons for the variation are suggested.</li>
</ul>

<h3>Title: Forgetting Through Transforming: Enabling Federated Unlearning via Class-Aware Representation Transformation</h3>
<ul>
<li><strong>Authors: </strong>Qi Guo, Zhen Tian, Minghao Yao, Yong Qi, Saiyu Qi, Yun Li, Jin Song Dong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06848">https://arxiv.org/abs/2410.06848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06848">https://arxiv.org/pdf/2410.06848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06848]] Forgetting Through Transforming: Enabling Federated Unlearning via Class-Aware Representation Transformation(https://arxiv.org/abs/2410.06848)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Unlearning (FU) enables clients to selectively remove the influence of specific data from a trained federated learning model, addressing privacy concerns and regulatory requirements. However, existing FU methods often struggle to balance effective erasure with model utility preservation, especially for class-level unlearning in non-IID settings. We propose Federated Unlearning via Class-aware Representation Transformation (FUCRT), a novel method that achieves unlearning through class-aware representation transformation. FUCRT employs two key components: (1) a transformation class selection strategy to identify optimal forgetting directions, and (2) a transformation alignment technique using dual class-aware contrastive learning to ensure consistent transformations across clients. Extensive experiments on four datasets demonstrate FUCRT's superior performance in terms of erasure guarantee, model utility preservation, and efficiency. FUCRT achieves complete (100\%) erasure of unlearning classes while maintaining or improving performance on remaining classes, outperforming state-of-the-art baselines across both IID and Non-IID settings. Analysis of the representation space reveals FUCRT's ability to effectively merge unlearning class representations with the transformation class from remaining classes, closely mimicking the model retrained from scratch.</li>
</ul>

<h3>Title: On the Security and Design of Cryptosystems Using Gabidulin-Kronecker Product Codes</h3>
<ul>
<li><strong>Authors: </strong>Terry Shue Chien Lau, Zhe Sun, Sook-Chin Yip, Ji-Jian Chin, Choo-Yee Ting</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06849">https://arxiv.org/abs/2410.06849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06849">https://arxiv.org/pdf/2410.06849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06849]] On the Security and Design of Cryptosystems Using Gabidulin-Kronecker Product Codes(https://arxiv.org/abs/2410.06849)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>This paper is a preliminary study on the security and design of cryptosystems using Gabidulin-Kronecker Product Codes. In particular, we point out the design impracticality of the system, and propose ways to improve it.</li>
</ul>

<h3>Title: Understanding Model Ensemble in Transferable Adversarial Attack</h3>
<ul>
<li><strong>Authors: </strong>Wei Yao, Zeliang Zhang, Huayi Tang, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06851">https://arxiv.org/abs/2410.06851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06851">https://arxiv.org/pdf/2410.06851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06851]] Understanding Model Ensemble in Transferable Adversarial Attack(https://arxiv.org/abs/2410.06851)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Model ensemble adversarial attack has become a powerful method for generating transferable adversarial examples that can target even unknown models, but its theoretical foundation remains underexplored. To address this gap, we provide early theoretical insights that serve as a roadmap for advancing model ensemble adversarial attack. We first define transferability error to measure the error in adversarial transferability, alongside concepts of diversity and empirical model ensemble Rademacher complexity. We then decompose the transferability error into vulnerability, diversity, and a constant, which rigidly explains the origin of transferability error in model ensemble attack: the vulnerability of an adversarial example to ensemble components, and the diversity of ensemble components. Furthermore, we apply the latest mathematical tools in information theory to bound the transferability error using complexity and generalization terms, contributing to three practical guidelines for reducing transferability error: (1) incorporating more surrogate models, (2) increasing their diversity, and (3) reducing their complexity in cases of overfitting. Finally, extensive experiments with 54 models validate our theoretical framework, representing a significant step forward in understanding transferable model ensemble adversarial attacks.</li>
</ul>

<h3>Title: Secure Video Quality Assessment Resisting Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Ao-Xiang Zhang, Yu Ran, Weixuan Tang, Yuan-Gen Wang, Qingxiao Guan, Chunsheng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06866">https://arxiv.org/abs/2410.06866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06866">https://arxiv.org/pdf/2410.06866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06866]] Secure Video Quality Assessment Resisting Adversarial Attacks(https://arxiv.org/abs/2410.06866)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack</a></li>
<li><strong>Abstract: </strong>The exponential surge in video traffic has intensified the imperative for Video Quality Assessment (VQA). Leveraging cutting-edge architectures, current VQA models have achieved human-comparable accuracy. However, recent studies have revealed the vulnerability of existing VQA models against adversarial attacks. To establish a reliable and practical assessment system, a secure VQA model capable of resisting such malicious attacks is urgently demanded. Unfortunately, no attempt has been made to explore this issue. This paper first attempts to investigate general adversarial defense principles, aiming at endowing existing VQA models with security. Specifically, we first introduce random spatial grid sampling on the video frame for intra-frame defense. Then, we design pixel-wise randomization through a guardian map, globally neutralizing adversarial perturbations. Meanwhile, we extract temporal information from the video sequence as compensation for inter-frame defense. Building upon these principles, we present a novel VQA framework from the security-oriented perspective, termed SecureVQA. Extensive experiments indicate that SecureVQA sets a new benchmark in security while achieving competitive VQA performance compared with state-of-the-art models. Ablation studies delve deeper into analyzing the principles of SecureVQA, demonstrating their generalization and contributions to the security of leading VQA models.</li>
</ul>

<h3>Title: Noise is All You Need: Private Second-Order Convergence of Noisy SGD</h3>
<ul>
<li><strong>Authors: </strong>Dmitrii Avdiukhin, Michael Dinitz, Chenglin Fan, Grigory Yaroslavtsev</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06878">https://arxiv.org/abs/2410.06878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06878">https://arxiv.org/pdf/2410.06878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06878]] Noise is All You Need: Private Second-Order Convergence of Noisy SGD(https://arxiv.org/abs/2410.06878)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Private optimization is a topic of major interest in machine learning, with differentially private stochastic gradient descent (DP-SGD) playing a key role in both theory and practice. Furthermore, DP-SGD is known to be a powerful tool in contexts beyond privacy, including robustness, machine unlearning, etc. Existing analyses of DP-SGD either make relatively strong assumptions (e.g., Lipschitz continuity of the loss function, or even convexity) or prove only first-order convergence (and thus might end at a saddle point in the non-convex setting). At the same time, there has been progress in proving second-order convergence of the non-private version of ``noisy SGD'', as well as progress in designing algorithms that are more complex than DP-SGD and do guarantee second-order convergence. We revisit DP-SGD and show that ``noise is all you need'': the noise necessary for privacy already implies second-order convergence under the standard smoothness assumptions, even for non-Lipschitz loss functions. Hence, we get second-order convergence essentially for free: DP-SGD, the workhorse of modern private optimization, under minimal assumptions can be used to find a second-order stationary point.</li>
</ul>

<h3>Title: FltLM: An Intergrated Long-Context Large Language Model for Effective Context Filtering and Understanding</h3>
<ul>
<li><strong>Authors: </strong>Jingyang Deng, Zhengyang Shen, Boyang Wang, Lixin Su, Suqi Cheng, Ying Nie, Junfeng Wang, Dawei Yin, Jinwen Ma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06886">https://arxiv.org/abs/2410.06886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06886">https://arxiv.org/pdf/2410.06886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06886]] FltLM: An Intergrated Long-Context Large Language Model for Effective Context Filtering and Understanding(https://arxiv.org/abs/2410.06886)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The development of Long-Context Large Language Models (LLMs) has markedly advanced natural language processing by facilitating the process of textual data across long documents and multiple corpora. However, Long-Context LLMs still face two critical challenges: The lost in the middle phenomenon, where crucial middle-context information is likely to be missed, and the distraction issue that the models lose focus due to overly extended contexts. To address these challenges, we propose the Context Filtering Language Model (FltLM), a novel integrated Long-Context LLM which enhances the ability of the model on multi-document question-answering (QA) tasks. Specifically, FltLM innovatively incorporates a context filter with a soft mask mechanism, identifying and dynamically excluding irrelevant content to concentrate on pertinent information for better comprehension and reasoning. Our approach not only mitigates these two challenges, but also enables the model to operate conveniently in a single forward pass. Experimental results demonstrate that FltLM significantly outperforms supervised fine-tuning and retrieval-based methods in complex QA scenarios, suggesting a promising solution for more accurate and reliable long-context natural language understanding applications.</li>
</ul>

<h3>Title: Learning from Spatio-temporal Correlation for Semi-Supervised LiDAR Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Seungho Lee, Hwijeong Lee, Hyunjung Shim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06893">https://arxiv.org/abs/2410.06893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06893">https://arxiv.org/pdf/2410.06893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06893]] Learning from Spatio-temporal Correlation for Semi-Supervised LiDAR Semantic Segmentation(https://arxiv.org/abs/2410.06893)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We address the challenges of the semi-supervised LiDAR segmentation (SSLS) problem, particularly in low-budget scenarios. The two main issues in low-budget SSLS are the poor-quality pseudo-labels for unlabeled data, and the performance drops due to the significant imbalance between ground-truth and pseudo-labels. This imbalance leads to a vicious training cycle. To overcome these challenges, we leverage the spatio-temporal prior by recognizing the substantial overlap between temporally adjacent LiDAR scans. We propose a proximity-based label estimation, which generates highly accurate pseudo-labels for unlabeled data by utilizing semantic consistency with adjacent labeled data. Additionally, we enhance this method by progressively expanding the pseudo-labels from the nearest unlabeled scans, which helps significantly reduce errors linked to dynamic classes. Additionally, we employ a dual-branch structure to mitigate performance degradation caused by data imbalance. Experimental results demonstrate remarkable performance in low-budget settings (i.e., <= 5%) and meaningful improvements in normal budget settings (i.e., 5 - 50%). Finally, our method has achieved new state-of-the-art results on SemanticKITTI and nuScenes in semi-supervised LiDAR segmentation. With only 5% labeled data, it offers competitive results against fully-supervised counterparts. Moreover, it surpasses the performance of the previous state-of-the-art at 100% labeled data (75.2%) using only 20% of labeled data (76.0%) on nuScenes. The code is available on this https URL.</li>
</ul>

<h3>Title: Average Certified Radius is a Poor Metric for Randomized Smoothing</h3>
<ul>
<li><strong>Authors: </strong>Chenhao Sun, Yuhao Mao, Mark Niklas Müller, Martin Vechev</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06895">https://arxiv.org/abs/2410.06895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06895">https://arxiv.org/pdf/2410.06895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06895]] Average Certified Radius is a Poor Metric for Randomized Smoothing(https://arxiv.org/abs/2410.06895)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Randomized smoothing is a popular approach for providing certified robustness guarantees against adversarial attacks, and has become a very active area of research. Over the past years, the average certified radius (ACR) has emerged as the single most important metric for comparing methods and tracking progress in the field. However, in this work, we show that ACR is an exceptionally poor metric for evaluating robustness guarantees provided by randomized smoothing. We theoretically show not only that a trivial classifier can have arbitrarily large ACR, but also that ACR is much more sensitive to improvements on easy samples than on hard ones. Empirically, we confirm that existing training strategies that improve ACR reduce the model's robustness on hard samples. Further, we show that by focusing on easy samples, we can effectively replicate the increase in ACR. We develop strategies, including explicitly discarding hard samples, reweighing the dataset with certified radius, and extreme optimization for easy samples, to achieve state-of-the-art ACR, although these strategies ignore robustness for the general data distribution. Overall, our results suggest that ACR has introduced a strong undesired bias to the field, and better metrics are required to holistically evaluate randomized smoothing.</li>
</ul>

<h3>Title: Generative Model for Less-Resourced Language with 1 billion parameters</h3>
<ul>
<li><strong>Authors: </strong>Domen Vreš, Martin Božič, Aljaž Potočnik, Tomaž Martinčič, Marko Robnik-Šikonja</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06898">https://arxiv.org/abs/2410.06898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06898">https://arxiv.org/pdf/2410.06898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06898]] Generative Model for Less-Resourced Language with 1 billion parameters(https://arxiv.org/abs/2410.06898)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are a basic infrastructure for modern natural language processing. Many commercial and open-source LLMs exist for English, e.g., ChatGPT, Llama, Falcon, and Mistral. As these models are trained on mostly English texts, their fluency and knowledge of low-resource languages and societies are superficial. We present the development of large generative language models for a less-resourced language. GaMS 1B - Generative Model for Slovene with 1 billion parameters was created by continuing pretraining of the existing English OPT model. We developed a new tokenizer adapted to Slovene, Croatian, and English languages and used embedding initialization methods FOCUS and WECHSEL to transfer the embeddings from the English OPT model. We evaluate our models on several classification datasets from the Slovene suite of benchmarks and generative sentence simplification task SENTA. We only used a few-shot in-context learning of our models, which are not yet instruction-tuned. For classification tasks, in this mode, the generative models lag behind the existing Slovene BERT-type models fine-tuned for specific tasks. On a sentence simplification task, the GaMS models achieve comparable or better performance than the GPT-3.5-Turbo model.</li>
</ul>

<h3>Title: Reliable Probabilistic Human Trajectory Prediction for Autonomous Applications</h3>
<ul>
<li><strong>Authors: </strong>Manuel Hetzel, Hannes Reichert, Konrad Doll, Bernhard Sick</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06905">https://arxiv.org/abs/2410.06905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06905">https://arxiv.org/pdf/2410.06905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06905]] Reliable Probabilistic Human Trajectory Prediction for Autonomous Applications(https://arxiv.org/abs/2410.06905)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Autonomous systems, like vehicles or robots, require reliable, accurate, fast, resource-efficient, scalable, and low-latency trajectory predictions to get initial knowledge about future locations and movements of surrounding objects for safe human-machine interaction. Furthermore, they need to know the uncertainty of the predictions for risk assessment to provide safe path planning. This paper presents a lightweight method to address these requirements, combining Long Short-Term Memory and Mixture Density Networks. Our method predicts probability distributions, including confidence level estimations for positional uncertainty to support subsequent risk management applications and runs on a low-power embedded platform. We discuss essential requirements for human trajectory prediction in autonomous vehicle applications and demonstrate our method's performance using multiple traffic-related datasets. Furthermore, we explain reliability and sharpness metrics and show how important they are to guarantee the correctness and robustness of a model's predictions and uncertainty assessments. These essential evaluations have so far received little attention for no good reason. Our approach focuses entirely on real-world applicability. Verifying prediction uncertainties and a model's reliability are central to autonomous real-world applications. Our framework and code are available at: this https URL.</li>
</ul>

<h3>Title: Utilize the Flow before Stepping into the Same River Twice: Certainty Represented Knowledge Flow for Refusal-Aware Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Runchuan Zhu, Zhipeng Ma, Jiang Wu, Junyuan Gao, Jiaqi Wang, Dahua Lin, Conghui He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06913">https://arxiv.org/abs/2410.06913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06913">https://arxiv.org/pdf/2410.06913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06913]] Utilize the Flow before Stepping into the Same River Twice: Certainty Represented Knowledge Flow for Refusal-Aware Instruction Tuning(https://arxiv.org/abs/2410.06913)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs) to refuse to answer unknown questions. By modifying responses of unknown questions in the training data to refusal responses such as "I don't know", RAIT enhances the reliability of LLMs and reduces their hallucination. Generally, RAIT modifies training samples based on the correctness of the initial LLM's response. However, this crude approach can cause LLMs to excessively refuse answering questions they could have correctly answered, the problem we call over-refusal. In this paper, we explore two primary causes of over-refusal: Static conflict emerges when the RAIT data is constructed solely on correctness criteria, causing similar samples in the LLM's feature space to be assigned different labels (original vs. modified "I don't know"). Dynamic conflict occurs due to the changes of LLM's knowledge state during fine-tuning, which transforms previous unknown questions into knowns, while the training data, which is constructed based on the initial LLM, remains unchanged. These conflicts cause the trained LLM to misclassify known questions as unknown, resulting in over-refusal. To address this issue, we introduce Certainty Represented Knowledge Flow for Refusal-Aware Instructions Construction (CRaFT). CRaFT centers on two main contributions: First, we additionally incorporate response certainty to selectively filter and modify data, reducing static conflicts. Second, we implement preliminary rehearsal training to characterize changes in the LLM's knowledge state, which helps mitigate dynamic conflicts during the fine-tuning process. We conducted extensive experiments on open-ended question answering and multiple-choice question task. Experiment results show that CRaFT can improve LLM's overall performance during the RAIT process. Source code and training data will be released at Github.</li>
</ul>

<h3>Title: SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration</h3>
<ul>
<li><strong>Authors: </strong>Heming Xia, Yongqi Li, Jun Zhang, Cunxiao Du, Wenjie Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06916">https://arxiv.org/abs/2410.06916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06916">https://arxiv.org/pdf/2410.06916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06916]] SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration(https://arxiv.org/abs/2410.06916)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding (SD) has emerged as a widely used paradigm to accelerate the inference of large language models (LLMs) without compromising generation quality. It works by first employing a compact model to draft multiple tokens efficiently and then using the target LLM to verify them in parallel. While this technique has achieved notable speedups, most existing approaches necessitate either additional parameters or extensive training to construct effective draft models, thereby restricting their applicability across different LLMs and tasks. To address this limitation, we explore a novel plug-and-play SD solution with layer-skipping, which skips intermediate layers of the target LLM as the compact draft model. Our analysis reveals that LLMs exhibit great potential for self-acceleration through layer sparsity and the task-specific nature of this sparsity. Building on these insights, we introduce SWIFT, an on-the-fly self-speculative decoding algorithm that adaptively selects intermediate layers of LLMs to skip during inference. SWIFT does not require auxiliary models or additional training, making it a plug-and-play solution for accelerating LLM inference across diverse input data streams. Our extensive experiments across a wide range of models and downstream tasks demonstrate that SWIFT can achieve over a 1.3x-1.6x speedup while preserving the original distribution of the generated text.</li>
</ul>

<h3>Title: Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think</h3>
<ul>
<li><strong>Authors: </strong>Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, Saining Xie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06940">https://arxiv.org/abs/2410.06940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06940">https://arxiv.org/pdf/2410.06940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06940]] Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think(https://arxiv.org/abs/2410.06940)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods. We argue that one main bottleneck in training large-scale diffusion models for generation lies in effectively learning these representations. Moreover, training can be made easier by incorporating high-quality external visual representations, rather than relying solely on the diffusion models to learn them independently. We study this by introducing a straightforward regularization called REPresentation Alignment (REPA), which aligns the projections of noisy input hidden states in denoising networks with clean image representations obtained from external, pretrained visual encoders. The results are striking: our simple strategy yields significant improvements in both training efficiency and generation quality when applied to popular diffusion and flow-based transformers, such as DiTs and SiTs. For instance, our method can speed up SiT training by over 17.5$\times$, matching the performance (without classifier-free guidance) of a SiT-XL model trained for 7M steps in less than 400K steps. In terms of final generation quality, our approach achieves state-of-the-art results of FID=1.42 using classifier-free guidance with the guidance interval.</li>
</ul>

<h3>Title: CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Pretam Ray, Jivnesh Sandhan, Amrith Krishna, Pawan Goyal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06944">https://arxiv.org/abs/2410.06944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06944">https://arxiv.org/pdf/2410.06944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06944]] CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages(https://arxiv.org/abs/2410.06944)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural dependency parsing has achieved remarkable performance for low resource morphologically rich languages. It has also been well-studied that morphologically rich languages exhibit relatively free word order. This prompts a fundamental investigation: Is there a way to enhance dependency parsing performance, making the model robust to word order variations utilizing the relatively free word order nature of morphologically rich languages? In this work, we examine the robustness of graph-based parsing architectures on 7 relatively free word order languages. We focus on scrutinizing essential modifications such as data augmentation and the removal of position encoding required to adapt these architectures accordingly. To this end, we propose a contrastive self-supervised learning method to make the model robust to word order variations. Furthermore, our proposed modification demonstrates a substantial average gain of 3.03/2.95 points in 7 relatively free word order languages, as measured by the UAS/LAS Score metric when compared to the best performing baseline.</li>
</ul>

<h3>Title: Faithful Interpretation for Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Lijie Hu, Tianhao Huang, Lu Yu, Wanyu Lin, Tianhang Zheng, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06950">https://arxiv.org/abs/2410.06950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06950">https://arxiv.org/pdf/2410.06950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06950]] Faithful Interpretation for Graph Neural Networks(https://arxiv.org/abs/2410.06950)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Currently, attention mechanisms have garnered increasing attention in Graph Neural Networks (GNNs), such as Graph Attention Networks (GATs) and Graph Transformers (GTs). It is not only due to the commendable boost in performance they offer but also its capacity to provide a more lucid rationale for model behaviors, which are often viewed as inscrutable. However, Attention-based GNNs have demonstrated instability in interpretability when subjected to various sources of perturbations during both training and testing phases, including factors like additional edges or nodes. In this paper, we propose a solution to this problem by introducing a novel notion called Faithful Graph Attention-based Interpretation (FGAI). In particular, FGAI has four crucial properties regarding stability and sensitivity to interpretation and final output distribution. Built upon this notion, we propose an efficient methodology for obtaining FGAI, which can be viewed as an ad hoc modification to the canonical Attention-based GNNs. To validate our proposed solution, we introduce two novel metrics tailored for graph interpretation assessment. Experimental results demonstrate that FGAI exhibits superior stability and preserves the interpretability of attention under various forms of perturbations and randomness, which makes FGAI a more faithful and reliable explanation tool.</li>
</ul>

<h3>Title: Support Vector Boosting Machine (SVBM): Enhancing Classification Performance with AdaBoost and Residual Connections</h3>
<ul>
<li><strong>Authors: </strong>Junbo Jacob Lian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06957">https://arxiv.org/abs/2410.06957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06957">https://arxiv.org/pdf/2410.06957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06957]] Support Vector Boosting Machine (SVBM): Enhancing Classification Performance with AdaBoost and Residual Connections(https://arxiv.org/abs/2410.06957)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In traditional boosting algorithms, the focus on misclassified training samples emphasizes their importance based on difficulty during the learning process. While using a standard Support Vector Machine (SVM) as a weak learner in an AdaBoost framework can enhance model performance by concentrating on error samples, this approach introduces significant challenges. Specifically, SVMs, characterized by their stability and robustness, may require destabilization to fit the boosting paradigm, which in turn can constrain performance due to reliance on the weighted results from preceding iterations. To address these challenges, we propose the Support Vector Boosting Machine (SVBM), which integrates a novel subsampling process with SVM algorithms and residual connection techniques. This method updates sample weights by considering both the current model's predictions and the outputs from prior rounds, allowing for effective sparsity control. The SVBM framework enhances the ability to form complex decision boundaries, thereby improving classification performance. The MATLAB source code for SVBM can be accessed at this https URL.</li>
</ul>

<h3>Title: Self-Boosting Large Language Models with Synthetic Preference Data</h3>
<ul>
<li><strong>Authors: </strong>Qingxiu Dong, Li Dong, Xingxing Zhang, Zhifang Sui, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06961">https://arxiv.org/abs/2410.06961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06961">https://arxiv.org/pdf/2410.06961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06961]] Self-Boosting Large Language Models with Synthetic Preference Data(https://arxiv.org/abs/2410.06961)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Through alignment with human preferences, Large Language Models (LLMs) have advanced significantly in generating honest, harmless, and helpful responses. However, collecting high-quality preference data is a resource-intensive and creativity-demanding process, especially for the continual improvement of LLMs. We introduce SynPO, a self-boosting paradigm that leverages synthetic preference data for model alignment. SynPO employs an iterative mechanism wherein a self-prompt generator creates diverse prompts, and a response improver refines model responses progressively. This approach trains LLMs to autonomously learn the generative rewards for their own outputs and eliminates the need for large-scale annotation of prompts and human preferences. After four SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements in instruction-following abilities, achieving over 22.1% win rate improvements on AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general performance of LLMs on various tasks, validated by a 3.2 to 5.0 average score increase on the well-recognized Open LLM leaderboard.</li>
</ul>

<h3>Title: Bridge the Points: Graph-based Few-shot Segment Anything Semantically</h3>
<ul>
<li><strong>Authors: </strong>Anqi Zhang, Guangyu Gao, Jianbo Jiao, Chi Harold Liu, Yunchao Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06964">https://arxiv.org/abs/2410.06964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06964">https://arxiv.org/pdf/2410.06964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06964]] Bridge the Points: Graph-based Few-shot Segment Anything Semantically(https://arxiv.org/abs/2410.06964)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The recent advancements in large-scale pre-training techniques have significantly enhanced the capabilities of vision foundation models, notably the Segment Anything Model (SAM), which can generate precise masks based on point and box prompts. Recent studies extend SAM to Few-shot Semantic Segmentation (FSS), focusing on prompt generation for SAM-based automatic semantic segmentation. However, these methods struggle with selecting suitable prompts, require specific hyperparameter settings for different scenarios, and experience prolonged one-shot inference times due to the overuse of SAM, resulting in low efficiency and limited automation ability. To address these issues, we propose a simple yet effective approach based on graph analysis. In particular, a Positive-Negative Alignment module dynamically selects the point prompts for generating masks, especially uncovering the potential of the background context as the negative reference. Another subsequent Point-Mask Clustering module aligns the granularity of masks and selected points as a directed graph, based on mask coverage over points. These points are then aggregated by decomposing the weakly connected components of the directed graph in an efficient manner, constructing distinct natural clusters. Finally, the positive and overshooting gating, benefiting from graph-based granularity alignment, aggregate high-confident masks and filter out the false-positive masks for final prediction, reducing the usage of additional hyperparameters and redundant mask generation. Extensive experimental analysis across standard FSS, One-shot Part Segmentation, and Cross Domain FSS datasets validate the effectiveness and efficiency of the proposed approach, surpassing state-of-the-art generalist models with a mIoU of 58.7% on COCO-20i and 35.2% on LVIS-92i. The code is available in this https URL.</li>
</ul>

<h3>Title: Uncovering Factor Level Preferences to Improve Human-Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Juhyun Oh, Eunsu Kim, Jiseon Kim, Wenda Xu, Inha Cha, William Yang Wang, Alice Oh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06965">https://arxiv.org/abs/2410.06965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06965">https://arxiv.org/pdf/2410.06965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06965]] Uncovering Factor Level Preferences to Improve Human-Model Alignment(https://arxiv.org/abs/2410.06965)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Despite advancements in Large Language Model (LLM) alignment, understanding the reasons behind LLM preferences remains crucial for bridging the gap between desired and actual behavior. LLMs often exhibit biases or tendencies that diverge from human preferences, such as favoring certain writing styles or producing overly verbose outputs. However, current methods for evaluating preference alignment often lack explainability, relying on coarse-grained comparisons. To address this, we introduce PROFILE (PRObing Factors of InfLuence for Explainability), a novel framework that uncovers and quantifies the influence of specific factors driving preferences. PROFILE's factor level analysis explains the 'why' behind human-model alignment and misalignment, offering insights into the direction of model improvement. We apply PROFILE to analyze human and LLM preferences across three tasks: summarization, helpful response generation, and document-based question-answering. Our factor level analysis reveals a substantial discrepancy between human and LLM preferences in generation tasks, whereas LLMs show strong alignment with human preferences in evaluation tasks. We demonstrate how leveraging factor level insights, including addressing misaligned factors or exploiting the generation-evaluation gap, can improve alignment with human preferences. This work underscores the importance of explainable preference analysis and highlights PROFILE's potential to provide valuable training signals, driving further improvements in human-model alignment.</li>
</ul>

<h3>Title: Personal Intelligence System UniLM: Hybrid On-Device Small Language Model and Server-Based Large Language Model for Malay Nusantara</h3>
<ul>
<li><strong>Authors: </strong>Azree Nazri, Olalekan Agbolade, Faisal Aziz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06973">https://arxiv.org/abs/2410.06973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06973">https://arxiv.org/pdf/2410.06973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06973]] Personal Intelligence System UniLM: Hybrid On-Device Small Language Model and Server-Based Large Language Model for Malay Nusantara(https://arxiv.org/abs/2410.06973)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In contexts with limited computational and data resources, high-resource language models often prove inadequate, particularly when addressing the specific needs of Malay languages. This paper introduces a Personal Intelligence System designed to efficiently integrate both on-device and server-based models. The system incorporates SLiM-34M for on-device processing, optimized for low memory and power usage, and MANYAK-1.3B for server-based tasks, allowing for scalable, high-performance language processing. The models achieve significant results across various tasks, such as machine translation, question-answering, and translate IndoMMLU. Particularly noteworthy is SLiM-34M's ability to achieve a high improvement in accuracy compared to other LLMs while using 2 times fewer pre-training tokens. This work challenges the prevailing assumption that large-scale computational resources are necessary to build effective language models, contributing to the development of resource-efficient models for the Malay language with the unique orchestration between SLiM-34M and MANYAK-1.3B.</li>
</ul>

<h3>Title: AdaRC: Mitigating Graph Structure Shifts during Test-Time</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Bao, Zhichen Zeng, Zhining Liu, Hanghang Tong, Jingrui He</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06976">https://arxiv.org/abs/2410.06976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06976">https://arxiv.org/pdf/2410.06976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06976]] AdaRC: Mitigating Graph Structure Shifts during Test-Time(https://arxiv.org/abs/2410.06976)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Powerful as they are, graph neural networks (GNNs) are known to be vulnerable to distribution shifts. Recently, test-time adaptation (TTA) has attracted attention due to its ability to adapt a pre-trained model to a target domain without re-accessing the source domain. However, existing TTA algorithms are primarily designed for attribute shifts in vision tasks, where samples are independent. These methods perform poorly on graph data that experience structure shifts, where node connectivity differs between source and target graphs. We attribute this performance gap to the distinct impact of node attribute shifts versus graph structure shifts: the latter significantly degrades the quality of node representations and blurs the boundaries between different node categories. To address structure shifts in graphs, we propose AdaRC, an innovative framework designed for effective and efficient adaptation to structure shifts by adjusting the hop-aggregation parameters in GNNs. To enhance the representation quality, we design a prediction-informed clustering loss to encourage the formation of distinct clusters for different node categories. Additionally, AdaRC seamlessly integrates with existing TTA algorithms, allowing it to handle attribute shifts effectively while improving overall performance under combined structure and attribute shifts. We validate the effectiveness of AdaRC on both synthetic and real-world datasets, demonstrating its robustness across various combinations of structure and attribute shifts.</li>
</ul>

<h3>Title: Adaptive High-Frequency Transformer for Diverse Wildlife Re-Identification</h3>
<ul>
<li><strong>Authors: </strong>Chenyue Li, Shuoyi Chen, Mang Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06977">https://arxiv.org/abs/2410.06977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06977">https://arxiv.org/pdf/2410.06977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06977]] Adaptive High-Frequency Transformer for Diverse Wildlife Re-Identification(https://arxiv.org/abs/2410.06977)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Wildlife ReID involves utilizing visual technology to identify specific individuals of wild animals in different scenarios, holding significant importance for wildlife conservation, ecological research, and environmental monitoring. Existing wildlife ReID methods are predominantly tailored to specific species, exhibiting limited applicability. Although some approaches leverage extensively studied person ReID techniques, they struggle to address the unique challenges posed by wildlife. Therefore, in this paper, we present a unified, multi-species general framework for wildlife ReID. Given that high-frequency information is a consistent representation of unique features in various species, significantly aiding in identifying contours and details such as fur textures, we propose the Adaptive High-Frequency Transformer model with the goal of enhancing high-frequency information learning. To mitigate the inevitable high-frequency interference in the wilderness environment, we introduce an object-aware high-frequency selection strategy to adaptively capture more valuable high-frequency components. Notably, we unify the experimental settings of multiple wildlife datasets for ReID, achieving superior performance over state-of-the-art ReID methods. In domain generalization scenarios, our approach demonstrates robust generalization to unknown species.</li>
</ul>

<h3>Title: Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Michael Lan, Philip Torr, Austin Meek, Ashkan Khakzar, David Krueger, Fazl Barez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06981">https://arxiv.org/abs/2410.06981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06981">https://arxiv.org/pdf/2410.06981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06981]] Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models(https://arxiv.org/abs/2410.06981)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We investigate feature universality in large language models (LLMs), a research field that aims to understand how different models similarly represent concepts in the latent spaces of their intermediate layers. Demonstrating feature universality allows discoveries about latent representations to generalize across several models. However, comparing features across LLMs is challenging due to polysemanticity, in which individual neurons often correspond to multiple features rather than distinct ones. This makes it difficult to disentangle and match features across different models. To address this issue, we employ a method known as dictionary learning by using sparse autoencoders (SAEs) to transform LLM activations into more interpretable spaces spanned by neurons corresponding to individual features. After matching feature neurons across models via activation correlation, we apply representational space similarity metrics like Singular Value Canonical Correlation Analysis to analyze these SAE features across different LLMs. Our experiments reveal significant similarities in SAE feature spaces across various LLMs, providing new evidence for feature universality.</li>
</ul>

<h3>Title: Structure-Centric Robust Monocular Depth Estimation via Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Runze Chen, Haiyong Luo, Fang Zhao, Jingze Yu, Yupeng Jia, Juan Wang, Xuepeng Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06982">https://arxiv.org/abs/2410.06982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06982">https://arxiv.org/pdf/2410.06982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06982]] Structure-Centric Robust Monocular Depth Estimation via Knowledge Distillation(https://arxiv.org/abs/2410.06982)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation, enabled by self-supervised learning, is a key technique for 3D perception in computer vision. However, it faces significant challenges in real-world scenarios, which encompass adverse weather variations, motion blur, as well as scenes with poor lighting conditions at night. Our research reveals that we can divide monocular depth estimation into three sub-problems: depth structure consistency, local texture disambiguation, and semantic-structural correlation. Our approach tackles the non-robustness of existing self-supervised monocular depth estimation models to interference textures by adopting a structure-centered perspective and utilizing the scene structure characteristics demonstrated by semantics and illumination. We devise a novel approach to reduce over-reliance on local textures, enhancing robustness against missing or interfering patterns. Additionally, we incorporate a semantic expert model as the teacher and construct inter-model feature dependencies via learnable isomorphic graphs to enable aggregation of semantic structural knowledge. Our approach achieves state-of-the-art out-of-distribution monocular depth estimation performance across a range of public adverse scenario datasets. It demonstrates notable scalability and compatibility, without necessitating extensive model engineering. This showcases the potential for customizing models for diverse industrial applications.</li>
</ul>

<h3>Title: Jointly Generating Multi-view Consistent PBR Textures using Collaborative Control</h3>
<ul>
<li><strong>Authors: </strong>Shimon Vainer, Konstantin Kutsy, Dante De Nigris, Ciara Rowles, Slava Elizarov, Simon Donné</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06985">https://arxiv.org/abs/2410.06985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06985">https://arxiv.org/pdf/2410.06985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06985]] Jointly Generating Multi-view Consistent PBR Textures using Collaborative Control(https://arxiv.org/abs/2410.06985)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Multi-view consistency remains a challenge for image diffusion models. Even within the Text-to-Texture problem, where perfect geometric correspondences are known a priori, many methods fail to yield aligned predictions across views, necessitating non-trivial fusion methods to incorporate the results onto the original mesh. We explore this issue for a Collaborative Control workflow specifically in PBR Text-to-Texture. Collaborative Control directly models PBR image probability distributions, including normal bump maps; to our knowledge, the only diffusion model to directly output full PBR stacks. We discuss the design decisions involved in making this model multi-view consistent, and demonstrate the effectiveness of our approach in ablation studies, as well as practical applications.</li>
</ul>

<h3>Title: Diffusion Density Estimators</h3>
<ul>
<li><strong>Authors: </strong>Akhil Premkumar</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06986">https://arxiv.org/abs/2410.06986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06986">https://arxiv.org/pdf/2410.06986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06986]] Diffusion Density Estimators(https://arxiv.org/abs/2410.06986)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We investigate the use of diffusion models as neural density estimators. The current approach to this problem involves converting the generative process to a smooth flow, known as the Probability Flow ODE. The log density at a given sample can be obtained by solving the ODE with a black-box solver. We introduce a new, highly parallelizable method that computes log densities without the need to solve a flow. Our approach is based on estimating a path integral by Monte Carlo, in a manner identical to the simulation-free training of diffusion models. We also study how different training parameters affect the accuracy of the density calculation, and offer insights into how these models can be made more scalable and efficient.</li>
</ul>

<h3>Title: Efficient Distribution Matching of Representations via Noise-Injected Deep InfoMax</h3>
<ul>
<li><strong>Authors: </strong>Ivan Butakov, Alexander Sememenko, Alexander Tolmachev, Andrey Gladkov, Marina Munkhoeva, Alexey Frolov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.06993">https://arxiv.org/abs/2410.06993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.06993">https://arxiv.org/pdf/2410.06993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.06993]] Efficient Distribution Matching of Representations via Noise-Injected Deep InfoMax(https://arxiv.org/abs/2410.06993)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep InfoMax (DIM) is a well-established method for self-supervised representation learning (SSRL) based on maximization of the mutual information between the input and the output of a deep neural network encoder. Despite the DIM and contrastive SSRL in general being well-explored, the task of learning representations conforming to a specific distribution (i.e., distribution matching, DM) is still under-addressed. Motivated by the importance of DM to several downstream tasks (including generative modeling, disentanglement, outliers detection and other), we enhance DIM to enable automatic matching of learned representations to a selected prior distribution. To achieve this, we propose injecting an independent noise into the normalized outputs of the encoder, while keeping the same InfoMax training objective. We show that such modification allows for learning uniformly and normally distributed representations, as well as representations of other absolutely continuous distributions. Our approach is tested on various downstream tasks. The results indicate a moderate trade-off between the performance on the downstream tasks and quality of DM.</li>
</ul>

<h3>Title: CursorCore: Assist Programming through Aligning Anything</h3>
<ul>
<li><strong>Authors: </strong>Hao Jiang, Qi Liu, Rui Li, Shengyu Ye, Shijin Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07002">https://arxiv.org/abs/2410.07002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07002">https://arxiv.org/pdf/2410.07002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07002]] CursorCore: Assist Programming through Aligning Anything(https://arxiv.org/abs/2410.07002)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have been successfully applied to programming assistance tasks, such as code completion, code insertion, and instructional code editing. However, these applications remain insufficiently automated and struggle to effectively integrate various types of information during the programming process, including coding history, current code, and user instructions. In this work, we propose a new conversational framework that comprehensively integrates these information sources, collect data to train our models and evaluate their performance. Firstly, to thoroughly evaluate how well models align with different types of information and the quality of their outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to comprehensively assess the performance of models in programming assistance tasks. Then, for data collection, we develop a data generation pipeline, Programming-Instruct, which synthesizes training data from diverse sources, such as GitHub and online judge platforms. This pipeline can automatically generate various types of messages throughout the programming process. Finally, using this pipeline, we generate 219K samples, fine-tune multiple models, and develop the CursorCore series. We show that CursorCore outperforms other models of comparable size. This framework unifies applications such as inline chat and automated editing, contributes to the advancement of coding assistants. Code, models and data are freely available at this https URL.</li>
</ul>

<h3>Title: Pap2Pat: Towards Automated Paper-to-Patent Drafting using Chunk-based Outline-guided Generation</h3>
<ul>
<li><strong>Authors: </strong>Valentin Knappich, Simon Razniewski, Anna Hätty, Annemarie Friedrich</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07009">https://arxiv.org/abs/2410.07009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07009">https://arxiv.org/pdf/2410.07009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07009]] Pap2Pat: Towards Automated Paper-to-Patent Drafting using Chunk-based Outline-guided Generation(https://arxiv.org/abs/2410.07009)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The patent domain is gaining attention in natural language processing research, offering practical applications in streamlining the patenting process and providing challenging benchmarks for large language models (LLMs). However, the generation of the description sections of patents, which constitute more than 90% of the patent document, has not been studied to date. We address this gap by introducing the task of outline-guided paper-to-patent generation, where an academic paper provides the technical specification of the invention and an outline conveys the desired patent structure. We present PAP2PAT, a new challenging benchmark of 1.8k patent-paper pairs with document outlines, collected using heuristics that reflect typical research lab practices. Our experiments with current open-weight LLMs and outline-guided chunk-based generation show that they can effectively use information from the paper but struggle with repetitions, likely due to the inherent repetitiveness of patent language. We release our data and code.</li>
</ul>

<h3>Title: Optimizing Estimators of Squared Calibration Errors in Classification</h3>
<ul>
<li><strong>Authors: </strong>Sebastian G. Gruber, Francis Bach</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07014">https://arxiv.org/abs/2410.07014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07014">https://arxiv.org/pdf/2410.07014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07014]] Optimizing Estimators of Squared Calibration Errors in Classification(https://arxiv.org/abs/2410.07014)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>In this work, we propose a mean-squared error-based risk that enables the comparison and optimization of estimators of squared calibration errors in practical settings. Improving the calibration of classifiers is crucial for enhancing the trustworthiness and interpretability of machine learning models, especially in sensitive decision-making scenarios. Although various calibration (error) estimators exist in the current literature, there is a lack of guidance on selecting the appropriate estimator and tuning its hyperparameters. By leveraging the bilinear structure of squared calibration errors, we reformulate calibration estimation as a regression problem with independent and identically distributed (i.i.d.) input pairs. This reformulation allows us to quantify the performance of different estimators even for the most challenging calibration criterion, known as canonical calibration. Our approach advocates for a training-validation-testing pipeline when estimating a calibration error on an evaluation dataset. We demonstrate the effectiveness of our pipeline by optimizing existing calibration estimators and comparing them with novel kernel ridge regression-based estimators on standard image classification tasks.</li>
</ul>

<h3>Title: Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization</h3>
<ul>
<li><strong>Authors: </strong>Chengtao Jian, Kai Yang, Yang Jiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07018">https://arxiv.org/abs/2410.07018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07018">https://arxiv.org/pdf/2410.07018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07018]] Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization(https://arxiv.org/abs/2410.07018)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Out-of-Distribution (OOD) generalization in machine learning is a burgeoning area of study. Its primary goal is to enhance the adaptability and resilience of machine learning models when faced with new, unseen, and potentially adversarial data that significantly diverges from their original training datasets. In this paper, we investigate time series OOD generalization via pre-trained Large Language Models (LLMs). We first propose a novel \textbf{T}ri-level learning framework for \textbf{T}ime \textbf{S}eries \textbf{O}OD generalization, termed TTSO, which considers both sample-level and group-level uncertainties. This formula offers a fresh theoretic perspective for formulating and analyzing OOD generalization problem. In addition, we provide a theoretical analysis to justify this method is well motivated. We then develop a stratified localization algorithm tailored for this tri-level optimization problem, theoretically demonstrating the guaranteed convergence of the proposed algorithm. Our analysis also reveals that the iteration complexity to obtain an $\epsilon$-stationary point is bounded by O($\frac{1}{\epsilon^{2}}$). Extensive experiments on real-world datasets have been conducted to elucidate the effectiveness of the proposed method.</li>
</ul>

<h3>Title: Clean Evaluations on Contaminated Visual Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongyuan Lu, Shujie Miao, Wai Lam</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07030">https://arxiv.org/abs/2410.07030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07030">https://arxiv.org/pdf/2410.07030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07030]] Clean Evaluations on Contaminated Visual Language Models(https://arxiv.org/abs/2410.07030)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>How to evaluate large language models (LLMs) cleanly has been established as an important research era to genuinely report the performance of possibly contaminated LLMs. Yet, how to cleanly evaluate the visual language models (VLMs) is an under-studied problem. We propose a novel approach to achieve such goals through data augmentation methods on the visual input information. We then craft a new visual clean evaluation benchmark with thousands of data instances. Through extensive experiments, we found that the traditional visual data augmentation methods are useful, but they are at risk of being used as a part of the training data as a workaround. We further propose using BGR augmentation to switch the colour channel of the visual information. We found that it is a simple yet effective method for reducing the effect of data contamination and fortunately, it is also harmful to be used as a data augmentation method during training. It means that it is hard to integrate such data augmentation into training by malicious trainers and it could be a promising technique to cleanly evaluate visual LLMs. Our code, data, and model weights will be released upon publication.</li>
</ul>

<h3>Title: PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness</h3>
<ul>
<li><strong>Authors: </strong>Zekun Wang, Feiyu Duan, Yibo Zhang, Wangchunshu Zhou, Ke Xu, Wenhao Huang, Jie Fu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07035">https://arxiv.org/abs/2410.07035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07035">https://arxiv.org/pdf/2410.07035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07035]] PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness(https://arxiv.org/abs/2410.07035)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate impressive capabilities across various domains, including role-playing, creative writing, mathematical reasoning, and coding. Despite these advancements, LLMs still encounter challenges with length control, frequently failing to adhere to specific length constraints due to their token-level operations and insufficient training on data with strict length limitations. We identify this issue as stemming from a lack of positional awareness and propose novel approaches--PositionID Prompting and PositionID Fine-Tuning--to address it. These methods enhance the model's ability to continuously monitor and manage text length during generation. Additionally, we introduce PositionID CP Prompting to enable LLMs to perform copy and paste operations accurately. Furthermore, we develop two benchmarks for evaluating length control and copy-paste abilities. Our experiments demonstrate that our methods significantly improve the model's adherence to length constraints and copy-paste accuracy without compromising response quality.</li>
</ul>

<h3>Title: Distributionally Robust Clustered Federated Learning: A Case Study in Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Xenia Konti, Hans Riess, Manos Giannopoulos, Yi Shen, Michael J. Pencina, Nicoleta J. Economou-Zavlanos, Michael M. Zavlanos</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07039">https://arxiv.org/abs/2410.07039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07039">https://arxiv.org/pdf/2410.07039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07039]] Distributionally Robust Clustered Federated Learning: A Case Study in Healthcare(https://arxiv.org/abs/2410.07039)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>In this paper, we address the challenge of heterogeneous data distributions in cross-silo federated learning by introducing a novel algorithm, which we term Cross-silo Robust Clustered Federated Learning (CS-RCFL). Our approach leverages the Wasserstein distance to construct ambiguity sets around each client's empirical distribution that capture possible distribution shifts in the local data, enabling evaluation of worst-case model performance. We then propose a model-agnostic integer fractional program to determine the optimal distributionally robust clustering of clients into coalitions so that possible biases in the local models caused by statistically heterogeneous client datasets are avoided, and analyze our method for linear and logistic regression models. Finally, we discuss a federated learning protocol that ensures the privacy of client distributions, a critical consideration, for instance, when clients are healthcare institutions. We evaluate our algorithm on synthetic and real-world healthcare data.</li>
</ul>

<h3>Title: Emergent properties with repeated examples</h3>
<ul>
<li><strong>Authors: </strong>François Charton, Julia Kempe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07041">https://arxiv.org/abs/2410.07041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07041">https://arxiv.org/pdf/2410.07041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07041]] Emergent properties with repeated examples(https://arxiv.org/abs/2410.07041)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We study the performance of transformers as a function of the number of repetitions of training examples with algorithmically generated datasets. On three problems of mathematics: the greatest common divisor, modular multiplication, and matrix eigenvalues, we show that for a fixed number of training steps, models trained on smaller sets of repeated examples outperform models trained on larger sets of single-use examples. We also demonstrate that two-set training - repeated use of a small random subset of examples, along normal sampling on the rest of the training set - provides for faster learning and better performance. This highlights that the benefits of repetition can outweigh those of data diversity. These datasets and problems provide a controlled setting to shed light on the still poorly understood interplay between generalization and memorization in deep learning.</li>
</ul>

<h3>Title: Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing</h3>
<ul>
<li><strong>Authors: </strong>Weichuan Wang, Zhaoyi Li, Defu Lian, Chen Ma, Linqi Song, Ying Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07054">https://arxiv.org/abs/2410.07054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07054">https://arxiv.org/pdf/2410.07054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07054]] Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing(https://arxiv.org/abs/2410.07054)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently revolutionized the NLP field, while they still fall short in some specific down-stream tasks. In the work, we focus on utilizing LLMs to perform machine translation, where we observe that two patterns of errors frequently occur and drastically affect the translation quality: language mismatch and repetition. The work sets out to explore the potential for mitigating these two issues by leveraging model editing methods, e.g., by locating Feed-Forward Network (FFN) neurons or something that are responsible for the errors and deactivating them in the inference time. We find that directly applying such methods either limited effect on the targeted errors or has significant negative side-effect on the general translation quality, indicating that the located components may also be crucial for ensuring machine translation with LLMs on the rails. To this end, we propose to refine the located components by fetching the intersection of the locating results under different language settings, filtering out the aforementioned information that is irrelevant to targeted errors. The experiment results empirically demonstrate that our methods can effectively reduce the language mismatch and repetition ratios and meanwhile enhance or keep the general translation quality in most cases.</li>
</ul>

<h3>Title: Online Epsilon Net and Piercing Set for Geometric Concepts</h3>
<ul>
<li><strong>Authors: </strong>Sujoy Bhore, Devdan Dey, Satyam Singh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07059">https://arxiv.org/abs/2410.07059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07059">https://arxiv.org/pdf/2410.07059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07059]] Online Epsilon Net and Piercing Set for Geometric Concepts(https://arxiv.org/abs/2410.07059)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>VC-dimension and $\varepsilon$-nets are key concepts in Statistical Learning Theory. Intuitively, VC-dimension is a measure of the size of a class of sets. The famous $\varepsilon$-net theorem, a fundamental result in Discrete Geometry, asserts that if the VC-dimension of a set system is bounded, then a small sample exists that intersects all sufficiently large sets. In online learning scenarios where data arrives sequentially, the VC-dimension helps to bound the complexity of the set system, and $\varepsilon$-nets ensure the selection of a small representative set. This sampling framework is crucial in various domains, including spatial data analysis, motion planning in dynamic environments, optimization of sensor networks, and feature extraction in computer vision, among others. Motivated by these applications, we study the online $\varepsilon$-net problem for geometric concepts with bounded VC-dimension. While the offline version of this problem has been extensively studied, surprisingly, there are no known theoretical results for the online version to date. We present the first deterministic online algorithm with an optimal competitive ratio for intervals in $\mathbb{R}$. Next, we give a randomized online algorithm with a near-optimal competitive ratio for axis-aligned boxes in $\mathbb{R}^d$, for $d\le 3$. Furthermore, we introduce a novel technique to analyze similar-sized objects of constant description complexity in $\mathbb{R}^d$, which may be of independent interest. Next, we focus on the continuous version of this problem, where ranges of the set system are geometric concepts in $\mathbb{R}^d$ arriving in an online manner, but the universe is the entire space, and the objective is to choose a small sample that intersects all the ranges.</li>
</ul>

<h3>Title: TinyEmo: Scaling down Emotional Reasoning via Metric Projection</h3>
<ul>
<li><strong>Authors: </strong>Cristian Gutierrez</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07062">https://arxiv.org/abs/2410.07062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07062">https://arxiv.org/pdf/2410.07062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07062]] TinyEmo: Scaling down Emotional Reasoning via Metric Projection(https://arxiv.org/abs/2410.07062)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces TinyEmo, a family of small multi-modal language models for emotional reasoning and classification. Our approach features: (1) a synthetic emotional instruct dataset for both pre-training and fine-tuning stages, (2) a Metric Projector that delegates classification from the language model allowing for more efficient training and inference, (3) a multi-modal large language model (MM-LLM) for emotional reasoning, and (4) a semi-automated framework for bias detection. TinyEmo is able to perform emotion classification and emotional reasoning, all while using substantially fewer parameters than comparable models. This efficiency allows us to freely incorporate more diverse emotional datasets, enabling strong performance on classification tasks, with our smallest model (700M parameters) outperforming larger state-of-the-art models based on general-purpose MM-LLMs with over 7B parameters. Additionally, the Metric Projector allows for interpretability and indirect bias detection in large models without additional training, offering an approach to understand and improve AI systems. We release code, models, and dataset at this https URL</li>
</ul>

<h3>Title: InAttention: Linear Context Scaling for Transformers</h3>
<ul>
<li><strong>Authors: </strong>Joseph Eisner</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07063">https://arxiv.org/abs/2410.07063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07063">https://arxiv.org/pdf/2410.07063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07063]] InAttention: Linear Context Scaling for Transformers(https://arxiv.org/abs/2410.07063)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>VRAM requirements for transformer models scale quadratically with context length due to the self-attention mechanism. In this paper we modify the decoder-only transformer, replacing self-attention with InAttention, which scales linearly with context length during inference by having tokens attend only to initial states. Benchmarking shows that InAttention significantly reduces VRAM usage during inference, enabling handling of long sequences on consumer GPUs. We corroborate that fine-tuning extends context length efficiently, improving performance on long sequences without high training costs. InAttention offers a scalable solution for long-range dependencies in transformer models, paving the way for further optimization.</li>
</ul>

<h3>Title: A Gentle Introduction and Tutorial on Deep Generative Models in Transportation Research</h3>
<ul>
<li><strong>Authors: </strong>Seongjin Choi, Zhixiong Jin, Seungwoo Ham, Jiwon Kim, Lijun Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07066">https://arxiv.org/abs/2410.07066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07066">https://arxiv.org/pdf/2410.07066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07066]] A Gentle Introduction and Tutorial on Deep Generative Models in Transportation Research(https://arxiv.org/abs/2410.07066)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>Deep Generative Models (DGMs) have rapidly advanced in recent years, becoming essential tools in various fields due to their ability to learn complex data distributions and generate synthetic data. Their importance in transportation research is increasingly recognized, particularly for applications like traffic data generation, prediction, and feature extraction. This paper offers a comprehensive introduction and tutorial on DGMs, with a focus on their applications in transportation. It begins with an overview of generative models, followed by detailed explanations of fundamental models, a systematic review of the literature, and practical tutorial code to aid implementation. The paper also discusses current challenges and opportunities, highlighting how these models can be effectively utilized and further developed in transportation research. This paper serves as a valuable reference, guiding researchers and practitioners from foundational knowledge to advanced applications of DGMs in transportation research.</li>
</ul>

<h3>Title: ReIFE: Re-evaluating Instruction-Following Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Yixin Liu, Kejian Shi, Alexander R. Fabbri, Yilun Zhao, Peifeng Wang, Chien-Sheng Wu, Shafiq Joty, Arman Cohan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07069">https://arxiv.org/abs/2410.07069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07069">https://arxiv.org/pdf/2410.07069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07069]] ReIFE: Re-evaluating Instruction-Following Evaluation(https://arxiv.org/abs/2410.07069)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The automatic evaluation of instruction following typically involves using large language models (LLMs) to assess response quality. However, there is a lack of comprehensive evaluation of these LLM-based evaluators across two dimensions: the base LLMs and the evaluation protocols. Therefore, we present a thorough meta-evaluation of instruction following, including 25 base LLMs and 15 recently proposed evaluation protocols, on 4 human-annotated datasets, assessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows us to identify the best-performing base LLMs and evaluation protocols with a high degree of robustness. Moreover, our large-scale evaluation reveals: (1) Base LLM performance ranking remains largely consistent across evaluation protocols, with less capable LLMs showing greater improvement from protocol enhancements; (2) Robust evaluation of evaluation protocols requires many base LLMs with varying capability levels, as protocol effectiveness can depend on the base LLM used; (3) Evaluation results on different datasets are not always consistent, so a rigorous evaluation requires multiple datasets with distinctive features. We release our meta-evaluation suite ReIFE, which provides the codebase and evaluation result collection for more than 500 LLM-evaluator configurations, to support future research in instruction-following evaluation.</li>
</ul>

<h3>Title: Retrieval-Augmented Decision Transformer: External Memory for In-context RL</h3>
<ul>
<li><strong>Authors: </strong>Thomas Schmied, Fabian Paischer, Vihang Patil, Markus Hofmarcher, Razvan Pascanu, Sepp Hochreiter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07071">https://arxiv.org/abs/2410.07071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07071">https://arxiv.org/pdf/2410.07071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07071]] Retrieval-Augmented Decision Transformer: External Memory for In-context RL(https://arxiv.org/abs/2410.07071)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) is the ability of a model to learn a new task by observing a few exemplars in its context. While prevalent in NLP, this capability has recently also been observed in Reinforcement Learning (RL) settings. Prior in-context RL methods, however, require entire episodes in the agent's context. Given that complex environments typically lead to long episodes with sparse rewards, these methods are constrained to simple environments with short episodes. To address these challenges, we introduce Retrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external memory mechanism to store past experiences from which it retrieves only sub-trajectories relevant for the current situation. The retrieval component in RA-DT does not require training and can be entirely domain-agnostic. We evaluate the capabilities of RA-DT on grid-world environments, robotics simulations, and procedurally-generated video games. On grid-worlds, RA-DT outperforms baselines, while using only a fraction of their context length. Furthermore, we illuminate the limitations of current in-context RL methods on complex environments and discuss future directions. To facilitate future research, we release datasets for four of the considered environments.</li>
</ul>

<h3>Title: Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhengyu Hu, Yichuan Li, Zhengyu Chen, Jingang Wang, Han Liu, Kyumin Lee, Kaize Ding</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07074">https://arxiv.org/abs/2410.07074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07074">https://arxiv.org/pdf/2410.07074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07074]] Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning(https://arxiv.org/abs/2410.07074)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world systems, yet leveraging large language models (LLMs) for TAGs presents unique challenges due to the gap between sequential text processing and graph-structured data. We introduce AskGNN, a novel approach that bridges this gap by leveraging In-Context Learning (ICL) to integrate graph data and task-specific information into LLMs. AskGNN employs a Graph Neural Network (GNN)-powered structure-enhanced retriever to select labeled nodes across graphs, incorporating complex graph structures and their supervision signals. Our learning-to-retrieve algorithm optimizes the retriever to select example nodes that maximize LLM performance on graph. Experiments across three tasks and seven LLMs demonstrate AskGNN's superior effectiveness in graph task performance, opening new avenues for applying LLMs to graph-structured data without extensive fine-tuning.</li>
</ul>

<h3>Title: MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses</h3>
<ul>
<li><strong>Authors: </strong>Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07076">https://arxiv.org/abs/2410.07076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07076">https://arxiv.org/pdf/2410.07076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07076]] MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses(https://arxiv.org/abs/2410.07076)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scientific discovery contributes largely to human society's prosperity, and recent progress shows that LLMs could potentially catalyze this process. However, it is still unclear whether LLMs can discover novel and valid hypotheses in chemistry. In this work, we investigate this central research question: Can LLMs automatically discover novel and valid chemistry research hypotheses given only a chemistry research background (consisting of a research question and/or a background survey), without limitation on the domain of the research question? After extensive discussions with chemistry experts, we propose an assumption that a majority of chemistry hypotheses can be resulted from a research background and several inspirations. With this key insight, we break the central question into three smaller fundamental questions. In brief, they are: (1) given a background question, whether LLMs can retrieve good inspirations; (2) with background and inspirations, whether LLMs can lead to hypothesis; and (3) whether LLMs can identify good hypotheses to rank them higher. To investigate these questions, we construct a benchmark consisting of 51 chemistry papers published in Nature, Science, or a similar level in 2024 (all papers are only available online since 2024). Every paper is divided by chemistry PhD students into three components: background, inspirations, and hypothesis. The goal is to rediscover the hypothesis, given only the background and a large randomly selected chemistry literature corpus consisting the ground truth inspiration papers, with LLMs trained with data up to 2023. We also develop an LLM-based multi-agent framework that leverages the assumption, consisting of three stages reflecting the three smaller questions. The proposed method can rediscover many hypotheses with very high similarity with the ground truth ones, covering the main innovations.</li>
</ul>

<h3>Title: JPEG Inspired Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Ahmed H. Salamah, Kaixiang Zheng, Yiwen Liu, En-Hui Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07081">https://arxiv.org/abs/2410.07081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07081">https://arxiv.org/pdf/2410.07081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07081]] JPEG Inspired Deep Learning(https://arxiv.org/abs/2410.07081)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Although it is traditionally believed that lossy image compression, such as JPEG compression, has a negative impact on the performance of deep neural networks (DNNs), it is shown by recent works that well-crafted JPEG compression can actually improve the performance of deep learning (DL). Inspired by this, we propose JPEG-DL, a novel DL framework that prepends any underlying DNN architecture with a trainable JPEG compression layer. To make the quantization operation in JPEG compression trainable, a new differentiable soft quantizer is employed at the JPEG layer, and then the quantization operation and underlying DNN are jointly trained. Extensive experiments show that in comparison with the standard DL, JPEG-DL delivers significant accuracy improvements across various datasets and model architectures while enhancing robustness against adversarial attacks. Particularly, on some fine-grained image classification datasets, JPEG-DL can increase prediction accuracy by as much as 20.9%. Our code is available on this https URL.</li>
</ul>

<h3>Title: Stanceformer: Target-Aware Transformer for Stance Detection</h3>
<ul>
<li><strong>Authors: </strong>Krishna Garg, Cornelia Caragea</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07083">https://arxiv.org/abs/2410.07083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07083">https://arxiv.org/pdf/2410.07083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07083]] Stanceformer: Target-Aware Transformer for Stance Detection(https://arxiv.org/abs/2410.07083)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The task of Stance Detection involves discerning the stance expressed in a text towards a specific subject or target. Prior works have relied on existing transformer models that lack the capability to prioritize targets effectively. Consequently, these models yield similar performance regardless of whether we utilize or disregard target information, undermining the task's significance. To address this challenge, we introduce Stanceformer, a target-aware transformer model that incorporates enhanced attention towards the targets during both training and inference. Specifically, we design a \textit{Target Awareness} matrix that increases the self-attention scores assigned to the targets. We demonstrate the efficacy of the Stanceformer with various BERT-based models, including state-of-the-art models and Large Language Models (LLMs), and evaluate its performance across three stance detection datasets, alongside a zero-shot dataset. Our approach Stanceformer not only provides superior performance but also generalizes even to other domains, such as Aspect-based Sentiment Analysis. We make the code publicly available.\footnote{\scriptsize\url{this https URL}}</li>
</ul>

<h3>Title: LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning</h3>
<ul>
<li><strong>Authors: </strong>Zhe Li, Weihao Yuan, Yisheng He, Lingteng Qiu, Shenhao Zhu, Xiaodong Gu, Weichao Shen, Yuan Dong, Zilong Dong, Laurence T. Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07093">https://arxiv.org/abs/2410.07093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07093">https://arxiv.org/pdf/2410.07093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07093]] LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning(https://arxiv.org/abs/2410.07093)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Language plays a vital role in the realm of human motion. Existing methods have largely depended on CLIP text embeddings for motion generation, yet they fall short in effectively aligning language and motion due to CLIP's pretraining on static image-text pairs. This work introduces LaMP, a novel Language-Motion Pretraining model, which transitions from a language-vision to a more suitable language-motion latent space. It addresses key limitations by generating motion-informative text embeddings, significantly enhancing the relevance and semantics of generated motion sequences. With LaMP, we advance three key tasks: text-to-motion generation, motion-text retrieval, and motion captioning through aligned language-motion representation learning. For generation, we utilize LaMP to provide the text condition instead of CLIP, and an autoregressive masked prediction is designed to achieve mask modeling without rank collapse in transformers. For retrieval, motion features from LaMP's motion transformer interact with query tokens to retrieve text features from the text transformer, and vice versa. For captioning, we finetune a large language model with the language-informative motion features to develop a strong motion captioning model. In addition, we introduce the LaMP-BertScore metric to assess the alignment of generated motions with textual descriptions. Extensive experimental results on multiple datasets demonstrate substantial improvements over previous methods across all three tasks. The code of our method will be made public.</li>
</ul>

<h3>Title: Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context</h3>
<ul>
<li><strong>Authors: </strong>Sangwon Yu, Ik-hwan Kim, Jongyoon Song, Saehyung Lee, Junsung Park, Sungroh Yoon</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07103">https://arxiv.org/abs/2410.07103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07103">https://arxiv.org/pdf/2410.07103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07103]] Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context(https://arxiv.org/abs/2410.07103)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-hop reasoning, which requires multi-step reasoning based on the supporting documents within a given context, remains challenging for large language models (LLMs). LLMs often struggle to filter out irrelevant documents within the context, and their performance is sensitive to the position of supporting documents within that context. In this paper, we identify an additional challenge: LLMs' performance is also sensitive to the order in which the supporting documents are presented. We refer to this as the misordered context problem. To address this issue, we propose a simple yet effective method called context repetition (CoRe), which involves prompting the model by repeatedly presenting the context to ensure the supporting documents are presented in the optimal order for the model. Using CoRe, we improve the F1 score by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p on a synthetic task. Additionally, CoRe helps mitigate the well-known "lost-in-the-middle" problem in LLMs and can be effectively combined with retrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.</li>
</ul>

<h3>Title: I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy</h3>
<ul>
<li><strong>Authors: </strong>Gian Maria Campedelli, Nicolò Penzo, Massimo Stefan, Roberto Dessì, Marco Guerini, Bruno Lepri, Jacopo Staiano</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07109">https://arxiv.org/abs/2410.07109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07109">https://arxiv.org/pdf/2410.07109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07109]] I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy(https://arxiv.org/abs/2410.07109)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Model (LLM)-based agents become increasingly autonomous and will more freely interact with each other, studying interactions between them becomes crucial to anticipate emergent phenomena and potential risks. Drawing inspiration from the widely popular Stanford Prison Experiment, we contribute to this line of research by studying interaction patterns of LLM agents in a context characterized by strict social hierarchy. We do so by specifically studying two types of phenomena: persuasion and anti-social behavior in simulated scenarios involving a guard and a prisoner agent who seeks to achieve a specific goal (i.e., obtaining additional yard time or escape from prison). Leveraging 200 experimental scenarios for a total of 2,000 machine-machine conversations across five different popular LLMs, we provide a set of noteworthy findings. We first document how some models consistently fail in carrying out a conversation in our multi-agent setup where power dynamics are at play. Then, for the models that were able to engage in successful interactions, we empirically show how the goal that an agent is set to achieve impacts primarily its persuasiveness, while having a negligible effect with respect to the agent's anti-social behavior. Third, we highlight how agents' personas, and particularly the guard's personality, drive both the likelihood of successful persuasion from the prisoner and the emergence of anti-social behaviors. Fourth, we show that even without explicitly prompting for specific personalities, anti-social behavior emerges by simply assigning agents' roles. These results bear implications for the development of interactive LLM agents as well as the debate on their societal impact.</li>
</ul>

<h3>Title: VHELM: A Holistic Evaluation of Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tony Lee, Haoqin Tu, Chi Heem Wong, Wenhao Zheng, Yiyang Zhou, Yifan Mai, Josselin Somerville Roberts, Michihiro Yasunaga, Huaxiu Yao, Cihang Xie, Percy Liang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07112">https://arxiv.org/abs/2410.07112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07112">https://arxiv.org/pdf/2410.07112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07112]] VHELM: A Holistic Evaluation of Vision Language Models(https://arxiv.org/abs/2410.07112)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various datasets to cover one or more of the 9 aspects: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 22 VLMs on 21 existing datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the fact that efficiency-focused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects. For transparency, we release the raw model generations and complete results on our website (this https URL). VHELM is intended to be a living benchmark, and we hope to continue adding new datasets and models over time.</li>
</ul>

<h3>Title: Personalized Visual Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Renjie Pi, Jianshu Zhang, Tianyang Han, Jipeng Zhang, Rui Pan, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07113">https://arxiv.org/abs/2410.07113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07113">https://arxiv.org/pdf/2410.07113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07113]] Personalized Visual Instruction Tuning(https://arxiv.org/abs/2410.07113)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in multimodal large language models (MLLMs) have demonstrated significant progress; however, these models exhibit a notable limitation, which we refer to as "face blindness". Specifically, they can engage in general conversations but fail to conduct personalized dialogues targeting at specific individuals. This deficiency hinders the application of MLLMs in personalized settings, such as tailored visual assistants on mobile devices, or domestic robots that need to recognize members of the family. In this paper, we introduce Personalized Visual Instruction Tuning (PVIT), a novel data curation and training framework designed to enable MLLMs to identify target individuals within an image and engage in personalized and coherent dialogues. Our approach involves the development of a sophisticated pipeline that autonomously generates training data containing personalized conversations. This pipeline leverages the capabilities of various visual experts, image generation models, and (multi-modal) large language models. To evaluate the personalized potential of MLLMs, we present a benchmark called P-Bench, which encompasses various question types with different levels of difficulty. The experiments demonstrate a substantial personalized performance enhancement after fine-tuning with our curated dataset.</li>
</ul>

<h3>Title: Exploring the Readiness of Prominent Small Language Models for the Democratization of Financial Literacy</h3>
<ul>
<li><strong>Authors: </strong>Tagore Rao Kosireddy, Jeffrey D. Wall, Evan Lucas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07118">https://arxiv.org/abs/2410.07118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07118">https://arxiv.org/pdf/2410.07118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07118]] Exploring the Readiness of Prominent Small Language Models for the Democratization of Financial Literacy(https://arxiv.org/abs/2410.07118)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The use of small language models (SLMs), herein defined as models with less than three billion parameters, is increasing across various domains and applications. Due to their ability to run on more accessible hardware and preserve user privacy, SLMs possess the potential to democratize access to language models for individuals of different socioeconomic status and with different privacy preferences. This study assesses several state-of-the-art SLMs (e.g., Apple's OpenELM, Microsoft's Phi, Google's Gemma, and the Tinyllama project) for use in the financial domain to support the development of financial literacy LMs. Democratizing access to quality financial information for those who are financially under educated is greatly needed in society, particularly as new financial markets and products emerge and participation in financial markets increases due to ease of access. We are the first to examine the use of open-source SLMs to democratize access to financial question answering capabilities for individuals and students. To this end, we provide an analysis of the memory usage, inference time, similarity comparisons to ground-truth answers, and output readability of prominent SLMs to determine which models are most accessible and capable of supporting access to financial information. We analyze zero-shot and few-shot learning variants of the models. The results suggest that some off-the-shelf SLMs merit further exploration and fine-tuning to prepare them for individual use, while others may have limits to their democratization.</li>
</ul>

<h3>Title: Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Adrian Galdran</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07124">https://arxiv.org/abs/2410.07124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07124">https://arxiv.org/pdf/2410.07124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07124]] Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation(https://arxiv.org/abs/2410.07124)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This short abstract describes a solution to the COSAS 2024 competition on Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation from histopathological image patches. The main challenge in the task of segmenting this type of cancer is a noticeable domain shift encountered when changing acquisition devices (microscopes) and also when tissue comes from different organs. The two tasks proposed in COSAS were to train on a dataset of images from three different organs, and then predict segmentations on data from unseen organs (dataset T1), and to train on a dataset of images acquired on three different scanners and then segment images acquired with another unseen microscope. We attempted to bridge the domain shift gap by experimenting with three different strategies: standard training for each dataset, pretraining on dataset T1 and then fine-tuning on dataset T2 (and vice-versa, a strategy we call \textit{Cross-Task Pretraining}), and training on the combination of dataset A and B. Our experiments showed that Cross-Task Pre-training is a more promising approach to domain generalization.</li>
</ul>

<h3>Title: Mental Disorders Detection in the Era of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gleb Kuzmin, Petr Strepetov, Maksim Stankevich, Ivan Smirnov, Artem Shelmanov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07129">https://arxiv.org/abs/2410.07129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07129">https://arxiv.org/pdf/2410.07129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07129]] Mental Disorders Detection in the Era of Large Language Models(https://arxiv.org/abs/2410.07129)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>This paper compares the effectiveness of traditional machine learning methods, encoder-based models, and large language models (LLMs) on the task of detecting depression and anxiety. Five datasets were considered, each differing in format and the method used to define the target pathology class. We tested AutoML models based on linguistic features, several variations of encoder-based Transformers such as BERT, and state-of-the-art LLMs as pathology classification models. The results demonstrated that LLMs outperform traditional methods, particularly on noisy and small datasets where training examples vary significantly in text length and genre. However, psycholinguistic features and encoder-based models can achieve performance comparable to language models when trained on texts from individuals with clinically confirmed depression, highlighting their potential effectiveness in targeted clinical applications.</li>
</ul>

<h3>Title: Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu Han, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07145">https://arxiv.org/abs/2410.07145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07145">https://arxiv.org/pdf/2410.07145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07145]] Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling(https://arxiv.org/abs/2410.07145)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>One essential advantage of recurrent neural networks (RNNs) over transformer-based language models is their linear computational complexity concerning the sequence length, which makes them much faster in handling long sequences during inference. However, most publicly available RNNs (e.g., Mamba and RWKV) are trained on sequences with less than 10K tokens, and their effectiveness in longer contexts remains largely unsatisfying so far. In this paper, we study the cause of the inability to process long context for RNNs and suggest critical mitigations. We examine two practical concerns when applying state-of-the-art RNNs to long contexts: (1) the inability to extrapolate to inputs longer than the training length and (2) the upper bound of memory capacity. Addressing the first concern, we first investigate *state collapse* (SC), a phenomenon that causes severe performance degradation on sequence lengths not encountered during training. With controlled experiments, we attribute this to overfitting due to the recurrent state being overparameterized for the training length. For the second concern, we train a series of Mamba-2 models on long documents to empirically estimate the recurrent state capacity in language modeling and passkey retrieval. Then, three SC mitigation methods are proposed to improve Mamba-2's length generalizability, allowing the model to process more than 1M tokens without SC. We also find that the recurrent state capacity in passkey retrieval scales exponentially to the state size, and we empirically train a Mamba-2 370M with near-perfect passkey retrieval accuracy on 256K context length. This suggests a promising future for RNN-based long-context modeling.</li>
</ul>

<h3>Title: FaceVid-1K: A Large-Scale High-Quality Multiracial Human Face Video Dataset</h3>
<ul>
<li><strong>Authors: </strong>Donglin Di, He Feng, Wenzhang Sun, Yongjia Ma, Hao Li, Wei Chen, Xiaofei Gou, Tonghua Su, Xun Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07151">https://arxiv.org/abs/2410.07151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07151">https://arxiv.org/pdf/2410.07151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07151]] FaceVid-1K: A Large-Scale High-Quality Multiracial Human Face Video Dataset(https://arxiv.org/abs/2410.07151)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generating talking face videos from various conditions has recently become a highly popular research area within generative tasks. However, building a high-quality face video generation model requires a well-performing pre-trained backbone, a key obstacle that universal models fail to adequately address. Most existing works rely on universal video or image generation models and optimize control mechanisms, but they neglect the evident upper bound in video quality due to the limited capabilities of the backbones, which is a result of the lack of high-quality human face video datasets. In this work, we investigate the unsatisfactory results from related studies, gather and trim existing public talking face video datasets, and additionally collect and annotate a large-scale dataset, resulting in a comprehensive, high-quality multiracial face collection named \textbf{FaceVid-1K}. Using this dataset, we craft several effective pre-trained backbone models for face video generation. Specifically, we conduct experiments with several well-established video generation models, including text-to-video, image-to-video, and unconditional video generation, under various settings. We obtain the corresponding performance benchmarks and compared them with those trained on public datasets to demonstrate the superiority of our dataset. These experiments also allow us to investigate empirical strategies for crafting domain-specific video generation tasks with cost-effective settings. We will make our curated dataset, along with the pre-trained talking face video generation models, publicly available as a resource contribution to hopefully advance the research field.</li>
</ul>

<h3>Title: Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Bohan Zeng, Ling Yang, Siyu Li, Jiaming Liu, Zixiang Zhang, Juanxi Tian, Kaixin Zhu, Yongzhen Guo, Fu-Yun Wang, Minkai Xu, Stefano Ermon, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07155">https://arxiv.org/abs/2410.07155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07155">https://arxiv.org/pdf/2410.07155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07155]] Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis(https://arxiv.org/abs/2410.07155)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have demonstrated exceptional capabilities in image and video generation, further improving the effectiveness of 4D synthesis. Existing 4D generation methods can generate high-quality 4D objects or scenes based on user-friendly conditions, benefiting the gaming and video industries. However, these methods struggle to synthesize significant object deformation of complex 4D transitions and interactions within scenes. To address this challenge, we propose Trans4D, a novel text-to-4D synthesis framework that enables realistic complex scene transitions. Specifically, we first use multi-modal large language models (MLLMs) to produce a physic-aware scene description for 4D scene initialization and effective transition timing planning. Then we propose a geometry-aware 4D transition network to realize a complex scene-level 4D transition based on the plan, which involves expressive geometrical object deformation. Extensive experiments demonstrate that Trans4D consistently outperforms existing state-of-the-art methods in generating 4D scenes with accurate and high-quality transitions, validating its effectiveness. Code: this https URL</li>
</ul>

<h3>Title: Quanda: An Interpretability Toolkit for Training Data Attribution Evaluation and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Dilyara Bareeva, Galip Ümit Yolcu, Anna Hedström, Niklas Schmolenski, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07158">https://arxiv.org/abs/2410.07158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07158">https://arxiv.org/pdf/2410.07158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07158]] Quanda: An Interpretability Toolkit for Training Data Attribution Evaluation and Beyond(https://arxiv.org/abs/2410.07158)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>In recent years, training data attribution (TDA) methods have emerged as a promising direction for the interpretability of neural networks. While research around TDA is thriving, limited effort has been dedicated to the evaluation of attributions. Similar to the development of evaluation metrics for traditional feature attribution approaches, several standalone metrics have been proposed to evaluate the quality of TDA methods across various contexts. However, the lack of a unified framework that allows for systematic comparison limits trust in TDA methods and stunts their widespread adoption. To address this research gap, we introduce Quanda, a Python toolkit designed to facilitate the evaluation of TDA methods. Beyond offering a comprehensive set of evaluation metrics, Quanda provides a uniform interface for seamless integration with existing TDA implementations across different repositories, thus enabling systematic benchmarking. The toolkit is user-friendly, thoroughly tested, well-documented, and available as an open-source library on PyPi and under this https URL.</li>
</ul>

<h3>Title: Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Chongyu Fan, Jiancheng Liu, Licong Lin, Jinghan Jia, Ruiqi Zhang, Song Mei, Sijia Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07163">https://arxiv.org/abs/2410.07163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07163">https://arxiv.org/pdf/2410.07163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07163]] Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning(https://arxiv.org/abs/2410.07163)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>In this work, we address the problem of large language model (LLM) unlearning, aiming to remove unwanted data influences and associated model capabilities (e.g., copyrighted data or harmful content generation) while preserving essential model utilities, without the need for retraining from scratch. Despite the growing need for LLM unlearning, a principled optimization framework remains lacking. To this end, we revisit the state-of-the-art approach, negative preference optimization (NPO), and identify the issue of reference model bias, which could undermine NPO's effectiveness, particularly when unlearning forget data of varying difficulty. Given that, we propose a simple yet effective unlearning optimization framework, called SimNPO, showing that 'simplicity' in removing the reliance on a reference model (through the lens of simple preference optimization) benefits unlearning. We also provide deeper insights into SimNPO's advantages, supported by analysis using mixtures of Markov chains. Furthermore, we present extensive experiments validating SimNPO's superiority over existing unlearning baselines in benchmarks like TOFU and MUSE, and robustness against relearning attacks. Codes are available at this https URL.</li>
</ul>

<h3>Title: AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation</h3>
<ul>
<li><strong>Authors: </strong>Yukang Cao, Liang Pan, Kai Han, Kwan-Yee K. Wong, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07164">https://arxiv.org/abs/2410.07164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07164">https://arxiv.org/pdf/2410.07164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07164]] AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation(https://arxiv.org/abs/2410.07164)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion models have led to significant improvements in the generation and animation of 4D full-body human-object interactions (HOI). Nevertheless, existing methods primarily focus on SMPL-based motion generation, which is limited by the scarcity of realistic large-scale interaction data. This constraint affects their ability to create everyday HOI scenes. This paper addresses this challenge using a zero-shot approach with a pre-trained diffusion model. Despite this potential, achieving our goals is difficult due to the diffusion model's lack of understanding of ''where'' and ''how'' objects interact with the human body. To tackle these issues, we introduce AvatarGO, a novel framework designed to generate animatable 4D HOI scenes directly from textual inputs. Specifically, 1) for the ''where'' challenge, we propose LLM-guided contact retargeting, which employs Lang-SAM to identify the contact body part from text prompts, ensuring precise representation of human-object spatial relations. 2) For the ''how'' challenge, we introduce correspondence-aware motion optimization that constructs motion fields for both human and object models using the linear blend skinning function from SMPL-X. Our framework not only generates coherent compositional motions, but also exhibits greater robustness in handling penetration issues. Extensive experiments with existing methods validate AvatarGO's superior generation and animation capabilities on a variety of human-object pairs and diverse poses. As the first attempt to synthesize 4D avatars with object interactions, we hope AvatarGO could open new doors for human-centric 4D content creation.</li>
</ul>

<h3>Title: Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, Jiajun Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07166">https://arxiv.org/abs/2410.07166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07166">https://arxiv.org/pdf/2410.07166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07166]] Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making(https://arxiv.org/abs/2410.07166)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We aim to evaluate Large Language Models (LLMs) for embodied decision making. While a significant body of work has been leveraging LLMs for decision making in embodied environments, we still lack a systematic understanding of their performance because they are usually applied in different domains, for different purposes, and built based on different inputs and outputs. Furthermore, existing evaluations tend to rely solely on a final success rate, making it difficult to pinpoint what ability is missing in LLMs and where the problem lies, which in turn blocks embodied agents from leveraging LLMs effectively and selectively. To address these limitations, we propose a generalized interface (Embodied Agent Interface) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules. Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics which break down evaluation into various types of errors, such as hallucination errors, affordance errors, various types of planning errors, etc. Overall, our benchmark offers a comprehensive assessment of LLMs' performance for different subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems, and providing insights for effective and selective use of LLMs in embodied decision making.</li>
</ul>

<h3>Title: Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate</h3>
<ul>
<li><strong>Authors: </strong>Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07167">https://arxiv.org/abs/2410.07167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07167">https://arxiv.org/pdf/2410.07167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07167]] Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate(https://arxiv.org/abs/2410.07167)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We present the Modality Integration Rate (MIR), an effective, robust, and generalized metric to indicate the multi-modal pre-training quality of Large Vision Language Models (LVLMs). Large-scale pre-training plays a critical role in building capable LVLMs, while evaluating its training quality without the costly supervised fine-tuning stage is under-explored. Loss, perplexity, and in-context evaluation results are commonly used pre-training metrics for Large Language Models (LLMs), while we observed that these metrics are less indicative when aligning a well-trained LLM with a new modality. Due to the lack of proper metrics, the research of LVLMs in the critical pre-training stage is hindered greatly, including the training data choice, efficient module design, etc. In this paper, we propose evaluating the pre-training quality from the inter-modal distribution distance perspective and present MIR, the Modality Integration Rate, which is 1) \textbf{Effective} to represent the pre-training quality and show a positive relation with the benchmark performance after supervised fine-tuning. 2) \textbf{Robust} toward different training/evaluation data. 3) \textbf{Generalize} across training configurations and architecture choices. We conduct a series of pre-training experiments to explore the effectiveness of MIR and observe satisfactory results that MIR is indicative about training data selection, training strategy schedule, and model architecture design to get better pre-training results. We hope MIR could be a helpful metric for building capable LVLMs and inspire the following research about modality alignment in different areas. Our code is at: this https URL.</li>
</ul>

<h3>Title: Sylber: Syllabic Embedding Representation of Speech from Raw Audio</h3>
<ul>
<li><strong>Authors: </strong>Cheol Jun Cho, Nicholas Lee, Akshat Gupta, Dhruv Agarwal, Ethan Chen, Alan W Black, Gopala K. Anumanchipalli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07168">https://arxiv.org/abs/2410.07168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07168">https://arxiv.org/pdf/2410.07168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07168]] Sylber: Syllabic Embedding Representation of Speech from Raw Audio(https://arxiv.org/abs/2410.07168)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Syllables are compositional units of spoken language that play a crucial role in human speech perception and production. However, current neural speech representations lack structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised model that regresses features on syllabic segments distilled from a teacher model which is an exponential moving average of the model in training. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) syllabic units better suited for lexical and syntactic understanding. We also train token-to-speech generative models with our syllabic units and show that fully intelligible speech can be reconstructed from these tokens. Lastly, we observe that categorical perception, a linguistic phenomenon of speech perception, emerges naturally in our model, making the embedding space more categorical and sparse than previous self-supervised learning approaches. Together, we present a novel self-supervised approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.</li>
</ul>

<h3>Title: IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Xinchen Zhang, Ling Yang, Guohao Li, Yaqi Cai, Jiake Xie, Yong Tang, Yujiu Yang, Mengdi Wang, Bin Cui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07171">https://arxiv.org/abs/2410.07171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07171">https://arxiv.org/pdf/2410.07171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07171]] IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation(https://arxiv.org/abs/2410.07171)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial relationships. This disparity highlights the need for an approach that can leverage the complementary strengths of various models to comprehensively improve the composition capability. To this end, we introduce IterComp, a novel framework that aggregates composition-aware model preferences from multiple models and employs an iterative feedback learning approach to enhance compositional generation. Specifically, we curate a gallery of six powerful open-source diffusion models and evaluate their three key compositional metrics: attribute binding, spatial relationships, and non-spatial relationships. Based on these metrics, we develop a composition-aware model preference dataset comprising numerous image-rank pairs to train composition-aware reward models. Then, we propose an iterative feedback learning method to enhance compositionality in a closed-loop manner, enabling the progressive self-refinement of both the base diffusion model and reward models over multiple iterations. Theoretical proof demonstrates the effectiveness and extensive experiments show our significant superiority over previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category object composition and complex semantic alignment. IterComp opens new research avenues in reward feedback learning for diffusion models and compositional generation. Code: this https URL</li>
</ul>

<h3>Title: Do better language models have crisper vision?</h3>
<ul>
<li><strong>Authors: </strong>Jona Ruthardt, Gertjan J. Burghouts, Serge Belongie, Yuki M. Asano</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07173">https://arxiv.org/abs/2410.07173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07173">https://arxiv.org/pdf/2410.07173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07173]] Do better language models have crisper vision?(https://arxiv.org/abs/2410.07173)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>How well do text-only Large Language Models (LLMs) grasp the visual world? As LLMs are increasingly used in computer vision, addressing this question becomes both fundamental and pertinent. However, existing studies have primarily focused on limited scenarios, such as their ability to generate visual content or cluster multimodal data. To this end, we propose the Visual Text Representation Benchmark (ViTeRB) to isolate key properties that make language models well-aligned with the visual world. With this, we identify large-scale decoder-based LLMs as ideal candidates for representing text in vision-centric contexts, counter to the current practice of utilizing text encoders. Building on these findings, we propose ShareLock, an ultra-lightweight CLIP-like model. By leveraging precomputable frozen features from strong vision and language models, ShareLock achieves an impressive 51% accuracy on ImageNet despite utilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU hour (or 10 hours including the precomputation of features) - orders of magnitude less than prior methods. Code will be released.</li>
</ul>

<h3>Title: Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, Sercan Ö. Arık</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07176">https://arxiv.org/abs/2410.07176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07176">https://arxiv.org/pdf/2410.07176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07176]] Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models(https://arxiv.org/abs/2410.07176)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG), while effective in integrating external knowledge to address the limitations of large language models (LLMs), can be undermined by imperfect retrieval, which may introduce irrelevant, misleading, or even malicious information. Despite its importance, previous studies have rarely explored the behavior of RAG through joint analysis on how errors from imperfect retrieval attribute and propagate, and how potential conflicts arise between the LLMs' internal knowledge and external sources. We find that imperfect retrieval augmentation might be inevitable and quite harmful, through controlled analysis under realistic conditions. We identify the knowledge conflicts between LLM-internal and external knowledge from retrieval as a bottleneck to overcome in the post-retrieval stage of RAG. To render LLMs resilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach that adaptively elicits essential information from LLMs' internal knowledge, iteratively consolidates internal and external knowledge with source-awareness, and finalizes the answer according to information reliability. Our experiments using Gemini and Claude demonstrate that Astute RAG significantly outperforms previous robustness-enhanced RAG methods. Notably, Astute RAG is the only approach that matches or exceeds the performance of LLMs without RAG under worst-case scenarios. Further analysis reveals that Astute RAG effectively resolves knowledge conflicts, improving the reliability and trustworthiness of RAG systems.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
