<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-26</h1>
<h3>Title: CliqueParcel: An Approach For Batching LLM Prompts That Jointly  Optimizes Efficiency And Faithfulness</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Liu, Tinghan Yang, Jennifer Neville</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14833">https://arxiv.org/abs/2402.14833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14833">https://arxiv.org/pdf/2402.14833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14833]] CliqueParcel: An Approach For Batching LLM Prompts That Jointly  Optimizes Efficiency And Faithfulness(https://arxiv.org/abs/2402.14833)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have become pivotal in recent research. However, during the inference process, LLMs still require substantial resources. In this paper, we propose CliqueParcel, a method designed to improve the efficiency of LLMs via prompt batching. Existing strategies to optimize inference efficiency often compromise on output quality, leading to a discounted output problem. This issue might result in reduced accuracy or outputs that are less detailed. CliqueParcel is our answer to this challenge. While ensuring accuracy and minimizing deviations from the original outputs (i.e., faithfulness), our method significantly improves efficiency during inference. To lay the groundwork, we first redefine efficiency measurements by excluding the reduction in running time due to shorter lengths. Then, we provide a comprehensive trade-off between efficiency and faithfulness to clarify the nature of the 'discounted output' problem. Within the CliqueParcel framework, we suggest multiple batching sub-methods and discuss the specific scenarios in which they can be applied. During evaluation, CliqueParcel is tested on eight widely recognized datasets, which can be classified into three types: reading comprehension, open-source question-answering, and reasoning. Our experiments explore the performance of CliqueParcel, including efficiency, faithfulness, and the trade-off between them. This work provides novel insights into inference efficiency and demonstrates promising performance.</li>
</ul>

<h3>Title: MSynFD: Multi-hop Syntax aware Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Liang Xiao, Qi Zhang, Chongyang Shi, Shoujin Wang, Usman Naseem, Liang Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14834">https://arxiv.org/abs/2402.14834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14834">https://arxiv.org/pdf/2402.14834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14834]] MSynFD: Multi-hop Syntax aware Fake News Detection(https://arxiv.org/abs/2402.14834)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The proliferation of social media platforms has fueled the rapid dissemination of fake news, posing threats to our real-life society. Existing methods use multimodal data or contextual information to enhance the detection of fake news by analyzing news content and/or its social context. However, these methods often overlook essential textual news content (articles) and heavily rely on sequential modeling and global attention to extract semantic information. These existing methods fail to handle the complex, subtle twists in news articles, such as syntax-semantics mismatches and prior biases, leading to lower performance and potential failure when modalities or social context are missing. To bridge these significant gaps, we propose a novel multi-hop syntax aware fake news detection (MSynFD) method, which incorporates complementary syntax information to deal with subtle twists in fake news. Specifically, we introduce a syntactical dependency graph and design a multi-hop subgraph aggregation mechanism to capture multi-hop syntax. It extends the effect of word perception, leading to effective noise filtering and adjacent relation enhancement. Subsequently, a sequential relative position-aware Transformer is designed to capture the sequential information, together with an elaborate keyword debiasing module to mitigate the prior bias. Extensive experimental results on two public benchmark datasets verify the effectiveness and superior performance of our proposed MSynFD over state-of-the-art detection models.</li>
</ul>

<h3>Title: MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge  Editing</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Li, Miaozeng Du, Chuanyi Zhang, Yongrui Chen, Nan Hu, Guilin Qi, Haiyun Jiang, Siyuan Cheng, Bozhong Tian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14835">https://arxiv.org/abs/2402.14835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14835">https://arxiv.org/pdf/2402.14835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14835]] MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge  Editing(https://arxiv.org/abs/2402.14835)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal knowledge editing represents a critical advancement in enhancing the capabilities of Multimodal Large Language Models (MLLMs). Despite its potential, current benchmarks predominantly focus on coarse-grained knowledge, leaving the intricacies of fine-grained (FG) multimodal entity knowledge largely unexplored. This gap presents a notable challenge, as FG entity recognition is pivotal for the practical deployment and effectiveness of MLLMs in diverse real-world scenarios. To bridge this gap, we introduce MIKE, a comprehensive benchmark and dataset specifically designed for the FG multimodal entity knowledge editing. MIKE encompasses a suite of tasks tailored to assess different perspectives, including Vanilla Name Answering, Entity-Level Caption, and Complex-Scenario Recognition. In addition, a new form of knowledge editing, Multi-step Editing, is introduced to evaluate the editing efficiency. Through our extensive evaluations, we demonstrate that the current state-of-the-art methods face significant challenges in tackling our proposed benchmark, underscoring the complexity of FG knowledge editing in MLLMs. Our findings spotlight the urgent need for novel approaches in this domain, setting a clear agenda for future research and development efforts within the community.</li>
</ul>

<h3>Title: Stealthy Attack on Large Language Model based Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Jinghao Zhang, Yuting Liu, Qiang Liu, Shu Wu, Guibing Guo, Liang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14836">https://arxiv.org/abs/2402.14836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14836">https://arxiv.org/pdf/2402.14836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14836]] Stealthy Attack on Large Language Model based Recommendation(https://arxiv.org/abs/2402.14836)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Recently, the powerful large language models (LLMs) have been instrumental in propelling the progress of recommender systems (RS). However, while these systems have flourished, their susceptibility to security threats has been largely overlooked. In this work, we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items. We demonstrate that attackers can significantly boost an item's exposure by merely altering its textual content during the testing phase, without requiring direct interference with the model's training process. Additionally, the attack is notably stealthy, as it does not affect the overall recommendation performance and the modifications to the text are subtle, making it difficult for users and platforms to detect. Our comprehensive experiments across four mainstream LLM-based recommendation models demonstrate the superior efficacy and stealthiness of our approach. Our work unveils a significant security gap in LLM-based recommendation systems and paves the way for future research on protecting these systems.</li>
</ul>

<h3>Title: An Empirical Categorization of Prompting Techniques for Large Language  Models: A Practitioner's Guide</h3>
<ul>
<li><strong>Authors: </strong>Oluwole Fagbohun, Rachel M. Harrison, Anton Dereventsov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14837">https://arxiv.org/abs/2402.14837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14837">https://arxiv.org/pdf/2402.14837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14837]] An Empirical Categorization of Prompting Techniques for Large Language  Models: A Practitioner's Guide(https://arxiv.org/abs/2402.14837)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Due to rapid advancements in the development of Large Language Models (LLMs), programming these models with prompts has recently gained significant attention. However, the sheer number of available prompt engineering techniques creates an overwhelming landscape for practitioners looking to utilize these tools. For the most efficient and effective use of LLMs, it is important to compile a comprehensive list of prompting techniques and establish a standardized, interdisciplinary categorization framework. In this survey, we examine some of the most well-known prompting techniques from both academic and practical viewpoints and classify them into seven distinct categories. We present an overview of each category, aiming to clarify their unique contributions and showcase their practical applications in real-world examples in order to equip fellow practitioners with a structured framework for understanding and categorizing prompting techniques tailored to their specific domains. We believe that this approach will help simplify the complex landscape of prompt engineering and enable more effective utilization of LLMs in various applications. By providing practitioners with a systematic approach to prompt categorization, we aim to assist in navigating the intricacies of effective prompt design for conversational pre-trained LLMs and inspire new possibilities in their respective fields.</li>
</ul>

<h3>Title: RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic  Features for Distinguishing AI-Generated and Human-Written Texts</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Heydari Rad, Farhan Farsi, Shayan Bali, Romina Etezadi, Mehrnoush Shamsfard</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14838">https://arxiv.org/abs/2402.14838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14838">https://arxiv.org/pdf/2402.14838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14838]] RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic  Features for Distinguishing AI-Generated and Human-Written Texts(https://arxiv.org/abs/2402.14838)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Nowadays, the usage of Large Language Models (LLMs) has increased, and LLMs have been used to generate texts in different languages and for different tasks. Additionally, due to the participation of remarkable companies such as Google and OpenAI, LLMs are now more accessible, and people can easily use them. However, an important issue is how we can detect AI-generated texts from human-written ones. In this article, we have investigated the problem of AI-generated text detection from two different aspects: semantics and syntax. Finally, we presented an AI model that can distinguish AI-generated texts from human-written ones with high accuracy on both multilingual and monolingual tasks using the M4 dataset. According to our results, using a semantic approach would be more helpful for detection. However, there is a lot of room for improvement in the syntactic approach, and it would be a good approach for future work.</li>
</ul>

<h3>Title: RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question  Answering and Clinical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Congyun Jin, Ming Zhang, Xiaowei Ma, Li Yujiao, Yingbo Wang, Yabo Jia, Yuliang Du, Tao Sun, Haowen Wang, Cong Fan, Jinjie Gu, Chenfei Chi, Xiangguo Lv, Fangzhou Li, Wei Xue, Yiran Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14840">https://arxiv.org/abs/2402.14840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14840">https://arxiv.org/pdf/2402.14840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14840]] RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question  Answering and Clinical Reasoning(https://arxiv.org/abs/2402.14840)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) and Large Multi-modal Models (LMMs) have shown potential in various medical applications, such as Intelligent Medical Diagnosis. Although impressive results have been achieved, we find that existing benchmarks do not reflect the complexity of real medical reports and specialized in-depth reasoning capabilities. In this work, we introduced RJUA-MedDQA, a comprehensive benchmark in the field of medical specialization, which poses several challenges: comprehensively interpreting imgage content across diverse challenging layouts, possessing numerical reasoning ability to identify abnormal indicators and demonstrating clinical reasoning ability to provide statements of disease diagnosis, status and advice based on medical contexts. We carefully design the data generation pipeline and proposed the Efficient Structural Restoration Annotation (ESRA) Method, aimed at restoring textual and tabular content in medical report images. This method substantially enhances annotation efficiency, doubling the productivity of each annotator, and yields a 26.8% improvement in accuracy. We conduct extensive evaluations, including few-shot assessments of 5 LMMs which are capable of solving Chinese medical QA tasks. To further investigate the limitations and potential of current LMMs, we conduct comparative experiments on a set of strong LLMs by using image-text generated by ESRA method. We report the performance of baselines and offer several observations: (1) The overall performance of existing LMMs is still limited; however LMMs more robust to low-quality and diverse-structured images compared to LLMs. (3) Reasoning across context and image content present significant challenges. We hope this benchmark helps the community make progress on these challenging tasks in multi-modal medical document understanding and facilitate its application in healthcare.</li>
</ul>

<h3>Title: Text Diffusion with Reinforced Conditioning</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14843">https://arxiv.org/abs/2402.14843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14843">https://arxiv.org/pdf/2402.14843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14843]] Text Diffusion with Reinforced Conditioning(https://arxiv.org/abs/2402.14843)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated exceptional capability in generating high-quality images, videos, and audio. Due to their adaptiveness in iterative refinement, they provide a strong potential for achieving better non-autoregressive sequence generation. However, existing text diffusion models still fall short in their performance due to a challenge in handling the discreteness of language. This paper thoroughly analyzes text diffusion models and uncovers two significant limitations: degradation of self-conditioning during training and misalignment between training and sampling. Motivated by our findings, we propose a novel Text Diffusion model called TREC, which mitigates the degradation with Reinforced Conditioning and the misalignment by Time-Aware Variance Scaling. Our extensive experiments demonstrate the competitiveness of TREC against autoregressive, non-autoregressive, and diffusion baselines. Moreover, qualitative analysis shows its advanced ability to fully utilize the diffusion process in refining samples.</li>
</ul>

<h3>Title: Purifying Large Language Models by Ensembling a Small Language Model</h3>
<ul>
<li><strong>Authors: </strong>Tianlin Li, Qian Liu, Tianyu Pang, Chao Du, Qing Guo, Yang Liu, Min Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14845">https://arxiv.org/abs/2402.14845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14845">https://arxiv.org/pdf/2402.14845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14845]] Purifying Large Language Models by Ensembling a Small Language Model(https://arxiv.org/abs/2402.14845)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>The emerging success of large language models (LLMs) heavily relies on collecting abundant training data from external (untrusted) sources. Despite substantial efforts devoted to data cleaning and curation, well-constructed LLMs have been reported to suffer from copyright infringement, data poisoning, and/or privacy violations, which would impede practical deployment of LLMs. In this study, we propose a simple and easily implementable method for purifying LLMs from the negative effects caused by uncurated data, namely, through ensembling LLMs with benign and small language models (SLMs). Aside from theoretical guarantees, we perform comprehensive experiments to empirically confirm the efficacy of ensembling LLMs with SLMs, which can effectively preserve the performance of LLMs while mitigating issues such as copyright infringement, data poisoning, and privacy violations.</li>
</ul>

<h3>Title: Stick to your Role! Stability of Personal Values Expressed in Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Grgur Kovač, Rémy Portelas, Masataka Sawayama, Peter Ford Dominey, Pierre-Yves Oudeyer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14846">https://arxiv.org/abs/2402.14846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14846">https://arxiv.org/pdf/2402.14846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14846]] Stick to your Role! Stability of Personal Values Expressed in Large  Language Models(https://arxiv.org/abs/2402.14846)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The standard way to study Large Language Models (LLMs) through benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLM's highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced LLMs from five families. Reusing methods from psychology, we study Rank-order stability on the population (interpersonal) level, and Ipsative stability on the individual (intrapersonal) level. We explore two settings: with and without instructing LLMs to simulate particular personalities. We observe similar trends in the stability of models and model families - Mixtral, Mistral and Qwen families being more stable than LLaMa-2 and Phi - over those two settings, two different simulated populations, and even in the downstream behavioral task. When instructed to simulate particular personas, LLMs exhibit low Rank-Order stability, and this stability further diminishes with conversation length. This highlights the need for future research directions on LLMs that can coherently simulate a diversity of personas, as well as how context-dependence can be studied in more thorough and efficient ways. This paper provides a foundational step in that direction, and, to our knowledge, it is the first study of value stability in LLMs.</li>
</ul>

<h3>Title: Same Task, More Tokens: the Impact of Input Length on the Reasoning  Performance of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mosh Levy, Alon Jacoby, Yoav Goldberg</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14848">https://arxiv.org/abs/2402.14848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14848">https://arxiv.org/pdf/2402.14848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14848]] Same Task, More Tokens: the Impact of Input Length on the Reasoning  Performance of Large Language Models(https://arxiv.org/abs/2402.14848)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that traditional perplexity metrics do not correlate with performance of LLMs' in long input reasoning tasks. We analyse our results and identify failure modes that can serve as useful guides for future research, potentially informing strategies to address the limitations observed in LLMs.</li>
</ul>

<h3>Title: Asynchronous and Segmented Bidirectional Encoding for NMT</h3>
<ul>
<li><strong>Authors: </strong>Jingpu Yang, Zehua Han, Mengyu Xiang, Helin Wang, Yuxiao Huang, Miao Fang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14849">https://arxiv.org/abs/2402.14849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14849">https://arxiv.org/pdf/2402.14849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14849]] Asynchronous and Segmented Bidirectional Encoding for NMT(https://arxiv.org/abs/2402.14849)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of Neural Machine Translation (NMT), enhancing translation efficiency and quality has become a focal point of research. Despite the commendable performance of general models such as the Transformer in various aspects, they still fall short in processing long sentences and fully leveraging bidirectional contextual information. This paper introduces an improved model based on the Transformer, implementing an asynchronous and segmented bidirectional decoding strategy aimed at elevating translation efficiency and accuracy. Compared to traditional unidirectional translations from left-to-right or right-to-left, our method demonstrates heightened efficiency and improved translation quality, particularly in handling long sentences. Experimental results on the IWSLT2017 dataset confirm the effectiveness of our approach in accelerating translation and increasing accuracy, especially surpassing traditional unidirectional strategies in long sentence translation. Furthermore, this study analyzes the impact of sentence length on decoding outcomes and explores the model's performance in various scenarios. The findings of this research not only provide an effective encoding strategy for the NMT field but also pave new avenues and directions for future studies.</li>
</ul>

<h3>Title: CHATATC: Large Language Model-Driven Conversational Agents for  Supporting Strategic Air Traffic Flow Management</h3>
<ul>
<li><strong>Authors: </strong>Sinan Abdulhak, Wayne Hubbard, Karthik Gopalakrishnan, Max Z. Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14850">https://arxiv.org/abs/2402.14850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14850">https://arxiv.org/pdf/2402.14850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14850]] CHATATC: Large Language Model-Driven Conversational Agents for  Supporting Strategic Air Traffic Flow Management(https://arxiv.org/abs/2402.14850)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative artificial intelligence (AI) and large language models (LLMs) have gained rapid popularity through publicly available tools such as ChatGPT. The adoption of LLMs for personal and professional use is fueled by the natural interactions between human users and computer applications such as ChatGPT, along with powerful summarization and text generation capabilities. Given the widespread use of such generative AI tools, in this work we investigate how these tools can be deployed in a non-safety critical, strategic traffic flow management setting. Specifically, we train an LLM, CHATATC, based on a large historical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023 and consisting of over 80,000 GDP implementations, revisions, and cancellations. We test the query and response capabilities of CHATATC, documenting successes (e.g., providing correct GDP rates, durations, and reason) and shortcomings (e.g,. superlative questions). We also detail the design of a graphical user interface for future users to interact and collaborate with the CHATATC conversational agent.</li>
</ul>

<h3>Title: HumanEval on Latest GPT Models -- 2024</h3>
<ul>
<li><strong>Authors: </strong>Daniel Li, Lincoln Murr</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14852">https://arxiv.org/abs/2402.14852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14852">https://arxiv.org/pdf/2402.14852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14852]] HumanEval on Latest GPT Models -- 2024(https://arxiv.org/abs/2402.14852)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In 2023, we are using the latest models of GPT-4 to advance program synthesis. The large language models have significantly improved the state-of-the-art for this purpose. To make these advancements more accessible, we have created a repository that connects these models to Huamn Eval. This dataset was initally developed to be used with a language model called CODEGEN on natural and programming language data. The utility of these trained models is showcased by demonstrating their competitive performance in zero-shot Python code generation on HumanEval tasks compared to previous state-of-the-art solutions. Additionally, this gives way to developing more multi-step paradigm synthesis. This benchmark features 160 diverse problem sets factorized into multistep prompts that our analysis shows significantly improves program synthesis over single-turn inputs. All code is open source at https://github.com/daniel442li/gpt-human-eval .</li>
</ul>

<h3>Title: A Dual-Prompting for Interpretable Mental Health Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hyolim Jeon, Dongje Yoo, Daeun Lee, Sejung Son, Seungbae Kim, Jinyoung Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14854">https://arxiv.org/abs/2402.14854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14854">https://arxiv.org/pdf/2402.14854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14854]] A Dual-Prompting for Interpretable Mental Health Language Models(https://arxiv.org/abs/2402.14854)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Despite the increasing demand for AI-based mental health monitoring tools, their practical utility for clinicians is limited by the lack of interpretability.The CLPsych 2024 Shared Task (Chim et al., 2024) aims to enhance the interpretability of Large Language Models (LLMs), particularly in mental health analysis, by providing evidence of suicidality through linguistic content. We propose a dual-prompting approach: (i) Knowledge-aware evidence extraction by leveraging the expert identity and a suicide dictionary with a mental health-specific LLM; and (ii) Evidence summarization by employing an LLM-based consistency evaluator. Comprehensive experiments demonstrate the effectiveness of combining domain-specific information, revealing performance improvements and the approach's potential to aid clinicians in assessing mental state progression.</li>
</ul>

<h3>Title: An LLM Maturity Model for Reliable and Transparent Text-to-Query</h3>
<ul>
<li><strong>Authors: </strong>Lei Yu (Expression), Abir Ray (Expression)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14855">https://arxiv.org/abs/2402.14855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14855">https://arxiv.org/pdf/2402.14855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14855]] An LLM Maturity Model for Reliable and Transparent Text-to-Query(https://arxiv.org/abs/2402.14855)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recognizing the imperative to address the reliability and transparency issues of Large Language Models (LLM), this work proposes an LLM maturity model tailored for text-to-query applications. This maturity model seeks to fill the existing void in evaluating LLMs in such applications by incorporating dimensions beyond mere correctness or accuracy. Moreover, this work introduces a real-world use case from the law enforcement domain and showcases QueryIQ, an LLM-powered, domain-specific text-to-query assistant to expedite user workflows and reveal hidden relationship in data.</li>
</ul>

<h3>Title: Comparing Inferential Strategies of Humans and Large Language Models in  Deductive Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Philipp Mondorf, Barbara Plank</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14856">https://arxiv.org/abs/2402.14856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14856">https://arxiv.org/pdf/2402.14856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14856]] Comparing Inferential Strategies of Humans and Large Language Models in  Deductive Reasoning(https://arxiv.org/abs/2402.14856)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Deductive reasoning plays a pivotal role in the formulation of sound and cohesive arguments. It allows individuals to draw conclusions that logically follow, given the truth value of the information provided. Recent progress in the domain of large language models (LLMs) has showcased their capability in executing deductive reasoning tasks. Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior. In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by LLMs, through a detailed evaluation of their responses to propositional logic problems. Our findings indicate that LLMs display reasoning patterns akin to those observed in humans, including strategies like $\textit{supposition following}$ or $\textit{chain construction}$. Moreover, our research demonstrates that the architecture and scale of the model significantly affect its preferred method of reasoning, with more advanced models tending to adopt strategies more frequently than less sophisticated ones. Importantly, we assert that a model's accuracy, that is the correctness of its final conclusion, does not necessarily reflect the validity of its reasoning process. This distinction underscores the necessity for more nuanced evaluation procedures in the field.</li>
</ul>

<h3>Title: Is the System Message Really Important to Jailbreaks in Large Language  Models?</h3>
<ul>
<li><strong>Authors: </strong>Xiaotian Zou, Yongkang Chen, Ke Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14857">https://arxiv.org/abs/2402.14857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14857">https://arxiv.org/pdf/2402.14857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14857]] Is the System Message Really Important to Jailbreaks in Large Language  Models?(https://arxiv.org/abs/2402.14857)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid evolution of Large Language Models (LLMs) has rendered them indispensable in modern society. While security measures are typically in place to align LLMs with human values prior to release, recent studies have unveiled a concerning phenomenon named "jailbreak." This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions. Existing research focuses on generating jailbreak prompts but our study aim to answer a different question: Is the system message really important to jailbreak in LLMs? To address this question, we conducted experiments in a stable GPT version gpt-3.5-turbo-0613 to generated jailbreak prompts with varying system messages: short, long, and none. We discover that different system messages have distinct resistances to jailbreak by experiments. Additionally, we explore the transferability of jailbreak across LLMs. This finding underscores the significant impact system messages can have on mitigating LLMs jailbreak. To generate system messages that are more resistant to jailbreak prompts, we propose System Messages Evolutionary Algorithms (SMEA). Through SMEA, we can get robust system messages population that demonstrate up to 98.9% resistance against jailbreak prompts. Our research not only bolsters LLMs security but also raises the bar for jailbreak, fostering advancements in this field of study.</li>
</ul>

<h3>Title: ChatEL: Entity Linking with Chatbots</h3>
<ul>
<li><strong>Authors: </strong>Yifan Ding, Qingkai Zeng, Tim Weninger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14858">https://arxiv.org/abs/2402.14858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14858">https://arxiv.org/pdf/2402.14858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14858]] ChatEL: Entity Linking with Chatbots(https://arxiv.org/abs/2402.14858)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Entity Linking (EL) is an essential and challenging task in natural language processing that seeks to link some text representing an entity within a document or sentence with its corresponding entry in a dictionary or knowledge base. Most existing approaches focus on creating elaborate contextual models that look for clues the words surrounding the entity-text to help solve the linking problem. Although these fine-tuned language models tend to work, they can be unwieldy, difficult to train, and do not transfer well to other domains. Fortunately, Large Language Models (LLMs) like GPT provide a highly-advanced solution to the problems inherent in EL models, but simply naive prompts to LLMs do not work well. In the present work, we define ChatEL, which is a three-step framework to prompt LLMs to return accurate results. Overall the ChatEL framework improves the average F1 performance across 10 datasets by more than 2%. Finally, a thorough error analysis shows many instances with the ground truth labels were actually incorrect, and the labels predicted by ChatEL were actually correct. This indicates that the quantitative results presented in this paper may be a conservative estimate of the actual performance. All data and code are available as an open-source package on GitHub at https://github.com/yifding/In_Context_EL.</li>
</ul>

<h3>Title: The Wolf Within: Covert Injection of Malice into MLLM Societies via an  MLLM Operative</h3>
<ul>
<li><strong>Authors: </strong>Zhen Tan, Chengshuai Zhao, Raha Moraffah, Yifan Li, Yu Kong, Tianlong Chen, Huan Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14859">https://arxiv.org/abs/2402.14859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14859">https://arxiv.org/pdf/2402.14859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14859]] The Wolf Within: Covert Injection of Malice into MLLM Societies via an  MLLM Operative(https://arxiv.org/abs/2402.14859)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Due to their unprecedented ability to process and respond to various types of data, Multimodal Large Language Models (MLLMs) are constantly defining the new boundary of Artificial General Intelligence (AGI). As these advanced generative models increasingly form collaborative networks for complex tasks, the integrity and security of these systems are crucial. Our paper, ``The Wolf Within'', explores a novel vulnerability in MLLM societies - the indirect propagation of malicious content. Unlike direct harmful output generation for MLLMs, our research demonstrates how a single MLLM agent can be subtly influenced to generate prompts that, in turn, induce other MLLM agents in the society to output malicious content. This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs. Our findings reveal that, with minimal or even no access to MLLMs' parameters, an MLLM agent, when manipulated to produce specific prompts or instructions, can effectively ``infect'' other agents within a society of MLLMs. This infection leads to the generation and circulation of harmful outputs, such as dangerous instructions or misinformation, across the society. We also show the transferability of these indirectly generated prompts, highlighting their possibility in propagating malice through inter-agent communication. This research provides a critical insight into a new dimension of threat posed by MLLMs, where a single agent can act as a catalyst for widespread malevolent influence. Our work underscores the urgent need for developing robust mechanisms to detect and mitigate such covert manipulations within MLLM societies, ensuring their safe and ethical utilization in societal applications. Our implementation is released at \url{https://github.com/ChengshuaiZhao0/The-Wolf-Within.git}.</li>
</ul>

<h3>Title: Ranking Large Language Models without Ground Truth</h3>
<ul>
<li><strong>Authors: </strong>Amit Dhurandhar, Rahul Nair, Moninder Singh, Elizabeth Daly, Karthikeyan Natesan Ramamurthy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14860">https://arxiv.org/abs/2402.14860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14860">https://arxiv.org/pdf/2402.14860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14860]] Ranking Large Language Models without Ground Truth(https://arxiv.org/abs/2402.14860)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generative tasks (summarization, multiple-choice, and dialog), our methods reliably recover close to true rankings without reference data. This points to a viable low-resource mechanism for practical use.</li>
</ul>

<h3>Title: SISSA: Real-time Monitoring of Hardware Functional Safety and  Cybersecurity with In-vehicle SOME/IP Ethernet Traffic</h3>
<ul>
<li><strong>Authors: </strong>Qi Liu, Xingyu Li, Ke Sun, Yufeng Li, Yanchen Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14862">https://arxiv.org/abs/2402.14862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14862">https://arxiv.org/pdf/2402.14862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14862]] SISSA: Real-time Monitoring of Hardware Functional Safety and  Cybersecurity with In-vehicle SOME/IP Ethernet Traffic(https://arxiv.org/abs/2402.14862)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Scalable service-Oriented Middleware over IP (SOME/IP) is an Ethernet communication standard protocol in the Automotive Open System Architecture (AUTOSAR), promoting ECU-to-ECU communication over the IP stack. However, SOME/IP lacks a robust security architecture, making it susceptible to potential attacks. Besides, random hardware failure of ECU will disrupt SOME/IP communication. In this paper, we propose SISSA, a SOME/IP communication traffic-based approach for modeling and analyzing in-vehicle functional safety and cyber security. Specifically, SISSA models hardware failures with the Weibull distribution and addresses five potential attacks on SOME/IP communication, including Distributed Denial-of-Services, Man-in-the-Middle, and abnormal communication processes, assuming a malicious user accesses the in-vehicle network. Subsequently, SISSA designs a series of deep learning models with various backbones to extract features from SOME/IP sessions among ECUs. We adopt residual self-attention to accelerate the model's convergence and enhance detection accuracy, determining whether an ECU is under attack, facing functional failure, or operating normally. Additionally, we have created and annotated a dataset encompassing various classes, including indicators of attack, functionality, and normalcy. This contribution is noteworthy due to the scarcity of publicly accessible datasets with such characteristics.Extensive experimental results show the effectiveness and efficiency of SISSA.</li>
</ul>

<h3>Title: DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing  Agents</h3>
<ul>
<li><strong>Authors: </strong>Kaijie Zhu, Jindong Wang, Qinlin Zhao, Ruochen Xu, Xing Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14865">https://arxiv.org/abs/2402.14865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14865">https://arxiv.org/pdf/2402.14865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14865]] DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing  Agents(https://arxiv.org/abs/2402.14865)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are also dynamically configurable, allowing multifaceted analysis. We conducted extensive evaluations using MPA and found that most LLMs achieve poorer performance, indicating room for improvement. Our multifaceted analysis demonstrated the strong correlation between the basic abilities and an implicit Matthew effect on model size, i.e., larger models possess stronger correlations of the abilities. MPA can also be used as a data augmentation approach to enhance LLMs.</li>
</ul>

<h3>Title: APTQ: Attention-aware Post-Training Mixed-Precision Quantization for  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Guan, Hantao Huang, Yupeng Su, Hong Huang, Ngai Wong, Hao Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14866">https://arxiv.org/abs/2402.14866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14866">https://arxiv.org/pdf/2402.14866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14866]] APTQ: Attention-aware Post-Training Mixed-Precision Quantization for  Large Language Models(https://arxiv.org/abs/2402.14866)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs, which considers not only the second-order information of each layer's weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model. We leverage the Hessian trace as a sensitivity metric for mixed-precision quantization, ensuring an informed precision reduction that retains model performance. Experiments show APTQ surpasses previous quantization methods, achieving an average of 4 bit width a 5.22 perplexity nearly equivalent to full precision in the C4 dataset. In addition, APTQ attains state-of-the-art zero-shot accuracy of 68.24\% and 70.48\% at an average bitwidth of 3.8 in LLaMa-7B and LLaMa-13B, respectively, demonstrating its effectiveness to produce high-quality quantized LLMs.</li>
</ul>

<h3>Title: Using Harmonics for Low-Cost Jamming</h3>
<ul>
<li><strong>Authors: </strong>Vasilis Ieropoulos, Eirini Anthi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14869">https://arxiv.org/abs/2402.14869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14869">https://arxiv.org/pdf/2402.14869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14869]] Using Harmonics for Low-Cost Jamming(https://arxiv.org/abs/2402.14869)</code><input type="text"></li>
<li><strong>Keywords: </strong>steal</a></li>
<li><strong>Abstract: </strong>The digitalisation of the modern schooling system has led to multiple schools and organisations buying similar hardware. Electronic equipment like wireless microphones, projectors, touchscreen displays etc., have been almost standardised with a few well-known brands leading the market. This has led to the adoption of common frequency ranges between brands with many sticking between 600-670 MHz. The popularity of low-cost computing devices like the Raspberry Pi which has been used in a plethora of applications has also taken the path of being used as low-cost transmitters. There have been many implementations where the Raspberry Pi has been used as the target device but few cases where the PI is the actual threat. In this paper, we explore the use of the Raspberry Pi as a stealth radio frequency jamming device to disable wireless conference microphones. Harmonics were used to achieve frequencies outside the Pi's transmission frequency by taking advantage of its unfiltered transmission.</li>
</ul>

<h3>Title: LLM Based Multi-Agent Generation of Semi-structured Documents from  Semantic Templates in the Public Administration Domain</h3>
<ul>
<li><strong>Authors: </strong>Emanuele Musumeci, Michele Brienza, Vincenzo Suriani, Daniele Nardi, Domenico Daniele Bloisi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14871">https://arxiv.org/abs/2402.14871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14871">https://arxiv.org/pdf/2402.14871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14871]] LLM Based Multi-Agent Generation of Semi-structured Documents from  Semantic Templates in the Public Administration Domain(https://arxiv.org/abs/2402.14871)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>In the last years' digitalization process, the creation and management of documents in various domains, particularly in Public Administration (PA), have become increasingly complex and diverse. This complexity arises from the need to handle a wide range of document types, often characterized by semi-structured forms. Semi-structured documents present a fixed set of data without a fixed format. As a consequence, a template-based solution cannot be used, as understanding a document requires the extraction of the data structure. The recent introduction of Large Language Models (LLMs) has enabled the creation of customized text output satisfying user requests. In this work, we propose a novel approach that combines the LLMs with prompt engineering and multi-agent systems for generating new documents compliant with a desired structure. The main contribution of this work concerns replacing the commonly used manual prompting with a task description generated by semantic retrieval from an LLM. The potential of this approach is demonstrated through a series of experiments and case studies, showcasing its effectiveness in real-world PA scenarios.</li>
</ul>

<h3>Title: Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts  Against Open-source LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxia Li, Siyuan Liang, Jiyi Zhang, Han Fang, Aishan Liu, Ee-Chien Chang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14872">https://arxiv.org/abs/2402.14872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14872">https://arxiv.org/pdf/2402.14872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14872]] Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts  Against Open-source LLMs(https://arxiv.org/abs/2402.14872)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), used in creative writing, code generation, and translation, generate text based on input sequences but are vulnerable to jailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak prompt methods use a combination of jailbreak templates followed by questions to ask to create jailbreak prompts. However, existing jailbreak prompt designs generally suffer from excessive semantic differences, resulting in an inability to resist defenses that use simple semantic metrics as thresholds. Jailbreak prompts are semantically more varied than the original questions used for queries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach that bypasses LLMs by generating jailbreak prompts that are semantically similar to the original question. We model the search for jailbreak prompts that satisfy both semantic similarity and jailbreak validity as a multi-objective optimization problem and employ a standardized set of genetic algorithms for generating eligible prompts. Compared to the baseline AutoDAN-GA, SMJ achieves attack success rates (ASR) that are at most 35.4% higher without ONION defense and 85.2% higher with ONION defense. SMJ's better performance in all three semantic meaningfulness metrics of Jailbreak Prompt, Similarity, and Outlier, also means that SMJ is resistant to defenses that use those metrics as thresholds.</li>
</ul>

<h3>Title: Technical Report on the Checkfor.ai AI-Generated Text Classifier</h3>
<ul>
<li><strong>Authors: </strong>Bradley Emi, Max Spero</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14873">https://arxiv.org/abs/2402.14873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14873">https://arxiv.org/pdf/2402.14873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14873]] Technical Report on the Checkfor.ai AI-Generated Text Classifier(https://arxiv.org/abs/2402.14873)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>We present the Checkfor.ai text classifier, a transformer-based neural network trained to distinguish text written by large language models from text written by humans. Checkfor.ai outperforms zero-shot methods such as DetectGPT as well as leading commercial AI detection tools with over 9 times lower error rates on a comprehensive benchmark comprised of ten text domains (student writing, creative writing, scientific writing, books, encyclopedias, news, email, scientific papers, short-form Q\&A) and 8 open- and closed-source large language models. We propose a training algorithm, hard negative mining with synthetic mirrors, that enables our classifier to achieve orders of magnitude lower false positive rates on high-data domains such as reviews. Finally, we show that Checkfor.ai is not biased against nonnative English speakers and generalizes to domains and models unseen during training.</li>
</ul>

<h3>Title: Distillation Contrastive Decoding: Improving LLMs Reasoning with  Contrastive Decoding and Distillation</h3>
<ul>
<li><strong>Authors: </strong>Phuc Phan, Hieu Tran, Long Phan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14874">https://arxiv.org/abs/2402.14874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14874">https://arxiv.org/pdf/2402.14874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14874]] Distillation Contrastive Decoding: Improving LLMs Reasoning with  Contrastive Decoding and Distillation(https://arxiv.org/abs/2402.14874)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose a straightforward approach called Distillation Contrastive Decoding (DCD) to enhance the reasoning capabilities of Large Language Models (LLMs) during inference. In contrast to previous approaches that relied on smaller amateur models or analysis of hidden state differences, DCD employs Contrastive Chain-of-thought Prompting and advanced distillation techniques, including Dropout and Quantization. This approach effectively addresses the limitations of Contrastive Decoding (CD), which typically requires both an expert and an amateur model, thus increasing computational resource demands. By integrating contrastive prompts with distillation, DCD obviates the need for an amateur model and reduces memory usage. Our evaluations demonstrate that DCD significantly enhances LLM performance across a range of reasoning benchmarks, surpassing both CD and existing methods in the GSM8K and StrategyQA datasets.</li>
</ul>

<h3>Title: What's in a Name? Auditing Large Language Models for Race and Gender  Bias</h3>
<ul>
<li><strong>Authors: </strong>Amit Haim, Alejandro Salinas, Julian Nyarko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14875">https://arxiv.org/abs/2402.14875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14875">https://arxiv.org/pdf/2402.14875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14875]] What's in a Name? Auditing Large Language Models for Race and Gender  Bias(https://arxiv.org/abs/2402.14875)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4. In our study, we elicit prompt the models for advice regarding an individual across a variety of scenarios, such as during car purchase negotiations or election outcome predictions. We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women. Names associated with Black women receive the least advantageous outcomes. The biases are consistent across 42 prompt templates and several models, indicating a systemic issue rather than isolated incidents. While providing numerical, decision-relevant anchors in the prompt can successfully counteract the biases, qualitative details have inconsistent effects and may even increase disparities. Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for harm against marginalized communities.</li>
</ul>

<h3>Title: Driving Generative Agents With Their Personality</h3>
<ul>
<li><strong>Authors: </strong>Lawrence J. Klinkert, Stephanie Buongiorno, Corey Clark</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14879">https://arxiv.org/abs/2402.14879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14879">https://arxiv.org/pdf/2402.14879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14879]] Driving Generative Agents With Their Personality(https://arxiv.org/abs/2402.14879)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>This research explores the potential of Large Language Models (LLMs) to utilize psychometric values, specifically personality information, within the context of video game character development. Affective Computing (AC) systems quantify a Non-Player character's (NPC) psyche, and an LLM can take advantage of the system's information by using the values for prompt generation. The research shows an LLM can consistently represent a given personality profile, thereby enhancing the human-like characteristics of game characters. Repurposing a human examination, the International Personality Item Pool (IPIP) questionnaire, to evaluate an LLM shows that the model can accurately generate content concerning the personality provided. Results show that the improvement of LLM, such as the latest GPT-4 model, can consistently utilize and interpret a personality to represent behavior.</li>
</ul>

<h3>Title: Automatic Histograms: Leveraging Language Models for Text Dataset  Exploration</h3>
<ul>
<li><strong>Authors: </strong>Emily Reif, Crystal Qian, James Wexler, Minsuk Kahng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14880">https://arxiv.org/abs/2402.14880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14880">https://arxiv.org/pdf/2402.14880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14880]] Automatic Histograms: Leveraging Language Models for Text Dataset  Exploration(https://arxiv.org/abs/2402.14880)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Making sense of unstructured text datasets is perennially difficult, yet increasingly relevant with Large Language Models. Data workers often rely on dataset summaries, especially distributions of various derived features. Some features, like toxicity or topics, are relevant to many datasets, but many interesting features are domain specific: instruments and genres for a music dataset, or diseases and symptoms for a medical dataset. Accordingly, data workers often run custom analyses for each dataset, which is cumbersome and difficult. We present AutoHistograms, a visualization tool leveragingLLMs. AutoHistograms automatically identifies relevant features, visualizes them with histograms, and allows the user to interactively query the dataset for categories of entities and create new histograms. In a user study with 10 data workers (n=10), we observe that participants can quickly identify insights and explore the data using AutoHistograms, and conceptualize a broad range of applicable use cases. Together, this tool and user study contributeto the growing field of LLM-assisted sensemaking tools.</li>
</ul>

<h3>Title: A Study on the Vulnerability of Test Questions against ChatGPT-based  Cheating</h3>
<ul>
<li><strong>Authors: </strong>Shanker Ram, Chen Qian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14881">https://arxiv.org/abs/2402.14881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14881">https://arxiv.org/pdf/2402.14881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14881]] A Study on the Vulnerability of Test Questions against ChatGPT-based  Cheating(https://arxiv.org/abs/2402.14881)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>ChatGPT is a chatbot that can answer text prompts fairly accurately, even performing very well on postgraduate-level questions. Many educators have found that their take-home or remote tests and exams are vulnerable to ChatGPT-based cheating because students may directly use answers provided by tools like ChatGPT. In this paper, we try to provide an answer to an important question: how well ChatGPT can answer test questions and how we can detect whether the questions of a test can be answered correctly by ChatGPT. We generated ChatGPT's responses to the MedMCQA dataset, which contains over 10,000 medical school entrance exam questions. We analyzed the responses and uncovered certain types of questions ChatGPT answers more inaccurately than others. In addition, we have created a basic natural language processing model to single out the most vulnerable questions to ChatGPT in a collection of questions or a sample exam. Our tool can be used by test-makers to avoid ChatGPT-vulnerable test questions.</li>
</ul>

<h3>Title: Deep Generative Model-based Synthesis of Four-bar Linkage Mechanisms  with Target Conditions</h3>
<ul>
<li><strong>Authors: </strong>Sumin Lee, Jihoon Kim, Namwoo Kang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14882">https://arxiv.org/abs/2402.14882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14882">https://arxiv.org/pdf/2402.14882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14882]] Deep Generative Model-based Synthesis of Four-bar Linkage Mechanisms  with Target Conditions(https://arxiv.org/abs/2402.14882)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Mechanisms are essential components designed to perform specific tasks in various mechanical systems. However, designing a mechanism that satisfies certain kinematic or quasi-static requirements is a challenging task. The kinematic requirements may include the workspace of a mechanism, while the quasi-static requirements of a mechanism may include its torque transmission, which refers to the ability of the mechanism to transfer power and torque effectively. In this paper, we propose a deep learning-based generative model for generating multiple crank-rocker four-bar linkage mechanisms that satisfy both the kinematic and quasi-static requirements aforementioned. The proposed model is based on a conditional generative adversarial network (cGAN) with modifications for mechanism synthesis, which is trained to learn the relationship between the requirements of a mechanism with respect to linkage lengths. The results demonstrate that the proposed model successfully generates multiple distinct mechanisms that satisfy specific kinematic and quasi-static requirements. To evaluate the novelty of our approach, we provide a comparison of the samples synthesized by the proposed cGAN, traditional cVAE and NSGA-II. Our approach has several advantages over traditional design methods. It enables designers to efficiently generate multiple diverse and feasible design candidates while exploring a large design space. Also, the proposed model considers both the kinematic and quasi-static requirements, which can lead to more efficient and effective mechanisms for real-world use, making it a promising tool for linkage mechanism design.</li>
</ul>

<h3>Title: Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Shen Li, Liuyi Yao, Jinyang Gao, Lan Zhang, Yaliang Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14883">https://arxiv.org/abs/2402.14883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14883">https://arxiv.org/pdf/2402.14883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14883]] Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning(https://arxiv.org/abs/2402.14883)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, watermark</a></li>
<li><strong>Abstract: </strong>To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named "Double-I watermark". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed "Double-I watermark" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification.</li>
</ul>

<h3>Title: COBIAS: Contextual Reliability in Bias Assessment</h3>
<ul>
<li><strong>Authors: </strong>Priyanshul Govil, Vamshi Krishna Bonagiri, Manas Gaur, Ponnurangam Kumaraguru, Sanorita Dey</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14889">https://arxiv.org/abs/2402.14889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14889">https://arxiv.org/pdf/2402.14889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14889]] COBIAS: Contextual Reliability in Bias Assessment(https://arxiv.org/abs/2402.14889)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are trained on inherently biased data. Previous works on debiasing models rely on benchmark datasets to measure model performance. However, these datasets suffer from several pitfalls due to the extremely subjective understanding of bias, highlighting a critical need for contextual exploration. We propose understanding the context of user inputs with consideration of the diverse situations in which input statements are possible. This approach would allow for frameworks that foster bias awareness rather than guardrails that hurt user engagement. Our contribution is twofold: (i) we create a dataset of 2287 stereotyped statements augmented with points for adding context; (ii) we develop the Context-Oriented Bias Indicator and Assessment Score (COBIAS) to assess statements' contextual reliability in measuring bias. Our metric is a significant predictor of the contextual reliability of bias-benchmark datasets ($\chi^2=71.02, p<2.2 \cdot 10^{-16})$. COBIAS can be used to create reliable datasets, resulting in an improvement in bias mitigation works.</li>
</ul>

<h3>Title: LLMBind: A Unified Modality-Task Integration Framework</h3>
<ul>
<li><strong>Authors: </strong>Bin Zhu, Peng Jin, Munan Ning, Bin Lin, Jinfa Huang, Qi Song, Mingjun Pan, Li Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14891">https://arxiv.org/abs/2402.14891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14891">https://arxiv.org/pdf/2402.14891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14891]] LLMBind: A Unified Modality-Task Integration Framework(https://arxiv.org/abs/2402.14891)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>While recent progress in multimodal large language models tackles various modality tasks, they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field. In this work, we take the initiative to explore and propose the LLMBind, a unified framework for modality task integration, which binds Large Language Models and corresponding pre-trained task models with task-specific tokens. Consequently, LLMBind can interpret inputs and produce outputs in versatile combinations of image, text, video, and audio. Specifically, we introduce a Mixture-of-Experts technique to enable effective learning for different multimodal tasks through collaboration among diverse experts. Furthermore, we create a multi-task dataset comprising 400k instruction data, which unlocks the ability for interactive visual generation and editing tasks. Extensive experiments show the effectiveness of our framework across various tasks, including image, video, audio generation, image segmentation, and image editing. More encouragingly, our framework can be easily extended to other modality tasks, showcasing the promising potential of creating a unified AI agent for modeling universal modalities.</li>
</ul>

<h3>Title: Chain-of-Thought Unfaithfulness as Disguised Accuracy</h3>
<ul>
<li><strong>Authors: </strong>Oliver Bentham, Nathan Stringham, Ana Marasović</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14897">https://arxiv.org/abs/2402.14897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14897">https://arxiv.org/pdf/2402.14897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14897]] Chain-of-Thought Unfaithfulness as Disguised Accuracy(https://arxiv.org/abs/2402.14897)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, arXiv:2307.13702 propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate their experimental setup with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, we discover that simply changing the order of answer choices in the prompt can reduce the metric by 73 percentage points. The faithfulness metric is also highly correlated ($R^2$ = 0.91) with accuracy, raising doubts about its validity as a construct for evaluating faithfulness.</li>
</ul>

<h3>Title: Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning  Meets Adversarial Images</h3>
<ul>
<li><strong>Authors: </strong>Zefeng Wang, Zhen Han, Shuo Chen, Fan Xue, Zifeng Ding, Xun Xiao, Volker Tresp, Philip Torr, Jindong Gu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14899">https://arxiv.org/abs/2402.14899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14899">https://arxiv.org/pdf/2402.14899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14899]] Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning  Meets Adversarial Images(https://arxiv.org/abs/2402.14899)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, explainability</a></li>
<li><strong>Abstract: </strong>Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand images. However, like traditional vision models, they are still vulnerable to adversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely explored on MLLMs, which not only improves model's performance, but also enhances model's explainability by giving intermediate reasoning steps. Nevertheless, there is still a lack of study regarding MLLMs' adversarial robustness with CoT and an understanding of what the rationale looks like when MLLMs infer wrong answers with adversarial images. Our research evaluates the adversarial robustness of MLLMs when employing CoT reasoning, finding that CoT marginally improves adversarial robustness against existing attack methods. Moreover, we introduce a novel stop-reasoning attack technique that effectively bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the alterations in CoT reasoning when MLLMs confront adversarial images, shedding light on their reasoning process under adversarial attacks.</li>
</ul>

<h3>Title: BIONIB: Blockchain-based IoT using Novelty Index in Bridge Health  Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Divija Swetha Gadiraju, Ryan McMaster, Saeed Eftekhar Azam, Deepak Khazanchi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14902">https://arxiv.org/abs/2402.14902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14902">https://arxiv.org/pdf/2402.14902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14902]] BIONIB: Blockchain-based IoT using Novelty Index in Bridge Health  Monitoring(https://arxiv.org/abs/2402.14902)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Bridge health monitoring becomes crucial with the deployment of IoT sensors. The challenge lies in securely storing vast amounts of data and extracting useful information to promptly identify unhealthy bridge conditions. To address this challenge, we propose BIONIB, wherein real-time IoT data is stored on the blockchain for monitoring bridges. One of the emerging blockchains, EOSIO is used because of its exceptional scaling capabilities for monitoring the health of bridges. The approach involves collecting data from IoT sensors and using an unsupervised machine learning-based technique called the Novelty Index (NI) to observe meaningful patterns in the data. Smart contracts of EOSIO are used in implementation because of their efficiency, security, and programmability, making them well-suited for handling complex transactions and automating processes within decentralized applications. BIONIB provides secure storage benefits of blockchain, as well as useful predictions based on the NI. Performance analysis uses real-time data collected from IoT sensors at the bridge in healthy and unhealthy states. The data is collected with extensive experimentation with different loads, climatic conditions, and the health of the bridge. The performance of BIONIB under varying numbers of sensors and various numbers of participating blockchain nodes is observed. We observe a tradeoff between throughput, latency, and computational resources. Storage efficiency can be increased by manifolds with a slight increase in latency caused by NI calculation. As latency is not a significant concern in bridge health applications, the results demonstrate that BIONIB has high throughput, parallel processing, and high security while efficiently scaled.</li>
</ul>

<h3>Title: Tokenization counts: the impact of tokenization on arithmetic in  frontier LLMs</h3>
<ul>
<li><strong>Authors: </strong>Aaditya K. Singh, DJ Strouse</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14903">https://arxiv.org/abs/2402.14903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14903">https://arxiv.org/pdf/2402.14903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14903]] Tokenization counts: the impact of tokenization on arithmetic in  frontier LLMs(https://arxiv.org/abs/2402.14903)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-right tokenization follow stereotyped error patterns, suggesting that model computations are systematic rather than approximate. We show that the model is able to convert between tokenizations easily, thus allowing chain-of-thought-inspired approaches to recover performance on left-to-right tokenized inputs. We also find the gap between tokenization directions decreases when models are scaled, possibly indicating that larger models are better able to override this tokenization-dependent inductive bias. In summary, our work performs the first study of how number tokenization choices lead to differences in model performance on arithmetic tasks, accompanied by a thorough analysis of error patterns. We hope this work inspires practitioners to more carefully ablate number tokenization-related choices when working towards general models of numerical reasoning.</li>
</ul>

<h3>Title: Watermarking Makes Language Models Radioactive</h3>
<ul>
<li><strong>Authors: </strong>Tom Sander, Pierre Fernandez, Alain Durmus, Matthijs Douze, Teddy Furon</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14904">https://arxiv.org/abs/2402.14904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14904">https://arxiv.org/pdf/2402.14904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14904]] Watermarking Makes Language Models Radioactive(https://arxiv.org/abs/2402.14904)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, membership infer, watermark</a></li>
<li><strong>Abstract: </strong>This paper investigates the radioactivity of LLM-generated texts, i.e. whether it is possible to detect that such input was used as training data. Conventional methods like membership inference can carry out this detection with some level of accuracy. We show that watermarked training data leaves traces easier to detect and much more reliable than membership inference. We link the contamination level to the watermark robustness, its proportion in the training set, and the fine-tuning process. We notably demonstrate that training on watermarked synthetic instructions can be detected with high confidence (p-value < 1e-5) even when as little as 5% of training text is watermarked. Thus, LLM watermarking, originally designed for detecting machine-generated text, gives the ability to easily identify if the outputs of a watermarked LLM were used to fine-tune another LLM.</li>
</ul>

<h3>Title: MobileLLM: Optimizing Sub-billion Parameter Language Models for  On-Device Use Cases</h3>
<ul>
<li><strong>Authors: </strong>Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas Chandra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14905">https://arxiv.org/abs/2402.14905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14905">https://arxiv.org/pdf/2402.14905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14905]] MobileLLM: Optimizing Sub-billion Parameter Language Models for  On-Device Use Cases(https://arxiv.org/abs/2402.14905)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a further accuracy enhancement of 0.7%/0.8% than MobileLLM 125M/350M. Moreover, MobileLLM model family shows significant improvements compared to previous sub-billion models on chat benchmarks, and demonstrates close correctness to LLaMA-v2 7B in API calling tasks, highlighting the capability of small models for common on-device use cases.</li>
</ul>

<h3>Title: Practical Insights into Knowledge Distillation for Pre-Trained Models</h3>
<ul>
<li><strong>Authors: </strong>Norah Alballa, Marco Canini</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14922">https://arxiv.org/abs/2402.14922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14922">https://arxiv.org/pdf/2402.14922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14922]] Practical Insights into Knowledge Distillation for Pre-Trained Models(https://arxiv.org/abs/2402.14922)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>This research investigates the enhancement of knowledge distillation (KD) processes in pre-trained models, an emerging field in knowledge transfer with significant implications for distributed training and federated learning environments. These environments benefit from reduced communication demands and accommodate various model architectures. Despite the adoption of numerous KD approaches for transferring knowledge among pre-trained models, a comprehensive understanding of KD's application in these scenarios is lacking. Our study conducts an extensive comparison of multiple KD techniques, including standard KD, tuned KD (via optimized temperature and weight parameters), deep mutual learning, and data partitioning KD. We assess these methods across various data distribution strategies to identify the most effective contexts for each. Through detailed examination of hyperparameter tuning, informed by extensive grid search evaluations, we pinpoint when adjustments are crucial to enhance model performance. This paper sheds light on optimal hyperparameter settings for distinct data partitioning scenarios and investigates KD's role in improving federated learning by minimizing communication rounds and expediting the training process. By filling a notable void in current research, our findings serve as a practical framework for leveraging KD in pre-trained models within collaborative and federated learning frameworks.</li>
</ul>

<h3>Title: Federated Fairness without Access to Sensitive Groups</h3>
<ul>
<li><strong>Authors: </strong>Afroditi Papadaki, Natalia Martinez, Martin Bertran, Guillermo Sapiro, Miguel Rodrigues</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14929">https://arxiv.org/abs/2402.14929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14929">https://arxiv.org/pdf/2402.14929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14929]] Federated Fairness without Access to Sensitive Groups(https://arxiv.org/abs/2402.14929)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, federate, fair</a></li>
<li><strong>Abstract: </strong>Current approaches to group fairness in federated learning assume the existence of predefined and labeled sensitive groups during training. However, due to factors ranging from emerging regulations to dynamics and location-dependency of protected groups, this assumption may be unsuitable in many real-world scenarios. In this work, we propose a new approach to guarantee group fairness that does not rely on any predefined definition of sensitive groups or additional labels. Our objective allows the federation to learn a Pareto efficient global model ensuring worst-case group fairness and it enables, via a single hyper-parameter, trade-offs between fairness and utility, subject only to a group size constraint. This implies that any sufficiently large subset of the population is guaranteed to receive at least a minimum level of utility performance from the model. The proposed objective encompasses existing approaches as special cases, such as empirical risk minimization and subgroup robustness objectives from centralized machine learning. We provide an algorithm to solve this problem in federation that enjoys convergence and excess risk guarantees. Our empirical results indicate that the proposed approach can effectively improve the worst-performing group that may be present without unnecessarily hurting the average performance, exhibits superior or comparable performance to relevant baselines, and achieves a large set of solutions with different fairness-utility trade-offs.</li>
</ul>

<h3>Title: SoK: Analyzing Adversarial Examples: A Framework to Study Adversary  Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Lucas Fenaux, Florian Kerschbaum</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14937">https://arxiv.org/abs/2402.14937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14937">https://arxiv.org/pdf/2402.14937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14937]] SoK: Analyzing Adversarial Examples: A Framework to Study Adversary  Knowledge(https://arxiv.org/abs/2402.14937)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Adversarial examples are malicious inputs to machine learning models that trigger a misclassification. This type of attack has been studied for close to a decade, and we find that there is a lack of study and formalization of adversary knowledge when mounting attacks. This has yielded a complex space of attack research with hard-to-compare threat models and attacks. We focus on the image classification domain and provide a theoretical framework to study adversary knowledge inspired by work in order theory. We present an adversarial example game, inspired by cryptographic games, to standardize attacks. We survey recent attacks in the image classification domain and classify their adversary's knowledge in our framework. From this systematization, we compile results that both confirm existing beliefs about adversary knowledge, such as the potency of information about the attacked model as well as allow us to derive new conclusions on the difficulty associated with the white-box and transferable threat models, for example, that transferable attacks might not be as difficult as previously thought.</li>
</ul>

<h3>Title: Enhancing Power Quality Event Classification with AI Transformer Models</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Mohammad Saber, Amr Youssef, Davor Svetinovic, Hatem Zeineldin, Deepa Kundur, Ehab El-Saadany</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14949">https://arxiv.org/abs/2402.14949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14949">https://arxiv.org/pdf/2402.14949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14949]] Enhancing Power Quality Event Classification with AI Transformer Models(https://arxiv.org/abs/2402.14949)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Recently, there has been a growing interest in utilizing machine learning for accurate classification of power quality events (PQEs). However, most of these studies are performed assuming an ideal situation, while in reality, we can have measurement noise, DC offset, and variations in the voltage signal's amplitude and frequency. Building on the prior PQE classification works using deep learning, this paper proposes a deep-learning framework that leverages attention-enabled Transformers as a tool to accurately classify PQEs under the aforementioned considerations. The proposed framework can operate directly on the voltage signals with no need for a separate feature extraction or calculation phase. Our results show that the proposed framework outperforms recently proposed learning-based techniques. It can accurately classify PQEs under the aforementioned conditions with an accuracy varying between 99.81%$-$91.43% depending on the signal-to-noise ratio, DC offsets, and variations in the signal amplitude and frequency.</li>
</ul>

<h3>Title: Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich  Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Hanqi Yan, Qinglin Zhu, Xinyu Wang, Lin Gui, Yulan He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14963">https://arxiv.org/abs/2402.14963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14963">https://arxiv.org/pdf/2402.14963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14963]] Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich  Reasoning(https://arxiv.org/abs/2402.14963)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large language models (LLMs) have the capability to iteratively reflect on their own outputs, recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback. Therefore, We propose Mirror, a Multiple-perspective self-reflection method for knowledge-rich reasoning, to avoid getting stuck at a particular reflection iteration. Mirror enables LLMs to reflect from multiple-perspective clues, achieved through a heuristic interaction between a Navigator and a Reasoner. It guides agents toward diverse yet plausibly reliable reasoning trajectory without access to ground truth by encouraging (1) diversity of directions generated by Navigator and (2) agreement among strategically induced perturbations in responses generated by the Reasoner. The experiments on five reasoning datasets demonstrate that Mirror's superiority over several contemporary self-reflection approaches. Additionally, the ablation study studies clearly indicate that our strategies alleviate the aforementioned challenges.</li>
</ul>

<h3>Title: Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment</h3>
<ul>
<li><strong>Authors: </strong>Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Muhao Chen, Junjie Hu, Yixuan Li, Bo Li, Chaowei Xiao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14968">https://arxiv.org/abs/2402.14968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14968">https://arxiv.org/pdf/2402.14968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14968]] Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment(https://arxiv.org/abs/2402.14968)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Despite the general capabilities of Large Language Models (LLMs) like GPT-4 and Llama-2, these models still request fine-tuning or adaptation with customized data when it comes to meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack), where incorporating just a few harmful examples into the fine-tuning dataset can significantly compromise the model safety. Though potential defenses have been proposed by incorporating safety examples into the fine-tuning dataset to reduce the safety issues, such approaches require incorporating a substantial amount of safety examples, making it inefficient. To effectively defend against the FJAttack with limited safety examples, we propose a Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In particular, we construct prefixed safety examples by integrating a secret prompt, acting as a "backdoor trigger", that is prefixed to safety examples. Our comprehensive experiments demonstrate that through the Backdoor Enhanced Safety Alignment with adding as few as 11 prefixed safety examples, the maliciously fine-tuned LLMs will achieve similar safety performance as the original aligned models. Furthermore, we also explore the effectiveness of our method in a more practical setting where the fine-tuning data consists of both FJAttack examples and the fine-tuning task data. Our method shows great efficacy in defending against FJAttack without harming the performance of fine-tuning tasks.</li>
</ul>

<h3>Title: MultiLS: A Multi-task Lexical Simplification Framework</h3>
<ul>
<li><strong>Authors: </strong>Kai North, Tharindu Ranasinghe, Matthew Shardlow, Marcos Zampieri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14972">https://arxiv.org/abs/2402.14972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14972">https://arxiv.org/pdf/2402.14972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14972]] MultiLS: A Multi-task Lexical Simplification Framework(https://arxiv.org/abs/2402.14972)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Lexical Simplification (LS) automatically replaces difficult to read words for easier alternatives while preserving a sentence's original meaning. LS is a precursor to Text Simplification with the aim of improving text accessibility to various target demographics, including children, second language learners, individuals with reading disabilities or low literacy. Several datasets exist for LS. These LS datasets specialize on one or two sub-tasks within the LS pipeline. However, as of this moment, no single LS dataset has been developed that covers all LS sub-tasks. We present MultiLS, the first LS framework that allows for the creation of a multi-task LS dataset. We also present MultiLS-PT, the first dataset to be created using the MultiLS framework. We demonstrate the potential of MultiLS-PT by carrying out all LS sub-tasks of (1). lexical complexity prediction (LCP), (2). substitute generation, and (3). substitute ranking for Portuguese. Model performances are reported, ranging from transformer-based models to more recent large language models (LLMs).</li>
</ul>

<h3>Title: GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data</h3>
<ul>
<li><strong>Authors: </strong>Lele Cao, Valentin Buchner, Zineb Senane, Fangkai Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14973">https://arxiv.org/abs/2402.14973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14973">https://arxiv.org/pdf/2402.14973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14973]] GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data(https://arxiv.org/abs/2402.14973)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.</li>
</ul>

<h3>Title: Unsupervised Domain Adaptation within Deep Foundation Latent Spaces</h3>
<ul>
<li><strong>Authors: </strong>Dmitry Kangin, Plamen Angelov</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14976">https://arxiv.org/abs/2402.14976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14976">https://arxiv.org/pdf/2402.14976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14976]] Unsupervised Domain Adaptation within Deep Foundation Latent Spaces(https://arxiv.org/abs/2402.14976)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The vision transformer-based foundation models, such as ViT or Dino-V2, are aimed at solving problems with little or no finetuning of features. Using a setting of prototypical networks, we analyse to what extent such foundation models can solve unsupervised domain adaptation without finetuning over the source or target domain. Through quantitative analysis, as well as qualitative interpretations of decision making, we demonstrate that the suggested method can improve upon existing baselines, as well as showcase the limitations of such approach yet to be solved.</li>
</ul>

<h3>Title: Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Hongbin Liu, Michael K. Reiter, Neil Zhenqiang Gong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14977">https://arxiv.org/abs/2402.14977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14977">https://arxiv.org/pdf/2402.14977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14977]] Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models(https://arxiv.org/abs/2402.14977)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Foundation model has become the backbone of the AI ecosystem. In particular, a foundation model can be used as a general-purpose feature extractor to build various downstream classifiers. However, foundation models are vulnerable to backdoor attacks and a backdoored foundation model is a single-point-of-failure of the AI ecosystem, e.g., multiple downstream classifiers inherit the backdoor vulnerabilities simultaneously. In this work, we propose Mudjacking, the first method to patch foundation models to remove backdoors. Specifically, given a misclassified trigger-embedded input detected after a backdoored foundation model is deployed, Mudjacking adjusts the parameters of the foundation model to remove the backdoor. We formulate patching a foundation model as an optimization problem and propose a gradient descent based method to solve it. We evaluate Mudjacking on both vision and language foundation models, eleven benchmark datasets, five existing backdoor attacks, and thirteen adaptive backdoor attacks. Our results show that Mudjacking can remove backdoor from a foundation model while maintaining its utility.</li>
</ul>

<h3>Title: Optimizing Language Models for Human Preferences is a Causal Inference  Problem</h3>
<ul>
<li><strong>Authors: </strong>Victoria Lin, Eli Ben-Michael, Louis-Philippe Morency</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14979">https://arxiv.org/abs/2402.14979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14979">https://arxiv.org/pdf/2402.14979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14979]] Optimizing Language Models for Human Preferences is a Causal Inference  Problem(https://arxiv.org/abs/2402.14979)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from direct outcome datasets, where each sample consists of a text and an associated numerical outcome measuring the reader's response. We first propose that language model optimization should be viewed as a causal problem to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method--causal preference optimization (CPO)--that solves an unbiased surrogate objective for the problem. We further extend CPO with doubly robust CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarantees on bias. Finally, we empirically demonstrate the effectiveness of (DR-)CPO in optimizing state-of-the-art LLMs for human preferences on direct outcome data, and we validate the robustness of DR-CPO under difficult confounding conditions.</li>
</ul>

<h3>Title: Privacy-Enhancing Collaborative Information Sharing through Federated  Learning -- A Case of the Insurance Industry</h3>
<ul>
<li><strong>Authors: </strong>Panyi Dong, Zhiyu Quan, Brandon Edwards, Shih-han Wang, Runhuan Feng, Tianyang Wang, Patrick Foley, Prashant Shah</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, q-fin.RM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14983">https://arxiv.org/abs/2402.14983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14983">https://arxiv.org/pdf/2402.14983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14983]] Privacy-Enhancing Collaborative Information Sharing through Federated  Learning -- A Case of the Insurance Industry(https://arxiv.org/abs/2402.14983)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>The report demonstrates the benefits (in terms of improved claims loss modeling) of harnessing the value of Federated Learning (FL) to learn a single model across multiple insurance industry datasets without requiring the datasets themselves to be shared from one company to another. The application of FL addresses two of the most pressing concerns: limited data volume and data variety, which are caused by privacy concerns, the rarity of claim events, the lack of informative rating factors, etc.. During each round of FL, collaborators compute improvements on the model using their local private data, and these insights are combined to update a global model. Such aggregation of insights allows for an increase to the effectiveness in forecasting claims losses compared to models individually trained at each collaborator. Critically, this approach enables machine learning collaboration without the need for raw data to leave the compute infrastructure of each respective data owner. Additionally, the open-source framework, OpenFL, that is used in our experiments is designed so that it can be run using confidential computing as well as with additional algorithmic protections against leakage of information via the shared model updates. In such a way, FL is implemented as a privacy-enhancing collaborative learning technique that addresses the challenges posed by the sensitivity and privacy of data in traditional machine learning solutions. This paper's application of FL can also be expanded to other areas including fraud detection, catastrophe modeling, etc., that have a similar need to incorporate data privacy into machine learning collaborations. Our framework and empirical results provide a foundation for future collaborations among insurers, regulators, academic researchers, and InsurTech experts.</li>
</ul>

<h3>Title: Verifiable Boosted Tree Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Stefano Calzavara, Lorenzo Cazzaro, Claudio Lucchese, Giulio Ermanno Pibiri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.LO, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14988">https://arxiv.org/abs/2402.14988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14988">https://arxiv.org/pdf/2402.14988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14988]] Verifiable Boosted Tree Ensembles(https://arxiv.org/abs/2402.14988)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Verifiable learning advocates for training machine learning models amenable to efficient security verification. Prior research demonstrated that specific classes of decision tree ensembles -- called large-spread ensembles -- allow for robustness verification in polynomial time against any norm-based attacker. This study expands prior work on verifiable learning from basic ensemble methods (i.e., hard majority voting) to advanced boosted tree ensembles, such as those trained using XGBoost or LightGBM. Our formal results indicate that robustness verification is achievable in polynomial time when considering attackers based on the $L_\infty$-norm, but remains NP-hard for other norm-based attackers. Nevertheless, we present a pseudo-polynomial time algorithm to verify robustness against attackers based on the $L_p$-norm for any $p \in \mathbb{N} \cup \{0\}$, which in practice grants excellent performance. Our experimental evaluation shows that large-spread boosted ensembles are accurate enough for practical adoption, while being amenable to efficient security verification.</li>
</ul>

<h3>Title: Stable Neural Stochastic Differential Equations in Analyzing Irregular  Time Series Data</h3>
<ul>
<li><strong>Authors: </strong>YongKyung Oh, Dongyoung Lim, Sungil Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14989">https://arxiv.org/abs/2402.14989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14989">https://arxiv.org/pdf/2402.14989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14989]] Stable Neural Stochastic Differential Equations in Analyzing Irregular  Time Series Data(https://arxiv.org/abs/2402.14989)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In this study, we propose three stable classes of Neural SDEs: Langevin-type SDE, Linear Noise SDE, and Geometric SDE. Then, we rigorously demonstrate their robustness in maintaining excellent performance under distribution shift, while effectively preventing overfitting. To assess the effectiveness of our approach, we conduct extensive experiments on four benchmark datasets for interpolation, forecasting, and classification tasks, and analyze the robustness of our methods with 30 public datasets under different missing rates. Our results demonstrate the efficacy of the proposed method in handling real-world irregular time series data.</li>
</ul>

<h3>Title: tinyBenchmarks: evaluating LLMs with fewer examples</h3>
<ul>
<li><strong>Authors: </strong>Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, Mikhail Yurochkin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14992">https://arxiv.org/abs/2402.14992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14992">https://arxiv.org/pdf/2402.14992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14992]] tinyBenchmarks: evaluating LLMs with fewer examples(https://arxiv.org/abs/2402.14992)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.</li>
</ul>

<h3>Title: Divide-or-Conquer? Which Part Should You Distill Your LLM?</h3>
<ul>
<li><strong>Authors: </strong>Zhuofeng Wu, He Bai, Aonan Zhang, Jiatao Gu, VG Vinod Vydiswaran, Navdeep Jaitly, Yizhe Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15000">https://arxiv.org/abs/2402.15000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15000">https://arxiv.org/pdf/2402.15000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15000]] Divide-or-Conquer? Which Part Should You Distill Your LLM?(https://arxiv.org/abs/2402.15000)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent methods have demonstrated that Large Language Models (LLMs) can solve reasoning tasks better when they are encouraged to solve subtasks of the main task first. In this paper we devise a similar strategy that breaks down reasoning tasks into a problem decomposition phase and a problem solving phase and show that the strategy is able to outperform a single stage solution. Further, we hypothesize that the decomposition should be easier to distill into a smaller model compared to the problem solving because the latter requires large amounts of domain knowledge while the former only requires learning general problem solving strategies. We propose methods to distill these two capabilities and evaluate their impact on reasoning outcomes and inference cost. We find that we can distill the problem decomposition phase and at the same time achieve good generalization across tasks, datasets, and models. However, it is harder to distill the problem solving capability without losing performance and the resulting distilled model struggles with generalization. These results indicate that by using smaller, distilled problem decomposition models in combination with problem solving LLMs we can achieve reasoning with cost-efficient inference and local adaptation.</li>
</ul>

<h3>Title: opp/ai: Optimistic Privacy-Preserving AI on Blockchain</h3>
<ul>
<li><strong>Authors: </strong>Cathie So, KD Conway, Xiaohang Yu, Suning Yao, Kartin Wong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15006">https://arxiv.org/abs/2402.15006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15006">https://arxiv.org/pdf/2402.15006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15006]] opp/ai: Optimistic Privacy-Preserving AI on Blockchain(https://arxiv.org/abs/2402.15006)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect</a></li>
<li><strong>Abstract: </strong>The convergence of Artificial Intelligence (AI) and blockchain technology is reshaping the digital world, offering decentralized, secure, and efficient AI services on blockchain platforms. Despite the promise, the high computational demands of AI on blockchain raise significant privacy and efficiency concerns. The Optimistic Privacy-Preserving AI (opp/ai) framework is introduced as a pioneering solution to these issues, striking a balance between privacy protection and computational efficiency. The framework integrates Zero-Knowledge Machine Learning (zkML) for privacy with Optimistic Machine Learning (opML) for efficiency, creating a hybrid model tailored for blockchain AI services. This study presents the opp/ai framework, delves into the privacy features of zkML, and assesses the framework's performance and adaptability across different scenarios.</li>
</ul>

<h3>Title: How Important Is Tokenization in French Medical Masked Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Yanis Labrak, Adrien Bazoge, Beatrice Daille, Mickael Rouvier, Richard Dufour</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15010">https://arxiv.org/abs/2402.15010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15010">https://arxiv.org/pdf/2402.15010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15010]] How Important Is Tokenization in French Medical Masked Language Models?(https://arxiv.org/abs/2402.15010)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tokenizers, and the role of morphological information in Indo-European languages remain insufficiently explored. This is particularly pertinent for biomedical terminology, characterized by specific rules governing morpheme combinations. Despite the agglutinative nature of biomedical terminology, existing language models do not explicitly incorporate this knowledge, leading to inconsistent tokenization strategies for common terms. In this paper, we seek to delve into the complexities of subword tokenization in French biomedical domain across a variety of NLP tasks and pinpoint areas where further enhancements can be made. We analyze classical tokenization algorithms, including BPE and SentencePiece, and introduce an original tokenization strategy that integrates morpheme-enriched word segmentation into existing tokenization methods.</li>
</ul>

<h3>Title: Unintended Impacts of LLM Alignment on Global Representation</h3>
<ul>
<li><strong>Authors: </strong>Michael J. Ryan, William Held, Diyi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15018">https://arxiv.org/abs/2402.15018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15018">https://arxiv.org/pdf/2402.15018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15018]] Unintended Impacts of LLM Alignment on Global Representation(https://arxiv.org/abs/2402.15018)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable preference tuning.</li>
</ul>

<h3>Title: Consistency-Guided Temperature Scaling Using Style and Content  Information for Out-of-Domain Calibration</h3>
<ul>
<li><strong>Authors: </strong>Wonjeong Choi, Jungwuk Park, Dong-Jun Han, Younghyun Park, Jaekyun Moon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15019">https://arxiv.org/abs/2402.15019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15019">https://arxiv.org/pdf/2402.15019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15019]] Consistency-Guided Temperature Scaling Using Style and Content  Information for Out-of-Domain Calibration(https://arxiv.org/abs/2402.15019)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Research interests in the robustness of deep neural networks against domain shifts have been rapidly increasing in recent years. Most existing works, however, focus on improving the accuracy of the model, not the calibration performance which is another important requirement for trustworthy AI systems. Temperature scaling (TS), an accuracy-preserving post-hoc calibration method, has been proven to be effective in in-domain settings, but not in out-of-domain (OOD) due to the difficulty in obtaining a validation set for the unseen domain beforehand. In this paper, we propose consistency-guided temperature scaling (CTS), a new temperature scaling strategy that can significantly enhance the OOD calibration performance by providing mutual supervision among data samples in the source domains. Motivated by our observation that over-confidence stemming from inconsistent sample predictions is the main obstacle to OOD calibration, we propose to guide the scaling process by taking consistencies into account in terms of two different aspects -- style and content -- which are the key components that can well-represent data samples in multi-domain settings. Experimental results demonstrate that our proposed strategy outperforms existing works, achieving superior OOD calibration performance on various datasets. This can be accomplished by employing only the source domains without compromising accuracy, making our scheme directly applicable to various trustworthy AI systems.</li>
</ul>

<h3>Title: Descripción automática de secciones delgadas de rocas: una  aplicación Web</h3>
<ul>
<li><strong>Authors: </strong>Stalyn Paucar, Christian Mejía-Escobar y Víctor Collaguazo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15039">https://arxiv.org/abs/2402.15039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15039">https://arxiv.org/pdf/2402.15039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15039]] Descripción automática de secciones delgadas de rocas: una  aplicación Web(https://arxiv.org/abs/2402.15039)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The identification and characterization of various rock types is one of the fundamental activities for geology and related areas such as mining, petroleum, environment, industry and construction. Traditionally, a human specialist is responsible for analyzing and explaining details about the type, composition, texture, shape and other properties using rock samples collected in-situ or prepared in a laboratory. The results become subjective based on experience, in addition to consuming a large investment of time and effort. The present proposal uses artificial intelligence techniques combining computer vision and natural language processing to generate a textual and verbal description from a thin section image of rock. We build a dataset of images and their respective textual descriptions for the training of a model that associates the relevant features of the image extracted by EfficientNetB7 with the textual description generated by a Transformer network, reaching an accuracy value of 0.892 and a BLEU value of 0.71. This model can be a useful resource for research, professional and academic work, so it has been deployed through a Web application for public use.</li>
</ul>

<h3>Title: KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, Shikun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15043">https://arxiv.org/abs/2402.15043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15043">https://arxiv.org/pdf/2402.15043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15043]] KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large  Language Models(https://arxiv.org/abs/2402.15043)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered "interactor" role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KIEval's effectiveness and generalization. We also reveal that data contamination brings no contribution or even negative effect to models' real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning.</li>
</ul>

<h3>Title: Fiducial Focus Augmentation for Facial Landmark Detection</h3>
<ul>
<li><strong>Authors: </strong>Purbayan Kar, Vishal Chudasama, Naoyuki Onoe, Pankaj Wasnik, Vineeth Balasubramanian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15044">https://arxiv.org/abs/2402.15044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15044">https://arxiv.org/pdf/2402.15044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15044]] Fiducial Focus Augmentation for Facial Landmark Detection(https://arxiv.org/abs/2402.15044)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Deep learning methods have led to significant improvements in the performance on the facial landmark detection (FLD) task. However, detecting landmarks in challenging settings, such as head pose changes, exaggerated expressions, or uneven illumination, continue to remain a challenge due to high variability and insufficient samples. This inadequacy can be attributed to the model's inability to effectively acquire appropriate facial structure information from the input images. To address this, we propose a novel image augmentation technique specifically designed for the FLD task to enhance the model's understanding of facial structures. To effectively utilize the newly proposed augmentation technique, we employ a Siamese architecture-based training mechanism with a Deep Canonical Correlation Analysis (DCCA)-based loss to achieve collective learning of high-level feature representations from two different views of the input images. Furthermore, we employ a Transformer + CNN-based network with a custom hourglass module as the robust backbone for the Siamese framework. Extensive experiments show that our approach outperforms multiple state-of-the-art approaches across various benchmark datasets.</li>
</ul>

<h3>Title: Unlocking the Power of Large Language Models for Entity Alignment</h3>
<ul>
<li><strong>Authors: </strong>Xuhui Jiang, Yinghan Shen, Zhichao Shi, Chengjin Xu, Wei Li, Zixuan Li, Jian Guo, Huawei Shen, Yuanzhuo Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15048">https://arxiv.org/abs/2402.15048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15048">https://arxiv.org/pdf/2402.15048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15048]] Unlocking the Power of Large Language Models for Entity Alignment(https://arxiv.org/abs/2402.15048)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG) data, playing a crucial role in data-driven AI applications. Traditional EA methods primarily rely on comparing entity embeddings, but their effectiveness is constrained by the limited input KG data and the capabilities of the representation learning techniques. Against this backdrop, we introduce ChatEA, an innovative framework that incorporates large language models (LLMs) to improve EA. To address the constraints of limited input KG data, ChatEA introduces a KG-code translation module that translates KG structures into a format understandable by LLMs, thereby allowing LLMs to utilize their extensive background knowledge to improve EA accuracy. To overcome the over-reliance on entity embedding comparisons, ChatEA implements a two-stage EA strategy that capitalizes on LLMs' capability for multi-step reasoning in a dialogue format, thereby enhancing accuracy while preserving efficiency. Our experimental results affirm ChatEA's superior performance, highlighting LLMs' potential in facilitating EA tasks.</li>
</ul>

<h3>Title: ToMBench: Benchmarking Theory of Mind in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhuang Chen, Jincenzi Wu, Jinfeng Zhou, Bosi Wen, Guanqun Bi, Gongyao Jiang, Yaru Cao, Mengting Hu, Yunghwei Lai, Zexuan Xiong, Minlie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15052">https://arxiv.org/abs/2402.15052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15052">https://arxiv.org/pdf/2402.15052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15052]] ToMBench: Benchmarking Theory of Mind in Large Language Models(https://arxiv.org/abs/2402.15052)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce ToMBench with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on ToMBench, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicating that LLMs have not achieved a human-level theory of mind yet. Our aim with ToMBench is to enable an efficient and effective evaluation of LLMs' ToM capabilities, thereby facilitating the development of LLMs with inherent social intelligence.</li>
</ul>

<h3>Title: Interpreting Context Look-ups in Transformers: Investigating  Attention-MLP Interactions</h3>
<ul>
<li><strong>Authors: </strong>Clement Neo, Shay B. Cohen, Fazl Barez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15055">https://arxiv.org/abs/2402.15055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15055">https://arxiv.org/pdf/2402.15055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15055]] Interpreting Context Look-ups in Transformers: Investigating  Attention-MLP Interactions(https://arxiv.org/abs/2402.15055)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate the interplay between attention heads and specialized "next-token" neurons in the Multilayer Perceptron that predict specific tokens. By prompting an LLM like GPT-4 to explain these model internals, we can elucidate attention mechanisms that activate certain next-token neurons. Our analysis identifies attention heads that recognize contexts relevant to predicting a particular token, activating the associated neuron through the residual connection. We focus specifically on heads in earlier layers consistently activating the same next-token neuron across similar prompts. Exploring these differential activation patterns reveals that heads that specialize for distinct linguistic contexts are tied to generating certain tokens. Overall, our method combines neural explanations and probing isolated components to illuminate how attention enables context-dependent, specialized processing in LLMs.</li>
</ul>

<h3>Title: On the Multi-turn Instruction Following for Conversational Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Yang Deng, Xuan Zhang, Wenxuan Zhang, Yifei Yuan, See-Kiong Ng, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15057">https://arxiv.org/abs/2402.15057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15057">https://arxiv.org/pdf/2402.15057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15057]] On the Multi-turn Instruction Following for Conversational Web Agents(https://arxiv.org/abs/2402.15057)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for LLM-powered agents to effectively engage with sequential user instructions in real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive experiments are conducted to benchmark the MT-Mind2Web dataset, and validate the effectiveness of the proposed method.</li>
</ul>

<h3>Title: Fine-tuning Large Language Models for Domain-specific Machine  Translation</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Zheng, Hanghai Hong, Xiaoli Wang, Jingsong Su, Yonggui Liang, Shikai Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15061">https://arxiv.org/abs/2402.15061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15061">https://arxiv.org/pdf/2402.15061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15061]] Fine-tuning Large Language Models for Domain-specific Machine  Translation(https://arxiv.org/abs/2402.15061)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have made significant progress in machine translation (MT). However, their potential in domain-specific MT remains under-explored. Current LLM-based MT systems still face several challenges. First, for LLMs with in-context learning, their effectiveness is highly sensitive to input translation examples, and processing them can increase inference costs. They often require extra post-processing due to over-generation. Second, LLMs with fine-tuning on domain-specific data often require high training costs for domain adaptation, and may weaken the zero-shot MT capabilities of LLMs due to over-specialization. The aforementioned methods can struggle to translate rare words in domain transfer scenarios. To address these challenges, this paper proposes a prompt-oriented fine-tuning method, denoted as LlamaIT, to effectively and efficiently fine-tune a general-purpose LLM for domain-specific MT tasks. First, we construct a task-specific mix-domain dataset, which is then used to fine-tune the LLM with LoRA. This can eliminate the need for input translation examples, post-processing, or over-specialization. By zero-shot prompting with instructions, we adapt the MT tasks to the target domain at inference time. To further elicit the MT capability for rare words, we construct new prompts by incorporating domain-specific bilingual vocabulary. We also conduct extensive experiments on both publicly available and self-constructed datasets. The results show that our LlamaIT can significantly enhance the domain-specific MT capabilities of the LLM, meanwhile preserving its zero-shot MT capabilities.</li>
</ul>

<h3>Title: Gotcha! Don't trick me with unanswerable questions! Self-aligning Large  Language Models for Responding to Unknown Questions</h3>
<ul>
<li><strong>Authors: </strong>Yang Deng, Yong Zhao, Moxin Li, See-Kiong Ng, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15062">https://arxiv.org/abs/2402.15062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15062">https://arxiv.org/pdf/2402.15062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15062]] Gotcha! Don't trick me with unanswerable questions! Self-aligning Large  Language Models for Responding to Unknown Questions(https://arxiv.org/abs/2402.15062)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the remarkable abilities of Large Language Models (LLMs) to answer questions, they often display a considerable level of overconfidence even when the question does not have a definitive answer. To avoid providing hallucinated answers to these unknown questions, existing studies typically investigate approaches to refusing to answer these questions. In this work, we propose a novel and scalable self-alignment method to utilize the LLM itself to enhance its response-ability to different types of unknown questions, being capable of not only refusing to answer but also providing explanation to the unanswerability of unknown questions. Specifically, the Self-Align method first employ a two-stage class-aware self-augmentation approach to generate a large amount of unknown question-response data. Then we conduct disparity-driven self-curation to select qualified data for fine-tuning the LLM itself for aligning the responses to unknown questions as desired. Experimental results on two datasets across four types of unknown questions validate the superiority of the Self-Align method over existing baselines in terms of three types of task formulation.</li>
</ul>

<h3>Title: Enhancing One-Shot Federated Learning Through Data and Ensemble  Co-Boosting</h3>
<ul>
<li><strong>Authors: </strong>Rong Dai, Yonggang Zhang, Ang Li, Tongliang Liu, Xun Yang, Bo Han</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15070">https://arxiv.org/abs/2402.15070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15070">https://arxiv.org/pdf/2402.15070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15070]] Enhancing One-Shot Federated Learning Through Data and Ensemble  Co-Boosting(https://arxiv.org/abs/2402.15070)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>One-shot Federated Learning (OFL) has become a promising learning paradigm, enabling the training of a global server model via a single communication round. In OFL, the server model is aggregated by distilling knowledge from all client models (the ensemble), which are also responsible for synthesizing samples for distillation. In this regard, advanced works show that the performance of the server model is intrinsically related to the quality of the synthesized data and the ensemble model. To promote OFL, we introduce a novel framework, Co-Boosting, in which synthesized data and the ensemble model mutually enhance each other progressively. Specifically, Co-Boosting leverages the current ensemble model to synthesize higher-quality samples in an adversarial manner. These hard samples are then employed to promote the quality of the ensemble model by adjusting the ensembling weights for each client model. Consequently, Co-Boosting periodically achieves high-quality data and ensemble models. Extensive experiments demonstrate that Co-Boosting can substantially outperform existing baselines under various settings. Moreover, Co-Boosting eliminates the need for adjustments to the client's local training, requires no additional data or model transmission, and allows client models to have heterogeneous architectures.</li>
</ul>

<h3>Title: AttributionBench: How Hard is Automatic Attribution Evaluation?</h3>
<ul>
<li><strong>Authors: </strong>Yifei Li, Xiang Yue, Zeyi Liao, Huan Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15089">https://arxiv.org/abs/2402.15089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15089">https://arxiv.org/pdf/2402.15089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15089]] AttributionBench: How Hard is Automatic Attribution Evaluation?(https://arxiv.org/abs/2402.15089)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Modern generative search engines enhance the reliability of large language model (LLM) responses by providing cited evidence. However, evaluating the answer's attribution, i.e., whether every claim within the generated responses is fully supported by its cited evidence, remains an open problem. This verification, traditionally dependent on costly human evaluation, underscores the urgent need for automatic attribution evaluation methods. To bridge the gap in the absence of standardized benchmarks for these methods, we present AttributionBench, a comprehensive benchmark compiled from various existing attribution datasets. Our extensive experiments on AttributionBench reveal the challenges of automatic attribution evaluation, even for state-of-the-art LLMs. Specifically, our findings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error cases indicates that a majority of failures stem from the model's inability to process nuanced information, and the discrepancy between the information the model has access to and that human annotators do.</li>
</ul>

<h3>Title: Multimodal Transformer With a Low-Computational-Cost Guarantee</h3>
<ul>
<li><strong>Authors: </strong>Sungjin Park, Edward Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15096">https://arxiv.org/abs/2402.15096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15096">https://arxiv.org/pdf/2402.15096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15096]] Multimodal Transformer With a Low-Computational-Cost Guarantee(https://arxiv.org/abs/2402.15096)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based models have significantly improved performance across a range of multimodal understanding tasks, such as visual question answering and action recognition. However, multimodal Transformers significantly suffer from a quadratic complexity of the multi-head attention with the input sequence length, especially as the number of modalities increases. To address this, we introduce Low-Cost Multimodal Transformer (LoCoMT), a novel multimodal attention mechanism that aims to reduce computational cost during training and inference with minimal performance loss. Specifically, by assigning different multimodal attention patterns to each attention head, LoCoMT can flexibly control multimodal signals and theoretically ensures a reduced computational cost compared to existing multimodal Transformer variants. Experimental results on two multimodal datasets, namely Audioset and MedVidCL demonstrate that LoCoMT not only reduces GFLOPs but also matches or even outperforms established models.</li>
</ul>

<h3>Title: A First Look at GPT Apps: Landscape and Vulnerability</h3>
<ul>
<li><strong>Authors: </strong>Zejun Zhang, Li Zhang, Xin Yuan, Anlan Zhang, Mengwei Xu, Feng Qian</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15105">https://arxiv.org/abs/2402.15105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15105">https://arxiv.org/pdf/2402.15105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15105]] A First Look at GPT Apps: Landscape and Vulnerability(https://arxiv.org/abs/2402.15105)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, large language model</a></li>
<li><strong>Abstract: </strong>With the advancement of Large Language Models (LLMs), increasingly sophisticated and powerful GPTs are entering the market. Despite their popularity, the LLM ecosystem still remains unexplored. Additionally, LLMs' susceptibility to attacks raises concerns over safety and plagiarism. Thus, in this work, we conduct a pioneering exploration of GPT stores, aiming to study vulnerabilities and plagiarism within GPT applications. To begin with, we conduct, to our knowledge, the first large-scale monitoring and analysis of two stores, an unofficial GPTStore.AI, and an official OpenAI GPT Store. Then, we propose a TriLevel GPT Reversing (T-GR) strategy for extracting GPT internals. To complete these two tasks efficiently, we develop two automated tools: one for web scraping and another designed for programmatically interacting with GPTs. Our findings reveal a significant enthusiasm among users and developers for GPT interaction and creation, as evidenced by the rapid increase in GPTs and their creators. However, we also uncover a widespread failure to protect GPT internals, with nearly 90% of system prompts easily accessible, leading to considerable plagiarism and duplication among GPTs.</li>
</ul>

<h3>Title: Chu-ko-nu: A Reliable, Efficient, and Anonymously Authentication-Enabled  Realization for Multi-Round Secure Aggregation in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Kaiping Cui, Xia Feng, Liangmin Wang, Haiqin Wu, Xiaoyu Zhang, Boris Düdder</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15111">https://arxiv.org/abs/2402.15111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15111">https://arxiv.org/pdf/2402.15111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15111]] Chu-ko-nu: A Reliable, Efficient, and Anonymously Authentication-Enabled  Realization for Multi-Round Secure Aggregation in Federated Learning(https://arxiv.org/abs/2402.15111)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, attack, federate</a></li>
<li><strong>Abstract: </strong>Secure aggregation enables federated learning (FL) to perform collaborative training of clients from local gradient updates without exposing raw data. However, existing secure aggregation schemes inevitably perform an expensive fresh setup per round because each client needs to establish fresh input-independent secrets over different rounds. The latest research, Flamingo (S&P 2023), designed a share-transfer-based reusable secret key to support the server continuously performing multiple rounds of aggregation. Nevertheless, the share transfer mechanism it proposed can only be achieved with P probability, which has limited reliability. To tackle the aforementioned problems, we propose a more reliable and anonymously authenticated scheme called Chu-ko-nu for multi-round secure aggregation. Specifically, in terms of share transfer, Chu-ko-nu breaks the probability P barrier by supplementing a redistribution process of secret key components (the sum of all components is the secret key), thus ensuring the reusability of the secret key. Based on this reusable secret key, Chu-ko-nu can efficiently perform consecutive aggregation in the following rounds. Furthermore, considering the client identity authentication and privacy protection issue most approaches ignore, Chu-ko-nu introduces a zero-knowledge proof-based authentication mechanism. It can support clients anonymously participating in FL training and enables the server to authenticate clients effectively in the presence of various attacks. Rigorous security proofs and extensive experiments demonstrated that Chu-ko-nu can provide reliable and anonymously authenticated aggregation for FL with low aggregation costs, at least a 21.02% reduction compared to the state-of-the-art schemes.</li>
</ul>

<h3>Title: Large Multimodal Agents: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, Guanbin Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15116">https://arxiv.org/abs/2402.15116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15116">https://arxiv.org/pdf/2402.15116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15116]] Large Multimodal Agents: A Survey(https://arxiv.org/abs/2402.15116)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved superior performance in powering text-based AI agents, endowing them with decision-making and reasoning abilities akin to humans. Concurrently, there is an emerging research trend focused on extending these LLM-powered AI agents into the multimodal domain. This extension enables AI agents to interpret and respond to diverse multimodal user queries, thereby handling more intricate and nuanced tasks. In this paper, we conduct a systematic review of LLM-driven multimodal agents, which we refer to as large multimodal agents ( LMAs for short). First, we introduce the essential components involved in developing LMAs and categorize the current body of research into four distinct types. Subsequently, we review the collaborative frameworks integrating multiple LMAs , enhancing collective efficacy. One of the critical challenges in this field is the diverse evaluation methods used across existing studies, hindering effective comparison among different LMAs . Therefore, we compile these evaluation methodologies and establish a comprehensive framework to bridge the gaps. This framework aims to standardize evaluations, facilitating more meaningful comparisons. Concluding our review, we highlight the extensive applications of LMAs and propose possible future research directions. Our discussion aims to provide valuable insights and guidelines for future research in this rapidly evolving field. An up-to-date resource list is available at https://github.com/jun0wanan/awesome-large-multimodal-agents.</li>
</ul>

<h3>Title: Fine-tuning CLIP Text Encoders with Two-step Paraphrasing</h3>
<ul>
<li><strong>Authors: </strong>Hyunjae Kim, Seunghyun Yoon, Trung Bui, Handong Zhao, Quan Tran, Franck Dernoncourt, Jaewoo Kang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15120">https://arxiv.org/abs/2402.15120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15120">https://arxiv.org/pdf/2402.15120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15120]] Fine-tuning CLIP Text Encoders with Two-step Paraphrasing(https://arxiv.org/abs/2402.15120)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Contrastive language-image pre-training (CLIP) models have demonstrated considerable success across various vision-language tasks, such as text-to-image retrieval, where the model is required to effectively process natural language input to produce an accurate visual output. However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications. In this study, we introduce a straightforward fine-tuning approach to enhance the representations of CLIP models for paraphrases. Our approach involves a two-step paraphrase generation process, where we automatically create two categories of paraphrases from web-scale image captions by leveraging large language models. Subsequently, we fine-tune the CLIP text encoder using these generated paraphrases while freezing the image encoder. Our resulting model, which we call ParaCLIP, exhibits significant improvements over baseline CLIP models across various tasks, including paraphrased retrieval (with rank similarity scores improved by up to 2.0% and 5.6%), Visual Genome Relation and Attribution, as well as seven semantic textual similarity tasks.</li>
</ul>

<h3>Title: Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question  Answering with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guanming Xiong, Junwei Bao, Wen Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15131">https://arxiv.org/abs/2402.15131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15131">https://arxiv.org/pdf/2402.15131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15131]] Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question  Answering with Large Language Models(https://arxiv.org/abs/2402.15131)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study explores the realm of knowledge-base question answering (KBQA). KBQA is considered a challenging task, particularly in parsing intricate questions into executable logical forms. Traditional semantic parsing (SP)-based methods require extensive data annotations, which result in significant costs. Recently, the advent of few-shot in-context learning, powered by large language models (LLMs), has showcased promising capabilities. Yet, fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive-KBQA, a framework designed to generate logical forms through direct interaction with knowledge bases (KBs). Within this framework, we have developed three generic APIs for KB interaction. For each category of complex question, we devised exemplars to guide LLMs through the reasoning processes. Our method achieves competitive results on the WebQuestionsSP, ComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of examples (shots). Importantly, our approach supports manual intervention, allowing for the iterative refinement of LLM outputs. By annotating a dataset with step-wise reasoning processes, we showcase our model's adaptability and highlight its potential for contributing significant enhancements to the field.</li>
</ul>

<h3>Title: Improving Sentence Embeddings with an Automatically Generated NLI  Dataset</h3>
<ul>
<li><strong>Authors: </strong>Soma Sato, Hayato Tsukagoshi, Ryohei Sasano, Koichi Takeda</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15132">https://arxiv.org/abs/2402.15132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15132">https://arxiv.org/pdf/2402.15132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15132]] Improving Sentence Embeddings with an Automatically Generated NLI  Dataset(https://arxiv.org/abs/2402.15132)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Decoder-based large language models (LLMs) have shown high performance on many tasks in natural language processing. This is also true for sentence embedding learning, where a decoder-based model, PromptEOL, has achieved the best performance on semantic textual similarity (STS) tasks. However, PromptEOL makes great use of fine-tuning with a manually annotated natural language inference (NLI) dataset. We aim to improve sentence embeddings learned in an unsupervised setting by automatically generating an NLI dataset with an LLM and using it to fine-tune PromptEOL. In experiments on STS tasks, the proposed method achieved an average Spearman's rank correlation coefficient of 82.21 with respect to human evaluation, thus outperforming existing methods without using large, manually annotated datasets.</li>
</ul>

<h3>Title: Modified CycleGAN for the synthesization of samples for wheat head  segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jaden Myers, Keyhan Najafian, Farhad Maleki, Katie Ovens</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15135">https://arxiv.org/abs/2402.15135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15135">https://arxiv.org/pdf/2402.15135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15135]] Modified CycleGAN for the synthesization of samples for wheat head  segmentation(https://arxiv.org/abs/2402.15135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Deep learning models have been used for a variety of image processing tasks. However, most of these models are developed through supervised learning approaches, which rely heavily on the availability of large-scale annotated datasets. Developing such datasets is tedious and expensive. In the absence of an annotated dataset, synthetic data can be used for model development; however, due to the substantial differences between simulated and real data, a phenomenon referred to as domain gap, the resulting models often underperform when applied to real data. In this research, we aim to address this challenge by first computationally simulating a large-scale annotated dataset and then using a generative adversarial network (GAN) to fill the gap between simulated and real images. This approach results in a synthetic dataset that can be effectively utilized to train a deep-learning model. Using this approach, we developed a realistic annotated synthetic dataset for wheat head segmentation. This dataset was then used to develop a deep-learning model for semantic segmentation. The resulting model achieved a Dice score of 83.4\% on an internal dataset and Dice scores of 79.6% and 83.6% on two external Global Wheat Head Detection datasets. While we proposed this approach in the context of wheat head segmentation, it can be generalized to other crop types or, more broadly, to images with dense, repeated patterns such as those found in cellular imagery.</li>
</ul>

<h3>Title: PUAD: Frustratingly Simple Method for Robust Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Shota Sugawara, Ryuji Imamura</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15143">https://arxiv.org/abs/2402.15143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15143">https://arxiv.org/pdf/2402.15143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15143]] PUAD: Frustratingly Simple Method for Robust Anomaly Detection(https://arxiv.org/abs/2402.15143)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Developing an accurate and fast anomaly detection model is an important task in real-time computer vision applications. There has been much research to develop a single model that detects either structural or logical anomalies, which are inherently distinct. The majority of the existing approaches implicitly assume that the anomaly can be represented by identifying the anomalous location. However, we argue that logical anomalies, such as the wrong number of objects, can not be well-represented by the spatial feature maps and require an alternative approach. In addition, we focused on the possibility of detecting logical anomalies by using an out-of-distribution detection approach on the feature space, which aggregates the spatial information of the feature map. As a demonstration, we propose a method that incorporates a simple out-of-distribution detection method on the feature space against state-of-the-art reconstruction-based approaches. Despite the simplicity of our proposal, our method PUAD (Picturable and Unpicturable Anomaly Detection) achieves state-of-the-art performance on the MVTec LOCO AD dataset.</li>
</ul>

<h3>Title: TREC: APT Tactic / Technique Recognition via Few-Shot Provenance  Subgraph Learning</h3>
<ul>
<li><strong>Authors: </strong>Mingqi Lv, HongZhe Gao, Xuebo Qiu, Tieming Chen, Tiantian Zhu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15147">https://arxiv.org/abs/2402.15147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15147">https://arxiv.org/pdf/2402.15147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15147]] TREC: APT Tactic / Technique Recognition via Few-Shot Provenance  Subgraph Learning(https://arxiv.org/abs/2402.15147)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>APT (Advanced Persistent Threat) with the characteristics of persistence, stealth, and diversity is one of the greatest threats against cyber-infrastructure. As a countermeasure, existing studies leverage provenance graphs to capture the complex relations between system entities in a host for effective APT detection. In addition to detecting single attack events as most existing work does, understanding the tactics / techniques (e.g., Kill-Chain, ATT&CK) applied to organize and accomplish the APT attack campaign is more important for security operations. Existing studies try to manually design a set of rules to map low-level system events to high-level APT tactics / techniques. However, the rule based methods are coarse-grained and lack generalization ability, thus they can only recognize APT tactics and cannot identify fine-grained APT techniques and mutant APT attacks. In this paper, we propose TREC, the first attempt to recognize APT tactics / techniques from provenance graphs by exploiting deep learning techniques. To address the "needle in a haystack" problem, TREC segments small and compact subgraphs covering individual APT technique instances from a large provenance graph based on a malicious node detection model and a subgraph sampling algorithm. To address the "training sample scarcity" problem, TREC trains the APT tactic / technique recognition model in a few-shot learning manner by adopting a Siamese neural network. We evaluate TREC based on a customized dataset collected and made public by our team. The experiment results show that TREC significantly outperforms state-of-the-art systems in APT tactic recognition and TREC can also effectively identify APT techniques.</li>
</ul>

<h3>Title: On the Duality Between Sharpness-Aware Minimization and Adversarial  Training</h3>
<ul>
<li><strong>Authors: </strong>Yihao Zhang, Hangzhou He, Jingyu Zhu, Huanran Chen, Yifei Wang, Zeming Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15152">https://arxiv.org/abs/2402.15152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15152">https://arxiv.org/pdf/2402.15152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15152]] On the Duality Between Sharpness-Aware Minimization and Adversarial  Training(https://arxiv.org/abs/2402.15152)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Adversarial Training (AT), which adversarially perturb the input samples during training, has been acknowledged as one of the most effective defenses against adversarial attacks, yet suffers from a fundamental tradeoff that inevitably decreases clean accuracy. Instead of perturbing the samples, Sharpness-Aware Minimization (SAM) perturbs the model weights during training to find a more flat loss landscape and improve generalization. However, as SAM is designed for better clean accuracy, its effectiveness in enhancing adversarial robustness remains unexplored. In this work, considering the duality between SAM and AT, we investigate the adversarial robustness derived from SAM. Intriguingly, we find that using SAM alone can improve adversarial robustness. To understand this unexpected property of SAM, we first provide empirical and theoretical insights into how SAM can implicitly learn more robust features, and conduct comprehensive experiments to show that SAM can improve adversarial robustness notably without sacrificing any clean accuracy, shedding light on the potential of SAM to be a substitute for AT when accuracy comes at a higher priority. Code is available at https://github.com/weizeming/SAM_AT.</li>
</ul>

<h3>Title: Machine Unlearning of Pre-trained Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, Xiang Yue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15159">https://arxiv.org/abs/2402.15159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15159">https://arxiv.org/pdf/2402.15159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15159]] Machine Unlearning of Pre-trained Large Language Models(https://arxiv.org/abs/2402.15159)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering substantive insights into the mechanics of machine unlearning for pre-trained LLMs and underscoring the potential for responsible AI development.</li>
</ul>

<h3>Title: Spatially-Aware Transformer Memory for Embodied Agents</h3>
<ul>
<li><strong>Authors: </strong>Junmo Cho, Jaesik Yoon, Sungjin Ahn</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15160">https://arxiv.org/abs/2402.15160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15160">https://arxiv.org/pdf/2402.15160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15160]] Spatially-Aware Transformer Memory for Embodied Agents(https://arxiv.org/abs/2402.15160)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanced accuracy in various place-centric downstream tasks. Additionally, we propose the Adaptive Memory Allocator, a memory management method based on reinforcement learning that aims to optimize efficiency of memory utilization. Our experiments demonstrate the advantages of our proposed model in various environments and across multiple downstream tasks, including prediction, generation, reasoning, and reinforcement learning. The source code for our models and experiments will be available at https://github.com/junmokane/spatially-aware-transformer.</li>
</ul>

<h3>Title: Entity-level Factual Adaptiveness of Fine-tuning based Abstractive  Summarization Models</h3>
<ul>
<li><strong>Authors: </strong>Jongyoon Song, Nohil Park, Bongkyu Hwang, Jaewoong Yun, Seongho Joe, Youngjune L. Gwon, Sungroh Yoon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15162">https://arxiv.org/abs/2402.15162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15162">https://arxiv.org/pdf/2402.15162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15162]] Entity-level Factual Adaptiveness of Fine-tuning based Abstractive  Summarization Models(https://arxiv.org/abs/2402.15162)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Abstractive summarization models often generate factually inconsistent content particularly when the parametric knowledge of the model conflicts with the knowledge in the input document. In this paper, we analyze the robustness of fine-tuning based summarization models to the knowledge conflict, which we call factual adaptiveness. We utilize pre-trained language models to construct evaluation sets and find that factual adaptiveness is not strongly correlated with factual consistency on original datasets. Furthermore, we introduce a controllable counterfactual data augmentation method where the degree of knowledge conflict within the augmented data can be adjustable. Our experimental results on two pre-trained language models (PEGASUS and BART) and two fine-tuning datasets (XSum and CNN/DailyMail) demonstrate that our method enhances factual adaptiveness while achieving factual consistency on original datasets on par with the contrastive learning baseline.</li>
</ul>

<h3>Title: The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Ma, Shuchen Xue, Tianyang Hu, Wenjia Wang, Zhaoqiang Liu, Zhenguo Li, Zhi-Ming Ma, Kenji Kawaguchi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15170">https://arxiv.org/abs/2402.15170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15170">https://arxiv.org/pdf/2402.15170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15170]] The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling(https://arxiv.org/abs/2402.15170)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the incorporation of the UNet architecture, diffusion probabilistic models have become a dominant force in image generation tasks. One key design in UNet is the skip connections between the encoder and decoder blocks. Although skip connections have been shown to improve training stability and model performance, we reveal that such shortcuts can be a limiting factor for the complexity of the transformation. As the sampling steps decrease, the generation process and the role of the UNet get closer to the push-forward transformations from Gaussian distribution to the target, posing a challenge for the network's complexity. To address this challenge, we propose Skip-Tuning, a simple yet surprisingly effective training-free tuning method on the skip connections. Our method can achieve 100% FID improvement for pretrained EDM on ImageNet 64 with only 19 NFEs (1.75), breaking the limit of ODE samplers regardless of sampling steps. Surprisingly, the improvement persists when we increase the number of sampling steps and can even surpass the best result from EDM-2 (1.58) with only 39 NFEs (1.57). Comprehensive exploratory experiments are conducted to shed light on the surprising effectiveness. We observe that while Skip-Tuning increases the score-matching losses in the pixel space, the losses in the feature space are reduced, particularly at intermediate noise levels, which coincide with the most effective range accounting for image quality improvement.</li>
</ul>

<h3>Title: Attention-Guided Masked Autoencoders For Learning Image Representations</h3>
<ul>
<li><strong>Authors: </strong>Leon Sick, Dominik Engel, Pedro Hermosilla, Timo Ropinski</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15172">https://arxiv.org/abs/2402.15172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15172">https://arxiv.org/pdf/2402.15172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15172]] Attention-Guided Masked Autoencoders For Learning Image Representations(https://arxiv.org/abs/2402.15172)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Masked autoencoders (MAEs) have established themselves as a powerful method for unsupervised pre-training for computer vision tasks. While vanilla MAEs put equal emphasis on reconstructing the individual parts of the image, we propose to inform the reconstruction process through an attention-guided loss function. By leveraging advances in unsupervised object discovery, we obtain an attention map of the scene which we employ in the loss function to put increased emphasis on reconstructing relevant objects, thus effectively incentivizing the model to learn more object-focused representations without compromising the established masking strategy. Our evaluations show that our pre-trained models learn better latent representations than the vanilla MAE, demonstrated by improved linear probing and k-NN classification results on several benchmarks while at the same time making ViTs more robust against varying backgrounds.</li>
</ul>

<h3>Title: Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed  Zeroth-Order Optimizer</h3>
<ul>
<li><strong>Authors: </strong>Yanjun Zhao, Sizhe Dang, Haishan Ye, Guang Dai, Yi Qian, Ivor W.Tsang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15173">https://arxiv.org/abs/2402.15173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15173">https://arxiv.org/pdf/2402.15173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15173]] Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed  Zeroth-Order Optimizer(https://arxiv.org/abs/2402.15173)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) with classic first-order optimizers entails prohibitive GPU memory due to the backpropagation process. Recent works have turned to zeroth-order optimizers for fine-tuning, which save substantial memory by using two forward passes. However, these optimizers are plagued by the heterogeneity of parameter curvatures across different dimensions. In this work, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer which is the first work to leverage the diagonal Hessian to enhance zeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the expensive memory cost and only increases one forward pass per step. Extensive experiments on various models (350M~66B parameters) indicate that HiZOO improves model convergence, significantly reducing training steps and effectively enhancing model accuracy. Moreover, we visualize the optimization trajectories of HiZOO on test functions, illustrating its effectiveness in handling heterogeneous curvatures. Lastly, we provide theoretical proofs of convergence for HiZOO. Code is publicly available at https://anonymous.4open.science/r/HiZOO27F8.</li>
</ul>

<h3>Title: Unified View of Grokking, Double Descent and Emergent Abilities: A  Perspective from Circuits Competition</h3>
<ul>
<li><strong>Authors: </strong>Yufei Huang, Shengding Hu, Xu Han, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15175">https://arxiv.org/abs/2402.15175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15175">https://arxiv.org/pdf/2402.15175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15175]] Unified View of Grokking, Double Descent and Emergent Abilities: A  Perspective from Circuits Competition(https://arxiv.org/abs/2402.15175)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent studies have uncovered intriguing phenomena in deep learning, such as grokking, double descent, and emergent abilities in large language models, which challenge human intuition and are crucial for a deeper understanding of neural models. In this paper, we present a comprehensive framework that provides a unified view of these three phenomena, focusing on the competition between memorization and generalization circuits. This approach, initially employed to explain grokking, is extended in our work to encompass a wider range of model sizes and training data volumes. Our framework delineates four distinct training dynamics, each depending on varying combinations of model size and training data quantity. Utilizing this framework, we provide a detailed analysis of the double descent phenomenon and propose two verifiable predictions regarding its occurrence, both substantiated by our experimental results. Moreover, we expand our framework to the multi-task learning paradigm, demonstrating how algorithm tasks can be turned into emergent abilities. This offers a novel perspective to understand emergent abilities in Large Language Models.</li>
</ul>

<h3>Title: Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks  with Self-Refinement</h3>
<ul>
<li><strong>Authors: </strong>Heegyu Kim, Sehyun Yuk, Hyunsouk Cho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15180">https://arxiv.org/abs/2402.15180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15180">https://arxiv.org/pdf/2402.15180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15180]] Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks  with Self-Refinement(https://arxiv.org/abs/2402.15180)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Caution: This paper includes offensive words that could potentially cause unpleasantness. Language models (LMs) are vulnerable to exploitation for adversarial misuse. Training LMs for safety alignment is extensive and makes it hard to respond to fast-developing attacks immediately, such as jailbreaks. We propose self-refine with formatting that achieves outstanding safety even in non-safety-aligned LMs and evaluate our method alongside several defense baselines, demonstrating that it is the safest training-free method against jailbreak attacks. Additionally, we proposed a formatting method that improves the efficiency of the self-refine process while reducing attack success rates in fewer iterations. We've also observed that non-safety-aligned LMs outperform safety-aligned LMs in safety tasks by giving more helpful and safe responses. In conclusion, our findings can achieve less safety risk with fewer computational costs, allowing non-safety LM to be easily utilized in real-world service.</li>
</ul>

<h3>Title: GraphEdit: Large Language Models for Graph Structure Learning</h3>
<ul>
<li><strong>Authors: </strong>Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei, Liang Pang, Tat-Seng Chua, Chao Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15183">https://arxiv.org/abs/2402.15183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15183">https://arxiv.org/pdf/2402.15183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15183]] GraphEdit: Large Language Models for Graph Structure Learning(https://arxiv.org/abs/2402.15183)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy connections but also identifies node-wise dependencies from a global perspective, providing a comprehensive understanding of the graph structure. We conduct extensive experiments on multiple benchmark datasets to demonstrate the effectiveness and robustness of GraphEdit across various settings. We have made our model implementation available at: https://github.com/HKUDS/GraphEdit.</li>
</ul>

<h3>Title: Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized  Control</h3>
<ul>
<li><strong>Authors: </strong>Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Tommaso Biancalani, Sergey Levine</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15194">https://arxiv.org/abs/2402.15194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15194">https://arxiv.org/pdf/2402.15194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15194]] Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized  Control(https://arxiv.org/abs/2402.15194)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth "genuine" reward, as is the case in many practical applications. These challenges, collectively termed "reward collapse," pose a substantial obstacle. To address this reward collapse, we frame the finetuning problem as entropy-regularized control against the pretrained diffusion model, i.e., directly optimizing entropy-enhanced rewards with neural SDEs. We present theoretical and empirical evidence that demonstrates our framework is capable of efficiently generating diverse samples with high genuine rewards, mitigating the overoptimization of imperfect reward models.</li>
</ul>

<h3>Title: DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be  Better Context-aware Translators</h3>
<ul>
<li><strong>Authors: </strong>Xinglin Lyu, Junhui Li, Yanqing Zhao, Min Zhang, Daimeng Wei, Shimin Tao, Hao Yang, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15200">https://arxiv.org/abs/2402.15200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15200">https://arxiv.org/pdf/2402.15200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15200]] DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be  Better Context-aware Translators(https://arxiv.org/abs/2402.15200)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Generally, the decoder-only large language models (LLMs) are adapted to context-aware neural machine translation (NMT) in a concatenating way, where LLMs take the concatenation of the source sentence (i.e., intra-sentence context) and the inter-sentence context as the input, and then to generate the target tokens sequentially. This adaptation strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence contexts with the same priority, despite an apparent difference between the two kinds of contexts. In this paper, we propose an alternative adaptation approach, named Decoding-enhanced Multi-phase Prompt Tuning (DeMPT), to make LLMs discriminately model and utilize the inter- and intra-sentence context and more effectively adapt LLMs to context-aware NMT. First, DeMPT divides the context-aware NMT process into three separate phases. During each phase, different continuous prompts are introduced to make LLMs discriminately model various information. Second, DeMPT employs a heuristic way to further discriminately enhance the utilization of the source-side inter- and intra-sentence information at the final decoding phase. Experiments show that our approach significantly outperforms the concatenation method, and further improves the performance of LLMs in discourse modeling.</li>
</ul>

<h3>Title: Fine-Grained Detoxification via Instance-Level Prefixes for Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xin Yi, Linlin Wang, Xiaoling Wang, Liang He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15202">https://arxiv.org/abs/2402.15202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15202">https://arxiv.org/pdf/2402.15202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15202]] Fine-Grained Detoxification via Instance-Level Prefixes for Large  Language Models(https://arxiv.org/abs/2402.15202)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Impressive results have been achieved in natural language processing (NLP) tasks through the training of large language models (LLMs). However, these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility. To tackle this issue, various finetuning-based and decoding-based approaches have been utilized to mitigate toxicity. However, these methods typically necessitate additional costs such as high-quality training data or auxiliary models. In this paper, we propose fine-grained detoxification via instance-level prefixes (FGDILP) to mitigate toxic text without additional cost. Specifically, FGDILP contrasts the contextualized representation in attention space using a positive prefix-prepended prompt against multiple negative prefix-prepended prompts at the instance level. This allows for constructing fine-grained subtoxicity vectors, which enables collaborative detoxification by fusing them to correct the normal generation process when provided with a raw prompt. We validate that FGDILP enables controlled text generation with regard to toxicity at both the utterance and context levels. Our method surpasses prompt-based baselines in detoxification, although at a slight cost to generation fluency and diversity.</li>
</ul>

<h3>Title: Source-Guided Similarity Preservation for Online Person  Re-Identification</h3>
<ul>
<li><strong>Authors: </strong>Hamza Rami, Jhony H. Giraldo, Nicolas Winckler, Stéphane Lathuilière</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15206">https://arxiv.org/abs/2402.15206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15206">https://arxiv.org/pdf/2402.15206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15206]] Source-Guided Similarity Preservation for Online Person  Re-Identification(https://arxiv.org/abs/2402.15206)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Online Unsupervised Domain Adaptation (OUDA) for person Re-Identification (Re-ID) is the task of continuously adapting a model trained on a well-annotated source domain dataset to a target domain observed as a data stream. In OUDA, person Re-ID models face two main challenges: catastrophic forgetting and domain shift. In this work, we propose a new Source-guided Similarity Preservation (S2P) framework to alleviate these two problems. Our framework is based on the extraction of a support set composed of source images that maximizes the similarity with the target data. This support set is used to identify feature similarities that must be preserved during the learning process. S2P can incorporate multiple existing UDA methods to mitigate catastrophic forgetting. Our experiments show that S2P outperforms previous state-of-the-art methods on multiple real-to-real and synthetic-to-real challenging OUDA benchmarks.</li>
</ul>

<h3>Title: Label-efficient Multi-organ Segmentation Method with Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yongzhi Huang, Jinxin Zhu, Haseeb Hassan, Liyilei Su, Jingyu Li, Binding Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15216">https://arxiv.org/abs/2402.15216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15216">https://arxiv.org/pdf/2402.15216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15216]] Label-efficient Multi-organ Segmentation Method with Diffusion Model(https://arxiv.org/abs/2402.15216)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of multiple organs in Computed Tomography (CT) images plays a vital role in computer-aided diagnosis systems. Various supervised-learning approaches have been proposed recently. However, these methods heavily depend on a large amount of high-quality labeled data, which is expensive to obtain in practice. In this study, we present a label-efficient learning approach using a pre-trained diffusion model for multi-organ segmentation tasks in CT images. First, a denoising diffusion model was trained using unlabeled CT data, generating additional two-dimensional (2D) CT images. Then the pre-trained denoising diffusion network was transferred to the downstream multi-organ segmentation task, effectively creating a semi-supervised learning model that requires only a small amount of labeled data. Furthermore, linear classification and fine-tuning decoder strategies were employed to enhance the network's segmentation performance. Our generative model at 256x256 resolution achieves impressive performance in terms of Fr\'echet inception distance, spatial Fr\'echet inception distance, and F1-score, with values of 11.32, 46.93, and 73.1\%, respectively. These results affirm the diffusion model's ability to generate diverse and realistic 2D CT images. Additionally, our method achieves competitive multi-organ segmentation performance compared to state-of-the-art methods on the FLARE 2022 dataset, particularly in limited labeled data scenarios. Remarkably, even with only 1\% and 10\% labeled data, our method achieves Dice similarity coefficients (DSCs) of 71.56\% and 78.51\% after fine-tuning, respectively. The method achieves a DSC score of 51.81\% using just four labeled CT scans. These results demonstrate the efficacy of our approach in overcoming the limitations of supervised learning heavily reliant on large-scale labeled data.</li>
</ul>

<h3>Title: BSPA: Exploring Black-box Stealthy Prompt Attacks against Image  Generators</h3>
<ul>
<li><strong>Authors: </strong>Yu Tian, Xiao Yang, Yinpeng Dong, Heming Yang, Hang Su, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15218">https://arxiv.org/abs/2402.15218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15218">https://arxiv.org/pdf/2402.15218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15218]] BSPA: Exploring Black-box Stealthy Prompt Attacks against Image  Generators(https://arxiv.org/abs/2402.15218)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal, diffusion</a></li>
<li><strong>Abstract: </strong>Extremely large image generators offer significant transformative potential across diverse sectors. It allows users to design specific prompts to generate realistic images through some black-box APIs. However, some studies reveal that image generators are notably susceptible to attacks and generate Not Suitable For Work (NSFW) contents by manually designed toxin texts, especially imperceptible to human observers. We urgently need a multitude of universal and transferable prompts to improve the safety of image generators, especially black-box-released APIs. Nevertheless, they are constrained by labor-intensive design processes and heavily reliant on the quality of the given instructions. To achieve this, we introduce a black-box stealthy prompt attack (BSPA) that adopts a retriever to simulate attacks from API users. It can effectively harness filter scores to tune the retrieval space of sensitive words for matching the input prompts, thereby crafting stealthy prompts tailored for image generators. Significantly, this approach is model-agnostic and requires no internal access to the model's features, ensuring its applicability to a wide range of image generators. Building on BSPA, we have constructed an automated prompt tool and a comprehensive prompt attack dataset (NSFWeval). Extensive experiments demonstrate that BSPA effectively explores the security vulnerabilities in a variety of state-of-the-art available black-box models, including Stable Diffusion XL, Midjourney, and DALL-E 2/3. Furthermore, we develop a resilient text filter and offer targeted recommendations to ensure the security of image generators against prompt attacks in the future.</li>
</ul>

<h3>Title: ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and  Two-Phase Partition</h3>
<ul>
<li><strong>Authors: </strong>Lu Ye, Ze Tao, Yong Huang, Yang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15220">https://arxiv.org/abs/2402.15220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15220">https://arxiv.org/pdf/2402.15220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15220]] ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and  Two-Phase Partition(https://arxiv.org/abs/2402.15220)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts. Experiments show that ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\times$ compared to the start-of-the-art implementation, with the length of the system prompt ranging from 1024 to 4096.</li>
</ul>

<h3>Title: Unsupervised Domain Adaptation for Brain Vessel Segmentation through  Transwarp Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Fengming Lin, Yan Xia, Michael MacRaild, Yash Deo, Haoran Dou, Qiongyao Liu, Kun Wu, Nishant Ravikumar, Alejandro F. Frangi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15237">https://arxiv.org/abs/2402.15237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15237">https://arxiv.org/pdf/2402.15237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15237]] Unsupervised Domain Adaptation for Brain Vessel Segmentation through  Transwarp Contrastive Learning(https://arxiv.org/abs/2402.15237)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Unsupervised domain adaptation (UDA) aims to align the labelled source distribution with the unlabelled target distribution to obtain domain-invariant predictive models. Since cross-modality medical data exhibit significant intra and inter-domain shifts and most are unlabelled, UDA is more important while challenging in medical image analysis. This paper proposes a simple yet potent contrastive learning framework for UDA to narrow the inter-domain gap between labelled source and unlabelled target distribution. Our method is validated on cerebral vessel datasets. Experimental results show that our approach can learn latent features from labelled 3DRA modality data and improve vessel segmentation performance in unlabelled MRA modality data.</li>
</ul>

<h3>Title: GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech  Detection?</h3>
<ul>
<li><strong>Authors: </strong>Yiping Jin, Leo Wanner, Alexander Shvets</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15238">https://arxiv.org/abs/2402.15238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15238">https://arxiv.org/pdf/2402.15238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15238]] GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech  Detection?(https://arxiv.org/abs/2402.15238)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Online hate detection suffers from biases incurred in data sampling, annotation, and model pre-training. Therefore, measuring the averaged performance over all examples in held-out test data is inadequate. Instead, we must identify specific model weaknesses and be informed when it is more likely to fail. A recent proposal in this direction is HateCheck, a suite for testing fine-grained model functionalities on synthesized data generated using templates of the kind "You are just a [slur] to me." However, despite enabling more detailed diagnostic insights, the HateCheck test cases are often generic and have simplistic sentence structures that do not match the real-world data. To address this limitation, we propose GPT-HateCheck, a framework to generate more diverse and realistic functional tests from scratch by instructing large language models (LLMs). We employ an additional natural language inference (NLI) model to verify the generations. Crowd-sourced annotation demonstrates that the generated test cases are of high quality. Using the new functional tests, we can uncover model weaknesses that would be overlooked using the original HateCheck dataset.</li>
</ul>

<h3>Title: GS-EMA: Integrating Gradient Surgery Exponential Moving Average with  Boundary-Aware Contrastive Learning for Enhanced Domain Generalization in  Aneurysm Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Fengming Lin, Yan Xia, Michael MacRaild, Yash Deo, Haoran Dou, Qiongyao Liu, Nina Cheng, Nishant Ravikumar, Alejandro F. Frangi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15239">https://arxiv.org/abs/2402.15239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15239">https://arxiv.org/pdf/2402.15239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15239]] GS-EMA: Integrating Gradient Surgery Exponential Moving Average with  Boundary-Aware Contrastive Learning for Enhanced Domain Generalization in  Aneurysm Segmentation(https://arxiv.org/abs/2402.15239)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>The automated segmentation of cerebral aneurysms is pivotal for accurate diagnosis and treatment planning. Confronted with significant domain shifts and class imbalance in 3D Rotational Angiography (3DRA) data from various medical institutions, the task becomes challenging. These shifts include differences in image appearance, intensity distribution, resolution, and aneurysm size, all of which complicate the segmentation process. To tackle these issues, we propose a novel domain generalization strategy that employs gradient surgery exponential moving average (GS-EMA) optimization technique coupled with boundary-aware contrastive learning (BACL). Our approach is distinct in its ability to adapt to new, unseen domains by learning domain-invariant features, thereby improving the robustness and accuracy of aneurysm segmentation across diverse clinical datasets. The results demonstrate that our proposed approach can extract more domain-invariant features, minimizing over-segmentation and capturing more complete aneurysm structures.</li>
</ul>

<h3>Title: A Bargaining-based Approach for Feature Trading in Vertical Federated  Learning</h3>
<ul>
<li><strong>Authors: </strong>Yue Cui, Liuyi Yao, Zitao Li, Yaliang Li, Bolin Ding, Xiaofang Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15247">https://arxiv.org/abs/2402.15247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15247">https://arxiv.org/pdf/2402.15247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15247]] A Bargaining-based Approach for Feature Trading in Vertical Federated  Learning(https://arxiv.org/abs/2402.15247)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, federate, fair</a></li>
<li><strong>Abstract: </strong>Vertical Federated Learning (VFL) has emerged as a popular machine learning paradigm, enabling model training across the data and the task parties with different features about the same user set while preserving data privacy. In production environment, VFL usually involves one task party and one data party. Fair and economically efficient feature trading is crucial to the commercialization of VFL, where the task party is considered as the data consumer who buys the data party's features. However, current VFL feature trading practices often price the data party's data as a whole and assume transactions occur prior to the performing VFL. Neglecting the performance gains resulting from traded features may lead to underpayment and overpayment issues. In this study, we propose a bargaining-based feature trading approach in VFL to encourage economically efficient transactions. Our model incorporates performance gain-based pricing, taking into account the revenue-based optimization objectives of both parties. We analyze the proposed bargaining model under perfect and imperfect performance information settings, proving the existence of an equilibrium that optimizes the parties' objectives. Moreover, we develop performance gain estimation-based bargaining strategies for imperfect performance information scenarios and discuss potential security issues and solutions. Experiments on three real-world datasets demonstrate the effectiveness of the proposed bargaining model.</li>
</ul>

<h3>Title: Optimal Transport for Structure Learning Under Missing Data</h3>
<ul>
<li><strong>Authors: </strong>Vy Vo, He Zhao, Trung Le, Edwin V. Bonilla, Dinh Phung</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15255">https://arxiv.org/abs/2402.15255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15255">https://arxiv.org/pdf/2402.15255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15255]] Optimal Transport for Structure Learning Under Missing Data(https://arxiv.org/abs/2402.15255)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Causal discovery in the presence of missing data introduces a chicken-and-egg dilemma. While the goal is to recover the true causal structure, robust imputation requires considering the dependencies or preferably causal relations among variables. Merely filling in missing values with existing imputation methods and subsequently applying structure learning on the complete data is empirical shown to be sub-optimal. To this end, we propose in this paper a score-based algorithm, based on optimal transport, for learning causal structure from missing data. This optimal transport viewpoint diverges from existing score-based approaches that are dominantly based on EM. We project structure learning as a density fitting problem, where the goal is to find the causal model that induces a distribution of minimum Wasserstein distance with the distribution over the observed data. Through extensive simulations and real-data experiments, our framework is shown to recover the true causal graphs more effectively than the baselines in various simulations and real-data experiments. Empirical evidences also demonstrate the superior scalability of our approach, along with the flexibility to incorporate any off-the-shelf causal discovery methods for complete data.</li>
</ul>

<h3>Title: DEEM: Dynamic Experienced Expert Modeling for Stance Detection</h3>
<ul>
<li><strong>Authors: </strong>Xiaolong Wang, Yile Wang, Sijie Cheng, Peng Li, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15264">https://arxiv.org/abs/2402.15264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15264">https://arxiv.org/pdf/2402.15264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15264]] DEEM: Dynamic Experienced Expert Modeling for Stance Detection(https://arxiv.org/abs/2402.15264)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent work has made a preliminary attempt to use large language models (LLMs) to solve the stance detection task, showing promising results. However, considering that stance detection usually requires detailed background knowledge, the vanilla reasoning method may neglect the domain knowledge to make a professional and accurate analysis. Thus, there is still room for improvement of LLMs reasoning, especially in leveraging the generation capability of LLMs to simulate specific experts (i.e., multi-agents) to detect the stance. In this paper, different from existing multi-agent works that require detailed descriptions and use fixed experts, we propose a Dynamic Experienced Expert Modeling (DEEM) method which can leverage the generated experienced experts and let LLMs reason in a semi-parametric way, making the experts more generalizable and reliable. Experimental results demonstrate that DEEM consistently achieves the best results on three standard benchmarks, outperforms methods with self-consistency reasoning, and reduces the bias of LLMs.</li>
</ul>

<h3>Title: Calibration of Deep Learning Classification Models in fNIRS</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Cao, Zizhou Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15266">https://arxiv.org/abs/2402.15266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15266">https://arxiv.org/pdf/2402.15266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15266]] Calibration of Deep Learning Classification Models in fNIRS(https://arxiv.org/abs/2402.15266)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool for monitoring brain activity. The classification of fNIRS data in relation to conscious activity holds significance for advancing our understanding of the brain and facilitating the development of brain-computer interfaces (BCI). Many researchers have turned to deep learning to tackle the classification challenges inherent in fNIRS data due to its strong generalization and robustness. In the application of fNIRS, reliability is really important, and one mathematical formulation of the reliability of confidence is calibration. However, many researchers overlook the important issue of calibration. To address this gap, we propose integrating calibration into fNIRS field and assess the reliability of existing models. Surprisingly, our results indicate poor calibration performance in many proposed models. To advance calibration development in the fNIRS field, we summarize three practical tips. Through this letter, we hope to emphasize the critical role of calibration in fNIRS research and argue for enhancing the reliability of deep learning-based predictions in fNIRS classification tasks. All data from our experimental process are openly available on GitHub.</li>
</ul>

<h3>Title: Adversarial Robustness of Deep Learning-based Malware Detectors via  (De)Randomized Smoothing</h3>
<ul>
<li><strong>Authors: </strong>Daniel Gibert, Giulio Zizzo, Quan Le, Jordi Planes</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15267">https://arxiv.org/abs/2402.15267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15267">https://arxiv.org/pdf/2402.15267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15267]] Adversarial Robustness of Deep Learning-based Malware Detectors via  (De)Randomized Smoothing(https://arxiv.org/abs/2402.15267)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep learning-based malware detectors have been shown to be susceptible to adversarial malware examples, i.e. malware examples that have been deliberately manipulated in order to avoid detection. In light of the vulnerability of deep learning detectors to subtle input file modifications, we propose a practical defense against adversarial malware examples inspired by (de)randomized smoothing. In this work, we reduce the chances of sampling adversarial content injected by malware authors by selecting correlated subsets of bytes, rather than using Gaussian noise to randomize inputs like in the Computer Vision (CV) domain. During training, our ablation-based smoothing scheme trains a base classifier to make classifications on a subset of contiguous bytes or chunk of bytes. At test time, a large number of chunks are then classified by a base classifier and the consensus among these classifications is then reported as the final prediction. We propose two strategies to determine the location of the chunks used for classification: (1) randomly selecting the locations of the chunks and (2) selecting contiguous adjacent chunks. To showcase the effectiveness of our approach, we have trained two classifiers with our chunk-based ablation schemes on the BODMAS dataset. Our findings reveal that the chunk-based smoothing classifiers exhibit greater resilience against adversarial malware examples generated with state-of-the-are evasion attacks, outperforming a non-smoothed classifier and a randomized smoothing-based classifier by a great margin.</li>
</ul>

<h3>Title: MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nathanaël Carraz Rakotonirina, Marco Baroni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15268">https://arxiv.org/abs/2402.15268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15268">https://arxiv.org/pdf/2402.15268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15268]] MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained  Language Models(https://arxiv.org/abs/2402.15268)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based language models (LMs) track contextual information through large, hard-coded input windows. We introduce MemoryPrompt, a leaner approach in which the LM is complemented by a small auxiliary recurrent network that passes information to the LM by prefixing its regular input with a sequence of vectors, akin to soft prompts, without requiring LM finetuning. Tested on a task designed to probe a LM's ability to keep track of multiple fact updates, a MemoryPrompt-augmented LM outperforms much larger LMs that have access to the full input history. We also test MemoryPrompt on a long-distance dialogue dataset, where its performance is comparable to that of a model conditioned on the entire conversation history. In both experiments we also observe that, unlike full-finetuning approaches, MemoryPrompt does not suffer from catastrophic forgetting when adapted to new tasks, thus not disrupting the generalist capabilities of the underlying LM.</li>
</ul>

<h3>Title: Trustworthy confidential virtual machines for the masses</h3>
<ul>
<li><strong>Authors: </strong>Anna Galanou, Khushboo Bindlish, Luca Preibsch, Yvonne-Anne Pignolet, Christof Fetzer, Rüdiger Kapitza</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15277">https://arxiv.org/abs/2402.15277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15277">https://arxiv.org/pdf/2402.15277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15277]] Trustworthy confidential virtual machines for the masses(https://arxiv.org/abs/2402.15277)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect</a></li>
<li><strong>Abstract: </strong>Confidential computing alleviates the concerns of distrustful customers by removing the cloud provider from their trusted computing base and resolves their disincentive to migrate their workloads to the cloud. This is facilitated by new hardware extensions, like AMD's SEV Secure Nested Paging (SEV-SNP), which can run a whole virtual machine with confidentiality and integrity protection against a potentially malicious hypervisor owned by an untrusted cloud provider. However, the assurance of such protection to either the service providers deploying sensitive workloads or the end-users passing sensitive data to services requires sending proof to the interested parties. Service providers can retrieve such proof by performing remote attestation while end-users have typically no means to acquire this proof or validate its correctness and therefore have to rely on the trustworthiness of the service providers. In this paper, we present Revelio, an approach that features two main contributions: i) it allows confidential virtual machine (VM)-based workloads to be designed and deployed in a way that disallows any tampering even by the service providers and ii) it empowers users to easily validate their integrity. In particular, we focus on web-facing workloads, protect them leveraging SEV-SNP, and enable end-users to remotely attest them seamlessly each time a new web session is established. To highlight the benefits of Revelio, we discuss how a standalone stateful VM that hosts an open-source collaboration office suite can be secured and present a replicated protocol proxy that enables commodity users to securely access the Internet Computer, a decentralized blockchain infrastructure.</li>
</ul>

<h3>Title: Let's Rectify Step by Step: Improving Aspect-based Sentiment Analysis  with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shunyu Liu, Jie Zhou, Qunxi Zhu, Qin Chen, Qingchun Bai, Jun Xiao, Liang He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15289">https://arxiv.org/abs/2402.15289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15289">https://arxiv.org/pdf/2402.15289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15289]] Let's Rectify Step by Step: Improving Aspect-based Sentiment Analysis  with Diffusion Models(https://arxiv.org/abs/2402.15289)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Aspect-Based Sentiment Analysis (ABSA) stands as a crucial task in predicting the sentiment polarity associated with identified aspects within text. However, a notable challenge in ABSA lies in precisely determining the aspects' boundaries (start and end indices), especially for long ones, due to users' colloquial expressions. We propose DiffusionABSA, a novel diffusion model tailored for ABSA, which extracts the aspects progressively step by step. Particularly, DiffusionABSA gradually adds noise to the aspect terms in the training process, subsequently learning a denoising process that progressively restores these terms in a reverse manner. To estimate the boundaries, we design a denoising neural network enhanced by a syntax-aware temporal attention mechanism to chronologically capture the interplay between aspects and surrounding text. Empirical evaluations conducted on eight benchmark datasets underscore the compelling advantages offered by DiffusionABSA when compared against robust baseline models. Our code is publicly available at https://github.com/Qlb6x/DiffusionABSA.</li>
</ul>

<h3>Title: SoK: What don't we know? Understanding Security Vulnerabilities in  SNARKs</h3>
<ul>
<li><strong>Authors: </strong>Stefanos Chaliasos, Jens Ernstberger, David Theodore, David Wong, Mohammad Jahanara, Benjamin Livshits</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15293">https://arxiv.org/abs/2402.15293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15293">https://arxiv.org/pdf/2402.15293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15293]] SoK: What don't we know? Understanding Security Vulnerabilities in  SNARKs(https://arxiv.org/abs/2402.15293)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, defense, robust</a></li>
<li><strong>Abstract: </strong>Zero-knowledge proofs (ZKPs) have evolved from being a theoretical concept providing privacy and verifiability to having practical, real-world implementations, with SNARKs (Succinct Non-Interactive Argument of Knowledge) emerging as one of the most significant innovations. Prior work has mainly focused on designing more efficient SNARK systems and providing security proofs for them. Many think of SNARKs as "just math," implying that what is proven to be correct and secure is correct in practice. In contrast, this paper focuses on assessing end-to-end security properties of real-life SNARK implementations. We start by building foundations with a system model and by establishing threat models and defining adversarial roles for systems that use SNARKs. Our study encompasses an extensive analysis of 141 actual vulnerabilities in SNARK implementations, providing a detailed taxonomy to aid developers and security researchers in understanding the security threats in systems employing SNARKs. Finally, we evaluate existing defense mechanisms and offer recommendations for enhancing the security of SNARK-based systems, paving the way for more robust and reliable implementations in the future.</li>
</ul>

<h3>Title: Semi-supervised Counting via Pixel-by-pixel Density Distribution  Modelling</h3>
<ul>
<li><strong>Authors: </strong>Hui Lin, Zhiheng Ma, Rongrong Ji, Yaowei Wang, Zhou Su, Xiaopeng Hong, Deyu Meng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15297">https://arxiv.org/abs/2402.15297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15297">https://arxiv.org/pdf/2402.15297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15297]] Semi-supervised Counting via Pixel-by-pixel Density Distribution  Modelling(https://arxiv.org/abs/2402.15297)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper focuses on semi-supervised crowd counting, where only a small portion of the training data are labeled. We formulate the pixel-wise density value to regress as a probability distribution, instead of a single deterministic value. On this basis, we propose a semi-supervised crowd-counting model. Firstly, we design a pixel-wise distribution matching loss to measure the differences in the pixel-wise density distributions between the prediction and the ground truth; Secondly, we enhance the transformer decoder by using density tokens to specialize the forwards of decoders w.r.t. different density intervals; Thirdly, we design the interleaving consistency self-supervised learning mechanism to learn from unlabeled data efficiently. Extensive experiments on four datasets are performed to show that our method clearly outperforms the competitors by a large margin under various labeled ratio settings. Code will be released at https://github.com/LoraLinH/Semi-supervised-Counting-via-Pixel-by-pixel-Density-Distribution-Modelling.</li>
</ul>

<h3>Title: Seeing is Believing: Mitigating Hallucination in Large Vision-Language  Models via CLIP-Guided Decoding</h3>
<ul>
<li><strong>Authors: </strong>Ailin Deng, Zhirui Chen, Bryan Hooi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15300">https://arxiv.org/abs/2402.15300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15300">https://arxiv.org/pdf/2402.15300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15300]] Seeing is Believing: Mitigating Hallucination in Large Vision-Language  Models via CLIP-Guided Decoding(https://arxiv.org/abs/2402.15300)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) are susceptible to object hallucinations, an issue in which their generated text contains non-existent objects, greatly limiting their reliability and practicality. Current approaches often rely on the model's token likelihoods or other internal information, instruction tuning on additional datasets, or incorporating complex external tools. We first perform empirical analysis on sentence-level LVLM hallucination, finding that CLIP similarity to the image acts as a stronger and more robust indicator of hallucination compared to token likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD) approach, a straightforward but effective training-free approach to reduce object hallucination at decoding time. CGD uses CLIP to guide the model's decoding process by enhancing visual grounding of generated text with the image. Experiments demonstrate that CGD effectively mitigates object hallucination across multiple LVLM families while preserving the utility of text generation.</li>
</ul>

<h3>Title: Causal Graph Discovery with Retrieval-Augmented Generation based Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuzhe Zhang, Yipeng Zhang, Yidong Gan, Lina Yao, Chen Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15301">https://arxiv.org/abs/2402.15301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15301">https://arxiv.org/pdf/2402.15301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15301]] Causal Graph Discovery with Retrieval-Augmented Generation based Large  Language Models(https://arxiv.org/abs/2402.15301)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Causal graph recovery is essential in the field of causal inference. Traditional methods are typically knowledge-based or statistical estimation-based, which are limited by data collection biases and individuals' knowledge about factors affecting the relations between variables of interests. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that utilizes the extensive knowledge contained within a large corpus of scientific literature to deduce causal relationships in general causal graph recovery tasks. This method leverages Retrieval Augmented-Generation (RAG) based LLMs to systematically analyze and extract pertinent information from a comprehensive collection of research papers. Our method first retrieves relevant text chunks from the aggregated literature. Then, the LLM is tasked with identifying and labelling potential associations between factors. Finally, we give a method to aggregate the associational relationships to build a causal graph. We demonstrate our method is able to construct high quality causal graphs on the well-known SACHS dataset solely from literature.</li>
</ul>

<h3>Title: How (un)ethical are instruction-centric responses of LLMs? Unveiling the  vulnerabilities of safety guardrails to harmful queries</h3>
<ul>
<li><strong>Authors: </strong>Somnath Banerjee, Sayan Layek, Rima Hazra, Animesh Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15302">https://arxiv.org/abs/2402.15302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15302">https://arxiv.org/pdf/2402.15302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15302]] How (un)ethical are instruction-centric responses of LLMs? Unveiling the  vulnerabilities of safety guardrails to harmful queries(https://arxiv.org/abs/2402.15302)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this study, we tackle a growing concern around the safety and ethical use of large language models (LLMs). Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation. Our work zeroes in on a specific issue: to what extent LLMs can be led astray by asking them to generate responses that are instruction-centric such as a pseudocode, a program or a software snippet as opposed to vanilla text. To investigate this question, we introduce TechHazardQA, a dataset containing complex queries which should be answered in both text and instruction-centric formats (e.g., pseudocodes), aimed at identifying triggers for unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b, Mistral-V2 and Mistral 8X7B -- and ask them to generate both text and instruction-centric responses. For evaluation we report the harmfulness score metric as well as judgements from GPT-4 and humans. Overall, we observe that asking LLMs to produce instruction-centric responses enhances the unethical response generation by ~2-38% across the models. As an additional objective, we investigate the impact of model editing using the ROME technique, which further increases the propensity for generating undesirable content. In particular, asking edited LLMs to generate instruction-centric responses further increases the unethical response generation by ~3-16% across the different models.</li>
</ul>

<h3>Title: ArabianGPT: Native Arabic GPT-based Large Language</h3>
<ul>
<li><strong>Authors: </strong>Anis Koubaa, Adel Ammar, Lahouari Ghouti, Omar Najar, Serry Sibaee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15313">https://arxiv.org/abs/2402.15313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15313">https://arxiv.org/pdf/2402.15313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15313]] ArabianGPT: Native Arabic GPT-based Large Language(https://arxiv.org/abs/2402.15313)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax. Consequently, there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements. To address this gap, this paper proposes ArabianGPT, a series of transformer-based models within the ArabianLLM suite designed explicitly for Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing. Empirical results from fine-tuning the models on tasks like sentiment analysis and summarization demonstrate significant improvements. For sentiment analysis, the fine-tuned ArabianGPT-0.1B model achieved a remarkable accuracy of 95%, a substantial increase from the base model's 56%. Similarly, in summarization tasks, fine-tuned models showed enhanced F1 scores, indicating improved precision and recall in generating concise summaries. Comparative analysis of fine-tuned ArabianGPT models against their base versions across various benchmarks reveals nuanced differences in performance, with fine-tuning positively impacting specific tasks like question answering and summarization. These findings underscore the efficacy of fine-tuning in aligning ArabianGPT models more closely with specific NLP tasks, highlighting the potential of tailored transformer architectures in advancing Arabic NLP.</li>
</ul>

<h3>Title: GPTVQ: The Blessing of Dimensionality for LLM Quantization</h3>
<ul>
<li><strong>Authors: </strong>Mart van Baalen, Andrey Kuzmin, Markus Nagel, Peter Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, Paul Whatmough</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15319">https://arxiv.org/abs/2402.15319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15319">https://arxiv.org/pdf/2402.15319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15319]] GPTVQ: The Blessing of Dimensionality for LLM Quantization(https://arxiv.org/abs/2402.15319)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this work we show that the size versus accuracy trade-off of neural network quantization can be significantly improved by increasing the quantization dimensionality. We propose the GPTVQ method, a new fast method for post-training vector quantization (VQ) that scales well to Large Language Models (LLMs). Our method interleaves quantization of one or more columns with updates to the remaining unquantized weights, using information from the Hessian of the per-layer output reconstruction MSE. Quantization codebooks are initialized using an efficient data-aware version of the EM algorithm. The codebooks are then updated, and further compressed by using integer quantization and SVD-based compression. GPTVQ establishes a new state-of-the art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2 and Mistral. Furthermore, our method is efficient: on a single H100 it takes between 3 and 11 hours to process a Llamav2-70B model, depending on quantization setting. Lastly, with on-device timings for VQ decompression on a mobile CPU we show that VQ leads to improved latency compared to using a 4-bit integer format.</li>
</ul>

<h3>Title: OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene  Understanding</h3>
<ul>
<li><strong>Authors: </strong>Francis Engelmann, Ayca Takmaz, Jonas Schult, Elisabetta Fedele, Johanna Wald, Songyou Peng, Xi Wang, Or Litany, Siyu Tang, Federico Tombari, Marc Pollefeys, Leonidas Guibas, Hongbo Tian, Chunjie Wang, Xiaosheng Yan, Bingwen Wang, Xuanyang Zhang, Xiao Liu, Phuc Nguyen, Khoi Nguyen, Anh Tran, Cuong Pham, Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao, Lei Zhu, Joan Lasenby</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15321">https://arxiv.org/abs/2402.15321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15321">https://arxiv.org/pdf/2402.15321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15321]] OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene  Understanding(https://arxiv.org/abs/2402.15321)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This report provides an overview of the challenge hosted at the OpenSUN3D Workshop on Open-Vocabulary 3D Scene Understanding held in conjunction with ICCV 2023. The goal of this workshop series is to provide a platform for exploration and discussion of open-vocabulary 3D scene understanding tasks, including but not limited to segmentation, detection and mapping. We provide an overview of the challenge hosted at the workshop, present the challenge dataset, the evaluation methodology, and brief descriptions of the winning methods. For additional details, please see https://opensun3d.github.io/index_iccv23.html.</li>
</ul>

<h3>Title: Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective  of Operator Semigroup Theory</h3>
<ul>
<li><strong>Authors: </strong>Weichen Zhao, Chenguang Wang, Xinyan Wang, Congying Han, Tiande Guo, Tianshu Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15326">https://arxiv.org/abs/2402.15326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15326">https://arxiv.org/pdf/2402.15326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15326]] Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective  of Operator Semigroup Theory(https://arxiv.org/abs/2402.15326)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper presents a novel study of the oversmoothing issue in diffusion-based Graph Neural Networks (GNNs). Diverging from extant approaches grounded in random walk analysis or particle systems, we approach this problem through operator semigroup theory. This theoretical framework allows us to rigorously prove that oversmoothing is intrinsically linked to the ergodicity of the diffusion operator. This finding further poses a general and mild ergodicity-breaking condition, encompassing the various specific solutions previously offered, thereby presenting a more universal and theoretically grounded approach to mitigating oversmoothing in diffusion-based GNNs. Additionally, we offer a probabilistic interpretation of our theory, forging a link with prior works and broadening the theoretical horizon. Our experimental results reveal that this ergodicity-breaking term effectively mitigates oversmoothing measured by Dirichlet energy, and simultaneously enhances performance in node classification tasks.</li>
</ul>

<h3>Title: A Blockchain-Enabled Framework of UAV Coordination for Post-Disaster  Networks</h3>
<ul>
<li><strong>Authors: </strong>Sana Hafeez, Runze Cheng, Lina Mohjazi, Muhammad Ali Imran, Yao Sun</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15331">https://arxiv.org/abs/2402.15331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15331">https://arxiv.org/pdf/2402.15331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15331]] A Blockchain-Enabled Framework of UAV Coordination for Post-Disaster  Networks(https://arxiv.org/abs/2402.15331)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, robust</a></li>
<li><strong>Abstract: </strong>Emergency communication is critical but challenging after natural disasters when ground infrastructure is devastated. Unmanned aerial vehicles (UAVs) offer enormous potential for agile relief coordination in these scenarios. However, effectively leveraging UAV fleets poses additional challenges around security, privacy, and efficient collaboration across response agencies. This paper presents a robust blockchain-enabled framework to address these challenges by integrating a consortium blockchain model, smart contracts, and cryptographic techniques to securely coordinate UAV fleets for disaster response. Specifically, we make two key contributions: a consortium blockchain architecture for secure and private multi-agency coordination; and an optimized consensus protocol balancing efficiency and fault tolerance using a delegated proof of stake practical byzantine fault tolerance (DPoS-PBFT). Comprehensive simulations showcase the framework's ability to enhance transparency, automation, scalability, and cyber-attack resilience for UAV coordination in post-disaster networks.</li>
</ul>

<h3>Title: Ranking Entities along Conceptual Space Dimensions with LLMs: An  Analysis of Fine-Tuning Strategies</h3>
<ul>
<li><strong>Authors: </strong>Nitesh Kumar, Usashi Chatterjee, Steven Schockaert</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15337">https://arxiv.org/abs/2402.15337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15337">https://arxiv.org/pdf/2402.15337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15337]] Ranking Entities along Conceptual Space Dimensions with LLMs: An  Analysis of Fine-Tuning Strategies(https://arxiv.org/abs/2402.15337)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Conceptual spaces represent entities in terms of their primitive semantic features. Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features. Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged as a promising strategy. However, existing work has been limited to probing pre-trained LLMs using relatively simple zero-shot strategies. We focus in particular on the task of ranking entities according to a given conceptual space dimension. Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare. We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features. We find that this is indeed the case, to some extent, but having perceptual and subjective features in the training data seems essential for achieving the best results. We furthermore find that pointwise ranking strategies are competitive against pairwise approaches, in defiance of common wisdom.</li>
</ul>

<h3>Title: NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data</h3>
<ul>
<li><strong>Authors: </strong>Sergei Bogdanov, Alexandre Constantin, Timothée Bernard, Benoit Crabbé, Etienne Bernard</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15343">https://arxiv.org/abs/2402.15343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15343">https://arxiv.org/pdf/2402.15343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15343]] NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data(https://arxiv.org/abs/2402.15343)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems. In this paper, we show how to use LLMs to create NuNER, a compact language representation model specialized in the Named Entity Recognition (NER) task. NuNER can be fine-tuned to solve downstream NER problems in a data-efficient way, outperforming similar-sized foundation models in the few-shot regime and competing with much larger LLMs. We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance. We view NuNER as a member of the broader family of task-specific foundation models, recently unlocked by LLMs.</li>
</ul>

<h3>Title: AutoMMLab: Automatically Generating Deployable Models from Language  Instructions for Computer Vision Tasks</h3>
<ul>
<li><strong>Authors: </strong>Zekang Yang, Wang Zeng, Sheng Jin, Chen Qian, Ping Luo, Wentao Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15351">https://arxiv.org/abs/2402.15351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15351">https://arxiv.org/pdf/2402.15351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15351]] AutoMMLab: Automatically Generating Deployable Models from Language  Instructions for Computer Vision Tasks(https://arxiv.org/abs/2402.15351)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Automated machine learning (AutoML) is a collection of techniques designed to automate the machine learning development process. While traditional AutoML approaches have been successfully applied in several critical steps of model development (e.g. hyperparameter optimization), there lacks a AutoML system that automates the entire end-to-end model production workflow. To fill this blank, we present AutoMMLab, a general-purpose LLM-empowered AutoML system that follows user's language instructions to automate the whole model production workflow for computer vision tasks. The proposed AutoMMLab system effectively employs LLMs as the bridge to connect AutoML and OpenMMLab community, empowering non-expert individuals to easily build task-specific models via a user-friendly language interface. Specifically, we propose RU-LLaMA to understand users' request and schedule the whole pipeline, and propose a novel LLM-based hyperparameter optimizer called HPO-LLaMA to effectively search for the optimal hyperparameters. Experiments show that our AutoMMLab system is versatile and covers a wide range of mainstream tasks, including classification, detection, segmentation and keypoint estimation. We further develop a new benchmark, called LAMP, for studying key components in the end-to-end prompt-based model training pipeline. Code, model, and data will be released.</li>
</ul>

<h3>Title: Dual Encoder: Exploiting the Potential of Syntactic and Semantic for  Aspect Sentiment Triplet Extraction</h3>
<ul>
<li><strong>Authors: </strong>Xiaowei Zhao, Yong Zhou, Xiujuan Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15370">https://arxiv.org/abs/2402.15370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15370">https://arxiv.org/pdf/2402.15370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15370]] Dual Encoder: Exploiting the Potential of Syntactic and Semantic for  Aspect Sentiment Triplet Extraction(https://arxiv.org/abs/2402.15370)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained sentiment analysis. Recent studies have employed Graph Neural Networks (GNN) to model the syntax-semantic relationships inherent in triplet elements. However, they have yet to fully tap into the vast potential of syntactic and semantic information within the ASTE task. In this work, we propose a \emph{Dual Encoder: Exploiting the potential of Syntactic and Semantic} model (D2E2S), which maximizes the syntactic and semantic relationships among words. Specifically, our model utilizes a dual-channel encoder with a BERT channel to capture semantic information, and an enhanced LSTM channel for comprehensive syntactic information capture. Subsequently, we introduce the heterogeneous feature interaction module to capture intricate interactions between dependency syntax and attention semantics, and to dynamically select vital nodes. We leverage the synergy of these modules to harness the significant potential of syntactic and semantic information in ASTE tasks. Testing on public benchmarks, our D2E2S model surpasses the current state-of-the-art(SOTA), demonstrating its effectiveness.</li>
</ul>

<h3>Title: On the Usability of Next-Generation Authentication: A Study on Eye  Movement and Brainwave-based Mechanisms</h3>
<ul>
<li><strong>Authors: </strong>Matin Fallahi, Patricia Arias Cabarcos, Thorsten Strufe</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15388">https://arxiv.org/abs/2402.15388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15388">https://arxiv.org/pdf/2402.15388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15388]] On the Usability of Next-Generation Authentication: A Study on Eye  Movement and Brainwave-based Mechanisms(https://arxiv.org/abs/2402.15388)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, biometric</a></li>
<li><strong>Abstract: </strong>Passwords remain a widely-used authentication mechanism, despite their well-known security and usability limitations. To improve on this situation, next-generation authentication mechanisms, based on behavioral biometric factors such as eye movement and brainwave have emerged. However, their usability remains relatively under-explored. To fill this gap, we conducted an empirical user study (n=32 participants) to evaluate three brain-based and three eye-based authentication mechanisms, using both qualitative and quantitative methods. Our findings show good overall usability according to the System Usability Scale for both categories of mechanisms, with average SUS scores in the range of 78.6-79.6 and the best mechanisms rated with an "excellent" score. Participants particularly identified brainwave authentication as more secure yet more privacy-invasive and effort-intensive compared to eye movement authentication. However, the significant number of neutral responses indicates participants' need for more detailed information about the security and privacy implications of these authentication methods. Building on the collected evidence, we identify three key areas for improvement: privacy, authentication interface design, and verification time. We offer recommendations for designers and developers to improve the usability and security of next-generation authentication mechanisms.</li>
</ul>

<h3>Title: Explorations of Self-Repair in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Cody Rushing, Neel Nanda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15390">https://arxiv.org/abs/2402.15390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15390">https://arxiv.org/pdf/2402.15390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15390]] Explorations of Self-Repair in Language Models(https://arxiv.org/abs/2402.15390)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor (which can repair up to 30% of the direct effect) and sparse sets of neurons implementing Anti-Erasure. We additionally discuss the implications of these results for interpretability practitioners and close with a more speculative discussion on the mystery of why self-repair occurs in these models at all, highlighting evidence for the Iterative Inference hypothesis in language models, a framework that predicts self-repair.</li>
</ul>

<h3>Title: Genie: Generative Interactive Environments</h3>
<ul>
<li><strong>Authors: </strong>Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, Tim Rocktäschel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15391">https://arxiv.org/abs/2402.15391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15391">https://arxiv.org/pdf/2402.15391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15391]] Genie: Generative Interactive Environments(https://arxiv.org/abs/2402.15391)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.</li>
</ul>

<h3>Title: TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow  Attention for Commuting Flow Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yan Luo, Zhuoyue Wan, Yuzhong Chen, Gengchen Mai, Fu-lai Chung, Kent Larson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15398">https://arxiv.org/abs/2402.15398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15398">https://arxiv.org/pdf/2402.15398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15398]] TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow  Attention for Commuting Flow Prediction(https://arxiv.org/abs/2402.15398)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, transformer</a></li>
<li><strong>Abstract: </strong>Understanding the link between urban planning and commuting flows is crucial for guiding urban development and policymaking. This research, bridging computer science and urban studies, addresses the challenge of integrating these fields with their distinct focuses. Traditional urban studies methods, like the gravity and radiation models, often underperform in complex scenarios due to their limited handling of multiple variables and reliance on overly simplistic and unrealistic assumptions, such as spatial isotropy. While deep learning models offer improved accuracy, their black-box nature poses a trade-off between performance and explainability -- both vital for analyzing complex societal phenomena like commuting flows. To address this, we introduce TransFlower, an explainable, transformer-based model employing flow-to-flow attention to predict urban commuting patterns. It features a geospatial encoder with an anisotropy-aware relative location encoder for nuanced flow representation. Following this, the transformer-based flow predictor enhances this by leveraging attention mechanisms to efficiently capture flow interactions. Our model outperforms existing methods by up to 30.8% Common Part of Commuters, offering insights into mobility dynamics crucial for urban planning and policy decisions.</li>
</ul>

<h3>Title: Distributionally Robust Off-Dynamics Reinforcement Learning: Provable  Efficiency with Linear Function Approximation</h3>
<ul>
<li><strong>Authors: </strong>Zhishuai Liu, Pan Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15399">https://arxiv.org/abs/2402.15399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15399">https://arxiv.org/pdf/2402.15399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15399]] Distributionally Robust Off-Dynamics Reinforcement Learning: Provable  Efficiency with Linear Function Approximation(https://arxiv.org/abs/2402.15399)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We study off-dynamics Reinforcement Learning (RL), where the policy is trained on a source domain and deployed to a distinct target domain. We aim to solve this problem via online distributionally robust Markov decision processes (DRMDPs), where the learning algorithm actively interacts with the source domain while seeking the optimal performance under the worst possible dynamics that is within an uncertainty set of the source domain's transition kernel. We provide the first study on online DRMDPs with function approximation for off-dynamics RL. We find that DRMDPs' dual formulation can induce nonlinearity, even when the nominal transition kernel is linear, leading to error propagation. By designing a $d$-rectangular uncertainty set using the total variation distance, we remove this additional nonlinearity and bypass the error propagation. We then introduce DR-LSVI-UCB, the first provably efficient online DRMDP algorithm for off-dynamics RL with function approximation, and establish a polynomial suboptimality bound that is independent of the state and action space sizes. Our work makes the first step towards a deeper understanding of the provable efficiency of online DRMDPs with linear function approximation. Finally, we substantiate the performance and robustness of DR-LSVI-UCB through different numerical experiments.</li>
</ul>

<h3>Title: The Impact of LoRA on the Emergence of Clusters in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Hugo Koubbi, Matthieu Boussard, Louis Hernandez</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15415">https://arxiv.org/abs/2402.15415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15415">https://arxiv.org/pdf/2402.15415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15415]] The Impact of LoRA on the Emergence of Clusters in Transformers(https://arxiv.org/abs/2402.15415)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we employ the mathematical framework on Transformers developed by \citet{sander2022sinkformers,geshkovski2023emergence,geshkovski2023mathematical} to explore how variations in attention parameters and initial token values impact the structural dynamics of token clusters. Our analysis demonstrates that while the clusters within a modified attention matrix dynamics can exhibit significant divergence from the original over extended periods, they maintain close similarities over shorter intervals, depending on the parameter differences. This work contributes to the fine-tuning field through practical applications to the LoRA algorithm \cite{hu2021lora,peft}, enhancing our understanding of the behavior of LoRA-enhanced Transformer models.</li>
</ul>

<h3>Title: A Data-Centric Approach To Generate Faithful and High Quality Patient  Summaries with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Stefan Hegselmann, Shannon Zejiang Shen, Florian Gierse, Monica Agrawal, David Sontag, Xiaoyi Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15422">https://arxiv.org/abs/2402.15422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15422">https://arxiv.org/pdf/2402.15422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15422]] A Data-Centric Approach To Generate Faithful and High Quality Patient  Summaries with Large Language Models(https://arxiv.org/abs/2402.15422)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we develop a rigorous labeling protocol for hallucinations, and have two medical experts annotate 100 real-world summaries and 100 generated summaries. We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information. Although the effect is still present, it is much smaller for GPT-4 when prompted with five examples (0.70 to 0.40). We also conduct a qualitative evaluation using hallucination-free and improved training data. GPT-4 shows very good results even in the zero-shot setting. We find that common quantitative metrics do not correlate well with faithfulness and quality. Finally, we test GPT-4 for automatic hallucination detection, which yields promising results.</li>
</ul>

<h3>Title: Prime+Retouch: When Cache is Locked and Leaked</h3>
<ul>
<li><strong>Authors: </strong>Jaehyuk Lee, Fan Sang, Taesoo Kim</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15425">https://arxiv.org/abs/2402.15425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15425">https://arxiv.org/pdf/2402.15425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15425]] Prime+Retouch: When Cache is Locked and Leaked(https://arxiv.org/abs/2402.15425)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Caches on the modern commodity CPUs have become one of the major sources of side-channel leakages and been abused as a new attack vector. To thwart the cache-based side-channel attacks, two types of countermeasures have been proposed: detection-based ones that limit the amount of microarchitectural traces an attacker can leave, and cache prefetching-and-locking techniques that claim to prevent such leakage by disallowing evictions on sensitive data. In this paper, we present the Prime+Retouch attack that completely bypasses these defense schemes by accurately inferring the cache activities with the metadata of the cache replacement policy. Prime+Retouch has three noticeable properties: 1) it incurs no eviction on the victim's data, allowing us to bypass the two known mitigation schemes, 2) it requires minimal synchronization of only one memory access to the attacker's pre-primed cache lines, and 3) it leaks data via non-shared memory, yet because underlying eviction metadata is shared. We demonstrate Prime+Retouch in two architectures: predominant Intel x86 and emerging Apple M1. We elucidate how Prime+Retouch can break the T-table implementation of AES with robust cache side-channel mitigations such as Cloak, under both normal and SGX-protected environments. We also manifest feasibility of the Prime+Retouch attack on the M1 platform imposing more restrictions where the precise measurement tools such as core clock cycle timer and performance counters are inaccessible to the attacker. Furthermore, we first demystify undisclosed cache architecture and its eviction policy of L1 data cache on Apple M1 architecture. We also devise a user-space noise-free cache monitoring tool by repurposing Intel TSX.</li>
</ul>

<h3>Title: ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion  Models against Stochastic Perturbation</h3>
<ul>
<li><strong>Authors: </strong>Yi Zhang, Yun Tang, Wenjie Ruan, Xiaowei Huang, Siddartha Khastgir, Paul Jennings, Xingyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15429">https://arxiv.org/abs/2402.15429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15429">https://arxiv.org/pdf/2402.15429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15429]] ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion  Models against Stochastic Perturbation(https://arxiv.org/abs/2402.15429)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in generating high-quality images based on simple text descriptions. However, as is common with many Deep Learning (DL) models, DMs are subject to a lack of robustness. While there are attempts to evaluate the robustness of T2I DMs as a binary or worst-case problem, they cannot answer how robust in general the model is whenever an adversarial example (AE) can be found. In this study, we first introduce a probabilistic notion of T2I DMs' robustness; and then establish an efficient framework, ProTIP, to evaluate it with statistical guarantees. The main challenges stem from: i) the high computational cost of the generation process; and ii) determining if a perturbed input is an AE involves comparing two output distributions, which is fundamentally harder compared to other DL tasks like classification where an AE is identified upon misprediction of labels. To tackle the challenges, we employ sequential analysis with efficacy and futility early stopping rules in the statistical testing for identifying AEs, and adaptive concentration inequalities to dynamically determine the "just-right" number of stochastic perturbations whenever the verification target is met. Empirical experiments validate the effectiveness and efficiency of ProTIP over common T2I DMs. Finally, we demonstrate an application of ProTIP to rank commonly used defence methods.</li>
</ul>

<h3>Title: Hierarchical Invariance for Robust and Interpretable Vision Tasks at  Larger Scales</h3>
<ul>
<li><strong>Authors: </strong>Shuren Qi, Yushu Zhang, Chao Wang, Zhihua Xia, Jian Weng, Xiaochun Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15430">https://arxiv.org/abs/2402.15430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15430">https://arxiv.org/pdf/2402.15430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15430]] Hierarchical Invariance for Robust and Interpretable Vision Tasks at  Larger Scales(https://arxiv.org/abs/2402.15430)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Developing robust and interpretable vision systems is a crucial step towards trustworthy artificial intelligence. In this regard, a promising paradigm considers embedding task-required invariant structures, e.g., geometric invariance, in the fundamental image representation. However, such invariant representations typically exhibit limited discriminability, limiting their applications in larger-scale trustworthy vision tasks. For this open problem, we conduct a systematic investigation of hierarchical invariance, exploring this topic from theoretical, practical, and application perspectives. At the theoretical level, we show how to construct over-complete invariants with a Convolutional Neural Networks (CNN)-like hierarchical architecture yet in a fully interpretable manner. The general blueprint, specific definitions, invariant properties, and numerical implementations are provided. At the practical level, we discuss how to customize this theoretical framework into a given task. With the over-completeness, discriminative features w.r.t. the task can be adaptively formed in a Neural Architecture Search (NAS)-like manner. We demonstrate the above arguments with accuracy, invariance, and efficiency results on texture, digit, and parasite classification experiments. Furthermore, at the application level, our representations are explored in real-world forensics tasks on adversarial perturbations and Artificial Intelligence Generated Content (AIGC). Such applications reveal that the proposed strategy not only realizes the theoretically promised invariance, but also exhibits competitive discriminability even in the era of deep learning. For robust and interpretable vision tasks at larger scales, hierarchical invariant representation can be considered as an effective alternative to traditional CNN and invariants.</li>
</ul>

<h3>Title: Selective disclosure of claims from multiple digital credentials</h3>
<ul>
<li><strong>Authors: </strong>Šeila Bećirović Ramić, Irfan Prazina, Damir Pozderac, Razija Turčinhodžić Mulahasanović, Saša Mrdović</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15447">https://arxiv.org/abs/2402.15447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15447">https://arxiv.org/pdf/2402.15447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15447]] Selective disclosure of claims from multiple digital credentials(https://arxiv.org/abs/2402.15447)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Digital credentials represent a cornerstone of digital identity on the Internet. To achieve privacy, certain functionalities in credentials should be implemented. One is selective disclosure, which allows users to disclose only the claims or attributes they want. This paper presents a novel approach to selective disclosure that combines Merkle hash trees and Boneh-Lynn-Shacham (BLS) signatures. Combining these approaches, we achieve selective disclosure of claims in a single credential and creation of a verifiable presentation containing selectively disclosed claims from multiple credentials signed by different parties. Besides selective disclosure, we enable issuing credentials signed by multiple issuers using this approach.</li>
</ul>

<h3>Title: Repetition Improves Language Model Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, Aditi Raghunathan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15449">https://arxiv.org/abs/2402.15449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15449">https://arxiv.org/pdf/2402.15449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15449]] Repetition Improves Language Model Embeddings(https://arxiv.org/abs/2402.15449)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Recent approaches to improving the extraction of text embeddings from autoregressive large language models (LLMs) have largely focused on improvements to data, backbone pretrained language models, or improving task-differentiation via instructions. In this work, we address an architectural limitation of autoregressive models: token embeddings cannot contain information from tokens that appear later in the input. To address this limitation, we propose a simple approach, "echo embeddings," in which we repeat the input twice in context and extract embeddings from the second occurrence. We show that echo embeddings of early tokens can encode information about later tokens, allowing us to maximally leverage high-quality LLMs for embeddings. On the MTEB leaderboard, echo embeddings improve over classical embeddings by over 9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a Mistral-7B model achieve state-of-the-art compared to prior open source models that do not leverage synthetic fine-tuning data.</li>
</ul>

<h3>Title: Benchmarking the Robustness of Panoptic Segmentation for Automated  Driving</h3>
<ul>
<li><strong>Authors: </strong>Yiting Wang, Haonan Zhao, Daniel Gummadi, Mehrdad Dianati, Kurt Debattista, Valentina Donzella</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15469">https://arxiv.org/abs/2402.15469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15469">https://arxiv.org/pdf/2402.15469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15469]] Benchmarking the Robustness of Panoptic Segmentation for Automated  Driving(https://arxiv.org/abs/2402.15469)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Precise situational awareness is required for the safe decision-making of assisted and automated driving (AAD) functions. Panoptic segmentation is a promising perception technique to identify and categorise objects, impending hazards, and driveable space at a pixel level. While segmentation quality is generally associated with the quality of the camera data, a comprehensive understanding and modelling of this relationship are paramount for AAD system designers. Motivated by such a need, this work proposes a unifying pipeline to assess the robustness of panoptic segmentation models for AAD, correlating it with traditional image quality. The first step of the proposed pipeline involves generating degraded camera data that reflects real-world noise factors. To this end, 19 noise factors have been identified and implemented with 3 severity levels. Of these factors, this work proposes novel models for unfavourable light and snow. After applying the degradation models, three state-of-the-art CNN- and vision transformers (ViT)-based panoptic segmentation networks are used to analyse their robustness. The variations of the segmentation performance are then correlated to 8 selected image quality metrics. This research reveals that: 1) certain specific noise factors produce the highest impact on panoptic segmentation, i.e. droplets on lens and Gaussian noise; 2) the ViT-based panoptic segmentation backbones show better robustness to the considered noise factors; 3) some image quality metrics (i.e. LPIPS and CW-SSIM) correlate strongly with panoptic segmentation performance and therefore they can be used as predictive metrics for network performance.</li>
</ul>

<h3>Title: FAIR: Filtering of Automatically Induced Rules</h3>
<ul>
<li><strong>Authors: </strong>Divya Jyoti Bajpai, Ayush Maheshwari, Manjesh Kumar Hanawal, Ganesh Ramakrishnan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15472">https://arxiv.org/abs/2402.15472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15472">https://arxiv.org/pdf/2402.15472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15472]] FAIR: Filtering of Automatically Induced Rules(https://arxiv.org/abs/2402.15472)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The availability of large annotated data can be a critical bottleneck in training machine learning algorithms successfully, especially when applied to diverse domains. Weak supervision offers a promising alternative by accelerating the creation of labeled training data using domain-specific rules. However, it requires users to write a diverse set of high-quality rules to assign labels to the unlabeled data. Automatic Rule Induction (ARI) approaches circumvent this problem by automatically creating rules from features on a small labeled set and filtering a final set of rules from them. In the ARI approach, the crucial step is to filter out a set of a high-quality useful subset of rules from the large set of automatically created rules. In this paper, we propose an algorithm (Filtering of Automatically Induced Rules) to filter rules from a large number of automatically induced rules using submodular objective functions that account for the collective precision, coverage, and conflicts of the rule set. We experiment with three ARI approaches and five text classification datasets to validate the superior performance of our algorithm with respect to several semi-supervised label aggregation approaches. Further, we show that achieves statistically significant results in comparison to existing rule-filtering approaches.</li>
</ul>

<h3>Title: Debiasing Machine Learning Models by Using Weakly Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Renan D. B. Brotto, Jean-Michel Loubes, Laurent Risser, Jean-Pierre Florens, Kenji Nose-Filho, João M. T. Romano</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15477">https://arxiv.org/abs/2402.15477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15477">https://arxiv.org/pdf/2402.15477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15477]] Debiasing Machine Learning Models by Using Weakly Supervised Learning(https://arxiv.org/abs/2402.15477)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We tackle the problem of bias mitigation of algorithmic decisions in a setting where both the output of the algorithm and the sensitive variable are continuous. Most of prior work deals with discrete sensitive variables, meaning that the biases are measured for subgroups of persons defined by a label, leaving out important algorithmic bias cases, where the sensitive variable is continuous. Typical examples are unfair decisions made with respect to the age or the financial status. In our work, we then propose a bias mitigation strategy for continuous sensitive variables, based on the notion of endogeneity which comes from the field of econometrics. In addition to solve this new problem, our bias mitigation strategy is a weakly supervised learning method which requires that a small portion of the data can be measured in a fair manner. It is model agnostic, in the sense that it does not make any hypothesis on the prediction model. It also makes use of a reasonably large amount of input observations and their corresponding predictions. Only a small fraction of the true output predictions should be known. This therefore limits the need for expert interventions. Results obtained on synthetic data show the effectiveness of our approach for examples as close as possible to real-life applications in econometrics.</li>
</ul>

<h3>Title: Transformers are Expressive, But Are They Expressive Enough for  Regression?</h3>
<ul>
<li><strong>Authors: </strong>Swaroop Nath, Harshad Khadilkar, Pushpak Bhattacharyya</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15478">https://arxiv.org/abs/2402.15478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15478">https://arxiv.org/pdf/2402.15478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15478]] Transformers are Expressive, But Are They Expressive Enough for  Regression?(https://arxiv.org/abs/2402.15478)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers have become pivotal in Natural Language Processing, demonstrating remarkable success in applications like Machine Translation and Summarization. Given their widespread adoption, several works have attempted to analyze the expressivity of Transformers. Expressivity of a neural network is the class of functions it can approximate. A neural network is fully expressive if it can act as a universal function approximator. We attempt to analyze the same for Transformers. Contrary to existing claims, our findings reveal that Transformers struggle to reliably approximate continuous functions, relying on piecewise constant approximations with sizable intervals. The central question emerges as: "\textit{Are Transformers truly Universal Function Approximators}?" To address this, we conduct a thorough investigation, providing theoretical insights and supporting evidence through experiments. Our contributions include a theoretical analysis pinpointing the root of Transformers' limitation in function approximation and extensive experiments to verify the limitation. By shedding light on these challenges, we advocate a refined understanding of Transformers' capabilities.</li>
</ul>

<h3>Title: Retinotopic Mapping Enhances the Robustness of Convolutional Neural  Networks</h3>
<ul>
<li><strong>Authors: </strong>Jean-Nicolas Jérémie, Emmanuel Daucé, Laurent U Perrinet</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15480">https://arxiv.org/abs/2402.15480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15480">https://arxiv.org/pdf/2402.15480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15480]] Retinotopic Mapping Enhances the Robustness of Convolutional Neural  Networks(https://arxiv.org/abs/2402.15480)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Foveated vision, a trait shared by many animals, including humans, has not been fully utilized in machine learning applications, despite its significant contributions to biological visual function. This study investigates whether retinotopic mapping, a critical component of foveated vision, can enhance image categorization and localization performance when integrated into deep convolutional neural networks (CNNs). Retinotopic mapping was integrated into the inputs of standard off-the-shelf convolutional neural networks (CNNs), which were then retrained on the ImageNet task. As expected, the logarithmic-polar mapping improved the network's ability to handle arbitrary image zooms and rotations, particularly for isolated objects. Surprisingly, the retinotopically mapped network achieved comparable performance in classification. Furthermore, the network demonstrated improved classification localization when the foveated center of the transform was shifted. This replicates a crucial ability of the human visual system that is absent in typical convolutional neural networks (CNNs). These findings suggest that retinotopic mapping may be fundamental to significant preattentive visual processes.</li>
</ul>

<h3>Title: Prejudice and Caprice: A Statistical Framework for Measuring Social  Discrimination in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yiran Liu (1 and 2), Ke Yang (1 and 3), Zehan Qi (2), Xiao Liu (2), Yang Yu (2), Chengxiang Zhai (3) ((1) Equal contributions, (2) Tsinghua University, (3) University of Illinois Urbana-Champaign)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15481">https://arxiv.org/abs/2402.15481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15481">https://arxiv.org/pdf/2402.15481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15481]] Prejudice and Caprice: A Statistical Framework for Measuring Social  Discrimination in Large Language Models(https://arxiv.org/abs/2402.15481)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability. However, prior discrimination measuring frameworks solely assess the average discriminatory behavior of LLMs, often proving inadequate due to the overlook of an additional discrimination-leading factor, i.e., the LLMs' prediction variation across diverse contexts. In this work, we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently biased preference and preference variation across diverse contexts. Specifically, we mathematically dissect the aggregated contextualized discrimination risk of LLMs into prejudice risk, originating from LLMs' persistent prejudice, and caprice risk, stemming from their generation inconsistency. In addition, we utilize a data-mining approach to gather preference-detecting probes from sentence skeletons, devoid of attribute indications, to approximate LLMs' applied contexts. While initially intended for assessing discrimination in LLMs, our proposed PCF facilitates the comprehensive and flexible measurement of any inductive biases, including knowledge alongside prejudice, across various modality models. We apply our discrimination-measuring framework to 12 common LLMs, yielding intriguing findings: i) modern LLMs demonstrate significant pro-male stereotypes, ii) LLMs' exhibited discrimination correlates with several social and economic factors, iii) prejudice risk dominates the overall discrimination risk and follows a normal distribution, and iv) caprice risk contributes minimally to the overall risk but follows a fat-tailed distribution, suggesting that it is wild risk requiring enhanced surveillance.</li>
</ul>

<h3>Title: A Comprehensive Survey of Convolutions in Deep Learning: Applications,  Challenges, and Future Trends</h3>
<ul>
<li><strong>Authors: </strong>Abolfazl Younesi, Mohsen Ansari, MohammadAmin Fazli, Alireza Ejlali, Muhammad Shafique, Jörg Henkel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15490">https://arxiv.org/abs/2402.15490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15490">https://arxiv.org/pdf/2402.15490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15490]] A Comprehensive Survey of Convolutions in Deep Learning: Applications,  Challenges, and Future Trends(https://arxiv.org/abs/2402.15490)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>In today's digital age, Convolutional Neural Networks (CNNs), a subset of Deep Learning (DL), are widely used for various computer vision tasks such as image classification, object detection, and image segmentation. There are numerous types of CNNs designed to meet specific needs and requirements, including 1D, 2D, and 3D CNNs, as well as dilated, grouped, attention, depthwise convolutions, and NAS, among others. Each type of CNN has its unique structure and characteristics, making it suitable for specific tasks. It's crucial to gain a thorough understanding and perform a comparative analysis of these different CNN types to understand their strengths and weaknesses. Furthermore, studying the performance, limitations, and practical applications of each type of CNN can aid in the development of new and improved architectures in the future. We also dive into the platforms and frameworks that researchers utilize for their research or development from various perspectives. Additionally, we explore the main research fields of CNN like 6D vision, generative models, and meta-learning. This survey paper provides a comprehensive examination and comparison of various CNN architectures, highlighting their architectural differences and emphasizing their respective advantages, disadvantages, applications, challenges, and future trends.</li>
</ul>

<h3>Title: API-BLEND: A Comprehensive Corpora for Training and Benchmarking API  LLMs</h3>
<ul>
<li><strong>Authors: </strong>Kinjal Basu, Ibrahim Abdelaziz, Subhajit Chaudhury, Soham Dan, Maxwell Crouse, Asim Munawar, Sadhana Kumaravel, Vinod Muthusamy, Pavan Kapanipathi, Luis A. Lastras</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15491">https://arxiv.org/abs/2402.15491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15491">https://arxiv.org/pdf/2402.15491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15491]] API-BLEND: A Comprehensive Corpora for Training and Benchmarking API  LLMs(https://arxiv.org/abs/2402.15491)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks. As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs. Two lines of research have emerged as the predominant strategies for addressing this challenge. The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks. In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-BLEND, a large corpora for training and systematic testing of tool-augmented LLMs. The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, slot filling, and sequencing of the detected APIs. We demonstrate the utility of the API-BLEND dataset for both training and benchmarking purposes.</li>
</ul>

<h3>Title: Gen4Gen: Generative Data Pipeline for Generative Multi-Concept  Composition</h3>
<ul>
<li><strong>Authors: </strong>Chun-Hsiao Yeh, Ta-Ying Cheng, He-Yen Hsieh, Chuan-En Lin, Yi Ma, Andrew Markham, Niki Trigoni, H.T. Kung, Yubei Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15504">https://arxiv.org/abs/2402.15504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15504">https://arxiv.org/pdf/2402.15504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15504]] Gen4Gen: Generative Data Pipeline for Generative Multi-Concept  Composition(https://arxiv.org/abs/2402.15504)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent text-to-image diffusion models are able to learn and synthesize images containing novel, personalized concepts (e.g., their own pets or specific items) with just a few examples for training. This paper tackles two interconnected issues within this realm of personalizing text-to-image diffusion models. First, current personalization techniques fail to reliably extend to multiple concepts -- we hypothesize this to be due to the mismatch between complex scenes and simple text descriptions in the pre-training dataset (e.g., LAION). Second, given an image containing multiple personalized concepts, there lacks a holistic metric that evaluates performance on not just the degree of resemblance of personalized concepts, but also whether all concepts are present in the image and whether the image accurately reflects the overall text description. To address these issues, we introduce Gen4Gen, a semi-automated dataset creation pipeline utilizing generative models to combine personalized concepts into complex compositions along with text-descriptions. Using this, we create a dataset called MyCanvas, that can be used to benchmark the task of multi-concept personalization. In addition, we design a comprehensive metric comprising two scores (CP-CLIP and TI-CLIP) for better quantifying the performance of multi-concept, personalized text-to-image diffusion methods. We provide a simple baseline built on top of Custom Diffusion with empirical prompting strategies for future researchers to evaluate on MyCanvas. We show that by improving data quality and prompting strategies, we can significantly increase multi-concept personalized image generation quality, without requiring any modifications to model architecture or training algorithms.</li>
</ul>

<h3>Title: Seamless Human Motion Composition with Blended Positional Encodings</h3>
<ul>
<li><strong>Authors: </strong>German Barquero, Sergio Escalera, Cristina Palmero</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15509">https://arxiv.org/abs/2402.15509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15509">https://arxiv.org/pdf/2402.15509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15509]] Seamless Human Motion Composition with Blended Positional Encodings(https://arxiv.org/abs/2402.15509)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Conditional human motion generation is an important topic with many applications in virtual reality, gaming, and robotics. While prior works have focused on generating motion guided by text, music, or scenes, these typically result in isolated motions confined to short durations. Instead, we address the generation of long, continuous sequences guided by a series of varying textual descriptions. In this context, we introduce FlowMDM, the first diffusion-based model that generates seamless Human Motion Compositions (HMC) without any postprocessing or redundant denoising steps. For this, we introduce the Blended Positional Encodings, a technique that leverages both absolute and relative positional encodings in the denoising chain. More specifically, global motion coherence is recovered at the absolute stage, whereas smooth and realistic transitions are built at the relative stage. As a result, we achieve state-of-the-art results in terms of accuracy, realism, and smoothness on the Babel and HumanML3D datasets. FlowMDM excels when trained with only a single description per motion sequence thanks to its Pose-Centric Cross-ATtention, which makes it robust against varying text descriptions at inference time. Finally, to address the limitations of existing HMC metrics, we propose two new metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt transitions.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
