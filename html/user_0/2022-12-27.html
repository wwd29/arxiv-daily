<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Weakly-Supervised Semantic Segmentation of Ships Using Thermal Imagery. (arXiv:2212.13170v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.13170">http://arxiv.org/abs/2212.13170</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.13170] Weakly-Supervised Semantic Segmentation of Ships Using Thermal Imagery](http://arxiv.org/abs/2212.13170) #secure</code></li>
<li>Summary: <p>The United States coastline spans 95,471 miles; a distance that cannot be
effectively patrolled or secured by manual human effort alone. Unmanned Aerial
Vehicles (UAVs) equipped with infrared cameras and deep-learning based
algorithms represent a more efficient alternative for identifying and
segmenting objects of interest - namely, ships. However, standard approaches to
training these algorithms require large-scale datasets of densely labeled
infrared maritime images. Such datasets are not publicly available and manually
annotating every pixel in a large-scale dataset would have an extreme labor
cost. In this work we demonstrate that, in the context of segmenting ships in
infrared imagery, weakly-supervising an algorithm with sparsely labeled data
can drastically reduce data labeling costs with minimal impact on system
performance. We apply weakly-supervised learning to an unlabeled dataset of
7055 infrared images sourced from the Naval Air Warfare Center Aircraft
Division (NAWCAD). We find that by sparsely labeling only 32 points per image,
weakly-supervised segmentation models can still effectively detect and segment
ships, with a Jaccard score of up to 0.756.
</p></li>
</ul>

<h3>Title: Scalable and Secure Row-Swap: Efficient and Safe Row Hammer Mitigation in Memory Systems. (arXiv:2212.12613v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12613">http://arxiv.org/abs/2212.12613</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12613] Scalable and Secure Row-Swap: Efficient and Safe Row Hammer Mitigation in Memory Systems](http://arxiv.org/abs/2212.12613) #secure</code></li>
<li>Summary: <p>As Dynamic Random Access Memories (DRAM) scale, they are becoming
increasingly susceptible to Row Hammer. By rapidly activating rows of DRAM
cells (aggressor rows), attackers can exploit inter-cell interference through
Row Hammer to flip bits in neighboring rows (victim rows). A recent work,
called Randomized Row-Swap (RRS), proposed proactively swapping aggressor rows
with randomly selected rows before an aggressor row can cause Row Hammer.
</p></li>
</ul>

<p>Our paper observes that RRS is neither secure nor scalable. We first propose
the `Juggernaut attack pattern' that breaks RRS in under 1 day. Juggernaut
exploits the fact that the mitigative action of RRS, a swap operation, can
itself induce additional target row activations, defeating such a defense.
Second, this paper proposes a new defense Secure Row-Swap mechanism that avoids
the additional activations from swap (and unswap) operations and protects
against Juggernaut. Furthermore, this paper extends Secure Row-Swap with attack
detection to defend against even future attacks. While this provides better
security, it also allows for securely reducing the frequency of swaps, thereby
enabling Scalable and Secure Row-Swap. The Scalable and Secure Row-Swap
mechanism provides years of Row Hammer protection with 3.3X lower storage
overheads as compared to the RRS design. It incurs only a 0.7% slowdown as
compared to a not-secure baseline for a Row Hammer threshold of 1200.
</p>

<h2>security</h2>
<h3>Title: Human Activity Recognition from Wi-Fi CSI Data Using Principal Component-Based Wavelet CNN. (arXiv:2212.13161v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.13161">http://arxiv.org/abs/2212.13161</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.13161] Human Activity Recognition from Wi-Fi CSI Data Using Principal Component-Based Wavelet CNN](http://arxiv.org/abs/2212.13161) #security</code></li>
<li>Summary: <p>Human Activity Recognition (HAR) is an emerging technology with several
applications in surveillance, security, and healthcare sectors. Noninvasive HAR
systems based on Wi-Fi Channel State Information (CSI) signals can be developed
leveraging the quick growth of ubiquitous Wi-Fi technologies, and the
correlation between CSI dynamics and body motions. In this paper, we propose
Principal Component-based Wavelet Convolutional Neural Network (or PCWCNN) -- a
novel approach that offers robustness and efficiency for practical real-time
applications. Our proposed method incorporates two efficient preprocessing
algorithms -- the Principal Component Analysis (PCA) and the Discrete Wavelet
Transform (DWT). We employ an adaptive activity segmentation algorithm that is
accurate and computationally light. Additionally, we used the Wavelet CNN for
classification, which is a deep convolutional network analogous to the
well-studied ResNet and DenseNet networks. We empirically show that our
proposed PCWCNN model performs very well on a real dataset, outperforming
existing approaches.
</p></li>
</ul>

<h3>Title: Assessing the Impact of Interface Vulnerabilities in Compartmentalized Software. (arXiv:2212.12904v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12904">http://arxiv.org/abs/2212.12904</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12904] Assessing the Impact of Interface Vulnerabilities in Compartmentalized Software](http://arxiv.org/abs/2212.12904) #security</code></li>
<li>Summary: <p>Least-privilege separation decomposes applications into compartments limited
to accessing only what they need. When compartmentalizing existing software,
many approaches neglect securing the new inter-compartment interfaces, although
what used to be a function call from/to a trusted component is now potentially
a targeted attack from a malicious compartment. This results in an entire class
of security bugs: Compartment Interface Vulnerabilities (CIVs).
</p></li>
</ul>

<p>This paper provides an in-depth study of CIVs. We taxonomize these issues and
show that they affect all known compartmentalization approaches. We propose
ConfFuzz, an in-memory fuzzer specialized to detect CIVs at possible
compartment boundaries. We apply ConfFuzz to a set of 25 popular applications
and 36 possible compartment APIs, to uncover a wide data-set of 629
vulnerabilities. We systematically study these issues, and extract numerous
insights on the prevalence of CIVs, their causes, impact, and the complexity to
address them. We stress the critical importance of CIVs in compartmentalization
approaches, demonstrating an attack to extract isolated keys in OpenSSL and
uncovering a decade-old vulnerability in sudo. We show, among others, that not
all interfaces are affected in the same way, that API size is uncorrelated with
CIV prevalence, and that addressing interface vulnerabilities goes beyond
writing simple checks. We conclude the paper with guidelines for CIV-aware
compartment interface design, and appeal for more research towards systematic
CIV detection and mitigation.
</p>

<h3>Title: Talking to the Overlooked: A Nationwide Telephone Survey with Four Groups Under-represented in Privacy and Security Studies. (arXiv:2212.12964v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12964">http://arxiv.org/abs/2212.12964</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12964] Talking to the Overlooked: A Nationwide Telephone Survey with Four Groups Under-represented in Privacy and Security Studies](http://arxiv.org/abs/2212.12964) #security</code></li>
<li>Summary: <p>Online surveys - a primary research tool in the field of usable security and
privacy research - frequently rely on web-panel platforms. However, these
platforms tend not to generalize well to specific user groups. Our study
addresses this research gap by studying security and privacy perceptions of
four under-represented groups. We conducted telephone interviews with n = 1003
participants in Germany: (I) teenagers aged 14-17, (II) older adults 70+, (III)
people with low formal education, and (IV) people with migration background. We
found these groups to be under-represented in our online comparison survey. We
further identified target group-specific perceptions for each group compared to
the general population, e.g., regarding their experiences with cybercrime, and
provide detailed insight into the privacy and security knowledge and behavior
of each group. Our findings underscore the effectiveness of telephone
interviews and lay the foundation for further research on these groups.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: zkFaith: Soonami's Zero-Knowledge Identity Protocol. (arXiv:2212.12785v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12785">http://arxiv.org/abs/2212.12785</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12785] zkFaith: Soonami's Zero-Knowledge Identity Protocol](http://arxiv.org/abs/2212.12785) #privacy</code></li>
<li>Summary: <p>Individuals are encouraged to prove their eligibility to access specific
services regularly. However, providing various organizations with personal data
spreads sensitive information and endangers people's privacy. Hence,
privacy-preserving identification systems that enable individuals to prove they
are permitted to use specific services are required to fill the gap.
Cryptographic techniques are deployed to construct identity proofs across the
internet; nonetheless, they do not offer complete control over personal data or
prevent users from forging and submitting fake data.
</p></li>
</ul>

<p>In this paper, we design a privacy-preserving identity protocol called
"zkFaith." A new approach to obtain a verified zero-knowledge identity unique
to each individual. The protocol verifies the integrity of the documents
provided by the individuals and issues a zero-knowledge-based id without
revealing any information to the authenticator or verifier. The zkFaith
leverages an aggregated version of the Camenisch-Lysyanskaya (CL) signature
scheme to sign the user's commitment to the verified personal data. Then the
users with a zero-knowledge proof system can prove that they own the required
attributes of the access criterion of the requested service providers. Vector
commitment and their position binding property enables us to, later on, update
the commitments based on the modification of the personal data; hence update
the issued zkFaith id with no requirement of initiating the protocol from
scratch. We show that the design and implementation of the zkFaith with the
generated proofs in real-world scenarios are scalable and comparable with the
state-of-the-art schemes.
</p>

<h3>Title: Packing Privacy Budget Efficiently. (arXiv:2212.13228v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.13228">http://arxiv.org/abs/2212.13228</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.13228] Packing Privacy Budget Efficiently](http://arxiv.org/abs/2212.13228) #privacy</code></li>
<li>Summary: <p>Machine learning (ML) models can leak information about users, and
differential privacy (DP) provides a rigorous way to bound that leakage under a
given budget. This DP budget can be regarded as a new type of compute resource
in workloads of multiple ML models training on user data. Once it is used, the
DP budget is forever consumed. Therefore, it is crucial to allocate it most
efficiently to train as many models as possible. This paper presents the
scheduler for privacy that optimizes for efficiency. We formulate privacy
scheduling as a new type of multidimensional knapsack problem, called privacy
knapsack, which maximizes DP budget efficiency. We show that privacy knapsack
is NP-hard, hence practical algorithms are necessarily approximate. We develop
an approximation algorithm for privacy knapsack, DPK, and evaluate it on
microbenchmarks and on a new, synthetic private-ML workload we developed from
the Alibaba ML cluster trace. We show that DPK: (1) often approaches the
efficiency-optimal schedule, (2) consistently schedules more tasks compared to
a state-of-the-art privacy scheduling algorithm that focused on fairness
(1.3-1.7x in Alibaba, 1.0-2.6x in microbenchmarks), but (3) sacrifices some
level of fairness for efficiency. Therefore, using DPK, DP ML operators should
be able to train more models on the same amount of user data while offering the
same privacy guarantee to their users.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Bias Mitigation Framework for Intersectional Subgroups in Neural Networks. (arXiv:2212.13014v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.13014">http://arxiv.org/abs/2212.13014</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.13014] Bias Mitigation Framework for Intersectional Subgroups in Neural Networks](http://arxiv.org/abs/2212.13014) #protect</code></li>
<li>Summary: <p>We propose a fairness-aware learning framework that mitigates intersectional
subgroup bias associated with protected attributes. Prior research has
primarily focused on mitigating one kind of bias by incorporating complex
fairness-driven constraints into optimization objectives or designing
additional layers that focus on specific protected attributes. We introduce a
simple and generic bias mitigation approach that prevents models from learning
relationships between protected attributes and output variable by reducing
mutual information between them. We demonstrate that our approach is effective
in reducing bias with little or no drop in accuracy. We also show that the
models trained with our learning framework become causally fair and insensitive
to the values of protected attributes. Finally, we validate our approach by
studying feature interactions between protected and non-protected attributes.
We demonstrate that these interactions are significantly reduced when applying
our bias mitigation.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Simultaneously Optimizing Perturbations and Positions for Black-box Adversarial Patch Attacks. (arXiv:2212.12995v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12995">http://arxiv.org/abs/2212.12995</a></li>
<li>Code URL: <a href="https://github.com/shighghyujie/newpatch-rl">https://github.com/shighghyujie/newpatch-rl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12995] Simultaneously Optimizing Perturbations and Positions for Black-box Adversarial Patch Attacks](http://arxiv.org/abs/2212.12995) #attack</code></li>
<li>Summary: <p>Adversarial patch is an important form of real-world adversarial attack that
brings serious risks to the robustness of deep neural networks. Previous
methods generate adversarial patches by either optimizing their perturbation
values while fixing the pasting position or manipulating the position while
fixing the patch's content. This reveals that the positions and perturbations
are both important to the adversarial attack. For that, in this paper, we
propose a novel method to simultaneously optimize the position and perturbation
for an adversarial patch, and thus obtain a high attack success rate in the
black-box setting. Technically, we regard the patch's position, the
pre-designed hyper-parameters to determine the patch's perturbations as the
variables, and utilize the reinforcement learning framework to simultaneously
solve for the optimal solution based on the rewards obtained from the target
model with a small number of queries. Extensive experiments are conducted on
the Face Recognition (FR) task, and results on four representative FR models
show that our method can significantly improve the attack success rate and
query efficiency. Besides, experiments on the commercial FR service and
physical environments confirm its practical application value. We also extend
our method to the traffic sign recognition task to verify its generalization
ability.
</p></li>
</ul>

<h3>Title: Efficiently Hardening SGX Enclaves against Memory Access Pattern Attacks via Dynamic Program Partitioning. (arXiv:2212.12656v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12656">http://arxiv.org/abs/2212.12656</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12656] Efficiently Hardening SGX Enclaves against Memory Access Pattern Attacks via Dynamic Program Partitioning](http://arxiv.org/abs/2212.12656) #attack</code></li>
<li>Summary: <p>Intel SGX is known to be vulnerable to a class of practical attacks
exploiting memory access pattern side-channels, notably page-fault attacks and
cache timing attacks. A promising hardening scheme is to wrap applications in
hardware transactions, enabled by Intel TSX, that return control to the
software upon unexpected cache misses and interruptions so that the existing
side-channel attacks exploiting these micro-architectural events can be
detected and mitigated. However, existing hardening schemes scale only to
small-data computation, with a typical working set smaller than one or few
times (e.g., $8$ times) of a CPU data cache.
</p></li>
</ul>

<p>This work tackles the data scalability and performance efficiency of security
hardening schemes of Intel SGX enclaves against memory-access pattern side
channels. The key insight is that the size of TSX transactions in the target
computation is critical, both performance- and security-wise. Unlike the
existing designs, this work dynamically partitions target computations to
enlarge transactions while avoiding aborts, leading to lower performance
overhead and improved side-channel security. We materialize the dynamic
partitioning scheme and build a C++ library to monitor and model cache
utilization at runtime. We further build a data analytical system using the
library and implement various external oblivious algorithms. Performance
evaluation shows that our work can effectively increase transaction size and
reduce the execution time by up to two orders of magnitude compared with the
state-of-the-art solutions.
</p>

<h3>Title: Bernoulli honeywords. (arXiv:2212.12759v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12759">http://arxiv.org/abs/2212.12759</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12759] Bernoulli honeywords](http://arxiv.org/abs/2212.12759) #attack</code></li>
<li>Summary: <p>Decoy passwords, or ``honeywords,'' planted in a credential database can
alert a site to its breach if ever submitted in a login attempt. To be
effective, some honeywords must appear at least as likely to be user-chosen
passwords as the real ones, and honeywords must be very difficult to guess
without having breached the database, to prevent false breach alarms. These
goals have proved elusive, however, for heuristic honeyword generation
algorithms. In this paper we explore an alternative strategy in which the
defender treats honeyword selection as a Bernoulli process in which each
possible password (except the user-chosen one) is selected as a honeyword
independently with some fixed probability. We show how Bernoulli honeywords can
be integrated into two existing system designs for leveraging honeywords: one
based on a honeychecker that stores the secret index of the user-chosen
password in the list of account passwords, and another that does not leverage
secret state at all. We show that Bernoulli honeywords enable analytic
derivation of false breach-detection probabilities irrespective of what
information the attacker gathers about the sites' users; that their true and
false breach-detection probabilities demonstrate compelling efficacy; and that
Bernoulli honeywords can even enable performance improvements in modern
honeyword system designs.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Frequency Regularization for Improving Adversarial Robustness. (arXiv:2212.12732v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12732">http://arxiv.org/abs/2212.12732</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12732] Frequency Regularization for Improving Adversarial Robustness](http://arxiv.org/abs/2212.12732) #robust</code></li>
<li>Summary: <p>Deep neural networks are incredibly vulnerable to crafted,
human-imperceptible adversarial perturbations. Although adversarial training
(AT) has proven to be an effective defense approach, we find that the
AT-trained models heavily rely on the input low-frequency content for judgment,
accounting for the low standard accuracy. To close the large gap between the
standard and robust accuracies during AT, we investigate the frequency
difference between clean and adversarial inputs, and propose a frequency
regularization (FR) to align the output difference in the spectral domain.
Besides, we find Stochastic Weight Averaging (SWA), by smoothing the kernels
over epochs, further improves the robustness. Among various defense schemes,
our method achieves the strongest robustness against attacks by PGD-20, C\&amp;W
and Autoattack, on a WideResNet trained on CIFAR-10 without any extra data.
</p></li>
</ul>

<h3>Title: A Lightweight Reconstruction Network for Surface Defect Inspection. (arXiv:2212.12878v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12878">http://arxiv.org/abs/2212.12878</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12878] A Lightweight Reconstruction Network for Surface Defect Inspection](http://arxiv.org/abs/2212.12878) #robust</code></li>
<li>Summary: <p>Currently, most deep learning methods cannot solve the problem of scarcity of
industrial product defect samples and significant differences in
characteristics. This paper proposes an unsupervised defect detection algorithm
based on a reconstruction network, which is realized using only a large number
of easily obtained defect-free sample data. The network includes two parts:
image reconstruction and surface defect area detection. The reconstruction
network is designed through a fully convolutional autoencoder with a
lightweight structure. Only a small number of normal samples are used for
training so that the reconstruction network can be A defect-free reconstructed
image is generated. A function combining structural loss and $\mathit{L}1$ loss
is proposed as the loss function of the reconstruction network to solve the
problem of poor detection of irregular texture surface defects. Further, the
residual of the reconstructed image and the image to be tested is used as the
possible region of the defect, and conventional image operations can realize
the location of the fault. The unsupervised defect detection algorithm of the
proposed reconstruction network is used on multiple defect image sample sets.
Compared with other similar algorithms, the results show that the unsupervised
defect detection algorithm of the reconstructed network has strong robustness
and accuracy.
</p></li>
</ul>

<h3>Title: Human Health Indicator Prediction from Gait Video. (arXiv:2212.12948v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12948">http://arxiv.org/abs/2212.12948</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12948] Human Health Indicator Prediction from Gait Video](http://arxiv.org/abs/2212.12948) #robust</code></li>
<li>Summary: <p>Body Mass Index (BMI), age, height and weight are important indicators of
human health conditions, which can provide useful information for plenty of
practical purposes, such as health care, monitoring and re-identification. Most
existing methods of health indicator prediction mainly use front-view body or
face images. These inputs are hard to be obtained in daily life and often lead
to the lack of robustness for the models, considering their strict requirements
on view and pose. In this paper, we propose to employ gait videos to predict
health indicators, which are more prevalent in surveillance and home monitoring
scenarios. However, the study of health indicator prediction from gait videos
using deep learning was hindered due to the small amount of open-sourced data.
To address this issue, we analyse the similarity and relationship between pose
estimation and health indicator prediction tasks, and then propose a paradigm
enabling deep learning for small health indicator datasets by pre-training on
the pose estimation task. Furthermore, to better suit the health indicator
prediction task, we bring forward Global-Local Aware aNd Centrosymmetric
Encoder (GLANCE) module. It first extracts local and global features by
progressive convolutions and then fuses multi-level features by a
centrosymmetric double-path hourglass structure in two different ways.
</p></li>
</ul>

<p>Experiments demonstrate that the proposed paradigm achieves state-of-the-art
results for predicting health indicators on MoVi, and that the GLANCE module is
also beneficial for pose estimation on 3DPW.
</p>

<h3>Title: Improving Continuous Sign Language Recognition with Consistency Constraints and Signer Removal. (arXiv:2212.13023v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.13023">http://arxiv.org/abs/2212.13023</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.13023] Improving Continuous Sign Language Recognition with Consistency Constraints and Signer Removal](http://arxiv.org/abs/2212.13023) #robust</code></li>
<li>Summary: <p>Most deep-learning-based continuous sign language recognition (CSLR) models
share a similar backbone consisting of a visual module, a sequential module,
and an alignment module. However, due to limited training samples, a
connectionist temporal classification loss may not train such CSLR backbones
sufficiently. In this work, we propose three auxiliary tasks to enhance the
CSLR backbones. The first task enhances the visual module, which is sensitive
to the insufficient training problem, from the perspective of consistency.
Specifically, since the information of sign languages is mainly included in
signers' facial expressions and hand movements, a keypoint-guided spatial
attention module is developed to enforce the visual module to focus on
informative regions, i.e., spatial attention consistency. Second, noticing that
both the output features of the visual and sequential modules represent the
same sentence, to better exploit the backbone's power, a sentence embedding
consistency constraint is imposed between the visual and sequential modules to
enhance the representation power of both features. We name the CSLR model
trained with the above auxiliary tasks as consistency-enhanced CSLR, which
performs well on signer-dependent datasets in which all signers appear during
both training and testing. To make it more robust for the signer-independent
setting, a signer removal module based on feature disentanglement is further
proposed to remove signer information from the backbone. Extensive ablation
studies are conducted to validate the effectiveness of these auxiliary tasks.
More remarkably, with a transformer-based backbone, our model achieves
state-of-the-art or competitive performance on five benchmarks, PHOENIX-2014,
PHOENIX-2014-T, PHOENIX-2014-SI, CSL, and CSL-Daily.
</p></li>
</ul>

<h3>Title: Semantic Enhanced Knowledge Graph for Large-Scale Zero-Shot Learning. (arXiv:2212.13151v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.13151">http://arxiv.org/abs/2212.13151</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.13151] Semantic Enhanced Knowledge Graph for Large-Scale Zero-Shot Learning](http://arxiv.org/abs/2212.13151) #robust</code></li>
<li>Summary: <p>Zero-Shot Learning has been a highlighted research topic in both vision and
language areas. Recently, most existing methods adopt structured knowledge
information to model explicit correlations among categories and use deep graph
convolutional network to propagate information between different categories.
However, it is difficult to add new categories to existing structured knowledge
graph, and deep graph convolutional network suffers from over-smoothing
problem. In this paper, we provide a new semantic enhanced knowledge graph that
contains both expert knowledge and categories semantic correlation. Our
semantic enhanced knowledge graph can further enhance the correlations among
categories and make it easy to absorb new categories. To propagate information
on the knowledge graph, we propose a novel Residual Graph Convolutional Network
(ResGCN), which can effectively alleviate the problem of over-smoothing.
Experiments conducted on the widely used large-scale ImageNet-21K dataset and
AWA2 dataset show the effectiveness of our method, and establish a new
state-of-the-art on zero-shot learning. Moreover, our results on the
large-scale ImageNet-21K with various feature extraction networks show that our
method has better generalization and robustness.
</p></li>
</ul>

<h3>Title: Fully Differentiable RANSAC. (arXiv:2212.13185v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.13185">http://arxiv.org/abs/2212.13185</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.13185] Fully Differentiable RANSAC](http://arxiv.org/abs/2212.13185) #robust</code></li>
<li>Summary: <p>We propose the fully differentiable $\nabla$-RANSAC.It predicts the inlier
probabilities of the input data points, exploits the predictions in a guided
sampler, and estimates the model parameters (e.g., fundamental matrix) and its
quality while propagating the gradients through the entire procedure. The
random sampler in $\nabla$-RANSAC is based on a clever re-parametrization
strategy, i.e.\ the Gumbel Softmax sampler, that allows propagating the
gradients directly into the subsequent differentiable minimal solver. The model
quality function marginalizes over the scores from all models estimated within
$\nabla$-RANSAC to guide the network learning accurate and useful
probabilities.$\nabla$-RANSAC is the first to unlock the end-to-end training of
geometric estimation pipelines, containing feature detection, matching and
RANSAC-like randomized robust estimation. As a proof of its potential, we train
$\nabla$-RANSAC together with LoFTR, i.e. a recent detector-free feature
matcher, to find reliable correspondences in an end-to-end manner. We test
$\nabla$-RANSAC on a number of real-world datasets on fundamental and essential
matrix estimation. It is superior to the state-of-the-art in terms of accuracy
while being among the fastest methods. The code and trained models will be made
public.
</p></li>
</ul>

<h3>Title: A Bayesian Robust Regression Method for Corrupted Data Reconstruction. (arXiv:2212.12787v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12787">http://arxiv.org/abs/2212.12787</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12787] A Bayesian Robust Regression Method for Corrupted Data Reconstruction](http://arxiv.org/abs/2212.12787) #robust</code></li>
<li>Summary: <p>Because of the widespread existence of noise and data corruption, recovering
the true regression parameters with a certain proportion of corrupted response
variables is an essential task. Methods to overcome this problem often involve
robust least-squares regression, but few methods perform well when confronted
with severe adaptive adversarial attacks. In many applications, prior knowledge
is often available from historical data or engineering experience, and by
incorporating prior information into a robust regression method, we develop an
effective robust regression method that can resist adaptive adversarial
attacks. First, we propose the novel TRIP (hard Thresholding approach to Robust
regression with sImple Prior) algorithm, which improves the breakdown point
when facing adaptive adversarial attacks. Then, to improve the robustness and
reduce the estimation error caused by the inclusion of priors, we use the idea
of Bayesian reweighting to construct the more robust BRHT (robust Bayesian
Reweighting regression via Hard Thresholding) algorithm. We prove the
theoretical convergence of the proposed algorithms under mild conditions, and
extensive experiments show that under different types of dataset attacks, our
algorithms outperform other benchmark ones. Finally, we apply our methods to a
data-recovery problem in a real-world application involving a space solar
array, demonstrating their good applicability.
</p></li>
</ul>

<h3>Title: Robust computation of optimal transport by $\beta$-potential regularization. (arXiv:2212.13251v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.13251">http://arxiv.org/abs/2212.13251</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.13251] Robust computation of optimal transport by $\beta$-potential regularization](http://arxiv.org/abs/2212.13251) #robust</code></li>
<li>Summary: <p>Optimal transport (OT) has become a widely used tool in the machine learning
field to measure the discrepancy between probability distributions. For
instance, OT is a popular loss function that quantifies the discrepancy between
an empirical distribution and a parametric model. Recently, an entropic penalty
term and the celebrated Sinkhorn algorithm have been commonly used to
approximate the original OT in a computationally efficient way. However, since
the Sinkhorn algorithm runs a projection associated with the Kullback-Leibler
divergence, it is often vulnerable to outliers. To overcome this problem, we
propose regularizing OT with the \beta-potential term associated with the
so-called $\beta$-divergence, which was developed in robust statistics. Our
theoretical analysis reveals that the $\beta$-potential can prevent the mass
from being transported to outliers. We experimentally demonstrate that the
transport matrix computed with our algorithm helps estimate a probability
distribution robustly even in the presence of outliers. In addition, our
proposed method can successfully detect outliers from a contaminated dataset
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Artificial Pupil Dilation for Data Augmentation in Iris Semantic Segmentation. (arXiv:2212.12733v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12733">http://arxiv.org/abs/2212.12733</a></li>
<li>Code URL: <a href="https://github.com/dpbenalcazar/ArtificialDilation">https://github.com/dpbenalcazar/ArtificialDilation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12733] Artificial Pupil Dilation for Data Augmentation in Iris Semantic Segmentation](http://arxiv.org/abs/2212.12733) #biometric</code></li>
<li>Summary: <p>Biometrics is the science of identifying an individual based on their
intrinsic anatomical or behavioural characteristics, such as fingerprints,
face, iris, gait, and voice. Iris recognition is one of the most successful
methods because it exploits the rich texture of the human iris, which is unique
even for twins and does not degrade with age. Modern approaches to iris
recognition utilize deep learning to segment the valid portion of the iris from
the rest of the eye, so it can then be encoded, stored and compared. This paper
aims to improve the accuracy of iris semantic segmentation systems by
introducing a novel data augmentation technique. Our method can transform an
iris image with a certain dilation level into any desired dilation level, thus
augmenting the variability and number of training examples from a small
dataset. The proposed method is fast and does not require training. The results
indicate that our data augmentation method can improve segmentation accuracy up
to 15% for images with high pupil dilation, which creates a more reliable iris
recognition pipeline, even under extreme dilation.
</p></li>
</ul>

<h3>Title: TypeFormer: Transformers for Mobile Keystroke Biometrics. (arXiv:2212.13075v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.13075">http://arxiv.org/abs/2212.13075</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.13075] TypeFormer: Transformers for Mobile Keystroke Biometrics](http://arxiv.org/abs/2212.13075) #biometric</code></li>
<li>Summary: <p>The broad usage of mobile devices nowadays, the sensitiveness of the
information contained in them, and the shortcomings of current mobile user
authentication methods are calling for novel, secure, and unobtrusive solutions
to verify the users' identity. In this article, we propose TypeFormer, a novel
Transformer architecture to model free-text keystroke dynamics performed on
mobile devices for the purpose of user authentication. The proposed model
consists in Temporal and Channel Modules enclosing two Long Short-Term Memory
(LSTM) recurrent layers, Gaussian Range Encoding (GRE), a multi-head
Self-Attention mechanism, and a Block-Recurrent structure. Experimenting on one
of the largest public databases to date, the Aalto mobile keystroke database,
TypeFormer outperforms current state-of-the-art systems achieving Equal Error
Rate (EER) values of 3.25% using only 5 enrolment sessions of 50 keystrokes
each. In such way, we contribute to reducing the traditional performance gap of
the challenging mobile free-text scenario with respect to its desktop and
fixed-text counterparts. Additionally, we analyse the behaviour of the model
with different experimental configurations such as the length of the keystroke
sequences and the amount of enrolment sessions, showing margin for improvement
with more enrolment data. Finally, a cross-database evaluation is carried out,
demonstrating the robustness of the features extracted by TypeFormer in
comparison with existing approaches.
</p></li>
</ul>

<h3>Title: Advancements in Biometric Technology with Artificial Intelligence. (arXiv:2212.13187v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.13187">http://arxiv.org/abs/2212.13187</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.13187] Advancements in Biometric Technology with Artificial Intelligence](http://arxiv.org/abs/2212.13187) #biometric</code></li>
<li>Summary: <p>Authentication plays a significant part in dealing with security in public
and private sectors such as healthcare systems, banking system, transportation
system and law and security. Biometric technology has grown quickly recently,
especially in the areas of artificial intelligence and identity. Formerly,
authentication process has depended on security measures like passcodes,
identity fobs, and fingerprints. On the other hand, as just a consequence of
these precautions, theft has increased in frequency. In response, biometric
security was created, in which the identification of a person is based on
features derived from the physiological and behavioral traits of a human body
using biometric system. Biometric technology gadgets are available to the
public as they are embedded on computer systems, electronic devices, mobile
phones, and other consumer electronics. As the fraudulent is increasing demand
and use of biometric electronic devices has increased. As a consequence, it may
be possible to confirm a person's distinct identification. The goal of this
study is to examine developments in biometric systems in the disciplines of
medicine and engineering. The study will present the perspectives and different
points of view of the secondary data, highlighting the need for more in-depth
understanding and application of biometric technology to promote its
development in the digital era. The study's findings may inspire people and
businesses to more effectively incorporate biometric technologies in order to
reduce the risks to data and identity security.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: A Marker-based Neural Network System for Extracting Social Determinants of Health. (arXiv:2212.12800v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12800">http://arxiv.org/abs/2212.12800</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12800] A Marker-based Neural Network System for Extracting Social Determinants of Health](http://arxiv.org/abs/2212.12800) #extraction</code></li>
<li>Summary: <p>Objective. The impact of social determinants of health (SDoH) on patients'
healthcare quality and the disparity is well-known. Many SDoH items are not
coded in structured forms in electronic health records. These items are often
captured in free-text clinical notes, but there are limited methods for
automatically extracting them. We explore a multi-stage pipeline involving
named entity recognition (NER), relation classification (RC), and text
classification methods to extract SDoH information from clinical notes
automatically.
</p></li>
</ul>

<p>Materials and Methods. The study uses the N2C2 Shared Task data, which was
collected from two sources of clinical notes: MIMIC-III and University of
Washington Harborview Medical Centers. It contains 4480 social history sections
with full annotation for twelve SDoHs. In order to handle the issue of
overlapping entities, we developed a novel marker-based NER model. We used it
in a multi-stage pipeline to extract SDoH information from clinical notes.
</p>
<p>Results. Our marker-based system outperformed the state-of-the-art span-based
models at handling overlapping entities based on the overall Micro-F1 score
performance. It also achieved state-of-the-art performance compared to the
shared task methods.
</p>
<p>Conclusion. The major finding of this study is that the multi-stage pipeline
effectively extracts SDoH information from clinical notes. This approach can
potentially improve the understanding and tracking of SDoHs in clinical
settings. However, error propagation may be an issue, and further research is
needed to improve the extraction of entities with complex semantic meanings and
low-resource entities using external knowledge.
</p>

<h3>Title: Saliency-Augmented Memory Completion for Continual Learning. (arXiv:2212.13242v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.13242">http://arxiv.org/abs/2212.13242</a></li>
<li>Code URL: <a href="https://github.com/baithebest/samc">https://github.com/baithebest/samc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.13242] Saliency-Augmented Memory Completion for Continual Learning](http://arxiv.org/abs/2212.13242) #extraction</code></li>
<li>Summary: <p>Continual Learning is considered a key step toward next-generation Artificial
Intelligence. Among various methods, replay-based approaches that maintain and
replay a small episodic memory of previous samples are one of the most
successful strategies against catastrophic forgetting. However, since
forgetting is inevitable given bounded memory and unbounded tasks, how to
forget is a problem continual learning must address. Therefore, beyond simply
avoiding catastrophic forgetting, an under-explored issue is how to reasonably
forget while ensuring the merits of human memory, including 1. storage
efficiency, 2. generalizability, and 3. some interpretability. To achieve these
simultaneously, our paper proposes a new saliency-augmented memory completion
framework for continual learning, inspired by recent discoveries in memory
completion separation in cognitive neuroscience. Specifically, we innovatively
propose to store the part of the image most important to the tasks in episodic
memory by saliency map extraction and memory encoding. When learning new tasks,
previous data from memory are inpainted by an adaptive data generation module,
which is inspired by how humans complete episodic memory. The module's
parameters are shared across all tasks and it can be jointly trained with a
continual learning classifier as bilevel optimization. Extensive experiments on
several continual learning and image classification benchmarks demonstrate the
proposed method's effectiveness and efficiency.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: LOCKS: User Differentially Private and Federated Optimal Client Sampling. (arXiv:2212.13071v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.13071">http://arxiv.org/abs/2212.13071</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.13071] LOCKS: User Differentially Private and Federated Optimal Client Sampling](http://arxiv.org/abs/2212.13071) #federate</code></li>
<li>Summary: <p>With changes in privacy laws, there is often a hard requirement for client
data to remain on the device rather than being sent to the server. Therefore,
most processing happens on the device, and only an altered element is sent to
the server. Such mechanisms are developed by leveraging differential privacy
and federated learning. Differential privacy adds noise to the client outputs
and thus deteriorates the quality of each iteration. This distributed setting
adds a layer of complexity and additional communication and performance
overhead. These costs are additive per round, so we need to reduce the number
of iterations. In this work, we provide an analytical framework for studying
the convergence guarantees of gradient-based distributed algorithms. We show
that our private algorithm minimizes the expected gradient variance by
approximately $d^2$ rounds, where d is the dimensionality of the model. We
discuss and suggest novel ways to improve the convergence rate to minimize the
overhead using Importance Sampling (IS) and gradient diversity. Finally, we
provide alternative frameworks that might be better suited to exploit client
sampling techniques like IS and gradient diversity.
</p></li>
</ul>

<h3>Title: When Do Curricula Work in Federated Learning?. (arXiv:2212.12712v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12712">http://arxiv.org/abs/2212.12712</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12712] When Do Curricula Work in Federated Learning?](http://arxiv.org/abs/2212.12712) #federate</code></li>
<li>Summary: <p>An oft-cited open problem of federated learning is the existence of data
heterogeneity at the clients. One pathway to understanding the drastic accuracy
drop in federated learning is by scrutinizing the behavior of the clients' deep
models on data with different levels of "difficulty", which has been left
unaddressed. In this paper, we investigate a different and rarely studied
dimension of FL: ordered learning. Specifically, we aim to investigate how
ordered learning principles can contribute to alleviating the heterogeneity
effects in FL. We present theoretical analysis and conduct extensive empirical
studies on the efficacy of orderings spanning three kinds of learning:
curriculum, anti-curriculum, and random curriculum. We find that curriculum
learning largely alleviates non-IIDness. Interestingly, the more disparate the
data distributions across clients the more they benefit from ordered learning.
We provide analysis explaining this phenomenon, specifically indicating how
curriculum training appears to make the objective landscape progressively less
convex, suggesting fast converging iterations at the beginning of the training
procedure. We derive quantitative results of convergence for both convex and
nonconvex objectives by modeling the curriculum training on federated devices
as local SGD with locally biased stochastic gradients. Also, inspired by
ordered learning, we propose a novel client selection technique that benefits
from the real-world disparity in the clients. Our proposed approach to client
selection has a synergic effect when applied together with ordered learning in
FL.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: A Comprehensive Study of Gender Bias in Chemical Named Entity Recognition Models. (arXiv:2212.12799v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12799">http://arxiv.org/abs/2212.12799</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12799] A Comprehensive Study of Gender Bias in Chemical Named Entity Recognition Models](http://arxiv.org/abs/2212.12799) #fair</code></li>
<li>Summary: <p>Objective. Chemical named entity recognition (NER) models have the potential
to impact a wide range of downstream tasks, from identifying adverse drug
reactions to general pharmacoepidemiology. However, it is unknown whether these
models work the same for everyone. Performance disparities can potentially
cause harm rather than the intended good. Hence, in this paper, we measure
gender-related performance disparities of chemical NER systems.
</p></li>
</ul>

<p>Materials and Methods. We develop a framework to measure gender bias in
chemical NER models using synthetic data and a newly annotated dataset of over
92,405 words with self-identified gender information from Reddit. We applied
and evaluated state-of-the-art biomedical NER models.
</p>
<p>Results. Our findings indicate that chemical NER models are biased. The
results of the bias tests on the synthetic dataset and the real-world data
multiple fairness issues. For example, for synthetic data, we find that
female-related names are generally classified as chemicals, particularly in
datasets containing many brand names rather than standard ones. For both
datasets, we find consistent fairness issues resulting in substantial
performance disparities between female- and male-related data.
</p>
<p>Discussion. Our study highlights the issue of biases in chemical NER models.
For example, we find that many systems cannot detect contraceptives (e.g.,
birth control).
</p>
<p>Conclusion. Chemical NER models are biased and can be harmful to
female-related groups. Therefore, practitioners should carefully consider the
potential biases of these models and take steps to mitigate them.
</p>

<h3>Title: The URW-KG: a Resource for Tackling the Underrepresentation of non-Western Writers. (arXiv:2212.13104v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.13104">http://arxiv.org/abs/2212.13104</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.13104] The URW-KG: a Resource for Tackling the Underrepresentation of non-Western Writers](http://arxiv.org/abs/2212.13104) #fair</code></li>
<li>Summary: <p>Digital media have enabled the access to unprecedented literary knowledge.
Authors, readers, and scholars are now able to discover and share an increasing
amount of information about books and their authors. Notwithstanding, digital
archives are still unbalanced: writers from non-Western countries are less
represented, and such a condition leads to the perpetration of old forms of
discrimination. In this paper, we present the Under-Represented Writers
Knowledge Graph (URW-KG), a resource designed to explore and possibly amend
this lack of representation by gathering and mapping information about works
and authors from Wikidata and three other sources: Open Library, Goodreads, and
Google Books. The experiments based on KG embeddings showed that the integrated
information encoded in the graph allows scholars and users to be more easily
exposed to non-Western literary works and authors with respect to Wikidata
alone. This opens to the development of fairer and effective tools for author
discovery and exploration.
</p></li>
</ul>

<h3>Title: Stochastic Methods for AUC Optimization subject to AUC-based Fairness Constraints. (arXiv:2212.12603v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12603">http://arxiv.org/abs/2212.12603</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12603] Stochastic Methods for AUC Optimization subject to AUC-based Fairness Constraints](http://arxiv.org/abs/2212.12603) #fair</code></li>
<li>Summary: <p>As machine learning being used increasingly in making high-stakes decisions,
an arising challenge is to avoid unfair AI systems that lead to discriminatory
decisions for protected population. A direct approach for obtaining a fair
predictive model is to train the model through optimizing its prediction
performance subject to fairness constraints, which achieves Pareto efficiency
when trading off performance against fairness. Among various fairness metrics,
the ones based on the area under the ROC curve (AUC) are emerging recently
because they are threshold-agnostic and effective for unbalanced data. In this
work, we formulate the training problem of a fairness-aware machine learning
model as an AUC optimization problem subject to a class of AUC-based fairness
constraints. This problem can be reformulated as a min-max optimization problem
with min-max constraints, which we solve by stochastic first-order methods
based on a new Bregman divergence designed for the special structure of the
problem. We numerically demonstrate the effectiveness of our approach on
real-world data under different fairness metrics.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>explainability</h2>
<h2>watermark</h2>
<h3>Title: Adaptive Blind Watermarking Using Psychovisual Image Features. (arXiv:2212.12864v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12864">http://arxiv.org/abs/2212.12864</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12864] Adaptive Blind Watermarking Using Psychovisual Image Features](http://arxiv.org/abs/2212.12864) #watermark</code></li>
<li>Summary: <p>With the growth of editing and sharing images through the internet, the
importance of protecting the images' authorship has increased. Robust
watermarking is a known approach to maintaining copyright protection.
Robustness and imperceptibility are two factors that are tried to be maximized
through watermarking. Usually, there is a trade-off between these two
parameters. Increasing the robustness would lessen the imperceptibility of the
watermarking. This paper proposes an adaptive method that determines the
strength of the watermark embedding in different parts of the cover image
regarding its texture and brightness. Adaptive embedding increases the
robustness while preserving the quality of the watermarked image. Experimental
results also show that the proposed method can effectively reconstruct the
embedded payload in different kinds of common watermarking attacks. Our
proposed method has shown good performance compared to a recent technique.
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: Unsupervised Representation Learning from Pre-trained Diffusion Probabilistic Models. (arXiv:2212.12990v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12990">http://arxiv.org/abs/2212.12990</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12990] Unsupervised Representation Learning from Pre-trained Diffusion Probabilistic Models](http://arxiv.org/abs/2212.12990) #diffusion</code></li>
<li>Summary: <p>Diffusion Probabilistic Models (DPMs) have shown a powerful capacity of
generating high-quality image samples. Recently, diffusion autoencoders
(Diff-AE) have been proposed to explore DPMs for representation learning via
autoencoding. Their key idea is to jointly train an encoder for discovering
meaningful representations from images and a conditional DPM as the decoder for
reconstructing images. Considering that training DPMs from scratch will take a
long time and there have existed numerous pre-trained DPMs, we propose
\textbf{P}re-trained \textbf{D}PM \textbf{A}uto\textbf{E}ncoding
(\textbf{PDAE}), a general method to adapt existing pre-trained DPMs to the
decoders for image reconstruction, with better training efficiency and
performance than Diff-AE. Specifically, we find that the reason that
pre-trained DPMs fail to reconstruct an image from its latent variables is due
to the information loss of forward process, which causes a gap between their
predicted posterior mean and the true one. From this perspective, the
classifier-guided sampling method can be explained as computing an extra mean
shift to fill the gap, reconstructing the lost class information in samples.
These imply that the gap corresponds to the lost information of the image, and
we can reconstruct the image by filling the gap. Drawing inspiration from this,
we employ a trainable model to predict a mean shift according to encoded
representation and train it to fill as much gap as possible, in this way, the
encoder is forced to learn as much information as possible from images to help
the filling. By reusing a part of network of pre-trained DPMs and redesigning
the weighting scheme of diffusion loss, PDAE can learn meaningful
representations from images efficiently. Extensive experiments demonstrate the
effectiveness, efficiency and flexibility of PDAE.
</p></li>
</ul>

<h3>Title: Your diffusion model secretly knows the dimension of the data manifold. (arXiv:2212.12611v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.12611">http://arxiv.org/abs/2212.12611</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.12611] Your diffusion model secretly knows the dimension of the data manifold](http://arxiv.org/abs/2212.12611) #diffusion</code></li>
<li>Summary: <p>In this work, we propose a novel framework for estimating the dimension of
the data manifold using a trained diffusion model. A trained diffusion model
approximates the gradient of the log density of a noise-corrupted version of
the target distribution for varying levels of corruption. If the data
concentrates around a manifold embedded in the high-dimensional ambient space,
then as the level of corruption decreases, the score function points towards
the manifold, as this direction becomes the direction of maximum likelihood
increase. Therefore, for small levels of corruption, the diffusion model
provides us with access to an approximation of the normal bundle of the data
manifold. This allows us to estimate the dimension of the tangent space, thus,
the intrinsic dimension of the data manifold. Our method outperforms linear
methods for dimensionality detection such as PPCA in controlled experiments.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
