<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-06-02</h1>
<h3>Title: A comprehensive survey of cybercrimes in India over the last decade</h3>
<ul>
<li><strong>Authors: </strong>Sudhanshu Sekhar Tripathy</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23770">https://arxiv.org/abs/2505.23770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23770">https://arxiv.org/pdf/2505.23770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23770]] A comprehensive survey of cybercrimes in India over the last decade(https://arxiv.org/abs/2505.23770)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, robust</a></li>
<li><strong>Abstract: </strong>Since the 1990s, the integration of technology into daily life has led to the creation of an extensive network of interconnected devices, transforming how individuals and organizations operate. However, this digital transformation has also spurred the rise of cybercrime, criminal activities perpetrated through networks or computer systems. Cybercrime has become a global concern, presenting significant challenges to security systems. Although advancements in digital technology have enhanced efficiency, they have also opened new avenues for exploitation by cybercriminals, highlighting the urgent need for advanced cybersecurity measures. The escalating number of cyberattacks and associated risks in the past decade highlights the critical importance of protecting sensitive data and safeguarding information systems. Cybercrimes range from financial fraud and phishing scams to identity theft and online harassment, posing substantial risks to both individuals and organizations. In response, governments, law enforcement agencies, and cybersecurity units have intensified their efforts to address these threats. In recent years, India has experienced a significant surge in cybercrime incidents, with a notable increase in cases involving ransomware, data breaches, and social engineering attacks. The growing penetration of internet services, the expansion of e-commerce, and the rapid adoption of digital payment systems have made individuals and organizations more vulnerable to cyber threats. Key areas affected include banking, healthcare, and government sectors, which are frequently targeted due to the sensitive nature of the data they handle. To combat these risks, there is an increasing focus on public awareness, cybersecurity education, and robust regulatory frameworks. This paper examines cybercrime, prevention strategies, security protocols, and terminology to safeguard digital infrastructure.</li>
</ul>

<h3>Title: AESHA3: Efficient and Secure Sub-Key Generation for AES Using SHA-3</h3>
<ul>
<li><strong>Authors: </strong>Ankush Soni, Sanjay K. Sahay, Parit Mehta</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23771">https://arxiv.org/abs/2505.23771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23771">https://arxiv.org/pdf/2505.23771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23771]] AESHA3: Efficient and Secure Sub-Key Generation for AES Using SHA-3(https://arxiv.org/abs/2505.23771)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Advanced Encryption Standard (AES) is one of the most widely used symmetric cipher for the confidentiality of data. Also it is used for other security services, viz. integrity, authentication and key establishment. However, recently, authors have shown some weakness in the generation of sub-keys in AES, e.g. bit leakage attack, etc. Also, AES sub-keys are generated sequentially, which is an overhead, especially for resource-constrained devices. Therefore, we propose and investigate a novel encryption AESHA3, which uses sub-keys generated by Secure Hash Algorithm-3 (SHA3). The output of SHA3 is one-way and highly non-linear, and random. The experimental analysis shows that the average time taken for generating the sub-keys to be used for encrypting the data using our approach i.e. AESHA3 is ~1300 times faster than the sub-key generated by the standard AES. Accordingly, we find that AESHA3 will be very relevant not only in terms of security but also it will save the resources in IoT devices. We investigated AESHA3 in Intel Core i7, 6th Generation processor and Raspberry Pi 4B and found that up to two MB data encryption is very significant, and lesser the data size, more the resource saving compared to AES.</li>
</ul>

<h3>Title: LightDSA: A Python-Based Hybrid Digital Signature Library and Performance Analysis of RSA, DSA, ECDSA and EdDSA in Variable Configurations, Elliptic Curve Forms and Curves</h3>
<ul>
<li><strong>Authors: </strong>Sefik Serengil, Alper Ozpinar</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23773">https://arxiv.org/abs/2505.23773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23773">https://arxiv.org/pdf/2505.23773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23773]] LightDSA: A Python-Based Hybrid Digital Signature Library and Performance Analysis of RSA, DSA, ECDSA and EdDSA in Variable Configurations, Elliptic Curve Forms and Curves(https://arxiv.org/abs/2505.23773)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Digital signature algorithms (DSAs) are fundamental to cryptographic security, ensuring data integrity and authentication. While RSA, DSA, ECDSA, and EdDSA are widely used, their performance varies significantly depending on key sizes, hash functions, and elliptic curve configurations. In this paper, we introduce LightDSA, a hybrid and configurable digital signature library that supports RSA, DSA, ECDSA, and EdDSA with flexible form and curve selection, open sourced at this https URL. Unlike conventional implementations that impose strict curve-form mappings - such as Weierstrass for ECDSA and Edwards for EdDSA LightDSA - allows arbitrary combinations, enabling a broader performance evaluation. We analyze the computational efficiency of these algorithms across various configurations, comparing key generation, signing, and verification times. Our results provide insights into the trade-offs between security and efficiency, guiding the selection of optimal configurations for different cryptographic needs.</li>
</ul>

<h3>Title: Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale</h3>
<ul>
<li><strong>Authors: </strong>Cody Kommers, Drew Hemment, Maria Antoniak, Joel Z. Leibo, Hoyt Long, Emily Robinson, Adam Sobey</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23785">https://arxiv.org/abs/2505.23785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23785">https://arxiv.org/pdf/2505.23785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23785]] Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale(https://arxiv.org/abs/2505.23785)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>This position paper argues that large language models (LLMs) can make cultural context, and therefore human meaning, legible at an unprecedented scale in AI-based sociotechnical systems. We argue that such systems have previously been unable to represent human meaning because they rely on thin descriptions: numerical representations that enforce standardization and therefore strip human activity of the cultural context that gives it meaning. By contrast, scholars in the humanities and qualitative social sciences have developed frameworks for representing meaning through thick description: verbal representations that accommodate heterogeneity and retain contextual information needed to represent human meaning. While these methods can effectively codify meaning, they are difficult to deploy at scale. However, the verbal capabilities of LLMs now provide a means of (at least partially) automating the generation and processing of thick descriptions, potentially overcoming this bottleneck. We argue that the problem of rendering human meaning legible is not just about selecting better metrics, but about developing new representational formats (based on thick description). We frame this as a crucial direction for the application of generative AI and identify five key challenges: preserving context, maintaining interpretive pluralism, integrating perspectives based on lived experience and critical distance, distinguishing qualitative content from quantitative magnitude, and acknowledging meaning as dynamic rather than static. Furthermore, we suggest that thick description has the potential to serve as a unifying framework to address a number of emerging concerns about the difficulties of representing culture in (or using) LLMs.</li>
</ul>

<h3>Title: Mind the Gap: A Practical Attack on GGUF Quantization</h3>
<ul>
<li><strong>Authors: </strong>Kazuki Egashira, Robin Staab, Mark Vero, Jingxuan He, Martin Vechev</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23786">https://arxiv.org/abs/2505.23786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23786">https://arxiv.org/pdf/2505.23786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23786]] Mind the Gap: A Practical Attack on GGUF Quantization(https://arxiv.org/abs/2505.23786)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack</a></li>
<li><strong>Abstract: </strong>With the increasing size of frontier LLMs, post-training quantization has become the standard for memory-efficient deployment. Recent work has shown that basic rounding-based quantization schemes pose security risks, as they can be exploited to inject malicious behaviors into quantized models that remain hidden in full precision. However, existing attacks cannot be applied to more complex quantization methods, such as the GGUF family used in the popular ollama and this http URL frameworks. In this work, we address this gap by introducing the first attack on GGUF. Our key insight is that the quantization error -- the difference between the full-precision weights and their (de-)quantized version -- provides sufficient flexibility to construct malicious quantized models that appear benign in full precision. Leveraging this, we develop an attack that trains the target malicious LLM while constraining its weights based on quantization errors. We demonstrate the effectiveness of our attack on three popular LLMs across nine GGUF quantization data types on three diverse attack scenarios: insecure code generation ($\Delta$=$88.7\%$), targeted content injection ($\Delta$=$85.0\%$), and benign instruction refusal ($\Delta$=$30.1\%$). Our attack highlights that (1) the most widely used post-training quantization method is susceptible to adversarial interferences, and (2) the complexity of quantization schemes alone is insufficient as a defense.</li>
</ul>

<h3>Title: Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework</h3>
<ul>
<li><strong>Authors: </strong>Aakash Sen Sharma, Debdeep Sanyal, Priyansh Srivastava, Sundar Atreya H., Shirish Karande, Mohan Kankanhalli, Murari Mandal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23788">https://arxiv.org/abs/2505.23788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23788">https://arxiv.org/pdf/2505.23788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23788]] Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework(https://arxiv.org/abs/2505.23788)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, fair, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) commonly risk copyright infringement by reproducing protected content verbatim or with insufficient transformative modifications, posing significant ethical, legal, and practical concerns. Current inference-time safeguards predominantly rely on restrictive refusal-based filters, often compromising the practical utility of these models. To address this, we collaborated closely with intellectual property experts to develop FUA-LLM (Fair Use Aligned Language Models), a legally-grounded framework explicitly designed to align LLM outputs with fair-use doctrine. Central to our method is FairUseDB, a carefully constructed dataset containing 18,000 expert-validated examples covering nine realistic infringement scenarios. Leveraging this dataset, we apply Direct Preference Optimization (DPO) to fine-tune open-source LLMs, encouraging them to produce legally compliant and practically useful alternatives rather than resorting to blunt refusal. Recognizing the shortcomings of traditional evaluation metrics, we propose new measures: Weighted Penalty Utility and Compliance Aware Harmonic Mean (CAH) to balance infringement risk against response utility. Extensive quantitative experiments coupled with expert evaluations confirm that FUA-LLM substantially reduces problematic outputs (up to 20\%) compared to state-of-the-art approaches, while preserving real-world usability.</li>
</ul>

<h3>Title: Conversational Exploration of Literature Landscape with LitChat</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Huang, Shasha Zhou, Yuxuan Chen, Ke Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23789">https://arxiv.org/abs/2505.23789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23789">https://arxiv.org/pdf/2505.23789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23789]] Conversational Exploration of Literature Landscape with LitChat(https://arxiv.org/abs/2505.23789)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We are living in an era of "big literature", where the volume of digital scientific publications is growing exponentially. While offering new opportunities, this also poses challenges for understanding literature landscapes, as traditional manual reviewing is no longer feasible. Recent large language models (LLMs) have shown strong capabilities for literature comprehension, yet they are incapable of offering "comprehensive, objective, open and transparent" views desired by systematic reviews due to their limited context windows and trust issues like hallucinations. Here we present LitChat, an end-to-end, interactive and conversational literature agent that augments LLM agents with data-driven discovery tools to facilitate literature exploration. LitChat automatically interprets user queries, retrieves relevant sources, constructs knowledge graphs, and employs diverse data-mining techniques to generate evidence-based insights addressing user needs. We illustrate the effectiveness of LitChat via a case study on AI4Health, highlighting its capacity to quickly navigate the users through large-scale literature landscape with data-based evidence that is otherwise infeasible with traditional means.</li>
</ul>

<h3>Title: Rethinking the Understanding Ability across LLMs through Mutual Information</h3>
<ul>
<li><strong>Authors: </strong>Shaojie Wang, Sirui Ding, Na Zou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23790">https://arxiv.org/abs/2505.23790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23790">https://arxiv.org/pdf/2505.23790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23790]] Rethinking the Understanding Ability across LLMs through Mutual Information(https://arxiv.org/abs/2505.23790)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have revolutionized natural language processing, yet evaluating their intrinsic linguistic understanding remains challenging. Moving beyond specialized evaluation tasks, we propose an information-theoretic framework grounded in mutual information (MI) to achieve this. We formalize the understanding as MI between an input sentence and its latent representation (sentence-level MI), measuring how effectively input information is preserved in latent representation. Given that LLMs learn embeddings for individual tokens, we decompose sentence-level MI into token-level MI between tokens and sentence embeddings, establishing theoretical bounds connecting these measures. Based on this foundation, we theoretically derive a computable lower bound for token-level MI using Fano's inequality, which directly relates to token-level recoverability-the ability to predict original tokens from sentence embedding. We implement this recoverability task to comparatively measure MI across different LLMs, revealing that encoder-only models consistently maintain higher information fidelity than their decoder-only counterparts, with the latter exhibiting a distinctive late-layer "forgetting" pattern where mutual information is first enhanced and then discarded. Moreover, fine-tuning to maximize token-level recoverability consistently improves understanding ability of LLMs on tasks without task-specific supervision, demonstrating that mutual information can serve as a foundation for understanding and improving language model capabilities.</li>
</ul>

<h3>Title: Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Sayyed Farid Ahamed, Sandip Roy, Soumya Banerjee, Marc Vucovich, Kevin Choi, Abdul Rahman, Alison Hu, Edward Bowen, Sachin Shetty</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23791">https://arxiv.org/abs/2505.23791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23791">https://arxiv.org/pdf/2505.23791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23791]] Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning(https://arxiv.org/abs/2505.23791)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, extraction, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a collaborative learning framework designed to protect client data, yet it remains highly vulnerable to Intellectual Property (IP) threats. Model extraction (ME) attacks pose a significant risk to Machine Learning as a Service (MLaaS) platforms, enabling attackers to replicate confidential models by querying black-box (without internal insight) APIs. Despite FL's privacy-preserving goals, its distributed nature makes it particularly susceptible to such attacks. This paper examines the vulnerability of FL-based victim models to two types of model extraction attacks. For various federated clients built under the NVFlare platform, we implemented ME attacks across two deep learning architectures and three image datasets. We evaluate the proposed ME attack performance using various metrics, including accuracy, fidelity, and KL divergence. The experiments show that for different FL clients, the accuracy and fidelity of the extracted model are closely related to the size of the attack query set. Additionally, we explore a transfer learning based approach where pretrained models serve as the starting point for the extraction process. The results indicate that the accuracy and fidelity of the fine-tuned pretrained extraction models are notably higher, particularly with smaller query sets, highlighting potential advantages for attackers.</li>
</ul>

<h3>Title: Zero-Trust Foundation Models: A New Paradigm for Secure and Collaborative Artificial Intelligence for Internet of Things</h3>
<ul>
<li><strong>Authors: </strong>Kai Li, Conggai Li, Xin Yuan, Shenghong Li, Sai Zou, Syed Sohail Ahmed, Wei Ni, Dusit Niyato, Abbas Jamalipour, Falko Dressler, Ozgur B. Akan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23792">https://arxiv.org/abs/2505.23792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23792">https://arxiv.org/pdf/2505.23792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23792]] Zero-Trust Foundation Models: A New Paradigm for Secure and Collaborative Artificial Intelligence for Internet of Things(https://arxiv.org/abs/2505.23792)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, federate, segmentation</a></li>
<li><strong>Abstract: </strong>This paper focuses on Zero-Trust Foundation Models (ZTFMs), a novel paradigm that embeds zero-trust security principles into the lifecycle of foundation models (FMs) for Internet of Things (IoT) systems. By integrating core tenets, such as continuous verification, least privilege access (LPA), data confidentiality, and behavioral analytics into the design, training, and deployment of FMs, ZTFMs can enable secure, privacy-preserving AI across distributed, heterogeneous, and potentially adversarial IoT environments. We present the first structured synthesis of ZTFMs, identifying their potential to transform conventional trust-based IoT architectures into resilient, self-defending ecosystems. Moreover, we propose a comprehensive technical framework, incorporating federated learning (FL), blockchain-based identity management, micro-segmentation, and trusted execution environments (TEEs) to support decentralized, verifiable intelligence at the network edge. In addition, we investigate emerging security threats unique to ZTFM-enabled systems and evaluate countermeasures, such as anomaly detection, adversarial training, and secure aggregation. Through this analysis, we highlight key open research challenges in terms of scalability, secure orchestration, interpretable threat attribution, and dynamic trust calibration. This survey lays a foundational roadmap for secure, intelligent, and trustworthy IoT infrastructures powered by FMs.</li>
</ul>

<h3>Title: USB: A Comprehensive and Unified Safety Evaluation Benchmark for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Baolin Zheng, Guanlin Chen, Hongqiong Zhong, Qingyang Teng, Yingshui Tan, Zhendong Liu, Weixun Wang, Jiaheng Liu, Jian Yang, Huiyun Jing, Jincheng Wei, Wenbo Su, Xiaoyong Zhu, Bo Zheng, Kaifu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23793">https://arxiv.org/abs/2505.23793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23793">https://arxiv.org/pdf/2505.23793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23793]] USB: A Comprehensive and Unified Safety Evaluation Benchmark for Multimodal Large Language Models(https://arxiv.org/abs/2505.23793)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model</a></li>
<li><strong>Abstract: </strong>Despite their remarkable achievements and widespread adoption, Multimodal Large Language Models (MLLMs) have revealed significant security vulnerabilities, highlighting the urgent need for robust safety evaluation benchmarks. Existing MLLM safety benchmarks, however, fall short in terms of data quality and coverge, and modal risk combinations, resulting in inflated and contradictory evaluation results, which hinders the discovery and governance of security concerns. Besides, we argue that vulnerabilities to harmful queries and oversensitivity to harmless ones should be considered simultaneously in MLLMs safety evaluation, whereas these were previously considered separately. In this paper, to address these shortcomings, we introduce Unified Safety Benchmarks (USB), which is one of the most comprehensive evaluation benchmarks in MLLM safety. Our benchmark features high-quality queries, extensive risk categories, comprehensive modal combinations, and encompasses both vulnerability and oversensitivity evaluations. From the perspective of two key dimensions: risk categories and modality combinations, we demonstrate that the available benchmarks -- even the union of the vast majority of them -- are far from being truly comprehensive. To bridge this gap, we design a sophisticated data synthesis pipeline that generates extensive, high-quality complementary data addressing previously unexplored aspects. By combining open-source datasets with our synthetic data, our benchmark provides 4 distinct modality combinations for each of the 61 risk sub-categories, covering both English and Chinese across both vulnerability and oversensitivity dimensions.</li>
</ul>

<h3>Title: R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuan Li, Qi Luo, Xiaonan Li, Bufan Li, Qinyuan Cheng, Bo Wang, Yining Zheng, Yuxin Wang, Zhangyue Yin, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23794">https://arxiv.org/abs/2505.23794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23794">https://arxiv.org/pdf/2505.23794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23794]] R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning(https://arxiv.org/abs/2505.23794)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) integrates external knowledge with Large Language Models (LLMs) to enhance factual correctness and mitigate hallucination. However, dense retrievers often become the bottleneck of RAG systems due to their limited parameters compared to LLMs and their inability to perform step-by-step reasoning. While prompt-based iterative RAG attempts to address these limitations, it is constrained by human-designed workflows. To address these limitations, we propose $\textbf{R3-RAG}$, which uses $\textbf{R}$einforcement learning to make the LLM learn how to $\textbf{R}$eason and $\textbf{R}$etrieve step by step, thus retrieving comprehensive external knowledge and leading to correct answers. R3-RAG is divided into two stages. We first use cold start to make the model learn the manner of iteratively interleaving reasoning and retrieval. Then we use reinforcement learning to further harness its ability to better explore the external retrieval environment. Specifically, we propose two rewards for R3-RAG: 1) answer correctness for outcome reward, which judges whether the trajectory leads to a correct answer; 2) relevance-based document verification for process reward, encouraging the model to retrieve documents that are relevant to the user question, through which we can let the model learn how to iteratively reason and retrieve relevant documents to get the correct answer. Experimental results show that R3-RAG significantly outperforms baselines and can transfer well to different retrievers. We release R3-RAG at this https URL.</li>
</ul>

<h3>Title: Emergent LLM behaviors are observationally equivalent to data leakage</h3>
<ul>
<li><strong>Authors: </strong>Christopher Barrie, Petter TÃ¶rnberg</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23796">https://arxiv.org/abs/2505.23796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23796">https://arxiv.org/pdf/2505.23796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23796]] Emergent LLM behaviors are observationally equivalent to data leakage(https://arxiv.org/abs/2505.23796)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Ashery et al. recently argue that large language models (LLMs), when paired to play a classic "naming game," spontaneously develop linguistic conventions reminiscent of human social norms. Here, we show that their results are better explained by data leakage: the models simply reproduce conventions they already encountered during pre-training. Despite the authors' mitigation measures, we provide multiple analyses demonstrating that the LLMs recognize the structure of the coordination game and recall its outcomes, rather than exhibit "emergent" conventions. Consequently, the observed behaviors are indistinguishable from memorization of the training corpus. We conclude by pointing to potential alternative strategies and reflecting more generally on the place of LLMs for social science models.</li>
</ul>

<h3>Title: Detection of Suicidal Risk on Social Media: A Hybrid Model</h3>
<ul>
<li><strong>Authors: </strong>Zaihan Yang, Ryan Leonard, Hien Tran, Rory Driscoll, Chadbourne Davis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23797">https://arxiv.org/abs/2505.23797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23797">https://arxiv.org/pdf/2505.23797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23797]] Detection of Suicidal Risk on Social Media: A Hybrid Model(https://arxiv.org/abs/2505.23797)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Suicidal thoughts and behaviors are increasingly recognized as a critical societal concern, highlighting the urgent need for effective tools to enable early detection of suicidal risk. In this work, we develop robust machine learning models that leverage Reddit posts to automatically classify them into four distinct levels of suicide risk severity. We frame this as a multi-class classification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating the deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa), a state-of-the-art deep learning transformer model, with the statistical term-weighting of TF-IDF, further compressed with PCA, to boost the accuracy and reliability of suicide risk assessment. To address data imbalance and overfitting, we explore various data resampling techniques and data augmentation strategies to enhance model generalization. Additionally, we compare our model's performance against that of using RoBERTa only, the BERT model and other traditional machine learning classifiers. Experimental results demonstrate that the hybrid model can achieve improved performance, giving a best weighted $F_{1}$ score of 0.7512.</li>
</ul>

<h3>Title: My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals</h3>
<ul>
<li><strong>Authors: </strong>Jian Lan, Yifei Fu, Udo Schlegel, Gengyuan Zhang, Tanveer Hannan, Haokun Chen, Thomas Seidl</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23798">https://arxiv.org/abs/2505.23798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23798">https://arxiv.org/pdf/2505.23798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23798]] My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals(https://arxiv.org/abs/2505.23798)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative</a></li>
<li><strong>Abstract: </strong>Social bias is a critical issue in large vision-language models (VLMs), where fairness- and ethics-related problems harm certain groups of people in society. It is unknown to what extent VLMs yield social bias in generative responses. In this study, we focus on evaluating and mitigating social bias on both the model's response and probability distribution. To do so, we first evaluate four state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the multiple-choice selection task. Surprisingly, we find that models suffer from generating gender-biased or race-biased responses. We also observe that models are prone to stating their responses are fair, but indeed having mis-calibrated confidence levels towards particular social groups. While investigating why VLMs are unfair in this study, we observe that VLMs' hidden layers exhibit substantial fluctuations in fairness levels. Meanwhile, residuals in each layer show mixed effects on fairness, with some contributing positively while some lead to increased bias. Based on these findings, we propose a post-hoc method for the inference stage to mitigate social bias, which is training-free and model-agnostic. We achieve this by ablating bias-associated residuals while amplifying fairness-associated residuals on model hidden layers during inference. We demonstrate that our post-hoc method outperforms the competing training strategies, helping VLMs have fairer responses and more reliable confidence levels.</li>
</ul>

<h3>Title: Estimating LLM Consistency: A User Baseline vs Surrogate Metrics</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyuan Wu, Weiran Lin, Omer Akgul, Lujo Bauer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23799">https://arxiv.org/abs/2505.23799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23799">https://arxiv.org/pdf/2505.23799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23799]] Estimating LLM Consistency: A User Baseline vs Surrogate Metrics(https://arxiv.org/abs/2505.23799)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are prone to hallucinations and sensitive to prompt perturbations, often resulting in inconsistent or unreliable generated text. Different methods have been proposed to mitigate such hallucinations and fragility -- one of them being measuring the consistency (the model's confidence in the response, or likelihood of generating a similar response when resampled) of LLM responses. In previous work, measuring consistency often relied on the probability of a response appearing within a pool of resampled responses, or internal states or logits of responses. However, it is not yet clear how well these approaches approximate how humans perceive the consistency of LLM responses. We performed a user study (n=2,976) and found current methods typically do not approximate users' perceptions of LLM consistency very well. We propose a logit-based ensemble method for estimating LLM consistency, and we show that this method matches the performance of the best-performing existing metric in estimating human ratings of LLM consistency. Our results suggest that methods of estimating LLM consistency without human evaluation are sufficiently imperfect that we suggest evaluation with human input be more broadly used.</li>
</ul>

<h3>Title: Strengthening Cybersecurity Resilience in Agriculture Through Educational Interventions: A Case Study of the Ponca Tribe of Nebraska</h3>
<ul>
<li><strong>Authors: </strong>George Grispos, Logan Mears, Larry Loucks</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23800">https://arxiv.org/abs/2505.23800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23800">https://arxiv.org/pdf/2505.23800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23800]] Strengthening Cybersecurity Resilience in Agriculture Through Educational Interventions: A Case Study of the Ponca Tribe of Nebraska(https://arxiv.org/abs/2505.23800)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The increasing digitization of agricultural operations has introduced new cybersecurity challenges for the farming community. This paper introduces an educational intervention called Cybersecurity Improvement Initiative for Agriculture (CIIA), which aims to strengthen cybersecurity awareness and resilience among farmers and food producers. Using a case study that focuses on farmers from the Ponca Tribe of Nebraska, the research evaluates pre- and post- intervention survey data to assess participants' cybersecurity knowledge and awareness before and after exposure to the CIIA. The findings reveal a substantial baseline deficiency in cybersecurity education among participants, however, post-intervention assessments demonstrate improvements in the comprehension of cybersecurity concepts, such as password hygiene, multi-factor authentication, and the necessity of routine data backups. These initial findings highlight the need for a continued and sustained, community-specific cybersecurity education effort to help mitigate emerging cyber threats in the agricultural sector.</li>
</ul>

<h3>Title: SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks</h3>
<ul>
<li><strong>Authors: </strong>Sajid Hussain, Muhammad Sohail, Nauman Ali Khan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23801">https://arxiv.org/abs/2505.23801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23801">https://arxiv.org/pdf/2505.23801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23801]] SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks(https://arxiv.org/abs/2505.23801)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Background: Federated Learning (FL) has emerged as a promising paradigm for training machine learning models while preserving data privacy. However, applying FL to Natural Language Processing (NLP) tasks presents unique challenges due to semantic heterogeneity across clients, vocabulary mismatches, and varying resource constraints on edge devices. Objectives: This paper introduces SEMFED, a novel semantic-aware resource-efficient federated learning framework specifically designed for heterogeneous NLP tasks. Methods: SEMFED incorporates three key innovations: (1) a semantic-aware client selection mechanism that balances semantic diversity with resource constraints, (2) adaptive NLP-specific model architectures tailored to device capabilities while preserving semantic information, and (3) a communication-efficient semantic feature compression technique that significantly reduces bandwidth requirements. Results: Experimental results on various NLP classification tasks demonstrate that SEMFED achieves an 80.5% reduction in communication costs while maintaining model accuracy above 98%, outperforming state-of-the-art FL approaches. Conclusion: SEMFED effectively manages heterogeneous client environments with varying computational resources, network reliability, and semantic data distributions, making it particularly suitable for real-world federated NLP deployments.</li>
</ul>

<h3>Title: MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks</h3>
<ul>
<li><strong>Authors: </strong>Suhana Bedi, Hejie Cui, Miguel Fuentes, Alyssa Unell, Michael Wornow, Juan M. Banda, Nikesh Kotecha, Timothy Keyes, Yifan Mai, Mert Oez, Hao Qiu, Shrey Jain, Leonardo Schettini, Mehr Kashyap, Jason Alan Fries, Akshay Swaminathan, Philip Chung, Fateme Nateghi, Asad Aali, Ashwin Nayak, Shivam Vedak, Sneha S. Jain, Birju Patel, Oluseyi Fayanju, Shreya Shah, Ethan Goh, Dong-han Yao, Brian Soetikno, Eduardo Reis, Sergios Gatidis, Vasu Divi, Robson Capasso, Rachna Saralkar, Chia-Chun Chiang, Jenelle Jindal, Tho Pham, Faraz Ghoddusi, Steven Lin, Albert S. Chiou, Christy Hong, Mohana Roy, Michael F. Gensheimer, Hinesh Patel, Kevin Schulman, Dev Dash, Danton Char, Lance Downing, Francois Grolleau, Kameron Black, Bethel Mieso, Aydin Zahedivash, Wen-wai Yim, Harshita Sharma, Tony Lee, Hannah Kirsch, Jennifer Lee, Nerissa Ambers, Carlene Lugtu, Aditya Sharma, Bilal Mawji, Alex Alekseyev, Vicky Zhou, Vikas Kakkar, Jarrod Helzer, Anurang Revri, Yair Bannett, Roxana Daneshjou, Jonathan Chen, Emily Alsentzer, Keith Morse, Nirmal Ravi, Nima Aghaeepour, Vanessa Kennedy, Akshay Chaudhari, Thomas Wang, Sanmi Koyejo, Matthew P. Lungren, Eric Horvitz, Percy Liang, Mike Pfeffer, Nigam H. Shah</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23802">https://arxiv.org/abs/2505.23802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23802">https://arxiv.org/pdf/2505.23802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23802]] MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks(https://arxiv.org/abs/2505.23802)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) achieve near-perfect scores on medical licensing exams, these evaluations inadequately reflect the complexity and diversity of real-world clinical practice. We introduce MedHELM, an extensible evaluation framework for assessing LLM performance for medical tasks with three key contributions. First, a clinician-validated taxonomy spanning 5 categories, 22 subcategories, and 121 tasks developed with 29 clinicians. Second, a comprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly formulated) providing complete coverage of all categories and subcategories in the taxonomy. Third, a systematic comparison of LLMs with improved evaluation methods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9 frontier LLMs, using the 35 benchmarks, revealed significant performance variation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64% win-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved comparable results at 40% lower estimated computational cost. On a normalized accuracy scale (0-1), most models performed strongly in Clinical Note Generation (0.73-0.85) and Patient Communication & Education (0.78-0.83), moderately in Medical Research Assistance (0.65-0.75), and generally lower in Clinical Decision Support (0.56-0.72) and Administration & Workflow (0.53-0.63). Our LLM-jury evaluation method achieved good agreement with clinician ratings (ICC = 0.47), surpassing both average clinician-clinician agreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and BERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top models at lower estimated cost. These findings highlight the importance of real-world, task-specific evaluation for medical use of LLMs and provides an open source framework to enable this.</li>
</ul>

<h3>Title: MultiPhishGuard: An LLM-based Multi-Agent System for Phishing Email Detection</h3>
<ul>
<li><strong>Authors: </strong>Yinuo Xue, Eric Spero, Yun Sing Koh, Giovanni Russello</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23803">https://arxiv.org/abs/2505.23803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23803">https://arxiv.org/pdf/2505.23803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23803]] MultiPhishGuard: An LLM-based Multi-Agent System for Phishing Email Detection(https://arxiv.org/abs/2505.23803)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, generative</a></li>
<li><strong>Abstract: </strong>Phishing email detection faces critical challenges from evolving adversarial tactics and heterogeneous attack patterns. Traditional detection methods, such as rule-based filters and denylists, often struggle to keep pace with these evolving tactics, leading to false negatives and compromised security. While machine learning approaches have improved detection accuracy, they still face challenges adapting to novel phishing strategies. We present MultiPhishGuard, a dynamic LLM-based multi-agent detection system that synergizes specialized expertise with adversarial-aware reinforcement learning. Our framework employs five cooperative agents (text, URL, metadata, explanation simplifier, and adversarial agents) with automatically adjusted decision weights powered by a Proximal Policy Optimization reinforcement learning algorithm. To address emerging threats, we introduce an adversarial training loop featuring an adversarial agent that generates subtle context-aware email variants, creating a self-improving defense ecosystem and enhancing system robustness. Experimental evaluations on public datasets demonstrate that MultiPhishGuard significantly outperforms Chain-of-Thoughts, single-agent baselines and state-of-the-art detectors, as validated by ablation studies and comparative analyses. Experiments demonstrate that MultiPhishGuard achieves high accuracy (97.89\%) with low false positive (2.73\%) and false negative rates (0.20\%). Additionally, we incorporate an explanation simplifier agent, which provides users with clear and easily understandable explanations for why an email is classified as phishing or legitimate. This work advances phishing defense through dynamic multi-agent collaboration and generative adversarial resilience.</li>
</ul>

<h3>Title: Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies</h3>
<ul>
<li><strong>Authors: </strong>Terrance Liu, Shuyi Wang, Daniel Preotiuc-Pietro, Yash Chandarana, Chirag Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23804">https://arxiv.org/abs/2505.23804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23804">https://arxiv.org/pdf/2505.23804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23804]] Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies(https://arxiv.org/abs/2505.23804)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) achieve strong performance on text-to-SQL parsing, they sometimes exhibit unexpected failures in which they are confidently incorrect. Building trustworthy text-to-SQL systems thus requires eliciting reliable uncertainty measures from the LLM. In this paper, we study the problem of providing a calibrated confidence score that conveys the likelihood of an output query being correct. Our work is the first to establish a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In particular, we show that Platt scaling, a canonical method for calibration, provides substantial improvements over directly using raw model output probabilities as confidence scores. Furthermore, we propose a method for text-to-SQL calibration that leverages the structured nature of SQL queries to provide more granular signals of correctness, named "sub-clause frequency" (SCF) scores. Using multivariate Platt scaling (MPS), our extension of the canonical Platt scaling technique, we combine individual SCF scores into an overall accurate and calibrated score. Empirical evaluation on two popular text-to-SQL datasets shows that our approach of combining MPS and SCF yields further improvements in calibration and the related task of error detection over traditional Platt scaling.</li>
</ul>

<h3>Title: ADA: Automated Moving Target Defense for AI Workloads via Ephemeral Infrastructure-Native Rotation in Kubernetes</h3>
<ul>
<li><strong>Authors: </strong>Akram Sheriff, Ken Huang, Zsolt Nemeth, Madjid Nakhjiri</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23805">https://arxiv.org/abs/2505.23805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23805">https://arxiv.org/pdf/2505.23805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23805]] ADA: Automated Moving Target Defense for AI Workloads via Ephemeral Infrastructure-Native Rotation in Kubernetes(https://arxiv.org/abs/2505.23805)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>This paper introduces the Adaptive Defense Agent (ADA), an innovative Automated Moving Target Defense (AMTD) system designed to fundamentally enhance the security posture of AI workloads. ADA operates by continuously and automatically rotating these workloads at the infrastructure level, leveraging the inherent ephemerality of Kubernetes pods. This constant managed churn systematically invalidates attacker assumptions and disrupts potential kill chains by regularly destroying and respawning AI service instances. This methodology, applying principles of chaos engineering as a continuous, proactive defense, offers a paradigm shift from traditional static defenses that rely on complex and expensive confidential or trusted computing solutions to secure the underlying compute platforms, while at the same time agnostically supporting the latest advancements in agentic and nonagentic AI ecosystems and solutions such as agent-to-agent (A2A) communication frameworks or model context protocols (MCP). This AI-native infrastructure design, relying on the widely proliferated cloud-native Kubernetes technologies, facilitates easier deployment, simplifies maintenance through an inherent zero trust posture achieved by rotation, and promotes faster adoption. We posit that ADA's novel approach to AMTD provides a more robust, agile, and operationally efficient zero-trust model for AI services, achieving security through proactive environmental manipulation rather than reactive patching.</li>
</ul>

<h3>Title: MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation</h3>
<ul>
<li><strong>Authors: </strong>Sihyeon Lee, Hyunjoo Song, Jong-chan Lee, Yoon Jin Lee, Boram Lee, Hee-Eon Lim, Dongyeong Kim, Jinwook Seo, Bohyoung Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23806">https://arxiv.org/abs/2505.23806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23806">https://arxiv.org/pdf/2505.23806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23806]] MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation(https://arxiv.org/abs/2505.23806)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Deploying large language models (LLMs) in clinical settings faces critical trade-offs: cloud LLMs, with their extensive parameters and superior performance, pose risks to sensitive clinical data privacy, while local LLMs preserve privacy but often fail at complex clinical interpretation tasks. We propose MedOrchestra, a hybrid framework where a cloud LLM decomposes complex clinical tasks into manageable subtasks and prompt generation, while a local LLM executes these subtasks in a privacy-preserving manner. Without accessing clinical data, the cloud LLM generates and validates subtask prompts using clinical guidelines and synthetic test cases. The local LLM executes subtasks locally and synthesizes outputs generated by the cloud LLM. We evaluate MedOrchestra on pancreatic cancer staging using 100 radiology reports under NCCN guidelines. On free-text reports, MedOrchestra achieves 70.21% accuracy, outperforming local model baselines (without guideline: 48.94%, with guideline: 56.59%) and board-certified clinicians (gastroenterologists: 59.57%, surgeons: 65.96%, radiologists: 55.32%). On structured reports, MedOrchestra reaches 85.42% accuracy, showing clear superiority across all settings.</li>
</ul>

<h3>Title: DLP: Dynamic Layerwise Pruning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuli Chen, Bo Cheng, Jiale Han, Yingying Zhang, Yingting Li, Shuhao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23807">https://arxiv.org/abs/2505.23807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23807">https://arxiv.org/pdf/2505.23807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23807]] DLP: Dynamic Layerwise Pruning in Large Language Models(https://arxiv.org/abs/2505.23807)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at this https URL to facilitate future research.</li>
</ul>

<h3>Title: DenseLoRA: Dense Low-Rank Adaptation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lin Mu, Xiaoyu Wang, Li Ni, Yang Li, Zhize Wu, Peiquan Jin, Yiwen Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23808">https://arxiv.org/abs/2505.23808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23808">https://arxiv.org/pdf/2505.23808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23808]] DenseLoRA: Dense Low-Rank Adaptation of Large Language Models(https://arxiv.org/abs/2505.23808)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Low-rank adaptation (LoRA) has been developed as an efficient approach for adapting large language models (LLMs) by fine-tuning two low-rank matrices, thereby reducing the number of trainable parameters. However, prior research indicates that many of the weights in these matrices are redundant, leading to inefficiencies in parameter utilization. To address this limitation, we introduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances parameter efficiency while achieving superior performance compared to LoRA. DenseLoRA builds upon the concept of representation fine-tuning, incorporating a single Encoder-Decoder to refine and compress hidden representations across all adaptation layers before applying adaptation. Instead of relying on two redundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense low-rank matrix, improving parameter utilization and adaptation efficiency. We evaluate DenseLoRA on various benchmarks, showing that it achieves 83.8% accuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8% accuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we conduct extensive experiments to systematically assess the impact of DenseLoRA's components on overall model performance. Code is available at this https URL.</li>
</ul>

<h3>Title: MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Yang, Yinbo Luo, Zhoufutu Wen, Qi Chu, Tao Gong, Longxiang Liu, Kaiyuan Zhang, Jianpeng Jiao, Ge Zhang, Wenhao Huang, Nenghai Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23810">https://arxiv.org/abs/2505.23810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23810">https://arxiv.org/pdf/2505.23810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23810]] MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation(https://arxiv.org/abs/2505.23810)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (\textbf{LLMs}), e.g. ChatGPT, have been widely adopted in real-world dialogue applications. However, LLMs' robustness, especially in handling long complex dialogue sessions, including frequent motivation transfer, sophisticated cross-turn dependency, is criticized all along. Nevertheless, no existing benchmarks can fully reflect these weaknesses. We present \textbf{MARS-Bench}, a \textbf{M}ulti-turn \textbf{A}thletic \textbf{R}eal-world \textbf{S}cenario Dialogue \textbf{Bench}mark, designed to remedy the gap. MARS-Bench is constructed from play-by-play text commentary so to feature realistic dialogues specifically designed to evaluate three critical aspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn, and Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that closed-source LLMs significantly outperform open-source alternatives, explicit reasoning significantly boosts LLMs' robustness on handling long complex dialogue sessions, and LLMs indeed face significant challenges when handling motivation transfer and sophisticated cross-turn dependency. Moreover, we provide mechanistic interpretability on how attention sinks due to special tokens lead to LLMs' performance degradation when handling long complex dialogue sessions based on attention visualization experiment in Qwen2.5-7B-Instruction.</li>
</ul>

<h3>Title: LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions</h3>
<ul>
<li><strong>Authors: </strong>Hadi Askari, Shivanshu Gupta, Fei Wang, Anshuman Chhabra, Muhao Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23811">https://arxiv.org/abs/2505.23811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23811">https://arxiv.org/pdf/2505.23811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23811]] LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions(https://arxiv.org/abs/2505.23811)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pretrained Large Language Models (LLMs) achieve strong performance across a wide range of tasks, yet exhibit substantial variability in the various layers' training quality with respect to specific downstream applications, limiting their downstream this http URL is therefore critical to estimate layer-wise training quality in a manner that accounts for both model architecture and training data. However, existing approaches predominantly rely on model-centric heuristics (such as spectral statistics, outlier detection, or uniform allocation) while overlooking the influence of data. To address these limitations, we propose LayerIF, a data-driven framework that leverages Influence Functions to quantify the training quality of individual layers in a principled and task-sensitive manner. By isolating each layer's gradients and measuring the sensitivity of the validation loss to training examples by computing layer-wise influences, we derive data-driven estimates of layer importance. Notably, our method produces task-specific layer importance estimates for the same LLM, revealing how layers specialize for different test-time evaluation tasks. We demonstrate the utility of our scores by leveraging them for two downstream applications: (a) expert allocation in LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM pruning. Experiments across multiple LLM architectures demonstrate that our model-agnostic, influence-guided allocation leads to consistent gains in task performance.</li>
</ul>

<h3>Title: DP-RTFL: Differentially Private Resilient Temporal Federated Learning for Trustworthy AI in Regulated Industries</h3>
<ul>
<li><strong>Authors: </strong>Abhijit Talluri</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23813">https://arxiv.org/abs/2505.23813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23813">https://arxiv.org/pdf/2505.23813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23813]] DP-RTFL: Differentially Private Resilient Temporal Federated Learning for Trustworthy AI in Regulated Industries(https://arxiv.org/abs/2505.23813)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has emerged as a critical paradigm for enabling privacy-preserving machine learning, particularly in regulated sectors such as finance and healthcare. However, standard FL strategies often encounter significant operational challenges related to fault tolerance, system resilience against concurrent client and server failures, and the provision of robust, verifiable privacy guarantees essential for handling sensitive data. These deficiencies can lead to training disruptions, data loss, compromised model integrity, and non-compliance with data protection regulations (e.g., GDPR, CCPA). This paper introduces Differentially Private Resilient Temporal Federated Learning (DP-RTFL), an advanced FL framework designed to ensure training continuity, precise state recovery, and strong data privacy. DP-RTFL integrates local Differential Privacy (LDP) at the client level with resilient temporal state management and integrity verification mechanisms, such as hash-based commitments (referred to as Zero-Knowledge Integrity Proofs or ZKIPs in this context). The framework is particularly suited for critical applications like credit risk assessment using sensitive financial data, aiming to be operationally robust, auditable, and scalable for enterprise AI deployments. The implementation of the DP-RTFL framework is available as open-source.</li>
</ul>

<h3>Title: Watermarking Without Standards Is Not AI Governance</h3>
<ul>
<li><strong>Authors: </strong>Alexander Nemecek, Yuzhou Jiang, Erman Ayday</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23814">https://arxiv.org/abs/2505.23814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23814">https://arxiv.org/pdf/2505.23814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23814]] Watermarking Without Standards Is Not AI Governance(https://arxiv.org/abs/2505.23814)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, generative</a></li>
<li><strong>Abstract: </strong>Watermarking has emerged as a leading technical proposal for attributing generative AI content and is increasingly cited in global governance frameworks. This paper argues that current implementations risk serving as symbolic compliance rather than delivering effective oversight. We identify a growing gap between regulatory expectations and the technical limitations of existing watermarking schemes. Through analysis of policy proposals and industry practices, we show how incentive structures disincentivize robust, auditable deployments. To realign watermarking with governance goals, we propose a three-layer framework encompassing technical standards, audit infrastructure, and enforcement mechanisms. Without enforceable requirements and independent verification, watermarking will remain inadequate for accountability and ultimately undermine broader efforts in AI safety and regulation.</li>
</ul>

<h3>Title: A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Trenton Chang, Tobias Schnabel, Adith Swaminathan, Jenna Wiens</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23816">https://arxiv.org/abs/2505.23816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23816">https://arxiv.org/pdf/2505.23816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23816]] A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs(https://arxiv.org/abs/2505.23816)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite advances in large language models (LLMs) on reasoning and instruction-following benchmarks, it remains unclear whether they can reliably produce outputs aligned with a broad variety of user goals, a concept we refer to as steerability. The abundance of methods proposed to modify LLM behavior makes it unclear whether current LLMs are already steerable, or require further intervention. In particular, LLMs may exhibit (i) poor coverage, where rare user goals are underrepresented; (ii) miscalibration, where models overshoot requests; and (iii) side effects, where changes to one dimension of text inadvertently affect others. To systematically evaluate these failures, we introduce a framework based on a multi-dimensional goal space that models user goals and LLM outputs as vectors with dimensions corresponding to text attributes (e.g., reading difficulty). Applied to a text-rewriting task, we find that current LLMs struggle with steerability, as side effects are persistent. Interventions to improve steerability, such as prompt engineering, best-of-$N$ sampling, and reinforcement learning fine-tuning, have varying effectiveness, yet side effects remain problematic. Our findings suggest that even strong LLMs struggle with steerability, and existing alignment strategies may be insufficient. We open-source our steerability evaluation framework at this https URL.</li>
</ul>

<h3>Title: System Prompt Extraction Attacks and Defenses in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Badhan Chandra Das, M. Hadi Amini, Yanzhao Wu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23817">https://arxiv.org/abs/2505.23817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23817">https://arxiv.org/pdf/2505.23817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23817]] System Prompt Extraction Attacks and Defenses in Large Language Models(https://arxiv.org/abs/2505.23817)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, defense, attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>The system prompt in Large Language Models (LLMs) plays a pivotal role in guiding model behavior and response generation. Often containing private configuration details, user roles, and operational instructions, the system prompt has become an emerging attack target. Recent studies have shown that LLM system prompts are highly susceptible to extraction attacks through meticulously designed queries, raising significant privacy and security concerns. Despite the growing threat, there is a lack of systematic studies of system prompt extraction attacks and defenses. In this paper, we present a comprehensive framework, SPE-LLM, to systematically evaluate System Prompt Extraction attacks and defenses in LLMs. First, we design a set of novel adversarial queries that effectively extract system prompts in state-of-the-art (SOTA) LLMs, demonstrating the severe risks of LLM system prompt extraction attacks. Second, we propose three defense techniques to mitigate system prompt extraction attacks in LLMs, providing practical solutions for secure LLM deployments. Third, we introduce a set of rigorous evaluation metrics to accurately quantify the severity of system prompt extraction attacks in LLMs and conduct comprehensive experiments across multiple benchmark datasets, which validates the efficacy of our proposed SPE-LLM framework.</li>
</ul>

<h3>Title: Ratas framework: A comprehensive genai-based approach to rubric-based marking of real-world textual exams</h3>
<ul>
<li><strong>Authors: </strong>Masoud Safilian, Amin Beheshti, Stephen Elbourn</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23818">https://arxiv.org/abs/2505.23818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23818">https://arxiv.org/pdf/2505.23818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23818]] Ratas framework: A comprehensive genai-based approach to rubric-based marking of real-world textual exams(https://arxiv.org/abs/2505.23818)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Automated answer grading is a critical challenge in educational technology, with the potential to streamline assessment processes, ensure grading consistency, and provide timely feedback to students. However, existing approaches are often constrained to specific exam formats, lack interpretability in score assignment, and struggle with real-world applicability across diverse subjects and assessment types. To address these limitations, we introduce RATAS (Rubric Automated Tree-based Answer Scoring), a novel framework that leverages state-of-the-art generative AI models for rubric-based grading of textual responses. RATAS is designed to support a wide range of grading rubrics, enable subject-agnostic evaluation, and generate structured, explainable rationales for assigned scores. We formalize the automatic grading task through a mathematical framework tailored to rubric-based assessment and present an architecture capable of handling complex, real-world exam structures. To rigorously evaluate our approach, we construct a unique, contextualized dataset derived from real-world project-based courses, encompassing diverse response formats and varying levels of complexity. Empirical results demonstrate that RATAS achieves high reliability and accuracy in automated grading while providing interpretable feedback that enhances transparency for both students and nstructors.</li>
</ul>

<h3>Title: SpeechVerifier: Robust Acoustic Fingerprint against Tampering Attacks via Watermarking</h3>
<ul>
<li><strong>Authors: </strong>Lingfeng Yao (1), Chenpei Huang (1), Shengyao Wang (2), Junpei Xue (2), Hanqing Guo (3), Jiang Liu (2), Xun Chen (4), Miao Pan (1) ((1) University of Houston (2) Waseda University (3) University of Hawaii at MÄnoa (4) Independent Researcher)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23821">https://arxiv.org/abs/2505.23821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23821">https://arxiv.org/pdf/2505.23821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23821]] SpeechVerifier: Robust Acoustic Fingerprint against Tampering Attacks via Watermarking(https://arxiv.org/abs/2505.23821)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, extraction, watermark</a></li>
<li><strong>Abstract: </strong>With the surge of social media, maliciously tampered public speeches, especially those from influential figures, have seriously affected social stability and public trust. Existing speech tampering detection methods remain insufficient: they either rely on external reference data or fail to be both sensitive to attacks and robust to benign operations, such as compression and resampling. To tackle these challenges, we introduce SpeechVerifer to proactively verify speech integrity using only the published speech itself, i.e., without requiring any external references. Inspired by audio fingerprinting and watermarking, SpeechVerifier can (i) effectively detect tampering attacks, (ii) be robust to benign operations and (iii) verify the integrity only based on published speeches. Briefly, SpeechVerifier utilizes multiscale feature extraction to capture speech features across different temporal resolutions. Then, it employs contrastive learning to generate fingerprints that can detect modifications at varying granularities. These fingerprints are designed to be robust to benign operations, but exhibit significant changes when malicious tampering occurs. To enable speech verification in a self-contained manner, the generated fingerprints are then embedded into the speech signal by segment-wise watermarking. Without external references, SpeechVerifier can retrieve the fingerprint from the published audio and check it with the embedded watermark to verify the integrity of the speech. Extensive experimental results demonstrate that the proposed SpeechVerifier is effective in detecting tampering attacks and robust to benign operations.</li>
</ul>

<h3>Title: Speech as a Multimodal Digital Phenotype for Multi-Task LLM-based Mental Health Prediction</h3>
<ul>
<li><strong>Authors: </strong>Mai Ali, Christopher Lucasius, Tanmay P. Patel, Madison Aitken, Jacob Vorstman, Peter Szatmari, Marco Battaglia, Deepa Kundur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23822">https://arxiv.org/abs/2505.23822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23822">https://arxiv.org/pdf/2505.23822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23822]] Speech as a Multimodal Digital Phenotype for Multi-Task LLM-based Mental Health Prediction(https://arxiv.org/abs/2505.23822)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speech is a noninvasive digital phenotype that can offer valuable insights into mental health conditions, but it is often treated as a single modality. In contrast, we propose the treatment of patient speech data as a trimodal multimedia data source for depression detection. This study explores the potential of large language model-based architectures for speech-based depression prediction in a multimodal regime that integrates speech-derived text, acoustic landmarks, and vocal biomarkers. Adolescent depression presents a significant challenge and is often comorbid with multiple disorders, such as suicidal ideation and sleep disturbances. This presents an additional opportunity to integrate multi-task learning (MTL) into our study by simultaneously predicting depression, suicidal ideation, and sleep disturbances using the multimodal formulation. We also propose a longitudinal analysis strategy that models temporal changes across multiple clinical interactions, allowing for a comprehensive understanding of the conditions' progression. Our proposed approach, featuring trimodal, longitudinal MTL is evaluated on the Depression Early Warning dataset. It achieves a balanced accuracy of 70.8%, which is higher than each of the unimodal, single-task, and non-longitudinal methods.</li>
</ul>

<h3>Title: RAGPPI: RAG Benchmark for Protein-Protein Interactions in Drug Discovery</h3>
<ul>
<li><strong>Authors: </strong>Youngseung Jeon, Ziwen Li, Thomas Li, JiaSyuan Chang, Morteza Ziyadi, Xiang 'Anthony' Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23823">https://arxiv.org/abs/2505.23823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23823">https://arxiv.org/pdf/2505.23823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23823]] RAGPPI: RAG Benchmark for Protein-Protein Interactions in Drug Discovery(https://arxiv.org/abs/2505.23823)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieving the biological impacts of protein-protein interactions (PPIs) is essential for target identification (Target ID) in drug development. Given the vast number of proteins involved, this process remains time-consuming and challenging. Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) frameworks have supported Target ID; however, no benchmark currently exists for identifying the biological impacts of PPIs. To bridge this gap, we introduce the RAG Benchmark for PPIs (RAGPPI), a factual question-answer benchmark of 4,420 question-answer pairs that focus on the potential biological impacts of PPIs. Through interviews with experts, we identified criteria for a benchmark dataset, such as a type of QA and source. We built a gold-standard dataset (500 QA pairs) through expert-driven data annotation. We developed an ensemble auto-evaluation LLM that reflected expert labeling characteristics, which facilitates the construction of a silver-standard dataset (3,720 QA pairs). We are committed to maintaining RAGPPI as a resource to support the research community in advancing RAG systems for drug discovery QA solutions.</li>
</ul>

<h3>Title: Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Tianmai M. Zhang, Neil F. Abernethy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23824">https://arxiv.org/abs/2505.23824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23824">https://arxiv.org/pdf/2505.23824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23824]] Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation(https://arxiv.org/abs/2505.23824)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models have sparked interest in utilizing them to assist the peer review process of scientific publication. Instead of having AI models generate reviews in the same way as human reviewers, we propose adopting them as manuscript quality checkers. We introduce several baseline approaches and an extendable automatic evaluation framework using top LLMs as judges to tackle the difficulty of recruiting domain experts for manual evaluation. Utilizing papers withdrawn from arXiv, we validated our proposed methods with several leading reasoning LLMs from different providers and assessed their performance and API costs for identifying critical errors and unsoundness problems. The OpenAI o3 model performed the best, while o4-mini was the most cost-effective one in our evaluation. This paper provides insights into document-based scientific understanding/reasoning and lays the foundation for future applications.</li>
</ul>

<h3>Title: Privacy-Preserving Inconsistency Measurement</h3>
<ul>
<li><strong>Authors: </strong>Carl Corea, Timotheus Kampik, Nico Potyka</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23825">https://arxiv.org/abs/2505.23825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23825">https://arxiv.org/pdf/2505.23825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23825]] Privacy-Preserving Inconsistency Measurement(https://arxiv.org/abs/2505.23825)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>We investigate a new form of (privacy-preserving) inconsistency measurement for multi-party communication. Intuitively, for two knowledge bases K_A, K_B (of two agents A, B), our results allow to quantitatively assess the degree of inconsistency for K_A U K_B without having to reveal the actual contents of the knowledge bases. Using secure multi-party computation (SMPC) and cryptographic protocols, we develop two concrete methods for this use-case and show that they satisfy important properties of SMPC protocols -- notably, input privacy, i.e., jointly computing the inconsistency degree without revealing the inputs.</li>
</ul>

<h3>Title: ValueSim: Generating Backstories to Model Individual Value Systems</h3>
<ul>
<li><strong>Authors: </strong>Bangde Du, Ziyi Ye, Zhijing Wu, Jankowska Monika, Shuqi Zhu, Qingyao Ai, Yujia Zhou, Yiqun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23827">https://arxiv.org/abs/2505.23827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23827">https://arxiv.org/pdf/2505.23827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23827]] ValueSim: Generating Backstories to Model Individual Value Systems(https://arxiv.org/abs/2505.23827)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) continue to exhibit increasingly human-like capabilities, aligning them with human values has become critically important. Contemporary advanced techniques, such as prompt learning and reinforcement learning, are being deployed to better align LLMs with human values. However, while these approaches address broad ethical considerations and helpfulness, they rarely focus on simulating individualized human value systems. To address this gap, we present ValueSim, a framework that simulates individual values through the generation of personal backstories reflecting past experiences and demographic information. ValueSim converts structured individual data into narrative backstories and employs a multi-module architecture inspired by the Cognitive-Affective Personality System to simulate individual values based on these narratives. Testing ValueSim on a self-constructed benchmark derived from the World Values Survey demonstrates an improvement in top-1 accuracy by over 10% compared to retrieval-augmented generation methods. Further analysis reveals that performance enhances as additional user interaction history becomes available, indicating the model's ability to refine its persona simulation capabilities over time.</li>
</ul>

<h3>Title: Spa-VLM: Stealthy Poisoning Attacks on RAG-based VLM</h3>
<ul>
<li><strong>Authors: </strong>Lei Yu, Yechao Zhang, Ziqi Zhou, Yang Wu, Wei Wan, Minghui Li, Shengshan Hu, Pei Xiaobing, Jing Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23828">https://arxiv.org/abs/2505.23828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23828">https://arxiv.org/pdf/2505.23828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23828]] Spa-VLM: Stealthy Poisoning Attacks on RAG-based VLM(https://arxiv.org/abs/2505.23828)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, steal, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid development of the Vision-Language Model (VLM), significant progress has been made in Visual Question Answering (VQA) tasks. However, existing VLM often generate inaccurate answers due to a lack of up-to-date knowledge. To address this issue, recent research has introduced Retrieval-Augmented Generation (RAG) techniques, commonly used in Large Language Models (LLM), into VLM, incorporating external multi-modal knowledge to enhance the accuracy and practicality of VLM systems. Nevertheless, the RAG in LLM may be susceptible to data poisoning attacks. RAG-based VLM may also face the threat of this attack. This paper first reveals the vulnerabilities of the RAG-based large model under poisoning attack, showing that existing single-modal RAG poisoning attacks have a 100\% failure rate in multi-modal RAG scenarios. To address this gap, we propose Spa-VLM (Stealthy Poisoning Attack on RAG-based VLM), a new paradigm for poisoning attacks on large models. We carefully craft malicious multi-modal knowledge entries, including adversarial images and misleading text, which are then injected into the RAG's knowledge base. When users access the VLM service, the system may generate misleading outputs. We evaluate Spa-VLM on two Wikipedia datasets and across two different RAGs. Results demonstrate that our method achieves highly stealthy poisoning, with the attack success rate exceeding 0.8 after injecting just 5 malicious entries into knowledge bases with 100K and 2M entries, outperforming state-of-the-art poisoning attacks designed for RAG-based LLMs. Additionally, we evaluated several defense mechanisms, all of which ultimately proved ineffective against Spa-VLM, underscoring the effectiveness and robustness of our attack.</li>
</ul>

<h3>Title: BiasFilter: An Inference-Time Debiasing Framework for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaoqing Cheng, Ruizhe Chen, Hongying Zan, Yuxiang Jia, Min Peng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23829">https://arxiv.org/abs/2505.23829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23829">https://arxiv.org/pdf/2505.23829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23829]] BiasFilter: An Inference-Time Debiasing Framework for Large Language Models(https://arxiv.org/abs/2505.23829)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Mitigating social bias in large language models (LLMs) has become an increasingly important research objective. However, existing debiasing methods often incur high human and computational costs, exhibit limited effectiveness, and struggle to scale to larger models and open-ended generation tasks. To address these limitations, this paper proposes BiasFilter, a model-agnostic, inference-time debiasing framework that integrates seamlessly with both open-source and API-based LLMs. Instead of relying on retraining with balanced data or modifying model parameters, BiasFilter enforces fairness by filtering generation outputs in real time. Specifically, it periodically evaluates intermediate outputs every few tokens, maintains an active set of candidate continuations, and incrementally completes generation by discarding low-reward segments based on a fairness reward signal. To support this process, we construct a fairness preference dataset and train an implicit reward model to assess token-level fairness in generated responses. Extensive experiments demonstrate that BiasFilter effectively mitigates social bias across a range of LLMs while preserving overall generation quality.</li>
</ul>

<h3>Title: EvoMoE: Expert Evolution in Mixture of Experts for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Linglin Jing, Yuting Gao, Zhigang Wang, Wang Lan, Yiwen Tang, Wenhai Wang, Kaipeng Zhang, Qingpei Guo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23830">https://arxiv.org/abs/2505.23830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23830">https://arxiv.org/pdf/2505.23830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23830]] EvoMoE: Expert Evolution in Mixture of Experts for Multimodal Large Language Models(https://arxiv.org/abs/2505.23830)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements have shown that the Mixture of Experts (MoE) approach significantly enhances the capacity of large language models (LLMs) and improves performance on downstream tasks. Building on these promising results, multi-modal large language models (MLLMs) have increasingly adopted MoE techniques. However, existing multi-modal MoE tuning methods typically face two key challenges: expert uniformity and router rigidity. Expert uniformity occurs because MoE experts are often initialized by simply replicating the FFN parameters from LLMs, leading to homogenized expert functions and weakening the intended diversification of the MoE architecture. Meanwhile, router rigidity stems from the prevalent use of static linear routers for expert selection, which fail to distinguish between visual and textual tokens, resulting in similar expert distributions for image and text. To address these limitations, we propose EvoMoE, an innovative MoE tuning framework. EvoMoE introduces a meticulously designed expert initialization strategy that progressively evolves multiple robust experts from a single trainable expert, a process termed expert evolution that specifically targets severe expert homogenization. Furthermore, we introduce the Dynamic Token-aware Router (DTR), a novel routing mechanism that allocates input tokens to appropriate experts based on their modality and intrinsic token values. This dynamic routing is facilitated by hypernetworks, which dynamically generate routing weights tailored for each individual token. Extensive experiments demonstrate that EvoMoE significantly outperforms other sparse MLLMs across a variety of multi-modal benchmarks, including MME, MMBench, TextVQA, and POPE. Our results highlight the effectiveness of EvoMoE in enhancing the performance of MLLMs by addressing the critical issues of expert uniformity and router rigidity.</li>
</ul>

<h3>Title: ICH-Qwen: A Large Language Model Towards Chinese Intangible Cultural Heritage</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Ye, Tiansheng Zheng, Yue Qi, Wenhua Zhao, Xiyu Wang, Xue Zhao, Jiacheng He, Yaya Zheng, Dongbo Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23831">https://arxiv.org/abs/2505.23831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23831">https://arxiv.org/pdf/2505.23831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23831]] ICH-Qwen: A Large Language Model Towards Chinese Intangible Cultural Heritage(https://arxiv.org/abs/2505.23831)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>The intangible cultural heritage (ICH) of China, a cultural asset transmitted across generations by various ethnic groups, serves as a significant testament to the evolution of human civilization and holds irreplaceable value for the preservation of historical lineage and the enhancement of cultural self-confidence. However, the rapid pace of modernization poses formidable challenges to ICH, including threats damage, disappearance and discontinuity of inheritance. China has the highest number of items on the UNESCO Intangible Cultural Heritage List, which is indicative of the nation's abundant cultural resources and emphasises the pressing need for ICH preservation. In recent years, the rapid advancements in large language modelling have provided a novel technological approach for the preservation and dissemination of ICH. This study utilises a substantial corpus of open-source Chinese ICH data to develop a large language model, ICH-Qwen, for the ICH domain. The model employs natural language understanding and knowledge reasoning capabilities of large language models, augmented with synthetic data and fine-tuning techniques. The experimental results demonstrate the efficacy of ICH-Qwen in executing tasks specific to the ICH domain. It is anticipated that the model will provide intelligent solutions for the protection, inheritance and dissemination of intangible cultural heritage, as well as new theoretical and practical references for the sustainable development of intangible cultural heritage. Furthermore, it is expected that the study will open up new paths for digital humanities research.</li>
</ul>

<h3>Title: LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation</h3>
<ul>
<li><strong>Authors: </strong>Chaeeun Kim, Jinu Lee, Wonseok Hwang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23832">https://arxiv.org/abs/2505.23832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23832">https://arxiv.org/pdf/2505.23832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23832]] LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation(https://arxiv.org/abs/2505.23832)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Legal Case Retrieval (LCR), which retrieves relevant cases from a query case, is a fundamental task for legal professionals in research and decision-making. However, existing studies on LCR face two major limitations. First, they are evaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and use a narrow range of criminal query types, which cannot sufficiently reflect the complexity of real-world legal retrieval scenarios. Second, their reliance on embedding-based or lexical matching methods often results in limited representations and legally irrelevant matches. To address these issues, we present: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering 411 diverse crime types in queries over 1.2M legal cases; and (2) LegalSearchLM, a retrieval model that performs legal element reasoning over the query case and directly generates content grounded in the target cases through constrained decoding. Experimental results show that LegalSearchLM outperforms baselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It also demonstrates strong generalization to out-of-domain cases, outperforming naive generative models trained on in-domain data by 15%.</li>
</ul>

<h3>Title: Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective</h3>
<ul>
<li><strong>Authors: </strong>Qingchuan Ma, Yuhang Wu, Xiawu Zheng, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23833">https://arxiv.org/abs/2505.23833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23833">https://arxiv.org/pdf/2505.23833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23833]] Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective(https://arxiv.org/abs/2505.23833)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we aim to establish a simple, effective, and theoretically grounded benchmark for rigorously probing abstract reasoning in Large Language Models (LLMs). To achieve this, we first develop a mathematic framework that defines abstract reasoning as the ability to: (i) extract essential patterns independent of surface representations, and (ii) apply consistent rules to these abstract patterns. Based on this framework, we introduce two novel complementary metrics: \(\scoreGamma\) measures basic reasoning accuracy, while \(\scoreDelta\) quantifies a model's reliance on specific symbols rather than underlying patterns - a key indicator of true abstraction versus mere memorization. To implement this measurement, we design a benchmark: systematic symbol remapping in rule-based tasks, which forces models to demonstrate genuine pattern recognition beyond superficial token matching. Extensive LLM evaluations using this benchmark (commercial API models, 7B-70B, multi-agent) reveal:1) critical limitations in non-decimal arithmetic and symbolic reasoning; 2) persistent abstraction gaps despite chain-of-thought prompting; and 3) \(\scoreDelta\)'s effectiveness in robustly measuring memory dependence by quantifying performance degradation under symbol remapping, particularly highlighting operand-specific memorization. These findings underscore that current LLMs, despite domain-specific strengths, still lack robust abstract reasoning, highlighting key areas for future improvement.</li>
</ul>

<h3>Title: Say What You Mean: Natural Language Access Control with Large Language Models for Internet of Things</h3>
<ul>
<li><strong>Authors: </strong>Ye Cheng, Minghui Xu, Yue Zhang, Kun Li, Hao Wu, Yechao Zhang, Shaoyong Guo, Wangjie Qiu, Dongxiao Yu, Xiuzhen Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23835">https://arxiv.org/abs/2505.23835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23835">https://arxiv.org/pdf/2505.23835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23835]] Say What You Mean: Natural Language Access Control with Large Language Models for Internet of Things(https://arxiv.org/abs/2505.23835)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, large language model</a></li>
<li><strong>Abstract: </strong>Access control in the Internet of Things (IoT) is becoming increasingly complex, as policies must account for dynamic and contextual factors such as time, location, user behavior, and environmental conditions. However, existing platforms either offer only coarse-grained controls or rely on rigid rule matching, making them ill-suited for semantically rich or ambiguous access scenarios. Moreover, the policy authoring process remains fragmented: domain experts describe requirements in natural language, but developers must manually translate them into code, introducing semantic gaps and potential misconfiguration. In this work, we present LACE, the Language-based Access Control Engine, a hybrid framework that leverages large language models (LLMs) to bridge the gap between human intent and machine-enforceable logic. LACE combines prompt-guided policy generation, retrieval-augmented reasoning, and formal validation to support expressive, interpretable, and verifiable access control. It enables users to specify policies in natural language, automatically translates them into structured rules, validates semantic correctness, and makes access decisions using a hybrid LLM-rule-based engine. We evaluate LACE in smart home environments through extensive experiments. LACE achieves 100% correctness in verified policy generation and up to 88% decision accuracy with 0.79 F1-score using DeepSeek-V3, outperforming baselines such as GPT-3.5 and Gemini. The system also demonstrates strong scalability under increasing policy volume and request concurrency. Our results highlight LACE's potential to enable secure, flexible, and user-friendly access control across real-world IoT platforms.</li>
</ul>

<h3>Title: Large Language Models Often Know When They Are Being Evaluated</h3>
<ul>
<li><strong>Authors: </strong>Joe Needham, Giles Edkins, Govind Pimpale, Henning Bartsch, Marius Hobbhahn</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23836">https://arxiv.org/abs/2505.23836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23836">https://arxiv.org/pdf/2505.23836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23836]] Large Language Models Often Know When They Are Being Evaluated(https://arxiv.org/abs/2505.23836)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>If AI models can detect when they are being evaluated, the effectiveness of evaluations might be compromised. For example, models could have systematically different behavior during evaluations, leading to less reliable benchmarks for deployment and governance decisions. We investigate whether frontier language models can accurately classify transcripts based on whether they originate from evaluations or real-world deployment, a capability we call evaluation awareness. To achieve this, we construct a diverse benchmark of 1,000 prompts and transcripts from 61 distinct datasets. These span public benchmarks (e.g., MMLU, SWEBench), real-world deployment interactions, and agent trajectories from scaffolding frameworks (e.g., web-browsing agents). Frontier models clearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches an AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of $0.92$). Furthermore, both AI models and humans are better at identifying evaluations in agentic settings compared to chat settings. Additionally, we test whether models can identify the purpose of the evaluation. Under multiple-choice and open-ended questioning, AI models far outperform random chance in identifying what an evaluation is testing for. Our results indicate that frontier models already exhibit a substantial, though not yet superhuman, level of evaluation-awareness. We recommend tracking this capability in future models.</li>
</ul>

<h3>Title: CoMaPOI: A Collaborative Multi-Agent Framework for Next POI Prediction Bridging the Gap Between Trajectory and Language</h3>
<ul>
<li><strong>Authors: </strong>Lin Zhong, Lingzhi Wang, Xu Yang, Qing Liao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23837">https://arxiv.org/abs/2505.23837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23837">https://arxiv.org/pdf/2505.23837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23837]] CoMaPOI: A Collaborative Multi-Agent Framework for Next POI Prediction Bridging the Gap Between Trajectory and Language(https://arxiv.org/abs/2505.23837)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) offer new opportunities for the next Point-Of-Interest (POI) prediction task, leveraging their capabilities in semantic understanding of POI trajectories. However, previous LLM-based methods, which are superficially adapted to next POI prediction, largely overlook critical challenges associated with applying LLMs to this task. Specifically, LLMs encounter two critical challenges: (1) a lack of intrinsic understanding of numeric spatiotemporal data, which hinders accurate modeling of users' spatiotemporal distributions and preferences; and (2) an excessively large and unconstrained candidate POI space, which often results in random or irrelevant predictions. To address these issues, we propose a Collaborative Multi Agent Framework for Next POI Prediction, named CoMaPOI. Through the close interaction of three specialized agents (Profiler, Forecaster, and Predictor), CoMaPOI collaboratively addresses the two critical challenges. The Profiler agent is responsible for converting numeric data into language descriptions, enhancing semantic understanding. The Forecaster agent focuses on dynamically constraining and refining the candidate POI space. The Predictor agent integrates this information to generate high-precision predictions. Extensive experiments on three benchmark datasets (NYC, TKY, and CA) demonstrate that CoMaPOI achieves state of the art performance, improving all metrics by 5% to 10% compared to SOTA baselines. This work pioneers the investigation of challenges associated with applying LLMs to complex spatiotemporal tasks by leveraging tailored collaborative agents.</li>
</ul>

<h3>Title: Exploring the Landscape of Text-to-SQL with Large Language Models: Progresses, Challenges and Opportunities</h3>
<ul>
<li><strong>Authors: </strong>Yiming Huang, Jiyu Guo, Wenxin Mao, Cuiyun Gao, Peiyi Han, Chuanyi Liu, Qing Ling</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23838">https://arxiv.org/abs/2505.23838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23838">https://arxiv.org/pdf/2505.23838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23838]] Exploring the Landscape of Text-to-SQL with Large Language Models: Progresses, Challenges and Opportunities(https://arxiv.org/abs/2505.23838)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Converting natural language (NL) questions into SQL queries, referred to as Text-to-SQL, has emerged as a pivotal technology for facilitating access to relational databases, especially for users without SQL knowledge. Recent progress in large language models (LLMs) has markedly propelled the field of natural language processing (NLP), opening new avenues to improve text-to-SQL systems. This study presents a systematic review of LLM-based text-to-SQL, focusing on four key aspects: (1) an analysis of the research trends in LLM-based text-to-SQL; (2) an in-depth analysis of existing LLM-based text-to-SQL techniques from diverse perspectives; (3) summarization of existing text-to-SQL datasets and evaluation metrics; and (4) discussion on potential obstacles and avenues for future exploration in this domain. This survey seeks to furnish researchers with an in-depth understanding of LLM-based text-to-SQL, sparking new innovations and advancements in this field.</li>
</ul>

<h3>Title: GeneBreaker: Jailbreak Attacks against DNA Language Models with Pathogenicity Guidance</h3>
<ul>
<li><strong>Authors: </strong>Zaixi Zhang, Zhenghong Zhou, Ruofan Jin, Le Cong, Mengdi Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23839">https://arxiv.org/abs/2505.23839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23839">https://arxiv.org/pdf/2505.23839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23839]] GeneBreaker: Jailbreak Attacks against DNA Language Models with Pathogenicity Guidance(https://arxiv.org/abs/2505.23839)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>DNA, encoding genetic instructions for almost all living organisms, fuels groundbreaking advances in genomics and synthetic biology. Recently, DNA Foundation Models have achieved success in designing synthetic functional DNA sequences, even whole genomes, but their susceptibility to jailbreaking remains underexplored, leading to potential concern of generating harmful sequences such as pathogens or toxin-producing genes. In this paper, we introduce GeneBreaker, the first framework to systematically evaluate jailbreak vulnerabilities of DNA foundation models. GeneBreaker employs (1) an LLM agent with customized bioinformatic tools to design high-homology, non-pathogenic jailbreaking prompts, (2) beam search guided by PathoLM and log-probability heuristics to steer generation toward pathogen-like sequences, and (3) a BLAST-based evaluation pipeline against a curated Human Pathogen Database (JailbreakDNABench) to detect successful jailbreaks. Evaluated on our JailbreakDNABench, GeneBreaker successfully jailbreaks the latest Evo series models across 6 viral categories consistently (up to 60\% Attack Success Rate for Evo2-40B). Further case studies on SARS-CoV-2 spike protein and HIV-1 envelope protein demonstrate the sequence and structural fidelity of jailbreak output, while evolutionary modeling of SARS-CoV-2 underscores biosecurity risks. Our findings also reveal that scaling DNA foundation models amplifies dual-use risks, motivating enhanced safety alignment and tracing mechanisms. Our code is at this https URL.</li>
</ul>

<h3>Title: Measuring Sycophancy of Language Models in Multi-turn Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Jiseung Hong, Grace Byun, Seungone Kim, Kai Shu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23840">https://arxiv.org/abs/2505.23840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23840">https://arxiv.org/pdf/2505.23840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23840]] Measuring Sycophancy of Language Models in Multi-turn Dialogues(https://arxiv.org/abs/2505.23840)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are expected to provide helpful and harmless responses, yet they often exhibit sycophancy--conforming to user beliefs regardless of factual accuracy or ethical soundness. Prior research on sycophancy has primarily focused on single-turn factual correctness, overlooking the dynamics of real-world interactions. In this work, we introduce SYCON Bench, a novel benchmark for evaluating sycophantic behavior in multi-turn, free-form conversational settings. Our benchmark measures how quickly a model conforms to the user (Turn of Flip) and how frequently it shifts its stance under sustained user pressure (Number of Flip). Applying SYCON Bench to 17 LLMs across three real-world scenarios, we find that sycophancy remains a prevalent failure mode. Our analysis shows that alignment tuning amplifies sycophantic behavior, whereas model scaling and reasoning optimization strengthen the model's ability to resist undesirable user views. Reasoning models generally outperform instruction-tuned models but often fail when they over-index on logical exposition instead of directly addressing the user's underlying beliefs. Finally, we evaluate four additional prompting strategies and demonstrate that adopting a third-person perspective reduces sycophancy by up to 63.8% in debate scenario. We release our code and data at this https URL.</li>
</ul>

<h3>Title: Document Valuation in LLM Summaries: A Cluster Shapley Approach</h3>
<ul>
<li><strong>Authors: </strong>Zikun Ye, Hema Yoganarasimhan</a></li>
<li><strong>Subjects: </strong>cs.CL, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23842">https://arxiv.org/abs/2505.23842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23842">https://arxiv.org/pdf/2505.23842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23842]] Document Valuation in LLM Summaries: A Cluster Shapley Approach(https://arxiv.org/abs/2505.23842)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used in systems that retrieve and summarize content from multiple sources, such as search engines and AI assistants. While these models enhance user experience by generating coherent summaries, they obscure the contributions of original content creators, raising concerns about credit attribution and compensation. We address the challenge of valuing individual documents used in LLM-generated summaries. We propose using Shapley values, a game-theoretic method that allocates credit based on each document's marginal contribution. Although theoretically appealing, Shapley values are expensive to compute at scale. We therefore propose Cluster Shapley, an efficient approximation algorithm that leverages semantic similarity between documents. By clustering documents using LLM-based embeddings and computing Shapley values at the cluster level, our method significantly reduces computation while maintaining attribution quality. We demonstrate our approach to a summarization task using Amazon product reviews. Cluster Shapley significantly reduces computational complexity while maintaining high accuracy, outperforming baseline methods such as Monte Carlo sampling and Kernel SHAP with a better efficient frontier. Our approach is agnostic to the exact LLM used, the summarization process used, and the evaluation procedure, which makes it broadly applicable to a variety of summarization settings.</li>
</ul>

<h3>Title: Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks</h3>
<ul>
<li><strong>Authors: </strong>Wenhan Dong, Tianyi Hu, Jingyi Zheng, Zhen Sun, Yuemeng Zhao, Yule Liu, Xinlei He, Xinyi Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23843">https://arxiv.org/abs/2505.23843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23843">https://arxiv.org/pdf/2505.23843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23843]] Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks(https://arxiv.org/abs/2505.23843)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-round incomplete information tasks are crucial for evaluating the lateral thinking capabilities of large language models (LLMs). Currently, research primarily relies on multiple benchmarks and automated evaluation metrics to assess these abilities. However, our study reveals novel insights into the limitations of existing methods, as they often yield misleading results that fail to uncover key issues, such as shortcut-taking behaviors, rigid patterns, and premature task termination. These issues obscure the true reasoning capabilities of LLMs and undermine the reliability of evaluations. To address these limitations, we propose a refined set of evaluation standards, including inspection of reasoning paths, diversified assessment metrics, and comparative analyses with human performance.</li>
</ul>

<h3>Title: Enabling Flexible Multi-LLM Integration for Scalable Knowledge Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Zhenglun Kong, Zheng Zhan, Shiyue Hou, Yifan Gong, Xin Meng, Pengwei Sui, Peiyan Dong, Xuan Shen, Zifeng Wang, Pu Zhao, Hao Tang, Stratis Ioannidis, Yanzhi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23844">https://arxiv.org/abs/2505.23844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23844">https://arxiv.org/pdf/2505.23844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23844]] Enabling Flexible Multi-LLM Integration for Scalable Knowledge Aggregation(https://arxiv.org/abs/2505.23844)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable promise but remain challenging to continually improve through traditional finetuning, particularly when integrating capabilities from other specialized LLMs. Popular methods like ensemble and weight merging require substantial memory and struggle to adapt to changing data environments. Recent efforts have transferred knowledge from multiple LLMs into a single target model; however, they suffer from interference and degraded performance among tasks, largely due to limited flexibility in candidate selection and training pipelines. To address these issues, we propose a framework that adaptively selects and aggregates knowledge from diverse LLMs to build a single, stronger model, avoiding the high memory overhead of ensemble and inflexible weight merging. Specifically, we design an adaptive selection network that identifies the most relevant source LLMs based on their scores, thereby reducing knowledge interference. We further propose a dynamic weighted fusion strategy that accounts for the inherent strengths of candidate LLMs, along with a feedback-driven loss function that prevents the selector from converging on a single subset of sources. Experimental results demonstrate that our method can enable a more stable and scalable knowledge aggregation process while reducing knowledge interference by up to 50% compared to existing approaches. Code is avaliable at this https URL</li>
</ul>

<h3>Title: Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jakub Podolak, Rajeev Verma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23845">https://arxiv.org/abs/2505.23845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23845">https://arxiv.org/pdf/2505.23845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23845]] Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in LLMs(https://arxiv.org/abs/2505.23845)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We study the source of uncertainty in DeepSeek R1-32B by analyzing its self-reported verbal confidence on question answering (QA) tasks. In the default answer-then-confidence setting, the model is regularly over-confident, whereas semantic entropy - obtained by sampling many responses - remains reliable. We hypothesize that this is because of semantic entropy's larger test-time compute, which lets us explore the model's predictive distribution. We show that granting DeepSeek the budget to explore its distribution by forcing a long chain-of-thought before the final answer greatly improves its verbal score effectiveness, even on simple fact-retrieval questions that normally require no reasoning. Furthermore, a separate reader model that sees only the chain can reconstruct very similar confidences, indicating the verbal score might be merely a statistic of the alternatives surfaced during reasoning. Our analysis concludes that reliable uncertainty estimation requires explicit exploration of the generative space, and self-reported confidence is trustworthy only after such exploration.</li>
</ul>

<h3>Title: Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems</h3>
<ul>
<li><strong>Authors: </strong>Ronny Ko, Jiseong Jeong, Shuyuan Zheng, Chuan Xiao, Taewan Kim, Makoto Onizuka, Wonyong Shin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23847">https://arxiv.org/abs/2505.23847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23847">https://arxiv.org/pdf/2505.23847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23847]] Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems(https://arxiv.org/abs/2505.23847)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are rapidly evolving into autonomous agents that cooperate across organizational boundaries, enabling joint disaster response, supply-chain optimization, and other tasks that demand decentralized expertise without surrendering data ownership. Yet, cross-domain collaboration shatters the unified trust assumptions behind current alignment and containment techniques. An agent benign in isolation may, when receiving messages from an untrusted peer, leak secrets or violate policy, producing risks driven by emergent multi-agent dynamics rather than classical software bugs. This position paper maps the security agenda for cross-domain multi-agent LLM systems. We introduce seven categories of novel security challenges, for each of which we also present plausible attacks, security evaluation metrics, and future research guidelines.</li>
</ul>

<h3>Title: Derailing Non-Answers via Logit Suppression at Output Subspace Boundaries in RLHF-Aligned Language Models</h3>
<ul>
<li><strong>Authors: </strong>Harvey Dam, Jonas Knochelmann, Vinu Joseph, Ganesh Gopalakrishnan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23848">https://arxiv.org/abs/2505.23848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23848">https://arxiv.org/pdf/2505.23848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23848]] Derailing Non-Answers via Logit Suppression at Output Subspace Boundaries in RLHF-Aligned Language Models(https://arxiv.org/abs/2505.23848)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce a method to reduce refusal rates of large language models (LLMs) on sensitive content without modifying model weights or prompts. Motivated by the observation that refusals in certain models were often preceded by the specific token sequence of a token marking the beginning of the chain-of-thought (CoT) block (<think>) followed by a double newline token (\n\n), we investigate the impact of two simple formatting adjustments during generation: suppressing \n\n after <think> and suppressing the end-of-sequence token after the end of the CoT block (</think>). Our method requires no datasets, parameter changes, or training, relying solely on modifying token probabilities during generation. In our experiments with official DeepSeek-R1 distillations, these interventions increased the proportion of substantive answers to sensitive prompts without affecting performance on standard benchmarks. Our findings suggest that refusal behaviors can be circumvented by blocking refusal subspaces at specific points in the generation process.</li>
</ul>

<h3>Title: CADRE: Customizable Assurance of Data Readiness in Privacy-Preserving Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Kaveen Hiniduma, Zilinghan Li, Aditya Sinha, Ravi Madduri, Suren Byna</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23849">https://arxiv.org/abs/2505.23849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23849">https://arxiv.org/pdf/2505.23849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23849]] CADRE: Customizable Assurance of Data Readiness in Privacy-Preserving Federated Learning(https://arxiv.org/abs/2505.23849)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, federate, fair</a></li>
<li><strong>Abstract: </strong>Privacy-Preserving Federated Learning (PPFL) is a decentralized machine learning approach where multiple clients train a model collaboratively. PPFL preserves privacy and security of the client's data by not exchanging it. However, ensuring that data at each client is of high quality and ready for federated learning (FL) is a challenge due to restricted data access. In this paper, we introduce CADRE (Customizable Assurance of Data REadiness) for FL, a novel framework that allows users to define custom data readiness (DR) standards, metrics, rules, and remedies tailored to specific FL tasks. Our framework generates comprehensive DR reports based on the user-defined metrics, rules, and remedies to ensure datasets are optimally prepared for FL while preserving privacy. We demonstrate the framework's practical application by integrating it into an existing PPFL framework. We conducted experiments across six diverse datasets, addressing seven different DR issues. The results illustrate the framework's versatility and effectiveness in ensuring DR across various dimensions, including data quality, privacy, and fairness. This approach enhances the performance and reliability of FL models as well as utilizes valuable resources by identifying and addressing data-related issues before the training phase.</li>
</ul>

<h3>Title: ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Michael Shalyt, Rotem Elimelech, Ido Kaminer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23851">https://arxiv.org/abs/2505.23851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23851">https://arxiv.org/pdf/2505.23851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23851]] ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark(https://arxiv.org/abs/2505.23851)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are rapidly approaching the level of proficiency in university-level symbolic mathematics required for applications in advanced science and technology. However, existing benchmarks fall short in assessing the core skills of LLMs in symbolic mathematics-such as integration, differential equations, and algebraic simplification. To address this gap, we introduce ASyMOB, a novel assessment framework focused exclusively on symbolic manipulation, featuring 17,092 unique math challenges, organized by similarity and complexity. ASyMOB enables analysis of LLM generalization capabilities by comparing performance in problems that differ by simple numerical or symbolic `perturbations'. Evaluated LLMs exhibit substantial degradation in performance for all perturbation types (up to -70.3%), suggesting reliance on memorized patterns rather than deeper understanding of symbolic math, even among models achieving high baseline accuracy. Comparing LLM performance to computer algebra systems, we identify examples where they fail while LLMs succeed, as well as problems solved only by combining both approaches. Models capable of integrated code execution yielded higher accuracy compared to their performance without code, particularly stabilizing weaker models (up to +33.1% for certain perturbation types). Notably, the most advanced models (o4-mini, Gemini 2.5 Flash) demonstrate not only high symbolic math proficiency (scoring 96.8% and 97.6% on the unperturbed set), but also remarkable robustness against perturbations, (-21.7% and -21.2% vs. average -50.4% for the other models). This may indicate a recent "phase transition" in the generalization capabilities of frontier LLMs. It remains to be seen whether the path forward lies in deeper integration with sophisticated external tools, or in developing models so capable that symbolic math systems like CAS become unnecessary.</li>
</ul>

<h3>Title: Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease</h3>
<ul>
<li><strong>Authors: </strong>Nic Dobbins, Christelle Xiong, Kristine Lan, Meliha Yetisgen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23852">https://arxiv.org/abs/2505.23852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23852">https://arxiv.org/pdf/2505.23852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23852]] Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease(https://arxiv.org/abs/2505.23852)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Objective: To demonstrate the capabilities of Large Language Models (LLMs) as autonomous agents to reproduce findings of published research studies using the same or similar dataset. Materials and Methods: We used the "Quick Access" dataset of the National Alzheimer's Coordinating Center (NACC). We identified highly cited published research manuscripts using NACC data and selected five studies that appeared reproducible using this dataset alone. Using GPT-4o, we created a simulated research team of LLM-based autonomous agents tasked with writing and executing code to dynamically reproduce the findings of each study, given only study Abstracts, Methods sections, and data dictionary descriptions of the dataset. Results: We extracted 35 key findings described in the Abstracts across 5 Alzheimer's studies. On average, LLM agents approximately reproduced 53.2% of findings per study. Numeric values and range-based findings often differed between studies and agents. The agents also applied statistical methods or parameters that varied from the originals, though overall trends and significance were sometimes similar. Discussion: In some cases, LLM-based agents replicated research techniques and findings. In others, they failed due to implementation flaws or missing methodological detail. These discrepancies show the current limits of LLMs in fully automating reproducibility assessments. Still, this early investigation highlights the potential of structured agent-based systems to provide scalable evaluation of scientific rigor. Conclusion: This exploratory work illustrates both the promise and limitations of LLMs as autonomous agents for automating reproducibility in biomedical research.</li>
</ul>

<h3>Title: Revisiting Uncertainty Estimation and Calibration of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Linwei Tao, Yi-Fan Yeh, Minjing Dong, Tao Huang, Philip Torr, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23854">https://arxiv.org/abs/2505.23854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23854">https://arxiv.org/pdf/2505.23854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23854]] Revisiting Uncertainty Estimation and Calibration of Large Language Models(https://arxiv.org/abs/2505.23854)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are increasingly deployed in high-stakes applications, robust uncertainty estimation is essential for ensuring the safe and trustworthy deployment of LLMs. We present the most comprehensive study to date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and closed-source families, dense and Mixture-of-Experts (MoE) architectures, reasoning and non-reasoning modes, quantization variants and parameter scales from 0.6B to 671B. Focusing on three representative black-box single-pass methods, including token probability-based uncertainty (TPU), numerical verbal uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically evaluate uncertainty calibration and selective classification using the challenging MMLU-Pro benchmark, which covers both reasoning-intensive and knowledge-based tasks. Our results show that LVU consistently outperforms TPU and NVU, offering stronger calibration and discrimination while being more interpretable. We also find that high accuracy does not imply reliable uncertainty, and that model scale, post-training, reasoning ability and quantization all influence estimation performance. Notably, LLMs exhibit better uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good calibration does not necessarily translate to effective error ranking. These findings highlight the need for multi-perspective evaluation and position LVU as a practical tool for improving the reliability of LLMs in real-world settings.</li>
</ul>

<h3>Title: OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities</h3>
<ul>
<li><strong>Authors: </strong>Sahil Verma, Keegan Hines, Jeff Bilmes, Charlotte Siska, Luke Zettlemoyer, Hila Gonen, Chandan Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23856">https://arxiv.org/abs/2505.23856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23856">https://arxiv.org/pdf/2505.23856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23856]] OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities(https://arxiv.org/abs/2505.23856)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose OMNIGUARD, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\% over the strongest baseline in a multilingual setting, by 20.44\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, OMNIGUARD is also very efficient ($\approx 120 \times$ faster than the next fastest baseline). Code and data are available at: this https URL.</li>
</ul>

<h3>Title: BiBLDR: Bidirectional Behavior Learning for Drug Repositioning</h3>
<ul>
<li><strong>Authors: </strong>Renye Zhang, Mengyun Yang, Qichang Zhao, Jianxin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23861">https://arxiv.org/abs/2505.23861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23861">https://arxiv.org/pdf/2505.23861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23861]] BiBLDR: Bidirectional Behavior Learning for Drug Repositioning(https://arxiv.org/abs/2505.23861)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Drug repositioning aims to identify potential new indications for existing drugs to reduce the time and financial costs associated with developing new drugs. Most existing deep learning-based drug repositioning methods predominantly utilize graph-based representations. However, graph-based drug repositioning methods struggle to perform effective inference in cold-start scenarios involving novel drugs because of the lack of association information with the diseases. Unlike traditional graph-based approaches, we propose a bidirectional behavior learning strategy for drug repositioning, known as BiBLDR. This innovative framework redefines drug repositioning as a behavior sequential learning task to capture drug-disease interaction patterns. First, we construct bidirectional behavioral sequences based on drug and disease sides. The consideration of bidirectional information ensures a more meticulous and rigorous characterization of the behavioral sequences. Subsequently, we propose a two-stage strategy for drug repositioning. In the first stage, we construct prototype spaces to characterize the representational attributes of drugs and diseases. In the second stage, these refined prototypes and bidirectional behavior sequence data are leveraged to predict potential drug-disease associations. Based on this learning approach, the model can more robustly and precisely capture the interactive relationships between drug and disease features from bidirectional behavioral sequences. Extensive experiments demonstrate that our method achieves state-of-the-art performance on benchmark datasets. Meanwhile, BiBLDR demonstrates significantly superior performance compared to previous methods in cold-start scenarios. Our code is published in this https URL.</li>
</ul>

<h3>Title: Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Chang Liu, Bohao Zhao, Jingtao Ding, Huandong Wang, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23863">https://arxiv.org/abs/2505.23863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23863">https://arxiv.org/pdf/2505.23863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23863]] Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting(https://arxiv.org/abs/2505.23863)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Long-term forecasting of chaotic systems from short-term observations remains a fundamental and underexplored challenge due to the intrinsic sensitivity to initial conditions and the complex geometry of strange attractors. Existing approaches often rely on long-term training data or focus on short-term sequence correlations, struggling to maintain predictive stability and dynamical coherence over extended horizons. We propose PhyxMamba, a novel framework that integrates a Mamba-based state-space model with physics-informed principles to capture the underlying dynamics of chaotic systems. By reconstructing the attractor manifold from brief observations using time-delay embeddings, PhyxMamba extracts global dynamical features essential for accurate forecasting. Our generative training scheme enables Mamba to replicate the physical process, augmented by multi-token prediction and attractor geometry regularization for physical constraints, enhancing prediction accuracy and preserving key statistical invariants. Extensive evaluations on diverse simulated and real-world chaotic systems demonstrate that PhyxMamba delivers superior long-term forecasting and faithfully captures essential dynamical invariants from short-term data. This framework opens new avenues for reliably predicting chaotic systems under observation-scarce conditions, with broad implications across climate science, neuroscience, epidemiology, and beyond. Our code is open-source at this https URL.</li>
</ul>

<h3>Title: Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhuo, Zhaohuan Zhan, Ziduo Yang, Han Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23864">https://arxiv.org/abs/2505.23864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23864">https://arxiv.org/pdf/2505.23864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23864]] Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections(https://arxiv.org/abs/2505.23864)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) on graph-structured data typically faces non-IID challenges, particularly in scenarios where each client holds a distinct subgraph sampled from a global graph. In this paper, we introduce Federated learning with Auxiliary projections (FedAux), a personalized subgraph FL framework that learns to align, compare, and aggregate heterogeneously distributed local models without sharing raw data or node embeddings. In FedAux, each client jointly trains (i) a local GNN and (ii) a learnable auxiliary projection vector (APV) that differentiably projects node embeddings onto a 1D space. A soft-sorting operation followed by a lightweight 1D convolution refines these embeddings in the ordered space, enabling the APV to effectively capture client-specific information. After local training, these APVs serve as compact signatures that the server uses to compute inter-client similarities and perform similarity-weighted parameter mixing, yielding personalized models while preserving cross-client knowledge transfer. Moreover, we provide rigorous theoretical analysis to establish the convergence and rationality of our design. Empirical evaluations across diverse graph benchmarks demonstrate that FedAux substantially outperforms existing baselines in both accuracy and personalization performance.</li>
</ul>

<h3>Title: Combining Deep Architectures for Information Gain estimation and Reinforcement Learning for multiagent field exploration</h3>
<ul>
<li><strong>Authors: </strong>Emanuele Masiero, Vito Trianni, Giuseppe Vizzari, Dimitri Ognibene</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23865">https://arxiv.org/abs/2505.23865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23865">https://arxiv.org/pdf/2505.23865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23865]] Combining Deep Architectures for Information Gain estimation and Reinforcement Learning for multiagent field exploration(https://arxiv.org/abs/2505.23865)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Precision agriculture requires efficient autonomous systems for crop monitoring, where agents must explore large-scale environments while minimizing resource consumption. This work addresses the problem as an active exploration task in a grid environment representing an agricultural field. Each cell may contain targets (e.g., damaged crops) observable from nine predefined points of view (POVs). Agents must infer the number of targets per cell using partial, sequential observations. We propose a two-stage deep learning framework. A pre-trained LSTM serves as a belief model, updating a probabilistic map of the environment and its associated entropy, which defines the expected information gain (IG). This allows agents to prioritize informative regions. A key contribution is the inclusion of a POV visibility mask in the input, preserving the Markov property under partial observability and avoiding revisits to already explored views. Three agent architectures were compared: an untrained IG-based agent selecting actions to maximize entropy reduction; a DQN agent using CNNs over local 3x3 inputs with belief, entropy, and POV mask; and a Double-CNN DQN agent with wider spatial context. Simulations on 20x20 maps showed that the untrained agent performs well despite its simplicity. The DQN agent matches this performance when the POV mask is included, while the Double-CNN agent consistently achieves superior exploration efficiency, especially in larger environments. Results show that uncertainty-aware policies leveraging entropy, belief states, and visibility tracking lead to robust and scalable exploration. Future work includes curriculum learning, multi-agent cooperation with shared rewards, transformer-based models, and intrinsic motivation mechanisms to further enhance learning efficiency and policy generalization.</li>
</ul>

<h3>Title: Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Liu, Zhitian Hou, Yining Di, Kejing Yang, Zhijie Sang, Congkai Xie, Jingwen Yang, Siyuan Liu, Jialu Wang, Chunming Li, Ming Li, Hongxia Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23867">https://arxiv.org/abs/2505.23867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23867">https://arxiv.org/pdf/2505.23867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23867]] Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation(https://arxiv.org/abs/2505.23867)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have demonstrated promising prospects in healthcare, particularly for addressing complex medical tasks, supporting multidisciplinary treatment (MDT), and enabling personalized precision medicine. However, their practical deployment faces critical challenges in resource efficiency, diagnostic accuracy, clinical considerations, and ethical privacy. To address these limitations, we propose Infi-Med, a comprehensive framework for medical MLLMs that introduces three key innovations: (1) a resource-efficient approach through curating and constructing high-quality supervised fine-tuning (SFT) datasets with minimal sample requirements, with a forward-looking design that extends to both pretraining and posttraining phases; (2) enhanced multimodal reasoning capabilities for cross-modal integration and clinical task understanding; and (3) a systematic evaluation system that assesses model performance across medical modalities and task types. Our experiments demonstrate that Infi-Med achieves state-of-the-art (SOTA) performance in general medical reasoning while maintaining rapid adaptability to clinical scenarios. The framework establishes a solid foundation for deploying MLLMs in real-world healthcare settings by balancing model effectiveness with operational constraints.</li>
</ul>

<h3>Title: Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert</h3>
<ul>
<li><strong>Authors: </strong>Zhaokun Wang, Jinyu Guo, Jingwen Pu, Lingfeng Chen, Hongli Pu, Jie Ou.Libo Qin, Wenhong Tian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23868">https://arxiv.org/abs/2505.23868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23868">https://arxiv.org/pdf/2505.23868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23868]] Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert(https://arxiv.org/abs/2505.23868)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Current parameter-efficient fine-tuning methods for adapting pre-trained language models to downstream tasks are susceptible to interference from noisy data. Conventional noise-handling approaches either rely on laborious data pre-processing or employ model architecture modifications prone to error accumulation. In contrast to existing noise-process paradigms, we propose a noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a novel framework that enhances model robustness to noise only with generated noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE strategically integrates a dedicated poisoning expert in an asymmetric LoRA configuration. Through a two-stage paradigm, LoPE performs noise injection on the poisoning expert during fine-tuning to enhance its noise discrimination and processing ability. During inference, we selectively mask the dedicated poisoning expert to leverage purified knowledge acquired by normal experts for noise-robust output. Extensive experiments demonstrate that LoPE achieves strong performance and robustness purely through the low-cost noise injection, which completely eliminates the requirement of data cleaning.</li>
</ul>

<h3>Title: ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zeyuan Liu, Zhihe Yang, Jiawei Xu, Rui Yang, Jiafei Lyu, Baoxiang Wang, Yunjian Xu, Xiu Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23871">https://arxiv.org/abs/2505.23871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23871">https://arxiv.org/pdf/2505.23871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23871]] ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning(https://arxiv.org/abs/2505.23871)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Real-world datasets collected from sensors or human inputs are prone to noise and errors, posing significant challenges for applying offline reinforcement learning (RL). While existing methods have made progress in addressing corrupted actions and rewards, they remain insufficient for handling corruption in high-dimensional state spaces and for cases where multiple elements in the dataset are corrupted simultaneously. Diffusion models, known for their strong denoising capabilities, offer a promising direction for this problem-but their tendency to overfit noisy samples limits their direct applicability. To overcome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a novel approach that pioneers the use of diffusion models to tackle data corruption in offline RL. First, we introduce Ambient Denoising Diffusion Probabilistic Models (DDPM) from approximated distributions, which enable learning on partially corrupted datasets with theoretical guarantees. Second, we use the noise-prediction property of Ambient DDPM to distinguish between clean and corrupted data, and then use the clean subset to train a standard DDPM. Third, we employ the trained standard DDPM to refine the previously identified corrupted data, enhancing data quality for subsequent offline RL training. A notable strength of ADG is its versatility-it can be seamlessly integrated with any offline RL algorithm. Experiments on a range of benchmarks, including MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively mitigates the impact of corrupted data and improves the robustness of offline RL under various noise settings, achieving state-of-the-art results.</li>
</ul>

<h3>Title: KGMark: A Diffusion Watermark for Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Hongrui Peng, Haolang Lu, Yuanlong Yu, Weiye Fu, Kun Wang, Guoshun Nan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23873">https://arxiv.org/abs/2505.23873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23873">https://arxiv.org/pdf/2505.23873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23873]] KGMark: A Diffusion Watermark for Knowledge Graphs(https://arxiv.org/abs/2505.23873)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust, watermark, diffusion</a></li>
<li><strong>Abstract: </strong>Knowledge graphs (KGs) are ubiquitous in numerous real-world applications, and watermarking facilitates protecting intellectual property and preventing potential harm from AI-generated content. Existing watermarking methods mainly focus on static plain text or image data, while they can hardly be applied to dynamic graphs due to spatial and temporal variations of structured data. This motivates us to propose KGMARK, the first graph watermarking framework that aims to generate robust, detectable, and transparent diffusion fingerprints for dynamic KG data. Specifically, we propose a novel clustering-based alignment method to adapt the watermark to spatial variations. Meanwhile, we present a redundant embedding strategy to harden the diffusion watermark against various attacks, facilitating the robustness of the watermark to the temporal variations. Additionally, we introduce a novel learnable mask matrix to improve the transparency of diffusion fingerprints. By doing so, our KGMARK properly tackles the variation challenges of structured data. Experiments on various public benchmarks show the effectiveness of our proposed KGMARK.</li>
</ul>

<h3>Title: A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size</h3>
<ul>
<li><strong>Authors: </strong>Polad Geidarov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23876">https://arxiv.org/abs/2505.23876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23876">https://arxiv.org/pdf/2505.23876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23876]] A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size(https://arxiv.org/abs/2505.23876)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The paper discusses the capabilities of multilayer perceptron neural networks implementing metric recognition methods, for which the values of the weights are calculated analytically by formulas. Comparative experiments in training a neural network with pre-calculated weights and with random initialization of weights on different sizes of the MNIST training dataset are carried out. The results of the experiments show that a multilayer perceptron with pre-calculated weights can be trained much faster and is much more robust to the reduction of the training dataset.</li>
</ul>

<h3>Title: Actor-Critic based Online Data Mixing For Language Model Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Jing Ma, Chenhao Dang, Mingjie Liao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23878">https://arxiv.org/abs/2505.23878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23878">https://arxiv.org/pdf/2505.23878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23878]] Actor-Critic based Online Data Mixing For Language Model Pre-Training(https://arxiv.org/abs/2505.23878)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The coverage and composition of pretraining data significantly impacts the generalization ability of Large Language Models (LLMs). To reduce the carbon footprint and financial costs of training, some data mixing methods, which applied the optimized domain weights of a small proxy model to train a larger one, were proposed. However, these methods did not evolute with the training dynamics. The existing online data mixing (ODM) method addressed this limitation by applying the multi-armed bandit algorithm as data sampling strategy. Yet, it did not consider the intra-domain interactions. In this paper, we develop an actor-critic based online data mixing (AC-ODM) method, which captures the varying domain weights by auxiliary actor-critic networks and consider the intra-domain interactions with the reward function. While constructing the dataset to pretrain a large target LLM, we directly apply the actor, which is trained with a small proxy LLM as the environment, as the sampling strategy. The transfer of sampling strategy can not only ensure the efficiency of dynamical data mixing, but also expedite the convergence of pretraining the target LLM. Numerical results demonstrate that AC-ODM-410M, which invokes the sampling strategy obtained by a proxy LLM with 410M parameters, reaching the optimal validation perplexity of ODM 71% faster, and improves performance on the zero-shot MMLU benchmark by 27.5% of accuracy, about 2.23x better on pass@1 of HumanEval benchmark.</li>
</ul>

<h3>Title: CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from Spike Sequences and Clinical Data</h3>
<ul>
<li><strong>Authors: </strong>Caio Cheohen, VinnÃ­cius M. S. Gomes, Manuela L. da Silva</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23879">https://arxiv.org/abs/2505.23879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23879">https://arxiv.org/pdf/2505.23879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23879]] CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from Spike Sequences and Clinical Data(https://arxiv.org/abs/2505.23879)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>The COVID-19 pandemic, caused by SARS-CoV-2, highlighted the critical need for accurate prediction of disease severity to optimize healthcare resource allocation and patient management. The spike protein, which facilitates viral entry into host cells, exhibits high mutation rates, particularly in the receptor-binding domain, influencing viral pathogenicity. Artificial intelligence approaches, such as deep learning, offer promising solutions for leveraging genomic and clinical data to predict disease outcomes. Objective: This study aimed to develop a hybrid CNN-LSTM deep learning model to predict COVID-19 severity using spike protein sequences and associated clinical metadata from South American patients. Methods: We retrieved 9,570 spike protein sequences from the GISAID database, of which 3,467 met inclusion criteria after standardization. The dataset included 2,313 severe and 1,154 mild cases. A feature engineering pipeline extracted features from sequences, while demographic and clinical variables were one-hot encoded. A hybrid CNN-LSTM architecture was trained, combining CNN layers for local pattern extraction and an LSTM layer for long-term dependency modeling. Results: The model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%, and recall of 82.85%, demonstrating robust classification performance. Training stabilized at 85% accuracy with minimal overfitting. The most prevalent lineages (P.1, AY.99.2) and clades (GR, GK) aligned with regional epidemiological trends, suggesting potential associations between viral genetics and clinical outcomes. Conclusion: The CNN-LSTM hybrid model effectively predicted COVID-19 severity using spike protein sequences and clinical data, highlighting the utility of AI in genomic surveillance and precision public health. Despite limitations, this approach provides a framework for early severity prediction in future outbreaks.</li>
</ul>

<h3>Title: Synopsis: Secure and private trend inference from encrypted semantic embeddings</h3>
<ul>
<li><strong>Authors: </strong>Madelyne Xiao, Palak Jain, Micha Gorelick, Sarah Scheffler</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23880">https://arxiv.org/abs/2505.23880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23880">https://arxiv.org/pdf/2505.23880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23880]] Synopsis: Secure and private trend inference from encrypted semantic embeddings(https://arxiv.org/abs/2505.23880)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>WhatsApp and many other commonly used communication platforms guarantee end-to-end encryption (E2EE), which requires that service providers lack the cryptographic keys to read communications on their own platforms. WhatsApp's privacy-preserving design makes it difficult to study important phenomena like the spread of misinformation or political messaging, as users have a clear expectation and desire for privacy and little incentive to forfeit that privacy in the process of handing over raw data to researchers, journalists, or other parties. We introduce Synopsis, a secure architecture for analyzing messaging trends in consensually-donated E2EE messages using message embeddings. Since the goal of this system is investigative journalism workflows, Synopsis must facilitate both exploratory and targeted analyses -- a challenge for systems using differential privacy (DP), and, for different reasons, a challenge for private computation approaches based on cryptography. To meet these challenges, we combine techniques from the local and central DP models and wrap the system in malicious-secure multi-party computation to ensure the DP query architecture is the only way to access messages, preventing any party from directly viewing stored message embeddings. Evaluations on a dataset of Hindi-language WhatsApp messages (34,024 messages represented as 500-dimensional embeddings) demonstrate the efficiency and accuracy of our approach. Queries on this data run in about 30 seconds, and the accuracy of the fine-grained interface exceeds 94% on benchmark tasks.</li>
</ul>

<h3>Title: Test-Time Training Done Right</h3>
<ul>
<li><strong>Authors: </strong>Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William T. Freeman, Hao Tan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23884">https://arxiv.org/abs/2505.23884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23884">https://arxiv.org/pdf/2505.23884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23884]] Test-Time Training Done Right(https://arxiv.org/abs/2505.23884)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Test-Time Training (TTT) models context dependencies by adapting part of the model's weights (referred to as fast weights) during inference. This fast weight, akin to recurrent states in RNNs, stores temporary memories of past tokens in the current sequence. Existing TTT methods struggled to show effectiveness in handling long-context data, due to their inefficiency on modern GPUs. The TTT layers in many of these approaches operate with extremely low FLOPs utilization (often <5%) because they deliberately apply small online minibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover, a small minibatch implies fine-grained block-wise causal dependencies in the data, unsuitable for data beyond 1D ordered sequences, like sets or N-dimensional grids such as images or videos. In contrast, we pursue the opposite direction by using an extremely large chunk update, ranging from 2K to 1M tokens across tasks of varying modalities, which we refer to as Large Chunk Test-Time Training (LaCT). It improves hardware utilization by orders of magnitude, and more importantly, facilitates scaling of nonlinear state size (up to 40% of model parameters), hence substantially improving state capacity, all without requiring cumbersome and error-prone kernel implementations. It also allows easy integration of sophisticated optimizers, e.g. Muon for online updates. We validate our approach across diverse modalities and tasks, including novel view synthesis with image set, language models, and auto-regressive video diffusion. Our approach can scale up to 14B-parameter AR video diffusion model on sequences up to 56K tokens. In our longest sequence experiment, we perform novel view synthesis with 1 million context length. We hope this work will inspire and accelerate new research in the field of long-context modeling and test-time training. Website: this https URL</li>
</ul>

<h3>Title: Generating Fit Check Videos with a Handheld Camera</h3>
<ul>
<li><strong>Authors: </strong>Bowei Chen, Brian Curless, Ira Kemelmacher-Shlizerman, Steven M. Seitz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23886">https://arxiv.org/abs/2505.23886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23886">https://arxiv.org/pdf/2505.23886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23886]] Generating Fit Check Videos with a Handheld Camera(https://arxiv.org/abs/2505.23886)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Self-captured full-body videos are popular, but most deployments require mounted cameras, carefully-framed shots, and repeated practice. We propose a more convenient solution that enables full-body video capture using handheld mobile devices. Our approach takes as input two static photos (front and back) of you in a mirror, along with an IMU motion reference that you perform while holding your mobile phone, and synthesizes a realistic video of you performing a similar target motion. We enable rendering into a new scene, with consistent illumination and shadows. We propose a novel video diffusion-based model to achieve this. Specifically, we propose a parameter-free frame generation strategy, as well as a multi-reference attention mechanism, that effectively integrate appearance information from both the front and back selfies into the video diffusion model. Additionally, we introduce an image-based fine-tuning strategy to enhance frame sharpness and improve the generation of shadows and reflections, achieving a more realistic human-scene composition.</li>
</ul>

<h3>Title: Cora: Correspondence-aware image editing using few step diffusion</h3>
<ul>
<li><strong>Authors: </strong>Amirhossein Almohammadi, Aryan Mikaeili, Sauradip Nag, Negar Hassanpour, Andrea Tagliasacchi, Ali Mahdavi-Amiri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23907">https://arxiv.org/abs/2505.23907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23907">https://arxiv.org/pdf/2505.23907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23907]] Cora: Correspondence-aware image editing using few step diffusion(https://arxiv.org/abs/2505.23907)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image editing is an important task in computer graphics, vision, and VFX, with recent diffusion-based methods achieving fast and high-quality results. However, edits requiring significant structural changes, such as non-rigid deformations, object modifications, or content generation, remain challenging. Existing few step editing approaches produce artifacts such as irrelevant texture or struggle to preserve key attributes of the source image (e.g., pose). We introduce Cora, a novel editing framework that addresses these limitations by introducing correspondence-aware noise correction and interpolated attention maps. Our method aligns textures and structures between the source and target images through semantic correspondence, enabling accurate texture transfer while generating new content when necessary. Cora offers control over the balance between content generation and preservation. Extensive experiments demonstrate that, quantitatively and qualitatively, Cora excels in maintaining structure, textures, and identity across diverse edits, including pose changes, object addition, and texture refinements. User studies confirm that Cora delivers superior results, outperforming alternatives.</li>
</ul>

<h3>Title: One Task Vector is not Enough: A Large-Scale Study for In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Pavel Tikhonov, Ivan Oseledets, Elena Tutubalina</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23911">https://arxiv.org/abs/2505.23911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23911">https://arxiv.org/pdf/2505.23911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23911]] One Task Vector is not Enough: A Large-Scale Study for In-Context Learning(https://arxiv.org/abs/2505.23911)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) enables Large Language Models (LLMs) to adapt to new tasks using few examples, with task vectors - specific hidden state activations - hypothesized to encode task information. Existing studies are limited by small-scale benchmarks, restricting comprehensive analysis. We introduce QuiteAFew, a novel dataset of 3,096 diverse few-shot tasks, each with 30 input-output pairs derived from the Alpaca dataset. Experiments with Llama-3-8B on QuiteAFew reveal: (1) task vector performance peaks at an intermediate layer (e.g., 15th), (2) effectiveness varies significantly by task type, and (3) complex tasks rely on multiple, subtask-specific vectors rather than a single vector, suggesting distributed task knowledge representation.</li>
</ul>

<h3>Title: Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation</h3>
<ul>
<li><strong>Authors: </strong>Caiqi Zhang, Xiaochen Zhu, Chengzu Li, Nigel Collier, Andreas Vlachos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23912">https://arxiv.org/abs/2505.23912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23912">https://arxiv.org/pdf/2505.23912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23912]] Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation(https://arxiv.org/abs/2505.23912)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Hallucination remains a major challenge for the safe and trustworthy deployment of large language models (LLMs) in factual content generation. Prior work has explored confidence estimation as an effective approach to hallucination detection, but often relies on post-hoc self-consistency methods that require computationally expensive sampling. Verbalized confidence offers a more efficient alternative, but existing approaches are largely limited to short-form question answering (QA) tasks and do not generalize well to open-ended generation. In this paper, we propose LoVeC (Long-form Verbalized Confidence), an on-the-fly verbalized confidence estimation method for long-form generation. Specifically, we use reinforcement learning (RL) to train LLMs to append numerical confidence scores to each generated statement, serving as a direct and interpretable signal of the factuality of generation. Our experiments consider both on-policy and off-policy RL methods, including DPO, ORPO, and GRPO, to enhance the model calibration. We introduce two novel evaluation settings, free-form tagging and iterative tagging, to assess different verbalized confidence estimation methods. Experiments on three long-form QA datasets show that our RL-trained models achieve better calibration and generalize robustly across domains. Also, our method is highly efficient, as it only requires adding a few tokens to the output being decoded.</li>
</ul>

<h3>Title: Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling</h3>
<ul>
<li><strong>Authors: </strong>Gustavo Sutter Pessurno de Carvalho, Mohammed Abdulrahman, Hao Wang, Sriram Ganapathi Subramanian, Marc St-Aubin, Sharon O'Sullivan, Lawrence Wan, Luis Ricardez-Sandoval, Pascal Poupart, Agustinus Kristiadi</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23913">https://arxiv.org/abs/2505.23913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23913">https://arxiv.org/pdf/2505.23913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23913]] Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling(https://arxiv.org/abs/2505.23913)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The optimization of expensive black-box functions is ubiquitous in science and engineering. A common solution to this problem is Bayesian optimization (BO), which is generally comprised of two components: (i) a surrogate model and (ii) an acquisition function, which generally require expensive re-training and optimization steps at each iteration, respectively. Although recent work enabled in-context surrogate models that do not require re-training, virtually all existing BO methods still require acquisition function maximization to select the next observation, which introduces many knobs to tune, such as Monte Carlo samplers and multi-start optimizers. In this work, we propose a completely in-context, zero-shot solution for BO that does not require surrogate fitting or acquisition function optimization. This is done by using a pre-trained deep generative model to directly sample from the posterior over the optimum point. We show that this process is equivalent to Thompson sampling and demonstrate the capabilities and cost-effectiveness of our foundation model on a suite of real-world benchmarks. We achieve an efficiency gain of more than 35x in terms of wall-clock time when compared with Gaussian process-based BO, enabling efficient parallel and distributed BO, e.g., for high-throughput optimization.</li>
</ul>

<h3>Title: Probing Association Biases in LLM Moderation Over-Sensitivity</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Wang, Botao Yu, Ivory Yang, Saeed Hassanpour, Soroush Vosoughi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23914">https://arxiv.org/abs/2505.23914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23914">https://arxiv.org/pdf/2505.23914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23914]] Probing Association Biases in LLM Moderation Over-Sensitivity(https://arxiv.org/abs/2505.23914)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models are widely used for content moderation but often misclassify benign comments as toxic, leading to over-sensitivity. While previous research attributes this issue primarily to the presence of offensive terms, we reveal a potential cause beyond token level: LLMs exhibit systematic topic biases in their implicit associations. Inspired by cognitive psychology's implicit association tests, we introduce Topic Association Analysis, a semantic-level approach to quantify how LLMs associate certain topics with toxicity. By prompting LLMs to generate free-form scenario imagination for misclassified benign comments and analyzing their topic amplification levels, we find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger topic stereotype despite lower overall false positive rates. These biases suggest that LLMs do not merely react to explicit, offensive language but rely on learned topic associations, shaping their moderation decisions. Our findings highlight the need for refinement beyond keyword-based filtering, providing insights into the underlying mechanisms driving LLM over-sensitivity.</li>
</ul>

<h3>Title: Point-MoE: Towards Cross-Domain Generalization in 3D Semantic Segmentation via Mixture-of-Experts</h3>
<ul>
<li><strong>Authors: </strong>Xuweiyi Chen, Wentao Zhou, Aruni RoyChowdhury, Zezhou Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23926">https://arxiv.org/abs/2505.23926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23926">https://arxiv.org/pdf/2505.23926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23926]] Point-MoE: Towards Cross-Domain Generalization in 3D Semantic Segmentation via Mixture-of-Experts(https://arxiv.org/abs/2505.23926)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>While scaling laws have transformed natural language processing and computer vision, 3D point cloud understanding has yet to reach that stage. This can be attributed to both the comparatively smaller scale of 3D datasets, as well as the disparate sources of the data itself. Point clouds are captured by diverse sensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor, outdoor), each introducing unique scanning patterns, sampling densities, and semantic biases. Such domain heterogeneity poses a major barrier towards training unified models at scale, especially under the realistic constraint that domain labels are typically inaccessible at inference time. In this work, we propose Point-MoE, a Mixture-of-Experts architecture designed to enable large-scale, cross-domain generalization in 3D perception. We show that standard point cloud backbones degrade significantly in performance when trained on mixed-domain data, whereas Point-MoE with a simple top-k routing strategy can automatically specialize experts, even without access to domain labels. Our experiments demonstrate that Point-MoE not only outperforms strong multi-domain baselines but also generalizes better to unseen domains. This work highlights a scalable path forward for 3D understanding: letting the model discover structure in diverse 3D data, rather than imposing it via manual curation or domain supervision.</li>
</ul>

<h3>Title: Thompson Sampling in Online RLHF with General Function Approximation</h3>
<ul>
<li><strong>Authors: </strong>Songtao Feng, Jie Fu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23927">https://arxiv.org/abs/2505.23927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23927">https://arxiv.org/pdf/2505.23927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23927]] Thompson Sampling in Online RLHF with General Function Approximation(https://arxiv.org/abs/2505.23927)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) has achieved great empirical success in aligning large language models (LLMs) with human preference, and it is of great importance to study the statistical efficiency of RLHF algorithms from a theoretical perspective. In this work, we consider the online RLHF setting where the preference data is revealed during the learning process and study action value function approximation. We design a model-free posterior sampling algorithm for online RLHF inspired by Thompson sampling and provide its theoretical guarantee. Specifically, we adopt Bellman eluder (BE) dimension as the complexity measure of the function class and establish $O(\sqrt{T})$ regret bound for the proposed algorithm with other multiplicative factor depending on the horizon, BE dimension and the $log$-bracketing number of the function class. Further, in the analysis, we first establish the concentration-type inequality of the squared Bellman error bound based on the maximum likelihood estimator (MLE) generalization bound, which plays the crucial rules in obtaining the eluder-type regret bound and may be of independent interest.</li>
</ul>

<h3>Title: SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving</h3>
<ul>
<li><strong>Authors: </strong>Wendong Xu, Jing Xiong, Chenyang Zhao, Qiujiang Chen, Haoran Wang, Hui Shen, Zhongwei Wan, Jianbo Dai, Taiqiang Wu, He Xiao, Chaofan Tao, Z. Morley Mao, Ying Sheng, Zhijiang Guo, Hongxia Yang, Bei Yu, Lingpeng Kong, Quanquan Gu, Ngai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23932">https://arxiv.org/abs/2505.23932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23932">https://arxiv.org/pdf/2505.23932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23932]] SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving(https://arxiv.org/abs/2505.23932)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present SwingArena, a competitive evaluation framework for Large Language Models (LLMs) that closely mirrors real-world software development workflows. Unlike traditional static benchmarks, SwingArena models the collaborative process of software iteration by pairing LLMs as submitters, who generate patches, and reviewers, who create test cases and verify the patches through continuous integration (CI) pipelines. To support these interactive evaluations, we introduce a retrieval-augmented code generation (RACG) module that efficiently handles long-context challenges by providing syntactically and semantically relevant code snippets from large codebases, supporting multiple programming languages (C++, Python, Rust, and Go). This enables the framework to scale across diverse tasks and contexts while respecting token limitations. Our experiments, using over 400 high-quality real-world GitHub issues selected from a pool of 2,300 issues, show that models like GPT-4o excel at aggressive patch generation, whereas DeepSeek and Gemini prioritize correctness in CI validation. SwingArena presents a scalable and extensible methodology for evaluating LLMs in realistic, CI-driven software development settings. More details are available on our project page: this http URL</li>
</ul>

<h3>Title: BIRD: Behavior Induction via Representation-structure Distillation</h3>
<ul>
<li><strong>Authors: </strong>Galen Pogoncheff, Michael Beyeler</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23933">https://arxiv.org/abs/2505.23933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23933">https://arxiv.org/pdf/2505.23933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23933]] BIRD: Behavior Induction via Representation-structure Distillation(https://arxiv.org/abs/2505.23933)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Human-aligned deep learning models exhibit behaviors consistent with human values, such as robustness, fairness, and honesty. Transferring these behavioral properties to models trained on different tasks or data distributions remains challenging: aligned behavior is easily forgotten during fine-tuning, and collecting task-specific data that preserves this behavior can be prohibitively costly. We introduce BIRD (Behavior Induction via Representation-structure Distillation), a flexible framework for transferring aligned behavior by matching the internal representation structure of a student model to that of a teacher. Applied to out-of-distribution robustness in image classification, BIRD outperforms fine-tuning, transfer learning, and continual learning methods, improving robust accuracy by up to 16% over the next strongest baseline. It remains effective even when the teacher is trained on a much simpler dataset and is $25 \times$ smaller than the student. In a large-scale study of over 400 teacher-student pairs, we show that three interpretable and computable properties of the teacher's representations (i.e., task relevance, behavioral relevance, and complementary knowledge) explain up to 85% of the variance in transfer success. These insights offer practical guidance for teacher selection and design. BIRD turns small, well-aligned models into scalable alignment seeds, removing a key bottleneck in deploying safe AI systems in the wild.</li>
</ul>

<h3>Title: Searching Neural Architectures for Sensor Nodes on IoT Gateways</h3>
<ul>
<li><strong>Authors: </strong>Andrea Mattia Garavagno, Edoardo Ragusa, Antonio Frisoli, Paolo Gastaldo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23939">https://arxiv.org/abs/2505.23939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23939">https://arxiv.org/pdf/2505.23939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23939]] Searching Neural Architectures for Sensor Nodes on IoT Gateways(https://arxiv.org/abs/2505.23939)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>This paper presents an automatic method for the design of Neural Networks (NNs) at the edge, enabling Machine Learning (ML) access even in privacy-sensitive Internet of Things (IoT) applications. The proposed method runs on IoT gateways and designs NNs for connected sensor nodes without sharing the collected data outside the local network, keeping the data in the site of collection. This approach has the potential to enable ML for Healthcare Internet of Things (HIoT) and Industrial Internet of Things (IIoT), designing hardware-friendly and custom NNs at the edge for personalized healthcare and advanced industrial services such as quality control, predictive maintenance, or fault diagnosis. By preventing data from being disclosed to cloud services, this method safeguards sensitive information, including industrial secrets and personal data. The outcomes of a thorough experimental session confirm that -- on the Visual Wake Words dataset -- the proposed approach can achieve state-of-the-art results by exploiting a search procedure that runs in less than 10 hours on the Raspberry Pi Zero 2.</li>
</ul>

<h3>Title: Vision Language Models are Biased</h3>
<ul>
<li><strong>Authors: </strong>An Vo, Khai-Nguyen Nguyen, Mohammad Reza Taesiri, Vy Tuong Dang, Anh Totti Nguyen, Daeyoung Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23941">https://arxiv.org/abs/2505.23941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23941">https://arxiv.org/pdf/2505.23941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23941]] Vision Language Models are Biased(https://arxiv.org/abs/2505.23941)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) memorize a vast amount of prior knowledge from the Internet that help them on downstream tasks but also may notoriously sway their outputs towards wrong or biased answers. In this work, we test how the knowledge about popular subjects hurt the accuracy of vision language models (VLMs) on standard, objective visual tasks of counting and identification. We find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of 17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo) across 7 diverse domains from animals, logos, chess, board games, optical illusions, to patterned grids. Insert text (e.g., "Adidas") describing the subject name into the counterfactual image further decreases VLM accuracy. The biases in VLMs are so strong that instructing them to double-check their results or rely exclusively on image details to answer improves counting accuracy by only +2 points, on average. Our work presents an interesting failure mode in VLMs and an automated framework for testing VLM biases. Code and data are available at: this http URL.</li>
</ul>

<h3>Title: SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Sarkar, Jay Gala, Subarna Tripathi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23942">https://arxiv.org/abs/2505.23942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23942">https://arxiv.org/pdf/2505.23942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23942]] SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations(https://arxiv.org/abs/2505.23942)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The design of activation functions remains a pivotal component in optimizing deep neural networks. While prevailing choices like Swish and GELU demonstrate considerable efficacy, they often exhibit domain-specific optima. This work introduces SG-Blend, a novel activation function that blends our proposed SSwish, a first-order symmetric variant of Swish and the established GELU through dynamic interpolation. By adaptively blending these constituent functions via learnable parameters, SG-Blend aims to harness their complementary strengths: SSwish's controlled non-monotonicity and symmetry, and GELU's smooth, probabilistic profile, to achieve a more universally robust balance between model expressivity and gradient stability. We conduct comprehensive empirical evaluations across diverse modalities and architectures, showing performance improvements across all considered natural language and computer vision tasks and models. These results, achieved with negligible computational overhead, underscore SG-Blend's potential as a versatile, drop-in replacement that consistently outperforms strong contemporary baselines. The code is available at this https URL.</li>
</ul>

<h3>Title: Retrieval Augmented Generation based Large Language Models for Causality Mining</h3>
<ul>
<li><strong>Authors: </strong>Thushara Manjari Naduvilakandy, Hyeju Jang, Mohammad Al Hasan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23944">https://arxiv.org/abs/2505.23944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23944">https://arxiv.org/pdf/2505.23944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23944]] Retrieval Augmented Generation based Large Language Models for Causality Mining(https://arxiv.org/abs/2505.23944)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Causality detection and mining are important tasks in information retrieval due to their enormous use in information extraction, and knowledge graph construction. To solve these tasks, in existing literature there exist several solutions -- both unsupervised and supervised. However, the unsupervised methods suffer from poor performance and they often require significant human intervention for causal rule selection, leading to poor generalization across different domains. On the other hand, supervised methods suffer from the lack of large training datasets. Recently, large language models (LLMs) with effective prompt engineering are found to be effective to overcome the issue of unavailability of large training dataset. Yet, in existing literature, there does not exist comprehensive works on causality detection and mining using LLM prompting. In this paper, we present several retrieval-augmented generation (RAG) based dynamic prompting schemes to enhance LLM performance in causality detection and extraction tasks. Extensive experiments over three datasets and five LLMs validate the superiority of our proposed RAG-based dynamic prompting over other static prompting schemes.</li>
</ul>

<h3>Title: A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sriram Balasubramanian, Samyadeep Basu, Soheil Feizi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23945">https://arxiv.org/abs/2505.23945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23945">https://arxiv.org/pdf/2505.23945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23945]] A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models(https://arxiv.org/abs/2505.23945)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-thought (CoT) reasoning enhances performance of large language models, but questions remain about whether these reasoning traces faithfully reflect the internal processes of the model. We present the first comprehensive study of CoT faithfulness in large vision-language models (LVLMs), investigating how both text-based and previously unexplored image-based biases affect reasoning and bias articulation. Our work introduces a novel, fine-grained evaluation pipeline for categorizing bias articulation patterns, enabling significantly more precise analysis of CoT reasoning than previous methods. This framework reveals critical distinctions in how models process and respond to different types of biases, providing new insights into LVLM CoT faithfulness. Our findings reveal that subtle image-based biases are rarely articulated compared to explicit text-based ones, even in models specialized for reasoning. Additionally, many models exhibit a previously unidentified phenomenon we term ``inconsistent'' reasoning - correctly reasoning before abruptly changing answers, serving as a potential canary for detecting biased reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to revisit CoT faithfulness in LLMs across various levels of implicit cues. Our findings reveal that current language-only reasoning models continue to struggle with articulating cues that are not overtly stated.</li>
</ul>

<h3>Title: Estimating Misreporting in the Presence of Genuine Modification: A Causal Perspective</h3>
<ul>
<li><strong>Authors: </strong>Dylan Zapzalka, Trenton Chang, Lindsay Warrenburg, Sae-Hwan Park, Daniel K. Shenfeld, Ravi B. Parikh, Jenna Wiens, Maggie Makar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23954">https://arxiv.org/abs/2505.23954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23954">https://arxiv.org/pdf/2505.23954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23954]] Estimating Misreporting in the Presence of Genuine Modification: A Causal Perspective(https://arxiv.org/abs/2505.23954)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>In settings where ML models are used to inform the allocation of resources, agents affected by the allocation decisions might have an incentive to strategically change their features to secure better outcomes. While prior work has studied strategic responses broadly, disentangling misreporting from genuine modification remains a fundamental challenge. In this paper, we propose a causally-motivated approach to identify and quantify how much an agent misreports on average by distinguishing deceptive changes in their features from genuine modification. Our key insight is that, unlike genuine modification, misreported features do not causally affect downstream variables (i.e., causal descendants). We exploit this asymmetry by comparing the causal effect of misreported features on their causal descendants as derived from manipulated datasets against those from unmanipulated datasets. We formally prove identifiability of the misreporting rate and characterize the variance of our estimator. We empirically validate our theoretical results using a semi-synthetic and real Medicare dataset with misreported data, demonstrating that our approach can be employed to identify misreporting in real-world scenarios.</li>
</ul>

<h3>Title: Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation</h3>
<ul>
<li><strong>Authors: </strong>Henry Conklin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23960">https://arxiv.org/abs/2505.23960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23960">https://arxiv.org/pdf/2505.23960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23960]] Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation(https://arxiv.org/abs/2505.23960)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of large large-scale neural networks, we still lack unified notation for thinking about and describing their representational spaces. We lack methods to reliably describe how their representations are structured, how that structure emerges over training, and what kinds of structures are desirable. This thesis introduces quantitative methods for identifying systematic structure in a mapping between spaces, and leverages them to understand how deep-learning models learn to represent information, what representational structures drive generalisation, and how design decisions condition the structures that emerge. To do this I identify structural primitives present in a mapping, along with information theoretic quantifications of each. These allow us to analyse learning, structure, and generalisation across multi-agent reinforcement learning models, sequence-to-sequence models trained on a single task, and Large Language Models. I also introduce a novel, performant, approach to estimating the entropy of vector space, that allows this analysis to be applied to models ranging in size from 1 million to 12 billion parameters. The experiments here work to shed light on how large-scale distributed models of cognition learn, while allowing us to draw parallels between those systems and their human analogs. They show how the structures of language and the constraints that give rise to them in many ways parallel the kinds of structures that drive performance of contemporary neural networks.</li>
</ul>

<h3>Title: MangoLeafViT: Leveraging Lightweight Vision Transformer with Runtime Augmentation for Efficient Mango Leaf Disease Classification</h3>
<ul>
<li><strong>Authors: </strong>Rafi Hassan Chowdhury, Sabbir Ahmed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23961">https://arxiv.org/abs/2505.23961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23961">https://arxiv.org/pdf/2505.23961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23961]] MangoLeafViT: Leveraging Lightweight Vision Transformer with Runtime Augmentation for Efficient Mango Leaf Disease Classification(https://arxiv.org/abs/2505.23961)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Ensuring food safety is critical due to its profound impact on public health, economic stability, and global supply chains. Cultivation of Mango, a major agricultural product in several South Asian countries, faces high financial losses due to different diseases, affecting various aspects of the entire supply chain. While deep learning-based methods have been explored for mango leaf disease classification, there remains a gap in designing solutions that are computationally efficient and compatible with low-end devices. In this work, we propose a lightweight Vision Transformer-based pipeline with a self-attention mechanism to classify mango leaf diseases, achieving state-of-the-art performance with minimal computational overhead. Our approach leverages global attention to capture intricate patterns among disease types and incorporates runtime augmentation for enhanced performance. Evaluation on the MangoLeafBD dataset demonstrates a 99.43% accuracy, outperforming existing methods in terms of model size, parameter count, and FLOPs count.</li>
</ul>

<h3>Title: FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Tian, Ryan Solgi, Jinming Lu, Yifan Yang, Hai Li, Zheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23966">https://arxiv.org/abs/2505.23966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23966">https://arxiv.org/pdf/2505.23966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23966]] FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression(https://arxiv.org/abs/2505.23966)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have enabled remarkable progress in natural language processing, yet their high computational and memory demands pose challenges for deployment in resource-constrained environments. Although recent low-rank decomposition methods offer a promising path for structural compression, they often suffer from accuracy degradation, expensive calibration procedures, and result in inefficient model architectures that hinder real-world inference speedups. In this paper, we propose FLAT-LLM, a fast and accurate, training-free structural compression method based on fine-grained low-rank transformations in the activation space. Specifically, we reduce the hidden dimension by transforming the weights using truncated eigenvectors computed via head-wise Principal Component Analysis (PCA), and employ an importance-based metric to adaptively allocate ranks across decoders. FLAT-LLM achieves efficient and effective weight compression without recovery fine-tuning, which could complete the calibration within a few minutes. Evaluated across 4 models and 11 datasets, FLAT-LLM outperforms structural pruning baselines in generalization and downstream performance, while delivering inference speedups over decomposition-based methods.</li>
</ul>

<h3>Title: Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention</h3>
<ul>
<li><strong>Authors: </strong>Stephan Rabanser, Ali Shahin Shamsabadi, Olive Franzese, Xiao Wang, Adrian Weller, Nicolas Papernot</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23968">https://arxiv.org/abs/2505.23968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23968">https://arxiv.org/pdf/2505.23968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23968]] Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention(https://arxiv.org/abs/2505.23968)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>Cautious predictions -- where a machine learning model abstains when uncertain -- are crucial for limiting harmful errors in safety-critical applications. In this work, we identify a novel threat: a dishonest institution can exploit these mechanisms to discriminate or unjustly deny services under the guise of uncertainty. We demonstrate the practicality of this threat by introducing an uncertainty-inducing attack called Mirage, which deliberately reduces confidence in targeted input regions, thereby covertly disadvantaging specific individuals. At the same time, Mirage maintains high predictive performance across all data points. To counter this threat, we propose Confidential Guardian, a framework that analyzes calibration metrics on a reference dataset to detect artificially suppressed confidence. Additionally, it employs zero-knowledge proofs of verified inference to ensure that reported confidence scores genuinely originate from the deployed model. This prevents the provider from fabricating arbitrary model confidence values while protecting the model's proprietary details. Our results confirm that Confidential Guardian effectively prevents the misuse of cautious predictions, providing verifiable assurances that abstention reflects genuine model uncertainty rather than malicious intent.</li>
</ul>

<h3>Title: Adaptive Deadline and Batch Layered Synchronized Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Asaf Goren, Natalie Lang, Nir Shlezinger, Alejandro Cohen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23973">https://arxiv.org/abs/2505.23973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23973">https://arxiv.org/pdf/2505.23973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23973]] Adaptive Deadline and Batch Layered Synchronized Federated Learning(https://arxiv.org/abs/2505.23973)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) enables collaborative model training across distributed edge devices while preserving data privacy, and typically operates in a round-based synchronous manner. However, synchronous FL suffers from latency bottlenecks due to device heterogeneity, where slower clients (stragglers) delay or degrade global updates. Prior solutions, such as fixed deadlines, client selection, and layer-wise partial aggregation, alleviate the effect of stragglers, but treat round timing and local workload as static parameters, limiting their effectiveness under strict time constraints. We propose ADEL-FL, a novel framework that jointly optimizes per-round deadlines and user-specific batch sizes for layer-wise aggregation. Our approach formulates a constrained optimization problem minimizing the expected L2 distance to the global optimum under total training time and global rounds. We provide a convergence analysis under exponential compute models and prove that ADEL-FL yields unbiased updates with bounded variance. Extensive experiments demonstrate that ADEL-FL outperforms alternative methods in both convergence rate and final accuracy under heterogeneous conditions.</li>
</ul>

<h3>Title: DeepTopoNet: A Framework for Subglacial Topography Estimation on the Greenland Ice Sheets</h3>
<ul>
<li><strong>Authors: </strong>Bayu Adhi Tama, Mansa Krishna, Homayra Alam, Mostafa Cham, Omar Faruque, Gong Cheng, Jianwu Wang, Mathieu Morlighem, Vandana Janeja</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23980">https://arxiv.org/abs/2505.23980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23980">https://arxiv.org/pdf/2505.23980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23980]] DeepTopoNet: A Framework for Subglacial Topography Estimation on the Greenland Ice Sheets(https://arxiv.org/abs/2505.23980)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Understanding Greenland's subglacial topography is critical for projecting the future mass loss of the ice sheet and its contribution to global sea-level rise. However, the complex and sparse nature of observational data, particularly information about the bed topography under the ice sheet, significantly increases the uncertainty in model projections. Bed topography is traditionally measured by airborne ice-penetrating radar that measures the ice thickness directly underneath the aircraft, leaving data gap of tens of kilometers in between flight lines. This study introduces a deep learning framework, which we call as DeepTopoNet, that integrates radar-derived ice thickness observations and BedMachine Greenland data through a novel dynamic loss-balancing mechanism. Among all efforts to reconstruct bed topography, BedMachine has emerged as one of the most widely used datasets, combining mass conservation principles and ice thickness measurements to generate high-resolution bed elevation estimates. The proposed loss function adaptively adjusts the weighting between radar and BedMachine data, ensuring robustness in areas with limited radar coverage while leveraging the high spatial resolution of BedMachine predictions i.e. bed estimates. Our approach incorporates gradient-based and trend surface features to enhance model performance and utilizes a CNN architecture designed for subgrid-scale predictions. By systematically testing on the Upernavik IsstrÃ¸m) region, the model achieves high accuracy, outperforming baseline methods in reconstructing subglacial terrain. This work demonstrates the potential of deep learning in bridging observational gaps, providing a scalable and efficient solution to inferring subglacial topography.</li>
</ul>

<h3>Title: Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization</h3>
<ul>
<li><strong>Authors: </strong>Vishal Dey, Xiao Hu, Xia Ning</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23987">https://arxiv.org/abs/2505.23987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23987">https://arxiv.org/pdf/2505.23987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23987]] Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization(https://arxiv.org/abs/2505.23987)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In real-world drug design, molecule optimization requires selectively improving multiple molecular properties up to pharmaceutically relevant levels, while maintaining others that already meet such criteria. However, existing computational approaches and instruction-tuned LLMs fail to capture such nuanced property-specific objectives, limiting their practical applicability. To address this, we introduce C-MuMOInstruct, the first instruction-tuning dataset focused on multi-property optimization with explicit, property-specific objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of instruction-tuned LLMs that can perform targeted property-specific optimization. Our experiments across 5 in-distribution and 5 out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit impressive 0-shot generalization to novel optimization tasks and unseen instructions. This offers a step toward a foundational LLM to support realistic, diverse optimizations with property-specific objectives. C-MuMOInstruct and code are accessible through this https URL.</li>
</ul>

<h3>Title: Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yinong Oliver Wang, Nivedha Sivakumar, Falaah Arif Khan, Rin Metcalf Susa, Adam Golinski, Natalie Mackraz, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23996">https://arxiv.org/abs/2505.23996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23996">https://arxiv.org/pdf/2505.23996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23996]] Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs(https://arxiv.org/abs/2505.23996)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The recent rapid adoption of large language models (LLMs) highlights the critical need for benchmarking their fairness. Conventional fairness metrics, which focus on discrete accuracy-based evaluations (i.e., prediction correctness), fail to capture the implicit impact of model uncertainty (e.g., higher model confidence about one group over another despite similar accuracy). To address this limitation, we propose an uncertainty-aware fairness metric, UCerF, to enable a fine-grained evaluation of model fairness that is more reflective of the internal bias in model decisions compared to conventional fairness measures. Furthermore, observing data size, diversity, and clarity issues in current datasets, we introduce a new gender-occupation fairness evaluation dataset with 31,756 samples for co-reference resolution, offering a more diverse and suitable dataset for evaluating modern LLMs. We establish a benchmark, using our metric and dataset, and apply it to evaluate the behavior of ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness due to high confidence in incorrect predictions, a detail overlooked by Equalized Odds but captured by UCerF. Overall, our proposed LLM benchmark, which evaluates fairness with uncertainty awareness, paves the way for developing more transparent and accountable AI systems.</li>
</ul>

<h3>Title: DGIQA: Depth-guided Feature Attention and Refinement for Generalizable Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Vaishnav Ramesh, Junliang Liu, Haining Wang, Md Jahidul Islam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24002">https://arxiv.org/abs/2505.24002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24002">https://arxiv.org/pdf/2505.24002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24002]] DGIQA: Depth-guided Feature Attention and Refinement for Generalizable Image Quality Assessment(https://arxiv.org/abs/2505.24002)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>A long-held challenge in no-reference image quality assessment (NR-IQA) learning from human subjective perception is the lack of objective generalization to unseen natural distortions. To address this, we integrate a novel Depth-Guided cross-attention and refinement (Depth-CAR) mechanism, which distills scene depth and spatial features into a structure-aware representation for improved NR-IQA. This brings in the knowledge of object saliency and relative contrast of the scene for more discriminative feature learning. Additionally, we introduce the idea of TCB (Transformer-CNN Bridge) to fuse high-level global contextual dependencies from a transformer backbone with local spatial features captured by a set of hierarchical CNN (convolutional neural network) layers. We implement TCB and Depth-CAR as multimodal attention-based projection functions to select the most informative features, which also improve training time and inference efficiency. Experimental results demonstrate that our proposed DGIQA model achieves state-of-the-art (SOTA) performance on both synthetic and authentic benchmark datasets. More importantly, DGIQA outperforms SOTA models on cross-dataset evaluations as well as in assessing natural image distortions such as low-light effects, hazy conditions, and lens flares.</li>
</ul>

<h3>Title: Preemptive Hallucination Reduction: An Input-Level Approach for Multimodal Language Model</h3>
<ul>
<li><strong>Authors: </strong>Nokimul Hasan Arif, Shadman Rabby, Md Hefzul Hossain Papon, Sabbir Ahmed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24007">https://arxiv.org/abs/2505.24007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24007">https://arxiv.org/pdf/2505.24007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24007]] Preemptive Hallucination Reduction: An Input-Level Approach for Multimodal Language Model(https://arxiv.org/abs/2505.24007)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Visual hallucinations in Large Language Models (LLMs), where the model generates responses that are inconsistent with the visual input, pose a significant challenge to their reliability, particularly in contexts where precise and trustworthy outputs are critical. Current research largely emphasizes post-hoc correction or model-specific fine-tuning strategies, with limited exploration of preprocessing techniques to address hallucination issues at the input stage. This study presents a novel ensemble-based preprocessing framework that adaptively selects the most appropriate filtering approach -- noise reduced (NR), edge enhanced (EE), or unaltered input (org) based on the type of question posed, resulting into reduced hallucination without requiring any modifications to the underlying model architecture or training pipeline. Evaluated on the `HaloQuest' dataset -- a benchmark designed to test multimodal reasoning on visually complex inputs, our method achieves a 44.3% reduction in hallucination rates, as measured by Natural Language Inference (NLI) scores using SelfCheckGPT. This demonstrates that intelligent input conditioning alone can significantly enhance factual grounding in LLM responses. The findings highlight the importance of adaptive preprocessing techniques in mitigating hallucinations, paving the way for more reliable multimodal systems capable of addressing real-world challenges.</li>
</ul>

<h3>Title: HoneySat: A Network-based Satellite Honeypot Framework</h3>
<ul>
<li><strong>Authors: </strong>EfrÃ©n LÃ³pez-Morales (Texas A&amp;M University-Corpus Christi), Ulysse Planta (CISPA Helmholtz Center for Information Security), Gabriele Marra (CISPA Helmholtz Center for Information Security), Carlos GonzÃ¡lez (German Aerospace Center (DLR) and Departamento de IngenierÃ­a InformÃ¡tica, Universidad de Santiago de Chile), Jacob Hopkins (Texas A&amp;M University-Corpus Christi), Majid Garoosi (CISPA Helmholtz Center for Information Security), ElÃ­as Obreque (Universidad de Chile), Carlos Rubio-Medrano (Texas A&amp;M University-Corpus Christi), Ali Abbasi (CISPA Helmholtz Center for Information Security)</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24008">https://arxiv.org/abs/2505.24008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24008">https://arxiv.org/pdf/2505.24008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24008]] HoneySat: A Network-based Satellite Honeypot Framework(https://arxiv.org/abs/2505.24008)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Satellites are the backbone of several mission-critical services, such as GPS that enable our modern society to function. For many years, satellites were assumed to be secure because of their indecipherable architectures and the reliance on security by obscurity. However, technological advancements have made these assumptions obsolete, paving the way for potential attacks, and sparking a renewed interest in satellite security. Unfortunately, to this day, there is no efficient way to collect data on adversarial techniques for satellites, which severely hurts the generation of security intelligence. In this paper, we present HoneySat, the first high-interaction satellite honeypot framework, which is fully capable of convincingly simulating a real-world CubeSat, a type of Small Satellite (SmallSat) widely used in practice. To provide evidence of the effectiveness of HoneySat, we surveyed experienced SmallSat operators currently in charge of active in-orbit satellite missions. Results revealed that the majority of satellite operators (71.4%) agreed that HoneySat provides realistic and engaging simulations of CubeSat missions. Further experimental evaluations also showed that HoneySat provides adversaries with extensive interaction opportunities by supporting the majority of adversarial techniques (86.8%) and tactics (100%) that target satellites. Additionally, we also obtained a series of real interactions from actual adversaries by deploying HoneySat on the internet over several months, confirming that HoneySat can operate covertly and efficiently while collecting highly valuable interaction data.</li>
</ul>

<h3>Title: Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws</h3>
<ul>
<li><strong>Authors: </strong>Hidetaka Kamigaito, Ying Zhang, Jingun Kwon, Katsuhiko Hayashi, Manabu Okumura, Taro Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24009">https://arxiv.org/abs/2505.24009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24009">https://arxiv.org/pdf/2505.24009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24009]] Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws(https://arxiv.org/abs/2505.24009)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformers deliver outstanding performance across a wide range of tasks and are now a dominant backbone architecture for large language models (LLMs). Their task-solving performance is improved by increasing parameter size, as shown in the recent studies on parameter scaling laws. Although recent mechanistic-interpretability studies have deepened our understanding of the internal behavior of Transformers by analyzing their residual stream, the relationship between these internal mechanisms and the parameter scaling laws remains unclear. To bridge this gap, we focus on layers and their size, which mainly decide the parameter size of Transformers. For this purpose, we first theoretically investigate the layers within the residual stream through a bias-diversity decomposition. The decomposition separates (i) bias, the error of each layer's output from the ground truth, and (ii) diversity, which indicates how much the outputs of each layer differ from each other. Analyzing Transformers under this theory reveals that performance improves when individual layers make predictions close to the correct answer and remain mutually diverse. We show that diversity becomes especially critical when individual layers' outputs are far from the ground truth. Finally, we introduce an information-theoretic diversity and show our main findings that adding layers enhances performance only when those layers behave differently, i.e., are diverse. We also reveal the performance gains from increasing the number of layers exhibit submodularity: marginal improvements diminish as additional layers increase, mirroring the logarithmic convergence predicted by the parameter scaling laws. Experiments on multiple semantic-understanding tasks with various LLMs empirically confirm the theoretical properties derived in this study.</li>
</ul>

<h3>Title: Large Language Model Meets Constraint Propagation</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Bonlarron, Florian RÃ©gin, Elisabetta De Maria, Jean-Charles RÃ©gin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24012">https://arxiv.org/abs/2505.24012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24012">https://arxiv.org/pdf/2505.24012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24012]] Large Language Model Meets Constraint Propagation(https://arxiv.org/abs/2505.24012)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel at generating fluent text but struggle to enforce external constraints because they generate tokens sequentially without explicit control mechanisms. GenCP addresses this limitation by combining LLM predictions with Constraint Programming (CP) reasoning, formulating text generation as a Constraint Satisfaction Problem (CSP). In this paper, we improve GenCP by integrating Masked Language Models (MLMs) for domain generation, which allows bidirectional constraint propagation that leverages both past and future tokens. This integration bridges the gap between token-level prediction and structured constraint enforcement, leading to more reliable and constraint-aware text generation. Our evaluation on COLLIE benchmarks demonstrates that incorporating domain preview via MLM calls significantly improves GenCP's performance. Although this approach incurs additional MLM calls and, in some cases, increased backtracking, the overall effect is a more efficient use of LLM inferences and an enhanced ability to generate feasible and meaningful solutions, particularly in tasks with strict content constraints.</li>
</ul>

<h3>Title: LLM Agents Should Employ Security Principles</h3>
<ul>
<li><strong>Authors: </strong>Kaiyuan Zhang, Zian Su, Pin-Yu Chen, Elisa Bertino, Xiangyu Zhang, Ninghui Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24019">https://arxiv.org/abs/2505.24019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24019">https://arxiv.org/pdf/2505.24019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24019]] LLM Agents Should Employ Security Principles(https://arxiv.org/abs/2505.24019)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) agents show considerable promise for automating complex tasks using contextual reasoning; however, interactions involving multiple agents and the system's susceptibility to prompt injection and other forms of context manipulation introduce new vulnerabilities related to privacy leakage and system exploitation. This position paper argues that the well-established design principles in information security, which are commonly referred to as security principles, should be employed when deploying LLM agents at scale. Design principles such as defense-in-depth, least privilege, complete mediation, and psychological acceptability have helped guide the design of mechanisms for securing information systems over the last five decades, and we argue that their explicit and conscientious adoption will help secure agentic systems. To illustrate this approach, we introduce AgentSandbox, a conceptual framework embedding these security principles to provide safeguards throughout an agent's life-cycle. We evaluate with state-of-the-art LLMs along three dimensions: benign utility, attack utility, and attack success rate. AgentSandbox maintains high utility for its intended functions under both benign and adversarial evaluations while substantially mitigating privacy risks. By embedding secure design principles as foundational elements within emerging LLM agent protocols, we aim to promote trustworthy agent ecosystems aligned with user privacy expectations and evolving regulatory requirements.</li>
</ul>

<h3>Title: An Advanced Cyber-Physical System Security Testbed for Substation Automation</h3>
<ul>
<li><strong>Authors: </strong>Akila Herath, Chen-Ching Liu, Junho Hong, Mansi Girdhar</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24021">https://arxiv.org/abs/2505.24021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24021">https://arxiv.org/pdf/2505.24021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24021]] An Advanced Cyber-Physical System Security Testbed for Substation Automation(https://arxiv.org/abs/2505.24021)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>A Cyber-Physical System (CPS) testbed serves as a powerful platform for testing and validating cyber intrusion detection and mitigation strategies in substations. This study presents the design and development of a CPS testbed that can effectively assess the real-time dynamics of a substation. Cyber attacks exploiting IEC 61850-based SV and GOOSE protocols are demonstrated using the testbed, along with an analysis on attack detection. Realistic timing measurements are obtained, and the time frames for deploying detection and mitigation strategies are evaluated.</li>
</ul>

<h3>Title: Multi-Group Proportional Representation for Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Sangwon Jung, Alex Oesterling, Claudio Mayrink Verdun, Sajani Vithana, Taesup Moon, Flavio P. Calmon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24023">https://arxiv.org/abs/2505.24023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24023">https://arxiv.org/pdf/2505.24023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24023]] Multi-Group Proportional Representation for Text-to-Image Models(https://arxiv.org/abs/2505.24023)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) generative models can create vivid, realistic images from textual descriptions. As these models proliferate, they expose new concerns about their ability to represent diverse demographic groups, propagate stereotypes, and efface minority populations. Despite growing attention to the "safe" and "responsible" design of artificial intelligence (AI), there is no established methodology to systematically measure and control representational harms in image generation. This paper introduces a novel framework to measure the representation of intersectional groups in images generated by T2I models by applying the Multi-Group Proportional Representation (MPR) metric. MPR evaluates the worst-case deviation of representation statistics across given population groups in images produced by a generative model, allowing for flexible and context-specific measurements based on user requirements. We also develop an algorithm to optimize T2I models for this metric. Through experiments, we demonstrate that MPR can effectively measure representation statistics across multiple intersectional groups and, when used as a training objective, can guide models toward a more balanced generation across demographic groups while maintaining generation quality.</li>
</ul>

<h3>Title: DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Chenbin Pan, Wenbin He, Zhengzhong Tu, Liu Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24025">https://arxiv.org/abs/2505.24025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24025">https://arxiv.org/pdf/2505.24025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24025]] DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models(https://arxiv.org/abs/2505.24025)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representation models like the DINO series. In this work, we propose \textbf{DINO-R1}, the first such attempt to incentivize visual in-context reasoning capabilities of vision foundation models using reinforcement learning. Specifically, DINO-R1 introduces \textbf{Group Relative Query Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly designed for query-based representation models, which computes query-level rewards based on group-normalized alignment quality. We also apply KL-regularization to stabilize the objectness distribution to reduce the training instability. This joint optimization enables dense and expressive supervision across queries while mitigating overfitting and distributional drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models that integrate a visual prompt encoder and a visual-guided query selection mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving strong generalization in both open-vocabulary and closed-set visual prompting scenarios.</li>
</ul>

<h3>Title: MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking</h3>
<ul>
<li><strong>Authors: </strong>Numair Nadeem, Muhammad Hamza Asad, Saeed Anwar, Abdul Bais</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24026">https://arxiv.org/abs/2505.24026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24026">https://arxiv.org/pdf/2505.24026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24026]] MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking(https://arxiv.org/abs/2505.24026)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation of crops and weeds is crucial for site-specific farm management; however, most existing methods depend on labor intensive pixel-level annotations. A further challenge arises when models trained on one field (source domain) fail to generalize to new fields (target domain) due to domain shifts, such as variations in lighting, camera setups, soil composition, and crop growth stages. Unsupervised Domain Adaptation (UDA) addresses this by enabling adaptation without target-domain labels, but current UDA methods struggle with occlusions and visual blending between crops and weeds, leading to misclassifications in real-world conditions. To overcome these limitations, we introduce MaskAdapt, a novel approach that enhances segmentation accuracy through multimodal contextual learning by integrating RGB images with features derived from depth data. By computing depth gradients from depth maps, our method captures spatial transitions that help resolve texture ambiguities. These gradients, through a cross-attention mechanism, refines RGB feature representations, resulting in sharper boundary delineation. In addition, we propose a geometry-aware masking strategy that applies horizontal, vertical, and stochastic masks during training. This encourages the model to focus on the broader spatial context for robust visual recognition. Evaluations on real agricultural datasets demonstrate that MaskAdapt consistently outperforms existing State-of-the-Art (SOTA) UDA methods, achieving improved segmentation mean Intersection over Union (mIOU) across diverse field conditions.</li>
</ul>

<h3>Title: From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?</h3>
<ul>
<li><strong>Authors: </strong>Ziming Zhao, ChengAo Shen, Hanghang Tong, Dongjin Song, Zhigang Deng, Qingsong Wen, Jingchao Ni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24030">https://arxiv.org/abs/2505.24030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24030">https://arxiv.org/pdf/2505.24030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24030]] From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?(https://arxiv.org/abs/2505.24030)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based models have gained increasing attention in time series research, driving interest in Large Language Models (LLMs) and foundation models for time series analysis. As the field moves toward multi-modality, Large Vision Models (LVMs) are emerging as a promising direction. In the past, the effectiveness of Transformer and LLMs in time series has been debated. When it comes to LVMs, a similar question arises: are LVMs truely useful for time series analysis? To address it, we design and conduct the first principled study involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across both high-level (classification) and low-level (forecasting) tasks, with extensive ablation analysis. Our findings indicate LVMs are indeed useful for time series classification but face challenges in forecasting. Although effective, the contemporary best LVM forecasters are limited to specific types of LVMs and imaging methods, exhibit a bias toward forecasting periods, and have limited ability to utilize long look-back windows. We hope our findings could serve as a cornerstone for future research on LVM- and multimodal-based solutions to different time series tasks.</li>
</ul>

<h3>Title: LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin</h3>
<ul>
<li><strong>Authors: </strong>Bo Wu, Sid Wang, Yunhao Tang, Jia Ding, Eryk Helenowski, Liang Tan, Tengyu Xu, Tushar Gowda, Zhengxing Chen, Chen Zhu, Xiaocheng Tang, Yundi Qian, Beibei Zhu, Rui Hou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24034">https://arxiv.org/abs/2505.24034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24034">https://arxiv.org/pdf/2505.24034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24034]] LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin(https://arxiv.org/abs/2505.24034)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has become the most effective post-training approach for improving the capabilities of Large Language Models (LLMs). In practice, because of the high demands on latency and memory, it is particularly challenging to develop an efficient RL framework that reliably manages policy models with hundreds to thousands of billions of parameters. In this paper, we present LlamaRL, a fully distributed, asynchronous RL framework optimized for efficient training of large-scale LLMs with various model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a handful to thousands of devices. LlamaRL introduces a streamlined, single-controller architecture built entirely on native PyTorch, enabling modularity, ease of use, and seamless scalability to thousands of GPUs. We also provide a theoretical analysis of LlamaRL's efficiency, including a formal proof that its asynchronous design leads to strict RL speed-up. Empirically, by leveraging best practices such as colocated model offloading, asynchronous off-policy training, and distributed direct memory access for weight synchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy model. Furthermore, the efficiency advantage continues to grow with increasing model scale, demonstrating the framework's suitability for future large-scale RL training.</li>
</ul>

<h3>Title: MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Yuexing Hao, Kumail Alhamoud, Hyewon Jeong, Haoran Zhang, Isha Puri, Philip Torr, Mike Schaekermann, Ariel D. Stern, Marzyeh Ghassemi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24040">https://arxiv.org/abs/2505.24040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24040">https://arxiv.org/pdf/2505.24040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24040]] MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering(https://arxiv.org/abs/2505.24040)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable performance on various medical question-answering (QA) benchmarks, including standardized medical exams. However, correct answers alone do not ensure correct logic, and models may reach accurate conclusions through flawed processes. In this study, we introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance Estimation and Question Answering) dataset to evaluate how physician trainees and LLMs prioritize relevant information when answering QA questions. We obtain annotations on 1,300 QA pairs from 36 physician trainees, labeling each sentence within the question components for relevance. We compare these relevance estimates to those for LLMs, and further evaluate the impact of these "relevant" subsets on downstream task performance for both physician trainees and LLMs. We find that LLMs are frequently not aligned with the content relevance estimates of physician trainees. After filtering out physician trainee-labeled irrelevant sentences, accuracy improves for both the trainees and the LLMs. All LLM and physician trainee-labeled data are available at: this http URL.</li>
</ul>

<h3>Title: NeuronTune: Towards Self-Guided Spurious Bias Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Guangtao Zheng, Wenqian Ye, Aidong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24048">https://arxiv.org/abs/2505.24048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24048">https://arxiv.org/pdf/2505.24048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24048]] NeuronTune: Towards Self-Guided Spurious Bias Mitigation(https://arxiv.org/abs/2505.24048)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks often develop spurious bias, reliance on correlations between non-essential features and classes for predictions. For example, a model may identify objects based on frequently co-occurring backgrounds rather than intrinsic features, resulting in degraded performance on data lacking these correlations. Existing mitigation approaches typically depend on external annotations of spurious correlations, which may be difficult to obtain and are not relevant to the spurious bias in a model. In this paper, we take a step towards self-guided mitigation of spurious bias by proposing NeuronTune, a post hoc method that directly intervenes in a model's internal decision process. Our method probes in a model's latent embedding space to identify and regulate neurons that lead to spurious prediction behaviors. We theoretically justify our approach and show that it brings the model closer to an unbiased one. Unlike previous methods, NeuronTune operates without requiring spurious correlation annotations, making it a practical and effective tool for improving model robustness. Experiments across different architectures and data modalities demonstrate that our method significantly mitigates spurious bias in a self-guided way.</li>
</ul>

<h3>Title: Differential Gated Self-Attention</h3>
<ul>
<li><strong>Authors: </strong>Elpiniki Maria Lygizou, MÃ³nika Farsang, Radu Grosu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24054">https://arxiv.org/abs/2505.24054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24054">https://arxiv.org/pdf/2505.24054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24054]] Differential Gated Self-Attention(https://arxiv.org/abs/2505.24054)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Transformers excel across a large variety of tasks but remain susceptible to corrupted inputs, since standard self-attention treats all query-key interactions uniformly. Inspired by lateral inhibition in biological neural circuits and building on the recent use by the Differential Transformer's use of two parallel softmax subtraction for noise cancellation, we propose Multihead Differential Gated Self-Attention (M-DGSA) that learns per-head input-dependent gating to dynamically suppress attention noise. Each head splits into excitatory and inhibitory branches whose dual softmax maps are fused by a sigmoid gate predicted from the token embedding, yielding a context-aware contrast enhancement. M-DGSA integrates seamlessly into existing Transformer stacks with minimal computational overhead. We evaluate on both vision and language benchmarks, demonstrating consistent robustness gains over vanilla Transformer, Vision Transformer, and Differential Transformer baselines. Our contributions are (i) a novel input-dependent gating mechanism for self-attention grounded in lateral inhibition, (ii) a principled synthesis of biological contrast-enhancement and self-attention theory, and (iii) comprehensive experiments demonstrating noise resilience and cross-domain applicability.</li>
</ul>

<h3>Title: Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning</h3>
<ul>
<li><strong>Authors: </strong>Jiashun Liu, Zihao Wu, Johan Obando-Ceron, Pablo Samuel Castro, Aaron Courville, Ling Pan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24061">https://arxiv.org/abs/2505.24061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24061">https://arxiv.org/pdf/2505.24061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24061]] Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning(https://arxiv.org/abs/2505.24061)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Deep reinforcement learning (RL) agents frequently suffer from neuronal activity loss, which impairs their ability to adapt to new data and learn continually. A common method to quantify and address this issue is the tau-dormant neuron ratio, which uses activation statistics to measure the expressive ability of neurons. While effective for simple MLP-based agents, this approach loses statistical power in more complex architectures. To address this, we argue that in advanced RL agents, maintaining a neuron's learning capacity, its ability to adapt via gradient updates, is more critical than preserving its expressive ability. Based on this insight, we shift the statistical objective from activations to gradients, and introduce GraMa (Gradient Magnitude Neural Activity Metric), a lightweight, architecture-agnostic metric for quantifying neuron-level learning capacity. We show that GraMa effectively reveals persistent neuron inactivity across diverse architectures, including residual networks, diffusion models, and agents with varied activation functions. Moreover, resetting neurons guided by GraMa (ReGraMa) consistently improves learning performance across multiple deep RL algorithms and benchmarks, such as MuJoCo and the DeepMind Control Suite.</li>
</ul>

<h3>Title: TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional Chinese Medicine</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Xie, Yang Yu, Ziyang Zhang, Shuai Zeng, Jiaxuan He, Ayush Vasireddy, Xiaoting Tang, Congyu Guo, Lening Zhao, Congcong Jing, Guanghui An, Dong Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24063">https://arxiv.org/abs/2505.24063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24063">https://arxiv.org/pdf/2505.24063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24063]] TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional Chinese Medicine(https://arxiv.org/abs/2505.24063)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Traditional Chinese Medicine (TCM), as an effective alternative medicine, has been receiving increasing attention. In recent years, the rapid development of large language models (LLMs) tailored for TCM has underscored the need for an objective and comprehensive evaluation framework to assess their performance on real-world tasks. However, existing evaluation datasets are limited in scope and primarily text-based, lacking a unified and standardized multimodal question-answering (QA) benchmark. To address this issue, we introduce TCM-Ladder, the first multimodal QA dataset specifically designed for evaluating large TCM language models. The dataset spans multiple core disciplines of TCM, including fundamental theory, diagnostics, herbal formulas, internal medicine, surgery, pharmacognosy, and pediatrics. In addition to textual content, TCM-Ladder incorporates various modalities such as images and videos. The datasets were constructed using a combination of automated and manual filtering processes and comprise 52,000+ questions in total. These questions include single-choice, multiple-choice, fill-in-the-blank, diagnostic dialogue, and visual comprehension tasks. We trained a reasoning model on TCM-Ladder and conducted comparative experiments against 9 state-of-the-art general domain and 5 leading TCM-specific LLMs to evaluate their performance on the datasets. Moreover, we propose Ladder-Score, an evaluation method specifically designed for TCM question answering that effectively assesses answer quality regarding terminology usage and semantic expression. To our knowledge, this is the first work to evaluate mainstream general domain and TCM-specific LLMs on a unified multimodal benchmark. The datasets and leaderboard are publicly available at this https URL or this https URL and will be continuously updated.</li>
</ul>

<h3>Title: Primal-Dual Neural Algorithmic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yu He, Ellen Vitercik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24067">https://arxiv.org/abs/2505.24067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24067">https://arxiv.org/pdf/2505.24067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24067]] Primal-Dual Neural Algorithmic Reasoning(https://arxiv.org/abs/2505.24067)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural Algorithmic Reasoning (NAR) trains neural networks to simulate classical algorithms, enabling structured and interpretable reasoning over complex data. While prior research has predominantly focused on learning exact algorithms for polynomial-time-solvable problems, extending NAR to harder problems remains an open challenge. In this work, we introduce a general NAR framework grounded in the primal-dual paradigm, a classical method for designing efficient approximation algorithms. By leveraging a bipartite representation between primal and dual variables, we establish an alignment between primal-dual algorithms and Graph Neural Networks. Furthermore, we incorporate optimal solutions from small instances to greatly enhance the model's reasoning capabilities. Our empirical results demonstrate that our model not only simulates but also outperforms approximation algorithms for multiple tasks, exhibiting robust generalization to larger and out-of-distribution graphs. Moreover, we highlight the framework's practical utility by integrating it with commercial solvers and applying it to real-world datasets.</li>
</ul>

<h3>Title: DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via Data Structures</h3>
<ul>
<li><strong>Authors: </strong>Yu He, Yingxi Li, Colin White, Ellen Vitercik</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24069">https://arxiv.org/abs/2505.24069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24069">https://arxiv.org/pdf/2505.24069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24069]] DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via Data Structures(https://arxiv.org/abs/2505.24069)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly deployed for real-world tasks that fundamentally involve data manipulation. A core requirement across these tasks is the ability to perform structural reasoning--that is, to understand and reason about data relationships. For example, customer requests require a temporal ordering, which can be represented by data structures such as queues. However, existing benchmarks primarily focus on high-level, application-driven evaluations without isolating this fundamental capability. To address this gap, we introduce DSR-Bench, a novel benchmark evaluating LLMs' structural reasoning capabilities through data structures, which provide interpretable representations of data relationships. DSR-Bench includes 20 data structures, 35 operations, and 4,140 problem instances, organized hierarchically for fine-grained analysis of reasoning limitations. Our evaluation pipeline is fully automated and deterministic, eliminating subjective human or model-based judgments. Its synthetic nature also ensures scalability and minimizes data contamination risks. We benchmark nine state-of-the-art LLMs. Our analysis shows that instruction-tuned models struggle with basic multi-attribute and multi-hop reasoning. Furthermore, while reasoning-oriented models perform better, they remain fragile on complex and hybrid structures, with the best model achieving an average score of only 47% on the challenge subset. Crucially, models often perform poorly on multi-dimensional data and natural language task descriptions, highlighting a critical gap for real-world deployment.</li>
</ul>

<h3>Title: DeepBoost-AF: A Novel Unsupervised Feature Learning and Gradient Boosting Fusion for Robust Atrial Fibrillation Detection in Raw ECG Signals</h3>
<ul>
<li><strong>Authors: </strong>Alireza Jafari, Fereshteh Yousefirizi, Vahid Seydi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24085">https://arxiv.org/abs/2505.24085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24085">https://arxiv.org/pdf/2505.24085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24085]] DeepBoost-AF: A Novel Unsupervised Feature Learning and Gradient Boosting Fusion for Robust Atrial Fibrillation Detection in Raw ECG Signals(https://arxiv.org/abs/2505.24085)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Atrial fibrillation (AF) is a prevalent cardiac arrhythmia associated with elevated health risks, where timely detection is pivotal for mitigating stroke-related morbidity. This study introduces an innovative hybrid methodology integrating unsupervised deep learning and gradient boosting models to improve AF detection. A 19-layer deep convolutional autoencoder (DCAE) is coupled with three boosting classifiers-AdaBoost, XGBoost, and LightGBM (LGBM)-to harness their complementary advantages while addressing individual limitations. The proposed framework uniquely combines DCAE with gradient boosting, enabling end-to-end AF identification devoid of manual feature extraction. The DCAE-LGBM model attains an F1-score of 95.20%, sensitivity of 99.99%, and inference latency of four seconds, outperforming existing methods and aligning with clinical deployment requirements. The DCAE integration significantly enhances boosting models, positioning this hybrid system as a reliable tool for automated AF detection in clinical settings.</li>
</ul>

<h3>Title: ComposeAnything: Composite Object Priors for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zeeshan Khan, Shizhe Chen, Cordelia Schmid</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24086">https://arxiv.org/abs/2505.24086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24086">https://arxiv.org/pdf/2505.24086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24086]] ComposeAnything: Composite Object Priors for Text-to-Image Generation(https://arxiv.org/abs/2505.24086)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating images from text involving complex and novel object arrangements remains a significant challenge for current text-to-image (T2I) models. Although prior layout-based methods improve object arrangements using spatial constraints with 2D layouts, they often struggle to capture 3D positioning and sacrifice quality and coherence. In this work, we introduce ComposeAnything, a novel framework for improving compositional image generation without retraining existing T2I models. Our approach first leverages the chain-of-thought reasoning abilities of LLMs to produce 2.5D semantic layouts from text, consisting of 2D object bounding boxes enriched with depth information and detailed captions. Based on this layout, we generate a spatial and depth aware coarse composite of objects that captures the intended composition, serving as a strong and interpretable prior that replaces stochastic noise initialization in diffusion-based T2I models. This prior guides the denoising process through object prior reinforcement and spatial-controlled denoising, enabling seamless generation of compositional objects and coherent backgrounds, while allowing refinement of inaccurate priors. ComposeAnything outperforms state-of-the-art methods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D spatial arrangements, high object counts, and surreal compositions. Human evaluations further demonstrate that our model generates high-quality images with compositions that faithfully reflect the text.</li>
</ul>

<h3>Title: Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting</h3>
<ul>
<li><strong>Authors: </strong>Chen Huang, Skyler Seto, Hadi Pouransari, Mehrdad Farajtabar, Raviteja Vemulapalli, Fartash Faghri, Oncel Tuzel, Barry-John Theobald, Josh Susskind</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24088">https://arxiv.org/abs/2505.24088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24088">https://arxiv.org/pdf/2505.24088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24088]] Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting(https://arxiv.org/abs/2505.24088)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision foundation models pre-trained on massive data encode rich representations of real-world concepts, which can be adapted to downstream tasks by fine-tuning. However, fine-tuning foundation models on one task often leads to the issue of concept forgetting on other tasks. Recent methods of robust fine-tuning aim to mitigate forgetting of prior knowledge without affecting the fine-tuning performance. Knowledge is often preserved by matching the original and fine-tuned model weights or feature pairs. However, such point-wise matching can be too strong, without explicit awareness of the feature neighborhood structures that encode rich knowledge as well. We propose a novel regularization method Proxy-FDA that explicitly preserves the structural knowledge in feature space. Proxy-FDA performs Feature Distribution Alignment (using nearest neighbor graphs) between the pre-trained and fine-tuned feature spaces, and the alignment is further improved by informative proxies that are generated dynamically to increase data diversity. Experiments show that Proxy-FDA significantly reduces concept forgetting during fine-tuning, and we find a strong correlation between forgetting and a distributional distance metric (in comparison to L2 distance). We further demonstrate Proxy-FDA's benefits in various fine-tuning settings (end-to-end, few-shot and continual tuning) and across different tasks like image classification, captioning and VQA.</li>
</ul>

<h3>Title: Practical Bayes-Optimal Membership Inference Attacks</h3>
<ul>
<li><strong>Authors: </strong>Marcus Lassila, Johan Ãstman, Khac-Hoang Ngo, Alexandre Graell i Amat</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24089">https://arxiv.org/abs/2505.24089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24089">https://arxiv.org/pdf/2505.24089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24089]] Practical Bayes-Optimal Membership Inference Attacks(https://arxiv.org/abs/2505.24089)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, membership infer</a></li>
<li><strong>Abstract: </strong>We develop practical and theoretically grounded membership inference attacks (MIAs) against both independent and identically distributed (i.i.d.) data and graph-structured data. Building on the Bayesian decision-theoretic framework of Sablayrolles et al., we derive the Bayes-optimal membership inference rule for node-level MIAs against graph neural networks, addressing key open questions about optimal query strategies in the graph setting. We introduce BASE and G-BASE, computationally efficient approximations of the Bayes-optimal attack. G-BASE achieves superior performance compared to previously proposed classifier-based node-level MIA attacks. BASE, which is also applicable to non-graph data, matches or exceeds the performance of prior state-of-the-art MIAs, such as LiRA and RMIA, at a significantly lower computational cost. Finally, we show that BASE and RMIA are equivalent under a specific hyperparameter setting, providing a principled, Bayes-optimal justification for the RMIA attack.</li>
</ul>

<h3>Title: HardTests: Synthesizing High-Quality Test Cases for LLM Coding</h3>
<ul>
<li><strong>Authors: </strong>Zhongmou He, Yee Man Choi, Kexun Zhang, Jiabao Ji, Junting Zhou, Dejia Xu, Ivan Bercovich, Aidan Zhang, Lei Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24098">https://arxiv.org/abs/2505.24098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24098">https://arxiv.org/pdf/2505.24098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24098]] HardTests: Synthesizing High-Quality Test Cases for LLM Coding(https://arxiv.org/abs/2505.24098)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Verifiers play a crucial role in large language model (LLM) reasoning, needed by post-training techniques such as reinforcement learning. However, reliable verifiers are hard to get for difficult coding problems, because a well-disguised wrong solution may only be detected by carefully human-written edge cases that are difficult to synthesize. To address this issue, we propose HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this pipeline, we curate a comprehensive competitive programming dataset HARDTESTS with 47k problems and synthetic high-quality tests. Compared with existing tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points higher and recall that is 17.5 percentage points higher when evaluating LLM-generated code. For harder problems, the improvement in precision can be as large as 40 points. HARDTESTS also proves to be more effective for model training, measured by downstream code generation performance. We will open-source our dataset and synthesis pipeline at this https URL.</li>
</ul>

<h3>Title: A SHAP-based explainable multi-level stacking ensemble learning method for predicting the length of stay in acute stroke</h3>
<ul>
<li><strong>Authors: </strong>Zhenran Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24101">https://arxiv.org/abs/2505.24101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24101">https://arxiv.org/pdf/2505.24101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24101]] A SHAP-based explainable multi-level stacking ensemble learning method for predicting the length of stay in acute stroke(https://arxiv.org/abs/2505.24101)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Length of stay (LOS) prediction in acute stroke is critical for improving care planning. Existing machine learning models have shown suboptimal predictive performance, limited generalisability, and have overlooked system-level factors. We aimed to enhance model efficiency, performance, and interpretability by refining predictors and developing an interpretable multi-level stacking ensemble model. Data were accessed from the biennial Stroke Foundation Acute Audit (2015, 2017, 2019, 2021) in Australia. Models were developed for ischaemic and haemorrhagic stroke separately. The outcome was prolonged LOS (the LOS above the 75th percentile). Candidate predictors (ischaemic: n=89; haemorrhagic: n=83) were categorised into patient, clinical, and system domains. Feature selection with correlation-based approaches was used to refine key predictors. The evaluation of models included discrimination (AUC), calibration curves, and interpretability (SHAP plots). In ischaemic stroke (N=12,575), prolonged LOS was >=9 days, compared to >=11 days in haemorrhagic stroke (N=1,970). The ensemble model achieved superior performance [AUC: 0.824 (95% CI: 0.801-0.846)] and statistically outperformed logistic regression [AUC: 0.805 (95% CI: 0.782-0.829); P=0.0004] for ischaemic. However, the model [AUC: 0.843 (95% CI: 0.790-0.895)] did not statistically outperform logistic regression [AUC: 0.828 (95% CI: 0.774-0.882); P=0.136] for haemorrhagic. SHAP analysis identified shared predictors for both types of stroke: rehabilitation assessment, urinary incontinence, stroke unit care, inability to walk independently, physiotherapy, and stroke care coordinators involvement. An explainable ensemble model effectively predicted the prolonged LOS in ischaemic stroke. Further validation in larger cohorts is needed for haemorrhagic stroke.</li>
</ul>

<h3>Title: Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors</h3>
<ul>
<li><strong>Authors: </strong>Peiran Xu, Yadong Mu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24103">https://arxiv.org/abs/2505.24103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24103">https://arxiv.org/pdf/2505.24103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24103]] Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors(https://arxiv.org/abs/2505.24103)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this work, we focus on the task of weakly supervised affordance grounding, where a model is trained to identify affordance regions on objects using human-object interaction images and egocentric object images without dense labels. Previous works are mostly built upon class activation maps, which are effective for semantic segmentation but may not be suitable for locating actions and functions. Leveraging recent advanced foundation models, we develop a supervised training pipeline based on pseudo labels. The pseudo labels are generated from an off-the-shelf part segmentation model, guided by a mapping from affordance to part names. Furthermore, we introduce three key enhancements to the baseline model: a label refining stage, a fine-grained feature alignment process, and a lightweight reasoning module. These techniques harness the semantic knowledge of static objects embedded in off-the-shelf foundation models to improve affordance learning, effectively bridging the gap between objects and actions. Extensive experiments demonstrate that the performance of the proposed model has achieved a breakthrough improvement over existing methods. Our codes are available at this https URL .</li>
</ul>

<h3>Title: Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Lin, Zhenbang Wu, Jimeng Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24105">https://arxiv.org/abs/2505.24105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24105">https://arxiv.org/pdf/2505.24105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24105]] Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning(https://arxiv.org/abs/2505.24105)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>We present EHRMIND, a practical recipe for adapting large language models (LLMs) to complex clinical reasoning tasks using reinforcement learning with verifiable rewards (RLVR). While RLVR has succeeded in mathematics and coding, its application to healthcare contexts presents unique challenges due to the specialized knowledge and reasoning required for electronic health record (EHR) interpretation. Our pilot study on the MEDCALC benchmark reveals two key failure modes: (1) misapplied knowledge, where models possess relevant medical knowledge but apply it incorrectly, and (2) missing knowledge, where models lack essential domain knowledge. To address these cases, EHRMIND applies a two-stage solution: a lightweight supervised fine-tuning (SFT) warm-up that injects missing domain knowledge, stabilizes subsequent training, and encourages structured, interpretable outputs; followed by RLVR, which reinforces outcome correctness and refines the model's decision-making. We demonstrate the effectiveness of our method across diverse clinical applications, including medical calculations (MEDCALC), patient-trial matching (TREC CLINICAL TRIALS), and disease diagnosis (EHRSHOT). EHRMIND delivers consistent gains in accuracy, interpretability, and cross-task generalization. These findings offer practical guidance for applying RLVR to enhance LLM capabilities in healthcare settings.</li>
</ul>

<h3>Title: Federated Foundation Model for GI Endoscopy Images</h3>
<ul>
<li><strong>Authors: </strong>Alina Devkota, Annahita Amireskandari, Joel Palko, Shyam Thakkar, Donald Adjeroh, Xiajun Jiang, Binod Bhattarai, Prashnna K. Gyawali</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24108">https://arxiv.org/abs/2505.24108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24108">https://arxiv.org/pdf/2505.24108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24108]] Federated Foundation Model for GI Endoscopy Images(https://arxiv.org/abs/2505.24108)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate, segmentation</a></li>
<li><strong>Abstract: </strong>Gastrointestinal (GI) endoscopy is essential in identifying GI tract abnormalities in order to detect diseases in their early stages and improve patient outcomes. Although deep learning has shown success in supporting GI diagnostics and decision-making, these models require curated datasets with labels that are expensive to acquire. Foundation models offer a promising solution by learning general-purpose representations, which can be finetuned for specific tasks, overcoming data scarcity. Developing foundation models for medical imaging holds significant potential, but the sensitive and protected nature of medical data presents unique challenges. Foundation model training typically requires extensive datasets, and while hospitals generate large volumes of data, privacy restrictions prevent direct data sharing, making foundation model training infeasible in most scenarios. In this work, we propose a FL framework for training foundation models for gastroendoscopy imaging, enabling data to remain within local hospital environments while contributing to a shared model. We explore several established FL algorithms, assessing their suitability for training foundation models without relying on task-specific labels, conducting experiments in both homogeneous and heterogeneous settings. We evaluate the trained foundation model on three critical downstream tasks--classification, detection, and segmentation--and demonstrate that it achieves improved performance across all tasks, highlighting the effectiveness of our approach in a federated, privacy-preserving setting.</li>
</ul>

<h3>Title: The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It</h3>
<ul>
<li><strong>Authors: </strong>Zheng-Xin Yong, Beyza Ermis, Marzieh Fadaee, Stephen H. Bach, Julia Kreutzer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24119">https://arxiv.org/abs/2505.24119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24119">https://arxiv.org/pdf/2505.24119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24119]] The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It(https://arxiv.org/abs/2505.24119)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive analysis of the linguistic diversity of LLM safety research, highlighting the English-centric nature of the field. Through a systematic review of nearly 300 publications from 2020--2024 across major NLP conferences and workshops at *ACL, we identify a significant and growing language gap in LLM safety research, with even high-resource non-English languages receiving minimal attention. We further observe that non-English languages are rarely studied as a standalone language and that English safety research exhibits poor language documentation practice. To motivate future research into multilingual safety, we make several recommendations based on our survey, and we then pose three concrete future directions on safety evaluation, training data generation, and crosslingual safety generalization. Based on our survey and proposed directions, the field can develop more robust, inclusive AI safety practices for diverse global populations.</li>
</ul>

<h3>Title: AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits</h3>
<ul>
<li><strong>Authors: </strong>Yichen Shi, Ze Zhang, Hongyang Wang, Zhuofu Tao, Zhongyi Li, Bingyu Chen, Yaxin Wang, Zhiping Yu, Ting-Jung Lin, Lei He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24138">https://arxiv.org/abs/2505.24138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24138">https://arxiv.org/pdf/2505.24138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24138]] AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits(https://arxiv.org/abs/2505.24138)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated circuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit design has remained a longstanding challenge due to its difficulty and complexity. Recent advances in Multi-modal Large Language Models (MLLMs) offer promising potential for supporting AMS circuit analysis and design. However, current research typically evaluates MLLMs on isolated tasks within the domain, lacking a comprehensive benchmark that systematically assesses model capabilities across diverse AMS-related challenges. To address this gap, we introduce AMSbench, a benchmark suite designed to evaluate MLLM performance across critical tasks including circuit schematic perception, circuit analysis, and circuit design. AMSbench comprises approximately 8000 test questions spanning multiple difficulty levels and assesses eight prominent models, encompassing both open-source and proprietary solutions such as Qwen 2.5-VL and Gemini 2.5 Pro. Our evaluation highlights significant limitations in current MLLMs, particularly in complex multi-modal reasoning and sophisticated circuit design tasks. These results underscore the necessity of advancing MLLMs' understanding and effective application of circuit-specific knowledge, thereby narrowing the existing performance gap relative to human expertise and moving toward fully automated AMS circuit design workflows. Our data is released at this https URL</li>
</ul>

<h3>Title: S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation</h3>
<ul>
<li><strong>Authors: </strong>Yichen Xie, Runsheng Xu, Tong He, Jyh-Jing Hwang, Katie Luo, Jingwei Ji, Hubert Lin, Letian Chen, Yiren Lu, Zhaoqi Leng, Dragomir Anguelov, Mingxing Tan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24139">https://arxiv.org/abs/2505.24139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24139">https://arxiv.org/pdf/2505.24139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24139]] S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation(https://arxiv.org/abs/2505.24139)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The latest advancements in multi-modal large language models (MLLMs) have spurred a strong renewed interest in end-to-end motion planning approaches for autonomous driving. Many end-to-end approaches rely on human annotations to learn intermediate perception and prediction tasks, while purely self-supervised approaches--which directly learn from sensor inputs to generate planning trajectories without human annotations often underperform the state of the art. We observe a key gap in the input representation space: end-to-end approaches built on MLLMs are often pretrained with reasoning tasks in 2D image space rather than the native 3D space in which autonomous vehicles plan. To this end, we propose S4-Driver, a scalable self-supervised motion planning algorithm with spatio-temporal visual representation, based on the popular PaLI multimodal large language model. S4-Driver uses a novel sparse volume strategy to seamlessly transform the strong visual representation of MLLMs from perspective view to 3D space without the need to finetune the vision encoder. This representation aggregates multi-view and multi-frame visual inputs and enables better prediction of planning trajectories in 3D space. To validate our method, we run experiments on both nuScenes and Waymo Open Motion Dataset (with in-house camera data). Results show that S4-Driver performs favorably against existing supervised multi-task approaches while requiring no human annotations. It also demonstrates great scalability when pretrained on large volumes of unannotated driving logs.</li>
</ul>

<h3>Title: The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Jiashuai Liu, Yingjia Shang, Yingkang Zhan, Di Zhang, Yi Niu, Dong Wei, Xian Wu, Zeyu Gao, Chen Li, Yefeng Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24141">https://arxiv.org/abs/2505.24141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24141">https://arxiv.org/pdf/2505.24141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24141]] The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models(https://arxiv.org/abs/2505.24141)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>With the widespread adoption of pathology foundation models in both research and clinical decision support systems, exploring their security has become a critical concern. However, despite their growing impact, the vulnerability of these models to adversarial attacks remains largely unexplored. In this work, we present the first systematic investigation into the security of pathology foundation models for whole slide image~(WSI) analysis against adversarial attacks. Specifically, we introduce the principle of \textit{local perturbation with global impact} and propose a label-free attack framework that operates without requiring access to downstream task labels. Under this attack framework, we revise four classical white-box attack methods and redefine the perturbation budget based on the characteristics of WSI. We conduct comprehensive experiments on three representative pathology foundation models across five datasets and six downstream tasks. Despite modifying only 0.1\% of patches per slide with imperceptible noise, our attack leads to downstream accuracy degradation that can reach up to 20\% in the worst cases. Furthermore, we analyze key factors that influence attack success, explore the relationship between patch-level vulnerability and semantic content, and conduct a preliminary investigation into potential defence strategies. These findings lay the groundwork for future research on the adversarial robustness and reliable deployment of pathology foundation models. Our code is publicly available at: this https URL.</li>
</ul>

<h3>Title: CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer</h3>
<ul>
<li><strong>Authors: </strong>Jinglong Gao, Xiao Ding, Lingxiao Zou, Bing Qin, Ting Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24143">https://arxiv.org/abs/2505.24143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24143">https://arxiv.org/pdf/2505.24143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24143]] CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer(https://arxiv.org/abs/2505.24143)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In-Context Learning (ICL) enhances the performance of large language models (LLMs) with demonstrations. However, obtaining these demonstrations primarily relies on manual effort. In most real-world scenarios, users are often unwilling or unable to provide such demonstrations. Inspired by the human analogy, we explore a new ICL paradigm CrossICL to study how to utilize existing source task demonstrations in the ICL for target tasks, thereby obtaining reliable guidance without any additional manual effort. To explore this, we first design a two-stage alignment strategy to mitigate the interference caused by gaps across tasks, as the foundation for our experimental exploration. Based on it, we conduct comprehensive exploration of CrossICL, with 875 NLP tasks from the Super-NI benchmark and six types of LLMs, including GPT-4o. Experimental results demonstrate the effectiveness of CrossICL and provide valuable insights on questions like the criteria for selecting cross-task demonstrations, as well as the types of task-gap-induced interference in CrossICL.</li>
</ul>

<h3>Title: Autoregressive regularized score-based diffusion models for multi-scenarios fluid flow prediction</h3>
<ul>
<li><strong>Authors: </strong>Wilfried Genuist, Ãric Savin, Filippo Gatti, Didier Clouteau</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24145">https://arxiv.org/abs/2505.24145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24145">https://arxiv.org/pdf/2505.24145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24145]] Autoregressive regularized score-based diffusion models for multi-scenarios fluid flow prediction(https://arxiv.org/abs/2505.24145)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Building on recent advances in scientific machine learning and generative modeling for computational fluid dynamics, we propose a conditional score-based diffusion model designed for multi-scenarios fluid flow prediction. Our model integrates an energy constraint rooted in the statistical properties of turbulent flows, improving prediction quality with minimal training, while enabling efficient sampling at low cost. The method features a simple and general architecture that requires no problem-specific design, supports plug-and-play enhancements, and enables fast and flexible solution generation. It also demonstrates an efficient conditioning mechanism that simplifies training across different scenarios without demanding a redesign of existing models. We further explore various stochastic differential equation formulations to demonstrate how thoughtful design choices enhance performance. We validate the proposed methodology through extensive experiments on complex fluid dynamics datasets encompassing a variety of flow regimes and configurations. Results demonstrate that our model consistently achieves stable, robust, and physically faithful predictions, even under challenging turbulent conditions. With properly tuned parameters, it achieves accurate results across multiple scenarios while preserving key physical and statistical properties. We present a comprehensive analysis of stochastic differential equation impact and discuss our approach across diverse fluid mechanics tasks.</li>
</ul>

<h3>Title: Biological Pathway Guided Gene Selection Through Collaborative Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ehtesamul Azim, Dongjie Wang, Tae Hyun Hwang, Yanjie Fu, Wei Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24155">https://arxiv.org/abs/2505.24155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24155">https://arxiv.org/pdf/2505.24155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24155]] Biological Pathway Guided Gene Selection Through Collaborative Reinforcement Learning(https://arxiv.org/abs/2505.24155)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Gene selection in high-dimensional genomic data is essential for understanding disease mechanisms and improving therapeutic outcomes. Traditional feature selection methods effectively identify predictive genes but often ignore complex biological pathways and regulatory networks, leading to unstable and biologically irrelevant signatures. Prior approaches, such as Lasso-based methods and statistical filtering, either focus solely on individual gene-outcome associations or fail to capture pathway-level interactions, presenting a key challenge: how to integrate biological pathway knowledge while maintaining statistical rigor in gene selection? To address this gap, we propose a novel two-stage framework that integrates statistical selection with biological pathway knowledge using multi-agent reinforcement learning (MARL). First, we introduce a pathway-guided pre-filtering strategy that leverages multiple statistical methods alongside KEGG pathway information for initial dimensionality reduction. Next, for refined selection, we model genes as collaborative agents in a MARL framework, where each agent optimizes both predictive power and biological relevance. Our framework incorporates pathway knowledge through Graph Neural Network-based state representations, a reward mechanism combining prediction performance with gene centrality and pathway coverage, and collaborative learning strategies using shared memory and a centralized critic component. Extensive experiments on multiple gene expression datasets demonstrate that our approach significantly improves both prediction accuracy and biological interpretability compared to traditional methods.</li>
</ul>

<h3>Title: Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction</h3>
<ul>
<li><strong>Authors: </strong>Chenyou Fan, Fangzheng Yan, Chenjia Bai, Jiepeng Wang, Chi Zhang, Zhen Wang, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24156">https://arxiv.org/abs/2505.24156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24156">https://arxiv.org/pdf/2505.24156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24156]] Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction(https://arxiv.org/abs/2505.24156)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Learning a generalizable bimanual manipulation policy is extremely challenging for embodied agents due to the large action space and the need for coordinated arm movements. Existing approaches rely on Vision-Language-Action (VLA) models to acquire bimanual policies. However, transferring knowledge from single-arm datasets or pre-trained VLA models often fails to generalize effectively, primarily due to the scarcity of bimanual data and the fundamental differences between single-arm and bimanual manipulation. In this paper, we propose a novel bimanual foundation policy by fine-tuning the leading text-to-video models to predict robot trajectories and training a lightweight diffusion policy for action generation. Given the lack of embodied knowledge in text-to-video models, we introduce a two-stage paradigm that fine-tunes independent text-to-flow and flow-to-video models derived from a pre-trained text-to-video model. Specifically, optical flow serves as an intermediate variable, providing a concise representation of subtle movements between images. The text-to-flow model predicts optical flow to concretize the intent of language instructions, and the flow-to-video model leverages this flow for fine-grained video prediction. Our method mitigates the ambiguity of language in single-stage text-to-video prediction and significantly reduces the robot-data requirement by avoiding direct use of low-level actions. In experiments, we collect high-quality manipulation data for real dual-arm robot, and the results of simulation and real-world experiments demonstrate the effectiveness of our method.</li>
</ul>

<h3>Title: Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents</h3>
<ul>
<li><strong>Authors: </strong>Seungjoon Lee, Suhwan Kim, Minhyeon Oh, Youngsik Yoon, Jungseul Ok</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24157">https://arxiv.org/abs/2505.24157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24157">https://arxiv.org/pdf/2505.24157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24157]] Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents(https://arxiv.org/abs/2505.24157)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Developing autonomous agents capable of mastering complex, multi-step tasks in unpredictable, interactive environments presents a significant challenge. While Large Language Models (LLMs) offer promise for planning, existing approaches often rely on problematic internal knowledge or make unrealistic environmental assumptions. Although recent work explores learning planning knowledge, they still retain limitations due to partial reliance on external knowledge or impractical setups. Indeed, prior research has largely overlooked developing agents capable of acquiring planning knowledge from scratch, directly in realistic settings. While realizing this capability is necessary, it presents significant challenges, primarily achieving robustness given the substantial risk of incorporating LLMs' inaccurate knowledge. Moreover, efficiency is crucial for practicality as learning can demand prohibitive exploration. In response, we introduce Robust and Efficient Planning for Open-world Agents (REPOA), a novel framework designed to tackle these issues. REPOA features three key components: adaptive dependency learning and fine-grained failure-aware operation memory to enhance robustness to knowledge inaccuracies, and difficulty-based exploration to improve learning efficiency. Our evaluation in two established open-world testbeds demonstrates REPOA's robust and efficient planning, showcasing its capability to successfully obtain challenging late-game items that were beyond the reach of prior approaches.</li>
</ul>

<h3>Title: Threading Keyframe with Narratives: MLLMs as Strong Long Video Comprehenders</h3>
<ul>
<li><strong>Authors: </strong>Bo Fang, Wenhao Wu, Qiangqiang Wu, Yuxin Song, Antoni B. Chan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24158">https://arxiv.org/abs/2505.24158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24158">https://arxiv.org/pdf/2505.24158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24158]] Threading Keyframe with Narratives: MLLMs as Strong Long Video Comprehenders(https://arxiv.org/abs/2505.24158)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Employing Multimodal Large Language Models (MLLMs) for long video understanding remains a challenging problem due to the dilemma between the substantial number of video frames (i.e., visual tokens) versus the limited context length of language models. Traditional uniform sampling often leads to selection of irrelevant content, while post-training MLLMs on thousands of frames imposes a substantial computational burden. In this paper, we propose threading keyframes with narratives (Nar-KFC), a plug-and-play module to facilitate effective and efficient long video perception. Nar-KFC generally involves two collaborative steps. First, we formulate the keyframe selection process as an integer quadratic programming problem, jointly optimizing query-relevance and frame-diversity. To avoid its computational complexity, a customized greedy search strategy is designed as an efficient alternative. Second, to mitigate the temporal discontinuity caused by sparse keyframe sampling, we further introduce interleaved textual narratives generated from non-keyframes using off-the-shelf captioners. These narratives are inserted between keyframes based on their true temporal order, forming a coherent and compact representation. Nar-KFC thus serves as a temporal- and content-aware compression strategy that complements visual and textual modalities. Experimental results on multiple long-video benchmarks demonstrate that Nar-KFC significantly improves the performance of popular MLLMs. Code will be made publicly available.</li>
</ul>

<h3>Title: LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Sun, Shiyou Qian, Zhangchi Han, Wei Li, Zelin Qian, Dingyu Yang, Jian Cao, Guangtao Xue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24163">https://arxiv.org/abs/2505.24163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24163">https://arxiv.org/pdf/2505.24163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24163]] LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing(https://arxiv.org/abs/2505.24163)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Knowledge Graphs (KGs) structure real-world entities and their relationships into triples, enhancing machine reasoning for various tasks. While domain-specific KGs offer substantial benefits, their manual construction is often inefficient and requires specialized knowledge. Recent approaches for knowledge graph construction (KGC) based on large language models (LLMs), such as schema-guided KGC and reference knowledge integration, have proven efficient. However, these methods are constrained by their reliance on manually defined schema, single-document processing, and public-domain references, making them less effective for domain-specific corpora that exhibit complex knowledge dependencies and specificity, as well as limited reference knowledge. To address these challenges, we propose LKD-KGC, a novel framework for unsupervised domain-specific KG construction. LKD-KGC autonomously analyzes document repositories to infer knowledge dependencies, determines optimal processing sequences via LLM driven prioritization, and autoregressively generates entity schema by integrating hierarchical inter-document contexts. This schema guides the unsupervised extraction of entities and relationships, eliminating reliance on predefined structures or external knowledge. Extensive experiments show that compared with state-of-the-art baselines, LKD-KGC generally achieves improvements of 10% to 20% in both precision and recall rate, demonstrating its potential in constructing high-quality domain-specific KGs.</li>
</ul>

<h3>Title: Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shilin Xu, Yanwei Li, Rui Yang, Tao Zhang, Yueyi Sun, Wei Chow, Linfeng Li, Hang Song, Qi Xu, Yunhai Tong, Xiangtai Li, Hao Fei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24164">https://arxiv.org/abs/2505.24164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24164">https://arxiv.org/pdf/2505.24164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24164]] Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models(https://arxiv.org/abs/2505.24164)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent works on large language models (LLMs) have successfully demonstrated the emergence of reasoning capabilities via reinforcement learning (RL). Although recent efforts leverage group relative policy optimization (GRPO) for MLLMs post-training, they constantly explore one specific aspect, such as grounding tasks, math problems, or chart analysis. There are no works that can leverage multi-source MLLM tasks for stable reinforcement learning. In this work, we present a unified perspective to solve this problem. We present Mixed-R1, a unified yet straightforward framework that contains a mixed reward function design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K). We first design a data engine to select high-quality examples to build the Mixed-45K post-training dataset. Then, we present a Mixed-Reward design, which contains various reward functions for various MLLM tasks. In particular, it has four different reward functions: matching reward for binary answer or multiple-choice problems, chart reward for chart-aware datasets, IoU reward for grounding problems, and open-ended reward for long-form text responses such as caption datasets. To handle the various long-form text content, we propose a new open-ended reward named Bidirectional Max-Average Similarity (BMAS) by leveraging tokenizer embedding matching between the generated response and the ground truth. Extensive experiments show the effectiveness of our proposed method on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes. Our dataset and model are available at this https URL.</li>
</ul>

<h3>Title: Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem</h3>
<ul>
<li><strong>Authors: </strong>Katherine Tieu, Dongqi Fu, Jun Wu, Jingrui He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24178">https://arxiv.org/abs/2505.24178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24178">https://arxiv.org/pdf/2505.24178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24178]] Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem(https://arxiv.org/abs/2505.24178)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the era of foundation models, Out-of- Distribution (OOD) problems, i.e., the data discrepancy between the training environments and testing environments, hinder AI generalization. Further, relational data like graphs disobeying the Independent and Identically Distributed (IID) condition makes the problem more challenging, especially much harder when it is associated with time. Motivated by this, to realize the robust invariant learning over temporal graphs, we want to investigate what components in temporal graphs are most invariant and representative with respect to labels. With the Information Bottleneck (IB) method, we propose an error-bounded Invariant Link Selector that can distinguish invariant components and variant components during the training process to make the deep learning model generalizable for different testing scenarios. Besides deriving a series of rigorous generalizable optimization functions, we also equip the training with task-specific loss functions, e.g., temporal link prediction, to make pretrained models solve real-world application tasks like citation recommendation and merchandise recommendation, as demonstrated in our experiments with state-of-the-art (SOTA) methods. Our code is available at this https URL.</li>
</ul>

<h3>Title: SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling</h3>
<ul>
<li><strong>Authors: </strong>Xiaodong Ji, Hailin Zhang, Fangcheng Fu, Bin Cui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24179">https://arxiv.org/abs/2505.24179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24179">https://arxiv.org/pdf/2505.24179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24179]] SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling(https://arxiv.org/abs/2505.24179)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Many advanced Large Language Model (LLM) applications require long-context processing, but the self-attention module becomes a bottleneck during the prefilling stage of inference due to its quadratic time complexity with respect to sequence length. Existing sparse attention methods accelerate attention computation by skipping less significant regions of the attention map. However, these approaches typically perform coarse-grained inspection of the attention map, rendering considerable loss in model accuracy. In this paper, we propose SALE, a fine-grained sparse attention method that accelerates the long-context prefilling stage of LLM with negligible loss in model accuracy. SALE achieves fast and accurate fine-grained attention weight estimation through 4-bit quantized query-key products, followed by block-sparse attention to accelerate prefilling computations. For importance evaluation for query-key pairs, we adopt our Relative Attention Score metric, which offers significantly higher efficiency within our framework. We implement a custom CUDA kernel optimized for our approach for hardware efficiency, reducing the additional overhead to approximately 11% of the full attention latency. Notably, SALE requires no parameter training and can be seamlessly integrated into existing systems with trivial code modifications. Experiments on long-context benchmarks demonstrate that our method outperforms existing approaches in accuracy-efficiency trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences longer than 64K while maintaining model quality.</li>
</ul>

<h3>Title: Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT</h3>
<ul>
<li><strong>Authors: </strong>Zhuobai Dong, Junchao Yi, Ziyuan Zheng, Haochen Han, Xiangxi Zheng, Alex Jinpeng Wang, Fangming Liu, Linjie Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24182">https://arxiv.org/abs/2505.24182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24182">https://arxiv.org/pdf/2505.24182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24182]] Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT(https://arxiv.org/abs/2505.24182)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding the physical world - governed by laws of motion, spatial relations, and causality - poses a fundamental challenge for multimodal large language models (MLLMs). While recent advances such as OpenAI o3 and GPT-4o demonstrate impressive perceptual and reasoning capabilities, our investigation reveals these models struggle profoundly with visual physical reasoning, failing to grasp basic physical laws, spatial interactions, and causal effects in complex scenes. More importantly, they often fail to follow coherent reasoning chains grounded in visual evidence, especially when multiple steps are needed to arrive at the correct answer. To rigorously evaluate this capability, we introduce MVPBench, a curated benchmark designed to rigorously evaluate visual physical reasoning through the lens of visual chain-of-thought (CoT). Each example features interleaved multi-image inputs and demands not only the correct final answer but also a coherent, step-by-step reasoning path grounded in evolving visual cues. This setup mirrors how humans reason through real-world physical processes over time. To ensure fine-grained evaluation, we introduce a graph-based CoT consistency metric that verifies whether the reasoning path of model adheres to valid physical logic. Additionally, we minimize shortcut exploitation from text priors, encouraging models to rely on visual understanding. Experimental results reveal a concerning trend: even cutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text alignment in physical domains. Surprisingly, RL-based post-training alignment - commonly believed to improve visual reasoning performance - often harms spatial reasoning, suggesting a need to rethink current fine-tuning practices.</li>
</ul>

<h3>Title: CodeV-R1: Reasoning-Enhanced Verilog Generation</h3>
<ul>
<li><strong>Authors: </strong>Yaoyu Zhu, Di Huang, Hanqi Lyu, Xiaoyun Zhang, Chongxiao Li, Wenxuan Shi, Yutong Wu, Jianan Mu, Jinghua Wang, Yang Zhao, Pengwei Jin, Shuyao Cheng, Shengwen Liang, Xishan Zhang, Rui Zhang, Zidong Du, Qi Guo, Xing Hu, Yunji Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24183">https://arxiv.org/abs/2505.24183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24183">https://arxiv.org/pdf/2505.24183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24183]] CodeV-R1: Reasoning-Enhanced Verilog Generation(https://arxiv.org/abs/2505.24183)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage "distill-then-RL" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities.</li>
</ul>

<h3>Title: Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling</h3>
<ul>
<li><strong>Authors: </strong>Yipan Wei, Yuchen Zou, Yapeng Li, Bo Du</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24185">https://arxiv.org/abs/2505.24185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24185">https://arxiv.org/pdf/2505.24185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24185]] Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling(https://arxiv.org/abs/2505.24185)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Multi-Task Learning (FMTL) enables multiple clients performing heterogeneous tasks without exchanging their local data, offering broad potential for privacy preserving multi-task collaboration. However, most existing methods focus on building personalized models for each client and unable to support the aggregation of multiple heterogeneous tasks into a unified model. As a result, in real-world scenarios where task objectives, label spaces, and optimization paths vary significantly, conventional FMTL methods struggle to achieve effective joint training. To address this challenge, we propose FedDEA (Federated Decoupled Aggregation), an update-structure-aware aggregation method specifically designed for multi-task model integration. Our method dynamically identifies task-relevant dimensions based on the response strength of local updates and enhances their optimization effectiveness through rescaling. This mechanism effectively suppresses cross-task interference and enables task-level decoupled aggregation within a unified global model. FedDEA does not rely on task labels or architectural modifications, making it broadly applicable and deployment-friendly. Experimental results demonstrate that it can be easily integrated into various mainstream federated optimization algorithms and consistently delivers significant overall performance improvements on widely used NYUD-V2 and PASCAL-Context. These results validate the robustness and generalization capabilities of FedDEA under highly heterogeneous task settings.</li>
</ul>

<h3>Title: Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mikhail L. Arbuzov, Alexey A. Shvets, Sisong Beir</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24187">https://arxiv.org/abs/2505.24187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24187">https://arxiv.org/pdf/2505.24187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24187]] Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models(https://arxiv.org/abs/2505.24187)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The prevailing assumption of an exponential decay in large language model (LLM) reliability with sequence length, predicated on independent per-token error probabilities, posits an inherent limitation for long autoregressive outputs. Our research fundamentally challenges this view by synthesizing emerging evidence that LLM errors are not uniformly distributed but are concentrated at sparse "key tokens" ($5-10\%$ of total tokens) representing critical decision junctions. By distinguishing these high-impact tokens from the increasingly predictable majority, we introduce a new reliability formula explaining the sustained coherence of modern LLMs over thousands of tokens. Converging research streams reveal that long-context performance primarily depends on accurately navigating a few crucial semantic decision points rather than on uniform token-level accuracy, enabling targeted strategies that significantly outperform brute-force approaches. We thus propose a framework for next-generation systems centered on selective preservation of semantically vital tokens, dynamic computational allocation at uncertain decision boundaries, multi-path exploration at ambiguities, and architectures aligned with natural semantic domains. This marks a fundamental shift from raw scaling to strategic reasoning, promising breakthrough performance without proportionate computational scaling and offering a more nuanced understanding that supersedes the exponential decay hypothesis, thereby opening pathways toward substantially more powerful and efficient language systems.</li>
</ul>

<h3>Title: Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows</h3>
<ul>
<li><strong>Authors: </strong>Orlando Marquez Ayala, Patrice Bechard, Emily Chen, Maggie Baird, Jingfei Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24189">https://arxiv.org/abs/2505.24189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24189">https://arxiv.org/pdf/2505.24189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24189]] Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows(https://arxiv.org/abs/2505.24189)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. In this work, we present evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. We compare fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form. We observe that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. We also perform systematic error analysis to reveal model limitations.</li>
</ul>

<h3>Title: CLaSp: In-Context Layer Skip for Self-Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Longze Chen, Renke Shan, Huiming Wang, Lu Wang, Ziqiang Liu, Run Luo, Jiawei Wang, Hamid Alinejad-Rokny, Min Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24196">https://arxiv.org/abs/2505.24196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24196">https://arxiv.org/pdf/2505.24196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24196]] CLaSp: In-Context Layer Skip for Self-Speculative Decoding(https://arxiv.org/abs/2505.24196)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding (SD) is a promising method for accelerating the decoding process of Large Language Models (LLMs). The efficiency of SD primarily hinges on the consistency between the draft model and the verify model. However, existing drafting approaches typically require additional modules to be trained, which can be challenging to implement and ensure compatibility across various LLMs. In this paper, we propose CLaSp, an in-context layer-skipping strategy for self-speculative decoding. Unlike prior methods, CLaSp does not require additional drafting modules or extra training. Instead, it employs a plug-and-play mechanism by skipping intermediate layers of the verify model to construct a compressed draft model. Specifically, we develop a dynamic programming algorithm that optimizes the layer-skipping process by leveraging the complete hidden states from the last verification stage as an objective. This enables CLaSp to dynamically adjust its layer-skipping strategy after each verification stage, without relying on pre-optimized sets of skipped layers. Experimental results across diverse downstream tasks demonstrate that CLaSp achieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the original distribution of the generated text.</li>
</ul>

<h3>Title: Intuitionistic Fuzzy Sets for Large Language Model Data Annotation: A Novel Approach to Side-by-Side Preference Labeling</h3>
<ul>
<li><strong>Authors: </strong>Yimin Du</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24199">https://arxiv.org/abs/2505.24199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24199">https://arxiv.org/pdf/2505.24199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24199]] Intuitionistic Fuzzy Sets for Large Language Model Data Annotation: A Novel Approach to Side-by-Side Preference Labeling(https://arxiv.org/abs/2505.24199)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The quality of human preference data is crucial for training and evaluating large language models (LLMs), particularly in reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) scenarios. Traditional side-by-side (SBS) annotation approaches often struggle with inherent uncertainty, annotator disagreement, and the complexity of preference judgments. This paper introduces a novel framework based on intuitionistic fuzzy sets (IFS) for modeling and aggregating human preferences in LLM data annotation tasks. Our approach captures not only the degree of preference but also the uncertainty and hesitation inherent in human judgment through membership, non-membership, and hesitation degrees. We propose an IFS-based annotation protocol that enables more nuanced preference modeling, develops aggregation methods for handling annotator disagreement, and introduces quality metrics for preference data assessment. Experimental validation on multiple datasets demonstrates that our IFS-based approach significantly improves annotation consistency, reduces annotator fatigue, and produces higher-quality preference data compared to traditional binary and Likert-scale methods. The resulting preference datasets lead to improved model performance in downstream tasks, with 12.3\% improvement in win-rate against baseline models and 15.7\% reduction in annotation time. Our framework provides a principled approach to handling uncertainty in human preference annotation and offers practical benefits for large-scale LLM training.</li>
</ul>

<h3>Title: STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow Matching Models</h3>
<ul>
<li><strong>Authors: </strong>Zheng Tan, Weizhen Wang, Andrea L. Bertozzi, Ernest K. Ryu</a></li>
<li><strong>Subjects: </strong>cs.CV, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24210">https://arxiv.org/abs/2505.24210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24210">https://arxiv.org/pdf/2505.24210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24210]] STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow Matching Models(https://arxiv.org/abs/2505.24210)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) have demonstrated remarkable performance in high-fidelity image and video generation. Because high-quality generations with DMs typically require a large number of function evaluations (NFEs), resulting in slow sampling, there has been extensive research successfully reducing the NFE to a small range (<10) while maintaining acceptable image quality. However, many practical applications, such as those involving Stable Diffusion 3.5, FLUX, and SANA, commonly operate in the mid-NFE regime (20-50 NFE) to achieve superior results, and, despite the practical relevance, research on the effective sampling within this mid-NFE regime remains underexplored. In this work, we propose a novel, training-free, and structure-independent DM ODE solver called the Stabilized Taylor Orthogonal Runge--Kutta (STORK) method, based on a class of stiff ODE solvers with a Taylor expansion adaptation. Unlike prior work such as DPM-Solver, which is dependent on the semi-linear structure of the DM ODE, STORK is applicable to any DM sampling, including noise-based and flow matching-based models. Within the 20-50 NFE range, STORK achieves improved generation quality, as measured by FID scores, across unconditional pixel-level generation and conditional latent-space generation tasks using models like Stable Diffusion 3.5 and SANA. Code is available at this https URL.</li>
</ul>

<h3>Title: Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?</h3>
<ul>
<li><strong>Authors: </strong>Jiwan Chung, Janghan Yoon, Junhyeong Park, Sangeyl Lee, Joowon Yang, Sooyeon Park, Youngjae Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24211">https://arxiv.org/abs/2505.24211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24211">https://arxiv.org/pdf/2505.24211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24211]] Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?(https://arxiv.org/abs/2505.24211)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Any-to-any generative models aim to enable seamless interpretation and generation across multiple modalities within a unified framework, yet their ability to preserve relationships across modalities remains uncertain. Do unified models truly achieve cross-modal coherence, or is this coherence merely perceived? To explore this, we introduce ACON, a dataset of 1,000 images (500 newly contributed) paired with captions, editing instructions, and Q&A pairs to evaluate cross-modal transfers rigorously. Using three consistency criteria-cyclic consistency, forward equivariance, and conjugated equivariance-our experiments reveal that any-to-any models do not consistently demonstrate greater cross-modal consistency than specialized models in pointwise evaluations such as cyclic consistency. However, equivariance evaluations uncover weak but observable consistency through structured analyses of the intermediate latent space enabled by multiple editing operations. We release our code and data at this https URL.</li>
</ul>

<h3>Title: Benchmarking Foundation Models for Zero-Shot Biometric Tasks</h3>
<ul>
<li><strong>Authors: </strong>Redwan Sony, Parisa Farmanifard, Hamzeh Alzwairy, Nitish Shukla, Arun Ross</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24214">https://arxiv.org/abs/2505.24214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24214">https://arxiv.org/pdf/2505.24214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24214]] Benchmarking Foundation Models for Zero-Shot Biometric Tasks(https://arxiv.org/abs/2505.24214)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, biometric, large language model</a></li>
<li><strong>Abstract: </strong>The advent of foundation models, particularly Vision-Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), has redefined the frontiers of artificial intelligence, enabling remarkable generalization across diverse tasks with minimal or no supervision. Yet, their potential in biometric recognition and analysis remains relatively underexplored. In this work, we introduce a comprehensive benchmark that evaluates the zero-shot and few-shot performance of state-of-the-art publicly available VLMs and MLLMs across six biometric tasks spanning the face and iris modalities: face verification, soft biometric attribute prediction (gender and race), iris recognition, presentation attack detection (PAD), and face manipulation detection (morphs and deepfakes). A total of 41 VLMs were used in this evaluation. Experiments show that embeddings from these foundation models can be used for diverse biometric tasks with varying degrees of success. For example, in the case of face verification, a True Match Rate (TMR) of 96.77 percent was obtained at a False Match Rate (FMR) of 1 percent on the Labeled Face in the Wild (LFW) dataset, without any fine-tuning. In the case of iris recognition, the TMR at 1 percent FMR on the IITD-R-Full dataset was 97.55 percent without any fine-tuning. Further, we show that applying a simple classifier head to these embeddings can help perform DeepFake detection for faces, Presentation Attack Detection (PAD) for irides, and extract soft biometric attributes like gender and ethnicity from faces with reasonably high accuracy. This work reiterates the potential of pretrained models in achieving the long-term vision of Artificial General Intelligence.</li>
</ul>

<h3>Title: Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Prasanna Reddy Pulakurthi, Majid Rabbani, Jamison Heard, Sohail Dianat, Celso M. de Melo, Raghuveer Rao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24216">https://arxiv.org/abs/2505.24216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24216">https://arxiv.org/pdf/2505.24216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24216]] Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation(https://arxiv.org/abs/2505.24216)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This work investigates Source-Free Domain Adaptation (SFDA), where a model adapts to a target domain without access to source data. A new augmentation technique, Shuffle PatchMix (SPM), and a novel reweighting strategy are introduced to enhance performance. SPM shuffles and blends image patches to generate diverse and challenging augmentations, while the reweighting strategy prioritizes reliable pseudo-labels to mitigate label noise. These techniques are particularly effective on smaller datasets like PACS, where overfitting and pseudo-label noise pose greater risks. State-of-the-art results are achieved on three major benchmarks: PACS, VisDA-C, and DomainNet-126. Notably, on PACS, improvements of 7.3% (79.4% to 86.7%) and 7.2% are observed in single-target and multi-target settings, respectively, while gains of 2.8% and 0.7% are attained on DomainNet-126 and VisDA-C. This combination of advanced augmentation and robust pseudo-label reweighting establishes a new benchmark for SFDA. The code is available at: this https URL</li>
</ul>

<h3>Title: Semi-structured LLM Reasoners Can Be Rigorously Audited</h3>
<ul>
<li><strong>Authors: </strong>Jixuan Leng, Cassandra A. Cohen, Zhixian Zhang, Chenyan Xiong, William W. Cohen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24217">https://arxiv.org/abs/2505.24217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24217">https://arxiv.org/pdf/2505.24217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24217]] Semi-structured LLM Reasoners Can Be Rigorously Audited(https://arxiv.org/abs/2505.24217)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) become increasingly capable at reasoning, the problem of "faithfulness" persists: LLM "reasoning traces" can contain errors and omissions that are difficult to detect, and may obscure biases in model outputs. To address these limitations, we introduce Semi-Structured Reasoning Models (SSRMs), which internalize a semi-structured Chain-of-Thought (CoT) reasoning format within the model. Our SSRMs generate reasoning traces in a Pythonic syntax. While SSRM traces are not executable, they adopt a restricted, task-specific vocabulary to name distinct reasoning steps, and to mark each step's inputs and outputs. Through extensive evaluation on ten benchmarks, SSRMs demonstrate strong performance and generality: they outperform comparably sized baselines by nearly ten percentage points on in-domain tasks while remaining competitive with specialized models on out-of-domain medical benchmarks. Furthermore, we show that semi-structured reasoning is more amenable to analysis: in particular, they can be automatically audited to identify reasoning flaws. We explore both hand-crafted structured audits, which detect task-specific problematic reasoning patterns, and learned typicality audits, which apply probabilistic models over reasoning patterns, and show that both audits can be used to effectively flag probable reasoning errors.</li>
</ul>

<h3>Title: ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation</h3>
<ul>
<li><strong>Authors: </strong>Lam Thanh Do, Aaditya Bodke, Pritom Saha Akash, Kevin Chen-Chuan Chang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24219">https://arxiv.org/abs/2505.24219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24219">https://arxiv.org/pdf/2505.24219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24219]] ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation(https://arxiv.org/abs/2505.24219)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Unsupervised keyphrase prediction has gained growing interest in recent years. However, existing methods typically rely on heuristically defined importance scores, which may lead to inaccurate informativeness estimation. In addition, they lack consideration for time efficiency. To solve these problems, we propose ERU-KG, an unsupervised keyphrase generation (UKG) model that consists of an informativeness and a phraseness module. The former estimates the relevance of keyphrase candidates, while the latter generate those candidates. The informativeness module innovates by learning to model informativeness through references (e.g., queries, citation contexts, and titles) and at the term-level, thereby 1) capturing how the key concepts of documents are perceived in different contexts and 2) estimating informativeness of phrases more efficiently by aggregating term informativeness, removing the need for explicit modeling of the candidates. ERU-KG demonstrates its effectiveness on keyphrase generation benchmarks by outperforming unsupervised baselines and achieving on average 89\% of the performance of a supervised model for top 10 predictions. Additionally, to highlight its practical utility, we evaluate the model on text retrieval tasks and show that keyphrases generated by ERU-KG are effective when employed as query and document expansions. Furthermore, inference speed tests reveal that ERU-KG is the fastest among baselines of similar model sizes. Finally, our proposed model can switch between keyphrase generation and extraction by adjusting hyperparameters, catering to diverse application requirements.</li>
</ul>

<h3>Title: Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin</h3>
<ul>
<li><strong>Authors: </strong>Fangyikang Wang, Hubery Yin, Lei Qian, Yinan Li, Shaobin Zhuang, Huminhao Zhu, Yilin Zhang, Yanlong Tang, Chao Zhang, Hanbin Zhao, Hui Qian, Chen Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24222">https://arxiv.org/abs/2505.24222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24222">https://arxiv.org/pdf/2505.24222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24222]] Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin(https://arxiv.org/abs/2505.24222)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The diffusion models (DMs) have demonstrated the remarkable capability of generating images via learning the noised score function of data distribution. Current DM sampling techniques typically rely on first-order Langevin dynamics at each noise level, with efforts concentrated on refining inter-level denoising strategies. While leveraging additional second-order Hessian geometry to enhance the sampling quality of Langevin is a common practice in Markov chain Monte Carlo (MCMC), the naive attempts to utilize Hessian geometry in high-dimensional DMs lead to quadratic-complexity computational costs, rendering them non-scalable. In this work, we introduce a novel Levenberg-Marquardt-Langevin (LML) method that approximates the diffusion Hessian geometry in a training-free manner, drawing inspiration from the celebrated Levenberg-Marquardt optimization algorithm. Our approach introduces two key innovations: (1) A low-rank approximation of the diffusion Hessian, leveraging the DMs' inherent structure and circumventing explicit quadratic-complexity computations; (2) A damping mechanism to stabilize the approximated Hessian. This LML approximated Hessian geometry enables the diffusion sampling to execute more accurate steps and improve the image generation quality. We further conduct a theoretical analysis to substantiate the approximation error bound of low-rank approximation and the convergence property of the damping mechanism. Extensive experiments across multiple pretrained DMs validate that the LML method significantly improves image generation quality, with negligible computational overhead.</li>
</ul>

<h3>Title: Automated Structured Radiology Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Jean-Benoit Delbrouck, Justin Xu, Johannes Moll, Alois Thomas, Zhihong Chen, Sophie Ostmeier, Asfandyar Azhar, Kelvin Zhenghao Li, Andrew Johnston, Christian Bluethgen, Eduardo Reis, Mohamed Muneer, Maya Varma, Curtis Langlotz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24223">https://arxiv.org/abs/2505.24223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24223">https://arxiv.org/pdf/2505.24223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24223]] Automated Structured Radiology Report Generation(https://arxiv.org/abs/2505.24223)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automated radiology report generation from chest X-ray (CXR) images has the potential to improve clinical efficiency and reduce radiologists' workload. However, most datasets, including the publicly available MIMIC-CXR and CheXpert Plus, consist entirely of free-form reports, which are inherently variable and unstructured. This variability poses challenges for both generation and evaluation: existing models struggle to produce consistent, clinically meaningful reports, and standard evaluation metrics fail to capture the nuances of radiological interpretation. To address this, we introduce Structured Radiology Report Generation (SRRG), a new task that reformulates free-text radiology reports into a standardized format, ensuring clarity, consistency, and structured clinical reporting. We create a novel dataset by restructuring reports using large language models (LLMs) following strict structured reporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained disease classification model trained on 55 labels, enabling more precise and clinically informed evaluation of structured reports. To assess report quality, we propose F1-SRR-BERT, a metric that leverages SRR-BERT's hierarchical disease taxonomy to bridge the gap between free-text variability and structured clinical reporting. We validate our dataset through a reader study conducted by five board-certified radiologists and extensive benchmarking experiments.</li>
</ul>

<h3>Title: Reasoning Can Hurt the Inductive Abilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haibo Jin, Peiyan Zhang, Man Luo, Haohan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24225">https://arxiv.org/abs/2505.24225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24225">https://arxiv.org/pdf/2505.24225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24225]] Reasoning Can Hurt the Inductive Abilities of Large Language Models(https://arxiv.org/abs/2505.24225)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable progress across domains, yet their ability to perform inductive reasoning - inferring latent rules from sparse examples - remains limited. It is often assumed that chain-of-thought (CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such reasoning. We investigate this assumption with creating four controlled, diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack - with hidden human-defined rules. We find that CoT reasoning can degrade inductive performance, with LRMs often underperforming their non-reasoning counterparts. To explain this, we present a theoretical framework that reveals how reasoning steps can amplify error through three failure modes: incorrect sub-task decomposition, incorrect sub-task solving, and incorrect final answer summarization. Based on our theoretical and empirical analysis, we introduce structured interventions that adapt CoT generation according to our identified failure types. These interventions improve inductive accuracy without retraining. Our findings suggest that effective (CoT) reasoning depends not only on taking more steps but also on ensuring those steps are well-structured.</li>
</ul>

<h3>Title: Light as Deception: GPT-driven Natural Relighting Against Vision-Language Pre-training Models</h3>
<ul>
<li><strong>Authors: </strong>Ying Yang, Jie Zhang, Xiao Lv, Di Lin, Tao Xiang, Qing Guo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24227">https://arxiv.org/abs/2505.24227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24227">https://arxiv.org/pdf/2505.24227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24227]] Light as Deception: GPT-driven Natural Relighting Against Vision-Language Pre-training Models(https://arxiv.org/abs/2505.24227)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>While adversarial attacks on vision-and-language pretraining (VLP) models have been explored, generating natural adversarial samples crafted through realistic and semantically meaningful perturbations remains an open challenge. Existing methods, primarily designed for classification tasks, struggle when adapted to VLP models due to their restricted optimization spaces, leading to ineffective attacks or unnatural artifacts. To address this, we propose \textbf{LightD}, a novel framework that generates natural adversarial samples for VLP models via semantically guided relighting. Specifically, LightD leverages ChatGPT to propose context-aware initial lighting parameters and integrates a pretrained relighting model (IC-light) to enable diverse lighting adjustments. LightD expands the optimization space while ensuring perturbations align with scene semantics. Additionally, gradient-based optimization is applied to the reference lighting image to further enhance attack effectiveness while maintaining visual naturalness. The effectiveness and superiority of the proposed LightD have been demonstrated across various VLP models in tasks such as image captioning and visual question answering.</li>
</ul>

<h3>Title: Dynamic Context-Aware Streaming Pretrained Language Model For Inverse Text Normalization</h3>
<ul>
<li><strong>Authors: </strong>Luong Ho, Khanh Le, Vinh Pham, Bao Nguyen, Tan Tran, Duc Chau</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24229">https://arxiv.org/abs/2505.24229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24229">https://arxiv.org/pdf/2505.24229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24229]] Dynamic Context-Aware Streaming Pretrained Language Model For Inverse Text Normalization(https://arxiv.org/abs/2505.24229)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Inverse Text Normalization (ITN) is crucial for converting spoken Automatic Speech Recognition (ASR) outputs into well-formatted written text, enhancing both readability and usability. Despite its importance, the integration of streaming ITN within streaming ASR remains largely unexplored due to challenges in accuracy, efficiency, and adaptability, particularly in low-resource and limited-context scenarios. In this paper, we introduce a streaming pretrained language model for ITN, leveraging pretrained linguistic representations for improved robustness. To address streaming constraints, we propose Dynamic Context-Aware during training and inference, enabling adaptive chunk size adjustments and the integration of right-context information. Experimental results demonstrate that our method achieves accuracy comparable to non-streaming ITN and surpasses existing streaming ITN models on a Vietnamese dataset, all while maintaining low latency, ensuring seamless integration into ASR systems.</li>
</ul>

<h3>Title: Dynamic Malware Classification of Windows PE Files using CNNs and Greyscale Images Derived from Runtime API Call Argument Conversion</h3>
<ul>
<li><strong>Authors: </strong>Md Shahnawaz, Bishwajit Prasad Gond, Durga Prasad Mohapatra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24231">https://arxiv.org/abs/2505.24231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24231">https://arxiv.org/pdf/2505.24231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24231]] Dynamic Malware Classification of Windows PE Files using CNNs and Greyscale Images Derived from Runtime API Call Argument Conversion(https://arxiv.org/abs/2505.24231)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Malware detection and classification remains a topic of concern for cybersecurity, since it is becoming common for attackers to use advanced obfuscation on their malware to stay undetected. Conventional static analysis is not effective against polymorphic and metamorphic malware as these change their appearance without modifying their behavior, thus defying the analysis by code structure alone. This makes it important to use dynamic detection that monitors malware behavior at runtime. In this paper, we present a dynamic malware categorization framework that extracts API argument calls at the runtime execution of Windows Portable Executable (PE) files. Extracting and encoding the dynamic features of API names, argument return values, and other relative features, we convert raw behavioral data to temporal patterns. To enhance feature portrayal, the generated patterns are subsequently converted into grayscale pictures using a magma colormap. These improved photos are used to teach a Convolutional Neural Network (CNN) model discriminative features, which allows for reliable and accurate malware classification. Results from experiments indicate that our method, with an average accuracy of 98.36% is effective in classifying different classes of malware and benign by integrating dynamic analysis and deep learning. It not only achieves high classification accuracy but also demonstrates significant resilience against typical evasion strategies.</li>
</ul>

<h3>Title: From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Haibo Jin, Peiyan Zhang, Peiran Wang, Man Luo, Haohan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24232">https://arxiv.org/abs/2505.24232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24232">https://arxiv.org/pdf/2505.24232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24232]] From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models(https://arxiv.org/abs/2505.24232)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Large foundation models (LFMs) are susceptible to two distinct vulnerabilities: hallucinations and jailbreak attacks. While typically studied in isolation, we observe that defenses targeting one often affect the other, hinting at a deeper connection. We propose a unified theoretical framework that models jailbreaks as token-level optimization and hallucinations as attention-level optimization. Within this framework, we establish two key propositions: (1) \textit{Similar Loss Convergence} - the loss functions for both vulnerabilities converge similarly when optimizing for target-specific outputs; and (2) \textit{Gradient Consistency in Attention Redistribution} - both exhibit consistent gradient behavior driven by shared attention dynamics. We validate these propositions empirically on LLaVA-1.5 and MiniGPT-4, showing consistent optimization trends and aligned gradients. Leveraging this connection, we demonstrate that mitigation techniques for hallucinations can reduce jailbreak success rates, and vice versa. Our findings reveal a shared failure mode in LFMs and suggest that robustness strategies should jointly address both vulnerabilities.</li>
</ul>

<h3>Title: MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM</h3>
<ul>
<li><strong>Authors: </strong>Bowen Dong, Minheng Ni, Zitong Huang, Guanglei Yang, Wangmeng Zuo, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24238">https://arxiv.org/abs/2505.24238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24238">https://arxiv.org/pdf/2505.24238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24238]] MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM(https://arxiv.org/abs/2505.24238)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal hallucination in multimodal large language models (MLLMs) restricts the correctness of MLLMs. However, multimodal hallucinations are multi-sourced and arise from diverse causes. Existing benchmarks fail to adequately distinguish between perception-induced hallucinations and reasoning-induced hallucinations. This failure constitutes a significant issue and hinders the diagnosis of multimodal reasoning failures within MLLMs. To address this, we propose the {\dataset} benchmark, which isolates reasoning hallucinations by constructing questions where input images are correctly perceived by MLLMs yet reasoning errors persist. {\dataset} introduces multi-granular evaluation metrics: accuracy, factuality, and LLMs hallucination score for hallucination quantification. Our analysis reveals that (1) the model scale, data scale, and training stages significantly affect the degree of logical, fabrication, and factual hallucinations; (2) current MLLMs show no effective improvement on spatial hallucinations caused by misinterpreted spatial relationships, indicating their limited visual reasoning capabilities; and (3) question types correlate with distinct hallucination patterns, highlighting targeted challenges and potential mitigation strategies. To address these challenges, we propose {\method}, a method that combines curriculum reinforcement fine-tuning to encourage models to generate logic-consistent reasoning chains by stepwise reducing learning difficulty, and collaborative hint inference to reduce reasoning complexity. {\method} establishes a baseline on {\dataset}, and reduces the logical hallucinations in original base models.</li>
</ul>

<h3>Title: Advantageous Parameter Expansion Training Makes Better Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Naibin Gu, Yilong Chen, Zhenyu Zhang, Peng Fu, Zheng Lin, Shuohuan Wang, Yu Sun, Hua Wu, Weiping Wang, Haifeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24241">https://arxiv.org/abs/2505.24241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24241">https://arxiv.org/pdf/2505.24241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24241]] Advantageous Parameter Expansion Training Makes Better Large Language Models(https://arxiv.org/abs/2505.24241)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although scaling up the number of trainable parameters in both pre-training and fine-tuning can effectively improve the performance of large language models, it also leads to increased computational overhead. When delving into the parameter difference, we find that a subset of parameters, termed advantageous parameters, plays a crucial role in determining model performance. Further analysis reveals that stronger models tend to possess more such parameters. In this paper, we propose Advantageous Parameter EXpansion Training (APEX), a method that progressively expands advantageous parameters into the space of disadvantageous ones, thereby increasing their proportion and enhancing training effectiveness. Further theoretical analysis from the perspective of matrix effective rank explains the performance gains of APEX. Extensive experiments on both instruction tuning and continued pre-training demonstrate that, in instruction tuning, APEX outperforms full-parameter tuning while using only 52% of the trainable parameters. In continued pre-training, APEX achieves the same perplexity level as conventional training with just 33% of the training data, and yields significant improvements on downstream tasks.</li>
</ul>

<h3>Title: Mamba Knockout for Unraveling Factual Information Flow</h3>
<ul>
<li><strong>Authors: </strong>Nir Endy, Idan Daniel Grosbard, Yuval Ran-Milo, Yonatan Slutzky, Itay Tshuva, Raja Giryes</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24244">https://arxiv.org/abs/2505.24244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24244">https://arxiv.org/pdf/2505.24244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24244]] Mamba Knockout for Unraveling Factual Information Flow(https://arxiv.org/abs/2505.24244)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>This paper investigates the flow of factual information in Mamba State-Space Model (SSM)-based language models. We rely on theoretical and empirical connections to Transformer-based architectures and their attention mechanisms. Exploiting this relationship, we adapt attentional interpretability techniques originally developed for Transformers--specifically, the Attention Knockout methodology--to both Mamba-1 and Mamba-2. Using them we trace how information is transmitted and localized across tokens and layers, revealing patterns of subject-token information emergence and layer-wise dynamics. Notably, some phenomena vary between mamba models and Transformer based models, while others appear universally across all models inspected--hinting that these may be inherent to LLMs in general. By further leveraging Mamba's structured factorization, we disentangle how distinct "features" either enable token-to-token information exchange or enrich individual tokens, thus offering a unified lens to understand Mamba internal operations.</li>
</ul>

<h3>Title: LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework</h3>
<ul>
<li><strong>Authors: </strong>Xin Kang, Zihan Zheng, Lei Chu, Yue Gao, Jiahao Li, Hao Pan, Xuejin Chen, Yan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24245">https://arxiv.org/abs/2505.24245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24245">https://arxiv.org/pdf/2505.24245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24245]] LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework(https://arxiv.org/abs/2505.24245)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present LTM3D, a Latent Token space Modeling framework for conditional 3D shape generation that integrates the strengths of diffusion and auto-regressive (AR) models. While diffusion-based methods effectively model continuous latent spaces and AR models excel at capturing inter-token dependencies, combining these paradigms for 3D shape generation remains a challenge. To address this, LTM3D features a Conditional Distribution Modeling backbone, leveraging a masked autoencoder and a diffusion model to enhance token dependency learning. Additionally, we introduce Prefix Learning, which aligns condition tokens with shape latent tokens during generation, improving flexibility across modalities. We further propose a Latent Token Reconstruction module with Reconstruction-Guided Sampling to reduce uncertainty and enhance structural fidelity in generated shapes. Our approach operates in token space, enabling support for multiple 3D representations, including signed distance fields, point clouds, meshes, and 3D Gaussian Splatting. Extensive experiments on image- and text-conditioned shape generation tasks demonstrate that LTM3D outperforms existing methods in prompt fidelity and structural accuracy while offering a generalizable framework for multi-modal, multi-representation 3D generation.</li>
</ul>

<h3>Title: Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization</h3>
<ul>
<li><strong>Authors: </strong>Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Hongbin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24249">https://arxiv.org/abs/2505.24249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24249">https://arxiv.org/pdf/2505.24249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24249]] Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization(https://arxiv.org/abs/2505.24249)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-based 6-DOF bronchoscopy localization offers a promising solution for accurate and cost-effective interventional guidance. However, existing methods struggle with 1) limited generalization across patient cases due to scarce labeled data, and 2) poor robustness under visual degradation, as bronchoscopy procedures frequently involve artifacts such as occlusions and motion blur that impair visual information. To address these challenges, we propose PANSv2, a generalizable and robust bronchoscopy localization framework. Motivated by PANS that leverages multiple visual cues for pose likelihood measurement, PANSv2 integrates depth estimation, landmark detection, and centerline constraints into a unified pose optimization framework that evaluates pose probability and solves for the optimal bronchoscope pose. To further enhance generalization capabilities, we leverage the endoscopic foundation model EndoOmni for depth estimation and the video foundation model EndoMamba for landmark detection, incorporating both spatial and temporal analyses. Pretrained on diverse endoscopic datasets, these models provide stable and transferable visual representations, enabling reliable performance across varied bronchoscopy scenarios. Additionally, to improve robustness to visual degradation, we introduce an automatic re-initialization module that detects tracking failures and re-establishes pose using landmark detections once clear views are available. Experimental results on bronchoscopy dataset encompassing 10 patient cases show that PANSv2 achieves the highest tracking success rate, with an 18.1% improvement in SR-5 (percentage of absolute trajectory error under 5 mm) compared to existing methods, showing potential towards real clinical usage.</li>
</ul>

<h3>Title: Proactive Guidance of Multi-Turn Conversation in Industrial Search</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Li, Xiao Li, Li Gao, Yiding Liu, Xiaoyang Wang, Shuaiqiang Wang, Junfeng Wang, Dawei Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24251">https://arxiv.org/abs/2505.24251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24251">https://arxiv.org/pdf/2505.24251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24251]] Proactive Guidance of Multi-Turn Conversation in Industrial Search(https://arxiv.org/abs/2505.24251)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The evolution of Large Language Models (LLMs) has significantly advanced multi-turn conversation systems, emphasizing the need for proactive guidance to enhance users' interactions. However, these systems face challenges in dynamically adapting to shifts in users' goals and maintaining low latency for real-time interactions. In the Baidu Search AI assistant, an industrial-scale multi-turn search system, we propose a novel two-phase framework to provide proactive guidance. The first phase, Goal-adaptive Supervised Fine-Tuning (G-SFT), employs a goal adaptation agent that dynamically adapts to user goal shifts and provides goal-relevant contextual information. G-SFT also incorporates scalable knowledge transfer to distill insights from LLMs into a lightweight model for real-time interaction. The second phase, Click-oriented Reinforcement Learning (C-RL), adopts a generate-rank paradigm, systematically constructs preference pairs from user click signals, and proactively improves click-through rates through more engaging guidance. This dual-phase architecture achieves complementary objectives: G-SFT ensures accurate goal tracking, while C-RL optimizes interaction quality through click signal-driven reinforcement learning. Extensive experiments demonstrate that our framework achieves 86.10% accuracy in offline evaluation (+23.95% over baseline) and 25.28% CTR in online deployment (149.06% relative improvement), while reducing inference latency by 69.55% through scalable knowledge distillation.</li>
</ul>

<h3>Title: A Reward-driven Automated Webshell Malicious-code Generator for Red-teaming</h3>
<ul>
<li><strong>Authors: </strong>Yizhong Ding</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24252">https://arxiv.org/abs/2505.24252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24252">https://arxiv.org/pdf/2505.24252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24252]] A Reward-driven Automated Webshell Malicious-code Generator for Red-teaming(https://arxiv.org/abs/2505.24252)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Frequent cyber-attacks have elevated WebShell exploitation and defense to a critical research focus within network security. However, there remains a significant shortage of publicly available, well-categorized malicious-code datasets organized by obfuscation method. Existing malicious-code generation methods, which primarily rely on prompt engineering, often suffer from limited diversity and high redundancy in the payloads they produce. To address these limitations, we propose \textbf{RAWG}, a \textbf{R}eward-driven \textbf{A}utomated \textbf{W}ebshell Malicious-code \textbf{G}enerator designed for red-teaming applications. Our approach begins by categorizing webshell samples from common datasets into seven distinct types of obfuscation. We then employ a large language model (LLM) to extract and normalize key tokens from each sample, creating a standardized, high-quality corpus. Using this curated dataset, we perform supervised fine-tuning (SFT) on an open-source large model to enable the generation of diverse, highly obfuscated webshell malicious payloads. To further enhance generation quality, we apply Proximal Policy Optimization (PPO), treating malicious-code samples as "chosen" data and benign code as "rejected" data during reinforcement learning. Extensive experiments demonstrate that RAWG significantly outperforms current state-of-the-art methods in both payload diversity and escape effectiveness.</li>
</ul>

<h3>Title: Interactive Video Generation via Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Ishaan Rawal, Suryansh Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24253">https://arxiv.org/abs/2505.24253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24253">https://arxiv.org/pdf/2505.24253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24253]] Interactive Video Generation via Domain Adaptation(https://arxiv.org/abs/2505.24253)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-conditioned diffusion models have emerged as powerful tools for high-quality video generation. However, enabling Interactive Video Generation (IVG), where users control motion elements such as object trajectory, remains challenging. Recent training-free approaches introduce attention masking to guide trajectory, but this often degrades perceptual quality. We identify two key failure modes in these methods, both of which we interpret as domain shift problems, and propose solutions inspired by domain adaptation. First, we attribute the perceptual degradation to internal covariate shift induced by attention masking, as pretrained models are not trained to handle masked attention. To address this, we propose mask normalization, a pre-normalization layer designed to mitigate this shift via distribution matching. Second, we address initialization gap, where the randomly sampled initial noise does not align with IVG conditioning, by introducing a temporal intrinsic diffusion prior that enforces spatio-temporal consistency at each denoising step. Extensive qualitative and quantitative evaluations demonstrate that mask normalization and temporal intrinsic denoising improve both perceptual quality and trajectory control over the existing state-of-the-art IVG techniques.</li>
</ul>

<h3>Title: Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games</h3>
<ul>
<li><strong>Authors: </strong>Neemesh Yadav, Palakorn Achananuparp, Jing Jiang, Ee-Peng Lim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24255">https://arxiv.org/abs/2505.24255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24255">https://arxiv.org/pdf/2505.24255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24255]] Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games(https://arxiv.org/abs/2505.24255)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown potential in simulating human behaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for complex social interactions. In this study, we investigate the role of ToM reasoning in aligning agentic behaviors with human norms in negotiation tasks, using the ultimatum game as a controlled environment. We initialized LLM agents with different prosocial beliefs (including Greedy, Fair, and Selfless) and reasoning methods like chain-of-thought (CoT) and varying ToM levels, and examined their decision-making processes across diverse LLMs, including reasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from 2,700 simulations indicated that ToM reasoning enhances behavior alignment, decision-making consistency, and negotiation outcomes. Consistent with previous findings, reasoning models exhibit limited capability compared to models with ToM reasoning, different roles of the game benefits with different orders of ToM reasoning. Our findings contribute to the understanding of ToM's role in enhancing human-AI interaction and cooperative decision-making. The code used for our experiments can be found at this https URL.</li>
</ul>

<h3>Title: Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames</h3>
<ul>
<li><strong>Authors: </strong>Sahithya Ravi, Gabriel Sarch, Vibhav Vineet, Andrew D. Wilson, Balasaravanan Thoravi Kumaravel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24257">https://arxiv.org/abs/2505.24257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24257">https://arxiv.org/pdf/2505.24257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24257]] Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames(https://arxiv.org/abs/2505.24257)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>An embodied AI assistant operating on egocentric video must integrate spatial cues across time - for instance, determining where an object A, glimpsed a few moments ago lies relative to an object B encountered later. We introduce Disjoint-3DQA , a generative QA benchmark that evaluates this ability of VLMs by posing questions about object pairs that are not co-visible in the same frame. We evaluated seven state-of-the-art VLMs and found that models lag behind human performance by 28%, with steeper declines in accuracy (60% to 30 %) as the temporal gap widens. Our analysis further reveals that providing trajectories or bird's-eye-view projections to VLMs results in only marginal improvements, whereas providing oracle 3D coordinates leads to a substantial 20% performance increase. This highlights a core bottleneck of multi-frame VLMs in constructing and maintaining 3D scene representations over time from visual signals. Disjoint-3DQA therefore sets a clear, measurable challenge for long-horizon spatial reasoning and aims to catalyze future research at the intersection of vision, language, and embodied AI.</li>
</ul>

<h3>Title: On Fairness of Task Arithmetic: The Role of Task Vectors</h3>
<ul>
<li><strong>Authors: </strong>Hiroki Naganuma, Kotaro Yoshida, Laura Gomezjurado Gonzalez, Takafumi Horie, Yuji Naraki, Ryotaro Shimizu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24262">https://arxiv.org/abs/2505.24262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24262">https://arxiv.org/pdf/2505.24262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24262]] On Fairness of Task Arithmetic: The Role of Task Vectors(https://arxiv.org/abs/2505.24262)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Model editing techniques, particularly task arithmetic using task vectors, have shown promise in efficiently modifying pre-trained models through arithmetic operations like task addition and negation. Despite computational advantages, these methods may inadvertently affect model fairness, creating risks in sensitive applications like hate speech detection. However, the fairness implications of task arithmetic remain largely unexplored, presenting a critical gap in the existing literature. We systematically examine how manipulating task vectors affects fairness metrics, including Demographic Parity and Equalized Odds. To rigorously assess these effects, we benchmark task arithmetic against full fine-tuning, a costly but widely used baseline, and Low-Rank Adaptation (LoRA), a prevalent parameter-efficient fine-tuning method. Additionally, we explore merging task vectors from models fine-tuned on demographic subgroups vulnerable to hate speech, investigating whether fairness outcomes can be controlled by adjusting task vector coefficients, potentially enabling tailored model behavior. Our results offer novel insights into the fairness implications of model editing and establish a foundation for fairness-aware and responsible model editing practices.</li>
</ul>

<h3>Title: Simulating Training Data Leakage in Multiple-Choice Benchmarks for LLM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Naila Shafirni Hidayat, Muhammad Dehan Al Kautsar, Alfan Farizki Wicaksono, Fajri Koto</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24263">https://arxiv.org/abs/2505.24263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24263">https://arxiv.org/pdf/2505.24263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24263]] Simulating Training Data Leakage in Multiple-Choice Benchmarks for LLM Evaluation(https://arxiv.org/abs/2505.24263)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The performance of large language models (LLMs) continues to improve, as reflected in rising scores on standard benchmarks. However, the lack of transparency around training data raises concerns about potential overlap with evaluation sets and the fairness of reported results. Although prior work has proposed methods for detecting data leakage, these approaches primarily focus on identifying outliers and have not been evaluated under controlled simulated leakage conditions. In this work, we compare existing leakage detection techniques, namely permutation and n-gram-based methods, under a continual pretraining setup that simulates real-world leakage scenarios, and additionally explore a lightweight method we call semi-half question. Although semi-half offers a low-cost alternative, our analysis shows that the n-gram method consistently achieves the highest F1-score. We also refine these techniques to support instance-level detection and reduce computational overhead. Leveraging the best-performing method, we create cleaned versions of MMLU and HellaSwag, and re-evaluate several LLMs. Our findings present a practical path toward more reliable and transparent evaluations, and we recommend contamination checks as a standard step before releasing benchmark results.</li>
</ul>

<h3>Title: Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations</h3>
<ul>
<li><strong>Authors: </strong>Xin Quan, Marco Valentino, Louise A. Dennis, AndrÃ© Freitas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24264">https://arxiv.org/abs/2505.24264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24264">https://arxiv.org/pdf/2505.24264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24264]] Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations(https://arxiv.org/abs/2505.24264)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Natural language explanations play a fundamental role in Natural Language Inference (NLI) by revealing how premises logically entail hypotheses. Recent work has shown that the interaction of large language models (LLMs) with theorem provers (TPs) can help verify and improve the validity of NLI explanations. However, TPs require translating natural language into machine-verifiable formal representations, a process that introduces the risk of semantic information loss and unfaithful interpretation, an issue compounded by LLMs' challenges in capturing critical logical structures with sufficient precision. Moreover, LLMs are still limited in their capacity for rigorous and robust proof construction within formal verification frameworks. To mitigate issues related to faithfulness and robustness, this paper investigates strategies to (1) alleviate semantic loss during autoformalisation, (2) efficiently identify and correct syntactic errors in logical representations, (3) explicitly use logical expressions to guide LLMs in generating structured proof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback for iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree using different LLMs demonstrate that the proposed strategies yield significant improvements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation refinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover, we show that specific interventions on the hybrid LLM-TP architecture can substantially improve efficiency, drastically reducing the number of iterations required for successful verification.</li>
</ul>

<h3>Title: MUSE: Model-Agnostic Tabular Watermarking via Multi-Sample Selection</h3>
<ul>
<li><strong>Authors: </strong>Liancheng Fang, Aiwei Liu, Henry Peng Zou, Yankai Chen, Hengrui Zhang, Zhongfen Deng, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24267">https://arxiv.org/abs/2505.24267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24267">https://arxiv.org/pdf/2505.24267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24267]] MUSE: Model-Agnostic Tabular Watermarking via Multi-Sample Selection(https://arxiv.org/abs/2505.24267)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark, diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce MUSE, a watermarking algorithm for tabular generative models. Previous approaches typically leverage DDIM invertibility to watermark tabular diffusion models, but tabular diffusion models exhibit significantly poorer invertibility compared to other modalities, compromising performance. Simultaneously, tabular diffusion models require substantially less computation than other modalities, enabling a multi-sample selection approach to tabular generative model watermarking. MUSE embeds watermarks by generating multiple candidate samples and selecting one based on a specialized scoring function, without relying on model invertibility. Our theoretical analysis establishes the relationship between watermark detectability, candidate count, and dataset size, allowing precise calibration of watermarking strength. Extensive experiments demonstrate that MUSE achieves state-of-the-art watermark detectability and robustness against various attacks while maintaining data quality, and remains compatible with any tabular generative model supporting repeated sampling, effectively addressing key challenges in tabular data watermarking. Specifically, it reduces the distortion rates on fidelity metrics by 81-89%, while achieving a 1.0 TPR@0.1%FPR detection rate. Implementation of MUSE can be found at this https URL.</li>
</ul>

<h3>Title: Transaction Proximity: A Graph-Based Approach to Blockchain Fraud Prevention</h3>
<ul>
<li><strong>Authors: </strong>Gordon Y. Liao, Ziming Zeng, Mira Belenkiy, Jacob Hirshman</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CE, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24284">https://arxiv.org/abs/2505.24284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24284">https://arxiv.org/pdf/2505.24284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24284]] Transaction Proximity: A Graph-Based Approach to Blockchain Fraud Prevention(https://arxiv.org/abs/2505.24284)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>This paper introduces a fraud-deterrent access validation system for public blockchains, leveraging two complementary concepts: "Transaction Proximity", which measures the distance between wallets in the transaction graph, and "Easily Attainable Identities (EAIs)", wallets with direct transaction connections to centralized exchanges. Recognizing the limitations of traditional approaches like blocklisting (reactive, slow) and strict allow listing (privacy-invasive, adoption barriers), we propose a system that analyzes transaction patterns to identify wallets with close connections to centralized exchanges. Our directed graph analysis of the Ethereum blockchain reveals that 56% of large USDC wallets (with a lifetime maximum balance greater than \$10,000) are EAI and 88% are within one transaction hop of an EAI. For transactions exceeding \$2,000, 91% involve at least one EAI. Crucially, an analysis of past exploits shows that 83% of the known exploiter addresses are not EAIs, with 21% being more than five hops away from any regulated exchange. We present three implementation approaches with varying gas cost and privacy tradeoffs, demonstrating that EAI-based access control can potentially prevent most of these incidents while preserving blockchain openness. Importantly, our approach does not restrict access or share personally identifiable information, but it provides information for protocols to implement their own validation or risk scoring systems based on specific needs. This middle-ground solution enables programmatic compliance while maintaining the core values of open blockchain.</li>
</ul>

<h3>Title: EgoExOR: An Ego-Exo-Centric Operating Room Dataset for Surgical Activity Understanding</h3>
<ul>
<li><strong>Authors: </strong>Ege Ãzsoy, Arda Mamur, Felix Tristram, Chantal Pellegrini, Magdalena Wysocki, Benjamin Busam, Nassir Navab</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24287">https://arxiv.org/abs/2505.24287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24287">https://arxiv.org/pdf/2505.24287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24287]] EgoExOR: An Ego-Exo-Centric Operating Room Dataset for Surgical Activity Understanding(https://arxiv.org/abs/2505.24287)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Operating rooms (ORs) demand precise coordination among surgeons, nurses, and equipment in a fast-paced, occlusion-heavy environment, necessitating advanced perception models to enhance safety and efficiency. Existing datasets either provide partial egocentric views or sparse exocentric multi-view context, but do not explore the comprehensive combination of both. We introduce EgoExOR, the first OR dataset and accompanying benchmark to fuse first-person and third-person perspectives. Spanning 94 minutes (84,553 frames at 15 FPS) of two emulated spine procedures, Ultrasound-Guided Needle Insertion and Minimally Invasive Spine Surgery, EgoExOR integrates egocentric data (RGB, gaze, hand tracking, audio) from wearable glasses, exocentric RGB and depth from RGB-D cameras, and ultrasound imagery. Its detailed scene graph annotations, covering 36 entities and 22 relations (568,235 triplets), enable robust modeling of clinical interactions, supporting tasks like action recognition and human-centric perception. We evaluate the surgical scene graph generation performance of two adapted state-of-the-art models and offer a new baseline that explicitly leverages EgoExOR's multimodal and multi-perspective signals. This new dataset and benchmark set a new foundation for OR perception, offering a rich, multimodal resource for next-generation clinical perception.</li>
</ul>

<h3>Title: Verifiable Weighted Secret Sharing</h3>
<ul>
<li><strong>Authors: </strong>Kareem Shehata, Han Fangqi, Sri AravindaKrishnan Thyagarajan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24289">https://arxiv.org/abs/2505.24289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24289">https://arxiv.org/pdf/2505.24289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24289]] Verifiable Weighted Secret Sharing(https://arxiv.org/abs/2505.24289)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Traditionally, threshold secret sharing (TSS) schemes assume all parties have equal weight, yet emerging systems like blockchains reveal disparities in party trustworthiness, such as stake or reputation. Weighted Secret Sharing (WSS) addresses this by assigning varying weights to parties, ensuring security even if adversaries control parties with total weight at most a threshold $t$. Current WSS schemes assume honest dealers, resulting in security from only honest-but-curious behaviour but not protection from malicious adversaries for downstream applications. \emph{Verifiable} secret sharing (VSS) is a well-known technique to address this, but existing VSS schemes are either tailored to TSS, or require additional trust assumptions. We propose the first efficient verifiable WSS scheme that tolerates malicious dealers and is compatible with the latest CRT-based WSS~\cite{crypto_w_weights}. Our solution uses Bulletproofs for efficient verification and introduces new privacy-preserving techniques for proving relations between committed values, which may be of independent interest. Evaluation on Ethereum show up to a $100\times$ improvement in communication complexity compared to the current design and $20\times$ improvement compared to unweighted VSS schemes.</li>
</ul>

<h3>Title: Large Language Models are Locally Linear Mappings</h3>
<ul>
<li><strong>Authors: </strong>James R. Golden</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24293">https://arxiv.org/abs/2505.24293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24293">https://arxiv.org/pdf/2505.24293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24293]] Large Language Models are Locally Linear Mappings(https://arxiv.org/abs/2505.24293)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process.</li>
</ul>

<h3>Title: AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, Yi Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24298">https://arxiv.org/abs/2505.24298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24298">https://arxiv.org/pdf/2505.24298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24298]] AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning(https://arxiv.org/abs/2505.24298)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a \emph{fully asynchronous} RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves \textbf{up to 2.57$\times$ training speedup} compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at this https URL.</li>
</ul>

<h3>Title: Category-aware EEG image generation based on wavelet transform and contrast semantic loss</h3>
<ul>
<li><strong>Authors: </strong>Enshang Zhang, Zhicheng Zhang, Takashi Hanakawa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24301">https://arxiv.org/abs/2505.24301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24301">https://arxiv.org/pdf/2505.24301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24301]] Category-aware EEG image generation based on wavelet transform and contrast semantic loss(https://arxiv.org/abs/2505.24301)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Reconstructing visual stimuli from EEG signals is a crucial step in realizing brain-computer interfaces. In this paper, we propose a transformer-based EEG signal encoder integrating the Discrete Wavelet Transform (DWT) and the gating mechanism. Guided by the feature alignment and category-aware fusion losses, this encoder is used to extract features related to visual stimuli from EEG signals. Subsequently, with the aid of a pre-trained diffusion model, these features are reconstructed into visual stimuli. To verify the effectiveness of the model, we conducted EEG-to-image generation and classification tasks using the THINGS-EEG dataset. To address the limitations of quantitative analysis at the semantic level, we combined WordNet-based classification and semantic similarity metrics to propose a novel semantic-based score, emphasizing the ability of our model to transfer neural activities into visual representations. Experimental results show that our model significantly improves semantic alignment and classification accuracy, which achieves a maximum single-subject accuracy of 43\%, outperforming other state-of-the-art methods. The source code and supplementary material is available at this https URL.</li>
</ul>

<h3>Title: ScienceMeter: Tracking Scientific Knowledge Updates in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yike Wang, Shangbin Feng, Yulia Tsvetkov, Hannaneh Hajishirzi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24302">https://arxiv.org/abs/2505.24302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24302">https://arxiv.org/pdf/2505.24302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24302]] ScienceMeter: Tracking Scientific Knowledge Updates in Language Models(https://arxiv.org/abs/2505.24302)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used to support scientific research, but their knowledge of scientific advancements can quickly become outdated. We introduce ScienceMeter, a new framework for evaluating scientific knowledge update methods over scientific knowledge spanning the past, present, and future. ScienceMeter defines three metrics: knowledge preservation, the extent to which models' understanding of previously learned papers are preserved; knowledge acquisition, how well scientific claims from newly introduced papers are acquired; and knowledge projection, the ability of the updated model to anticipate or generalize to related scientific claims that may emerge in the future. Using ScienceMeter, we examine the scientific knowledge of LLMs on claim judgment and generation tasks across a curated dataset of 15,444 scientific papers and 30,888 scientific claims from ten domains including medicine, biology, materials science, and computer science. We evaluate five representative knowledge update approaches including training- and inference-time methods. With extensive experiments, we find that the best-performing knowledge update methods can preserve only 85.9% of existing knowledge, acquire 71.7% of new knowledge, and project 37.7% of future knowledge. Inference-based methods work for larger models, whereas smaller models require training to achieve comparable performance. Cross-domain analysis reveals that performance on these objectives is correlated. Even when applying on specialized scientific LLMs, existing knowledge update methods fail to achieve these objectives collectively, underscoring that developing robust scientific knowledge update mechanisms is both crucial and challenging.</li>
</ul>

<h3>Title: InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing</h3>
<ul>
<li><strong>Authors: </strong>Jinlu Zhang, Yixin Chen, Zan Wang, Jie Yang, Yizhou Wang, Siyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24315">https://arxiv.org/abs/2505.24315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24315">https://arxiv.org/pdf/2505.24315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24315]] InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing(https://arxiv.org/abs/2505.24315)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in 3D human-aware generation have made significant progress. However, existing methods still struggle with generating novel Human Object Interaction (HOI) from text, particularly for open-set objects. We identify three main challenges of this task: precise human-object relation reasoning, affordance parsing for any object, and detailed human interaction pose synthesis aligning description and object geometry. In this work, we propose a novel zero-shot 3D HOI generation framework without training on specific datasets, leveraging the knowledge from large-scale pre-trained models. Specifically, the human-object relations are inferred from large language models (LLMs) to initialize object properties and guide the optimization process. Then we utilize a pre-trained 2D image diffusion model to parse unseen objects and extract contact points, avoiding the limitations imposed by existing 3D asset knowledge. The initial human pose is generated by sampling multiple hypotheses through multi-view SDS based on the input text and object geometry. Finally, we introduce a detailed optimization to generate fine-grained, precise, and natural interaction, enforcing realistic 3D contact between the 3D object and the involved body parts, including hands in grasping. This is achieved by distilling human-level feedback from LLMs to capture detailed human-object relations from the text instruction. Extensive experiments validate the effectiveness of our approach compared to prior works, particularly in terms of the fine-grained nature of interactions and the ability to handle open-set 3D objects.</li>
</ul>

<h3>Title: ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Yongming Chen, Miner Chen, Liewen Liao, Mingyang Jiang, Xiang Zuo, Hengrui Zhang, Yuchen Xi, Songan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24317">https://arxiv.org/abs/2505.24317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24317">https://arxiv.org/pdf/2505.24317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24317]] ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving(https://arxiv.org/abs/2505.24317)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) in autonomous driving employs a trial-and-error mechanism, enhancing robustness in unpredictable environments. However, crafting effective reward functions remains challenging, as conventional approaches rely heavily on manual design and demonstrate limited efficacy in complex scenarios. To address this issue, this study introduces a responsibility-oriented reward function that explicitly incorporates traffic regulations into the RL framework. Specifically, we introduced a Traffic Regulation Knowledge Graph and leveraged Vision-Language Models alongside Retrieval-Augmented Generation techniques to automate reward assignment. This integration guides agents to adhere strictly to traffic laws, thus minimizing rule violations and optimizing decision-making performance in diverse driving conditions. Experimental validations demonstrate that the proposed methodology significantly improves the accuracy of assigning accident responsibilities and effectively reduces the agent's liability in traffic incidents.</li>
</ul>

<h3>Title: HiCaM: A Hierarchical-Causal Modification Framework for Long-Form Text Modification</h3>
<ul>
<li><strong>Authors: </strong>Yuntao Shi, Yi Luo, Yeyun Gong, Chen Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24319">https://arxiv.org/abs/2505.24319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24319">https://arxiv.org/pdf/2505.24319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24319]] HiCaM: A Hierarchical-Causal Modification Framework for Long-Form Text Modification(https://arxiv.org/abs/2505.24319)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success in various domains. However, when handling long-form text modification tasks, they still face two major problems: (1) producing undesired modifications by inappropriately altering or summarizing irrelevant content, and (2) missing necessary modifications to implicitly related passages that are crucial for maintaining document coherence. To address these issues, we propose HiCaM, a Hierarchical-Causal Modification framework that operates through a hierarchical summary tree and a causal graph. Furthermore, to evaluate HiCaM, we derive a multi-domain dataset from various benchmarks, providing a resource for assessing its effectiveness. Comprehensive evaluations on the dataset demonstrate significant improvements over strong LLMs, with our method achieving up to a 79.50\% win rate. These results highlight the comprehensiveness of our approach, showing consistent performance improvements across multiple models and domains.</li>
</ul>

<h3>Title: SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Ivan Petrukha, Yana Kurliak, Nataliia Stulova</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.PL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24324">https://arxiv.org/abs/2505.24324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24324">https://arxiv.org/pdf/2505.24324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24324]] SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation(https://arxiv.org/abs/2505.24324)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, large language models (LLMs) have showcased significant advancements in code generation. However, most evaluation benchmarks are primarily oriented towards Python, making it difficult to evaluate other programming languages, such as Swift, with high quality. By examining widely established multilingual benchmarks like HumanEval-XL and MultiPL-E, we identified critical issues specific to their Swift components, making them insufficient or even irrelevant for assessing LLM coding capabilities on Swift. Unlike these existing approaches, which prioritize rapid scaling and generalization by automatically translating Python-centric benchmarks with LLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the first Swift-oriented benchmark consisting of 28 carefully hand-crafted problems, and evaluate 44 popular Code LLMs on it. Our results show significant LLM scores drop for problems requiring language-specific features, most noticeable in the models of smaller sizes.</li>
</ul>

<h3>Title: STAR-Net: An Interpretable Model-Aided Network for Remote Sensing Image Denoising</h3>
<ul>
<li><strong>Authors: </strong>Jingjing Liu, Jiashun Jin, Xianchao Xiu, Jianhua Zhang, Wanquan Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24327">https://arxiv.org/abs/2505.24327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24327">https://arxiv.org/pdf/2505.24327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24327]] STAR-Net: An Interpretable Model-Aided Network for Remote Sensing Image Denoising(https://arxiv.org/abs/2505.24327)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Remote sensing image (RSI) denoising is an important topic in the field of remote sensing. Despite the impressive denoising performance of RSI denoising methods, most current deep learning-based approaches function as black boxes and lack integration with physical information models, leading to limited interpretability. Additionally, many methods may struggle with insufficient attention to non-local self-similarity in RSI and require tedious tuning of regularization parameters to achieve optimal performance, particularly in conventional iterative optimization approaches. In this paper, we first propose a novel RSI denoising method named sparse tensor-aided representation network (STAR-Net), which leverages a low-rank prior to effectively capture the non-local self-similarity within RSI. Furthermore, we extend STAR-Net to a sparse variant called STAR-Net-S to deal with the interference caused by non-Gaussian noise in original RSI for the purpose of improving robustness. Different from conventional iterative optimization, we develop an alternating direction method of multipliers (ADMM)-guided deep unrolling network, in which all regularization parameters can be automatically learned, thus inheriting the advantages of both model-based and deep learning-based approaches and successfully addressing the above-mentioned shortcomings. Comprehensive experiments on synthetic and real-world datasets demonstrate that STAR-Net and STAR-Net-S outperform state-of-the-art RSI denoising methods.</li>
</ul>

<h3>Title: DisTime: Distribution-based Time Representation for Video Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yingsen Zeng, Zepeng Huang, Yujie Zhong, Chengjian Feng, Jie Hu, Lin Ma, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24329">https://arxiv.org/abs/2505.24329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24329">https://arxiv.org/pdf/2505.24329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24329]] DisTime: Distribution-based Time Representation for Video Large Language Models(https://arxiv.org/abs/2505.24329)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite advances in general video understanding, Video Large Language Models (Video-LLMs) face challenges in precise temporal localization due to discrete time representations and limited temporally aware datasets. Existing methods for temporal expression either conflate time with text-based numerical values, add a series of dedicated temporal tokens, or regress time using specialized temporal grounding heads. To address these issues, we introduce DisTime, a lightweight framework designed to enhance temporal comprehension in Video-LLMs. DisTime employs a learnable token to create a continuous temporal embedding space and incorporates a Distribution-based Time Decoder that generates temporal probability distributions, effectively mitigating boundary ambiguities and maintaining temporal continuity. Additionally, the Distribution-based Time Encoder re-encodes timestamps to provide time markers for Video-LLMs. To overcome temporal granularity limitations in existing datasets, we propose an automated annotation paradigm that combines the captioning capabilities of Video-LLMs with the localization expertise of dedicated temporal models. This leads to the creation of InternVid-TG, a substantial dataset with 1.25M temporally grounded events across 179k videos, surpassing ActivityNet-Caption by 55 times. Extensive experiments demonstrate that DisTime achieves state-of-the-art performance across benchmarks in three time-sensitive tasks while maintaining competitive performance in Video QA tasks. Code and data are released at this https URL.</li>
</ul>

<h3>Title: Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Shi, Haochen Tan, Chuqiao Kuang, Xiaoguang Li, Xiaozhe Ren, Chen Zhang, Hanting Chen, Yasheng Wang, Lifeng Shang, Fisher Yu, Yunhe Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24332">https://arxiv.org/abs/2505.24332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24332">https://arxiv.org/pdf/2505.24332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24332]] Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning(https://arxiv.org/abs/2505.24332)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Information seeking demands iterative evidence gathering and reflective reasoning, yet large language models (LLMs) still struggle with it in open-web question answering. Existing methods rely on static prompting rules or training with Wikipedia-based corpora and retrieval environments, limiting adaptability to the real-world web environment where ambiguity, conflicting evidence, and noise are prevalent. These constrained training settings hinder LLMs from learning to dynamically decide when and where to search, and how to adjust search depth and frequency based on informational demands. We define this missing capacity as Search Intensity Scaling (SIS)--the emergent skill to intensify search efforts under ambiguous or conflicting conditions, rather than settling on overconfident, under-verification answers. To study SIS, we introduce WebPuzzle, the first dataset designed to foster information-seeking behavior in open-world internet environments. WebPuzzle consists of 24K training instances and 275 test questions spanning both wiki-based and open-web queries. Building on this dataset, we propose DeepDiver, a Reinforcement Learning (RL) framework that promotes SIS by encouraging adaptive search policies through exploration under a real-world open-web environment. Experimental results show that Pangu-7B-Reasoner empowered by DeepDiver achieve performance on real-web tasks comparable to the 671B-parameter DeepSeek-R1. We detail DeepDiver's training curriculum from cold-start supervised fine-tuning to a carefully designed RL phase, and present that its capability of SIS generalizes from closed-form QA to open-ended tasks such as long-form writing. Our contributions advance adaptive information seeking in LLMs and provide a valuable benchmark and dataset for future research.</li>
</ul>

<h3>Title: GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gilles Quentin Hacheme, Girmaw Abebe Tadesse, Caleb Robinson, Akram Zaytar, Rahul Dodhia, Juan M. Lavista Ferres</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24340">https://arxiv.org/abs/2505.24340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24340">https://arxiv.org/pdf/2505.24340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24340]] GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models(https://arxiv.org/abs/2505.24340)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Classifying geospatial imagery remains a major bottleneck for applications such as disaster response and land-use monitoring-particularly in regions where annotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that claim zero-shot classification capabilities for satellite imagery nonetheless rely on task-specific pretraining and adaptation to reach competitive performance. We introduce GeoVision Labeler (GVL), a strictly zero-shot classification framework: a vision Large Language Model (vLLM) generates rich, human-readable image descriptions, which are then mapped to user-defined classes by a conventional Large Language Model (LLM). This modular, and interpretable pipeline enables flexible image classification for a large range of use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced, and RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary Buildings vs. No Buildings task on SpaceNet v7. For complex multi-class classification tasks (UC Merced, RESISC45), we implemented a recursive LLM-driven clustering to form meta-classes at successive depths, followed by hierarchical classification-first resolving coarse groups, then finer distinctions-to deliver competitive zero-shot performance. GVL is open-sourced at this https URL to catalyze adoption in real-world geospatial workflows.</li>
</ul>

<h3>Title: Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings</h3>
<ul>
<li><strong>Authors: </strong>Shujian Yang, Shiyao Cui, Chuanrui Hu, Haicheng Wang, Tianwei Zhang, Minlie Huang, Jialiang Lu, Han Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24341">https://arxiv.org/abs/2505.24341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24341">https://arxiv.org/pdf/2505.24341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24341]] Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings(https://arxiv.org/abs/2505.24341)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Detecting toxic content using language models is important but challenging. While large language models (LLMs) have demonstrated strong performance in understanding Chinese, recent studies show that simple character substitutions in toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In this paper, we highlight the multimodal nature of Chinese language as a key challenge for deploying LLMs in toxic Chinese detection. First, we propose a taxonomy of 3 perturbation strategies and 8 specific approaches in toxic Chinese content. Then, we curate a dataset based on this taxonomy, and benchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect perturbed toxic Chinese text. Additionally, we explore cost-effective enhancement solutions like in-context learning (ICL) and supervised fine-tuning (SFT). Our results reveal two important findings. (1) LLMs are less capable of detecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a small number of perturbed examples may cause the LLMs "overcorrect'': misidentify many normal Chinese contents as toxic.</li>
</ul>

<h3>Title: KEVER^2: Knowledge-Enhanced Visual Emotion Reasoning and Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Fanhang Man, Xiaoyue Chen, Huandong Wang, Baining Zhao, Han Li, Xinlei Chen, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24342">https://arxiv.org/abs/2505.24342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24342">https://arxiv.org/pdf/2505.24342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24342]] KEVER^2: Knowledge-Enhanced Visual Emotion Reasoning and Retrieval(https://arxiv.org/abs/2505.24342)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Understanding what emotions images evoke in their viewers is a foundational goal in human-centric visual computing. While recent advances in vision-language models (VLMs) have shown promise for visual emotion analysis (VEA), several key challenges remain unresolved. Emotional cues in images are often abstract, overlapping, and entangled, making them difficult to model and interpret. Moreover, VLMs struggle to align these complex visual patterns with emotional semantics due to limited supervision and sparse emotional grounding. Finally, existing approaches lack structured affective knowledge to resolve ambiguity and ensure consistent emotional reasoning across diverse visual domains. To address these limitations, we propose \textbf{K-EVER\textsuperscript{2}}, a knowledge-enhanced framework for emotion reasoning and retrieval. Our approach introduces a semantically structured formulation of visual emotion cues and integrates external affective knowledge through multimodal alignment. Without relying on handcrafted labels or direct emotion supervision, K-EVER\textsuperscript{2} achieves robust and interpretable emotion predictions across heterogeneous image types. We validate our framework on three representative benchmarks, Emotion6, EmoSet, and M-Disaster, covering social media imagery, human-centric scenes, and disaster contexts. K-EVER\textsuperscript{2} consistently outperforms strong CNN and VLM baselines, achieving up to a \textbf{19\% accuracy gain} for specific emotions and a \textbf{12.3\% average accuracy gain} across all emotion categories. Our results demonstrate a scalable and generalizable solution for advancing emotional understanding of visual content.</li>
</ul>

<h3>Title: VUDG: A Dataset for Video Understanding Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Wang, Zhi Gao, Boxuan Yu, Zirui Dai, Yuxiang Song, Qingyuan Lu, Jin Chen, Xinxiao Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24346">https://arxiv.org/abs/2505.24346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24346">https://arxiv.org/pdf/2505.24346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24346]] VUDG: A Dataset for Video Understanding Domain Generalization(https://arxiv.org/abs/2505.24346)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Video understanding has made remarkable progress in recent years, largely driven by advances in deep models and the availability of large-scale annotated datasets. However, existing works typically ignore the inherent domain shifts encountered in real-world video applications, leaving domain generalization (DG) in video understanding underexplored. Hence, we propose Video Understanding Domain Generalization (VUDG), a novel dataset designed specifically for evaluating the DG performance in video understanding. VUDG contains videos from 11 distinct domains that cover three types of domain shifts, and maintains semantic similarity across different domains to ensure fair and meaningful evaluation. We propose a multi-expert progressive annotation framework to annotate each video with both multiple-choice and open-ended question-answer pairs. Extensive experiments on 9 representative large video-language models (LVLMs) and several traditional video question answering methods show that most models (including state-of-the-art LVLMs) suffer performance degradation under domain shifts. These results highlight the challenges posed by VUDG and the difference in the robustness of current models to data distribution shifts. We believe VUDG provides a valuable resource for prompting future research in domain generalization video understanding.</li>
</ul>

<h3>Title: Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research</h3>
<ul>
<li><strong>Authors: </strong>Qianqian Zhang, Jiajia Liao, Heting Ying, Yibo Ma, Haozhan Shen, Jingcheng Li, Peng Liu, Lu Zhang, Chunxin Fang, Kyusong Lee, Ruochen Xu, Tiancheng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24354">https://arxiv.org/abs/2505.24354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24354">https://arxiv.org/pdf/2505.24354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24354]] Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research(https://arxiv.org/abs/2505.24354)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Language agents powered by large language models (LLMs) have demonstrated remarkable capabilities in understanding, reasoning, and executing complex tasks. However, developing robust agents presents significant challenges: substantial engineering overhead, lack of standardized components, and insufficient evaluation frameworks for fair comparison. We introduce Agent Graph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and extensible framework that addresses these challenges through three key contributions: (1) a modular architecture with a graph-based workflow engine, efficient memory management, and clean component abstraction; (2) a comprehensive suite of reusable agent algorithms implementing state-of-the-art reasoning approaches; and (3) a rigorous evaluation framework enabling systematic comparison across multiple dimensions. Through extensive experiments on mathematical reasoning and multimodal tasks, we evaluate various agent algorithms across different LLMs, revealing important insights about their relative strengths and applicability. Our results demonstrate that while sophisticated reasoning approaches can enhance agent capabilities, simpler methods like Chain-of-Thought often exhibit robust performance with significantly lower computational overhead. AGORA not only simplifies language agent development but also establishes a foundation for reproducible agent research through standardized evaluation protocols.</li>
</ul>

<h3>Title: ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration</h3>
<ul>
<li><strong>Authors: </strong>Xianglong Yan, Zhiteng Li, Tianao Zhang, Linghe Kong, Yulun Zhang, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24357">https://arxiv.org/abs/2505.24357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24357">https://arxiv.org/pdf/2505.24357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24357]] ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration(https://arxiv.org/abs/2505.24357)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable performance, yet their capability on long-context reasoning is often constrained by the excessive memory required to store the Key-Value (KV) cache. This makes KV cache compression an essential step toward enabling efficient long-context reasoning. Recent methods have explored reducing the hidden dimensions of the KV cache, but many introduce additional computation through projection layers or suffer from significant performance degradation under high compression ratios. To address these challenges, we propose ReCalKV, a post-training KV cache compression method that reduces the hidden dimensions of the KV cache. We develop distinct compression strategies for Keys and Values based on their different roles and varying importance in the attention mechanism. For Keys, we propose Head-wise Similarity-aware Reordering (HSR), which clusters similar heads and applies grouped SVD to the key projection matrix, reducing additional computation while preserving accuracy. For Values, we propose Offline Calibration and Matrix Fusion (OCMF) to preserve accuracy without extra computational overhead. Experiments show that ReCalKV outperforms existing low-rank compression methods, achieving high compression ratios with minimal performance loss. Code is available at: this https URL.</li>
</ul>

<h3>Title: Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning</h3>
<ul>
<li><strong>Authors: </strong>Stepan Shabalin, Ayush Panda, Dmitrii Kharlapenko, Abdur Raheem Ali, Yixiong Hao, Arthur Conmy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24360">https://arxiv.org/abs/2505.24360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24360">https://arxiv.org/pdf/2505.24360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24360]] Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning(https://arxiv.org/abs/2505.24360)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Sparse autoencoders are a promising new approach for decomposing language model activations for interpretation and control. They have been applied successfully to vision transformer image encoders and to small-scale diffusion models. Inference-Time Decomposition of Activations (ITDA) is a recently proposed variant of dictionary learning that takes the dictionary to be a set of data points from the activation distribution and reconstructs them with gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large text-to-image diffusion model, Flux 1, and consider the interpretability of embeddings of both by introducing a visual automated interpretation pipeline. We find that SAEs accurately reconstruct residual stream embeddings and beat MLP neurons on interpretability. We are able to use SAE features to steer image generation through activation addition. We find that ITDA has comparable interpretability to SAEs.</li>
</ul>

<h3>Title: Revisiting Cross-Modal Knowledge Distillation: A Disentanglement Approach for RGBD Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Roger Ferrod, CÃ¡ssio F. Dantas, Luigi Di Caro, Dino Ienco</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24361">https://arxiv.org/abs/2505.24361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24361">https://arxiv.org/pdf/2505.24361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24361]] Revisiting Cross-Modal Knowledge Distillation: A Disentanglement Approach for RGBD Semantic Segmentation(https://arxiv.org/abs/2505.24361)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Multi-modal RGB and Depth (RGBD) data are predominant in many domains such as robotics, autonomous driving and remote sensing. The combination of these multi-modal data enhances environmental perception by providing 3D spatial context, which is absent in standard RGB images. Although RGBD multi-modal data can be available to train computer vision models, accessing all sensor modalities during the inference stage may be infeasible due to sensor failures or resource constraints, leading to a mismatch between data modalities available during training and inference. Traditional Cross-Modal Knowledge Distillation (CMKD) frameworks, developed to address this task, are typically based on a teacher/student paradigm, where a multi-modal teacher distills knowledge into a single-modality student model. However, these approaches face challenges in teacher architecture choices and distillation process selection, thus limiting their adoption in real-world scenarios. To overcome these issues, we introduce CroDiNo-KD (Cross-Modal Disentanglement: a New Outlook on Knowledge Distillation), a novel cross-modal knowledge distillation framework for RGBD semantic segmentation. Our approach simultaneously learns single-modality RGB and Depth models by exploiting disentanglement representation, contrastive learning and decoupled data augmentation with the aim to structure the internal manifolds of neural network models through interaction and collaboration. We evaluated CroDiNo-KD on three RGBD datasets across diverse domains, considering recent CMKD frameworks as competitors. Our findings illustrate the quality of CroDiNo-KD, and they suggest reconsidering the conventional teacher/student paradigm to distill information from multi-modal data to single-modality neural networks.</li>
</ul>

<h3>Title: Adversarial Preference Learning for Robust LLM Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yuanfu Wang, Pengyu Wang, Chenyang Xi, Bo Tang, Junyi Zhu, Wenqiang Wei, Chen Chen, Chao Yang, Jingfeng Zhang, Chaochao Lu, Yijun Niu, Keming Mao, Zhiyu Li, Feiyu Xiong, Jie Hu, Mingchuan Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24369">https://arxiv.org/abs/2505.24369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24369">https://arxiv.org/pdf/2505.24369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24369]] Adversarial Preference Learning for Robust LLM Alignment(https://arxiv.org/abs/2505.24369)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, generative</a></li>
<li><strong>Abstract: </strong>Modern language models often rely on Reinforcement Learning from Human Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to adversarial attacks due to three key limitations: (1) the inefficiency and high cost of human annotation, (2) the vast diversity of potential adversarial attacks, and (3) the risk of feedback bias and reward hacking. To address these challenges, we introduce Adversarial Preference Learning (APL), an iterative adversarial training method incorporating three key innovations. First, a direct harmfulness metric based on the model's intrinsic preference probabilities, eliminating reliance on external assessment. Second, a conditional generative attacker that synthesizes input-specific adversarial variations. Third, an iterative framework with automated closed-loop feedback, enabling continuous adaptation through vulnerability discovery and mitigation. Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly enhances robustness, achieving 83.33% harmlessness win rate over the base model (evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured by LLaMA-Guard), and lowering attack success rate by up to 65% according to HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against the base model.</li>
</ul>

<h3>Title: Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Md Intisar Chowdhury, Kittinun Aukkapinyo, Hiroshi Fujimura, Joo Ann Woo, Wasu Wasusatein, Fadoua Ghourabi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24371">https://arxiv.org/abs/2505.24371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24371">https://arxiv.org/pdf/2505.24371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24371]] Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering(https://arxiv.org/abs/2505.24371)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a Grid-based Local and Global Area Transcription (Grid-LoGAT) system for Video Question Answering (VideoQA). The system operates in two phases. First, extracting text transcripts from video frames using a Vision-Language Model (VLM). Next, processing questions using these transcripts to generate answers through a Large Language Model (LLM). This design ensures image privacy by deploying the VLM on edge devices and the LLM in the cloud. To improve transcript quality, we propose grid-based visual prompting, which extracts intricate local details from each grid cell and integrates them with global information. Evaluation results show that Grid-LoGAT, using the open-source VLM (LLaVA-1.6-7B) and LLM (Llama-3.1-8B), outperforms state-of-the-art methods with similar baseline models on NExT-QA and STAR-QA datasets with an accuracy of 65.9% and 50.11% respectively. Additionally, our method surpasses the non-grid version by 24 points on localization-based questions we created using NExT-QA.</li>
</ul>

<h3>Title: D2AF: A Dual-Driven Annotation and Filtering Framework for Visual Grounding</h3>
<ul>
<li><strong>Authors: </strong>Yichi Zhang, Gongwei Chen, Jun Zhu, Jia Wan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24372">https://arxiv.org/abs/2505.24372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24372">https://arxiv.org/pdf/2505.24372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24372]] D2AF: A Dual-Driven Annotation and Filtering Framework for Visual Grounding(https://arxiv.org/abs/2505.24372)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Visual Grounding is a task that aims to localize a target region in an image based on a free-form natural language description. With the rise of Transformer architectures, there is an increasing need for larger datasets to boost performance. However, the high cost of manual annotation poses a challenge, hindering the scale of data and the ability of large models to enhance their effectiveness. Previous pseudo label generation methods heavily rely on human-labeled captions of the original dataset, limiting scalability and diversity. To address this, we propose D2AF, a robust annotation framework for visual grounding using only input images. This approach overcomes dataset size limitations and enriches both the quantity and diversity of referring expressions. Our approach leverages multimodal large models and object detection models. By implementing dual-driven annotation strategies, we effectively generate detailed region-text pairs using both closed-set and open-set approaches. We further conduct an in-depth analysis of data quantity and data distribution. Our findings demonstrate that increasing data volume enhances model performance. However, the degree of improvement depends on how well the pseudo labels broaden the original data distribution. Based on these insights, we propose a consistency and distribution aware filtering method to further improve data quality by effectively removing erroneous and redundant data. This approach effectively eliminates noisy data, leading to improved performance. Experiments on three visual grounding tasks demonstrate that our method significantly improves the performance of existing models and achieves state-of-the-art results.</li>
</ul>

<h3>Title: LLM Inference Enhanced by External Knowledge: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Yu-Hsuan Lin, Qian-Hui Chen, Yi-Jie Cheng, Jia-Ren Zhang, Yi-Hung Liu, Liang-Yu Hsia, Yun-Nung Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24377">https://arxiv.org/abs/2505.24377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24377">https://arxiv.org/pdf/2505.24377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24377]] LLM Inference Enhanced by External Knowledge: A Survey(https://arxiv.org/abs/2505.24377)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have enhanced natural-language reasoning. However, their limited parametric memory and susceptibility to hallucination present persistent challenges for tasks requiring accurate, context-based inference. To overcome these limitations, an increasing number of studies have proposed leveraging external knowledge to enhance LLMs. This study offers a systematic exploration of strategies for using external knowledge to enhance LLMs, beginning with a taxonomy that categorizes external knowledge into unstructured and structured data. We then focus on structured knowledge, presenting distinct taxonomies for tables and knowledge graphs (KGs), detailing their integration paradigms with LLMs, and reviewing representative methods. Our comparative analysis further highlights the trade-offs among interpretability, scalability, and performance, providing insights for developing trustworthy and generalizable knowledge-enhanced LLMs.</li>
</ul>

<h3>Title: Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Yilun Kong, Guozheng Ma, Qi Zhao, Haoyu Wang, Li Shen, Xueqian Wang, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24378">https://arxiv.org/abs/2505.24378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24378">https://arxiv.org/pdf/2505.24378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24378]] Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer(https://arxiv.org/abs/2505.24378)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Despite recent advancements in offline multi-task reinforcement learning (MTRL) have harnessed the powerful capabilities of the Transformer architecture, most approaches focus on a limited number of tasks, with scaling to extremely massive tasks remaining a formidable challenge. In this paper, we first revisit the key impact of task numbers on current MTRL method, and further reveal that naively expanding the parameters proves insufficient to counteract the performance degradation as the number of tasks escalates. Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE) framework that tackles task scalability by further unlocking the model's parameter scalability. Specifically, we enhance both the architecture and the optimization of the agent, where we strengthen the Decision Transformer (DT) backbone with MoE to reduce task load on parameter subsets, and introduce a three-stage training mechanism to facilitate efficient training with optimal performance. Experimental results show that, by increasing the number of experts, M3DT not only consistently enhances its performance as model expansion on the fixed task numbers, but also exhibits remarkable task scalability, successfully extending to 160 tasks with superior performance.</li>
</ul>

<h3>Title: Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Wu, Yifei Pang, Terrance Liu, Zhiwei Steven Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24379">https://arxiv.org/abs/2505.24379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24379">https://arxiv.org/pdf/2505.24379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24379]] Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models(https://arxiv.org/abs/2505.24379)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large language models are typically trained on datasets collected from the web, which may inadvertently contain harmful or sensitive personal information. To address growing privacy concerns, unlearning methods have been proposed to remove the influence of specific data from trained models. Of these, exact unlearning -- which retrains the model from scratch without the target data -- is widely regarded the gold standard, believed to be robust against privacy-related attacks. In this paper, we challenge this assumption by introducing a novel data extraction attack that compromises even exact unlearning. Our method leverages both the pre- and post-unlearning models: by guiding the post-unlearning model using signals from the pre-unlearning model, we uncover patterns that reflect the removed data distribution. Combining model guidance with a token filtering strategy, our attack significantly improves extraction success rates -- doubling performance in some cases -- across common benchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our attack's effectiveness on a simulated medical diagnosis dataset to highlight real-world privacy risks associated with exact unlearning. In light of our findings, which suggest that unlearning may, in a contradictory way, increase the risk of privacy leakage, we advocate for evaluation of unlearning methods to consider broader threat models that account not only for post-unlearning models but also for adversarial access to prior checkpoints.</li>
</ul>

<h3>Title: SASP: Strip-Aware Spatial Perception for Fine-Grained Bird Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Zheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24380">https://arxiv.org/abs/2505.24380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24380">https://arxiv.org/pdf/2505.24380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24380]] SASP: Strip-Aware Spatial Perception for Fine-Grained Bird Image Classification(https://arxiv.org/abs/2505.24380)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Fine-grained bird image classification (FBIC) is not only of great significance for ecological monitoring and species identification, but also holds broad research value in the fields of image recognition and fine-grained visual modeling. Compared with general image classification tasks, FBIC poses more formidable challenges: 1) the differences in species size and imaging distance result in the varying sizes of birds presented in the images; 2) complex natural habitats often introduce strong background interference; 3) and highly flexible poses such as flying, perching, or foraging result in substantial intra-class variability. These factors collectively make it difficult for traditional methods to stably extract discriminative features, thereby limiting the generalizability and interpretability of models in real-world applications. To address these challenges, this paper proposes a fine-grained bird classification framework based on strip-aware spatial perception, which aims to capture long-range spatial dependencies across entire rows or columns in bird images, thereby enhancing the model's robustness and interpretability. The proposed method incorporates two novel modules: extensional perception aggregator (EPA) and channel semantic weaving (CSW). Specifically, EPA integrates local texture details with global structural cues by aggregating information across horizontal and vertical spatial directions. CSW further refines the semantic representations by adaptively fusing long-range and short-range information along the channel dimension. Built upon a ResNet-50 backbone, the model enables jump-wise connection of extended structural features across the spatial domain. Experimental results on the CUB-200-2011 dataset demonstrate that our framework achieves significant performance improvements while maintaining architectural efficiency.</li>
</ul>

<h3>Title: ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Hao Chen, Yukun Yan, Sen Mei, Wanxiang Che, Zhenghao Liu, Qi Shi, Xinze Li, Yuchun Fan, Pengcheng Huang, Qiushi Xiong, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24388">https://arxiv.org/abs/2505.24388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24388">https://arxiv.org/pdf/2505.24388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24388]] ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation(https://arxiv.org/abs/2505.24388)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs) with external knowledge to improve factuality. However, existing RAG systems frequently underutilize the retrieved documents, failing to extract and integrate the key clues needed to support faithful and interpretable reasoning, especially in cases where relevant evidence is implicit, scattered, or obscured by noise. To address this issue, we propose ClueAnchor, a novel framework for enhancing RAG via clue-anchored reasoning exploration and optimization. ClueAnchor extracts key clues from retrieved content and generates multiple reasoning paths based on different knowledge configurations, optimizing the model by selecting the most effective one through reward-based preference optimization. Experiments show that ClueAnchor significantly outperforms prior RAG baselines in reasoning completeness and robustness. Further analysis confirms its strong resilience to noisy or partially relevant retrieved content, as well as its capability to identify supporting evidence even in the absence of explicit clue supervision during inference.</li>
</ul>

<h3>Title: Looking for Attention: Randomized Attention Test Design for Validator Monitoring in Optimistic Rollups</h3>
<ul>
<li><strong>Authors: </strong>Suhyeon Lee</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CE, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24393">https://arxiv.org/abs/2505.24393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24393">https://arxiv.org/pdf/2505.24393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24393]] Looking for Attention: Randomized Attention Test Design for Validator Monitoring in Optimistic Rollups(https://arxiv.org/abs/2505.24393)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Optimistic Rollups (ORUs) significantly enhance blockchain scalability but inherently suffer from the verifier's dilemma, particularly concerning validator attentiveness. Current systems lack mechanisms to proactively ensure validators are diligently monitoring L2 state transitions, creating a vulnerability where fraudulent states could be finalized. This paper introduces the Randomized Attention Test (RAT), a novel L1-based protocol designed to probabilistically challenge validators in ORUs, thereby verifying their liveness and computational readiness. Our game-theoretic analysis demonstrates that an Ideal Security Equilibrium, where all validators are attentive and proposers are honest, can be achieved with RAT. Notably, this equilibrium is attainable and stable with relatively low economic penalties (e.g., under $1000) for non-responsive validators and a low attention test frequency (e.g., under 1% per epoch). RAT thus provides a crucial, practical mechanism to enforce validator diligence, fortifying the overall security and integrity of ORU systems with minimizing additional costs.</li>
</ul>

<h3>Title: S3CE-Net: Spike-guided Spatiotemporal Semantic Coupling and Expansion Network for Long Sequence Event Re-Identification</h3>
<ul>
<li><strong>Authors: </strong>Xianheng Ma, Hongchen Tan, Xiuping Liu, Yi Zhang, Huasheng Wang, Jiang Liu, Ying Chen, Hantao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24401">https://arxiv.org/abs/2505.24401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24401">https://arxiv.org/pdf/2505.24401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24401]] S3CE-Net: Spike-guided Spatiotemporal Semantic Coupling and Expansion Network for Long Sequence Event Re-Identification(https://arxiv.org/abs/2505.24401)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust</a></li>
<li><strong>Abstract: </strong>In this paper, we leverage the advantages of event cameras to resist harsh lighting conditions, reduce background interference, achieve high time resolution, and protect facial information to study the long-sequence event-based person re-identification (Re-ID) task. To this end, we propose a simple and efficient long-sequence event Re-ID model, namely the Spike-guided Spatiotemporal Semantic Coupling and Expansion Network (S3CE-Net). To better handle asynchronous event data, we build S3CE-Net based on spiking neural networks (SNNs). The S3CE-Net incorporates the Spike-guided Spatial-temporal Attention Mechanism (SSAM) and the Spatiotemporal Feature Sampling Strategy (STFS). The SSAM is designed to carry out semantic interaction and association in both spatial and temporal dimensions, leveraging the capabilities of SNNs. The STFS involves sampling spatial feature subsequences and temporal feature subsequences from the spatiotemporal dimensions, driving the Re-ID model to perceive broader and more robust effective semantics. Notably, the STFS introduces no additional parameters and is only utilized during the training stage. Therefore, S3CE-Net is a low-parameter and high-efficiency model for long-sequence event-based person Re-ID. Extensive experiments have verified that our S3CE-Net achieves outstanding performance on many mainstream long-sequence event-based person Re-ID datasets. Code is available at:this https URL.</li>
</ul>

<h3>Title: Leveraging Intermediate Features of Vision Transformer for Face Anti-Spoofing</h3>
<ul>
<li><strong>Authors: </strong>Mika Feng, Koichi Ito, Takafumi Aoki, Tetsushi Ohki, Masakatsu Nishigaki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24402">https://arxiv.org/abs/2505.24402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24402">https://arxiv.org/pdf/2505.24402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24402]] Leveraging Intermediate Features of Vision Transformer for Face Anti-Spoofing(https://arxiv.org/abs/2505.24402)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Face recognition systems are designed to be robust against changes in head pose, illumination, and blurring during image capture. If a malicious person presents a face photo of the registered user, they may bypass the authentication process illegally. Such spoofing attacks need to be detected before face recognition. In this paper, we propose a spoofing attack detection method based on Vision Transformer (ViT) to detect minute differences between live and spoofed face images. The proposed method utilizes the intermediate features of ViT, which have a good balance between local and global features that are important for spoofing attack detection, for calculating loss in training and score in inference. The proposed method also introduces two data augmentation methods: face anti-spoofing data augmentation and patch-wise data augmentation, to improve the accuracy of spoofing attack detection. We demonstrate the effectiveness of the proposed method through experiments using the OULU-NPU and SiW datasets.</li>
</ul>

<h3>Title: On the Lipschitz Continuity of Set Aggregation Functions and Neural Networks for Sets</h3>
<ul>
<li><strong>Authors: </strong>Giannis Nikolentzos, Konstantinos Skianis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24403">https://arxiv.org/abs/2505.24403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24403">https://arxiv.org/pdf/2505.24403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24403]] On the Lipschitz Continuity of Set Aggregation Functions and Neural Networks for Sets(https://arxiv.org/abs/2505.24403)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Lipschitz constant of a neural network is connected to several important properties of the network such as its robustness and generalization. It is thus useful in many settings to estimate the Lipschitz constant of a model. Prior work has focused mainly on estimating the Lipschitz constant of multi-layer perceptrons and convolutional neural networks. Here we focus on data modeled as sets or multisets of vectors and on neural networks that can handle such data. These models typically apply some permutation invariant aggregation function, such as the sum, mean or max operator, to the input multisets to produce a single vector for each input sample. In this paper, we investigate whether these aggregation functions are Lipschitz continuous with respect to three distance functions for unordered multisets, and we compute their Lipschitz constants. In the general case, we find that each aggregation function is Lipschitz continuous with respect to only one of the three distance functions. Then, we build on these results to derive upper bounds on the Lipschitz constant of neural networks that can process multisets of vectors, while we also study their stability to perturbations and generalization under distribution shifts. To empirically verify our theoretical analysis, we conduct a series of experiments on datasets from different domains.</li>
</ul>

<h3>Title: IRBridge: Solving Image Restoration Bridge with Pre-trained Generative Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hanting Wang, Tao Jin, Wang Lin, Shulei Wang, Hai Huang, Shengpeng Ji, Zhou Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24406">https://arxiv.org/abs/2505.24406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24406">https://arxiv.org/pdf/2505.24406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24406]] IRBridge: Solving Image Restoration Bridge with Pre-trained Generative Diffusion Models(https://arxiv.org/abs/2505.24406)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Bridge models in image restoration construct a diffusion process from degraded to clear images. However, existing methods typically require training a bridge model from scratch for each specific type of degradation, resulting in high computational costs and limited performance. This work aims to efficiently leverage pretrained generative priors within existing image restoration bridges to eliminate this requirement. The main challenge is that standard generative models are typically designed for a diffusion process that starts from pure noise, while restoration tasks begin with a low-quality image, resulting in a mismatch in the state distributions between the two processes. To address this challenge, we propose a transition equation that bridges two diffusion processes with the same endpoint distribution. Based on this, we introduce the IRBridge framework, which enables the direct utilization of generative models within image restoration bridges, offering a more flexible and adaptable approach to image restoration. Extensive experiments on six image restoration tasks demonstrate that IRBridge efficiently integrates generative priors, resulting in improved robustness and generalization performance. Code will be available at GitHub.</li>
</ul>

<h3>Title: LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory</h3>
<ul>
<li><strong>Authors: </strong>Eojin Kang, Juae Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24409">https://arxiv.org/abs/2505.24409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24409">https://arxiv.org/pdf/2505.24409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24409]] LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory(https://arxiv.org/abs/2505.24409)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multilingual large language models (LLMs) open up new possibilities for leveraging information across languages, but their factual knowledge recall remains inconsistent depending on the input language. While previous studies have attempted to address this issue through English-based prompting and evaluation, we explore non-English to English transfer via Language and Thought Theory. This perspective allows us to examine language-thought binding in LLMs and uncover why factual knowledge often fails to transfer effectively. We propose the Language-to-Thought (L2T) prompting strategy, which analyzes the relationship between input language, internal cognitive processes, and knowledge. Experimental results challenge the assumption that English-based approaches consistently outperform other languages and offer a novel insight that aligning the model's internal thought with the knowledge required for the task is critical for successful cross-lingual transfer. Furthermore, we show that applying L2T during training can alleviate LLMs' reliance on the input language and facilitate cross-linguistic knowledge integration without translation-based learning. Code and datasets will be available.</li>
</ul>

<h3>Title: PCIE_Pose Solution for EgoExo4D Pose and Proficiency Estimation Challenge</h3>
<ul>
<li><strong>Authors: </strong>Feng Chen, Kanokphan Lertniphonphan, Qiancheng Yan, Xiaohui Fan, Jun Xie, Tao Zhang, Zhepeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24411">https://arxiv.org/abs/2505.24411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24411">https://arxiv.org/pdf/2505.24411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24411]] PCIE_Pose Solution for EgoExo4D Pose and Proficiency Estimation Challenge(https://arxiv.org/abs/2505.24411)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This report introduces our team's (PCIE_EgoPose) solutions for the EgoExo4D Pose and Proficiency Estimation Challenges at CVPR2025. Focused on the intricate task of estimating 21 3D hand joints from RGB egocentric videos, which are complicated by subtle movements and frequent occlusions, we developed the Hand Pose Vision Transformer (HP-ViT+). This architecture synergizes a Vision Transformer and a CNN backbone, using weighted fusion to refine the hand pose predictions. For the EgoExo4D Body Pose Challenge, we adopted a multimodal spatio-temporal feature integration strategy to address the complexities of body pose estimation across dynamic contexts. Our methods achieved remarkable performance: 8.31 PA-MPJPE in the Hand Pose Challenge and 11.25 MPJPE in the Body Pose Challenge, securing championship titles in both competitions. We extended our pose estimation solutions to the Proficiency Estimation task, applying core technologies such as transformer-based architectures. This extension enabled us to achieve a top-1 accuracy of 0.53, a SOTA result, in the Demonstrator Proficiency Estimation competition.</li>
</ul>

<h3>Title: EasyText: Controllable Diffusion Transformer for Multilingual Text Rendering</h3>
<ul>
<li><strong>Authors: </strong>Runnan Lu, Yuxuan Zhang, Jailing Liu, Haifa Wang, Yiren Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24417">https://arxiv.org/abs/2505.24417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24417">https://arxiv.org/pdf/2505.24417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24417]] EasyText: Controllable Diffusion Transformer for Multilingual Text Rendering(https://arxiv.org/abs/2505.24417)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Generating accurate multilingual text with diffusion models has long been desired but remains challenging. Recent methods have made progress in rendering text in a single language, but rendering arbitrary languages is still an unexplored area. This paper introduces EasyText, a text rendering framework based on DiT (Diffusion Transformer), which connects denoising latents with multilingual character tokens encoded as character tokens. We propose character positioning encoding and position encoding interpolation techniques to achieve controllable and precise text rendering. Additionally, we construct a large-scale synthetic text image dataset with 1 million multilingual image-text annotations as well as a high-quality dataset of 20K annotated images, which are used for pretraining and fine-tuning respectively. Extensive experiments and evaluations demonstrate the effectiveness and advancement of our approach in multilingual text rendering, visual quality, and layout-aware text integration.</li>
</ul>

<h3>Title: MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating LLMs and VLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Liu, Lingfei Qian, Qianqian Xie, Jimin Huang, Kailai Yang, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24423">https://arxiv.org/abs/2505.24423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24423">https://arxiv.org/pdf/2505.24423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24423]] MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating LLMs and VLMs(https://arxiv.org/abs/2505.24423)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models and vision-language models (which we jointly call LMs) have transformed NLP and CV, demonstrating remarkable potential across various fields. However, their capabilities in affective analysis (i.e. sentiment analysis and emotion detection) remain underexplored. This gap is largely due to the absence of comprehensive evaluation benchmarks, and the inherent complexity of affective analysis tasks. In this paper, we introduce MMAFFBen, the first extensive open-source benchmark for multilingual multimodal affective analysis. MMAFFBen encompasses text, image, and video modalities across 35 languages, covering four key affective analysis tasks: sentiment polarity, sentiment intensity, emotion classification, and emotion intensity. Moreover, we construct the MMAFFIn dataset for fine-tuning LMs on affective analysis tasks, and further develop MMAFFLM-3b and MMAFFLM-7b based on it. We evaluate various representative LMs, including GPT-4o-mini, providing a systematic comparison of their affective understanding capabilities. This project is available at this https URL.</li>
</ul>

<h3>Title: Donate or Create? Comparing Data Collection Strategies for Emotion-labeled Multimodal Social Media Posts</h3>
<ul>
<li><strong>Authors: </strong>Christopher Bagdon, Aidan Combs, Carina Silberer, Roman Klinger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24427">https://arxiv.org/abs/2505.24427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24427">https://arxiv.org/pdf/2505.24427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24427]] Donate or Create? Comparing Data Collection Strategies for Emotion-labeled Multimodal Social Media Posts(https://arxiv.org/abs/2505.24427)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Accurate modeling of subjective phenomena such as emotion expression requires data annotated with authors' intentions. Commonly such data is collected by asking study participants to donate and label genuine content produced in the real world, or create content fitting particular labels during the study. Asking participants to create content is often simpler to implement and presents fewer risks to participant privacy than data donation. However, it is unclear if and how study-created content may differ from genuine content, and how differences may impact models. We collect study-created and genuine multimodal social media posts labeled for emotion and compare them on several dimensions, including model performance. We find that compared to genuine posts, study-created posts are longer, rely more on their text and less on their images for emotion expression, and focus more on emotion-prototypical events. The samples of participants willing to donate versus create posts are demographically different. Study-created data is valuable to train models that generalize well to genuine data, but realistic effectiveness estimates require genuine data.</li>
</ul>

<h3>Title: Model Unlearning via Sparse Autoencoder Subspace Guided Projections</h3>
<ul>
<li><strong>Authors: </strong>Xu Wang, Zihao Li, Benyou Wang, Yan Hu, Difan Zou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24428">https://arxiv.org/abs/2505.24428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24428">https://arxiv.org/pdf/2505.24428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24428]] Model Unlearning via Sparse Autoencoder Subspace Guided Projections(https://arxiv.org/abs/2505.24428)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) store vast amounts of information, making them powerful yet raising privacy and safety concerns when selective knowledge removal is required. Existing unlearning strategies, ranging from gradient-based fine-tuning and model editing to sparse autoencoder (SAE) steering, either lack interpretability or fail to provide a robust defense against adversarial prompts. We propose SAE-Guided Subspace Projection Unlearning (SSPU), a novel framework that leverages SAE features to drive targeted updates in the model's parameter space, enabling precise, interpretable, and robust unlearning. SSPU's three-stage pipeline performs data-driven layer and feature selection, subspace construction via QR decomposition, and constrained optimization that controls activations into an "irrelevant" subspace while preserving retained knowledge. Overall, we use SAE features to construct a subspace that supervises unlearning, refining the loss and adding a regularization term to guide interpretable parameter updates. In experiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU, TruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared to the strongest baseline. It also improves adversarial robustness, lowering malicious accuracy under jailbreak prompts compared to baselines. Our findings expose the limitations of prior unlearning methods and demonstrate how interpretable subspace-guided optimization can achieve robust, controllable model behavior.</li>
</ul>

<h3>Title: Bridging 3D Anomaly Localization and Repair via High-Quality Continuous Geometric Representation</h3>
<ul>
<li><strong>Authors: </strong>Bozhong Zheng, Jinye Gan, Xiaohao Xu, Wenqiao Li, Xiaonan Huang, Na Ni, Yingna Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24431">https://arxiv.org/abs/2505.24431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24431">https://arxiv.org/pdf/2505.24431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24431]] Bridging 3D Anomaly Localization and Repair via High-Quality Continuous Geometric Representation(https://arxiv.org/abs/2505.24431)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>3D point cloud anomaly detection is essential for robust vision systems but is challenged by pose variations and complex geometric anomalies. Existing patch-based methods often suffer from geometric fidelity issues due to discrete voxelization or projection-based representations, limiting fine-grained anomaly localization. We introduce Pose-Aware Signed Distance Field (PASDF), a novel framework that integrates 3D anomaly detection and repair by learning a continuous, pose-invariant shape representation. PASDF leverages a Pose Alignment Module for canonicalization and a SDF Network to dynamically incorporate pose, enabling implicit learning of high-fidelity anomaly repair templates from the continuous SDF. This facilitates precise pixel-level anomaly localization through an Anomaly-Aware Scoring Module. Crucially, the continuous 3D representation in PASDF extends beyond detection, facilitating in-situ anomaly repair. Experiments on Real3D-AD and Anomaly-ShapeNet demonstrate state-of-the-art performance, achieving high object-level AUROC scores of 80.2% and 90.0%, respectively. These results highlight the effectiveness of continuous geometric representations in advancing 3D anomaly detection and facilitating practical anomaly region repair. The code is available at this https URL to support further research.</li>
</ul>

<h3>Title: Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields</h3>
<ul>
<li><strong>Authors: </strong>Md Shahriar Rahim Siddiqui, Moshe Eliasof, Eldad Haber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24434">https://arxiv.org/abs/2505.24434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24434">https://arxiv.org/pdf/2505.24434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24434]] Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields(https://arxiv.org/abs/2505.24434)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Flow matching casts sample generation as learning a continuous-time velocity field that transports noise to data. Existing flow matching networks typically predict each point's velocity independently, considering only its location and time along its flow trajectory, and ignoring neighboring points. However, this pointwise approach may overlook correlations between points along the generation trajectory that could enhance velocity predictions, thereby improving downstream generation quality. To address this, we propose Graph Flow Matching (GFM), a lightweight enhancement that decomposes the learned velocity into a reaction term -- any standard flow matching network -- and a diffusion term that aggregates neighbor information via a graph neural module. This reaction-diffusion formulation retains the scalability of deep flow models while enriching velocity predictions with local context, all at minimal additional computational cost. Operating in the latent space of a pretrained variational autoencoder, GFM consistently improves FrÃ©chet Inception Distance (FID) and recall across five image generation benchmarks (LSUN Church, LSUN Bedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\times256$), demonstrating its effectiveness as a modular enhancement to existing flow matching architectures.</li>
</ul>

<h3>Title: The Cost of Restaking vs. Proof-of-Stake</h3>
<ul>
<li><strong>Authors: </strong>Akaki Mamageishvili, Benny Sudakov</a></li>
<li><strong>Subjects: </strong>cs.CR, econ.TH</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24440">https://arxiv.org/abs/2505.24440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24440">https://arxiv.org/pdf/2505.24440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24440]] The Cost of Restaking vs. Proof-of-Stake(https://arxiv.org/abs/2505.24440)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>We compare the efficiency of restaking and Proof-of-Stake (PoS) protocols in terms of stake requirements. First, we consider the sufficient condition for the restaking graph to be secure. We show that the condition implies that it is always possible to transform such a restaking graph into secure PoS protocols. Next, we derive two main results, giving upper and lower bounds on required extra stakes that one needs to add to validators of the secure restaking graph to be able to transform it into secure PoS protocols. In particular, we show that the restaking savings compared to PoS protocols can be very large and can asymptotically grow in the worst case as a square root of the number of validators. We also study a complementary question of transforming secure PoS protocols into an aggregate secure restaking graph and provide lower and upper bounds on the PoS savings compared to restaking.</li>
</ul>

<h3>Title: SORCE: Small Object Retrieval in Complex Environments</h3>
<ul>
<li><strong>Authors: </strong>Chunxu Liu, Chi Xie, Xiaxu Chen, Wei Li, Feng Zhu, Rui Zhao, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24441">https://arxiv.org/abs/2505.24441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24441">https://arxiv.org/pdf/2505.24441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24441]] SORCE: Small Object Retrieval in Complex Environments(https://arxiv.org/abs/2505.24441)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text-to-Image Retrieval (T2IR) is a highly valuable task that aims to match a given textual query to images in a gallery. Existing benchmarks primarily focus on textual queries describing overall image semantics or foreground salient objects, possibly overlooking inconspicuous small objects, especially in complex environments. Such small object retrieval is crucial, as in real-world applications, the targets of interest are not always prominent in the image. Thus, we introduce SORCE (Small Object Retrieval in Complex Environments), a new subfield of T2IR, focusing on retrieving small objects in complex images with textual queries. We propose a new benchmark, SORCE-1K, consisting of images with complex environments and textual queries describing less conspicuous small objects with minimal contextual cues from other salient objects. Preliminary analysis on SORCE-1K finds that existing T2IR methods struggle to capture small objects and encode all the semantics into a single embedding, leading to poor retrieval performance on SORCE-1K. Therefore, we propose to represent each image with multiple distinctive embeddings. We leverage Multimodal Large Language Models (MLLMs) to extract multiple embeddings for each image instructed by a set of Regional Prompts (ReP). Experimental results show that our multi-embedding approach through MLLM and ReP significantly outperforms existing T2IR methods on SORCE-1K. Our experiments validate the effectiveness of SORCE-1K for benchmarking SORCE performances, highlighting the potential of multi-embedding representation and text-customized MLLM features for addressing this task.</li>
</ul>

<h3>Title: Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised Learning with Outliers</h3>
<ul>
<li><strong>Authors: </strong>Heejo Kong, Sung-Jin Kim, Gunho Jung, Seong-Whan Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24443">https://arxiv.org/abs/2505.24443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24443">https://arxiv.org/pdf/2505.24443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24443]] Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised Learning with Outliers(https://arxiv.org/abs/2505.24443)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Conventional semi-supervised learning (SSL) ideally assumes that labeled and unlabeled data share an identical class distribution, however in practice, this assumption is easily violated, as unlabeled data often includes unknown class data, i.e., outliers. The outliers are treated as noise, considerably degrading the performance of SSL models. To address this drawback, we propose a novel framework, Diversify and Conquer (DAC), to enhance SSL robustness in the context of open-set semi-supervised learning. In particular, we note that existing open-set SSL methods rely on prediction discrepancies between inliers and outliers from a single model trained on labeled data. This approach can be easily failed when the labeled data is insufficient, leading to performance degradation that is worse than naive SSL that do not account for outliers. In contrast, our approach exploits prediction disagreements among multiple models that are differently biased towards the unlabeled distribution. By leveraging the discrepancies arising from training on unlabeled data, our method enables robust outlier detection even when the labeled data is underspecified. Our key contribution is constructing a collection of differently biased models through a single training process. By encouraging divergent heads to be differently biased towards outliers while making consistent predictions for inliers, we exploit the disagreement among these heads as a measure to identify unknown concepts. Our code is available at this https URL.</li>
</ul>

<h3>Title: Learning Safety Constraints for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xin Chen, Yarden As, Andreas Krause</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24445">https://arxiv.org/abs/2505.24445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24445">https://arxiv.org/pdf/2505.24445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24445]] Learning Safety Constraints for Large Language Models(https://arxiv.org/abs/2505.24445)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have emerged as powerful tools but pose significant safety risks through harmful outputs and vulnerability to adversarial attacks. We propose SaP, short for Safety Polytope, a geometric approach to LLM safety that learns and enforces multiple safety constraints directly in the model's representation space. We develop a framework that identifies safe and unsafe regions via the polytope's facets, enabling both detection and correction of unsafe outputs through geometric steering. Unlike existing approaches that modify model weights, SaP operates post-hoc in the representation space, preserving model capabilities while enforcing safety constraints. Experiments across multiple LLMs demonstrate that our method can effectively detect unethical inputs, reduce adversarial attack success rates while maintaining performance on standard tasks, thus highlighting the importance of having an explicit geometric model for safety. Analysis of the learned polytope facets reveals emergence of specialization in detecting different semantic notions of safety, providing interpretable insights into how safety is captured in LLMs' representation space.</li>
</ul>

<h3>Title: Exploring the Impact of Occupational Personas on Domain-Specific QA</h3>
<ul>
<li><strong>Authors: </strong>Eojin Kang, Jaehyuk Yu, Juae Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24448">https://arxiv.org/abs/2505.24448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24448">https://arxiv.org/pdf/2505.24448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24448]] Exploring the Impact of Occupational Personas on Domain-Specific QA(https://arxiv.org/abs/2505.24448)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent studies on personas have improved the way Large Language Models (LLMs) interact with users. However, the effect of personas on domain-specific question-answering (QA) tasks remains a subject of debate. This study analyzes whether personas enhance specialized QA performance by introducing two types of persona: Profession-Based Personas (PBPs) (e.g., scientist), which directly relate to domain expertise, and Occupational Personality-Based Personas (OPBPs) (e.g., scientific person), which reflect cognitive tendencies rather than explicit expertise. Through empirical evaluations across multiple scientific domains, we demonstrate that while PBPs can slightly improve accuracy, OPBPs often degrade performance, even when semantically related to the task. Our findings suggest that persona relevance alone does not guarantee effective knowledge utilization and that they may impose cognitive constraints that hinder optimal knowledge application. Future research can explore how nuanced distinctions in persona representations guide LLMs, potentially contributing to reasoning and knowledge retrieval that more closely mirror human social conceptualization.</li>
</ul>

<h3>Title: LPASS: Linear Probes as Stepping Stones for vulnerability detection using compressed LLMs</h3>
<ul>
<li><strong>Authors: </strong>Luis Ibanez-Lissen, Lorena Gonzalez-Manzano, Jose Maria de Fuentes, Nicolas Anciaux</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24451">https://arxiv.org/abs/2505.24451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24451">https://arxiv.org/pdf/2505.24451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24451]] LPASS: Linear Probes as Stepping Stones for vulnerability detection using compressed LLMs(https://arxiv.org/abs/2505.24451)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are being extensively used for cybersecurity purposes. One of them is the detection of vulnerable codes. For the sake of efficiency and effectiveness, compression and fine-tuning techniques are being developed, respectively. However, they involve spending substantial computational efforts. In this vein, we analyse how Linear Probes (LPs) can be used to provide an estimation on the performance of a compressed LLM at an early phase -- before fine-tuning. We also show their suitability to set the cut-off point when applying layer pruning compression. Our approach, dubbed $LPASS$, is applied in BERT and Gemma for the detection of 12 of MITRE's Top 25 most dangerous vulnerabilities on 480k C/C++ samples. LPs can be computed in 142.97 s. and provide key findings: (1) 33.3 \% and 72.2\% of layers can be removed, respectively, with no precision loss; (2) they provide an early estimate of the post-fine-tuning and post-compression model effectiveness, with 3\% and 8.68\% as the lowest and average precision errors, respectively. $LPASS$-based LLMs outperform the state of the art, reaching 86.9\% of accuracy in multi-class vulnerability detection. Interestingly, $LPASS$-based compressed versions of Gemma outperform the original ones by 1.6\% of F1-score at a maximum while saving 29.4 \% and 23.8\% of training and inference time and 42.98\% of model size.</li>
</ul>

<h3>Title: Stepsize anything: A unified learning rate schedule for budgeted-iteration training</h3>
<ul>
<li><strong>Authors: </strong>Anda Tang, Yiming Dong, Yutao Zeng, zhou Xun, Zhouchen Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24452">https://arxiv.org/abs/2505.24452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24452">https://arxiv.org/pdf/2505.24452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24452]] Stepsize anything: A unified learning rate schedule for budgeted-iteration training(https://arxiv.org/abs/2505.24452)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The expanding computational costs and limited resources underscore the critical need for budgeted-iteration training, which aims to achieve optimal learning within predetermined iteration this http URL learning rate schedules fundamentally govern the performance of different networks and tasks, particularly in budgeted-iteration scenarios, their design remains largely heuristic, lacking theoretical this http URL addition, the optimal learning rate schedule requires extensive trial-and-error selection, making the training process this http URL this work, we propose the Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule that consistently outperforms commonly-used schedules among diverse architectures and tasks under different constrained training this http URL, we bridge the gap by constructing a novel training budget-aware optimization framework, which explicitly accounts for the robustness to landscape curvature this http URL this framework, we derive the UBA schedule, controlled by a single hyper-parameter $\varphi$ that provides a trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. Moreover, we establish a theoretical connection between $\varphi$ and the condition number, adding interpretation and justification to our approach. Besides, we prove the convergence for different values of $\varphi$.We offer practical guidelines for its selection via theoretical analysis and empirical this http URL experimental results show that UBA \textit{consistently surpasses} the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets.</li>
</ul>

<h3>Title: Domain Pre-training Impact on Representations</h3>
<ul>
<li><strong>Authors: </strong>Cesar Gonzalez-Gutierrez, Ariadna Quattoni</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24455">https://arxiv.org/abs/2505.24455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24455">https://arxiv.org/pdf/2505.24455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24455]] Domain Pre-training Impact on Representations(https://arxiv.org/abs/2505.24455)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This empirical study analyzes the effects of the pre-training corpus on the quality of learned transformer representations. We focus on the representation quality induced solely through pre-training. Our experiments show that pre-training on a small, specialized corpus can yield effective representations, and that the success of combining a generic and a specialized corpus depends on the distributional similarity between the target task and the specialized corpus.</li>
</ul>

<h3>Title: SA-Person: Text-Based Person Retrieval with Scene-aware Re-ranking</h3>
<ul>
<li><strong>Authors: </strong>Yingjia Xu, Jinlin Wu, Zhen Chen, Daming Gao, Yang Yang, Zhen Lei, Min Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24466">https://arxiv.org/abs/2505.24466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24466">https://arxiv.org/pdf/2505.24466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24466]] SA-Person: Text-Based Person Retrieval with Scene-aware Re-ranking(https://arxiv.org/abs/2505.24466)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text-based person retrieval aims to identify a target individual from a gallery of images based on a natural language description. It presents a significant challenge due to the complexity of real-world scenes and the ambiguity of appearance-related descriptions. Existing methods primarily emphasize appearance-based cross-modal retrieval, often neglecting the contextual information embedded within the scene, which can offer valuable complementary insights for retrieval. To address this, we introduce SCENEPERSON-13W, a large-scale dataset featuring over 100,000 scenes with rich annotations covering both pedestrian appearance and environmental cues. Based on this, we propose SA-Person, a two-stage retrieval framework. In the first stage, it performs discriminative appearance grounding by aligning textual cues with pedestrian-specific regions. In the second stage, it introduces SceneRanker, a training-free, scene-aware re-ranking method leveraging multimodal large language models to jointly reason over pedestrian appearance and the global scene context. Experiments on SCENEPERSON-13W validate the effectiveness of our framework in challenging scene-level retrieval scenarios. The code and dataset will be made publicly available.</li>
</ul>

<h3>Title: Train One Sparse Autoencoder Across Multiple Sparsity Budgets to Preserve Interpretability and Accuracy</h3>
<ul>
<li><strong>Authors: </strong>Nikita Balagansky, Yaroslav Aksenov, Daniil Laptev, Vadim Kurochkin, Gleb Gerasimov, Nikita Koryagin, Daniil Gavrilov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24473">https://arxiv.org/abs/2505.24473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24473">https://arxiv.org/pdf/2505.24473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24473]] Train One Sparse Autoencoder Across Multiple Sparsity Budgets to Preserve Interpretability and Accuracy(https://arxiv.org/abs/2505.24473)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting neural networks by decomposing hidden representations into disentangled, interpretable features via sparsity constraints. However, conventional SAEs are constrained by the fixed sparsity level chosen during training; meeting different sparsity requirements therefore demands separate models and increases the computational footprint during both training and evaluation. We introduce a novel training objective, \emph{HierarchicalTopK}, which trains a single SAE to optimise reconstructions across multiple sparsity levels simultaneously. Experiments with Gemma-2 2B demonstrate that our approach achieves Pareto-optimal trade-offs between sparsity and explained variance, outperforming traditional SAEs trained at individual sparsity levels. Further analysis shows that HierarchicalTopK preserves high interpretability scores even at higher sparsity. The proposed objective thus closes an important gap between flexibility and interpretability in SAE design.</li>
</ul>

<h3>Title: SPPSFormer: High-quality Superpoint-based Transformer for Roof Plane Instance Segmentation from Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Cheng Zeng, Xiatian Qi, Chi Chen, Kai Sun, Wangle Zhang, Yuxuan Liu, Yan Meng, Bisheng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24475">https://arxiv.org/abs/2505.24475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24475">https://arxiv.org/pdf/2505.24475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24475]] SPPSFormer: High-quality Superpoint-based Transformer for Roof Plane Instance Segmentation from Point Clouds(https://arxiv.org/abs/2505.24475)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Transformers have been seldom employed in point cloud roof plane instance segmentation, which is the focus of this study, and existing superpoint Transformers suffer from limited performance due to the use of low-quality superpoints. To address this challenge, we establish two criteria that high-quality superpoints for Transformers should satisfy and introduce a corresponding two-stage superpoint generation process. The superpoints generated by our method not only have accurate boundaries, but also exhibit consistent geometric sizes and shapes, both of which greatly benefit the feature learning of superpoint Transformers. To compensate for the limitations of deep learning features when the training set size is limited, we incorporate multidimensional handcrafted features into the model. Additionally, we design a decoder that combines a Kolmogorov-Arnold Network with a Transformer module to improve instance prediction and mask extraction. Finally, our network's predictions are refined using traditional algorithm-based postprocessing. For evaluation, we annotated a real-world dataset and corrected annotation errors in the existing RoofN3D dataset. Experimental results show that our method achieves state-of-the-art performance on our dataset, as well as both the original and reannotated RoofN3D datasets. Moreover, our model is not sensitive to plane boundary annotations during training, significantly reducing the annotation burden. Through comprehensive experiments, we also identified key factors influencing roof plane segmentation performance: in addition to roof types, variations in point cloud density, density uniformity, and 3D point precision have a considerable impact. These findings underscore the importance of incorporating data augmentation strategies that account for point cloud quality to enhance model robustness under diverse and challenging conditions.</li>
</ul>

<h3>Title: Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yuting Zhang, Hao Lu, Qingyong Hu, Yin Wang, Kaishen Yuan, Xin Liu, Kaishun Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24476">https://arxiv.org/abs/2505.24476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24476">https://arxiv.org/pdf/2505.24476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24476]] Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model(https://arxiv.org/abs/2505.24476)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Periodic or quasi-periodic phenomena reveal intrinsic characteristics in various natural processes, such as weather patterns, movement behaviors, traffic flows, and biological signals. Given that these phenomena span multiple modalities, the capabilities of Multimodal Large Language Models (MLLMs) offer promising potential to effectively capture and understand their complex nature. However, current MLLMs struggle with periodic tasks due to limitations in: 1) lack of temporal modelling and 2) conflict between short and long periods. This paper introduces Period-LLM, a multimodal large language model designed to enhance the performance of periodic tasks across various modalities, and constructs a benchmark of various difficulty for evaluating the cross-modal periodic capabilities of large models. Specially, We adopt an "Easy to Hard Generalization" paradigm, starting with relatively simple text-based tasks and progressing to more complex visual and multimodal tasks, ensuring that the model gradually builds robust periodic reasoning capabilities. Additionally, we propose a "Resisting Logical Oblivion" optimization strategy to maintain periodic reasoning abilities during semantic alignment. Extensive experiments demonstrate the superiority of the proposed Period-LLM over existing MLLMs in periodic tasks. The code is available at this https URL.</li>
</ul>

<h3>Title: ACM-UNet: Adaptive Integration of CNNs and Mamba for Efficient Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jing Huang, Yongkang Zhao, Yuhan Li, Zhitao Dai, Cheng Chen, Qiying Lai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24481">https://arxiv.org/abs/2505.24481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24481">https://arxiv.org/pdf/2505.24481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24481]] ACM-UNet: Adaptive Integration of CNNs and Mamba for Efficient Medical Image Segmentation(https://arxiv.org/abs/2505.24481)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The U-shaped encoder-decoder architecture with skip connections has become a prevailing paradigm in medical image segmentation due to its simplicity and effectiveness. While many recent works aim to improve this framework by designing more powerful encoders and decoders, employing advanced convolutional neural networks (CNNs) for local feature extraction, Transformers or state space models (SSMs) such as Mamba for global context modeling, or hybrid combinations of both, these methods often struggle to fully utilize pretrained vision backbones (e.g., ResNet, ViT, VMamba) due to structural mismatches. To bridge this gap, we introduce ACM-UNet, a general-purpose segmentation framework that retains a simple UNet-like design while effectively incorporating pretrained CNNs and Mamba models through a lightweight adapter mechanism. This adapter resolves architectural incompatibilities and enables the model to harness the complementary strengths of CNNs and SSMs-namely, fine-grained local detail extraction and long-range dependency modeling. Additionally, we propose a hierarchical multi-scale wavelet transform module in the decoder to enhance feature fusion and reconstruction fidelity. Extensive experiments on the Synapse and ACDC benchmarks demonstrate that ACM-UNet achieves state-of-the-art performance while remaining computationally efficient. Notably, it reaches 85.12% Dice Score and 13.89mm HD95 on the Synapse dataset with 17.93G FLOPs, showcasing its effectiveness and scalability. Code is available at: this https URL.</li>
</ul>

<h3>Title: Deformable Attention Mechanisms Applied to Object Detection, case of Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Anasse Boutayeb, Iyad Lahsen-cherif, Ahmed El Khadimi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24489">https://arxiv.org/abs/2505.24489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24489">https://arxiv.org/pdf/2505.24489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24489]] Deformable Attention Mechanisms Applied to Object Detection, case of Remote Sensing(https://arxiv.org/abs/2505.24489)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Object detection has recently seen an interesting trend in terms of the most innovative research work, this task being of particular importance in the field of remote sensing, given the consistency of these images in terms of geographical coverage and the objects present. Furthermore, Deep Learning (DL) models, in particular those based on Transformers, are especially relevant for visual computing tasks in general, and target detection in particular. Thus, the present work proposes an application of Deformable-DETR model, a specific architecture using deformable attention mechanisms, on remote sensing images in two different modes, especially optical and Synthetic Aperture Radar (SAR). To achieve this objective, two datasets are used, one optical, which is Pleiades Aircraft dataset, and the other SAR, in particular SAR Ship Detection Dataset (SSDD). The results of a 10-fold stratified validation showed that the proposed model performed particularly well, obtaining an F1 score of 95.12% for the optical dataset and 94.54% for SSDD, while comparing these results with several models detections, especially those based on CNNs and transformers, as well as those specifically designed to detect different object classes in remote sensing images.</li>
</ul>

<h3>Title: Object Centric Concept Bottlenecks</h3>
<ul>
<li><strong>Authors: </strong>David Steinmann, Wolfgang Stammer, Antonia WÃ¼st, Kristian Kersting</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24492">https://arxiv.org/abs/2505.24492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24492">https://arxiv.org/pdf/2505.24492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24492]] Object Centric Concept Bottlenecks(https://arxiv.org/abs/2505.24492)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Developing high-performing, yet interpretable models remains a critical challenge in modern AI. Concept-based models (CBMs) attempt to address this by extracting human-understandable concepts from a global encoding (e.g., image encoding) and then applying a linear classifier on the resulting concept activations, enabling transparent decision-making. However, their reliance on holistic image encodings limits their expressiveness in object-centric real-world settings and thus hinders their ability to solve complex vision tasks beyond single-label classification. To tackle these challenges, we introduce Object-Centric Concept Bottlenecks (OCB), a framework that combines the strengths of CBMs and pre-trained object-centric foundation models, boosting performance and interpretability. We evaluate OCB on complex image datasets and conduct a comprehensive ablation study to analyze key components of the framework, such as strategies for aggregating object-concept encodings. The results show that OCB outperforms traditional CBMs and allows one to make interpretable decisions for complex visual tasks.</li>
</ul>

<h3>Title: Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation</h3>
<ul>
<li><strong>Authors: </strong>Ximing Xing, Yandong Guan, Jing Zhang, Dong Xu, Qian Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24499">https://arxiv.org/abs/2505.24499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24499">https://arxiv.org/pdf/2505.24499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24499]] Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation(https://arxiv.org/abs/2505.24499)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Generating high-quality Scalable Vector Graphics (SVGs) is challenging for Large Language Models (LLMs), as it requires advanced reasoning for structural validity, semantic faithfulness, and visual coherence -- capabilities in which current LLMs often fall short. In this work, we introduce Reason-SVG, a novel framework designed to enhance LLM reasoning for SVG generation. Reason-SVG pioneers the "Drawing-with-Thought" (DwT) paradigm, in which models generate both SVG code and explicit design rationales, mimicking the human creative process. Reason-SVG adopts a two-stage training strategy: First, Supervised Fine-Tuning (SFT) trains the LLM on the DwT paradigm to activate foundational reasoning abilities. Second, Reinforcement Learning (RL), utilizing Group Relative Policy Optimization (GRPO), empowers the model to generate both DwT and SVGs rationales through refined, reward-driven reasoning. To facilitate reasoning-driven SVG generation, we design a Hybrid Reward function that evaluates the presence and utility of DwT reasoning, along with structural validity, semantic alignment, and visual quality. We also introduce the SVGX-DwT-10k dataset, a high-quality corpus of 10,000 SVG-DwT pairs, where each SVG code is generated based on explicit DwT reasoning. By integrating DwT, SFT, and Hybrid Reward-guided RL, Reason-SVG significantly improves LLM performance in generating accurate and visually compelling SVGs, potentially fostering "Aha moments" in design.</li>
</ul>

<h3>Title: TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Guiyang Hou, Xing Gao, Yuchuan Wu, Xiang Huang, Wenqi Zhang, Zhe Zheng, Yongliang Shen, Jialu Du, Fei Huang, Yongbin Li, Weiming Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24500">https://arxiv.org/abs/2505.24500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24500">https://arxiv.org/pdf/2505.24500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24500]] TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence(https://arxiv.org/abs/2505.24500)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights.</li>
</ul>

<h3>Title: Learning to Optimally Dispatch Power: Performance on a Nation-Wide Real-World Dataset</h3>
<ul>
<li><strong>Authors: </strong>Ignacio Boero, Santiago Diaz, TomÃ¡s VÃ¡zquez, Enzo Coppes, Pablo Belzarena, Federico Larroca</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24505">https://arxiv.org/abs/2505.24505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24505">https://arxiv.org/pdf/2505.24505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24505]] Learning to Optimally Dispatch Power: Performance on a Nation-Wide Real-World Dataset(https://arxiv.org/abs/2505.24505)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Optimal Reactive Power Dispatch (ORPD) problem plays a crucial role in power system operations, ensuring voltage stability and minimizing power losses. Recent advances in machine learning, particularly within the ``learning to optimize'' framework, have enabled fast and efficient approximations of ORPD solutions, typically by training models on precomputed optimization results. While these approaches have demonstrated promising performance on synthetic datasets, their effectiveness under real-world grid conditions remains largely unexplored. This paper makes two key contributions. First, we introduce a publicly available power system dataset that includes both the structural characteristics of Uruguay's electrical grid and nearly two years of real-world operational data, encompassing actual demand and generation profiles. Given Uruguay's high penetration of renewable energy, the ORPD problem has become the primary optimization challenge in its power network. Second, we assess the impact of real-world data on learning-based ORPD solutions, revealing a significant increase in prediction errors when transitioning from synthetic to actual demand and generation inputs. Our results highlight the limitations of existing models in learning under the complex statistical properties of real grid conditions and emphasize the need for more expressive architectures. By providing this dataset, we aim to facilitate further research into robust learning-based optimization techniques for power system management.</li>
</ul>

<h3>Title: Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Wang, Mingyue Cheng, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24511">https://arxiv.org/abs/2505.24511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24511">https://arxiv.org/pdf/2505.24511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24511]] Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting(https://arxiv.org/abs/2505.24511)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks.</li>
</ul>

<h3>Title: un$^2$CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP</h3>
<ul>
<li><strong>Authors: </strong>Yinqi Li, Jiahe Zhao, Hong Chang, Ruibing Hou, Shiguang Shan, Xilin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24517">https://arxiv.org/abs/2505.24517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24517">https://arxiv.org/pdf/2505.24517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24517]] un$^2$CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP(https://arxiv.org/abs/2505.24517)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Contrastive Language-Image Pre-training (CLIP) has become a foundation model and has been applied to various vision and multimodal tasks. However, recent works indicate that CLIP falls short in distinguishing detailed differences in images and shows suboptimal performance on dense-prediction and vision-centric multimodal tasks. Therefore, this work focuses on improving existing CLIP models, aiming to capture as many visual details in images as possible. We find that a specific type of generative models, unCLIP, provides a suitable framework for achieving our goal. Specifically, unCLIP trains an image generator conditioned on the CLIP image embedding. In other words, it inverts the CLIP image encoder. Compared to discriminative models like CLIP, generative models are better at capturing image details because they are trained to learn the data distribution of images. Additionally, the conditional input space of unCLIP aligns with CLIP's original image-text embedding space. Therefore, we propose to invert unCLIP (dubbed un$^2$CLIP) to improve the CLIP model. In this way, the improved image encoder can gain unCLIP's visual detail capturing ability while preserving its alignment with the original text encoder simultaneously. We evaluate our improved CLIP across various tasks to which CLIP has been applied, including the challenging MMVP-VLM benchmark, the dense-prediction open-vocabulary segmentation task, and multimodal large language model tasks. Experiments show that un$^2$CLIP significantly improves the original CLIP and previous CLIP improvement methods. Code and models will be available at this https URL.</li>
</ul>

<h3>Title: AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders</h3>
<ul>
<li><strong>Authors: </strong>Yuqi Zhang, Yuchun Miao, Zuchao Li, Liang Ding</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24519">https://arxiv.org/abs/2505.24519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24519">https://arxiv.org/pdf/2505.24519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24519]] AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders(https://arxiv.org/abs/2505.24519)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust</a></li>
<li><strong>Abstract: </strong>We introduce AMIA, a lightweight, inference-only defense for Large Vision-Language Models (LVLMs) that (1) Automatically Masks a small set of text-irrelevant image patches to disrupt adversarial perturbations, and (2) conducts joint Intention Analysis to uncover and mitigate hidden harmful intents before response generation. Without any retraining, AMIA improves defense success rates across diverse LVLMs and jailbreak benchmarks from an average of 52.4% to 81.7%, preserves general utility with only a 2% average accuracy drop, and incurs only modest inference overhead. Ablation confirms both masking and intention analysis are essential for a robust safety-utility trade-off.</li>
</ul>

<h3>Title: UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation</h3>
<ul>
<li><strong>Authors: </strong>Yang-Tian Sun, Xin Yu, Zehuan Huang, Yi-Hua Huang, Yuan-Chen Guo, Ziyi Yang, Yan-Pei Cao, Xiaojuan Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24521">https://arxiv.org/abs/2505.24521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24521">https://arxiv.org/pdf/2505.24521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24521]] UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation(https://arxiv.org/abs/2505.24521)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, methods leveraging diffusion model priors to assist monocular geometric estimation (e.g., depth and normal) have gained significant attention due to their strong generalization ability. However, most existing works focus on estimating geometric properties within the camera coordinate system of individual video frames, neglecting the inherent ability of diffusion models to determine inter-frame correspondence. In this work, we demonstrate that, through appropriate design and fine-tuning, the intrinsic consistency of video generation models can be effectively harnessed for consistent geometric estimation. Specifically, we 1) select geometric attributes in the global coordinate system that share the same correspondence with video frames as the prediction targets, 2) introduce a novel and efficient conditioning method by reusing positional encodings, and 3) enhance performance through joint training on multiple geometric attributes that share the same correspondence. Our results achieve superior performance in predicting global geometric attributes in videos and can be directly applied to reconstruction tasks. Even when trained solely on static video data, our approach exhibits the potential to generalize to dynamic video scenes.</li>
</ul>

<h3>Title: Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors</h3>
<ul>
<li><strong>Authors: </strong>Andrea Pedrotti, Michele Papucci, Cristiano Ciaccio, Alessio Miaschi, Giovanni Puccetti, Felice Dell'Orletta, Andrea Esuli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24523">https://arxiv.org/abs/2505.24523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24523">https://arxiv.org/pdf/2505.24523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24523]] Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors(https://arxiv.org/abs/2505.24523)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we present a pipeline to test the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. To challenge the detectors, we fine-tune language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT). This exploits the detectors' reliance on stylistic clues, making new generations more challenging to detect. Additionally, we analyze the linguistic shifts induced by the alignment and which features are used by detectors to detect MGT texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detection performance. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts.</li>
</ul>

<h3>Title: Optimal Density Functions for Weighted Convolution in Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Simone Cammarasana, Giuseppe PatanÃ¨</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24527">https://arxiv.org/abs/2505.24527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24527">https://arxiv.org/pdf/2505.24527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24527]] Optimal Density Functions for Weighted Convolution in Learning Models(https://arxiv.org/abs/2505.24527)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The paper introduces the weighted convolution, a novel approach to the convolution for signals defined on regular grids (e.g., 2D images) through the application of an optimal density function to scale the contribution of neighbouring pixels based on their distance from the central pixel. This choice differs from the traditional uniform convolution, which treats all neighbouring pixels equally. Our weighted convolution can be applied to convolutional neural network problems to improve the approximation accuracy. Given a convolutional network, we define a framework to compute the optimal density function through a minimisation model. The framework separates the optimisation of the convolutional kernel weights (using stochastic gradient descent) from the optimisation of the density function (using DIRECT-L). Experimental results on a learning model for an image-to-image task (e.g., image denoising) show that the weighted convolution significantly reduces the loss (up to 53% improvement) and increases the test accuracy compared to standard convolution. While this method increases execution time by 11%, it is robust across several hyperparameters of the learning model. Future work will apply the weighted convolution to real-case 2D and 3D image convolutional learning problems.</li>
</ul>

<h3>Title: Geospatial Foundation Models to Enable Progress on Sustainable Development Goals</h3>
<ul>
<li><strong>Authors: </strong>Pedram Ghamisi, Weikang Yu, Xiaokang Zhang, Aldino Rizaldy, Jian Wang, Chufeng Zhou, Richard Gloaguen, Gustau Camps-Valls</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24528">https://arxiv.org/abs/2505.24528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24528">https://arxiv.org/pdf/2505.24528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24528]] Geospatial Foundation Models to Enable Progress on Sustainable Development Goals(https://arxiv.org/abs/2505.24528)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Foundation Models (FMs) are large-scale, pre-trained AI systems that have revolutionized natural language processing and computer vision, and are now advancing geospatial analysis and Earth Observation (EO). They promise improved generalization across tasks, scalability, and efficient adaptation with minimal labeled data. However, despite the rapid proliferation of geospatial FMs, their real-world utility and alignment with global sustainability goals remain underexplored. We introduce SustainFM, a comprehensive benchmarking framework grounded in the 17 Sustainable Development Goals with extremely diverse tasks ranging from asset wealth prediction to environmental hazard detection. This study provides a rigorous, interdisciplinary assessment of geospatial FMs and offers critical insights into their role in attaining sustainability goals. Our findings show: (1) While not universally superior, FMs often outperform traditional approaches across diverse tasks and datasets. (2) Evaluating FMs should go beyond accuracy to include transferability, generalization, and energy efficiency as key criteria for their responsible use. (3) FMs enable scalable, SDG-grounded solutions, offering broad utility for tackling complex sustainability challenges. Critically, we advocate for a paradigm shift from model-centric development to impact-driven deployment, and emphasize metrics such as energy efficiency, robustness to domain shifts, and ethical considerations.</li>
</ul>

<h3>Title: Transformers Are Universally Consistent</h3>
<ul>
<li><strong>Authors: </strong>Sagar Ghosh, Kushal Bose, Swagatam Das</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24531">https://arxiv.org/abs/2505.24531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24531">https://arxiv.org/pdf/2505.24531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24531]] Transformers Are Universally Consistent(https://arxiv.org/abs/2505.24531)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Despite their central role in the success of foundational models and large-scale language modeling, the theoretical foundations governing the operation of Transformers remain only partially understood. Contemporary research has largely focused on their representational capacity for language comprehension and their prowess in in-context learning, frequently under idealized assumptions such as linearized attention mechanisms. Initially conceived to model sequence-to-sequence transformations, a fundamental and unresolved question is whether Transformers can robustly perform functional regression over sequences of input tokens. This question assumes heightened importance given the inherently non-Euclidean geometry underlying real-world data distributions. In this work, we establish that Transformers equipped with softmax-based nonlinear attention are uniformly consistent when tasked with executing Ordinary Least Squares (OLS) regression, provided both the inputs and outputs are embedded in hyperbolic space. We derive deterministic upper bounds on the empirical error which, in the asymptotic regime, decay at a provable rate of $\mathcal{O}(t^{-1/2d})$, where $t$ denotes the number of input tokens and $d$ the embedding dimensionality. Notably, our analysis subsumes the Euclidean setting as a special case, recovering analogous convergence guarantees parameterized by the intrinsic dimensionality of the data manifold. These theoretical insights are corroborated through empirical evaluations on real-world datasets involving both continuous and categorical response variables.</li>
</ul>

<h3>Title: Beyond Linear Steering: Unified Multi-Attribute Control for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Narmeen Oozeer, Luke Marks, Fazl Barez, Amirali Abdullah</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24535">https://arxiv.org/abs/2505.24535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24535">https://arxiv.org/pdf/2505.24535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24535]] Beyond Linear Steering: Unified Multi-Attribute Control for Language Models(https://arxiv.org/abs/2505.24535)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Controlling multiple behavioral attributes in large language models (LLMs) at inference time is a challenging problem due to interference between attributes and the limitations of linear steering methods, which assume additive behavior in activation space and require per-attribute tuning. We introduce K-Steering, a unified and flexible approach that trains a single non-linear multi-label classifier on hidden activations and computes intervention directions via gradients at inference time. This avoids linearity assumptions, removes the need for storing and tuning separate attribute vectors, and allows dynamic composition of behaviors without retraining. To evaluate our method, we propose two new benchmarks, ToneBank and DebateMix, targeting compositional behavioral control. Empirical results across 3 model families, validated by both activation-based classifiers and LLM-based judges, demonstrate that K-Steering outperforms strong baselines in accurately steering multiple behaviors.</li>
</ul>

<h3>Title: CHIP: Chameleon Hash-based Irreversible Passport for Robust Deep Model Ownership Verification and Active Usage Control</h3>
<ul>
<li><strong>Authors: </strong>Chaohui Xu, Qi Cui, Chip-Hong Chang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24536">https://arxiv.org/abs/2505.24536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24536">https://arxiv.org/pdf/2505.24536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24536]] CHIP: Chameleon Hash-based Irreversible Passport for Robust Deep Model Ownership Verification and Active Usage Control(https://arxiv.org/abs/2505.24536)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect, attack, robust, watermark</a></li>
<li><strong>Abstract: </strong>The pervasion of large-scale Deep Neural Networks (DNNs) and their enormous training costs make their intellectual property (IP) protection of paramount importance. Recently introduced passport-based methods attempt to steer DNN watermarking towards strengthening ownership verification against ambiguity attacks by modulating the affine parameters of normalization layers. Unfortunately, neither watermarking nor passport-based methods provide a holistic protection with robust ownership proof, high fidelity, active usage authorization and user traceability for offline access distributed models and multi-user Machine-Learning as a Service (MLaaS) cloud model. In this paper, we propose a Chameleon Hash-based Irreversible Passport (CHIP) protection framework that utilizes the cryptographic chameleon hash function to achieve all these goals. The collision-resistant property of chameleon hash allows for strong model ownership claim upon IP infringement and liable user traceability, while the trapdoor-collision property enables hashing of multiple user passports and licensee certificates to the same immutable signature to realize active usage control. Using the owner passport as an oracle, multiple user-specific triplets, each contains a passport-aware user model, a user passport, and a licensee certificate can be created for secure offline distribution. The watermarked master model can also be deployed for MLaaS with usage permission verifiable by the provision of any trapdoor-colliding user passports. CHIP is extensively evaluated on four datasets and two architectures to demonstrate its protection versatility and robustness. Our code is released at this https URL.</li>
</ul>

<h3>Title: Don't Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections</h3>
<ul>
<li><strong>Authors: </strong>Orfeas Menis Mastromichalakis, Jason Liartis, Kristina Rose, Antoine Isaac, Giorgos Stamou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24538">https://arxiv.org/abs/2505.24538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24538">https://arxiv.org/pdf/2505.24538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24538]] Don't Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections(https://arxiv.org/abs/2505.24538)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Cultural Heritage (CH) data hold invaluable knowledge, reflecting the history, traditions, and identities of societies, and shaping our understanding of the past and present. However, many CH collections contain outdated or offensive descriptions that reflect historical biases. CH Institutions (CHIs) face significant challenges in curating these data due to the vast scale and complexity of the task. To address this, we develop an AI-powered tool that detects offensive terms in CH metadata and provides contextual insights into their historical background and contemporary perception. We leverage a multilingual vocabulary co-created with marginalized communities, researchers, and CH professionals, along with traditional NLP techniques and Large Language Models (LLMs). Available as a standalone web app and integrated with major CH platforms, the tool has processed over 7.9 million records, contextualizing the contentious terms detected in their metadata. Rather than erasing these terms, our approach seeks to inform, making biases visible and providing actionable insights for creating more inclusive and accessible CH collections.</li>
</ul>

<h3>Title: Localizing Persona Representations in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Celia Cintas, Miriam Rateike, Erik Miehling, Elizabeth Daly, Skyler Speakman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24539">https://arxiv.org/abs/2505.24539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24539">https://arxiv.org/pdf/2505.24539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24539]] Localizing Persona Representations in LLMs(https://arxiv.org/abs/2505.24539)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present a study on how and where personas -- defined by distinct sets of human characteristics, values, and beliefs -- are encoded in the representation space of large language models (LLMs). Using a range of dimension reduction and pattern recognition methods, we first identify the model layers that show the greatest divergence in encoding these representations. We then analyze the activations within a selected layer to examine how specific personas are encoded relative to others, including their shared and distinct embedding spaces. We find that, across multiple pre-trained decoder-only LLMs, the analyzed personas show large differences in representation space only within the final third of the decoder layers. We observe overlapping activations for specific ethical perspectives -- such as moral nihilism and utilitarianism -- suggesting a degree of polysemy. In contrast, political ideologies like conservatism and liberalism appear to be represented in more distinct regions. These findings help to improve our understanding of how LLMs internally represent information and can inform future efforts in refining the modulation of specific human traits in LLM outputs. Warning: This paper includes potentially offensive sample statements.</li>
</ul>

<h3>Title: Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts</h3>
<ul>
<li><strong>Authors: </strong>Xin He, Xumeng Han, Longhui Wei, Lingxi Xie, Qi Tian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24541">https://arxiv.org/abs/2505.24541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24541">https://arxiv.org/pdf/2505.24541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24541]] Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts(https://arxiv.org/abs/2505.24541)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) require a nuanced interpretation of complex image information, typically leveraging a vision encoder to perceive various visual scenarios. However, relying solely on a single vision encoder to handle diverse task domains proves difficult and inevitably leads to conflicts. Recent work enhances data perception by directly integrating multiple domain-specific vision encoders, yet this structure adds complexity and limits the potential for joint optimization. In this paper, we introduce Mixpert, an efficient mixture-of-vision-experts architecture that inherits the joint learning advantages from a single vision encoder while being restructured into a multi-expert paradigm for task-specific fine-tuning across different visual tasks. Additionally, we design a dynamic routing mechanism that allocates input images to the most suitable visual expert. Mixpert effectively alleviates domain conflicts encountered by a single vision encoder in multi-task learning with minimal additional computational cost, making it more efficient than multiple encoders. Furthermore, Mixpert integrates seamlessly into any MLLM, with experimental results demonstrating substantial performance gains across various tasks.</li>
</ul>

<h3>Title: Cross-Attention Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhong, Manasa Bharadwaj, Yixiao Wang, Nikhil Verma, Yipeng Ji, Chul Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24544">https://arxiv.org/abs/2505.24544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24544">https://arxiv.org/pdf/2505.24544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24544]] Cross-Attention Speculative Decoding(https://arxiv.org/abs/2505.24544)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding (SD) is a widely adopted approach for accelerating inference in large language models (LLMs), particularly when the draft and target models are well aligned. However, state-of-the-art SD methods typically rely on tightly coupled, self-attention-based Transformer decoders, often augmented with auxiliary pooling or fusion layers. This coupling makes them increasingly complex and harder to generalize across different models. We present Budget EAGLE (Beagle), the first, to our knowledge, cross-attention-based Transformer decoder SD model that achieves performance on par with leading self-attention SD models (EAGLE-v2) while eliminating the need for pooling or auxiliary components, simplifying the architecture, improving training efficiency, and maintaining stable memory usage during training-time simulation. To enable effective training of this novel architecture, we propose Two-Stage Block-Attention Training, a new method that achieves training stability and convergence efficiency in block-level attention scenarios. Extensive experiments across multiple LLMs and datasets show that Beagle achieves competitive inference speedups and higher training efficiency than EAGLE-v2, offering a strong alternative for architectures in speculative decoding.</li>
</ul>

<h3>Title: CREFT: Sequential Multi-Agent LLM for Character Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Ye Eun Chun, Taeyoon Hwang, Seung-won Hwang, Byung-Hak Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24553">https://arxiv.org/abs/2505.24553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24553">https://arxiv.org/pdf/2505.24553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24553]] CREFT: Sequential Multi-Agent LLM for Character Relation Extraction(https://arxiv.org/abs/2505.24553)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Understanding complex character relations is crucial for narrative analysis and efficient script evaluation, yet existing extraction methods often fail to handle long-form narratives with nuanced interactions. To address this challenge, we present CREFT, a novel sequential framework leveraging specialized Large Language Model (LLM) agents. First, CREFT builds a base character graph through knowledge distillation, then iteratively refines character composition, relation extraction, role identification, and group assignments. Experiments on a curated Korean drama dataset demonstrate that CREFT significantly outperforms single-agent LLM baselines in both accuracy and completeness. By systematically visualizing character networks, CREFT streamlines narrative comprehension and accelerates script review -- offering substantial benefits to the entertainment, publishing, and educational sectors.</li>
</ul>

<h3>Title: Bench4KE: Benchmarking Automated Competency Question Generation</h3>
<ul>
<li><strong>Authors: </strong>Anna Sofia Lippolis, Minh Davide Ragagni, Paolo Ciancarini, Andrea Giovanni Nuzzolese, Valentina Presutti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24554">https://arxiv.org/abs/2505.24554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24554">https://arxiv.org/pdf/2505.24554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24554]] Bench4KE: Benchmarking Automated Competency Question Generation(https://arxiv.org/abs/2505.24554)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The availability of Large Language Models (LLMs) presents a unique opportunity to reinvigorate research on Knowledge Engineering (KE) automation, a trend already evident in recent efforts developing LLM-based methods and tools for the automatic generation of Competency Questions (CQs). However, the evaluation of these tools lacks standardisation. This undermines the methodological rigour and hinders the replication and comparison of results. To address this gap, we introduce Bench4KE, an extensible API-based benchmarking system for KE automation. Its first release focuses on evaluating tools that generate CQs automatically. CQs are natural language questions used by ontology engineers to define the functional requirements of an ontology. Bench4KE provides a curated gold standard consisting of CQ datasets from four real-world ontology projects. It uses a suite of similarity metrics to assess the quality of the CQs generated. We present a comparative analysis of four recent CQ generation systems, which are based on LLMs, establishing a baseline for future research. Bench4KE is also designed to accommodate additional KE automation tasks, such as SPARQL query generation, ontology testing and drafting. Code and datasets are publicly available under the Apache 2.0 license.</li>
</ul>

<h3>Title: Optimal Weighted Convolution for Classification and Denosing</h3>
<ul>
<li><strong>Authors: </strong>Simone Cammarasana, Giuseppe PatanÃ¨</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24558">https://arxiv.org/abs/2505.24558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24558">https://arxiv.org/pdf/2505.24558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24558]] Optimal Weighted Convolution for Classification and Denosing(https://arxiv.org/abs/2505.24558)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We introduce a novel weighted convolution operator that enhances traditional convolutional neural networks (CNNs) by integrating a spatial density function into the convolution operator. This extension enables the network to differentially weight neighbouring pixels based on their relative position to the reference pixel, improving spatial characterisation and feature extraction. The proposed operator maintains the same number of trainable parameters and is fully compatible with existing CNN architectures. Although developed for 2D image data, the framework is generalisable to signals on regular grids of arbitrary dimensions, such as 3D volumetric data or 1D time series. We propose an efficient implementation of the weighted convolution by pre-computing the density function and achieving execution times comparable to standard convolution layers. We evaluate our method on two deep learning tasks: image classification using the CIFAR-100 dataset [KH+09] and image denoising using the DIV2K dataset [AT17]. Experimental results with state-of-the-art classification (e.g., VGG [SZ15], ResNet [HZRS16]) and denoising (e.g., DnCNN [ZZC+17], NAFNet [CCZS22]) methods show that the weighted convolution improves performance with respect to standard convolution across different quantitative metrics. For example, VGG achieves an accuracy of 66.94% with weighted convolution versus 56.89% with standard convolution on the classification problem, while DnCNN improves the PSNR value from 20.17 to 22.63 on the denoising problem. All models were trained on the CINECA Leonardo cluster to reduce the execution time and improve the tuning of the density function values. The PyTorch implementation of the weighted convolution is publicly available at: this https URL.</li>
</ul>

<h3>Title: Unleashing the Power of Intermediate Domains for Mixed Domain Semi-Supervised Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Qinghe Ma, Jian Zhang, Lei Qi, Qian Yu, Yinghuan Shi, Yang Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24567">https://arxiv.org/abs/2505.24567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24567">https://arxiv.org/pdf/2505.24567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24567]] Unleashing the Power of Intermediate Domains for Mixed Domain Semi-Supervised Medical Image Segmentation(https://arxiv.org/abs/2505.24567)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Both limited annotation and domain shift are prevalent challenges in medical image segmentation. Traditional semi-supervised segmentation and unsupervised domain adaptation methods address one of these issues separately. However, the coexistence of limited annotation and domain shift is quite common, which motivates us to introduce a novel and challenging scenario: Mixed Domain Semi-supervised medical image Segmentation (MiDSS), where limited labeled data from a single domain and a large amount of unlabeled data from multiple domains. To tackle this issue, we propose the UST-RUN framework, which fully leverages intermediate domain information to facilitate knowledge transfer. We employ Unified Copy-paste (UCP) to construct intermediate domains, and propose a Symmetric GuiDance training strategy (SymGD) to supervise unlabeled data by merging pseudo-labels from intermediate samples. Subsequently, we introduce a Training Process aware Random Amplitude MixUp (TP-RAM) to progressively incorporate style-transition components into intermediate samples. To generate more diverse intermediate samples, we further select reliable samples with high-quality pseudo-labels, which are then mixed with other unlabeled data. Additionally, we generate sophisticated intermediate samples with high-quality pseudo-labels for unreliable samples, ensuring effective knowledge transfer for them. Extensive experiments on four public datasets demonstrate the superiority of UST-RUN. Notably, UST-RUN achieves a 12.94% improvement in Dice score on the Prostate dataset. Our code is available at this https URL</li>
</ul>

<h3>Title: Neuro-Symbolic Operator for Interpretable and Generalizable Characterization of Complex Piezoelectric Systems</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Chandra, Taniya Kapoor, Mitrofan Curti, Koen Tiels, Elena A. Lomonova</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24578">https://arxiv.org/abs/2505.24578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24578">https://arxiv.org/pdf/2505.24578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24578]] Neuro-Symbolic Operator for Interpretable and Generalizable Characterization of Complex Piezoelectric Systems(https://arxiv.org/abs/2505.24578)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Complex piezoelectric systems are foundational in industrial applications. Their performance, however, is challenged by the nonlinear voltage-displacement hysteretic relationships. Efficient characterization methods are, therefore, essential for reliable design, monitoring, and maintenance. Recently proposed neural operator methods serve as surrogates for system characterization but face two pressing issues: interpretability and generalizability. State-of-the-art (SOTA) neural operators are black-boxes, providing little insight into the learned operator. Additionally, generalizing them to novel voltages and predicting displacement profiles beyond the training domain is challenging, limiting their practical use. To address these limitations, this paper proposes a neuro-symbolic operator (NSO) framework that derives the analytical operators governing hysteretic relationships. NSO first learns a Fourier neural operator mapping voltage fields to displacement profiles, followed by a library-based sparse model discovery method, generating white-box parsimonious models governing the underlying hysteresis. These models enable accurate and interpretable prediction of displacement profiles across varying and out-of-distribution voltage fields, facilitating generalizability. The potential of NSO is demonstrated by accurately predicting voltage-displacement hysteresis, including butterfly-shaped relationships. Moreover, NSO predicts displacement profiles even for noisy and low-fidelity voltage data, emphasizing its robustness. The results highlight the advantages of NSO compared to SOTA neural operators and model discovery methods on several evaluation metrics. Consequently, NSO contributes to characterizing complex piezoelectric systems while improving the interpretability and generalizability of neural operators, essential for design, monitoring, maintenance, and other real-world scenarios.</li>
</ul>

<h3>Title: AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams</h3>
<ul>
<li><strong>Authors: </strong>Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24584">https://arxiv.org/abs/2505.24584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24584">https://arxiv.org/pdf/2505.24584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24584]] AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams(https://arxiv.org/abs/2505.24584)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in generative AI have accelerated the discovery of novel chemicals and materials; however, transitioning these discoveries to industrial-scale production remains a critical bottleneck, as it requires the development of entirely new chemical manufacturing processes. Current AI methods cannot auto-generate PFDs or PIDs, despite their critical role in scaling chemical processes, while adhering to engineering constraints. We present a closed loop, physics aware framework for the automated generation of industrially viable PFDs and PIDs. The framework integrates domain specialized small scale language models (SLMs) (trained for chemical process QA tasks) with first principles simulation, leveraging three key components: (1) a hierarchical knowledge graph of process flow and instrumentation descriptions for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure feasibility. To improve both runtime efficiency and model compactness, the framework incorporates advanced inference time optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test Time Inference Scaling and independently applies structural pruning techniques (width and depth) guided by importance heuristics to reduce model size with minimal accuracy loss. Experiments demonstrate that the framework generates simulator-validated process descriptions with high fidelity, outperforms baseline methods in correctness, and generalizes to unseen chemicals. By bridging AI-driven design with industrial-scale feasibility, this work significantly reduces R&D timelines from lab discovery to plant deployment.</li>
</ul>

<h3>Title: A Flat Minima Perspective on Understanding Augmentations and Model Robustness</h3>
<ul>
<li><strong>Authors: </strong>Weebum Yoo, Sung Whan Yoon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24592">https://arxiv.org/abs/2505.24592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24592">https://arxiv.org/pdf/2505.24592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24592]] A Flat Minima Perspective on Understanding Augmentations and Model Robustness(https://arxiv.org/abs/2505.24592)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Model robustness indicates a model's capability to generalize well on unforeseen distributional shifts, including data corruption, adversarial attacks, and domain shifts. Data augmentation is one of the prevalent and effective ways to enhance robustness. Despite the great success of augmentations in different fields, a general theoretical understanding of their efficacy in improving model robustness is lacking. We offer a unified theoretical framework to clarify how augmentations can enhance model robustness through the lens of loss surface flatness and PAC generalization bound. Our work diverges from prior studies in that our analysis i) broadly encompasses much of the existing augmentation methods, and ii) is not limited to specific types of distribution shifts like adversarial attacks. We confirm our theories through simulations on the existing common corruption and adversarial robustness benchmarks based on the CIFAR and ImageNet datasets, as well as domain generalization benchmarks including PACS and OfficeHome.</li>
</ul>

<h3>Title: Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis</h3>
<ul>
<li><strong>Authors: </strong>Junzhuo Li, Bo Wang, Xiuze Zhou, Peijie Jiang, Jia Liu, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24593">https://arxiv.org/abs/2505.24593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24593">https://arxiv.org/pdf/2505.24593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24593]] Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis(https://arxiv.org/abs/2505.24593)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>The interpretability of Mixture-of-Experts (MoE) models, especially those with heterogeneous designs, remains underexplored. Existing attribution methods for dense models fail to capture dynamic routing-expert interactions in sparse MoE architectures. To address this issue, we propose a cross-level attribution algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE, Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results show MoE models achieve 37% higher per-layer efficiency via a "mid-activation, late-amplification" pattern: early layers screen experts, while late layers refine knowledge collaboratively. Ablation studies reveal a "basic-refinement" framework--shared experts handle general tasks (entity recognition), while routed experts specialize in domain-specific processing (geographic attributes). Semantic-driven routing is evidenced by strong correlations between attention heads and experts (r=0.68), enabling task-aware coordination. Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10 experts) through shared expert redundancy, whereas shallow OLMoE suffers severe degradation (76% drop). Task sensitivity further guides design: core-sensitive tasks (geography) require concentrated expertise, while distributed-tolerant tasks (object attributes) leverage broader participation. These insights advance MoE interpretability, offering principles to balance efficiency, specialization, and robustness.</li>
</ul>

<h3>Title: Binary Cumulative Encoding meets Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Andrei Chernov, Vitaliy Pozdnyakov, Ilya Makarov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24595">https://arxiv.org/abs/2505.24595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24595">https://arxiv.org/pdf/2505.24595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24595]] Binary Cumulative Encoding meets Time Series Forecasting(https://arxiv.org/abs/2505.24595)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent studies in time series forecasting have explored formulating regression via classification task. By discretizing the continuous target space into bins and predicting over a fixed set of classes, these approaches benefit from stable training, robust uncertainty modeling, and compatibility with modern deep learning architectures. However, most existing methods rely on one-hot encoding that ignores the inherent ordinal structure of the underlying values. As a result, they fail to provide information about the relative distance between predicted and true values during training. In this paper, we propose to address this limitation by introducing binary cumulative encoding (BCE), that represents scalar targets into monotonic binary vectors. This encoding implicitly preserves order and magnitude information, allowing the model to learn distance-aware representations while still operating within a classification framework. We propose a convolutional neural network architecture specifically designed for BCE, incorporating residual and dilated convolutions to enable fast and expressive temporal modeling. Through extensive experiments on benchmark forecasting datasets, we show that our approach outperforms widely used methods in both point and probabilistic forecasting, while requiring fewer parameters and enabling faster training.</li>
</ul>

<h3>Title: SARD: A Large-Scale Synthetic Arabic OCR Dataset for Book-Style Text Recognition</h3>
<ul>
<li><strong>Authors: </strong>Omer Nacar, Yasser Al-Habashi, Serry Sibaee, Adel Ammar, Wadii Boulila</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24600">https://arxiv.org/abs/2505.24600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24600">https://arxiv.org/pdf/2505.24600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24600]] SARD: A Large-Scale Synthetic Arabic OCR Dataset for Book-Style Text Recognition(https://arxiv.org/abs/2505.24600)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Arabic Optical Character Recognition (OCR) is essential for converting vast amounts of Arabic print media into digital formats. However, training modern OCR models, especially powerful vision-language models, is hampered by the lack of large, diverse, and well-structured datasets that mimic real-world book layouts. Existing Arabic OCR datasets often focus on isolated words or lines or are limited in scale, typographic variety, or structural complexity found in books. To address this significant gap, we introduce SARD (Large-Scale Synthetic Arabic OCR Dataset). SARD is a massive, synthetically generated dataset specifically designed to simulate book-style documents. It comprises 843,622 document images containing 690 million words, rendered across ten distinct Arabic fonts to ensure broad typographic coverage. Unlike datasets derived from scanned documents, SARD is free from real-world noise and distortions, offering a clean and controlled environment for model training. Its synthetic nature provides unparalleled scalability and allows for precise control over layout and content variation. We detail the dataset's composition and generation process and provide benchmark results for several OCR models, including traditional and deep learning approaches, highlighting the challenges and opportunities presented by this dataset. SARD serves as a valuable resource for developing and evaluating robust OCR and vision-language models capable of processing diverse Arabic book-style texts.</li>
</ul>

<h3>Title: The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian Sketches</h3>
<ul>
<li><strong>Authors: </strong>Omri Lev, Vishwak Srinivasan, Moshe Shenfeld, Katrina Ligett, Ayush Sekhari, Ashia C. Wilson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24603">https://arxiv.org/abs/2505.24603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24603">https://arxiv.org/pdf/2505.24603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24603]] The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian Sketches(https://arxiv.org/abs/2505.24603)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Gaussian sketching, which consists of pre-multiplying the data with a random Gaussian matrix, is a widely used technique for multiple problems in data science and machine learning, with applications spanning computationally efficient optimization, coded computing, and federated learning. This operation also provides differential privacy guarantees due to its inherent randomness. In this work, we revisit this operation through the lens of Renyi Differential Privacy (RDP), providing a refined privacy analysis that yields significantly tighter bounds than prior results. We then demonstrate how this improved analysis leads to performance improvement in different linear regression settings, establishing theoretical utility guarantees. Empirically, our methods improve performance across multiple datasets and, in several cases, reduce runtime.</li>
</ul>

<h3>Title: Explainable Depression Detection using Masked Hard Instance Mining</h3>
<ul>
<li><strong>Authors: </strong>Patawee Prakrankamanant, Shinji Watanabe, Ekapol Chuangsuwanich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24609">https://arxiv.org/abs/2505.24609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24609">https://arxiv.org/pdf/2505.24609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24609]] Explainable Depression Detection using Masked Hard Instance Mining(https://arxiv.org/abs/2505.24609)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>This paper addresses the critical need for improved explainability in text-based depression detection. While offering predictive outcomes, current solutions often overlook the understanding of model predictions which can hinder trust in the system. We propose the use of Masked Hard Instance Mining (MHIM) to enhance the explainability in the depression detection task. MHIM strategically masks attention weights within the model, compelling it to distribute attention across a wider range of salient features. We evaluate MHIM on two datasets representing distinct languages: Thai (Thai-Maywe) and English (DAIC-WOZ). Our results demonstrate that MHIM significantly improves performance in terms of both prediction accuracy and explainability metrics.</li>
</ul>

<h3>Title: Multi-criteria Rank-based Aggregation for Explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Sujoy Chatterjee, Everton Romanzini Colombo, Marcos Medeiros Raimundo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24612">https://arxiv.org/abs/2505.24612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24612">https://arxiv.org/pdf/2505.24612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24612]] Multi-criteria Rank-based Aggregation for Explainable AI(https://arxiv.org/abs/2505.24612)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>Explainability is crucial for improving the transparency of black-box machine learning models. With the advancement of explanation methods such as LIME and SHAP, various XAI performance metrics have been developed to evaluate the quality of explanations. However, different explainers can provide contrasting explanations for the same prediction, introducing trade-offs across conflicting quality metrics. Although available aggregation approaches improve robustness, reducing explanations' variability, very limited research employed a multi-criteria decision-making approach. To address this gap, this paper introduces a multi-criteria rank-based weighted aggregation method that balances multiple quality metrics simultaneously to produce an ensemble of explanation models. Furthermore, we propose rank-based versions of existing XAI metrics (complexity, faithfulness and stability) to better evaluate ranked feature importance explanations. Extensive experiments on publicly available datasets demonstrate the robustness of the proposed model across these metrics. Comparative analyses of various multi-criteria decision-making and rank aggregation algorithms showed that TOPSIS and WSUM are the best candidates for this use case.</li>
</ul>

<h3>Title: When Harry Meets Superman: The Role of The Interlocutor in Persona-Based Dialogue Generation</h3>
<ul>
<li><strong>Authors: </strong>Daniela Occhipinti, Marco Guerini, Malvina Nissim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24613">https://arxiv.org/abs/2505.24613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24613">https://arxiv.org/pdf/2505.24613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24613]] When Harry Meets Superman: The Role of The Interlocutor in Persona-Based Dialogue Generation(https://arxiv.org/abs/2505.24613)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Endowing dialogue agents with persona information has proven to significantly improve the consistency and diversity of their generations. While much focus has been placed on aligning dialogues with provided personas, the adaptation to the interlocutor's profile remains largely underexplored. In this work, we investigate three key aspects: (1) a model's ability to align responses with both the provided persona and the interlocutor's; (2) its robustness when dealing with familiar versus unfamiliar interlocutors and topics, and (3) the impact of additional fine-tuning on specific persona-based dialogues. We evaluate dialogues generated with diverse speaker pairings and topics, framing the evaluation as an author identification task and employing both LLM-as-a-judge and human evaluations. By systematically masking or disclosing information about the interlocutor, we assess its impact on dialogue generation. Results show that access to the interlocutor's persona improves the recognition of the target speaker, while masking it does the opposite. Although models generalise well across topics, they struggle with unfamiliar interlocutors. Finally, we found that in zero-shot settings, LLMs often copy biographical details, facilitating identification but trivialising the task.</li>
</ul>

<h3>Title: Harnessing Large Language Models for Scientific Novelty Detection</h3>
<ul>
<li><strong>Authors: </strong>Yan Liu, Zonglin Yang, Soujanya Poria, Thanh-Son Nguyen, Erik Cambria</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24615">https://arxiv.org/abs/2505.24615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24615">https://arxiv.org/pdf/2505.24615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24615]] Harnessing Large Language Models for Scientific Novelty Detection(https://arxiv.org/abs/2505.24615)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In an era of exponential scientific growth, identifying novel research ideas is crucial and challenging in academia. Despite potential, the lack of an appropriate benchmark dataset hinders the research of novelty detection. More importantly, simply adopting existing NLP technologies, e.g., retrieving and then cross-checking, is not a one-size-fits-all solution due to the gap between textual similarity and idea conception. In this paper, we propose to harness large language models (LLMs) for scientific novelty detection (ND), associated with two new datasets in marketing and NLP domains. To construct the considerate datasets for ND, we propose to extract closure sets of papers based on their relationship, and then summarize their main ideas based on LLMs. To capture idea conception, we propose to train a lightweight retriever by distilling the idea-level knowledge from LLMs to align ideas with similar conception, enabling efficient and accurate idea retrieval for LLM novelty detection. Experiments show our method consistently outperforms others on the proposed benchmark datasets for idea retrieval and ND tasks. Codes and data are available at this https URL.</li>
</ul>

<h3>Title: Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX</h3>
<ul>
<li><strong>Authors: </strong>Nikita Martynov, Anastasia Mordasheva, Dmitriy Gorbetskiy, Danil Astafurov, Ulyana Isaeva, Elina Basyrova, Sergey Skachkov, Victoria Berestova, Nikolay Ivanov, Valeriia Zanina, Alena Fenogenova</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24616">https://arxiv.org/abs/2505.24616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24616">https://arxiv.org/pdf/2505.24616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24616]] Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX(https://arxiv.org/abs/2505.24616)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>We introduce POLLUX, a comprehensive open-source benchmark designed to evaluate the generative capabilities of large language models (LLMs) in Russian. Our main contribution is a novel evaluation methodology that enhances the interpretability of LLM assessment. For each task type, we define a set of detailed criteria and develop a scoring protocol where models evaluate responses and provide justifications for their ratings. This enables transparent, criteria-driven evaluation beyond traditional resource-consuming, side-by-side human comparisons. POLLUX includes a detailed, fine-grained taxonomy of 35 task types covering diverse generative domains such as code generation, creative writing, and practical assistant use cases, totaling 2,100 manually crafted and professionally authored prompts. Each task is categorized by difficulty (easy/medium/hard), with experts constructing the dataset entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B) evaluators trained for nuanced assessment of generative outputs. This approach provides scalable, interpretable evaluation and annotation tools for model development, effectively replacing costly and less precise human judgments.</li>
</ul>

<h3>Title: Interpretable phenotyping of Heart Failure patients with Dutch discharge letters</h3>
<ul>
<li><strong>Authors: </strong>Vittorio Torri, Machteld J. Boonstra, Marielle C. van de Veerdonk, Deborah N. Kalkman, Alicia Uijl, Francesca Ieva, Ameen Abu-Hanna, Folkert W. Asselbergs, Iacer Calixto</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24619">https://arxiv.org/abs/2505.24619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24619">https://arxiv.org/pdf/2505.24619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24619]] Interpretable phenotyping of Heart Failure patients with Dutch discharge letters(https://arxiv.org/abs/2505.24619)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Objective: Heart failure (HF) patients present with diverse phenotypes affecting treatment and prognosis. This study evaluates models for phenotyping HF patients based on left ventricular ejection fraction (LVEF) classes, using structured and unstructured data, assessing performance and interpretability. Materials and Methods: The study analyzes all HF hospitalizations at both Amsterdam UMC hospitals (AMC and VUmc) from 2015 to 2023 (33,105 hospitalizations, 16,334 patients). Data from AMC were used for model training, and from VUmc for external validation. The dataset was unlabelled and included tabular clinical measurements and discharge letters. Silver labels for LVEF classes were generated by combining diagnosis codes, echocardiography results, and textual mentions. Gold labels were manually annotated for 300 patients for testing. Multiple Transformer-based (black-box) and Aug-Linear (white-box) models were trained and compared with baselines on structured and unstructured data. To evaluate interpretability, two clinicians annotated 20 discharge letters by highlighting information they considered relevant for LVEF classification. These were compared to SHAP and LIME explanations from black-box models and the inherent explanations of Aug-Linear models. Results: BERT-based and Aug-Linear models, using discharge letters alone, achieved the highest classification results (AUC=0.84 for BERT, 0.81 for Aug-Linear on external validation), outperforming baselines. Aug-Linear explanations aligned more closely with clinicians' explanations than post-hoc explanations on black-box models. Conclusions: Discharge letters emerged as the most informative source for phenotyping HF patients. Aug-Linear models matched black-box performance while providing clinician-aligned interpretability, supporting their use in transparent clinical decision-making.</li>
</ul>

<h3>Title: Benchmarking Large Language Models for Cryptanalysis and Mismatched-Generalization</h3>
<ul>
<li><strong>Authors: </strong>Utsav Maskey, Chencheng Zhu, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24621">https://arxiv.org/abs/2505.24621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24621">https://arxiv.org/pdf/2505.24621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24621]] Benchmarking Large Language Models for Cryptanalysis and Mismatched-Generalization(https://arxiv.org/abs/2505.24621)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have transformed natural language understanding and generation, leading to extensive benchmarking across diverse tasks. However, cryptanalysis a critical area for data security and encryption has not yet been thoroughly explored in LLM evaluations. To address this gap, we evaluate cryptanalytic potential of state of the art LLMs on encrypted texts generated using a range of cryptographic algorithms. We introduce a novel benchmark dataset comprising diverse plain texts spanning various domains, lengths, writing styles, and topics paired with their encrypted versions. Using zero-shot and few shot settings, we assess multiple LLMs for decryption accuracy and semantic comprehension across different encryption schemes. Our findings reveal key insights into the strengths and limitations of LLMs in side-channel communication while raising concerns about their susceptibility to jailbreaking attacks. This research highlights the dual-use nature of LLMs in security contexts and contributes to the ongoing discussion on AI safety and security.</li>
</ul>

<h3>Title: Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors</h3>
<ul>
<li><strong>Authors: </strong>Duo Zheng, Shijia Huang, Yanyang Li, Liwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24625">https://arxiv.org/abs/2505.24625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24625">https://arxiv.org/pdf/2505.24625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24625]] Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors(https://arxiv.org/abs/2505.24625)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method, the Video-3D Geometry Large Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that extracts 3D prior information from video sequences. This information is integrated with visual tokens and fed into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations.</li>
</ul>

<h3>Title: The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Junyi Li, Hwee Tou Ng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24630">https://arxiv.org/abs/2505.24630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24630">https://arxiv.org/pdf/2505.24630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24630]] The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models(https://arxiv.org/abs/2505.24630)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have significantly advanced in reasoning tasks through reinforcement learning (RL) optimization, achieving impressive capabilities across various challenging benchmarks. However, our empirical analysis reveals a critical drawback: reasoning-oriented RL fine-tuning significantly increases the prevalence of hallucinations. We theoretically analyze the RL training dynamics, identifying high-variance gradient, entropy-induced randomness, and susceptibility to spurious local optima as key factors leading to hallucinations. To address this drawback, we propose Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL fine-tuning algorithm incorporating explicit factuality verification at each reasoning step. FSPO leverages automated verification against given evidence to dynamically adjust token-level advantage values, incentivizing factual correctness throughout the reasoning process. Experiments across mathematical reasoning and hallucination benchmarks using Qwen2.5 and Llama models demonstrate that FSPO effectively reduces hallucinations while enhancing reasoning accuracy, substantially improving both reliability and performance.</li>
</ul>

<h3>Title: NUC-Net: Non-uniform Cylindrical Partition Network for Efficient LiDAR Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xuzhi Wang, Wei Feng, Lingdong Kong, Liang Wan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24634">https://arxiv.org/abs/2505.24634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24634">https://arxiv.org/pdf/2505.24634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24634]] NUC-Net: Non-uniform Cylindrical Partition Network for Efficient LiDAR Semantic Segmentation(https://arxiv.org/abs/2505.24634)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>LiDAR semantic segmentation plays a vital role in autonomous driving. Existing voxel-based methods for LiDAR semantic segmentation apply uniform partition to the 3D LiDAR point cloud to form a structured representation based on cartesian/cylindrical coordinates. Although these methods show impressive performance, the drawback of existing voxel-based methods remains in two aspects: (1) it requires a large enough input voxel resolution, which brings a large amount of computation cost and memory consumption. (2) it does not well handle the unbalanced point distribution of LiDAR point cloud. In this paper, we propose a non-uniform cylindrical partition network named NUC-Net to tackle the above challenges. Specifically, we propose the Arithmetic Progression of Interval (API) method to non-uniformly partition the radial axis and generate the voxel representation which is representative and efficient. Moreover, we propose a non-uniform multi-scale aggregation method to improve contextual information. Our method achieves state-of-the-art performance on SemanticKITTI and nuScenes datasets with much faster speed and much less training time. And our method can be a general component for LiDAR semantic segmentation, which significantly improves both the accuracy and efficiency of the uniform counterpart by $4 \times$ training faster and $2 \times$ GPU memory reduction and $3 \times$ inference speedup. We further provide theoretical analysis towards understanding why NUC is effective and how point distribution affects performance. Code is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Disentangling Language and Culture for Evaluating Multilingual Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Ying, Wei Tang, Yiran Zhao, Yixin Cao, Yu Rong, Wenxuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24635">https://arxiv.org/abs/2505.24635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24635">https://arxiv.org/pdf/2505.24635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24635]] Disentangling Language and Culture for Evaluating Multilingual Large Language Models(https://arxiv.org/abs/2505.24635)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces a Dual Evaluation Framework to comprehensively assess the multilingual capabilities of LLMs. By decomposing the evaluation along the dimensions of linguistic medium and cultural context, this framework enables a nuanced analysis of LLMs' ability to process questions within both native and cross-cultural contexts cross-lingually. Extensive evaluations are conducted on a wide range of models, revealing a notable "CulturalLinguistic Synergy" phenomenon, where models exhibit better performance when questions are culturally aligned with the language. This phenomenon is further explored through interpretability probing, which shows that a higher proportion of specific neurons are activated in a language's cultural context. This activation proportion could serve as a potential indicator for evaluating multilingual performance during model training. Our findings challenge the prevailing notion that LLMs, primarily trained on English data, perform uniformly across languages and highlight the necessity of culturally and linguistically model evaluations. Our code can be found at https://yingjiahao14. this http URL.</li>
</ul>

<h3>Title: Category-Level 6D Object Pose Estimation in Agricultural Settings Using a Lattice-Deformation Framework and Diffusion-Augmented Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Marios Glytsos, Panagiotis P. Filntisis, George Retsinas, Petros Maragos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24636">https://arxiv.org/abs/2505.24636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24636">https://arxiv.org/pdf/2505.24636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24636]] Category-Level 6D Object Pose Estimation in Agricultural Settings Using a Lattice-Deformation Framework and Diffusion-Augmented Synthetic Data(https://arxiv.org/abs/2505.24636)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Accurate 6D object pose estimation is essential for robotic grasping and manipulation, particularly in agriculture, where fruits and vegetables exhibit high intra-class variability in shape, size, and texture. The vast majority of existing methods rely on instance-specific CAD models or require depth sensors to resolve geometric ambiguities, making them impractical for real-world agricultural applications. In this work, we introduce PLANTPose, a novel framework for category-level 6D pose estimation that operates purely on RGB input. PLANTPose predicts both the 6D pose and deformation parameters relative to a base mesh, allowing a single category-level CAD model to adapt to unseen instances. This enables accurate pose estimation across varying shapes without relying on instance-specific data. To enhance realism and improve generalization, we also leverage Stable Diffusion to refine synthetic training images with realistic texturing, mimicking variations due to ripeness and environmental factors and bridging the domain gap between synthetic data and the real world. Our evaluations on a challenging benchmark that includes bananas of various shapes, sizes, and ripeness status demonstrate the effectiveness of our framework in handling large intraclass variations while maintaining accurate 6D pose predictions, significantly outperforming the state-of-the-art RGB-based approach MegaPose.</li>
</ul>

<h3>Title: Cloud Optical Thickness Retrievals Using Angle Invariant Attention Based Deep Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Zahid Hassan Tushar, Adeleke Ademakinwa, Jianwu Wang, Zhibo Zhang, Sanjay Purushotham</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24638">https://arxiv.org/abs/2505.24638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24638">https://arxiv.org/pdf/2505.24638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24638]] Cloud Optical Thickness Retrievals Using Angle Invariant Attention Based Deep Learning Models(https://arxiv.org/abs/2505.24638)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Cloud Optical Thickness (COT) is a critical cloud property influencing Earth's climate, weather, and radiation budget. Satellite radiance measurements enable global COT retrieval, but challenges like 3D cloud effects, viewing angles, and atmospheric interference must be addressed to ensure accurate estimation. Traditionally, the Independent Pixel Approximation (IPA) method, which treats individual pixels independently, has been used for COT estimation. However, IPA introduces significant bias due to its simplified assumptions. Recently, deep learning-based models have shown improved performance over IPA but lack robustness, as they are sensitive to variations in radiance intensity, distortions, and cloud shadows. These models also introduce substantial errors in COT estimation under different solar and viewing zenith angles. To address these challenges, we propose a novel angle-invariant, attention-based deep model called Cloud-Attention-Net with Angle Coding (CAAC). Our model leverages attention mechanisms and angle embeddings to account for satellite viewing geometry and 3D radiative transfer effects, enabling more accurate retrieval of COT. Additionally, our multi-angle training strategy ensures angle invariance. Through comprehensive experiments, we demonstrate that CAAC significantly outperforms existing state-of-the-art deep learning models, reducing cloud property retrieval errors by at least a factor of nine.</li>
</ul>

<h3>Title: Efficient Text Encoders for Labor Market Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jens-Joris Decorte, Jeroen Van Hautte, Chris Develder, Thomas Demeester</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24640">https://arxiv.org/abs/2505.24640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24640">https://arxiv.org/pdf/2505.24640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24640]] Efficient Text Encoders for Labor Market Analysis(https://arxiv.org/abs/2505.24640)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Labor market analysis relies on extracting insights from job advertisements, which provide valuable yet unstructured information on job titles and corresponding skill requirements. While state-of-the-art methods for skill extraction achieve strong performance, they depend on large language models (LLMs), which are computationally expensive and slow. In this paper, we propose \textbf{ConTeXT-match}, a novel contrastive learning approach with token-level attention that is well-suited for the extreme multi-label classification task of skill classification. \textbf{ConTeXT-match} significantly improves skill extraction efficiency and performance, achieving state-of-the-art results with a lightweight bi-encoder model. To support robust evaluation, we introduce \textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill annotations that explicitly address the redundancy in the large label space. Finally, we present \textbf{JobBERT V2}, an improved job title normalization model that leverages extracted skills to produce high-quality job title representations. Experiments demonstrate that our models are efficient, accurate, and scalable, making them ideal for large-scale, real-time labor market analysis.</li>
</ul>

<h3>Title: PRISM: A Framework for Producing Interpretable Political Bias Embeddings with Political-Aware Cross-Encoder</h3>
<ul>
<li><strong>Authors: </strong>Yiqun Sun, Qiang Huang, Anthony K. H. Tung, Jun Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24646">https://arxiv.org/abs/2505.24646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24646">https://arxiv.org/pdf/2505.24646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24646]] PRISM: A Framework for Producing Interpretable Political Bias Embeddings with Political-Aware Cross-Encoder(https://arxiv.org/abs/2505.24646)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Semantic Text Embedding is a fundamental NLP task that encodes textual content into vector representations, where proximity in the embedding space reflects semantic similarity. While existing embedding models excel at capturing general meaning, they often overlook ideological nuances, limiting their effectiveness in tasks that require an understanding of political bias. To address this gap, we introduce PRISM, the first framework designed to Produce inteRpretable polItical biaS eMbeddings. PRISM operates in two key stages: (1) Controversial Topic Bias Indicator Mining, which systematically extracts fine-grained political topics and their corresponding bias indicators from weakly labeled news data, and (2) Cross-Encoder Political Bias Embedding, which assigns structured bias scores to news articles based on their alignment with these indicators. This approach ensures that embeddings are explicitly tied to bias-revealing dimensions, enhancing both interpretability and predictive power. Through extensive experiments on two large-scale datasets, we demonstrate that PRISM outperforms state-of-the-art text embedding models in political bias classification while offering highly interpretable representations that facilitate diversified retrieval and ideological analysis. The source code is available at this https URL.</li>
</ul>

<h3>Title: MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR</h3>
<ul>
<li><strong>Authors: </strong>Dimitrios Damianos, Georgios Paraskevopoulos, Alexandros Potamianos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24656">https://arxiv.org/abs/2505.24656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24656">https://arxiv.org/pdf/2505.24656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24656]] MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR(https://arxiv.org/abs/2505.24656)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this work, we investigate the Meta PL unsupervised domain adaptation framework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage Domain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation approach that integrates self-supervised learning with semi-supervised techniques. MSDA is designed to enhance the robustness and generalization of ASR models, making them more adaptable to diverse conditions. It is particularly effective for low-resource languages like Greek and in weakly supervised scenarios where labeled data is scarce or noisy. Through extensive experiments, we demonstrate that Meta PL can be applied effectively to ASR tasks, achieving state-of-the-art results, significantly outperforming state-of-the-art methods, and providing more robust solutions for unsupervised domain adaptation in ASR. Our ablations highlight the necessity of utilizing a cascading approach when combining self-supervision with self-training.</li>
</ul>

<h3>Title: Decoupled Competitive Framework for Semi-supervised Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jiahe Chen, Jiahe Ying, Shen Wang, Jianwei Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24667">https://arxiv.org/abs/2505.24667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24667">https://arxiv.org/pdf/2505.24667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24667]] Decoupled Competitive Framework for Semi-supervised Medical Image Segmentation(https://arxiv.org/abs/2505.24667)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Confronting the critical challenge of insufficiently annotated samples in medical domain, semi-supervised medical image segmentation (SSMIS) emerges as a promising solution. Specifically, most methodologies following the Mean Teacher (MT) or Dual Students (DS) architecture have achieved commendable results. However, to date, these approaches face a performance bottleneck due to two inherent limitations, \textit{e.g.}, the over-coupling problem within MT structure owing to the employment of exponential moving average (EMA) mechanism, as well as the severe cognitive bias between two students of DS structure, both of which potentially lead to reduced efficacy, or even model collapse eventually. To mitigate these issues, a Decoupled Competitive Framework (DCF) is elaborated in this work, which utilizes a straightforward competition mechanism for the update of EMA, effectively decoupling students and teachers in a dynamical manner. In addition, the seamless exchange of invaluable and precise insights is facilitated among students, guaranteeing a better learning paradigm. The DCF introduced undergoes rigorous validation on three publicly accessible datasets, which encompass both 2D and 3D datasets. The results demonstrate the superiority of our method over previous cutting-edge competitors. Code will be available at this https URL.</li>
</ul>

<h3>Title: Multiple LLM Agents Debate for Equitable Cultural Alignment</h3>
<ul>
<li><strong>Authors: </strong>Dayeon Ki, Rachel Rudinger, Tianyi Zhou, Marine Carpuat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24671">https://arxiv.org/abs/2505.24671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24671">https://arxiv.org/pdf/2505.24671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24671]] Multiple LLM Agents Debate for Equitable Cultural Alignment(https://arxiv.org/abs/2505.24671)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) need to adapt their predictions to diverse cultural contexts to benefit diverse communities across the world. While previous efforts have focused on single-LLM, single-turn approaches, we propose to exploit the complementary strengths of multiple LLMs to promote cultural adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based agents debate over a cultural scenario and collaboratively reach a final decision. We propose two variants: one where either LLM agents exclusively debate and another where they dynamically choose between self-reflection and debate during their turns. We evaluate these approaches on 7 open-weight LLMs (and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette norms in 75 countries. Experiments show that debate improves both overall accuracy and cultural group parity over single-LLM baselines. Notably, multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies comparable to that of a much larger model (27B parameters).</li>
</ul>

<h3>Title: TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Xiaorui Wu, Xiaofeng Mao, Fei Li, Xin Zhang, Xuanhong Li, Chong Teng, Donghong Ji, Zhuang Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24672">https://arxiv.org/abs/2505.24672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24672">https://arxiv.org/pdf/2505.24672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24672]] TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis(https://arxiv.org/abs/2505.24672)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel in various natural language processing tasks but remain vulnerable to generating harmful content or being exploited for malicious purposes. Although safety alignment datasets have been introduced to mitigate such risks through supervised fine-tuning (SFT), these datasets often lack comprehensive risk coverage. Most existing datasets focus primarily on lexical diversity while neglecting other critical dimensions. To address this limitation, we propose a novel analysis framework to systematically measure the risk coverage of alignment datasets across three essential dimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We further introduce TRIDENT, an automated pipeline that leverages persona-based, zero-shot LLM generation to produce diverse and comprehensive instructions spanning these dimensions. Each harmful instruction is paired with an ethically aligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311 examples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on TRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29% reduction in Harm Score, and a 20% decrease in Attack Success Rate compared to the best-performing baseline model fine-tuned on the WildBreak dataset.</li>
</ul>

<h3>Title: A Simple Linear Patch Revives Layer-Pruned Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinrui Chen, Haoli Bai, Tao Yuan, Ruikang Liu, Kang Zhao, Xianzhi Yu, Lu Hou, Tian Guan, Yonghong He, Chun Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24680">https://arxiv.org/abs/2505.24680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24680">https://arxiv.org/pdf/2505.24680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24680]] A Simple Linear Patch Revives Layer-Pruned Large Language Models(https://arxiv.org/abs/2505.24680)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Layer pruning has become a popular technique for compressing large language models (LLMs) due to its simplicity. However, existing layer pruning methods often suffer from significant performance drops. We identify that this degradation stems from the mismatch of activation magnitudes across layers and tokens at the pruning interface. To address this, we propose LinearPatch, a simple plug-and-play technique to revive the layer-pruned LLMs. The proposed method adopts Hadamard transformation to suppress massive outliers in particular tokens, and channel-wise scaling to align the activation magnitudes. These operations can be fused into a single matrix, which functions as a patch to bridge the pruning interface with negligible inference overhead. LinearPatch retains up to 94.15% performance of the original model when pruning 5 layers of LLaMA-3-8B on the question answering benchmark, surpassing existing state-of-the-art methods by 4%. In addition, the patch matrix can be further optimized with memory efficient offline knowledge distillation. With only 5K samples, the retained performance of LinearPatch can be further boosted to 95.16% within 30 minutes on a single computing card.</li>
</ul>

<h3>Title: Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs</h3>
<ul>
<li><strong>Authors: </strong>Zihao Chen, Yu Xiang, Wenyong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24684">https://arxiv.org/abs/2505.24684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24684">https://arxiv.org/pdf/2505.24684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24684]] Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs(https://arxiv.org/abs/2505.24684)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Despite the success in learning semantically meaningful, unsupervised disentangled representations, variational autoencoders (VAEs) and their variants face a fundamental theoretical challenge: substantial evidence indicates that unsupervised disentanglement is unattainable without implicit inductive bias, yet such bias remains elusive. In this work, we focus on exploring the implicit inductive bias that drive disentanglement in VAEs with factorization priors. By analyzing the total correlation in \b{eta}-TCVAE, we uncover a crucial implicit inductive bias called disentangling granularity, which leads to the discovery of an interesting "V"-shaped optimal Evidence Lower Bound (ELBO) trajectory within the parameter space. This finding is validated through over 100K experiments using factorized VAEs and our newly proposed model, \b{eta}-STCVAE. Notably, experimental results reveal that conventional factorized VAEs, constrained by fixed disentangling granularity, inherently tend to disentangle low-complexity feature. Whereas, appropriately tuning disentangling granularity, as enabled by \b{eta}-STCVAE, broadens the range of disentangled representations, allowing for the disentanglement of high-complexity features. Our findings unveil that disentangling granularity as an implicit inductive bias in factorized VAEs influence both disentanglement performance and the inference of the ELBO, offering fresh insights into the interpretability and inherent biases of VAEs.</li>
</ul>

<h3>Title: So, I climbed to the top of the pyramid of pain -- now what?</h3>
<ul>
<li><strong>Authors: </strong>Vasilis Katos, Emily Rosenorn-Lanng, Jane Henriksen-Bulmer, Ala Yankouskaya</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24685">https://arxiv.org/abs/2505.24685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24685">https://arxiv.org/pdf/2505.24685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24685]] So, I climbed to the top of the pyramid of pain -- now what?(https://arxiv.org/abs/2505.24685)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>This paper explores the evolving dynamics of cybersecurity in the age of advanced AI, from the perspective of the introduced Human Layer Kill Chain framework. As traditional attack models like Lockheed Martin's Cyber Kill Chain become inadequate in addressing human vulnerabilities exploited by modern adversaries, the Humal Layer Kill Chain offers a nuanced approach that integrates human psychology and behaviour into the analysis of cyber threats. We detail the eight stages of the Human Layer Kill Chain, illustrating how AI-enabled techniques can enhance psychological manipulation in attacks. By merging the Human Layer with the Cyber Kill Chain, we propose a Sociotechnical Kill Plane that allows for a holistic examination of attackers' tactics, techniques, and procedures (TTPs) across the sociotechnical landscape. This framework not only aids cybersecurity professionals in understanding adversarial methods, but also empowers non-technical personnel to engage in threat identification and response. The implications for incident response and organizational resilience are significant, particularly as AI continues to shape the threat landscape.</li>
</ul>

<h3>Title: Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration</h3>
<ul>
<li><strong>Authors: </strong>Qinglin Zhu, Runcong Zhao, Hanqi Yan, Yulan He, Yudong Chen, Lin Gui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24688">https://arxiv.org/abs/2505.24688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24688">https://arxiv.org/pdf/2505.24688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24688]] Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration(https://arxiv.org/abs/2505.24688)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) struggle with complex reasoning due to limited diversity and inefficient search. We propose Soft Reasoning, an embedding-based search framework that optimises the embedding of the first token to guide generation. It combines (1) embedding perturbation for controlled exploration and (2) Bayesian optimisation to refine embeddings via a verifier-guided objective, balancing exploration and exploitation. This approach improves reasoning accuracy and coherence while avoiding reliance on heuristic search. Experiments demonstrate superior correctness with minimal computation, making it a scalable, model-agnostic solution.</li>
</ul>

<h3>Title: BPE Stays on SCRIPT: Structured Encoding for Robust Multilingual Pretokenization</h3>
<ul>
<li><strong>Authors: </strong>Sander Land, Catherine Arnett</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24689">https://arxiv.org/abs/2505.24689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24689">https://arxiv.org/pdf/2505.24689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24689]] BPE Stays on SCRIPT: Structured Encoding for Robust Multilingual Pretokenization(https://arxiv.org/abs/2505.24689)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Byte Pair Encoding (BPE) tokenizers, widely used in Large Language Models, face challenges in multilingual settings, including penalization of non-Western scripts and the creation of tokens with partial UTF-8 sequences. Pretokenization, often reliant on complex regular expressions, can also introduce fragility and unexpected edge cases. We propose SCRIPT (Script Category Representation in PreTokenization), a novel encoding scheme that bypasses UTF-8 byte conversion by using initial tokens based on Unicode script and category properties. This approach enables a simple, rule-based pretokenization strategy that respects script boundaries, offering a robust alternative to pretokenization strategies based on regular expressions. We also introduce and validate a constrained BPE merging strategy that enforces character integrity, applicable to both SCRIPT-BPE and byte-based BPE. Our experiments demonstrate that SCRIPT-BPE achieves competitive compression while eliminating encoding-based penalties for non-Latin-script languages.</li>
</ul>

<h3>Title: Next Generation Authentication for Data Spaces: An Authentication Flow Based On Grant Negotiation And Authorization Protocol For Verifiable Presentations (GNAP4VP)</h3>
<ul>
<li><strong>Authors: </strong>Rodrigo MenÃ©ndez, Andres Munoz-Arcentales, JoaquÃ­n SalvachÃºa, Carlos Aparicio, Irene Plaza, Gabriel Huecas</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24698">https://arxiv.org/abs/2505.24698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24698">https://arxiv.org/pdf/2505.24698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24698]] Next Generation Authentication for Data Spaces: An Authentication Flow Based On Grant Negotiation And Authorization Protocol For Verifiable Presentations (GNAP4VP)(https://arxiv.org/abs/2505.24698)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, robust</a></li>
<li><strong>Abstract: </strong>Identity verification in Data Spaces is a fundamental aspect of ensuring security and privacy in digital environments. This paper presents an identity verification protocol tailored for shared data environments within Data Spaces. This protocol extends the Grant Negotiation and Authorization Protocol (GNAP) and integrates OpenID Connect for Verifiable Presentations (OIDC4VP) along with support for Linked Verifiable Presentations (LVP), providing a robust foundation for secure and privacy-preserving interactions. The proposed solution adheres to the principles of Self-Sovereign Identity (SSI) to facilitate decentralized, user-centric identity management while maintaining flexibility through protocol negotiation. Two alternative interaction flows are introduced: a "Wallet-Driven Interaction" utilizing OIDC4VP, and a "LVP Authorization" model for fully automated machine-to-machine communication. These flows address critical challenges encountered in Data Spaces, including privacy, interoperability, and regulatory compliance while simultaneously ensuring scalability and minimizing trust assumptions. The paper provides a detailed technical design, outlining the implementation considerations, and demonstrating how the proposed flows guarantee verifiable, secure, and efficient interactions between participants. This work contributes towards the establishment of a more trustworthy and sovereign digital infrastructure, in alignment with emerging European data governance initiatives.</li>
</ul>

<h3>Title: Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison</h3>
<ul>
<li><strong>Authors: </strong>Tejul Pandit, Meet Raval, Dhvani Upadhyay</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24701">https://arxiv.org/abs/2505.24701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24701">https://arxiv.org/pdf/2505.24701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24701]] Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison(https://arxiv.org/abs/2505.24701)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions but often suffers from the scarcity of diverse, labeled datasets that reflect real-world conversational nuances. This paper presents an approach for generating synthetic ABSA data using Large Language Models (LLMs) to address this gap. We detail the generation process aimed at producing data with consistent topic and sentiment distributions across multiple domains using GPT-4o. The quality and utility of the generated data were evaluated by assessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro, Claude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification tasks. Our results demonstrate the effectiveness of the synthetic data, revealing distinct performance trade-offs among the models: DeepSeekR1 showed higher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall, and Gemini 1.5 Pro offered significantly faster inference. We conclude that LLM-based synthetic data generation is a viable and flexible method for creating valuable ABSA resources, facilitating research and model evaluation without reliance on limited or inaccessible real-world labeled data.</li>
</ul>

<h3>Title: PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches</h3>
<ul>
<li><strong>Authors: </strong>Dennis Jacob, Chong Xiang, Prateek Mittal</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24703">https://arxiv.org/abs/2505.24703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24703">https://arxiv.org/pdf/2505.24703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24703]] PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches(https://arxiv.org/abs/2505.24703)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep learning techniques have enabled vast improvements in computer vision technologies. Nevertheless, these models are vulnerable to adversarial patch attacks which catastrophically impair performance. The physically realizable nature of these attacks calls for certifiable defenses, which feature provable guarantees on robustness. While certifiable defenses have been successfully applied to single-label classification, limited work has been done for multi-label classification. In this work, we present PatchDEMUX, a certifiably robust framework for multi-label classifiers against adversarial patches. Our approach is a generalizable method which can extend any existing certifiable defense for single-label classification; this is done by considering the multi-label classification task as a series of isolated binary classification problems to provably guarantee robustness. Furthermore, in the scenario where an attacker is limited to a single patch we propose an additional certification procedure that can provide tighter robustness bounds. Using the current state-of-the-art (SOTA) single-label certifiable defense PatchCleanser as a backbone, we find that PatchDEMUX can achieve non-trivial robustness on the MS-COCO and PASCAL VOC datasets while maintaining high clean performance</li>
</ul>

<h3>Title: RT-X Net: RGB-Thermal cross attention network for Low-Light Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Raman Jha, Adithya Lenka, Mani Ramanagopal, Aswin Sankaranarayanan, Kaushik Mitra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24705">https://arxiv.org/abs/2505.24705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24705">https://arxiv.org/pdf/2505.24705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24705]] RT-X Net: RGB-Thermal cross attention network for Low-Light Image Enhancement(https://arxiv.org/abs/2505.24705)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In nighttime conditions, high noise levels and bright illumination sources degrade image quality, making low-light image enhancement challenging. Thermal images provide complementary information, offering richer textures and structural details. We propose RT-X Net, a cross-attention network that fuses RGB and thermal images for nighttime image enhancement. We leverage self-attention networks for feature extraction and a cross-attention mechanism for fusion to effectively integrate information from both modalities. To support research in this domain, we introduce the Visible-Thermal Image Enhancement Evaluation (V-TIEE) dataset, comprising 50 co-located visible and thermal images captured under diverse nighttime conditions. Extensive evaluations on the publicly available LLVIP dataset and our V-TIEE dataset demonstrate that RT-X Net outperforms state-of-the-art methods in low-light image enhancement. The code and the V-TIEE can be found here this https URL.</li>
</ul>

<h3>Title: On Symmetric Losses for Robust Policy Optimization with Noisy Preferences</h3>
<ul>
<li><strong>Authors: </strong>Soichiro Nishimori, Yu-Jie Zhang, Thanawat Lodkaew, Masashi Sugiyama</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24709">https://arxiv.org/abs/2505.24709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24709">https://arxiv.org/pdf/2505.24709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24709]] On Symmetric Losses for Robust Policy Optimization with Noisy Preferences(https://arxiv.org/abs/2505.24709)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Optimizing policies based on human preferences is key to aligning language models with human intent. This work focuses on reward modeling, a core component in reinforcement learning from human feedback (RLHF), and offline preference optimization, such as direct preference optimization. Conventional approaches typically assume accurate annotations. However, real-world preference data often contains noise due to human errors or biases. We propose a principled framework for robust policy optimization under noisy preferences, viewing reward modeling as a classification problem. This allows us to leverage symmetric losses, known for their robustness to label noise in classification, leading to our Symmetric Preference Optimization (SymPO) method. We prove that symmetric losses enable successful policy optimization even under noisy labels, as the resulting reward remains rank-preserving -- a property sufficient for policy improvement. Experiments on synthetic and real-world tasks demonstrate the effectiveness of SymPO.</li>
</ul>

<h3>Title: Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting</h3>
<ul>
<li><strong>Authors: </strong>Wei Chen, Jiahao Zhang, Haipeng Zhu, Boyan Xu, Zhifeng Hao, Keli Zhang, Junjian Ye, Ruichu Cai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24710">https://arxiv.org/abs/2505.24710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24710">https://arxiv.org/pdf/2505.24710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24710]] Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting(https://arxiv.org/abs/2505.24710)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown great potential in decision-making due to the vast amount of knowledge stored within the models. However, these pre-trained models are prone to lack reasoning abilities and are difficult to adapt to new environments, further hindering their application to complex real-world tasks. To address these challenges, inspired by the human cognitive process, we propose Causal-aware LLMs, which integrate the structural causal model (SCM) into the decision-making process to model, update, and utilize structured knowledge of the environment in a ``learning-adapting-acting" paradigm. Specifically, in the learning stage, we first utilize an LLM to extract the environment-specific causal entities and their causal relations to initialize a structured causal model of the environment. Subsequently,in the adapting stage, we update the structured causal model through external feedback about the environment, via an idea of causal intervention. Finally, in the acting stage, Causal-aware LLMs exploit structured causal knowledge for more efficient policy-making through the reinforcement learning agent. The above processes are performed iteratively to learn causal knowledge, ultimately enabling the causal-aware LLMs to achieve a more accurate understanding of the environment and make more efficient decisions. Experimental results across 22 diverse tasks within the open-world game ``Crafter" validate the effectiveness of our proposed method.</li>
</ul>

<h3>Title: HESEIA: A community-based dataset for evaluating social biases in large language models, co-designed in real school settings in Latin America</h3>
<ul>
<li><strong>Authors: </strong>Guido Ivetta (1 and 2), Marcos J. Gomez (1 and 2), SofÃ­a Martinelli (1), Pietro Palombini (1), M. Emilia Echeveste (1 and 2), Nair Carolina Mazzeo (2), Beatriz Busaniche (2), Luciana Benotti (1 and 2) ((1) Universidad Nacional de CÃ³rdoba, Argentina, (2) FundaciÃ³n VÃ­a Libre)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24712">https://arxiv.org/abs/2505.24712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24712">https://arxiv.org/pdf/2505.24712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24712]] HESEIA: A community-based dataset for evaluating social biases in large language models, co-designed in real school settings in Latin America(https://arxiv.org/abs/2505.24712)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Most resources for evaluating social biases in Large Language Models are developed without co-design from the communities affected by these biases, and rarely involve participatory approaches. We introduce HESEIA, a dataset of 46,499 sentences created in a professional development course. The course involved 370 high-school teachers and 5,370 students from 189 Latin-American schools. Unlike existing benchmarks, HESEIA captures intersectional biases across multiple demographic axes and school subjects. It reflects local contexts through the lived experience and pedagogical expertise of educators. Teachers used minimal pairs to create sentences that express stereotypes relevant to their school subjects and communities. We show the dataset diversity in term of demographic axes represented and also in terms of the knowledge areas included. We demonstrate that the dataset contains more stereotypes unrecognized by current LLMs than previous datasets. HESEIA is available to support bias assessments grounded in educational communities.</li>
</ul>

<h3>Title: Voice Conversion Improves Cross-Domain Robustness for Spoken Arabic Dialect Identification</h3>
<ul>
<li><strong>Authors: </strong>Badr M. Abdullah, Matthew Baas, Bernd MÃ¶bius, Dietrich Klakow</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24713">https://arxiv.org/abs/2505.24713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24713">https://arxiv.org/pdf/2505.24713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24713]] Voice Conversion Improves Cross-Domain Robustness for Spoken Arabic Dialect Identification(https://arxiv.org/abs/2505.24713)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Arabic dialect identification (ADI) systems are essential for large-scale data collection pipelines that enable the development of inclusive speech technologies for Arabic language varieties. However, the reliability of current ADI systems is limited by poor generalization to out-of-domain speech. In this paper, we present an effective approach based on voice conversion for training ADI models that achieves state-of-the-art performance and significantly improves robustness in cross-domain scenarios. Evaluated on a newly collected real-world test set spanning four different domains, our approach yields consistent improvements of up to +34.1% in accuracy across domains. Furthermore, we present an analysis of our approach and demonstrate that voice conversion helps mitigate the speaker bias in the ADI dataset. We release our robust ADI model and cross-domain evaluation dataset to support the development of inclusive speech technologies for Arabic.</li>
</ul>

<h3>Title: FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Junyu Luo, Zhizhuo Kou, Liming Yang, Xiao Luo, Jinsheng Huang, Zhiping Xiao, Jingshu Peng, Chengzhong Liu, Jiaming Ji, Xuanzhe Liu, Sirui Han, Ming Zhang, Yike Guo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24714">https://arxiv.org/abs/2505.24714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24714">https://arxiv.org/pdf/2505.24714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24714]] FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation(https://arxiv.org/abs/2505.24714)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, in the financial domain, there is a notable lack of effective and specialized multimodal evaluation datasets. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20 annotators and carefully designed validation mechanisms. Additionally, we develop FinScore, an evaluation system incorporating hallucination penalties and multi-dimensional capability assessment to provide an unbiased evaluation. Extensive experimental results demonstrate that even state-of-the-art models like GPT-4o exhibit unsatisfactory performance on FinMME, highlighting its challenging nature. The benchmark exhibits high robustness with prediction variations under different prompts remaining below 1%, demonstrating superior reliability compared to existing datasets. Our dataset and evaluation protocol are available at this https URL and this https URL.</li>
</ul>

<h3>Title: PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Holzschuh, Qiang Liu, Georg Kohl, Nils Thuerey</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24717">https://arxiv.org/abs/2505.24717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24717">https://arxiv.org/pdf/2505.24717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24717]] PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations(https://arxiv.org/abs/2505.24717)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We introduce PDE-Transformer, an improved transformer-based architecture for surrogate modeling of physics simulations on regular grids. We combine recent architectural improvements of diffusion transformers with adjustments specific for large-scale simulations to yield a more scalable and versatile general-purpose transformer architecture, which can be used as the backbone for building large-scale foundation models in physical sciences. We demonstrate that our proposed architecture outperforms state-of-the-art transformer architectures for computer vision on a large dataset of 16 different types of PDEs. We propose to embed different physical channels individually as spatio-temporal tokens, which interact via channel-wise self-attention. This helps to maintain a consistent information density of tokens when learning multiple types of PDEs simultaneously. We demonstrate that our pre-trained models achieve improved performance on several challenging downstream tasks compared to training from scratch and also beat other foundation model architectures for physics simulations.</li>
</ul>

<h3>Title: Reinforcing Video Reasoning with Focused Thinking</h3>
<ul>
<li><strong>Authors: </strong>Jisheng Dang, Jingze Wu, Teng Wang, Xuanhui Lin, Nannan Zhu, Hongbo Chen, Wei-Shi Zheng, Meng Wang, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24718">https://arxiv.org/abs/2505.24718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24718">https://arxiv.org/pdf/2505.24718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24718]] Reinforcing Video Reasoning with Focused Thinking(https://arxiv.org/abs/2505.24718)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in reinforcement learning, particularly through Group Relative Policy Optimization (GRPO), have significantly improved multimodal large language models for complex reasoning tasks. However, two critical limitations persist: 1) they often produce unfocused, verbose reasoning chains that obscure salient spatiotemporal cues and 2) binary rewarding fails to account for partially correct answers, resulting in high reward variance and inefficient learning. In this paper, we propose TW-GRPO, a novel framework that enhances visual reasoning with focused thinking and dense reward granularity. Specifically, we employs a token weighting mechanism that prioritizes tokens with high informational density (estimated by intra-group variance), suppressing redundant tokens like generic reasoning prefixes. Furthermore, we reformulate RL training by shifting from single-choice to multi-choice QA tasks, where soft rewards enable finer-grained gradient estimation by distinguishing partial correctness. Additionally, we propose question-answer inversion, a data augmentation strategy to generate diverse multi-choice samples from existing benchmarks. Experiments demonstrate state-of-the-art performance on several video reasoning and general understanding benchmarks. Notably, TW-GRPO achieves 50.4\% accuracy on CLEVRER (18.8\% improvement over Video-R1) and 65.8\% on MMVU. Our codes are available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts</h3>
<ul>
<li><strong>Authors: </strong>Neil He, Rishabh Anand, Hiren Madhu, Ali Maatouk, Smita Krishnaswamy, Leandros Tassiulas, Menglin Yang, Rex Ying</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24722">https://arxiv.org/abs/2505.24722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24722">https://arxiv.org/pdf/2505.24722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24722]] HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts(https://arxiv.org/abs/2505.24722)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining.</li>
</ul>

<h3>Title: Talking Transactions: Decentralized Communication through Ethereum Input Data Messages (IDMs)</h3>
<ul>
<li><strong>Authors: </strong>Xihan Xiong, Zhipeng Wang, Qin Wang, Endong Liu, Pascal Berrang, William Knottenbelt</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24724">https://arxiv.org/abs/2505.24724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24724">https://arxiv.org/pdf/2505.24724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24724]] Talking Transactions: Decentralized Communication through Ethereum Input Data Messages (IDMs)(https://arxiv.org/abs/2505.24724)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Can you imagine, blockchain transactions can talk! In this paper, we study how they talk and what they talk about. We focus on the input data field of Ethereum transactions, which is designed to allow external callers to interact with smart contracts. In practice, this field also enables users to embed natural language messages into transactions. Users can leverage these Input Data Messages (IDMs) for peer-to-peer communication. This means that, beyond Ethereum's well-known role as a financial infrastructure, it also serves as a decentralized communication medium. We present the first large-scale analysis of Ethereum IDMs from the genesis block to February 2024 (3134 days). We filter IDMs to extract 867,140 transactions with informative IDMs and use LLMs for language detection. We find that English (95.4%) and Chinese (4.4%) dominate the use of natural languages in IDMs. Interestingly, English IDMs center on security and scam warnings (24%) with predominantly negative emotions, while Chinese IDMs emphasize emotional expression and social connection (44%) with a more positive tone. We also observe that longer English IDMs often transfer high ETH values for protocol-level purposes, while longer Chinese IDMs tend to involve symbolic transfer amounts for emotional intent. Moreover, we find that the IDM participants tend to form small, loosely connected communities (59.99%). Our findings highlight culturally and functionally divergent use cases of the IDM channel across user communities. We further examine the security relevance of IDMs in on-chain attacks. Many victims use them to appeal to attackers for fund recovery. IDMs containing negotiations or reward offers are linked to higher reply rates. We also analyze IDMs' regulatory implications. Their misuse for abuse, threats, and sexual solicitation reveals the urgent need for content moderation and regulation in decentralized systems.</li>
</ul>

<h3>Title: Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Shelly Bensal, Umar Jamil, Christopher Bryant, Melisa Russak, Kiran Kamble, Dmytro Mozolevskyi, Muayad Ali, Waseem AlShikh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24726">https://arxiv.org/abs/2505.24726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24726">https://arxiv.org/pdf/2505.24726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24726]] Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning(https://arxiv.org/abs/2505.24726)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We explore a method for improving the performance of large language models through self-reflection and reinforcement learning. By incentivizing the model to generate better self-reflections when it answers incorrectly, we demonstrate that a model's ability to solve complex, verifiable tasks can be enhanced even when generating synthetic data is infeasible and only binary feedback is available. Our framework operates in two stages: first, upon failing a given task, the model generates a self-reflective commentary analyzing its previous attempt; second, the model is given another attempt at the task with the self-reflection in context. If the subsequent attempt succeeds, the tokens generated during the self-reflection phase are rewarded. Our experimental results show substantial performance gains across a variety of model architectures, as high as 34.7% improvement at math equation writing and 18.1% improvement at function calling. Notably, smaller fine-tuned models (1.5 billion to 7 billion parameters) outperform models in the same family that are 10 times larger. Our novel paradigm is thus an exciting pathway to more useful and reliable language models that can self-improve on challenging tasks with limited external feedback.</li>
</ul>

<h3>Title: Robust Federated Learning against Model Perturbation in Edge Networks</h3>
<ul>
<li><strong>Authors: </strong>Dongzi Jin, Yong Xiao, Yingyu Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24728">https://arxiv.org/abs/2505.24728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24728">https://arxiv.org/pdf/2505.24728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24728]] Robust Federated Learning against Model Perturbation in Edge Networks(https://arxiv.org/abs/2505.24728)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a promising paradigm for realizing edge intelligence, allowing collaborative learning among distributed edge devices by sharing models instead of raw data. However, the shared models are often assumed to be ideal, which would be inevitably violated in practice due to various perturbations, leading to significant performance degradation. To overcome this challenge, we propose a novel method, termed Sharpness-Aware Minimization-based Robust Federated Learning (SMRFL), which aims to improve model robustness against perturbations by exploring the geometrical property of the model landscape. Specifically, SMRFL solves a min-max optimization problem that promotes model convergence towards a flat minimum by minimizing the maximum loss within a neighborhood of the model parameters. In this way, model sensitivity to perturbations is reduced, and robustness is enhanced since models in the neighborhood of the flat minimum also enjoy low loss values. The theoretical result proves that SMRFL can converge at the same rate as FL without perturbations. Extensive experimental results show that SMRFL significantly enhances robustness against perturbations compared to three baseline methods on two real-world datasets under three perturbation scenarios.</li>
</ul>

<h3>Title: Circuit Stability Characterizes Language Model Generalization</h3>
<ul>
<li><strong>Authors: </strong>Alan Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24731">https://arxiv.org/abs/2505.24731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24731">https://arxiv.org/pdf/2505.24731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24731]] Circuit Stability Characterizes Language Model Generalization(https://arxiv.org/abs/2505.24731)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Extensively evaluating the capabilities of (large) language models is difficult. Rapid development of state-of-the-art models induce benchmark saturation, while creating more challenging datasets is labor-intensive. Inspired by the recent developments in mechanistic interpretability, we introduce circuit stability as a new way to assess model performance. Circuit stability refers to a model's ability to apply a consistent reasoning process-its circuit-across various inputs. We mathematically formalize circuit stability and circuit equivalence. Then, through three case studies, we empirically show that circuit stability and the lack thereof can characterize and predict different aspects of generalization. Our proposed methods offer a step towards rigorously relating the generality of models to their interpretability.</li>
</ul>

<h3>Title: Authentication and authorization in Data Spaces: A relationship-based access control approach for policy specification based on ODRL</h3>
<ul>
<li><strong>Authors: </strong>Irene Plaza-Ortiz, Andres Munoz-Arcentales, JoaquÃ­n SalvachÃºa, Carlos Aparicio, Gabriel Huecas, Enrique Barra</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24742">https://arxiv.org/abs/2505.24742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24742">https://arxiv.org/pdf/2505.24742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24742]] Authentication and authorization in Data Spaces: A relationship-based access control approach for policy specification based on ODRL(https://arxiv.org/abs/2505.24742)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Data has become a crucial resource in the digital economy, fostering initiatives for secure and sovereign data sharing frameworks such as Data Spaces. However, these distributed environments require fine-grained access control mechanisms that balance openness with sovereignty and security. This paper proposes an extension of the Open Digital Rights Language (ODRL) standard, the ODRL Data Spaces (ODS) profile, aimed at supporting authorization and complementing existing authentication mechanisms throughout the data lifecycle. Additionally, a policy execution engine is introduced to translate ODRL policies into executable formats, enabling effective enforcement. The approach is validated through a use case involving OpenFGA, demonstrating its applicability to relationship-based access control scenarios.</li>
</ul>

<h3>Title: SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training</h3>
<ul>
<li><strong>Authors: </strong>Yehonathan Refael, Guy Smorodinsky, Tom Tirer, Ofir Lindenbaum</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24749">https://arxiv.org/abs/2505.24749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24749">https://arxiv.org/pdf/2505.24749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24749]] SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training(https://arxiv.org/abs/2505.24749)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Low-rank gradient-based optimization methods have significantly improved memory efficiency during the training of large language models (LLMs), enabling operations within constrained hardware without sacrificing performance. However, these methods primarily emphasize memory savings, often overlooking potential acceleration in convergence due to their reliance on standard isotropic steepest descent techniques, which can perform suboptimally in the highly anisotropic landscapes typical of deep networks, particularly LLMs. In this paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an optimizer that employs exact singular value decomposition (SVD) for moment orthogonalization within a dynamically adapted low-dimensional subspace, enabling norm-inducing steepest descent optimization steps. By explicitly aligning optimization steps with the spectral characteristics of the loss landscape, SUMO effectively mitigates approximation errors associated with commonly used methods like Newton-Schulz orthogonalization approximation. We theoretically establish an upper bound on these approximation errors, proving their dependence on the condition numbers of moments, conditions we analytically demonstrate are encountered during LLM training. Furthermore, we both theoretically and empirically illustrate that exact orthogonalization via SVD substantially improves convergence rates while reducing overall complexity. Empirical evaluations confirm that SUMO accelerates convergence, enhances stability, improves performance, and reduces memory requirements by up to 20% compared to state-of-the-art methods.</li>
</ul>

<h3>Title: LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews</h3>
<ul>
<li><strong>Authors: </strong>Christian Jaumann, Andreas Wiedholz, Annemarie Friedrich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24757">https://arxiv.org/abs/2505.24757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24757">https://arxiv.org/pdf/2505.24757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24757]] LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews(https://arxiv.org/abs/2505.24757)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The scientific literature is growing rapidly, making it hard to keep track of the state-of-the-art. Systematic literature reviews (SLRs) aim to identify and evaluate all relevant papers on a topic. After retrieving a set of candidate papers, the abstract screening phase determines initial relevance. To date, abstract screening methods using large language models (LLMs) focus on binary classification settings; existing question answering (QA) based ranking approaches suffer from error propagation. LLMs offer a unique opportunity to evaluate the SLR's inclusion and exclusion criteria, yet, existing benchmarks do not provide them exhaustively. We manually extract these criteria as well as research questions for 57 SLRs, mostly in the medical domain, enabling principled comparisons between approaches. Moreover, we propose LGAR, a zero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance scorer and a dense re-ranker. Our extensive experiments show that LGAR outperforms existing QA-based methods by 5-10 pp. in mean average precision. Our code and data is publicly available.</li>
</ul>

<h3>Title: From Macro to Micro: Probing Dataset Diversity in Language Model Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Li, Xuhong Li, Yiming Dong, Kun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24768">https://arxiv.org/abs/2505.24768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24768">https://arxiv.org/pdf/2505.24768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24768]] From Macro to Micro: Probing Dataset Diversity in Language Model Fine-Tuning(https://arxiv.org/abs/2505.24768)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Dataset diversity plays a pivotal role for the successful training of many machine learning models, particularly in the supervised fine-tuning (SFT) stage of large language model (LLM) development. Despite increasing recognition of its importance, systematic analyses of dataset diversity still remain underexplored. To address this gap, this work presents a systematic taxonomy of existing diversity-control strategies, which primarily focus on the instruction component, operating at either macroscopic (entire instruction semantics) or mesoscopic levels (instruction units), and furthermore introduces a novel analysis of microscopic diversity within the response component, specifically analyzing the statistical distribution of tokens in SFT training samples. In the experimental evaluation, we construct fixed-size datasets (e.g., 10,000 samples each) from a corpus of 117,000 open-source SFT samples, incorporating six distinct diversity-control strategies spanning macro-, meso-, and microscopic levels applied to both instructions and responses. We then fine-tune LLMs on these datasets to assess the six diversity-control strategies. Results reveal that while macroscopic and mesoscopic strategies lead to higher performance with increasing diversity, the microscopic strategy in responses exhibits both a stronger correlation between model performance and the degree of diversity and superior performance with maximum diversity across all strategies. These findings offer actionable insights for constructing high-performance SFT datasets.</li>
</ul>

<h3>Title: AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption</h3>
<ul>
<li><strong>Authors: </strong>Yajie Zhou, Xiaoyi Pang, Zhibo Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24773">https://arxiv.org/abs/2505.24773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24773">https://arxiv.org/pdf/2505.24773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24773]] AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption(https://arxiv.org/abs/2505.24773)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, large language model</a></li>
<li><strong>Abstract: </strong>Federated fine-tuning has emerged as a promising approach to adapt foundation models to downstream tasks using decentralized data. However, real-world deployment remains challenging due to the high computational and communication demands of fine-tuning Large Language Models (LLMs) on clients with data and system resources that are heterogeneous and constrained. In such settings, the global model's performance is often bottlenecked by the weakest clients and further degraded by the non-IID nature of local data. Although existing methods leverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to reduce communication and computation overhead, they often fail to simultaneously ensure accurate aggregation of low-rank updates and maintain low system costs, thereby hindering overall performance. To address these challenges, we propose AFLoRA, an adaptive and lightweight federated fine-tuning framework for LLMs. AFLoRA decouples shared and client-specific updates to reduce overhead and improve aggregation accuracy, incorporates diagonal matrix-based rank pruning to better utilize local resources, and employs rank-aware aggregation with public data refinement to strengthen generalization under data heterogeneity. Extensive experiments demonstrate that AFLoRA outperforms state-of-the-art methods in both accuracy and efficiency, providing a practical solution for efficient LLM adaptation in heterogeneous environments in the real world.</li>
</ul>

<h3>Title: Diffusion-Based Symbolic Regression</h3>
<ul>
<li><strong>Authors: </strong>Zachary Bastiani, Robert M. Kirby, Jacob Hochhalter, Shandian Zhe</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24776">https://arxiv.org/abs/2505.24776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24776">https://arxiv.org/pdf/2505.24776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24776]] Diffusion-Based Symbolic Regression(https://arxiv.org/abs/2505.24776)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion has emerged as a powerful framework for generative modeling, achieving remarkable success in applications such as image and audio synthesis. Enlightened by this progress, we propose a novel diffusion-based approach for symbolic regression. We construct a random mask-based diffusion and denoising process to generate diverse and high-quality equations. We integrate this generative processes with a token-wise Group Relative Policy Optimization (GRPO) method to conduct efficient reinforcement learning on the given measurement dataset. In addition, we introduce a long short-term risk-seeking policy to expand the pool of top-performing candidates, further enhancing performance. Extensive experiments and ablation studies have demonstrated the effectiveness of our approach.</li>
</ul>

<h3>Title: Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?</h3>
<ul>
<li><strong>Authors: </strong>Jiayu Liu, Qing Zong, Weiqi Wang, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24778">https://arxiv.org/abs/2505.24778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24778">https://arxiv.org/pdf/2505.24778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24778]] Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?(https://arxiv.org/abs/2505.24778)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are increasingly used in high-stakes domains, accurately assessing their confidence is crucial. Humans typically express confidence through epistemic markers (e.g., "fairly confident") instead of numerical values. However, it remains unclear whether LLMs consistently use these markers to reflect their intrinsic confidence due to the difficulty of quantifying uncertainty associated with various markers. To address this gap, we first define marker confidence as the observed accuracy when a model employs an epistemic marker. We evaluate its stability across multiple question-answering datasets in both in-distribution and out-of-distribution settings for open-source and proprietary LLMs. Our results show that while markers generalize well within the same distribution, their confidence is inconsistent in out-of-distribution scenarios. These findings raise significant concerns about the reliability of epistemic markers for confidence estimation, underscoring the need for improved alignment between marker based confidence and actual model uncertainty. Our code is available at this https URL.</li>
</ul>

<h3>Title: EVA-MILP: Towards Standardized Evaluation of MILP Instance Generation</h3>
<ul>
<li><strong>Authors: </strong>Yidong Luo, Chenguang Wang, Jiahao Yang, Fanzeng Xia, Tianshu Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24779">https://arxiv.org/abs/2505.24779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24779">https://arxiv.org/pdf/2505.24779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24779]] EVA-MILP: Towards Standardized Evaluation of MILP Instance Generation(https://arxiv.org/abs/2505.24779)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Mixed-Integer Linear Programming (MILP) is fundamental to solving complex decision-making problems. The proliferation of MILP instance generation methods, driven by machine learning's demand for diverse optimization datasets and the limitations of static benchmarks, has significantly outpaced standardized evaluation techniques. Consequently, assessing the fidelity and utility of synthetic MILP instances remains a critical, multifaceted challenge. This paper introduces a comprehensive benchmark framework designed for the systematic and objective evaluation of MILP instance generation methods. Our framework provides a unified and extensible methodology, assessing instance quality across crucial dimensions: mathematical validity, structural similarity, computational hardness, and utility in downstream machine learning tasks. A key innovation is its in-depth analysis of solver-internal features -- particularly by comparing distributions of key solver outputs including root node gap, heuristic success rates, and cut plane usage -- leveraging the solver's dynamic solution behavior as an `expert assessment' to reveal nuanced computational resemblances. By offering a structured approach with clearly defined solver-independent and solver-dependent metrics, our benchmark aims to facilitate robust comparisons among diverse generation techniques, spur the development of higher-quality instance generators, and ultimately enhance the reliability of research reliant on synthetic MILP data. The framework's effectiveness in systematically comparing the fidelity of instance sets is demonstrated using contemporary generative models.</li>
</ul>

<h3>Title: QGAN-based data augmentation for hybrid quantum-classical neural networks</h3>
<ul>
<li><strong>Authors: </strong>Run-Ze He, Jun-Jian Su, Su-Juan Qin, Zheng-Ping Jin, Fei Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24780">https://arxiv.org/abs/2505.24780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24780">https://arxiv.org/pdf/2505.24780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24780]] QGAN-based data augmentation for hybrid quantum-classical neural networks(https://arxiv.org/abs/2505.24780)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Quantum neural networks converge faster and achieve higher accuracy than classical models. However, data augmentation in quantum machine learning remains underexplored. To tackle data scarcity, we integrate quantum generative adversarial networks (QGANs) with hybrid quantum-classical neural networks (HQCNNs) to develop an augmentation framework. We propose two strategies: a general approach to enhance data processing and classification across HQCNNs, and a customized strategy that dynamically generates samples tailored to the HQCNN's performance on specific data categories, improving its ability to learn from complex datasets. Simulation experiments on the MNIST dataset demonstrate that QGAN outperforms traditional data augmentation methods and classical GANs. Compared to baseline DCGAN, QGAN achieves comparable performance with half the parameters, balancing efficiency and effectiveness. This suggests that QGANs can simplify models and generate high-quality data, enhancing HQCNN accuracy and performance. These findings pave the way for applying quantum data augmentation techniques in machine learning.</li>
</ul>

<h3>Title: Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Zhou, Jiahao Yuan, Qianning Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24787">https://arxiv.org/abs/2505.24787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24787">https://arxiv.org/pdf/2505.24787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24787]] Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation(https://arxiv.org/abs/2505.24787)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image (T2I) generation have enabled models to produce high-quality images from textual descriptions. However, these models often struggle with complex instructions involving multiple objects, attributes, and spatial relationships. Existing benchmarks for evaluating T2I models primarily focus on general text-image alignment and fail to capture the nuanced requirements of complex, multi-faceted prompts. Given this gap, we introduce LongBench-T2I, a comprehensive benchmark specifically designed to evaluate T2I models under complex instructions. LongBench-T2I consists of 500 intricately designed prompts spanning nine diverse visual evaluation dimensions, enabling a thorough assessment of a model's ability to follow complex instructions. Beyond benchmarking, we propose an agent framework (Plan2Gen) that facilitates complex instruction-driven image generation without requiring additional model training. This framework integrates seamlessly with existing T2I models, using large language models to interpret and decompose complex prompts, thereby guiding the generation process more effectively. As existing evaluation metrics, such as CLIPScore, fail to adequately capture the nuances of complex instructions, we introduce an evaluation toolkit that automates the quality assessment of generated images using a set of multi-dimensional metrics. The data and code are released at this https URL.</li>
</ul>

<h3>Title: Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding</h3>
<ul>
<li><strong>Authors: </strong>Jiaru Zhang, Juanwu Lu, Ziran Wang, Ruqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24791">https://arxiv.org/abs/2505.24791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24791">https://arxiv.org/pdf/2505.24791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24791]] Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding(https://arxiv.org/abs/2505.24791)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Normalizing flows are promising generative models with advantages such as theoretical rigor, analytical log-likelihood computation, and end-to-end training. However, the architectural constraints to ensure invertibility and tractable Jacobian computation limit their expressive power and practical usability. Recent advancements utilize autoregressive modeling, significantly enhancing expressive power and generation quality. However, such sequential modeling inherently restricts parallel computation during inference, leading to slow generation that impedes practical deployment. In this paper, we first identify that strict sequential dependency in inference is unnecessary to generate high-quality samples. We observe that patches in sequential modeling can also be approximated without strictly conditioning on all preceding patches. Moreover, the models tend to exhibit low dependency redundancy in the initial layer and higher redundancy in subsequent layers. Leveraging these observations, we propose a selective Jacobi decoding (SeJD) strategy that accelerates autoregressive inference through parallel iterative optimization. Theoretical analyses demonstrate the method's superlinear convergence rate and guarantee that the number of iterations required is no greater than the original sequential approach. Empirical evaluations across multiple datasets validate the generality and effectiveness of our acceleration technique. Experiments demonstrate substantial speed improvements up to 4.7 times faster inference while keeping the generation quality and fidelity.</li>
</ul>

<h3>Title: Lightweight Relational Embedding in Task-Interpolated Few-Shot Networks for Enhanced Gastrointestinal Disease Classification</h3>
<ul>
<li><strong>Authors: </strong>Xinliu Zhong, Leo Hwa Liang, Angela S. Koh, Yeo Si Yong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24792">https://arxiv.org/abs/2505.24792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24792">https://arxiv.org/pdf/2505.24792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24792]] Lightweight Relational Embedding in Task-Interpolated Few-Shot Networks for Enhanced Gastrointestinal Disease Classification(https://arxiv.org/abs/2505.24792)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Traditional diagnostic methods like colonoscopy are invasive yet critical tools necessary for accurately diagnosing colorectal cancer (CRC). Detection of CRC at early stages is crucial for increasing patient survival rates. However, colonoscopy is dependent on obtaining adequate and high-quality endoscopic images. Prolonged invasive procedures are inherently risky for patients, while suboptimal or insufficient images hamper diagnostic accuracy. These images, typically derived from video frames, often exhibit similar patterns, posing challenges in discrimination. To overcome these challenges, we propose a novel Deep Learning network built on a Few-Shot Learning architecture, which includes a tailored feature extractor, task interpolation, relational embedding, and a bi-level routing attention mechanism. The Few-Shot Learning paradigm enables our model to rapidly adapt to unseen fine-grained endoscopic image patterns, and the task interpolation augments the insufficient images artificially from varied instrument viewpoints. Our relational embedding approach discerns critical intra-image features and captures inter-image transitions between consecutive endoscopic frames, overcoming the limitations of Convolutional Neural Networks (CNNs). The integration of a light-weight attention mechanism ensures a concentrated analysis of pertinent image regions. By training on diverse datasets, the model's generalizability and robustness are notably improved for handling endoscopic images. Evaluated on Kvasir dataset, our model demonstrated superior performance, achieving an accuracy of 90.1\%, precision of 0.845, recall of 0.942, and an F1 score of 0.891. This surpasses current state-of-the-art methods, presenting a promising solution to the challenges of invasive colonoscopy by optimizing CRC detection through advanced image analysis.</li>
</ul>

<h3>Title: ByzFL: Research Framework for Robust Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Marc GonzÃ¡lez, Rachid Guerraoui, Rafael Pinot, Geovani Rizk, John Stephan, FranÃ§ois TaÃ¯ani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24802">https://arxiv.org/abs/2505.24802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24802">https://arxiv.org/pdf/2505.24802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24802]] ByzFL: Research Framework for Robust Federated Learning(https://arxiv.org/abs/2505.24802)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, federate</a></li>
<li><strong>Abstract: </strong>We present ByzFL, an open-source Python library for developing and benchmarking robust federated learning (FL) algorithms. ByzFL provides a unified and extensible framework that includes implementations of state-of-the-art robust aggregators, a suite of configurable attacks, and tools for simulating a variety of FL scenarios, including heterogeneous data distributions, multiple training algorithms, and adversarial threat models. The library enables systematic experimentation via a single JSON-based configuration file and includes built-in utilities for result visualization. Compatible with PyTorch tensors and NumPy arrays, ByzFL is designed to facilitate reproducible research and rapid prototyping of robust FL solutions. ByzFL is available at this https URL, with source code hosted on GitHub: this https URL.</li>
</ul>

<h3>Title: Guiding Generative Storytelling with Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Zhijun Pan, Antonios Andronis, Eva Hayek, Oscar AP Wilkinson, Ilya Lasy, Annette Parry, Guy Gadney, Tim J. Smith, Mick Grierson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24803">https://arxiv.org/abs/2505.24803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24803">https://arxiv.org/pdf/2505.24803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24803]] Guiding Generative Storytelling with Knowledge Graphs(https://arxiv.org/abs/2505.24803)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown great potential in automated story generation, but challenges remain in maintaining long-form coherence and providing users with intuitive and effective control. Retrieval-Augmented Generation (RAG) has proven effective in reducing hallucinations in text generation; however, the use of structured data to support generative storytelling remains underexplored. This paper investigates how knowledge graphs (KGs) can enhance LLM-based storytelling by improving narrative quality and enabling user-driven modifications. We propose a KG-assisted storytelling pipeline and evaluate its effectiveness through a user study with 15 participants. Participants created their own story prompts, generated stories, and edited knowledge graphs to shape their narratives. Through quantitative and qualitative analysis, our findings demonstrate that knowledge graphs significantly enhance story quality in action-oriented and structured narratives within our system settings. Additionally, editing the knowledge graph increases users' sense of control, making storytelling more engaging, interactive, and playful.</li>
</ul>

<h3>Title: PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yinggan Xu, Yue Liu, Zhiqiang Gao, Changnan Peng, Di Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24823">https://arxiv.org/abs/2505.24823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24823">https://arxiv.org/pdf/2505.24823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24823]] PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models(https://arxiv.org/abs/2505.24823)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have rapidly advanced and are increasingly capable of tackling complex scientific problems, including those in physics. Despite this progress, current LLMs often fail to emulate the concise, principle-based reasoning characteristic of human experts, instead generating lengthy and opaque solutions. This discrepancy highlights a crucial gap in their ability to apply core physical principles for efficient and interpretable problem solving. To systematically investigate this limitation, we introduce PhySense, a novel principle-based physics reasoning benchmark designed to be easily solvable by experts using guiding principles, yet deceptively difficult for LLMs without principle-first reasoning. Our evaluation across multiple state-of-the-art LLMs and prompt types reveals a consistent failure to align with expert-like reasoning paths, providing insights for developing AI systems with efficient, robust and interpretable principle-based scientific reasoning.</li>
</ul>

<h3>Title: Segmenting France Across Four Centuries</h3>
<ul>
<li><strong>Authors: </strong>Marta LÃ³pez-Rauhut, Hongyu Zhou, Mathieu Aubry, Loic Landrieu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24824">https://arxiv.org/abs/2505.24824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24824">https://arxiv.org/pdf/2505.24824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24824]] Segmenting France Across Four Centuries(https://arxiv.org/abs/2505.24824)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Historical maps offer an invaluable perspective into territory evolution across past centuries--long before satellite or remote sensing technologies existed. Deep learning methods have shown promising results in segmenting historical maps, but publicly available datasets typically focus on a single map type or period, require extensive and costly annotations, and are not suited for nationwide, long-term analyses. In this paper, we introduce a new dataset of historical maps tailored for analyzing large-scale, long-term land use and land cover evolution with limited annotations. Spanning metropolitan France (548,305 km^2), our dataset contains three map collections from the 18th, 19th, and 20th centuries. We provide both comprehensive modern labels and 22,878 km^2 of manually annotated historical labels for the 18th and 19th century maps. Our dataset illustrates the complexity of the segmentation task, featuring stylistic inconsistencies, interpretive ambiguities, and significant landscape changes (e.g., marshlands disappearing in favor of forests). We assess the difficulty of these challenges by benchmarking three approaches: a fully-supervised model trained with historical labels, and two weakly-supervised models that rely only on modern annotations. The latter either use the modern labels directly or first perform image-to-image translation to address the stylistic gap between historical and contemporary maps. Finally, we discuss how these methods can support long-term environment monitoring, offering insights into centuries of landscape transformation. Our official project repository is publicly available at this https URL.</li>
</ul>

<h3>Title: LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text</h3>
<ul>
<li><strong>Authors: </strong>Li yunhan, Wu gengshen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24826">https://arxiv.org/abs/2505.24826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24826">https://arxiv.org/pdf/2505.24826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24826]] LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text(https://arxiv.org/abs/2505.24826)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are increasingly used in legal applications, current evaluation benchmarks tend to focus mainly on factual accuracy while largely neglecting important linguistic quality aspects such as clarity, coherence, and terminology. To address this gap, we propose three steps: First, we develop a regression model to evaluate the quality of legal texts based on clarity, coherence, and terminology. Second, we create a specialized set of legal questions. Third, we analyze 49 LLMs using this evaluation framework. Our analysis identifies three key findings: First, model quality levels off at 14 billion parameters, with only a marginal improvement of $2.7\%$ noted at 72 billion parameters. Second, engineering choices such as quantization and context length have a negligible impact, as indicated by statistical significance thresholds above 0.016. Third, reasoning models consistently outperform base architectures. A significant outcome of our research is the release of a ranking list and Pareto analysis, which highlight the Qwen3 series as the optimal choice for cost-performance tradeoffs. This work not only establishes standardized evaluation protocols for legal LLMs but also uncovers fundamental limitations in current training data refinement approaches. Code and models are available at: this https URL.</li>
</ul>

<h3>Title: Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs</h3>
<ul>
<li><strong>Authors: </strong>Juraj Vladika, Annika Domres, Mai Nguyen, Rebecca Moser, Jana Nano, Felix Busch, Lisa C. Adams, Keno K. Bressem, Denise Bernhardt, Stephanie E. Combs, Kai J. Borm, Florian Matthes, Jan C. Peeken</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24830">https://arxiv.org/abs/2505.24830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24830">https://arxiv.org/pdf/2505.24830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24830]] Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs(https://arxiv.org/abs/2505.24830)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit extensive medical knowledge but are prone to hallucinations and inaccurate citations, which pose a challenge to their clinical adoption and regulatory compliance. Current methods, such as Retrieval Augmented Generation, partially address these issues by grounding answers in source documents, but hallucinations and low fact-level explainability persist. In this work, we introduce a novel atomic fact-checking framework designed to enhance the reliability and explainability of LLMs used in medical long-form question answering. This method decomposes LLM-generated responses into discrete, verifiable units called atomic facts, each of which is independently verified against an authoritative knowledge base of medical guidelines. This approach enables targeted correction of errors and direct tracing to source literature, thereby improving the factual accuracy and explainability of medical Q&A. Extensive evaluation using multi-reader assessments by medical experts and an automated open Q&A benchmark demonstrated significant improvements in factual accuracy and explainability. Our framework achieved up to a 40% overall answer improvement and a 50% hallucination detection rate. The ability to trace each atomic fact back to the most relevant chunks from the database provides a granular, transparent explanation of the generated responses, addressing a major gap in current medical AI applications. This work represents a crucial step towards more trustworthy and reliable clinical applications of LLMs, addressing key prerequisites for clinical application and fostering greater confidence in AI-assisted healthcare.</li>
</ul>

<h3>Title: How much do language models memorize?</h3>
<ul>
<li><strong>Authors: </strong>John X. Morris, Chawin Sitawarin, Chuan Guo, Narine Kokhlikyan, G. Edward Suh, Alexander M. Rush, Kamalika Chaudhuri, Saeed Mahloujifar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24832">https://arxiv.org/abs/2505.24832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24832">https://arxiv.org/pdf/2505.24832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24832]] How much do language models memorize?(https://arxiv.org/abs/2505.24832)</code><input type="text"></li>
<li><strong>Keywords: </strong>membership infer, transformer</a></li>
<li><strong>Abstract: </strong>We propose a new method for estimating how much a model ``knows'' about a datapoint and use it to measure the capacity of modern language models. Prior studies of language model memorization have struggled to disentangle memorization from generalization. We formally separate memorization into two components: \textit{unintended memorization}, the information a model contains about a specific dataset, and \textit{generalization}, the information a model contains about the true data-generation process. When we completely eliminate generalization, we can compute the total memorization, which provides an estimate of model capacity: our measurements estimate that GPT-style models have a capacity of approximately 3.6 bits per parameter. We train language models on datasets of increasing size and observe that models memorize until their capacity fills, at which point ``grokking'' begins, and unintended memorization decreases as models begin to generalize. We train hundreds of transformer language models ranging from $500K$ to $1.5B$ parameters and produce a series of scaling laws relating model capacity and data size to membership inference.</li>
</ul>

<h3>Title: VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and 3D Reasoning from CAD Software</h3>
<ul>
<li><strong>Authors: </strong>Brandon Man, Ghadi Nehme, Md Ferdous Alam, Faez Ahmed</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24838">https://arxiv.org/abs/2505.24838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24838">https://arxiv.org/pdf/2505.24838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24838]] VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and 3D Reasoning from CAD Software(https://arxiv.org/abs/2505.24838)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Computer-Aided Design (CAD) is a time-consuming and complex process, requiring precise, long-horizon user interactions with intricate 3D interfaces. While recent advances in AI-driven user interface (UI) agents show promise, most existing datasets and methods focus on short, low-complexity tasks in mobile or web applications, failing to capture the demands of professional engineering tools. In this work, we introduce VideoCAD, the first attempt at engineering UI interaction learning for precision tasks. Specifically, VideoCAD is a large-scale synthetic dataset consisting of over 41K annotated video recordings of CAD operations, generated using an automated framework for collecting high-fidelity UI action data from human-made CAD designs. Compared to existing datasets, VideoCAD offers an order of magnitude higher complexity in UI interaction learning for real-world engineering tasks, having up to a 20x longer time horizon than other datasets. We show two important downstream applications of VideoCAD: learning UI interactions from professional precision 3D CAD tools and a visual question-answering (VQA) benchmark designed to evaluate multimodal large language models' (LLM) spatial reasoning and video understanding abilities. To learn the UI interactions, we propose VideoCADFormer - a state-of-the-art model in learning CAD interactions directly from video, which outperforms multiple behavior cloning baselines. Both VideoCADFormer and the VQA benchmark derived from VideoCAD reveal key challenges in the current state of video-based UI understanding, including the need for precise action grounding, multi-modal and spatial reasoning, and long-horizon dependencies.</li>
</ul>

<h3>Title: Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck</h3>
<ul>
<li><strong>Authors: </strong>Yuwen Tan, Yuan Qing, Boqing Gong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24840">https://arxiv.org/abs/2505.24840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24840">https://arxiv.org/pdf/2505.24840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24840]] Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck(https://arxiv.org/abs/2505.24840)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper reveals that many state-of-the-art large language models (LLMs) lack hierarchical knowledge about our visual world, unaware of even well-established biology taxonomies. This shortcoming makes LLMs a bottleneck for vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone Fish but not Vertebrate). We arrive at these findings using about one million four-choice visual question answering (VQA) tasks constructed from six taxonomies and four image datasets. Interestingly, finetuning a vision LLM using our VQA tasks reaffirms LLMs' bottleneck effect to some extent because the VQA tasks improve the LLM's hierarchical consistency more than the vision LLM's. We conjecture that one cannot make vision LLMs understand visual concepts fully hierarchical until LLMs possess corresponding taxonomy knowledge.</li>
</ul>

<h3>Title: Cascading Adversarial Bias from Injection to Distillation in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Harsh Chaudhari, Jamie Hayes, Matthew Jagielski, Ilia Shumailov, Milad Nasr, Alina Oprea</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24842">https://arxiv.org/abs/2505.24842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24842">https://arxiv.org/pdf/2505.24842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24842]] Cascading Adversarial Bias from Injection to Distillation in Language Models(https://arxiv.org/abs/2505.24842)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack</a></li>
<li><strong>Abstract: </strong>Model distillation has become essential for creating smaller, deployable language models that retain larger system capabilities. However, widespread deployment raises concerns about resilience to adversarial manipulation. This paper investigates vulnerability of distilled models to adversarial injection of biased content during training. We demonstrate that adversaries can inject subtle biases into teacher models through minimal data poisoning, which propagates to student models and becomes significantly amplified. We propose two propagation modes: Untargeted Propagation, where bias affects multiple tasks, and Targeted Propagation, focusing on specific tasks while maintaining normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning rate), student models generate biased responses 76.9% of the time in targeted scenarios - higher than 69.4% in teacher models. For untargeted propagation, adversarial bias appears 6x-29x more frequently in student models on unseen tasks. We validate findings across six bias types (targeted advertisements, phishing links, narrative manipulations, insecure coding practices), various distillation methods, and different modalities spanning text and code generation. Our evaluation reveals shortcomings in current defenses - perplexity filtering, bias detection systems, and LLM-based autorater frameworks - against these attacks. Results expose significant security vulnerabilities in distilled models, highlighting need for specialized safeguards. We propose practical design principles for building effective adversarial bias mitigation strategies.</li>
</ul>

<h3>Title: From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching</h3>
<ul>
<li><strong>Authors: </strong>Ruqi Bai, Yao Ji, Zeyu Zhou, David I. Inouye</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24843">https://arxiv.org/abs/2505.24843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24843">https://arxiv.org/pdf/2505.24843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24843]] From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching(https://arxiv.org/abs/2505.24843)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Spurious correlations can cause model performance to degrade in new environments. Prior causality-inspired works aim to learn invariant representations (e.g., IRM) but typically underperform empirical risk minimization (ERM). Recent alternatives improve robustness by leveraging test-time data, but such data may be unavailable in practice. To address these issues, we take a data-centric approach by leveraging invariant data pairs, pairs of samples that would have the same prediction with the optimally robust classifier. We prove that certain counterfactual pairs will naturally satisfy this invariance property and introduce noisy counterfactual matching (NCM), a simple constraint-based method for leveraging invariant pairs for enhanced robustness, even with a small set of noisy pairs-in the ideal case, each pair can eliminate one spurious feature. For linear causal models, we prove that the test domain error can be upper bounded by the in-domain error and a term that depends on the counterfactuals' diversity and quality. We validate on a synthetic dataset and demonstrate on real-world benchmarks that linear probing on a pretrained backbone improves robustness.</li>
</ul>

<h3>Title: Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning</h3>
<ul>
<li><strong>Authors: </strong>Wanyun Xie, Francesco Tonin, Volkan Cevher</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24844">https://arxiv.org/abs/2505.24844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24844">https://arxiv.org/pdf/2505.24844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24844]] Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning(https://arxiv.org/abs/2505.24844)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Training data mixtures greatly impact the generalization performance of large language models. Existing domain reweighting methods often rely on costly weight computations and require retraining when new data is introduced. To this end, we introduce a flexible and efficient data mixing framework, Chameleon, that employs leverage scores to quantify domain importance within a learned embedding space. We first construct a domain affinity matrix over domain embeddings. The induced leverage scores determine a mixture that upweights domains sharing common representations in embedding space. This formulation allows direct transfer to new data by computing the new domain embeddings. In experiments, we demonstrate improvements over three key scenarios: (i) our computed weights improve performance on pretraining domains with a fraction of the compute of existing methods; (ii) Chameleon can adapt to data changes without proxy retraining, boosting few-shot reasoning accuracies when transferred to new data; (iii) our method enables efficient domain reweighting in finetuning, consistently improving test perplexity on all finetuning domains over uniform mixture. Our code is available at this https URL.</li>
</ul>

<h3>Title: Reading Recognition in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Charig Yang, Samiul Alam, Shakhrul Iman Siam, Michael J. Proulx, Lambert Mathias, Kiran Somasundaram, Luis Pesqueira, James Fort, Sheroze Sheriffdeen, Omkar Parkhi, Carl Ren, Mi Zhang, Yuning Chai, Richard Newcombe, Hyo Jin Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24848">https://arxiv.org/abs/2505.24848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24848">https://arxiv.org/pdf/2505.24848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24848]] Reading Recognition in the Wild(https://arxiv.org/abs/2505.24848)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>To enable egocentric contextual AI in always-on smart glasses, it is crucial to be able to keep a record of the user's interactions with the world, including during reading. In this paper, we introduce a new task of reading recognition to determine when the user is reading. We first introduce the first-of-its-kind large-scale multimodal Reading in the Wild dataset, containing 100 hours of reading and non-reading videos in diverse and realistic scenarios. We then identify three modalities (egocentric RGB, eye gaze, head pose) that can be used to solve the task, and present a flexible transformer model that performs the task using these modalities, either individually or combined. We show that these modalities are relevant and complementary to the task, and investigate how to efficiently and effectively encode each modality. Additionally, we show the usefulness of this dataset towards classifying types of reading, extending current reading understanding studies conducted in constrained settings to larger scale, diversity and realism. Code, model, and data will be public.</li>
</ul>

<h3>Title: Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking</h3>
<ul>
<li><strong>Authors: </strong>Heli Ben-Hamu, Itai Gat, Daniel Severo, Niklas Nolte, Brian Karrer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24857">https://arxiv.org/abs/2505.24857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24857">https://arxiv.org/pdf/2505.24857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24857]] Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking(https://arxiv.org/abs/2505.24857)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent masked diffusion models (MDMs) have shown competitive performance compared to autoregressive models (ARMs) for language modeling. While most literature has focused on performance enhancing sampling procedures, efficient sampling from MDMs has been scarcely explored. We make the observation that often a given sequence of partially masked tokens determines the values of multiple unknown tokens deterministically, meaning that a single prediction of a masked model holds additional information unused by standard sampling procedures. Based on this observation, we introduce EB-Sampler, a simple drop-in replacement for existing samplers, utilizing an Entropy Bounded unmasking procedure that dynamically unmasks multiple tokens in one function evaluation with predefined approximate error tolerance. We formulate the EB-Sampler as part of a broad family of adaptive samplers for which we provide an error analysis that motivates our algorithmic choices. EB-Sampler accelerates sampling from current state of the art MDMs by roughly 2-3x on standard coding and math reasoning benchmarks without loss in performance. We also validate the same procedure works well on smaller reasoning tasks including maze navigation and Sudoku, tasks ARMs often struggle with.</li>
</ul>

<h3>Title: MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Gabrielle Kaili-May Liu, Gal Yona, Avi Caciularu, Idan Szpektor, Tim G. J. Rudner, Arman Cohan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24858">https://arxiv.org/abs/2505.24858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24858">https://arxiv.org/pdf/2505.24858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24858]] MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs(https://arxiv.org/abs/2505.24858)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A critical component in the trustworthiness of LLMs is reliable uncertainty communication, yet LLMs often use assertive language when conveying false claims, leading to over-reliance and eroded trust. We present the first systematic study of $\textit{faithful confidence calibration}$ of LLMs, benchmarking models' ability to use linguistic expressions of uncertainty that $\textit{faithfully reflect}$ their intrinsic uncertainty, across a comprehensive array of models, datasets, and prompting strategies. Our results demonstrate that LLMs largely fail at this task, and that existing interventions are insufficient: standard prompt approaches provide only marginal gains, and existing, factuality-based calibration techniques can even harm faithful calibration. To address this critical gap, we introduce MetaFaith, a novel prompt-based calibration approach inspired by human metacognition. We show that MetaFaith robustly improves faithful calibration across diverse models and task domains, enabling up to 61% improvement in faithfulness and achieving an 83% win rate over original generations as judged by humans.</li>
</ul>

<h3>Title: ViStoryBench: Comprehensive Benchmark Suite for Story Visualization</h3>
<ul>
<li><strong>Authors: </strong>Cailin Zhuang, Ailin Huang, Wei Cheng, Jingwei Wu, Yaoqi Hu, Jiaqi Liao, Zhewei Huang, Hongyuan Wang, Xinyao Liao, Weiwei Cai, Hengyuan Xu, Xuanyang Zhang, Xianfang Zeng, Gang Yu, Chi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24862">https://arxiv.org/abs/2505.24862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24862">https://arxiv.org/pdf/2505.24862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24862]] ViStoryBench: Comprehensive Benchmark Suite for Story Visualization(https://arxiv.org/abs/2505.24862)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Story visualization, which aims to generate a sequence of visually coherent images aligning with a given narrative and reference images, has seen significant progress with recent advancements in generative models. To further enhance the performance of story visualization frameworks in real-world scenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We collect a diverse dataset encompassing various story types and artistic styles, ensuring models are evaluated across multiple dimensions such as different plots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D renderings). ViStoryBench is carefully curated to balance narrative structures and visual elements, featuring stories with single and multiple protagonists to test models' ability to maintain character consistency. Additionally, it includes complex plots and intricate world-building to challenge models in generating accurate visuals. To ensure comprehensive comparisons, our benchmark incorporates a wide range of evaluation metrics assessing critical aspects. This structured and multifaceted framework enables researchers to thoroughly identify both the strengths and weaknesses of different models, fostering targeted improvements.</li>
</ul>

<h3>Title: ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, Yi Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24864">https://arxiv.org/abs/2505.24864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24864">https://arxiv.org/pdf/2505.24864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24864]] ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models(https://arxiv.org/abs/2505.24864)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: this https URL</li>
</ul>

<h3>Title: TalkingHeadBench: A Multi-Modal Benchmark & Analysis of Talking-Head DeepFake Detection</h3>
<ul>
<li><strong>Authors: </strong>Xinqi Xiong, Prakrut Patel, Qingyuan Fan, Amisha Wadhwa, Sarathy Selvam, Xiao Guo, Luchao Qi, Xiaoming Liu, Roni Sengupta</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24866">https://arxiv.org/abs/2505.24866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24866">https://arxiv.org/pdf/2505.24866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24866]] TalkingHeadBench: A Multi-Modal Benchmark & Analysis of Talking-Head DeepFake Detection(https://arxiv.org/abs/2505.24866)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of talking-head deepfake generation fueled by advanced generative models has elevated the realism of synthetic videos to a level that poses substantial risks in domains such as media, politics, and finance. However, current benchmarks for deepfake talking-head detection fail to reflect this progress, relying on outdated generators and offering limited insight into model robustness and generalization. We introduce TalkingHeadBench, a comprehensive multi-model multi-generator benchmark and curated dataset designed to evaluate the performance of state-of-the-art detectors on the most advanced generators. Our dataset includes deepfakes synthesized by leading academic and commercial models and features carefully constructed protocols to assess generalization under distribution shifts in identity and generator characteristics. We benchmark a diverse set of existing detection methods, including CNNs, vision transformers, and temporal models, and analyze their robustness and generalization capabilities. In addition, we provide error analysis using Grad-CAM visualizations to expose common failure modes and detector biases. TalkingHeadBench is hosted on this https URL with open access to all data splits and protocols. Our benchmark aims to accelerate research towards more robust and generalizable detection models in the face of rapidly evolving generative techniques.</li>
</ul>

<h3>Title: SiLVR: A Simple Language-based Video Reasoning Framework</h3>
<ul>
<li><strong>Authors: </strong>Ce Zhang, Yan-Bo Lin, Ziyang Wang, Mohit Bansal, Gedas Bertasius</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24869">https://arxiv.org/abs/2505.24869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24869">https://arxiv.org/pdf/2505.24869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24869]] SiLVR: A Simple Language-based Video Reasoning Framework(https://arxiv.org/abs/2505.24869)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in test-time optimization have led to remarkable reasoning capabilities in Large Language Models (LLMs), enabling them to solve highly complex problems in math and coding. However, the reasoning capabilities of multimodal LLMs (MLLMs) still significantly lag, especially for complex video-language tasks. To address this issue, we present SiLVR, a Simple Language-based Video Reasoning framework that decomposes complex video understanding into two stages. In the first stage, SiLVR transforms raw video into language-based representations using multisensory inputs, such as short clip captions and audio/speech subtitles. In the second stage, language descriptions are fed into a powerful reasoning LLM to solve complex video-language understanding tasks. To handle long-context multisensory inputs, we use an adaptive token reduction scheme, which dynamically determines the temporal granularity with which to sample the tokens. Our simple, modular, and training-free video reasoning framework achieves the best-reported results on Video-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife. Furthermore, our empirical study focused on video reasoning capabilities shows that, despite not being explicitly trained on video, strong reasoning LLMs can effectively aggregate multisensory input information from video, speech, and audio for complex temporal, causal, long-context, and knowledge acquisition reasoning tasks in video. Code is available at this https URL.</li>
</ul>

<h3>Title: MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yiqing Liang, Jielin Qiu, Wenhao Ding, Zuxin Liu, James Tompkin, Mengdi Xu, Mengzhou Xia, Zhengzhong Tu, Laixi Shi, Jiacheng Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24871">https://arxiv.org/abs/2505.24871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24871">https://arxiv.org/pdf/2505.24871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24871]] MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning(https://arxiv.org/abs/2505.24871)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a powerful paradigm for post-training large language models (LLMs), achieving state-of-the-art performance on tasks with structured, verifiable answers. Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but is complicated by the broader, heterogeneous nature of vision-language tasks that demand nuanced visual, logical, and spatial capabilities. As such, training MLLMs using RLVR on multiple datasets could be beneficial but creates challenges with conflicting objectives from interaction among diverse datasets, highlighting the need for optimal dataset mixture strategies to improve generalization and reasoning. We introduce a systematic post-training framework for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation and benchmark implementation. Specifically, (1) We developed a multimodal RLVR framework for multi-dataset post-training by curating a dataset that contains different verifiable vision-language problems and enabling multi-domain online RL learning with different verifiable rewards; (2) We proposed a data mixture strategy that learns to predict the RL fine-tuning outcome from the data mixture distribution, and consequently optimizes the best mixture. Comprehensive experiments showcase that multi-domain RLVR training, when combined with mixture prediction strategies, can significantly boost MLLM general reasoning capacities. Our best mixture improves the post-trained model's accuracy on out-of-distribution benchmarks by an average of 5.24% compared to the same model post-trained with uniform data mixture, and by a total of 20.74% compared to the pre-finetuning baseline.</li>
</ul>

<h3>Title: MiniMax-Remover: Taming Bad Noise Helps Video Object Removal</h3>
<ul>
<li><strong>Authors: </strong>Bojia Zi, Weixuan Peng, Xianbiao Qi, Jianan Wang, Shihao Zhao, Rong Xiao, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24873">https://arxiv.org/abs/2505.24873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24873">https://arxiv.org/pdf/2505.24873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24873]] MiniMax-Remover: Taming Bad Noise Helps Video Object Removal(https://arxiv.org/abs/2505.24873)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in video diffusion models have driven rapid progress in video editing techniques. However, video object removal, a critical subtask of video editing, remains challenging due to issues such as hallucinated objects and visual artifacts. Furthermore, existing methods often rely on computationally expensive sampling procedures and classifier-free guidance (CFG), resulting in slow inference. To address these limitations, we propose MiniMax-Remover, a novel two-stage video object removal approach. Motivated by the observation that text condition is not best suited for this task, we simplify the pretrained video generation model by removing textual input and cross-attention layers, resulting in a more lightweight and efficient model architecture in the first stage. In the second stage, we distilled our remover on successful videos produced by the stage-1 model and curated by human annotators, using a minimax optimization strategy to further improve editing quality and inference speed. Specifically, the inner maximization identifies adversarial input noise ("bad noise") that makes failure removals, while the outer minimization step trains the model to generate high-quality removal results even under such challenging conditions. As a result, our method achieves a state-of-the-art video object removal results with as few as 6 sampling steps and doesn't rely on CFG, significantly improving inference efficiency. Extensive experiments demonstrate the effectiveness and superiority of MiniMax-Remover compared to existing methods. Codes and Videos are available at: this https URL.</li>
</ul>

<h3>Title: The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Adam Stein, Aaditya Naik, Neelay Velingker, Mayur Naik, Eric Wong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24874">https://arxiv.org/abs/2505.24874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24874">https://arxiv.org/pdf/2505.24874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24874]] The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models(https://arxiv.org/abs/2505.24874)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Neuro-symbolic learning was proposed to address challenges with training neural networks for complex reasoning tasks with the added benefits of interpretability, reliability, and efficiency. Neuro-symbolic learning methods traditionally train neural models in conjunction with symbolic programs, but they face significant challenges that limit them to simplistic problems. On the other hand, purely-neural foundation models now reach state-of-the-art performance through prompting rather than training, but they are often unreliable and lack interpretability. Supplementing foundation models with symbolic programs, which we call neuro-symbolic prompting, provides a way to use these models for complex reasoning tasks. Doing so raises the question: What role does specialized model training as part of neuro-symbolic learning have in the age of foundation models? To explore this question, we highlight three pitfalls of traditional neuro-symbolic learning with respect to the compute, data, and programs leading to generalization problems. This position paper argues that foundation models enable generalizable neuro-symbolic solutions, offering a path towards achieving the original goals of neuro-symbolic learning without the downsides of training from scratch.</li>
</ul>

<h3>Title: ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhang, Yunqi Li, Yifan Yang, Rui Wang, Yuqing Yang, Dai Qi, Jianmin Bao, Dongdong Chen, Chong Luo, Lili Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24875">https://arxiv.org/abs/2505.24875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24875">https://arxiv.org/pdf/2505.24875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24875]] ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL(https://arxiv.org/abs/2505.24875)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Although chain-of-thought reasoning and reinforcement learning (RL) have driven breakthroughs in NLP, their integration into generative vision models remains underexplored. We introduce ReasonGen-R1, a two-stage framework that first imbues an autoregressive image generator with explicit text-based "thinking" skills via supervised fine-tuning on a newly generated reasoning dataset of written rationales, and then refines its outputs using Group Relative Policy Optimization. To enable the model to reason through text before generating images, We automatically generate and release a corpus of model crafted rationales paired with visual prompts, enabling controlled planning of object layouts, styles, and scene compositions. Our GRPO algorithm uses reward signals from a pretrained vision language model to assess overall visual quality, optimizing the policy in each update. Evaluations on GenEval, DPG, and the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong baselines and prior state-of-the-art models. More: this http URL.</li>
</ul>

<h3>Title: Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks</h3>
<ul>
<li><strong>Authors: </strong>Tajamul Ashraf, Amal Saqib, Hanan Ghani, Muhra AlMahri, Yuhao Li, Noor Ahsan, Umair Nawaz, Jean Lahoud, Hisham Cholakkal, Mubarak Shah, Philip Torr, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman Khan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24876">https://arxiv.org/abs/2505.24876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24876">https://arxiv.org/pdf/2505.24876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24876]] Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks(https://arxiv.org/abs/2505.24876)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Deep reasoning is fundamental for solving complex tasks, especially in vision-centric scenarios that demand sequential, multimodal understanding. However, existing benchmarks typically evaluate agents with fully synthetic, single-turn queries, limited visual modalities, and lack a framework to assess reasoning quality over multiple steps as required in real-world settings. To address this, we introduce Agent-X, a large-scale benchmark for evaluating vision-centric agents multi-step and deep reasoning capabilities in real-world, multimodal settings. Agent- X features 828 agentic tasks with authentic visual contexts, including images, multi-image comparisons, videos, and instructional text. These tasks span six major agentic environments: general visual reasoning, web browsing, security and surveillance, autonomous driving, sports, and math reasoning. Our benchmark requires agents to integrate tool use with explicit, stepwise decision-making in these diverse settings. In addition, we propose a fine-grained, step-level evaluation framework that assesses the correctness and logical coherence of each reasoning step and the effectiveness of tool usage throughout the task. Our results reveal that even the best-performing models, including GPT, Gemini, and Qwen families, struggle to solve multi-step vision tasks, achieving less than 50% full-chain success. These findings highlight key bottlenecks in current LMM reasoning and tool-use capabilities and identify future research directions in vision-centric agentic reasoning models. Our data and code are publicly available at this https URL</li>
</ul>

<h3>Title: AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yangyi Huang, Ye Yuan, Xueting Li, Jan Kautz, Umar Iqbal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24877">https://arxiv.org/abs/2505.24877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24877">https://arxiv.org/pdf/2505.24877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24877]] AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion(https://arxiv.org/abs/2505.24877)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing methods for image-to-3D avatar generation struggle to produce highly detailed, animation-ready avatars suitable for real-world applications. We introduce AdaHuman, a novel framework that generates high-fidelity animatable 3D avatars from a single in-the-wild image. AdaHuman incorporates two key innovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes consistent multi-view images in arbitrary poses alongside corresponding 3D Gaussian Splats (3DGS) reconstruction at each diffusion step; (2) A compositional 3DGS refinement module that enhances the details of local body parts through image-to-image refinement and seamlessly integrates them using a novel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These components allow AdaHuman to generate highly realistic standardized A-pose avatars with minimal self-occlusion, enabling rigging and animation with any input motion. Extensive evaluation on public benchmarks and in-the-wild images demonstrates that AdaHuman significantly outperforms state-of-the-art methods in both avatar reconstruction and reposing. Code and models will be publicly available for research purposes.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
