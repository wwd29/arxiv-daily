<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-24</h1>
<h3>Title: Blockchain-Driven Solutions for Carbon Credit Trading: A Decentralized Platform for SMEs</h3>
<ul>
<li><strong>Authors: </strong>Yun-Cheng Tsai</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16085">https://arxiv.org/abs/2504.16085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16085">https://arxiv.org/pdf/2504.16085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16085]] Blockchain-Driven Solutions for Carbon Credit Trading: A Decentralized Platform for SMEs(https://arxiv.org/abs/2504.16085)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The increasing demand for sustainability and compliance with global carbon regulations has posed significant challenges for small and medium-sized enterprises (SMEs). This paper proposes a blockchain-based decentralized carbon credit trading platform tailored for SMEs in Taiwan, aiming to simplify the complex carbon trading process and lower market entry barriers. Drawing upon the Diffusion of Innovations theory and transaction cost economics, we illustrate how blockchain technology can reduce informational asymmetry and intermediary costs in carbon markets. By integrating Ethereum-based smart contracts, the platform automates transactions, enhances transparency, and reduces administrative burdens - addressing key obstacles such as technical complexity and market risks. A controlled experimental design was conducted to compare the proposed system with a conventional centralized carbon trading platform. Statistical analysis confirms its effectiveness in minimizing time and expenses while ensuring compliance with the Carbon Border Adjustment Mechanism (CBAM) and the Clean Competition Act (CCA). User satisfaction was measured using the Kano model, with the results identifying essential features and prioritizing future enhancements. This study contributes a more comprehensive solution for SMEs seeking to achieve carbon neutrality, underscoring the transformative potential of blockchain technology in global carbon markets.</li>
</ul>

<h3>Title: Surveillance Disguised as Protection: A Comparative Analysis of Sideloaded and In-Store Parental Control Apps</h3>
<ul>
<li><strong>Authors: </strong>Eva-Maria Maier, Leonie Maria Tanczer, Lukas Daniel Klausner</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16087">https://arxiv.org/abs/2504.16087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16087">https://arxiv.org/pdf/2504.16087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16087]] Surveillance Disguised as Protection: A Comparative Analysis of Sideloaded and In-Store Parental Control Apps(https://arxiv.org/abs/2504.16087)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Parental control applications, software tools designed to manage and monitor children's online activities, serve as essential safeguards for parents in the digital age. However, their usage has sparked concerns about security and privacy violations inherent in various child monitoring products. Sideloaded software (i. e. apps installed outside official app stores) poses an increased risk, as it is not bound by the regulations of trusted platforms. Despite this, the market of sideloaded parental control software has remained widely unexplored by the research community. This paper examines 20 sideloaded parental control apps and compares them to 20 apps available on the Google Play Store. We base our analysis on privacy policies, Android package kit (APK) files, application behaviour, network traffic and application functionalities. Our findings reveal that sideloaded parental control apps fall short compared to their in-store counterparts, lacking specialised parental control features and safeguards against misuse while concealing themselves on the user's device. Alarmingly, three apps transmitted sensitive data unencrypted, half lacked a privacy policy and 8 out of 20 were flagged for potential stalkerware indicators of compromise (IOC).</li>
</ul>

<h3>Title: Paths Not Taken: A Secure Computing Tutorial</h3>
<ul>
<li><strong>Authors: </strong>William Earl Boebert</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16088">https://arxiv.org/abs/2504.16088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16088">https://arxiv.org/pdf/2504.16088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16088]] Paths Not Taken: A Secure Computing Tutorial(https://arxiv.org/abs/2504.16088)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust</a></li>
<li><strong>Abstract: </strong>This paper is a tutorial on the proven but currently under-appreciated security mechanisms associated with "tagged" or "descriptor" architectures. The tutorial shows how the principles behind such architectures can be applied to mitigate or eliminate vulnerabilities. The tutorial incorporates systems engineering practices by presenting the mechanisms in an informal model of an integrated artifact in its operational environment. The artifact is a special-purpose hardware/software system called a "Guard" which robustly hosts defensive software. It is hoped that this tutorial may encourage teachers to include significant past work in their curricula and students who are self-teaching to add that work to their exploration of secure computing.</li>
</ul>

<h3>Title: Carbyne: An Ultra-Lightweight DoS-Resilient Mempool for Bitcoin</h3>
<ul>
<li><strong>Authors: </strong>Hina Binte Haq, Syed Taha Ali, Asad Salman, Patrick McCorry, Siamak F. Shahandashti</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16089">https://arxiv.org/abs/2504.16089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16089">https://arxiv.org/pdf/2504.16089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16089]] Carbyne: An Ultra-Lightweight DoS-Resilient Mempool for Bitcoin(https://arxiv.org/abs/2504.16089)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>The increasing adoption of cryptocurrencies has significantly amplified the resource requirements for operating full nodes, creating substantial barriers to entry. Unlike miners, who are financially incentivized through block rewards and transaction fees, full nodes lack direct economic compensation for their critical role in maintaining the network. A key resource burden is the transaction pool, which is particularly memory-intensive as it temporarily stores unconfirmed transactions awaiting verification and propagation across the network. We present Neonpool, a novel optimization for transaction pool leveraging bloom filter variants to drastically reduce memory consumption by up to 200 (e.g., 400 MB to 2 MB) while maintaining over 99.99% transaction processing accuracy. Implemented in C++ and evaluated on unique Bitcoin and Ethereum datasets, Neonpool enables efficient operation on lightweight clients, such as smartphones, IoT devices, and systems-on-a-chip, without requiring a hard fork. By lowering the cost of node participation, Neonpool enhances decentralization and strengthens the overall security and robustness of cryptocurrency networks.</li>
</ul>

<h3>Title: Post-Quantum Homomorphic Encryption: A Case for Code-Based Alternatives</h3>
<ul>
<li><strong>Authors: </strong>Siddhartha Siddhiprada Bhoi, Arathi Arakala, Amy Beth Corman, Asha Rao</a></li>
<li><strong>Subjects: </strong>cs.CR, math.HO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16091">https://arxiv.org/abs/2504.16091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16091">https://arxiv.org/pdf/2504.16091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16091]] Post-Quantum Homomorphic Encryption: A Case for Code-Based Alternatives(https://arxiv.org/abs/2504.16091)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Homomorphic Encryption (HE) allows secure and privacy-protected computation on encrypted data without the need to decrypt it. Since Shor's algorithm rendered prime factorisation and discrete logarithm-based ciphers insecure with quantum computations, researchers have been working on building post-quantum homomorphic encryption (PQHE) algorithms. Most of the current PQHE algorithms are secured by Lattice-based problems and there have been limited attempts to build ciphers based on error-correcting code-based problems. This review presents an overview of the current approaches to building PQHE schemes and justifies code-based encryption as a novel way to diversify post-quantum algorithms. We present the mathematical underpinnings of existing code-based cryptographic frameworks and their security and efficiency guarantees. We compare lattice-based and code-based homomorphic encryption solutions identifying challenges that have inhibited the progress of code-based schemes. We finally propose five new research directions to advance post-quantum code-based homomorphic encryption.</li>
</ul>

<h3>Title: Audio and Multiscale Visual Cues Driven Cross-modal Transformer for Idling Vehicle Detection</h3>
<ul>
<li><strong>Authors: </strong>Xiwen Li, Ross Whitaker, Tolga Tasdizen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16102">https://arxiv.org/abs/2504.16102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16102">https://arxiv.org/pdf/2504.16102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16102]] Audio and Multiscale Visual Cues Driven Cross-modal Transformer for Idling Vehicle Detection(https://arxiv.org/abs/2504.16102)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Idling vehicle detection (IVD) supports real-time systems that reduce pollution and emissions by dynamically messaging drivers to curb excess idling behavior. In computer vision, IVD has become an emerging task that leverages video from surveillance cameras and audio from remote microphones to localize and classify vehicles in each frame as moving, idling, or engine-off. As with other cross-modal tasks, the key challenge lies in modeling the correspondence between audio and visual modalities, which differ in representation but provide complementary cues -- video offers spatial and motion context, while audio conveys engine activity beyond the visual field. The previous end-to-end model, which uses a basic attention mechanism, struggles to align these modalities effectively, often missing vehicle detections. To address this issue, we propose AVIVDNetv2, a transformer-based end-to-end detection network. It incorporates a cross-modal transformer with global patch-level learning, a multiscale visual feature fusion module, and decoupled detection heads. Extensive experiments show that AVIVDNetv2 improves mAP by 7.66 over the disjoint baseline and 9.42 over the E2E baseline, with consistent AP gains across all vehicle categories. Furthermore, AVIVDNetv2 outperforms the state-of-the-art method for sounding object localization, establishing a new performance benchmark on the AVIVD dataset.</li>
</ul>

<h3>Title: Shape Your Ground: Refining Road Surfaces Beyond Planar Representations</h3>
<ul>
<li><strong>Authors: </strong>Oussema Dhaouadi, Johannes Meier, Jacques Kaiser, Daniel Cremers</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16103">https://arxiv.org/abs/2504.16103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16103">https://arxiv.org/pdf/2504.16103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16103]] Shape Your Ground: Refining Road Surfaces Beyond Planar Representations(https://arxiv.org/abs/2504.16103)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Road surface reconstruction from aerial images is fundamental for autonomous driving, urban planning, and virtual simulation, where smoothness, compactness, and accuracy are critical quality factors. Existing reconstruction methods often produce artifacts and inconsistencies that limit usability, while downstream tasks have a tendency to represent roads as planes for simplicity but at the cost of accuracy. We introduce FlexRoad, the first framework to directly address road surface smoothing by fitting Non-Uniform Rational B-Splines (NURBS) surfaces to 3D road points obtained from photogrammetric reconstructions or geodata providers. Our method at its core utilizes the Elevation-Constrained Spatial Road Clustering (ECSRC) algorithm for robust anomaly correction, significantly reducing surface roughness and fitting errors. To facilitate quantitative comparison between road surface reconstruction methods, we present GeoRoad Dataset (GeRoD), a diverse collection of road surface and terrain profiles derived from openly accessible geodata. Experiments on GeRoD and the photogrammetry-based DeepScenario Open 3D Dataset (DSC3D) demonstrate that FlexRoad considerably surpasses commonly used road surface representations across various metrics while being insensitive to various input sources, terrains, and noise types. By performing ablation studies, we identify the key role of each component towards high-quality reconstruction performance, making FlexRoad a generic method for realistic road surface modeling.</li>
</ul>

<h3>Title: Trusted Identities for AI Agents: Leveraging Telco-Hosted eSIM Infrastructure</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Barros</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16108">https://arxiv.org/abs/2504.16108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16108">https://arxiv.org/pdf/2504.16108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16108]] Trusted Identities for AI Agents: Leveraging Telco-Hosted eSIM Infrastructure(https://arxiv.org/abs/2504.16108)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>The rise of autonomous AI agents in enterprise and industrial environments introduces a critical challenge: how to securely assign, verify, and manage their identities across distributed systems. Existing identity frameworks based on API keys, certificates, or application-layer credentials lack the infrastructure-grade trust, lifecycle control, and interoperability needed to manage agents operating independently in sensitive contexts. In this paper, we propose a conceptual architecture that leverages telecom-grade eSIM infrastructure, specifically hosted by mobile network operators (MNOs), to serve as a root of trust for AI agents. Rather than embedding SIM credentials in hardware devices, we envision a model where telcos host secure, certified hardware modules (eUICC or HSM) that store and manage agent-specific eSIM profiles. Agents authenticate remotely via cryptographic APIs or identity gateways, enabling scalable and auditable access to enterprise networks and services. We explore use cases such as onboarding enterprise automation agents, securing AI-driven financial systems, and enabling trust in inter-agent communications. We identify current limitations in GSMA and 3GPP standards, particularly their device centric assumptions, and propose extensions to support non-physical, software-based agents within trusted execution environments. This paper is intended as a conceptual framework to open discussion around standardization, security architecture, and the role of telecom infrastructure in the evolving agent economy.</li>
</ul>

<h3>Title: Security-First AI: Foundations for Robust and Trustworthy Systems</h3>
<ul>
<li><strong>Authors: </strong>Krti Tallam</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16110">https://arxiv.org/abs/2504.16110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16110">https://arxiv.org/pdf/2504.16110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16110]] Security-First AI: Foundations for Robust and Trustworthy Systems(https://arxiv.org/abs/2504.16110)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>The conversation around artificial intelligence (AI) often focuses on safety, transparency, accountability, alignment, and responsibility. However, AI security (i.e., the safeguarding of data, models, and pipelines from adversarial manipulation) underpins all of these efforts. This manuscript posits that AI security must be prioritized as a foundational layer. We present a hierarchical view of AI challenges, distinguishing security from safety, and argue for a security-first approach to enable trustworthy and resilient AI systems. We discuss core threat models, key attack vectors, and emerging defense mechanisms, concluding that a metric-driven approach to AI security is essential for robust AI safety, transparency, and accountability.</li>
</ul>

<h3>Title: AI-Based Vulnerability Analysis of NFT Smart Contracts</h3>
<ul>
<li><strong>Authors: </strong>Xin Wang, Xiaoqi Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16113">https://arxiv.org/abs/2504.16113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16113">https://arxiv.org/pdf/2504.16113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16113]] AI-Based Vulnerability Analysis of NFT Smart Contracts(https://arxiv.org/abs/2504.16113)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In the research experiment of this article, our research work is divided into several stages. Firstly, we collected a large number of smart contract codes and classified them, identifying several common defects, including Risky Mutably Porxy, ERC-721 Recentrancy, Unlimited Mining, Missing Requirements, and Public Burns. Secondly, we used Python to process the smart contracts. On the one hand, we modified the file names, and on the other hand, we batched the process of the content for analysis and application. Next, we built a model of the decision tree. Firstly, we carried out the feature extraction. We selected the algorithm and divided the data. After comparing and processing, we chose the CART classification tree to process. By gene coefficient, we analyzed and sorted the data, and got the initial model of the decision tree. Then, we introduced the random forest model on the basis of the decision tree. From abstracting the same amount of samples to selecting features this http URL adjusting and optimizing parameters to completing the construction of the forest model. Finally, we compared and analyzed the decision tree, random forest, and self-built model in the paper and drew general conclusions.</li>
</ul>

<h3>Title: Persistence-based Hough Transform for Line Detection</h3>
<ul>
<li><strong>Authors: </strong>Johannes Ferner, Stefan Huber, Saverio Messineo, Angel Pop, Martin Uray</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16114">https://arxiv.org/abs/2504.16114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16114">https://arxiv.org/pdf/2504.16114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16114]] Persistence-based Hough Transform for Line Detection(https://arxiv.org/abs/2504.16114)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Hough transform is a popular and classical technique in computer vision for the detection of lines (or more general objects). It maps a pixel into a dual space -- the Hough space: each pixel is mapped to the set of lines through this pixel, which forms a curve in Hough space. The detection of lines then becomes a voting process to find those lines that received many votes by pixels. However, this voting is done by thresholding, which is susceptible to noise and other artifacts. In this work, we present an alternative voting technique to detect peaks in the Hough space based on persistent homology, which very naturally addresses limitations of simple thresholding. Experiments on synthetic data show that our method significantly outperforms the original method, while also demonstrating enhanced robustness. This work seeks to inspire future research in two key directions. First, we highlight the untapped potential of Topological Data Analysis techniques and advocate for their broader integration into existing methods, including well-established ones. Secondly, we initiate a discussion on the mathematical stability of the Hough transform, encouraging exploration of mathematically grounded improvements to enhance its robustness.</li>
</ul>

<h3>Title: DMind Benchmark: The First Comprehensive Benchmark for LLM Evaluation in the Web3 Domain</h3>
<ul>
<li><strong>Authors: </strong>Miracle Master, Rainy Sun, Anya Reese, Joey Ouyang, Alex Chen, Winter Dong, Frank Li, James Yi, Garry Zhao, Tony Ling, Hobert Wong, Lowes Yang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16116">https://arxiv.org/abs/2504.16116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16116">https://arxiv.org/pdf/2504.16116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16116]] DMind Benchmark: The First Comprehensive Benchmark for LLM Evaluation in the Web3 Domain(https://arxiv.org/abs/2504.16116)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have led to significant progress on a wide range of natural language processing tasks. However, their effectiveness in specialized and rapidly evolving domains such as Web3 remains underexplored. In this paper, we introduce DMind Benchmark, a novel framework that systematically tests LLMs across nine key categories encompassing blockchain fundamentals, infrastructure, smart contract analysis, decentralized finance (DeFi), decentralized autonomous organizations (DAOs), non-fungible tokens (NFTs), token economics, meme concepts, and security vulnerabilities. DMind Benchmark goes beyond conventional multiple-choice questions by incorporating domain-specific subjective tasks (e.g., smart contract code auditing and repair, numeric reasoning on on-chain data, and fill-in assessments), thereby capturing real-world complexities and stress-testing model adaptability. We evaluate fifteen popular LLMs (from ChatGPT, DeepSeek, Claude, and Gemini series) on DMind Benchmark, uncovering performance gaps in Web3-specific reasoning and application, particularly in emerging areas like token economics and meme concepts. Even the strongest models face significant challenges in identifying subtle security vulnerabilities and analyzing complex DeFi mechanisms. To foster progress in this area, we publicly release our benchmark dataset, evaluation pipeline, and annotated results at this http URL, offering a valuable resource for advancing specialized domain adaptation and the development of more robust Web3-enabled LLMs.</li>
</ul>

<h3>Title: Context-Awareness and Interpretability of Rare Occurrences for Discovery and Formalization of Critical Failure Modes</h3>
<ul>
<li><strong>Authors: </strong>Sridevi Polavaram, Xin Zhou, Meenu Ravi, Mohammad Zarei, Anmol Srivastava</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16117">https://arxiv.org/abs/2504.16117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16117">https://arxiv.org/pdf/2504.16117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16117]] Context-Awareness and Interpretability of Rare Occurrences for Discovery and Formalization of Critical Failure Modes(https://arxiv.org/abs/2504.16117)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, interpretability</a></li>
<li><strong>Abstract: </strong>Vision systems are increasingly deployed in critical domains such as surveillance, law enforcement, and transportation. However, their vulnerabilities to rare or unforeseen scenarios pose significant safety risks. To address these challenges, we introduce Context-Awareness and Interpretability of Rare Occurrences (CAIRO), an ontology-based human-assistive discovery framework for failure cases (or CP - Critical Phenomena) detection and formalization. CAIRO by design incentivizes human-in-the-loop for testing and evaluation of criticality that arises from misdetections, adversarial attacks, and hallucinations in AI black-box models. Our robust analysis of object detection model(s) failures in automated driving systems (ADS) showcases scalable and interpretable ways of formalizing the observed gaps between camera perception and real-world contexts, resulting in test cases stored as explicit knowledge graphs (in OWL/XML format) amenable for sharing, downstream analysis, logical reasoning, and accountability.</li>
</ul>

<h3>Title: Towards Explainable and Lightweight AI for Real-Time Cyber Threat Hunting in Edge Networks</h3>
<ul>
<li><strong>Authors: </strong>Milad Rahmati</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16118">https://arxiv.org/abs/2504.16118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16118">https://arxiv.org/pdf/2504.16118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16118]] Towards Explainable and Lightweight AI for Real-Time Cyber Threat Hunting in Edge Networks(https://arxiv.org/abs/2504.16118)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, federate, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>As cyber threats continue to evolve, securing edge networks has become increasingly challenging due to their distributed nature and resource limitations. Many AI-driven threat detection systems rely on complex deep learning models, which, despite their high accuracy, suffer from two major drawbacks: lack of interpretability and high computational cost. Black-box AI models make it difficult for security analysts to understand the reasoning behind their predictions, limiting their practical deployment. Moreover, conventional deep learning techniques demand significant computational resources, rendering them unsuitable for edge devices with limited processing power. To address these issues, this study introduces an Explainable and Lightweight AI (ELAI) framework designed for real-time cyber threat detection in edge networks. Our approach integrates interpretable machine learning algorithms with optimized lightweight deep learning techniques, ensuring both transparency and computational efficiency. The proposed system leverages decision trees, attention-based deep learning, and federated learning to enhance detection accuracy while maintaining explainability. We evaluate ELAI using benchmark cybersecurity datasets, such as CICIDS and UNSW-NB15, assessing its performance across diverse cyberattack scenarios. Experimental results demonstrate that the proposed framework achieves high detection rates with minimal false positives, all while significantly reducing computational demands compared to traditional deep learning methods. The key contributions of this work include: (1) a novel interpretable AI-based cybersecurity model tailored for edge computing environments, (2) an optimized lightweight deep learning approach for real-time cyber threat detection, and (3) a comprehensive analysis of explainability techniques in AI-driven cybersecurity applications.</li>
</ul>

<h3>Title: A Data-Centric Approach for Safe and Secure Large Language Models against Threatening and Toxic Content</h3>
<ul>
<li><strong>Authors: </strong>Chaima Njeh, Ha√Øfa Nakouri, Fehmi Jaafar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16120">https://arxiv.org/abs/2504.16120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16120">https://arxiv.org/pdf/2504.16120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16120]] A Data-Centric Approach for Safe and Secure Large Language Models against Threatening and Toxic Content(https://arxiv.org/abs/2504.16120)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLM) have made remarkable progress, but concerns about potential biases and harmful content persist. To address these apprehensions, we introduce a practical solution for ensuring LLM's safe and ethical use. Our novel approach focuses on a post-generation correction mechanism, the BART-Corrective Model, which adjusts generated content to ensure safety and security. Unlike relying solely on model fine-tuning or prompt engineering, our method provides a robust data-centric alternative for mitigating harmful content. We demonstrate the effectiveness of our approach through experiments on multiple toxic datasets, which show a significant reduction in mean toxicity and jail-breaking scores after integration. Specifically, our results show a reduction of 15% and 21% in mean toxicity and jail-breaking scores with GPT-4, a substantial reduction of 28% and 5% with PaLM2, a reduction of approximately 26% and 23% with Mistral-7B, and a reduction of 11.1% and 19% with Gemma-2b-it. These results demonstrate the potential of our approach to improve the safety and security of LLM, making them more suitable for real-world applications.</li>
</ul>

<h3>Title: Breaking the Prompt Wall (I): A Real-World Case Study of Attacking ChatGPT via Lightweight Prompt Injection</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Chang, Guang Dai, Hao Di, Haishan Ye</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16125">https://arxiv.org/abs/2504.16125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16125">https://arxiv.org/pdf/2504.16125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16125]] Breaking the Prompt Wall (I): A Real-World Case Study of Attacking ChatGPT via Lightweight Prompt Injection(https://arxiv.org/abs/2504.16125)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>This report presents a real-world case study demonstrating how prompt injection can attack large language model platforms such as ChatGPT according to a proposed injection framework. By providing three real-world examples, we show how adversarial prompts can be injected via user inputs, web-based retrieval, and system-level agent instructions. These attacks, though lightweight and low-cost, can cause persistent and misleading behaviors in LLM outputs. Our case study reveals that even commercial-grade LLMs remain vulnerable to subtle manipulations that bypass safety filters and influence user decisions. \textbf{More importantly, we stress that this report is not intended as an attack guide, but as a technical alert. As ethical researchers, we aim to raise awareness and call upon developers, especially those at OpenAI, to treat prompt-level security as a critical design priority.</li>
</ul>

<h3>Title: Hybrid Knowledge Transfer through Attention and Logit Distillation for On-Device Vision Systems in Agricultural IoT</h3>
<ul>
<li><strong>Authors: </strong>Stanley Mugisha, Rashid Kisitu, Florence Tushabe</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16128">https://arxiv.org/abs/2504.16128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16128">https://arxiv.org/pdf/2504.16128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16128]] Hybrid Knowledge Transfer through Attention and Logit Distillation for On-Device Vision Systems in Agricultural IoT(https://arxiv.org/abs/2504.16128)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Integrating deep learning applications into agricultural IoT systems faces a serious challenge of balancing the high accuracy of Vision Transformers (ViTs) with the efficiency demands of resource-constrained edge devices. Large transformer models like the Swin Transformers excel in plant disease classification by capturing global-local dependencies. However, their computational complexity (34.1 GFLOPs) limits applications and renders them impractical for real-time on-device inference. Lightweight models such as MobileNetV3 and TinyML would be suitable for on-device inference but lack the required spatial reasoning for fine-grained disease detection. To bridge this gap, we propose a hybrid knowledge distillation framework that synergistically transfers logit and attention knowledge from a Swin Transformer teacher to a MobileNetV3 student model. Our method includes the introduction of adaptive attention alignment to resolve cross-architecture mismatch (resolution, channels) and a dual-loss function optimizing both class probabilities and spatial focus. On the lantVillage-Tomato dataset (18,160 images), the distilled MobileNetV3 attains 92.4% accuracy relative to 95.9% for Swin-L but at an 95% reduction on PC and < 82% in inference latency on IoT devices. (23ms on PC CPU and 86ms/image on smartphone CPUs). Key innovations include IoT-centric validation metrics (13 MB memory, 0.22 GFLOPs) and dynamic resolution-matching attention maps. Comparative experiments show significant improvements over standalone CNNs and prior distillation methods, with a 3.5% accuracy gain over MobileNetV3 baselines. Significantly, this work advances real-time, energy-efficient crop monitoring in precision agriculture and demonstrates how we can attain ViT-level diagnostic precision on edge devices. Code and models will be made available for replication after acceptance.</li>
</ul>

<h3>Title: Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Abu Tami, Mohammed Elhenawy, Huthaifa I. Ashqar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16134">https://arxiv.org/abs/2504.16134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16134">https://arxiv.org/pdf/2504.16134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16134]] Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends(https://arxiv.org/abs/2504.16134)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Traffic safety remains a critical global challenge, with traditional Advanced Driver-Assistance Systems (ADAS) often struggling in dynamic real-world scenarios due to fragmented sensor processing and susceptibility to adversarial conditions. This paper reviews the transformative potential of Multimodal Large Language Models (MLLMs) in addressing these limitations by integrating cross-modal data such as visual, spatial, and environmental inputs to enable holistic scene understanding. Through a comprehensive analysis of MLLM-based approaches, we highlight their capabilities in enhancing perception, decision-making, and adversarial robustness, while also examining the role of key datasets (e.g., KITTI, DRAMA, ML4RoadSafety) in advancing research. Furthermore, we outline future directions, including real-time edge deployment, causality-driven reasoning, and human-AI collaboration. By positioning MLLMs as a cornerstone for next-generation traffic safety systems, this review underscores their potential to revolutionize the field, offering scalable, context-aware solutions that proactively mitigate risks and improve overall road safety.</li>
</ul>

<h3>Title: Active Learning Methods for Efficient Data Utilization and Model Performance Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Chiung-Yi Tseng, Junhao Song, Ziqian Bi, Tianyang Wang, Chia Xin Liang, Ming Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16136">https://arxiv.org/abs/2504.16136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16136">https://arxiv.org/pdf/2504.16136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16136]] Active Learning Methods for Efficient Data Utilization and Model Performance Enhancement(https://arxiv.org/abs/2504.16136)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>In the era of data-driven intelligence, the paradox of data abundance and annotation scarcity has emerged as a critical bottleneck in the advancement of machine learning. This paper gives a detailed overview of Active Learning (AL), which is a strategy in machine learning that helps models achieve better performance using fewer labeled examples. It introduces the basic concepts of AL and discusses how it is used in various fields such as computer vision, natural language processing, transfer learning, and real-world applications. The paper focuses on important research topics such as uncertainty estimation, handling of class imbalance, domain adaptation, fairness, and the creation of strong evaluation metrics and benchmarks. It also shows that learning methods inspired by humans and guided by questions can improve data efficiency and help models learn more effectively. In addition, this paper talks about current challenges in the field, including the need to rebuild trust, ensure reproducibility, and deal with inconsistent methodologies. It points out that AL often gives better results than passive learning, especially when good evaluation measures are used. This work aims to be useful for both researchers and practitioners by providing key insights and proposing directions for future progress in active learning.</li>
</ul>

<h3>Title: SparseJEPA: Sparse Representation Learning of Joint Embedding Predictive Architectures</h3>
<ul>
<li><strong>Authors: </strong>Max Hartman, Lav Varshney</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16140">https://arxiv.org/abs/2504.16140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16140">https://arxiv.org/pdf/2504.16140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16140]] SparseJEPA: Sparse Representation Learning of Joint Embedding Predictive Architectures(https://arxiv.org/abs/2504.16140)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Joint Embedding Predictive Architectures (JEPA) have emerged as a powerful framework for learning general-purpose representations. However, these models often lack interpretability and suffer from inefficiencies due to dense embedding representations. We propose SparseJEPA, an extension that integrates sparse representation learning into the JEPA framework to enhance the quality of learned representations. SparseJEPA employs a penalty method that encourages latent space variables to be shared among data features with strong semantic relationships, while maintaining predictive performance. We demonstrate the effectiveness of SparseJEPA by training on the CIFAR-100 dataset and pre-training a lightweight Vision Transformer. The improved embeddings are utilized in linear-probe transfer learning for both image classification and low-level tasks, showcasing the architecture's versatility across different transfer tasks. Furthermore, we provide a theoretical proof that demonstrates that the grouping mechanism enhances representation quality. This was done by displaying that grouping reduces Multiinformation among latent-variables, including proofing the Data Processing Inequality for Multiinformation. Our results indicate that incorporating sparsity not only refines the latent space but also facilitates the learning of more meaningful and interpretable representations. In further work, hope to further extend this method by finding new ways to leverage the grouping mechanism through object-centric representation learning.</li>
</ul>

<h3>Title: Deep Learning Meets Process-Based Models: A Hybrid Approach to Agricultural Challenges</h3>
<ul>
<li><strong>Authors: </strong>Yue Shi, Liangxiu Han, Xin Zhang, Tam Sobeih, Thomas Gaiser, Nguyen Huu Thuy, Dominik Behrend, Amit Kumar Srivastava, Krishnagopal Halder, Frank Ewert</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16141">https://arxiv.org/abs/2504.16141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16141">https://arxiv.org/pdf/2504.16141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16141]] Deep Learning Meets Process-Based Models: A Hybrid Approach to Agricultural Challenges(https://arxiv.org/abs/2504.16141)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Process-based models (PBMs) and deep learning (DL) are two key approaches in agricultural modelling, each offering distinct advantages and limitations. PBMs provide mechanistic insights based on physical and biological principles, ensuring interpretability and scientific rigour. However, they often struggle with scalability, parameterisation, and adaptation to heterogeneous environments. In contrast, DL models excel at capturing complex, nonlinear patterns from large datasets but may suffer from limited interpretability, high computational demands, and overfitting in data-scarce scenarios. This study presents a systematic review of PBMs, DL models, and hybrid PBM-DL frameworks, highlighting their applications in agricultural and environmental modelling. We classify hybrid PBM-DL approaches into DL-informed PBMs, where neural networks refine process-based models, and PBM-informed DL, where physical constraints guide deep learning predictions. Additionally, we conduct a case study on crop dry biomass prediction, comparing hybrid models against standalone PBMs and DL models under varying data quality, sample sizes, and spatial conditions. The results demonstrate that hybrid models consistently outperform traditional PBMs and DL models, offering greater robustness to noisy data and improved generalisation across unseen locations. Finally, we discuss key challenges, including model interpretability, scalability, and data requirements, alongside actionable recommendations for advancing hybrid modelling in agriculture. By integrating domain knowledge with AI-driven approaches, this study contributes to the development of scalable, interpretable, and reproducible agricultural models that support data-driven decision-making for sustainable agriculture.</li>
</ul>

<h3>Title: Progressive Language-guided Visual Learning for Multi-Task Visual Grounding</h3>
<ul>
<li><strong>Authors: </strong>Jingchao Wang, Hong Wang, Wenlong Zhang, Kunhua Ji, Dingjiang Huang, Yefeng Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16145">https://arxiv.org/abs/2504.16145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16145">https://arxiv.org/pdf/2504.16145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16145]] Progressive Language-guided Visual Learning for Multi-Task Visual Grounding(https://arxiv.org/abs/2504.16145)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES). The existing representative approaches generally follow the research pipeline which mainly consists of three core procedures, including independent feature extraction for visual and linguistic modalities, respectively, cross-modal interaction module, and independent prediction heads for different sub-tasks. Albeit achieving remarkable performance, this research line has two limitations: 1) The linguistic content has not been fully injected into the entire visual backbone for boosting more effective visual feature extraction and it needs an extra cross-modal interaction module; 2) The relationship between REC and RES tasks is not effectively exploited to help the collaborative prediction for more accurate output. To deal with these problems, in this paper, we propose a Progressive Language-guided Visual Learning framework for multi-task visual grounding, called PLVL, which not only finely mine the inherent feature expression of the visual modality itself but also progressively inject the language information to help learn linguistic-related visual features. In this manner, our PLVL does not need additional cross-modal fusion module while fully introducing the language guidance. Furthermore, we analyze that the localization center for REC would help identify the to-be-segmented object region for RES to some extent. Inspired by this investigation, we design a multi-task head to accomplish collaborative predictions for these two sub-tasks. Extensive experiments conducted on several benchmark datasets comprehensively substantiate that our PLVL obviously outperforms the representative methods in both REC and RES tasks. this https URL</li>
</ul>

<h3>Title: Classification of Firn Data via Topological Features</h3>
<ul>
<li><strong>Authors: </strong>Sarah Day, Jesse Dimino, Matt Jester, Kaitlin Keegan, Thomas Weighill</a></li>
<li><strong>Subjects: </strong>cs.CV, math.AT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16150">https://arxiv.org/abs/2504.16150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16150">https://arxiv.org/pdf/2504.16150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16150]] Classification of Firn Data via Topological Features(https://arxiv.org/abs/2504.16150)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>In this paper we evaluate the performance of topological features for generalizable and robust classification of firn image data, with the broader goal of understanding the advantages, pitfalls, and trade-offs in topological featurization. Firn refers to layers of granular snow within glaciers that haven't been compressed into ice. This compactification process imposes distinct topological and geometric structure on firn that varies with depth within the firn column, making topological data analysis (TDA) a natural choice for understanding the connection between depth and structure. We use two classes of topological features, sublevel set features and distance transform features, together with persistence curves, to predict sample depth from microCT images. A range of challenging training-test scenarios reveals that no one choice of method dominates in all categories, and uncoveres a web of trade-offs between accuracy, interpretability, and generalizability.</li>
</ul>

<h3>Title: CLIP-IT: CLIP-based Pairing for Histology Images Classification</h3>
<ul>
<li><strong>Authors: </strong>Banafsheh Karimian (1), Giulia Avanzato (2), Soufian Belharbi (1), Luke McCaffrey (3), Mohammadhadi Shateri (1), Eric Granger (1) ((1) LIVIA ILLS Dept. of Systems Engineering ETS Montreal Canada, (2) Dept. of Computer Engineering University of Cagliari Italy, (3) Goodman Cancer Research Centre Dept. of Oncology McGill University Canada)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16181">https://arxiv.org/abs/2504.16181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16181">https://arxiv.org/pdf/2504.16181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16181]] CLIP-IT: CLIP-based Pairing for Histology Images Classification(https://arxiv.org/abs/2504.16181)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Multimodal learning has shown significant promise for improving medical image analysis by integrating information from complementary data sources. This is widely employed for training vision-language models (VLMs) for cancer detection based on histology images and text reports. However, one of the main limitations in training these VLMs is the requirement for large paired datasets, raising concerns over privacy, and data collection, annotation, and maintenance costs. To address this challenge, we introduce CLIP-IT method to train a vision backbone model to classify histology images by pairing them with privileged textual information from an external source. At first, the modality pairing step relies on a CLIP-based model to match histology images with semantically relevant textual report data from external sources, creating an augmented multimodal dataset without the need for manually paired samples. Then, we propose a multimodal training procedure that distills the knowledge from the paired text modality to the unimodal image classifier for enhanced performance without the need for the textual data during inference. A parameter-efficient fine-tuning method is used to efficiently address the misalignment between the main (image) and paired (text) modalities. During inference, the improved unimodal histology classifier is used, with only minimal additional computational complexity. Our experiments on challenging PCAM, CRC, and BACH histology image datasets show that CLIP-IT can provide a cost-effective approach to leverage privileged textual information and outperform unimodal classifiers for histology.</li>
</ul>

<h3>Title: FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking</h3>
<ul>
<li><strong>Authors: </strong>Jabez Magomere, Elena Kochkina, Samuel Mensah, Simerjot Kaur, Charese H. Smiley</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16188">https://arxiv.org/abs/2504.16188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16188">https://arxiv.org/pdf/2504.16188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16188]] FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking(https://arxiv.org/abs/2504.16188)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce FinNLI, a benchmark dataset for Financial Natural Language Inference (FinNLI) across diverse financial texts like SEC Filings, Annual Reports, and Earnings Call transcripts. Our dataset framework ensures diverse premise-hypothesis pairs while minimizing spurious correlations. FinNLI comprises 21,304 pairs, including a high-quality test set of 3,304 instances annotated by finance experts. Evaluations show that domain shift significantly degrades general-domain NLI performance. The highest Macro F1 scores for pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and 78.62%, respectively, highlighting the dataset's difficulty. Surprisingly, instruction-tuned financial LLMs perform poorly, suggesting limited generalizability. FinNLI exposes weaknesses in current LLMs for financial reasoning, indicating room for improvement.</li>
</ul>

<h3>Title: ReGraph: A Tool for Binary Similarity Identification</h3>
<ul>
<li><strong>Authors: </strong>Li Zhou, Marc Dacier, Charalambos Konstantinou</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16219">https://arxiv.org/abs/2504.16219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16219">https://arxiv.org/pdf/2504.16219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16219]] ReGraph: A Tool for Binary Similarity Identification(https://arxiv.org/abs/2504.16219)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Binary Code Similarity Detection (BCSD) is not only essential for security tasks such as vulnerability identification but also for code copying detection, yet it remains challenging due to binary stripping and diverse compilation environments. Existing methods tend to adopt increasingly complex neural networks for better accuracy performance. The computation time increases with the complexity. Even with powerful GPUs, the treatment of large-scale software becomes time-consuming. To address these issues, we present a framework called ReGraph to efficiently compare binary code functions across architectures and optimization levels. Our evaluation with public datasets highlights that ReGraph exhibits a significant speed advantage, performing 700 times faster than Natural Language Processing (NLP)-based methods while maintaining comparable accuracy results with respect to the state-of-the-art models.</li>
</ul>

<h3>Title: Blockchain Meets Adaptive Honeypots: A Trust-Aware Approach to Next-Gen IoT Security</h3>
<ul>
<li><strong>Authors: </strong>Yazan Otoum, Arghavan Asad, Amiya Nayak</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.ET, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16226">https://arxiv.org/abs/2504.16226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16226">https://arxiv.org/pdf/2504.16226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16226]] Blockchain Meets Adaptive Honeypots: A Trust-Aware Approach to Next-Gen IoT Security(https://arxiv.org/abs/2504.16226)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, diffusion</a></li>
<li><strong>Abstract: </strong>Edge computing-based Next-Generation Wireless Networks (NGWN)-IoT offer enhanced bandwidth capacity for large-scale service provisioning but remain vulnerable to evolving cyber threats. Existing intrusion detection and prevention methods provide limited security as adversaries continually adapt their attack strategies. We propose a dynamic attack detection and prevention approach to address this challenge. First, blockchain-based authentication uses the Deoxys Authentication Algorithm (DAA) to verify IoT device legitimacy before data transmission. Next, a bi-stage intrusion detection system is introduced: the first stage uses signature-based detection via an Improved Random Forest (IRF) algorithm. In contrast, the second stage applies feature-based anomaly detection using a Diffusion Convolution Recurrent Neural Network (DCRNN). To ensure Quality of Service (QoS) and maintain Service Level Agreements (SLA), trust-aware service migration is performed using Heap-Based Optimization (HBO). Additionally, on-demand virtual High-Interaction honeypots deceive attackers and extract attack patterns, which are securely stored using the Bimodal Lattice Signature Scheme (BLISS) to enhance signature-based Intrusion Detection Systems (IDS). The proposed framework is implemented in the NS3 simulation environment and evaluated against existing methods across multiple performance metrics, including accuracy, attack detection rate, false negative rate, precision, recall, ROC curve, memory usage, CPU usage, and execution time. Experimental results demonstrate that the framework significantly outperforms existing approaches, reinforcing the security of NGWN-enabled IoT ecosystems</li>
</ul>

<h3>Title: General Post-Processing Framework for Fairness Adjustment of Machine Learning Models</h3>
<ul>
<li><strong>Authors: </strong>L√©andre Eberhard, Nirek Sharma, Filipp Shelobolin, Aalok Ganesh Shanbhag</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16238">https://arxiv.org/abs/2504.16238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16238">https://arxiv.org/pdf/2504.16238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16238]] General Post-Processing Framework for Fairness Adjustment of Machine Learning Models(https://arxiv.org/abs/2504.16238)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>As machine learning increasingly influences critical domains such as credit underwriting, public policy, and talent acquisition, ensuring compliance with fairness constraints is both a legal and ethical imperative. This paper introduces a novel framework for fairness adjustments that applies to diverse machine learning tasks, including regression and classification, and accommodates a wide range of fairness metrics. Unlike traditional approaches categorized as pre-processing, in-processing, or post-processing, our method adapts in-processing techniques for use as a post-processing step. By decoupling fairness adjustments from the model training process, our framework preserves model performance on average while enabling greater flexibility in model development. Key advantages include eliminating the need for custom loss functions, enabling fairness tuning using different datasets, accommodating proprietary models as black-box systems, and providing interpretable insights into the fairness adjustments. We demonstrate the effectiveness of this approach by comparing it to Adversarial Debiasing, showing that our framework achieves a comparable fairness/accuracy tradeoff on real-world datasets.</li>
</ul>

<h3>Title: FairPlay: A Collaborative Approach to Mitigate Bias in Datasets for Improved AI Fairness</h3>
<ul>
<li><strong>Authors: </strong>Tina Behzad, Mithilesh Kumar Singh, Anthony J. Ripa, Klaus Mueller</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16255">https://arxiv.org/abs/2504.16255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16255">https://arxiv.org/pdf/2504.16255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16255]] FairPlay: A Collaborative Approach to Mitigate Bias in Datasets for Improved AI Fairness(https://arxiv.org/abs/2504.16255)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The issue of fairness in decision-making is a critical one, especially given the variety of stakeholder demands for differing and mutually incompatible versions of fairness. Adopting a strategic interaction of perspectives provides an alternative to enforcing a singular standard of fairness. We present a web-based software application, FairPlay, that enables multiple stakeholders to debias datasets collaboratively. With FairPlay, users can negotiate and arrive at a mutually acceptable outcome without a universally agreed-upon theory of fairness. In the absence of such a tool, reaching a consensus would be highly challenging due to the lack of a systematic negotiation process and the inability to modify and observe changes. We have conducted user studies that demonstrate the success of FairPlay, as users could reach a consensus within about five rounds of gameplay, illustrating the application's potential for enhancing fairness in AI systems.</li>
</ul>

<h3>Title: Learning Energy-Based Generative Models via Potential Flow: A Variational Principle Approach to Probability Density Homotopy Matching</h3>
<ul>
<li><strong>Authors: </strong>Junn Yong Loo, Michelle Adeline, Julia Kaiwen Lau, Fang Yu Leong, Hwa Hui Tew, Arghya Pal, Vishnu Monn Baskaran, Chee-Ming Ting, Rapha√´l C.-W. Phan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16262">https://arxiv.org/abs/2504.16262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16262">https://arxiv.org/pdf/2504.16262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16262]] Learning Energy-Based Generative Models via Potential Flow: A Variational Principle Approach to Probability Density Homotopy Matching(https://arxiv.org/abs/2504.16262)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, generative</a></li>
<li><strong>Abstract: </strong>Energy-based models (EBMs) are a powerful class of probabilistic generative models due to their flexibility and interpretability. However, relationships between potential flows and explicit EBMs remain underexplored, while contrastive divergence training via implicit Markov chain Monte Carlo (MCMC) sampling is often unstable and expensive in high-dimensional settings. In this paper, we propose Variational Potential Flow Bayes (VPFB), a new energy-based generative framework that eliminates the need for implicit MCMC sampling and does not rely on auxiliary networks or cooperative training. VPFB learns an energy-parameterized potential flow by constructing a flow-driven density homotopy that is matched to the data distribution through a variational loss minimizing the Kullback-Leibler divergence between the flow-driven and marginal homotopies. This principled formulation enables robust and efficient generative modeling while preserving the interpretability of EBMs. Experimental results on image generation, interpolation, out-of-distribution detection, and compositional generation confirm the effectiveness of VPFB, showing that our method performs competitively with existing approaches in terms of sample quality and versatility across diverse generative modeling tasks.</li>
</ul>

<h3>Title: Gradient-Optimized Fuzzy Classifier: A Benchmark Study Against State-of-the-Art Models</h3>
<ul>
<li><strong>Authors: </strong>Magnus Sieverding, Nathan Steffen, Kelly Cohen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16263">https://arxiv.org/abs/2504.16263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16263">https://arxiv.org/pdf/2504.16263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16263]] Gradient-Optimized Fuzzy Classifier: A Benchmark Study Against State-of-the-Art Models(https://arxiv.org/abs/2504.16263)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a performance benchmarking study of a Gradient-Optimized Fuzzy Inference System (GF) classifier against several state-of-the-art machine learning models, including Random Forest, XGBoost, Logistic Regression, Support Vector Machines, and Neural Networks. The evaluation was conducted across five datasets from the UCI Machine Learning Repository, each chosen for their diversity in input types, class distributions, and classification complexity. Unlike traditional Fuzzy Inference Systems that rely on derivative-free optimization methods, the GF leverages gradient descent to significantly improving training efficiency and predictive performance. Results demonstrate that the GF model achieved competitive, and in several cases superior, classification accuracy while maintaining high precision and exceptionally low training times. In particular, the GF exhibited strong consistency across folds and datasets, underscoring its robustness in handling noisy data and variable feature sets. These findings support the potential of gradient optimized fuzzy systems as interpretable, efficient, and adaptable alternatives to more complex deep learning models in supervised learning tasks.</li>
</ul>

<h3>Title: Learning Explainable Dense Reward Shapes via Bayesian Optimization</h3>
<ul>
<li><strong>Authors: </strong>Ryan Koo, Ian Yang, Vipul Raheja, Mingyi Hong, Kwang-Sung Jun, Dongyeop Kang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16272">https://arxiv.org/abs/2504.16272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16272">https://arxiv.org/pdf/2504.16272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16272]] Learning Explainable Dense Reward Shapes via Bayesian Optimization(https://arxiv.org/abs/2504.16272)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Current reinforcement learning from human feedback (RLHF) pipelines for large language model (LLM) alignment typically assign scalar rewards to sequences, using the final token as a surrogate indicator for the quality of the entire sequence. However, this leads to sparse feedback and suboptimal token-level credit assignment. In this work, we frame reward shaping as an optimization problem focused on token-level credit assignment. We propose a reward-shaping function leveraging explainability methods such as SHAP and LIME to estimate per-token rewards from the reward model. To learn parameters of this shaping function, we employ a bilevel optimization framework that integrates Bayesian Optimization and policy training to handle noise from the token reward estimates. Our experiments show that achieving a better balance of token-level reward attribution leads to performance improvements over baselines on downstream tasks and finds an optimal policy faster during training. Furthermore, we show theoretically that explainability methods that are feature additive attribution functions maintain the optimal policy as the original reward.</li>
</ul>

<h3>Title: Quantum Doubly Stochastic Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jannis Born, Filip Skogh, Kahn Rhrissorrakrai, Filippo Utro, Nico Wagner, Aleksandros Sobczyk</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16275">https://arxiv.org/abs/2504.16275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16275">https://arxiv.org/pdf/2504.16275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16275]] Quantum Doubly Stochastic Transformers(https://arxiv.org/abs/2504.16275)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>At the core of the Transformer, the Softmax normalizes the attention matrix to be right stochastic. Previous research has shown that this often destabilizes training and that enforcing the attention matrix to be doubly stochastic (through Sinkhorn's algorithm) consistently improves performance across different tasks, domains and Transformer flavors. However, Sinkhorn's algorithm is iterative, approximative, non-parametric and thus inflexible w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been proven that DSMs can be obtained with a parametric quantum circuit, yielding a novel quantum inductive bias for DSMs with no known classical analogue. Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum doubly stochastic Transformer (QDSFormer) that replaces the Softmax in the self-attention layer with a variational quantum circuit. We study the expressive power of the circuit and find that it yields more diverse DSMs that better preserve information than classical operators. Across multiple small-scale object recognition tasks, we find that our QDSFormer consistently surpasses both a standard Vision Transformer and other doubly stochastic Transformers. Beyond the established Sinkformer, this comparison includes a novel quantum-inspired doubly stochastic Transformer (based on QR decomposition) that can be of independent interest. The QDSFormer also shows improved training stability and lower performance variation suggesting that it may mitigate the notoriously unstable training of ViTs on small-scale data.</li>
</ul>

<h3>Title: Affect Models Have Weak Generalizability to Atypical Speech</h3>
<ul>
<li><strong>Authors: </strong>Jaya Narain, Amrit Romana, Vikramjit Mitra, Colin Lea, Shirley Ren</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16283">https://arxiv.org/abs/2504.16283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16283">https://arxiv.org/pdf/2504.16283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16283]] Affect Models Have Weak Generalizability to Atypical Speech(https://arxiv.org/abs/2504.16283)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Speech and voice conditions can alter the acoustic properties of speech, which could impact the performance of paralinguistic models for affect for people with atypical speech. We evaluate publicly available models for recognizing categorical and dimensional affect from speech on a dataset of atypical speech, comparing results to datasets of typical speech. We investigate three dimensions of speech atypicality: intelligibility, which is related to pronounciation; monopitch, which is related to prosody, and harshness, which is related to voice quality. We look at (1) distributional trends of categorical affect predictions within the dataset, (2) distributional comparisons of categorical affect predictions to similar datasets of typical speech, and (3) correlation strengths between text and speech predictions for spontaneous speech for valence and arousal. We find that the output of affect models is significantly impacted by the presence and degree of speech atypicalities. For instance, the percentage of speech predicted as sad is significantly higher for all types and grades of atypical speech when compared to similar typical speech datasets. In a preliminary investigation on improving robustness for atypical speech, we find that fine-tuning models on pseudo-labeled atypical speech data improves performance on atypical speech without impacting performance on typical speech. Our results emphasize the need for broader training and evaluation datasets for speech emotion models, and for modeling approaches that are robust to voice and speech differences.</li>
</ul>

<h3>Title: The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation</h3>
<ul>
<li><strong>Authors: </strong>Li Weigang, Pedro Carvalho Brom</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16286">https://arxiv.org/abs/2504.16286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16286">https://arxiv.org/pdf/2504.16286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16286]] The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation(https://arxiv.org/abs/2504.16286)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has reshaped the landscape of machine translation, yet challenges persist in preserving poetic intent, cultural heritage, and handling specialized terminology in Chinese-English translation. This study constructs a diverse corpus encompassing Chinese scientific terminology, historical translation paradoxes, and literary metaphors. Utilizing a back-translation and Friedman test-based evaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic similarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three traditional translation tools. Key findings include: (1) Scientific abstracts often benefit from back-translation, while traditional tools outperform LLMs in linguistically distinct texts; (2) LLMs struggle with cultural and literary retention, exemplifying the "paradox of poetic intent"; (3) Some models exhibit "verbatim back-translation", reflecting emergent memory behavior; (4) A novel BLEU variant using Jieba segmentation and n-gram weighting is proposed. The study contributes to the empirical evaluation of Chinese NLP performance and advances understanding of cultural fidelity in AI-mediated translation.</li>
</ul>

<h3>Title: Naturally Computed Scale Invariance in the Residual Stream of ResNet18</h3>
<ul>
<li><strong>Authors: </strong>Andr√© Longon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16290">https://arxiv.org/abs/2504.16290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16290">https://arxiv.org/pdf/2504.16290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16290]] Naturally Computed Scale Invariance in the Residual Stream of ResNet18(https://arxiv.org/abs/2504.16290)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>An important capacity in visual object recognition is invariance to image-altering variables which leave the identity of objects unchanged, such as lighting, rotation, and scale. How do neural networks achieve this? Prior mechanistic interpretability research has illuminated some invariance-building circuitry in InceptionV1, but the results are limited and networks with different architectures have remained largely unexplored. This work investigates ResNet18 with a particular focus on its residual stream, an architectural component which InceptionV1 lacks. We observe that many convolutional channels in intermediate blocks exhibit scale invariant properties, computed by the element-wise residual summation of scale equivariant representations: the block input's smaller-scale copy with the block pre-sum output's larger-scale copy. Through subsequent ablation experiments, we attempt to causally link these neural properties with scale-robust object recognition behavior. Our tentative findings suggest how the residual stream computes scale invariance and its possible role in behavior. Code is available at: this https URL</li>
</ul>

<h3>Title: Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware Training Objectives</h3>
<ul>
<li><strong>Authors: </strong>Zhangdie Yuan, Andreas Vlachos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16312">https://arxiv.org/abs/2504.16312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16312">https://arxiv.org/pdf/2504.16312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16312]] Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware Training Objectives(https://arxiv.org/abs/2504.16312)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Capturing symmetric (e.g., country borders another country) and antisymmetric (e.g., parent_of) relations is crucial for a variety of applications. This paper tackles this challenge by introducing a novel Wikidata-derived natural language inference dataset designed to evaluate large language models (LLMs). Our findings reveal that LLMs perform comparably to random chance on this benchmark, highlighting a gap in relational understanding. To address this, we explore encoder retraining via contrastive learning with k-nearest neighbors. The retrained encoder matches the performance of fine-tuned classification heads while offering additional benefits, including greater efficiency in few-shot learning and improved mitigation of catastrophic forgetting.</li>
</ul>

<h3>Title: SignX: The Foundation Model for Sign Recognition</h3>
<ul>
<li><strong>Authors: </strong>Sen Fang, Chunyu Sui, Hongwei Yi, Carol Neidle, Dimitris N. Metaxas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16315">https://arxiv.org/abs/2504.16315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16315">https://arxiv.org/pdf/2504.16315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16315]] SignX: The Foundation Model for Sign Recognition(https://arxiv.org/abs/2504.16315)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>The complexity of sign language data processing brings many challenges. The current approach to recognition of ASL signs aims to translate RGB sign language videos through pose information into English-based ID glosses, which serve to uniquely identify ASL signs. Note that there is no shared convention for assigning such glosses to ASL signs, so it is essential that the same glossing conventions are used for all of the data in the datasets that are employed. This paper proposes SignX, a foundation model framework for sign recognition. It is a concise yet powerful framework applicable to multiple human activity recognition scenarios. First, we developed a Pose2Gloss component based on an inverse diffusion model, which contains a multi-track pose fusion layer that unifies five of the most powerful pose information sources--SMPLer-X, DWPose, Mediapipe, PrimeDepth, and Sapiens Segmentation--into a single latent pose representation. Second, we trained a Video2Pose module based on ViT that can directly convert raw video into signer pose representation. Through this 2-stage training framework, we enable sign language recognition models to be compatible with existing pose formats, laying the foundation for the common pose estimation necessary for sign recognition. Experimental results show that SignX can recognize signs from sign language video, producing predicted gloss representations with greater accuracy than has been reported in prior work.</li>
</ul>

<h3>Title: On the Consistency of GNN Explanations for Malware Detection</h3>
<ul>
<li><strong>Authors: </strong>Hossein Shokouhinejad, Griffin Higgins, Roozbeh Razavi-Far, Hesamodin Mohammadian, Ali A. Ghorbani</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16316">https://arxiv.org/abs/2504.16316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16316">https://arxiv.org/pdf/2504.16316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16316]] On the Consistency of GNN Explanations for Malware Detection(https://arxiv.org/abs/2504.16316)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Control Flow Graphs (CFGs) are critical for analyzing program execution and characterizing malware behavior. With the growing adoption of Graph Neural Networks (GNNs), CFG-based representations have proven highly effective for malware detection. This study proposes a novel framework that dynamically constructs CFGs and embeds node features using a hybrid approach combining rule-based encoding and autoencoder-based embedding. A GNN-based classifier is then constructed to detect malicious behavior from the resulting graph representations. To improve model interpretability, we apply state-of-the-art explainability techniques, including GNNExplainer, PGExplainer, and CaptumExplainer, the latter is utilized three attribution methods: Integrated Gradients, Guided Backpropagation, and Saliency. In addition, we introduce a novel aggregation method, called RankFusion, that integrates the outputs of the top-performing explainers to enhance the explanation quality. We also evaluate explanations using two subgraph extraction strategies, including the proposed Greedy Edge-wise Composition (GEC) method for improved structural coherence. A comprehensive evaluation using accuracy, fidelity, and consistency metrics demonstrates the effectiveness of the proposed framework in terms of accurate identification of malware samples and generating reliable and interpretable explanations.</li>
</ul>

<h3>Title: Transformer-Based Extraction of Statutory Definitions from the U.S. Code</h3>
<ul>
<li><strong>Authors: </strong>Arpana Hosabettu (Google), Harsh Shah (Cornell University)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16353">https://arxiv.org/abs/2504.16353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16353">https://arxiv.org/pdf/2504.16353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16353]] Transformer-Based Extraction of Statutory Definitions from the U.S. Code(https://arxiv.org/abs/2504.16353)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Automatic extraction of definitions from legal texts is critical for enhancing the comprehension and clarity of complex legal corpora such as the United States Code (U.S.C.). We present an advanced NLP system leveraging transformer-based architectures to automatically extract defined terms, their definitions, and their scope from the U.S.C. We address the challenges of automatically identifying legal definitions, extracting defined terms, and determining their scope within this complex corpus of over 200,000 pages of federal statutory law. Building upon previous feature-based machine learning methods, our updated model employs domain-specific transformers (Legal-BERT) fine-tuned specifically for statutory texts, significantly improving extraction accuracy. Our work implements a multi-stage pipeline that combines document structure analysis with state-of-the-art language models to process legal text from the XML version of the U.S. Code. Each paragraph is first classified using a fine-tuned legal domain BERT model to determine if it contains a definition. Our system then aggregates related paragraphs into coherent definitional units and applies a combination of attention mechanisms and rule-based patterns to extract defined terms and their jurisdictional scope. The definition extraction system is evaluated on multiple titles of the U.S. Code containing thousands of definitions, demonstrating significant improvements over previous approaches. Our best model achieves 96.8% precision and 98.9% recall (98.2% F1-score), substantially outperforming traditional machine learning classifiers. This work contributes to improving accessibility and understanding of legal information while establishing a foundation for downstream legal reasoning tasks.</li>
</ul>

<h3>Title: Property-Preserving Hashing for $\ell_1$-Distance Predicates: Applications to Countering Adversarial Input Attacks</h3>
<ul>
<li><strong>Authors: </strong>Hassan Asghar, Chenhan Zhang, Dali Kaafar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IT, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16355">https://arxiv.org/abs/2504.16355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16355">https://arxiv.org/pdf/2504.16355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16355]] Property-Preserving Hashing for $\ell_1$-Distance Predicates: Applications to Countering Adversarial Input Attacks(https://arxiv.org/abs/2504.16355)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Perceptual hashing is used to detect whether an input image is similar to a reference image with a variety of security applications. Recently, they have been shown to succumb to adversarial input attacks which make small imperceptible changes to the input image yet the hashing algorithm does not detect its similarity to the original image. Property-preserving hashing (PPH) is a recent construct in cryptography, which preserves some property (predicate) of its inputs in the hash domain. Researchers have so far shown constructions of PPH for Hamming distance predicates, which, for instance, outputs 1 if two inputs are within Hamming distance $t$. A key feature of PPH is its strong correctness guarantee, i.e., the probability that the predicate will not be correctly evaluated in the hash domain is negligible. Motivated by the use case of detecting similar images under adversarial setting, we propose the first PPH construction for an $\ell_1$-distance predicate. Roughly, this predicate checks if the two one-sided $\ell_1$-distances between two images are within a threshold $t$. Since many adversarial attacks use $\ell_2$-distance (related to $\ell_1$-distance) as the objective function to perturb the input image, by appropriately choosing the threshold $t$, we can force the attacker to add considerable noise to evade detection, and hence significantly deteriorate the image quality. Our proposed scheme is highly efficient, and runs in time $O(t^2)$. For grayscale images of size $28 \times 28$, we can evaluate the predicate in $0.0784$ seconds when pixel values are perturbed by up to $1 \%$. For larger RGB images of size $224 \times 224$, by dividing the image into 1,000 blocks, we achieve times of $0.0128$ seconds per block for $1 \%$ change, and up to $0.2641$ seconds per block for $14\%$ change.</li>
</ul>

<h3>Title: Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions</h3>
<ul>
<li><strong>Authors: </strong>Tian Bai, Huiyan Ying, Kailong Suo, Junqiu Wei, Tao Fan, Yuanfeng Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16358">https://arxiv.org/abs/2504.16358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16358">https://arxiv.org/pdf/2504.16358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16358]] Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions(https://arxiv.org/abs/2504.16358)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces the Text-to-TrajVis task, which aims to transform natural language questions into trajectory data visualizations, facilitating the development of natural language interfaces for trajectory visualization systems. As this is a novel task, there is currently no relevant dataset available in the community. To address this gap, we first devised a new visualization language called Trajectory Visualization Language (TVL) to facilitate querying trajectory data and generating visualizations. Building on this foundation, we further proposed a dataset construction method that integrates Large Language Models (LLMs) with human efforts to create high-quality data. Specifically, we first generate TVLs using a comprehensive and systematic process, and then label each TVL with corresponding natural language questions using LLMs. This process results in the creation of the first large-scale Text-to-TrajVis dataset, named TrajVL, which contains 18,140 (question, TVL) pairs. Based on this dataset, we systematically evaluated the performance of multiple LLMs (GPT, Qwen, Llama, etc.) on this task. The experimental results demonstrate that this task is both feasible and highly challenging and merits further exploration within the research community.</li>
</ul>

<h3>Title: VideoMark: A Distortion-Free Robust Watermarking Framework for Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xuming Hu, Hanqian Li, Jungang Li, Aiwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16359">https://arxiv.org/abs/2504.16359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16359">https://arxiv.org/pdf/2504.16359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16359]] VideoMark: A Distortion-Free Robust Watermarking Framework for Video Diffusion Models(https://arxiv.org/abs/2504.16359)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, extraction, watermark, diffusion</a></li>
<li><strong>Abstract: </strong>This work presents VideoMark, a training-free robust watermarking framework for video diffusion models. As diffusion models advance in generating highly realistic videos, the need for reliable content attribution mechanisms has become critical. While watermarking techniques for image diffusion models have made progress, directly extending these methods to videos presents unique challenges due to variable video lengths and vulnerability to temporal attacks. VideoMark addresses these limitations through a frame-wise watermarking strategy using pseudorandom error correction (PRC) codes to embed watermark information during the generation process. Our method generates an extended watermark message sequence and randomly selects starting positions for each video, ensuring uniform noise distribution in the latent space and maintaining generation quality. For watermark extraction, we introduce a Temporal Matching Module (TMM) that uses edit distance to align decoded messages with the original watermark sequence, providing robustness against temporal attacks such as frame deletion. Experimental results demonstrate that VideoMark achieves higher decoding accuracy than existing methods while maintaining video quality on par with watermark-free generation. Importantly, our watermark remains undetectable to attackers without the secret key, ensuring strong imperceptibility compared to other watermarking frameworks. VideoMark provides a practical solution for content attribution in diffusion-based video generation without requiring additional training or compromising video quality. Our code and data are available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Disentangled Graph Representation Based on Substructure-Aware Graph Optimal Matching Kernel Convolutional Networks</h3>
<ul>
<li><strong>Authors: </strong>Mao Wang, Tao Wu, Xingping Xian, Shaojie Qiao, Weina Niu, Canyixing Cui</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16360">https://arxiv.org/abs/2504.16360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16360">https://arxiv.org/pdf/2504.16360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16360]] Disentangled Graph Representation Based on Substructure-Aware Graph Optimal Matching Kernel Convolutional Networks(https://arxiv.org/abs/2504.16360)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Graphs effectively characterize relational data, driving graph representation learning methods that uncover underlying predictive information. As state-of-the-art approaches, Graph Neural Networks (GNNs) enable end-to-end learning for diverse tasks. Recent disentangled graph representation learning enhances interpretability by decoupling independent factors in graph data. However, existing methods often implicitly and coarsely characterize graph structures, limiting structural pattern analysis within the graph. This paper proposes the Graph Optimal Matching Kernel Convolutional Network (GOMKCN) to address this limitation. We view graphs as node-centric subgraphs, where each subgraph acts as a structural factor encoding position-specific information. This transforms graph prediction into structural pattern recognition. Inspired by CNNs, GOMKCN introduces the Graph Optimal Matching Kernel (GOMK) as a convolutional operator, computing similarities between subgraphs and learnable graph filters. Mathematically, GOMK maps subgraphs and filters into a Hilbert space, representing graphs as point sets. Disentangled representations emerge from projecting subgraphs onto task-optimized filters, which adaptively capture relevant structural patterns via gradient descent. Crucially, GOMK incorporates local correspondences in similarity measurement, resolving the trade-off between differentiability and accuracy in graph kernels. Experiments validate that GOMKCN achieves superior accuracy and interpretability in graph pattern mining and prediction. The framework advances the theoretical foundation for disentangled graph representation learning.</li>
</ul>

<h3>Title: Almost Right: Making First-layer Kernels Nearly Orthogonal Improves Model Generalization</h3>
<ul>
<li><strong>Authors: </strong>Colton R. Crum, Adam Czajka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16362">https://arxiv.org/abs/2504.16362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16362">https://arxiv.org/pdf/2504.16362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16362]] Almost Right: Making First-layer Kernels Nearly Orthogonal Improves Model Generalization(https://arxiv.org/abs/2504.16362)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, biometric</a></li>
<li><strong>Abstract: </strong>An ongoing research challenge within several domains in computer vision is how to increase model generalization capabilities. Several attempts to improve model generalization performance are heavily inspired by human perceptual intelligence, which is remarkable in both its performance and efficiency to generalize to unknown samples. Many of these methods attempt to force portions of the network to be orthogonal, following some observation within neuroscience related to early vision processes. In this paper, we propose a loss component that regularizes the filtering kernels in the first convolutional layer of a network to make them nearly orthogonal. Deviating from previous works, we give the network flexibility in which pairs of kernels it makes orthogonal, allowing the network to navigate to a better solution space, imposing harsh penalties. Without architectural modifications, we report substantial gains in generalization performance using the proposed loss against previous works (including orthogonalization- and saliency-based regularization methods) across three different architectures (ResNet-50, DenseNet-121, ViT-b-16) and two difficult open-set recognition tasks: presentation attack detection in iris biometrics, and anomaly detection in chest X-ray images.</li>
</ul>

<h3>Title: CLPSTNet: A Progressive Multi-Scale Convolutional Steganography Model Integrating Curriculum Learning</h3>
<ul>
<li><strong>Authors: </strong>Fengchun Liu, Tong Zhang, Chunying Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16364">https://arxiv.org/abs/2504.16364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16364">https://arxiv.org/pdf/2504.16364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16364]] CLPSTNet: A Progressive Multi-Scale Convolutional Steganography Model Integrating Curriculum Learning(https://arxiv.org/abs/2504.16364)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction</a></li>
<li><strong>Abstract: </strong>In recent years, a large number of works have introduced Convolutional Neural Networks (CNNs) into image steganography, which transform traditional steganography methods such as hand-crafted features and prior knowledge design into steganography methods that neural networks autonomically learn information embedding. However, due to the inherent complexity of digital images, issues of invisibility and security persist when using CNN models for information embedding. In this paper, we propose Curriculum Learning Progressive Steganophy Network (CLPSTNet). The network consists of multiple progressive multi-scale convolutional modules that integrate Inception structures and dilated convolutions. The module contains multiple branching pathways, starting from a smaller convolutional kernel and dilatation rate, extracting the basic, local feature information from the feature map, and gradually expanding to the convolution with a larger convolutional kernel and dilatation rate for perceiving the feature information of a larger receptive field, so as to realize the multi-scale feature extraction from shallow to deep, and from fine to coarse, allowing the shallow secret information features to be refined in different fusion stages. The experimental results show that the proposed CLPSTNet not only has high PSNR , SSIM metrics and decoding accuracy on three large public datasets, ALASKA2, VOC2012 and ImageNet, but also the steganographic images generated by CLPSTNet have low steganalysis this http URL can find our code at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: SplitReason: Learning To Offload Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yash Akhauri, Anthony Fei, Chi-Chih Chang, Ahmed F. AbouElhamayed, Yueying Li, Mohamed S. Abdelfattah</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16379">https://arxiv.org/abs/2504.16379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16379">https://arxiv.org/pdf/2504.16379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16379]] SplitReason: Learning To Offload Reasoning(https://arxiv.org/abs/2504.16379)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning in large language models (LLMs) tends to produce substantially longer token generation sequences than simpler language modeling tasks. This extended generation length reflects the multi-step, compositional nature of reasoning and is often correlated with higher solution accuracy. From an efficiency perspective, longer token generation exacerbates the inherently sequential and memory-bound decoding phase of LLMs. However, not all parts of this expensive reasoning process are equally difficult to generate. We leverage this observation by offloading only the most challenging parts of the reasoning process to a larger, more capable model, while performing most of the generation with a smaller, more efficient model; furthermore, we teach the smaller model to identify these difficult segments and independently trigger offloading when needed. To enable this behavior, we annotate difficult segments across 18k reasoning traces from the OpenR1-Math-220k chain-of-thought (CoT) dataset. We then apply supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to a 1.5B-parameter reasoning model, training it to learn to offload the most challenging parts of its own reasoning process to a larger model. This approach improves AIME24 reasoning accuracy by 24% and 28.3% while offloading 1.35% and 5% of the generated tokens respectively. We open-source our SplitReason model, data, code and logs.</li>
</ul>

<h3>Title: Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection</h3>
<ul>
<li><strong>Authors: </strong>Md Fahimuzzman Sohan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16404">https://arxiv.org/abs/2504.16404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16404">https://arxiv.org/pdf/2504.16404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16404]] Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection(https://arxiv.org/abs/2504.16404)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Cattle lameness is often caused by hoof injuries or interdigital dermatitis, leads to pain and significantly impacts essential physiological activities such as walking, feeding, and drinking. This study presents a deep learning-based model for detecting cattle lameness, sickness, or gait abnormalities using publicly available video data. The dataset consists of 50 unique videos from 40 individual cattle, recorded from various angles in both indoor and outdoor environments. Half of the dataset represents naturally walking (normal/non-lame) cattle, while the other half consists of cattle exhibiting gait abnormalities (lame). To enhance model robustness and generalizability, data augmentation was applied to the training data. The pre-processed videos were then classified using two deep learning models: ConvLSTM2D and 3D CNN. A comparative analysis of the results demonstrates strong classification performance. Specifically, the 3D CNN model achieved a video-level classification accuracy of 90%, with precision, recall, and f1-score of 90.9%, 90.9%, and 90.91% respectively. The ConvLSTM2D model exhibited a slightly lower accuracy of 85%. This study highlights the effectiveness of directly applying classification models to learn spatiotemporal features from video data, offering an alternative to traditional multi-stage approaches that typically involve object detection, pose estimation, and feature extraction. Besides, the findings demonstrate that the proposed deep learning models, particularly the 3D CNN, effectively classify and detect lameness in cattle while simplifying the processing pipeline.</li>
</ul>

<h3>Title: Out-of-the-Box Conditional Text Embeddings from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kosuke Yamada, Peinan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16411">https://arxiv.org/abs/2504.16411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16411">https://arxiv.org/pdf/2504.16411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16411]] Out-of-the-Box Conditional Text Embeddings from Large Language Models(https://arxiv.org/abs/2504.16411)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Conditional text embedding is a proposed representation that captures the shift in perspective on texts when conditioned on a specific aspect. Previous methods have relied on extensive training data for fine-tuning models, leading to challenges in terms of labor and resource costs. We propose PonTE, a novel unsupervised conditional text embedding method that leverages a causal large language model and a conditional prompt. Through experiments on conditional semantic text similarity and text clustering, we demonstrate that PonTE can generate useful conditional text embeddings and achieve performance comparable to supervised methods without fine-tuning. We also show the interpretability of text embeddings with PonTE by analyzing word generation following prompts and embedding visualization.</li>
</ul>

<h3>Title: Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Khodadad, Ali Shiraee Kasmaee, Mahdi Astaraki, Nicholas Sherck, Hamidreza Mahyar, Soheila Samiee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16414">https://arxiv.org/abs/2504.16414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16414">https://arxiv.org/pdf/2504.16414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16414]] Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study(https://arxiv.org/abs/2504.16414)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a fully automated pipeline, verified by subject matter experts, to facilitate this task. Our approach integrates OpenAI reasoning models with named entity recognition (NER) systems to extract chemical entities from recent literature, which are then augmented with external knowledge bases to form a comprehensive knowledge graph. By generating multi-hop questions across these graphs, we assess LLM performance in both context-augmented and non-context augmented settings. Our experiments reveal that even state-of-the-art models face significant challenges in multi-hop compositional reasoning. The results reflect the importance of augmenting LLMs with document retrieval, which can have a substantial impact on improving their performance. However, even perfect retrieval accuracy with full context does not eliminate reasoning errors, underscoring the complexity of compositional reasoning. This work not only benchmarks and highlights the limitations of current LLMs but also presents a novel data generation pipeline capable of producing challenging reasoning datasets across various domains. Overall, this research advances our understanding of reasoning in computational linguistics.</li>
</ul>

<h3>Title: PixelWeb: The First Web GUI Dataset with Pixel-Wise Labels</h3>
<ul>
<li><strong>Authors: </strong>Qi Yang, Weichen Bi, Haiyang Shen, Yaoqi Guo, Yun Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16419">https://arxiv.org/abs/2504.16419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16419">https://arxiv.org/pdf/2504.16419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16419]] PixelWeb: The First Web GUI Dataset with Pixel-Wise Labels(https://arxiv.org/abs/2504.16419)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Graphical User Interface (GUI) datasets are crucial for various downstream tasks. However, GUI datasets often generate annotation information through automatic labeling, which commonly results in inaccurate GUI element BBox annotations, including missing, duplicate, or meaningless BBoxes. These issues can degrade the performance of models trained on these datasets, limiting their effectiveness in real-world applications. Additionally, existing GUI datasets only provide BBox annotations visually, which restricts the development of visually related GUI downstream tasks. To address these issues, we introduce PixelWeb, a large-scale GUI dataset containing over 100,000 annotated web pages. PixelWeb is constructed using a novel automatic annotation approach that integrates visual feature extraction and Document Object Model (DOM) structure analysis through two core modules: channel derivation and layer analysis. Channel derivation ensures accurate localization of GUI elements in cases of occlusion and overlapping elements by extracting BGRA four-channel bitmap annotations. Layer analysis uses the DOM to determine the visibility and stacking order of elements, providing precise BBox annotations. Additionally, PixelWeb includes comprehensive metadata such as element images, contours, and mask annotations. Manual verification by three independent annotators confirms the high quality and accuracy of PixelWeb annotations. Experimental results on GUI element detection tasks show that PixelWeb achieves performance on the mAP95 metric that is 3-7 times better than existing datasets. We believe that PixelWeb has great potential for performance improvement in downstream tasks such as GUI generation and automated user interaction.</li>
</ul>

<h3>Title: Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Hanlei Zhang, Zhuohang Li, Yeshuang Zhu, Hua Xu, Peiwu Wang, Jinchao Zhang, Jie Zhou, Haige Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16427">https://arxiv.org/abs/2504.16427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16427">https://arxiv.org/pdf/2504.16427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16427]] Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark(https://arxiv.org/abs/2504.16427)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at this https URL.</li>
</ul>

<h3>Title: Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection</h3>
<ul>
<li><strong>Authors: </strong>Bo Lin, Shangwen Wang, Yihao Qin, Liqian Chen, Xiaoguang Mao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16429">https://arxiv.org/abs/2504.16429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16429">https://arxiv.org/pdf/2504.16429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16429]] Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection(https://arxiv.org/abs/2504.16429)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Code Generation (RACG) leverages external knowledge to enhance Large Language Models (LLMs) in code synthesis, improving the functional correctness of the generated code. However, existing RACG systems largely overlook security, leading to substantial risks. Especially, the poisoning of malicious code into knowledge bases can mislead LLMs, resulting in the generation of insecure outputs, which poses a critical threat in modern software development. To address this, we propose a security-hardening framework for RACG systems, CodeGuarder, that shifts the paradigm from retrieving only functional code examples to incorporating both functional code and security knowledge. Our framework constructs a security knowledge base from real-world vulnerability databases, including secure code samples and root cause annotations. For each code generation query, a retriever decomposes the query into fine-grained sub-tasks and fetches relevant security knowledge. To prioritize critical security guidance, we introduce a re-ranking and filtering mechanism by leveraging the LLMs' susceptibility to different vulnerability types. This filtered security knowledge is seamlessly integrated into the generation prompt. Our evaluation shows CodeGuarder significantly improves code security rates across various LLMs, achieving average improvements of 20.12\% in standard RACG, and 31.53\% and 21.91\% under two distinct poisoning scenarios without compromising functional correctness. Furthermore, CodeGuarder demonstrates strong generalization, enhancing security even when the targeted language's security knowledge is lacking. This work presents CodeGuarder as a pivotal advancement towards building secure and trustworthy RACG systems.</li>
</ul>

<h3>Title: Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Ruixiang Zhang, Shuangfei Zhai, Yizhe Zhang, James Thornton, Zijing Ou, Joshua Susskind, Navdeep Jaitly</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16431">https://arxiv.org/abs/2504.16431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16431">https://arxiv.org/pdf/2504.16431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16431]] Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion(https://arxiv.org/abs/2504.16431)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discrete diffusion is a promising framework for modeling and generating discrete data. In this work, we present Target Concrete Score Matching (TCSM), a novel and versatile objective for training and fine-tuning discrete diffusion models. TCSM provides a general framework with broad applicability. It supports pre-training discrete diffusion models directly from data samples, and many existing discrete diffusion approaches naturally emerge as special cases of our more general TCSM framework. Furthermore, the same TCSM objective extends to post-training of discrete diffusion models, including fine-tuning using reward functions or preference data, and distillation of knowledge from pre-trained autoregressive models. These new capabilities stem from the core idea of TCSM, estimating the concrete score of the target distribution, which resides in the original (clean) data space. This allows seamless integration with reward functions and pre-trained models, which inherently only operate in the clean data space rather than the noisy intermediate spaces of diffusion processes. Our experiments on language modeling tasks demonstrate that TCSM matches or surpasses current methods. Additionally, TCSM is versatile, applicable to both pre-training and post-training scenarios, offering greater flexibility and sample efficiency.</li>
</ul>

<h3>Title: iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network</h3>
<ul>
<li><strong>Authors: </strong>Ziran Liang, Rui An, Wenqi Fan, Yanghui Rao, Yuxuan Liang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16432">https://arxiv.org/abs/2504.16432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16432">https://arxiv.org/pdf/2504.16432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16432]] iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network(https://arxiv.org/abs/2504.16432)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>As time evolves, data within specific domains exhibit predictability that motivates time series forecasting to predict future trends from historical data. However, current deep forecasting methods can achieve promising performance but generally lack interpretability, hindering trustworthiness and practical deployment in safety-critical applications such as auto-driving and healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for credible time series forecasting. iTFKAN enables further exploration of model decision rationales and underlying data patterns due to its interpretability achieved through model symbolization. Besides, iTFKAN develops two strategies, prior knowledge injection, and time-frequency synergy learning, to effectively guide model learning under complex intertwined time series data. Extensive experimental results demonstrated that iTFKAN can achieve promising forecasting performance while simultaneously possessing high interpretive capabilities.</li>
</ul>

<h3>Title: FrogDogNet: Fourier frequency Retained visual prompt Output Guidance for Domain Generalization of CLIP in Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Hariseetharam Gunduboina (1), Muhammad Haris Khan (2), Biplab Banerjee (1) ((1) Indian Institute of Technology Bombay, India, (2) Mohamed Bin Zayed University of Artificial Intelligence, UAE)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16433">https://arxiv.org/abs/2504.16433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16433">https://arxiv.org/pdf/2504.16433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16433]] FrogDogNet: Fourier frequency Retained visual prompt Output Guidance for Domain Generalization of CLIP in Remote Sensing(https://arxiv.org/abs/2504.16433)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In recent years, large-scale vision-language models (VLMs) like CLIP have gained attention for their zero-shot inference using instructional text prompts. While these models excel in general computer vision, their potential for domain generalization in remote sensing (RS) remains underexplored. Existing approaches enhance prompt learning by generating visual prompt tokens but rely on full-image features, introducing noise and background artifacts that vary within a class, causing misclassification. To address this, we propose FrogDogNet, a novel prompt learning framework integrating Fourier frequency filtering and self-attention to improve RS scene classification and domain generalization. FrogDogNet selectively retains invariant low-frequency components while eliminating noise and irrelevant backgrounds, ensuring robust feature representation across domains. The model first extracts significant features via projection and self-attention, then applies frequency-based filtering to preserve essential structural information for prompt learning. Extensive experiments on four RS datasets and three domain generalization tasks show that FrogDogNet consistently outperforms state-of-the-art prompt learning methods, demonstrating superior adaptability across domain shifts. Our findings highlight the effectiveness of frequency-based invariant feature retention in generalization, paving the way for broader applications. Our code is available at this https URL</li>
</ul>

<h3>Title: Private Federated Learning using Preference-Optimized Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Charlie Hou, Mei-Yu Wang, Yige Zhu, Daniel Lazar, Giulia Fanti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16438">https://arxiv.org/abs/2504.16438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16438">https://arxiv.org/pdf/2504.16438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16438]] Private Federated Learning using Preference-Optimized Synthetic Data(https://arxiv.org/abs/2504.16438)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>In practical settings, differentially private Federated learning (DP-FL) is the dominant method for training models from private, on-device client data. Recent work has suggested that DP-FL may be enhanced or outperformed by methods that use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary algorithms for generating DP synthetic data for FL applications require careful prompt engineering based on public information and/or iterative private client feedback. Our key insight is that the private client feedback collected by prior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be viewed as a preference ranking. Our algorithm, Preference Optimization for Private Client Data (POPri) harnesses client feedback using preference optimization algorithms such as Direct Preference Optimization (DPO) to fine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri, we release LargeFedBench, a new federated text benchmark for uncontaminated LLM evaluations on federated client data. POPri substantially improves the utility of DP synthetic data relative to prior work on LargeFedBench datasets and an existing benchmark from Xie et al. (2024). POPri closes the gap between next-token prediction accuracy in the fully-private and non-private settings by up to 68%, compared to 52% for prior synthetic data methods, and 10% for state-of-the-art DP federated learning methods. The code and data are available at this https URL.</li>
</ul>

<h3>Title: Marginalized Generalized IoU (MGIoU): A Unified Objective Function for Optimizing Any Convex Parametric Shapes</h3>
<ul>
<li><strong>Authors: </strong>Duy-Tho Le, Trung Pham, Jianfei Cai, Hamid Rezatofighi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16443">https://arxiv.org/abs/2504.16443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16443">https://arxiv.org/pdf/2504.16443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16443]] Marginalized Generalized IoU (MGIoU): A Unified Objective Function for Optimizing Any Convex Parametric Shapes(https://arxiv.org/abs/2504.16443)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Optimizing the similarity between parametric shapes is crucial for numerous computer vision tasks, where Intersection over Union (IoU) stands as the canonical measure. However, existing optimization methods exhibit significant shortcomings: regression-based losses like L1/L2 lack correlation with IoU, IoU-based losses are unstable and limited to simple shapes, and task-specific methods are computationally intensive and not generalizable accross domains. As a result, the current landscape of parametric shape objective functions has become scattered, with each domain proposing distinct IoU approximations. To address this, we unify the parametric shape optimization objective functions by introducing Marginalized Generalized IoU (MGIoU), a novel loss function that overcomes these challenges by projecting structured convex shapes onto their unique shape Normals to compute one-dimensional normalized GIoU. MGIoU offers a simple, efficient, fully differentiable approximation strongly correlated with IoU. We then extend MGIoU to MGIoU+ that supports optimizing unstructured convex shapes. Together, MGIoU and MGIoU+ unify parametric shape optimization across diverse applications. Experiments on standard benchmarks demonstrate that MGIoU and MGIoU+ consistently outperform existing losses while reducing loss computation latency by 10-40x. Additionally, MGIoU and MGIoU+ satisfy metric properties and scale-invariance, ensuring robustness as an objective function. We further propose MGIoU- for minimizing overlaps in tasks like collision-free trajectory prediction. Code is available at this https URL</li>
</ul>

<h3>Title: EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records</h3>
<ul>
<li><strong>Authors: </strong>Shuguang Zhao, Qiangzhong Feng, Zhiyang He, Peipei Sun, Yingying Wang, Xiaodong Tao, Xiaoliang Lu, Mei Cheng, Xinyue Wu, Yanyan Wang, Wei Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16448">https://arxiv.org/abs/2504.16448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16448">https://arxiv.org/pdf/2504.16448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16448]] EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records(https://arxiv.org/abs/2504.16448)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Medical consultation dialogues contain critical clinical information, yet their unstructured nature hinders effective utilization in diagnosis and treatment. Traditional methods, relying on rule-based or shallow machine learning techniques, struggle to capture deep and implicit semantics. Recently, large pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight fine-tuning method, have shown promise for structured information extraction. We propose EMRModel, a novel approach that integrates LoRA-based fine-tuning with code-style prompt design, aiming to efficiently convert medical consultation dialogues into structured electronic medical records (EMRs). Additionally, we construct a high-quality, realistically grounded dataset of medical consultation dialogues with detailed annotations. Furthermore, we introduce a fine-grained evaluation benchmark for medical consultation information extraction and provide a systematic evaluation methodology, advancing the optimization of medical natural language processing (NLP) models. Experimental results show EMRModel achieves an F1 score of 88.1%, improving by49.5% over standard pre-trained models. Compared to traditional LoRA fine-tuning methods, our model shows superior performance, highlighting its effectiveness in structured medical record extraction tasks.</li>
</ul>

<h3>Title: From Past to Present: A Survey of Malicious URL Detection Techniques, Datasets and Code Repositories</h3>
<ul>
<li><strong>Authors: </strong>Ye Tian, Yanqiu Yu, Jianguo Sun, Yanbin Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16449">https://arxiv.org/abs/2504.16449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16449">https://arxiv.org/pdf/2504.16449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16449]] From Past to Present: A Survey of Malicious URL Detection Techniques, Datasets and Code Repositories(https://arxiv.org/abs/2504.16449)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, transformer</a></li>
<li><strong>Abstract: </strong>Malicious URLs persistently threaten the cybersecurity ecosystem, by either deceiving users into divulging private data or distributing harmful payloads to infiltrate host systems. Gaining timely insights into the current state of this ongoing battle holds significant importance. However, existing reviews exhibit 4 critical gaps: 1) Their reliance on algorithm-centric taxonomies obscures understanding of how detection approaches exploit specific modal information channels; 2) They fail to incorporate pivotal LLM/Transformer-based defenses; 3) No open-source implementations are collected to facilitate benchmarking; 4) Insufficient dataset this http URL paper presents a comprehensive review of malicious URL detection technologies, systematically analyzing methods from traditional blacklisting to advanced deep learning approaches (e.g. Transformer, GNNs, and LLMs). Unlike prior surveys, we propose a novel modality-based taxonomy that categorizes existing works according to their primary data modalities (URL, HTML, Visual, etc.). This hierarchical classification enables both rigorous technical analysis and clear understanding of multimodal information utilization. Furthermore, to establish a profile of accessible datasets and address the lack of standardized benchmarking (where current studies often lack proper baseline comparisons), we curate and analyze: 1) publicly available datasets (2016-2024), and 2) open-source implementations from published works(2013-2025). Then, we outline essential design principles and architectural frameworks for product-level implementations. The review concludes by examining emerging challenges and proposing actionable directions for future research. We maintain a GitHub repository for ongoing curating datasets and open-source implementations: this https URL.</li>
</ul>

<h3>Title: Cross Paradigm Representation and Alignment Transformer for Image Deraining</h3>
<ul>
<li><strong>Authors: </strong>Shun Zou, Yi Zou, Juncheng Li, Guangwei Gao, Guojun Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16455">https://arxiv.org/abs/2504.16455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16455">https://arxiv.org/pdf/2504.16455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16455]] Cross Paradigm Representation and Alignment Transformer for Image Deraining(https://arxiv.org/abs/2504.16455)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based networks have achieved strong performance in low-level vision tasks like image deraining by utilizing spatial or channel-wise self-attention. However, irregular rain patterns and complex geometric overlaps challenge single-paradigm architectures, necessitating a unified framework to integrate complementary global-local and spatial-channel representations. To address this, we propose a novel Cross Paradigm Representation and Alignment Transformer (CPRAformer). Its core idea is the hierarchical representation and alignment, leveraging the strengths of both paradigms (spatial-channel and global-local) to aid image reconstruction. It bridges the gap within and between paradigms, aligning and coordinating them to enable deep interaction and fusion of features. Specifically, we use two types of self-attention in the Transformer blocks: sparse prompt channel self-attention (SPC-SA) and spatial pixel refinement self-attention (SPR-SA). SPC-SA enhances global channel dependencies through dynamic sparsity, while SPR-SA focuses on spatial rain distribution and fine-grained texture recovery. To address the feature misalignment and knowledge differences between them, we introduce the Adaptive Alignment Frequency Module (AAFM), which aligns and interacts with features in a two-stage progressive manner, enabling adaptive guidance and complementarity. This reduces the information gap within and between paradigms. Through this unified cross-paradigm dynamic interaction framework, we achieve the extraction of the most valuable interactive fusion information from the two paradigms. Extensive experiments demonstrate that our model achieves state-of-the-art performance on eight benchmark datasets and further validates CPRAformer's robustness in other image restoration tasks and downstream applications.</li>
</ul>

<h3>Title: MTSGL: Multi-Task Structure Guided Learning for Robust and Interpretable SAR Aircraft Recognition</h3>
<ul>
<li><strong>Authors: </strong>Qishan He, Lingjun Zhao, Ru Luo, Siqian Zhang, Lin Lei, Kefeng Ji, Gangyao Kuang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16467">https://arxiv.org/abs/2504.16467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16467">https://arxiv.org/pdf/2504.16467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16467]] MTSGL: Multi-Task Structure Guided Learning for Robust and Interpretable SAR Aircraft Recognition(https://arxiv.org/abs/2504.16467)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Aircraft recognition in synthetic aperture radar (SAR) imagery is a fundamental mission in both military and civilian applications. Recently deep learning (DL) has emerged a dominant paradigm for its explosive performance on extracting discriminative features. However, current classification algorithms focus primarily on learning decision hyperplane without enough comprehension on aircraft structural knowledge. Inspired by the fined aircraft annotation methods for optical remote sensing images (RSI), we first introduce a structure-based SAR aircraft annotations approach to provide structural and compositional supplement information. On this basis, we propose a multi-task structure guided learning (MTSGL) network for robust and interpretable SAR aircraft recognition. Besides the classification task, MTSGL includes a structural semantic awareness (SSA) module and a structural consistency regularization (SCR) module. The SSA is designed to capture structure semantic information, which is conducive to gain human-like comprehension of aircraft knowledge. The SCR helps maintain the geometric consistency between the aircraft structure in SAR imagery and the proposed annotation. In this process, the structural attribute can be disentangled in a geometrically meaningful manner. In conclusion, the MTSGL is presented with the expert-level aircraft prior knowledge and structure guided learning paradigm, aiming to comprehend the aircraft concept in a way analogous to the human cognitive process. Extensive experiments are conducted on a self-constructed multi-task SAR aircraft recognition dataset (MT-SARD) and the effective results illustrate the superiority of robustness and interpretation ability of the proposed MTSGL.</li>
</ul>

<h3>Title: RGB-D Video Object Segmentation via Enhanced Multi-store Feature Memory</h3>
<ul>
<li><strong>Authors: </strong>Boyue Xu, Ruichao Hou, Tongwei Ren, Gangshan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16471">https://arxiv.org/abs/2504.16471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16471">https://arxiv.org/pdf/2504.16471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16471]] RGB-D Video Object Segmentation via Enhanced Multi-store Feature Memory(https://arxiv.org/abs/2504.16471)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>The RGB-Depth (RGB-D) Video Object Segmentation (VOS) aims to integrate the fine-grained texture information of RGB with the spatial geometric clues of depth modality, boosting the performance of segmentation. However, off-the-shelf RGB-D segmentation methods fail to fully explore cross-modal information and suffer from object drift during long-term prediction. In this paper, we propose a novel RGB-D VOS method via multi-store feature memory for robust segmentation. Specifically, we design the hierarchical modality selection and fusion, which adaptively combines features from both modalities. Additionally, we develop a segmentation refinement module that effectively utilizes the Segmentation Anything Model (SAM) to refine the segmentation mask, ensuring more reliable results as memory to guide subsequent segmentation tasks. By leveraging spatio-temporal embedding and modality embedding, mixed prompts and fused images are fed into SAM to unleash its potential in RGB-D VOS. Experimental results show that the proposed method achieves state-of-the-art performance on the latest RGB-D VOS benchmark.</li>
</ul>

<h3>Title: Seeking Flat Minima over Diverse Surrogates for Improved Adversarial Transferability: A Theoretical Framework and Algorithmic Instantiation</h3>
<ul>
<li><strong>Authors: </strong>Meixi Zheng, Kehan Wu, Yanbo Fan, Rui Huang, Baoyuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16474">https://arxiv.org/abs/2504.16474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16474">https://arxiv.org/pdf/2504.16474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16474]] Seeking Flat Minima over Diverse Surrogates for Improved Adversarial Transferability: A Theoretical Framework and Algorithmic Instantiation(https://arxiv.org/abs/2504.16474)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>The transfer-based black-box adversarial attack setting poses the challenge of crafting an adversarial example (AE) on known surrogate models that remain effective against unseen target models. Due to the practical importance of this task, numerous methods have been proposed to address this challenge. However, most previous methods are heuristically designed and intuitively justified, lacking a theoretical foundation. To bridge this gap, we derive a novel transferability bound that offers provable guarantees for adversarial transferability. Our theoretical analysis has the advantages of \textit{(i)} deepening our understanding of previous methods by building a general attack framework and \textit{(ii)} providing guidance for designing an effective attack algorithm. Our theoretical results demonstrate that optimizing AEs toward flat minima over the surrogate model set, while controlling the surrogate-target model shift measured by the adversarial model discrepancy, yields a comprehensive guarantee for AE transferability. The results further lead to a general transfer-based attack framework, within which we observe that previous methods consider only partial factors contributing to the transferability. Algorithmically, inspired by our theoretical results, we first elaborately construct the surrogate model set in which models exhibit diverse adversarial vulnerabilities with respect to AEs to narrow an instantiated adversarial model discrepancy. Then, a \textit{model-Diversity-compatible Reverse Adversarial Perturbation} (DRAP) is generated to effectively promote the flatness of AEs over diverse surrogate models to improve transferability. Extensive experiments on NIPS2017 and CIFAR-10 datasets against various target models demonstrate the effectiveness of our proposed attack.</li>
</ul>

<h3>Title: Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate</h3>
<ul>
<li><strong>Authors: </strong>Senmao Qi, Yifei Zou, Peng Li, Ziyi Lin, Xiuzhen Cheng, Dongxiao Yu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16489">https://arxiv.org/abs/2504.16489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16489">https://arxiv.org/pdf/2504.16489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16489]] Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate(https://arxiv.org/abs/2504.16489)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Multi-Agent Debate (MAD), leveraging collaborative interactions among Large Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks. However, the security implications of their iterative dialogues and role-playing characteristics, particularly susceptibility to jailbreak attacks eliciting harmful content, remain critically underexplored. This paper systematically investigates the jailbreak vulnerabilities of four prominent MAD frameworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo, and DeepSeek) without compromising internal agents. We introduce a novel structured prompt-rewriting framework specifically designed to exploit MAD dynamics via narrative encapsulation, role-driven escalation, iterative refinement, and rhetorical obfuscation. Our extensive experiments demonstrate that MAD systems are inherently more vulnerable than single-agent setups. Crucially, our proposed attack methodology significantly amplifies this fragility, increasing average harmfulness from 28.14% to 80.34% and achieving attack success rates as high as 80% in certain scenarios. These findings reveal intrinsic vulnerabilities in MAD architectures and underscore the urgent need for robust, specialized defenses prior to real-world deployment.</li>
</ul>

<h3>Title: TraveLLaMA: Facilitating Multi-modal Large Language Models to Understand Urban Scenes and Provide Travel Assistance</h3>
<ul>
<li><strong>Authors: </strong>Meng Chu, Yukang Chen, Haokun Gui, Shaozuo Yu, Yi Wang, Jiaya Jia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16505">https://arxiv.org/abs/2504.16505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16505">https://arxiv.org/pdf/2504.16505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16505]] TraveLLaMA: Facilitating Multi-modal Large Language Models to Understand Urban Scenes and Provide Travel Assistance(https://arxiv.org/abs/2504.16505)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tourism and travel planning increasingly rely on digital assistance, yet existing multimodal AI systems often lack specialized knowledge and contextual understanding of urban environments. We present TraveLLaMA, a specialized multimodal language model designed for urban scene understanding and travel assistance. Our work addresses the fundamental challenge of developing practical AI travel assistants through a novel large-scale dataset of 220k question-answer pairs. This comprehensive dataset uniquely combines 130k text QA pairs meticulously curated from authentic travel forums with GPT-enhanced responses, alongside 90k vision-language QA pairs specifically focused on map understanding and scene comprehension. Through extensive fine-tuning experiments on state-of-the-art vision-language models (LLaVA, Qwen-VL, Shikra), we demonstrate significant performance improvements ranging from 6.5\%-9.4\% in both pure text travel understanding and visual question answering tasks. Our model exhibits exceptional capabilities in providing contextual travel recommendations, interpreting map locations, and understanding place-specific imagery while offering practical information such as operating hours and visitor reviews. Comparative evaluations show TraveLLaMA significantly outperforms general-purpose models in travel-specific tasks, establishing a new benchmark for multi-modal travel assistance systems.</li>
</ul>

<h3>Title: A Comprehensive Survey of Synthetic Tabular Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruxue Shi, Yili Wang, Mengnan Du, Xu Shen, Xin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16506">https://arxiv.org/abs/2504.16506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16506">https://arxiv.org/pdf/2504.16506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16506]] A Comprehensive Survey of Synthetic Tabular Data Generation(https://arxiv.org/abs/2504.16506)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Tabular data remains one of the most prevalent and critical data formats across diverse real-world applications. However, its effective use in machine learning (ML) is often constrained by challenges such as data scarcity, privacy concerns, and class imbalance. Synthetic data generation has emerged as a promising solution, leveraging generative models to learn the distribution of real datasets and produce high-fidelity, privacy-preserving samples. Various generative paradigms have been explored, including energy-based models (EBMs), variational autoencoders (VAEs), generative adversarial networks (GANs), large language models (LLMs), and diffusion models. While several surveys have investigated synthetic tabular data generation, most focus on narrow subdomains or specific generative methods, such as GANs, diffusion models, or privacy-preserving techniques. This limited scope often results in fragmented insights, lacking a comprehensive synthesis that bridges diverse approaches. In particular, recent advances driven by LLMs and diffusion-based models remain underexplored. This gap hinders a holistic understanding of the field`s evolution, methodological interplay, and open challenges. To address this, our survey provides a unified and systematic review of synthetic tabular data generation. Our contributions are threefold: (1) we propose a comprehensive taxonomy that organizes existing methods into traditional approaches, diffusion-based methods, and LLM-based models, and provide an in-depth comparative analysis; (2) we detail the complete pipeline for synthetic tabular data generation, including data synthesis, post-processing, and evaluation; (3) we identify major challenges, explore real-world applications, and outline open research questions and future directions to guide future work in this rapidly evolving area.</li>
</ul>

<h3>Title: QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Fengze Liu, Weidong Zhou, Binbin Liu, Zhimiao Yu, Yifan Zhang, Haobin Lin, Yifeng Yu, Xiaohuan Zhou, Taifeng Wang, Yong Cao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16511">https://arxiv.org/abs/2504.16511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16511">https://arxiv.org/pdf/2504.16511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16511]] QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining(https://arxiv.org/abs/2504.16511)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Quality and diversity are two critical metrics for the training data of large language models (LLMs), positively impacting performance. Existing studies often optimize these metrics separately, typically by first applying quality filtering and then adjusting data proportions. However, these approaches overlook the inherent trade-off between quality and diversity, necessitating their joint consideration. Given a fixed training quota, it is essential to evaluate both the quality of each data point and its complementary effect on the overall dataset. In this paper, we introduce a unified data selection framework called QuaDMix, which automatically optimizes the data distribution for LLM pretraining while balancing both quality and diversity. Specifically, we first propose multiple criteria to measure data quality and employ domain classification to distinguish data points, thereby measuring overall diversity. QuaDMix then employs a unified parameterized data sampling function that determines the sampling probability of each data point based on these quality and diversity related labels. To accelerate the search for the optimal parameters involved in the QuaDMix framework, we conduct simulated experiments on smaller models and use LightGBM for parameters searching, inspired by the RegMix method. Our experiments across diverse models and datasets demonstrate that QuaDMix achieves an average performance improvement of 7.2% across multiple benchmarks. These results outperform the independent strategies for quality and diversity, highlighting the necessity and ability to balance data quality and diversity.</li>
</ul>

<h3>Title: Federated Learning of Low-Rank One-Shot Image Detection Models in Edge Devices with Scalable Accuracy and Compute Complexity</h3>
<ul>
<li><strong>Authors: </strong>Abdul Hannaan, Zubair Shah, Aiman Erbad, Amr Mohamed, Ali Safa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16515">https://arxiv.org/abs/2504.16515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16515">https://arxiv.org/pdf/2504.16515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16515]] Federated Learning of Low-Rank One-Shot Image Detection Models in Edge Devices with Scalable Accuracy and Compute Complexity(https://arxiv.org/abs/2504.16515)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel federated learning framework termed LoRa-FL designed for training low-rank one-shot image detection models deployed on edge devices. By incorporating low-rank adaptation techniques into one-shot detection architectures, our method significantly reduces both computational and communication overhead while maintaining scalable accuracy. The proposed framework leverages federated learning to collaboratively train lightweight image recognition models, enabling rapid adaptation and efficient deployment across heterogeneous, resource-constrained devices. Experimental evaluations on the MNIST and CIFAR10 benchmark datasets, both in an independent-and-identically-distributed (IID) and non-IID setting, demonstrate that our approach achieves competitive detection performance while significantly reducing communication bandwidth and compute complexity. This makes it a promising solution for adaptively reducing the communication and compute power overheads, while not sacrificing model accuracy.</li>
</ul>

<h3>Title: A Few-Shot Metric Learning Method with Dual-Channel Attention for Cross-Modal Same-Neuron Identification</h3>
<ul>
<li><strong>Authors: </strong>Wenwei Li, Liyi Cai, Wu Chen, Anan Li</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16520">https://arxiv.org/abs/2504.16520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16520">https://arxiv.org/pdf/2504.16520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16520]] A Few-Shot Metric Learning Method with Dual-Channel Attention for Cross-Modal Same-Neuron Identification(https://arxiv.org/abs/2504.16520)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>In neuroscience research, achieving single-neuron matching across different imaging modalities is critical for understanding the relationship between neuronal structure and function. However, modality gaps and limited annotations present significant challenges. We propose a few-shot metric learning method with a dual-channel attention mechanism and a pretrained vision transformer to enable robust cross-modal neuron identification. The local and global channels extract soma morphology and fiber context, respectively, and a gating mechanism fuses their outputs. To enhance the model's fine-grained discrimination capability, we introduce a hard sample mining strategy based on the MultiSimilarityMiner algorithm, along with the Circle Loss function. Experiments on two-photon and fMOST datasets demonstrate superior Top-K accuracy and recall compared to existing methods. Ablation studies and t-SNE visualizations validate the effectiveness of each module. The method also achieves a favorable trade-off between accuracy and training efficiency under different fine-tuning strategies. These results suggest that the proposed approach offers a promising technical solution for accurate single-cell level matching and multimodal neuroimaging integration.</li>
</ul>

<h3>Title: Transformers for Complex Query Answering over Knowledge Hypergraphs</h3>
<ul>
<li><strong>Authors: </strong>Hong Ting Tsang, Zihao Wang, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16537">https://arxiv.org/abs/2504.16537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16537">https://arxiv.org/pdf/2504.16537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16537]] Transformers for Complex Query Answering over Knowledge Hypergraphs(https://arxiv.org/abs/2504.16537)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Complex Query Answering (CQA) has been extensively studied in recent years. In order to model data that is closer to real-world distribution, knowledge graphs with different modalities have been introduced. Triple KGs, as the classic KGs composed of entities and relations of arity 2, have limited representation of real-world facts. Real-world data is more sophisticated. While hyper-relational graphs have been introduced, there are limitations in representing relationships of varying arity that contain entities with equal contributions. To address this gap, we sampled new CQA datasets: JF17k-HCQA and M-FB15k-HCQA. Each dataset contains various query types that include logical operations such as projection, negation, conjunction, and disjunction. In order to answer knowledge hypergraph (KHG) existential first-order queries, we propose a two-stage transformer model, the Logical Knowledge Hypergraph Transformer (LKHGT), which consists of a Projection Encoder for atomic projection and a Logical Encoder for complex logical operations. Both encoders are equipped with Type Aware Bias (TAB) for capturing token interactions. Experimental results on CQA datasets show that LKHGT is a state-of-the-art CQA method over KHG and is able to generalize to out-of-distribution query types.</li>
</ul>

<h3>Title: Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment and Mapping of Urban Scenes</h3>
<ul>
<li><strong>Authors: </strong>Joan Perez (1), Giovanni Fusco (2) ((1) Urban Geo Analytics, France (2) Universite Cote-Azur-CNRS-AMU-Avignon Universite, ESPACE, France)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16538">https://arxiv.org/abs/2504.16538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16538">https://arxiv.org/pdf/2504.16538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16538]] Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment and Mapping of Urban Scenes(https://arxiv.org/abs/2504.16538)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Streetscapes are an essential component of urban space. Their assessment is presently either limited to morphometric properties of their mass skeleton or requires labor-intensive qualitative evaluations of visually perceived qualities. This paper introduces SAGAI: Streetscape Analysis with Generative Artificial Intelligence, a modular workflow for scoring street-level urban scenes using open-access data and vision-language models. SAGAI integrates OpenStreetMap geometries, Google Street View imagery, and a lightweight version of the LLaVA model to generate structured spatial indicators from images via customizable natural language prompts. The pipeline includes an automated mapping module that aggregates visual scores at both the point and street levels, enabling direct cartographic interpretation. It operates without task-specific training or proprietary software dependencies, supporting scalable and interpretable analysis of urban environments. Two exploratory case studies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial outputs from vision-language inference. The initial results show strong performance for binary urban-rural scene classification, moderate precision in commercial feature detection, and lower estimates, but still informative, of sidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a wide range of urban research themes, such as walkability, safety, or urban design, through prompt modification alone.</li>
</ul>

<h3>Title: A Collaborative Intrusion Detection System Using Snort IDS Nodes</h3>
<ul>
<li><strong>Authors: </strong>Tom Davies, Max Hashem Eiza, Nathan Shone, Rob Lyon</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16550">https://arxiv.org/abs/2504.16550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16550">https://arxiv.org/pdf/2504.16550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16550]] A Collaborative Intrusion Detection System Using Snort IDS Nodes(https://arxiv.org/abs/2504.16550)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Intrusion Detection Systems (IDSs) are integral to safeguarding networks by detecting and responding to threats from malicious traffic or compromised devices. However, standalone IDS deployments often fall short when addressing the increasing complexity and scale of modern cyberattacks. This paper proposes a Collaborative Intrusion Detection System (CIDS) that leverages Snort, an open-source network intrusion detection system, to enhance detection accuracy and reduce false positives. The proposed architecture connects multiple Snort IDS nodes to a centralised node and integrates with a Security Information and Event Management (SIEM) platform to facilitate real-time data sharing, correlation, and analysis. The CIDS design includes a scalable configuration of Snort sensors, a centralised database for log storage, and LogScale SIEM for advanced analytics and visualisation. By aggregating and analysing intrusion data from multiple nodes, the system enables improved detection of distributed and sophisticated attack patterns that standalone IDSs may miss. Performance evaluation against simulated attacks, including Nmap port scans and ICMP flood attacks, demonstrates our CIDS's ability to efficiently process large-scale network traffic, detect threats with higher accuracy, and reduce alert fatigue. This paper highlights the potential of CIDS in modern network environments and explores future enhancements, such as integrating machine learning for advanced threat detection and creating public datasets to support collaborative research. The proposed CIDS framework provides a promising foundation for building more resilient and adaptive network security systems.</li>
</ul>

<h3>Title: Beyond Anonymization: Object Scrubbing for Privacy-Preserving 2D and 3D Vision Tasks</h3>
<ul>
<li><strong>Authors: </strong>Murat Bilgehan Ertan, Ronak Sahu, Phuong Ha Nguyen, Kaleel Mahmood, Marten van Dijk</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16557">https://arxiv.org/abs/2504.16557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16557">https://arxiv.org/pdf/2504.16557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16557]] Beyond Anonymization: Object Scrubbing for Privacy-Preserving 2D and 3D Vision Tasks(https://arxiv.org/abs/2504.16557)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, generative, segmentation</a></li>
<li><strong>Abstract: </strong>We introduce ROAR (Robust Object Removal and Re-annotation), a scalable framework for privacy-preserving dataset obfuscation that eliminates sensitive objects instead of modifying them. Our method integrates instance segmentation with generative inpainting to remove identifiable entities while preserving scene integrity. Extensive evaluations on 2D COCO-based object detection show that ROAR achieves 87.5% of the baseline detection average precision (AP), whereas image dropping achieves only 74.2% of the baseline AP, highlighting the advantage of scrubbing in preserving dataset utility. The degradation is even more severe for small objects due to occlusion and loss of fine-grained details. Furthermore, in NeRF-based 3D reconstruction, our method incurs a PSNR loss of at most 1.66 dB while maintaining SSIM and improving LPIPS, demonstrating superior perceptual quality. Our findings establish object removal as an effective privacy framework, achieving strong privacy guarantees with minimal performance trade-offs. The results highlight key challenges in generative inpainting, occlusion-robust segmentation, and task-specific scrubbing, setting the foundation for future advancements in privacy-preserving vision systems.</li>
</ul>

<h3>Title: Unified Molecule Generation and Property Prediction</h3>
<ul>
<li><strong>Authors: </strong>Adam Izdebski, Jan Olszewski, Pankhil Gawade, Krzysztof Koras, Serra Korkmaz, Valentin Rauscher, Jakub M. Tomczak, Ewa Szczurek</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16559">https://arxiv.org/abs/2504.16559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16559">https://arxiv.org/pdf/2504.16559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16559]] Unified Molecule Generation and Property Prediction(https://arxiv.org/abs/2504.16559)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Modeling the joint distribution of the data samples and their properties allows to construct a single model for both data generation and property prediction, with synergistic capabilities reaching beyond purely generative or predictive models. However, training joint models presents daunting architectural and optimization challenges. Here, we propose Hyformer, a transformer-based joint model that successfully blends the generative and predictive functionalities, using an alternating attention mask together with a unified pre-training scheme. We show that Hyformer rivals other joint models, as well as state-of-the-art molecule generation and property prediction models. Additionally, we show the benefits of joint modeling in downstream tasks of molecular representation learning, hit identification and antimicrobial peptide design.</li>
</ul>

<h3>Title: SAIP-Net: Enhancing Remote Sensing Image Segmentation via Spectral Adaptive Information Propagation</h3>
<ul>
<li><strong>Authors: </strong>Zhongtao Wang, Xizhe Cao, Yisong Chen, Guoping Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16564">https://arxiv.org/abs/2504.16564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16564">https://arxiv.org/pdf/2504.16564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16564]] SAIP-Net: Enhancing Remote Sensing Image Segmentation via Spectral Adaptive Information Propagation(https://arxiv.org/abs/2504.16564)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation of remote sensing imagery demands precise spatial boundaries and robust intra-class consistency, challenging conventional hierarchical models. To address limitations arising from spatial domain feature fusion and insufficient receptive fields, this paper introduces SAIP-Net, a novel frequency-aware segmentation framework that leverages Spectral Adaptive Information Propagation. SAIP-Net employs adaptive frequency filtering and multi-scale receptive field enhancement to effectively suppress intra-class feature inconsistencies and sharpen boundary lines. Comprehensive experiments demonstrate significant performance improvements over state-of-the-art methods, highlighting the effectiveness of spectral-adaptive strategies combined with expanded receptive fields for remote sensing image segmentation.</li>
</ul>

<h3>Title: LaSDVS : A Post-Quantum Secure Compact Strong-Designated Verifier Signature</h3>
<ul>
<li><strong>Authors: </strong>Shanu Poddar, Sweta Mishra, Tapaswini Mohanty, Vikas Srivastava, Sugata Gangopadhyay</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16571">https://arxiv.org/abs/2504.16571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16571">https://arxiv.org/pdf/2504.16571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16571]] LaSDVS : A Post-Quantum Secure Compact Strong-Designated Verifier Signature(https://arxiv.org/abs/2504.16571)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Digital signatures are fundamental cryptographic primitives that ensure the authenticity and integrity of digital communication. However, in scenarios involving sensitive interactions -- such as e-voting or e-cash -- there is a growing need for more controlled signing mechanisms. Strong-Designated Verifier Signature (SDVS) offers such control by allowing the signer to specify and restrict the verifier of a signature. The existing state-of-the-art SDVS are mostly based on number-theoretic hardness assumptions. Thus, they are not secure against quantum attacks. Moreover, Post-Quantum Cryptography (PQC)-based SDVS are inefficient and have large key and signature sizes. In this work, we address these challenges and propose an efficient post-quantum SDVS (namely, LaSDVS) based on ideal lattices under the hardness assumptions of the Ring-SIS and Ring-LWE problems. LaSDVS achieves advanced security properties including strong unforgeability under chosen-message attacks, non-transferability, non-delegatability, and signer anonymity. By employing the algebraic structure of rings and the gadget trapdoor mechanism of Micciancio et al., we design LaSDVS to minimize computational overhead and significantly reduce key and signature sizes. Notably, our scheme achieves a compact signature size of $\mathcal{O}(n\log q)$, compared to $\mathcal{O}(n^2)$ size, where $n$ is the security parameter, in the existing state-of-the-art PQC designs. To the best of our knowledge, LaSDVS offers the \textit{smallest private key and signature size} among the existing PQC-based SDVS schemes.</li>
</ul>

<h3>Title: PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression</h3>
<ul>
<li><strong>Authors: </strong>Lizhe Chen, Binjia Zhou, Yuyao Ge, Jiayi Chen, Shiguang NI</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16574">https://arxiv.org/abs/2504.16574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16574">https://arxiv.org/pdf/2504.16574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16574]] PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression(https://arxiv.org/abs/2504.16574)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable progress, demonstrating unprecedented capabilities across various natural language processing tasks. However, the high costs associated with such exceptional performance limit the widespread adoption of LLMs, highlighting the need for prompt compression. Existing prompt compression methods primarily rely on heuristic truncation or abstractive summarization techniques, which fundamentally overlook the intrinsic mechanisms of LLMs and lack a systematic evaluation of token importance for generation. In this work, we introduce Prompt Importance Sampling (PIS), a novel compression framework that dynamically compresses prompts by sampling important tokens based on the analysis of attention scores of hidden states. PIS employs a dual-level compression mechanism: 1) at the token level, we quantify saliency using LLM-native attention scores and implement adaptive compression through a lightweight 9-layer reinforcement learning (RL) network; 2) at the semantic level, we propose a Russian roulette sampling strategy for sentence-level importance sampling. Comprehensive evaluations across multiple domain benchmarks demonstrate that our method achieves state-of-the-art compression performance. Notably, our framework serendipitously enhances reasoning efficiency through optimized context structuring. This work advances prompt engineering by offering both theoretical grounding and practical efficiency in context management for LLMs.</li>
</ul>

<h3>Title: Hyper-Transforming Latent Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ignacio Peis, Batuhan Koyuncu, Isabel Valera, Jes Frellsen</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16580">https://arxiv.org/abs/2504.16580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16580">https://arxiv.org/pdf/2504.16580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16580]] Hyper-Transforming Latent Diffusion Models(https://arxiv.org/abs/2504.16580)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>We introduce a novel generative framework for functions by integrating Implicit Neural Representations (INRs) and Transformer-based hypernetworks into latent variable models. Unlike prior approaches that rely on MLP-based hypernetworks with scalability limitations, our method employs a Transformer-based decoder to generate INR parameters from latent variables, addressing both representation capacity and computational efficiency. Our framework extends latent diffusion models (LDMs) to INR generation by replacing standard decoders with a Transformer-based hypernetwork, which can be trained either from scratch or via hyper-transforming-a strategy that fine-tunes only the decoder while freezing the pre-trained latent space. This enables efficient adaptation of existing generative models to INR-based representations without requiring full retraining.</li>
</ul>

<h3>Title: Case Study: Fine-tuning Small Language Models for Accurate and Private CWE Detection in Python Code</h3>
<ul>
<li><strong>Authors: </strong>Md. Azizul Hakim Bappy (Institute of Information and Communication Technology, Bangladesh University of Engineering Technology, Dhaka, Bangladesh), Hossen A Mustafa (Institute of Information and Communication Technology, Bangladesh University of Engineering Technology, Dhaka, Bangladesh), Prottoy Saha (Institute of Information and Communication Technology, Bangladesh University of Engineering Technology, Dhaka, Bangladesh), Rajinus Salehat (Hajee Mohammad Danesh Science and Technology University, Dinajpur, Bangladesh)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16584">https://arxiv.org/abs/2504.16584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16584">https://arxiv.org/pdf/2504.16584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16584]] Case Study: Fine-tuning Small Language Models for Accurate and Private CWE Detection in Python Code(https://arxiv.org/abs/2504.16584)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated significant capabilities in understanding and analyzing code for security vulnerabilities, such as Common Weakness Enumerations (CWEs). However, their reliance on cloud infrastructure and substantial computational requirements pose challenges for analyzing sensitive or proprietary codebases due to privacy concerns and inference costs. This work explores the potential of Small Language Models (SLMs) as a viable alternative for accurate, on-premise vulnerability detection. We investigated whether a 350-million parameter pre-trained code model (codegen-mono) could be effectively fine-tuned to detect the MITRE Top 25 CWEs specifically within Python code. To facilitate this, we developed a targeted dataset of 500 examples using a semi-supervised approach involving LLM-driven synthetic data generation coupled with meticulous human review. Initial tests confirmed that the base codegen-mono model completely failed to identify CWEs in our samples. However, after applying instruction-following fine-tuning, the specialized SLM achieved remarkable performance on our test set, yielding approximately 99% accuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results strongly suggest that fine-tuned SLMs can serve as highly accurate and efficient tools for CWE detection, offering a practical and privacy-preserving solution for integrating advanced security analysis directly into development workflows.</li>
</ul>

<h3>Title: JEPA for RL: Investigating Joint-Embedding Predictive Architectures for Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Tristan Kenneweg, Philip Kenneweg, Barbara Hammer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16591">https://arxiv.org/abs/2504.16591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16591">https://arxiv.org/pdf/2504.16591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16591]] JEPA for RL: Investigating Joint-Embedding Predictive Architectures for Reinforcement Learning(https://arxiv.org/abs/2504.16591)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Joint-Embedding Predictive Architectures (JEPA) have recently become popular as promising architectures for self-supervised learning. Vision transformers have been trained using JEPA to produce embeddings from images and videos, which have been shown to be highly suitable for downstream tasks like classification and segmentation. In this paper, we show how to adapt the JEPA architecture to reinforcement learning from images. We discuss model collapse, show how to prevent it, and provide exemplary data on the classical Cart Pole task.</li>
</ul>

<h3>Title: Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study</h3>
<ul>
<li><strong>Authors: </strong>Andy Li, Wei Zhou, Rashina Hoda, Chris Bain, Peter Poon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16601">https://arxiv.org/abs/2504.16601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16601">https://arxiv.org/pdf/2504.16601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16601]] Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study(https://arxiv.org/abs/2504.16601)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study evaluates how well large language models (LLMs) and traditional machine translation (MT) tools translate medical consultation summaries from English into Arabic, Chinese, and Vietnamese. It assesses both patient, friendly and clinician, focused texts using standard automated metrics. Results showed that traditional MT tools generally performed better, especially for complex texts, while LLMs showed promise, particularly in Vietnamese and Chinese, when translating simpler summaries. Arabic translations improved with complexity due to the language's morphology. Overall, while LLMs offer contextual flexibility, they remain inconsistent, and current evaluation metrics fail to capture clinical relevance. The study highlights the need for domain-specific training, improved evaluation methods, and human oversight in medical translation.</li>
</ul>

<h3>Title: Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories</h3>
<ul>
<li><strong>Authors: </strong>Mareike Lisker, Christina Gottschalk, Helena Mihaljeviƒá</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16604">https://arxiv.org/abs/2504.16604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16604">https://arxiv.org/pdf/2504.16604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16604]] Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories(https://arxiv.org/abs/2504.16604)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic.</li>
</ul>

<h3>Title: Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections</h3>
<ul>
<li><strong>Authors: </strong>Max Kirchner, Alexander C. Jenke, Sebastian Bodenstedt, Fiona R. Kolbinger, Oliver Saldanha, Jakob N. Kather, Martin Wagner, Stefanie Speidel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16612">https://arxiv.org/abs/2504.16612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16612">https://arxiv.org/pdf/2504.16612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16612]] Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections(https://arxiv.org/abs/2504.16612)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Purpose: In this study, we investigate the training of foundation models using federated learning to address data-sharing limitations and enable collaborative model training without data transfer for minimally invasive surgery. Methods: Inspired by the EndoViT study, we adapt the Masked Autoencoder for federated learning, enhancing it with adaptive Sharpness-Aware Minimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is pretrained on the Endo700k dataset collection and later fine-tuned and evaluated for tasks such as Semantic Segmentation, Action Triplet Recognition, and Surgical Phase Recognition. Results: Our findings demonstrate that integrating adaptive FedSAM into the federated MAE approach improves pretraining, leading to a reduction in reconstruction loss per patch. The application of FL-EndoViT in surgical downstream tasks results in performance comparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over CEN-EndoViT in surgical scene segmentation when data is limited and in action triplet recognition when large datasets are used. Conclusion: These findings highlight the potential of federated learning for privacy-preserving training of surgical foundation models, offering a robust and generalizable solution for surgical data science. Effective collaboration requires adapting federated learning methods, such as the integration of FedSAM, which can accommodate the inherent data heterogeneity across institutions. In future, exploring FL in video-based models may enhance these capabilities by incorporating spatiotemporal dynamics crucial for real-world surgical environments.</li>
</ul>

<h3>Title: Security Science (SecSci), Basic Concepts and Mathematical Foundations</h3>
<ul>
<li><strong>Authors: </strong>Dusko Pavlovic, Peter-Michael Seidel</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY, cs.IT, cs.SI, math.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16617">https://arxiv.org/abs/2504.16617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16617">https://arxiv.org/pdf/2504.16617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16617]] Security Science (SecSci), Basic Concepts and Mathematical Foundations(https://arxiv.org/abs/2504.16617)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>This textbook compiles the lecture notes from security courses taught at Oxford in the 2000s, at Royal Holloway in the 2010s, and currently in Hawaii. The early chapters are suitable for a first course in security. The middle chapters have been used in advanced courses. Towards the end there are also some research problems.</li>
</ul>

<h3>Title: ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data</h3>
<ul>
<li><strong>Authors: </strong>Haoran Gu, Handing Wang, Yi Mei, Mengjie Zhang, Yaochu Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16628">https://arxiv.org/abs/2504.16628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16628">https://arxiv.org/pdf/2504.16628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16628]] ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data(https://arxiv.org/abs/2504.16628)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Aligning large language models with multiple human expectations and values is crucial for ensuring that they adequately serve a variety of user needs. To this end, offline multiobjective alignment algorithms such as the Rewards-in-Context algorithm have shown strong performance and efficiency. However, inappropriate preference representations and training with imbalanced reward scores limit the performance of such algorithms. In this work, we introduce ParetoHqD that addresses the above issues by representing human preferences as preference directions in the objective space and regarding data near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD follows a two-stage supervised fine-tuning process, where each stage uses an individual Pareto high-quality training set that best matches its preference direction. The experimental results have demonstrated the superiority of ParetoHqD over five baselines on two multiobjective alignment tasks.</li>
</ul>

<h3>Title: RouteWinFormer: A Route-Window Transformer for Middle-range Attention in Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Qifan Li, Tianyi Liang, Xingtao Wang, Xiaopeng Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16637">https://arxiv.org/abs/2504.16637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16637">https://arxiv.org/pdf/2504.16637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16637]] RouteWinFormer: A Route-Window Transformer for Middle-range Attention in Image Restoration(https://arxiv.org/abs/2504.16637)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer models have recently garnered significant attention in image restoration due to their ability to capture long-range pixel dependencies. However, long-range attention often results in computational overhead without practical necessity, as degradation and context are typically localized. Normalized average attention distance across various degradation datasets shows that middle-range attention is enough for image restoration. Building on this insight, we propose RouteWinFormer, a novel window-based Transformer that models middle-range context for image restoration. RouteWinFormer incorporates Route-Windows Attnetion Module, which dynamically selects relevant nearby windows based on regional similarity for attention aggregation, extending the receptive field to a mid-range size efficiently. In addition, we introduce Multi-Scale Structure Regularization during training, enabling the sub-scale of the U-shaped network to focus on structural information, while the original-scale learns degradation patterns based on generalized image structure priors. Extensive experiments demonstrate that RouteWinFormer outperforms state-of-the-art methods across 9 datasets in various image restoration tasks.</li>
</ul>

<h3>Title: SSLR: A Semi-Supervised Learning Method for Isolated Sign Language Recognition</h3>
<ul>
<li><strong>Authors: </strong>Hasan Algafri, Hamzah Luqman, Sarah Alyami, Issam Laradji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16640">https://arxiv.org/abs/2504.16640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16640">https://arxiv.org/pdf/2504.16640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16640]] SSLR: A Semi-Supervised Learning Method for Isolated Sign Language Recognition(https://arxiv.org/abs/2504.16640)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Sign language is the primary communication language for people with disabling hearing loss. Sign language recognition (SLR) systems aim to recognize sign gestures and translate them into spoken language. One of the main challenges in SLR is the scarcity of annotated datasets. To address this issue, we propose a semi-supervised learning (SSL) approach for SLR (SSLR), employing a pseudo-label method to annotate unlabeled samples. The sign gestures are represented using pose information that encodes the signer's skeletal joint points. This information is used as input for the Transformer backbone model utilized in the proposed approach. To demonstrate the learning capabilities of SSL across various labeled data sizes, several experiments were conducted using different percentages of labeled data with varying numbers of classes. The performance of the SSL approach was compared with a fully supervised learning-based model on the WLASL-100 dataset. The obtained results of the SSL model outperformed the supervised learning-based model with less labeled data in many cases.</li>
</ul>

<h3>Title: MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark</h3>
<ul>
<li><strong>Authors: </strong>William Corrias, Fabio De Gaspari, Dorjan Hitaj, Luigi V. Mancini</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16651">https://arxiv.org/abs/2504.16651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16651">https://arxiv.org/pdf/2504.16651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16651]] MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark(https://arxiv.org/abs/2504.16651)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, generative</a></li>
<li><strong>Abstract: </strong>The rapid evolution of generative models has led to their integration across various fields, including password guessing, aiming to generate passwords that resemble human-created ones in complexity, structure, and patterns. Despite generative model's promise, inconsistencies in prior research and a lack of rigorous evaluation have hindered a comprehensive understanding of their true potential. In this paper, we introduce MAYA, a unified, customizable, plug-and-play password benchmarking framework. MAYA provides a standardized approach for evaluating generative password-guessing models through a rigorous set of advanced testing scenarios and a collection of eight real-life password datasets. Using MAYA, we comprehensively evaluate six state-of-the-art approaches, which have been re-implemented and adapted to ensure standardization, for a total of over 15,000 hours of computation. Our findings indicate that these models effectively capture different aspects of human password distribution and exhibit strong generalization capabilities. However, their effectiveness varies significantly with long and complex passwords. Through our evaluation, sequential models consistently outperform other generative architectures and traditional password-guessing tools, demonstrating unique capabilities in generating accurate and complex guesses. Moreover, models learn and generate different password distributions, enabling a multi-model attack that outperforms the best individual model. By releasing MAYA, we aim to foster further research, providing the community with a new tool to consistently and reliably benchmark password-generation techniques. Our framework is publicly available at this https URL</li>
</ul>

<h3>Title: WiFi based Human Fall and Activity Recognition using Transformer based Encoder Decoder and Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Younggeol Cho, Elisa Motta, Olivia Nocentini, Marta Lagomarsino, Andrea Merello, Marco Crepaldi, Arash Ajoudani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16655">https://arxiv.org/abs/2504.16655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16655">https://arxiv.org/pdf/2504.16655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16655]] WiFi based Human Fall and Activity Recognition using Transformer based Encoder Decoder and Graph Neural Networks(https://arxiv.org/abs/2504.16655)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, transformer</a></li>
<li><strong>Abstract: </strong>Human pose estimation and action recognition have received attention due to their critical roles in healthcare monitoring, rehabilitation, and assistive technologies. In this study, we proposed a novel architecture named Transformer based Encoder Decoder Network (TED Net) designed for estimating human skeleton poses from WiFi Channel State Information (CSI). TED Net integrates convolutional encoders with transformer based attention mechanisms to capture spatiotemporal features from CSI signals. The estimated skeleton poses were used as input to a customized Directed Graph Neural Network (DGNN) for action recognition. We validated our model on two datasets: a publicly available multi modal dataset for assessing general pose estimation, and a newly collected dataset focused on fall related scenarios involving 20 participants. Experimental results demonstrated that TED Net outperformed existing approaches in pose estimation, and that the DGNN achieves reliable action classification using CSI based skeletons, with performance comparable to RGB based systems. Notably, TED Net maintains robust performance across both fall and non fall cases. These findings highlight the potential of CSI driven human skeleton estimation for effective action recognition, particularly in home environments such as elderly fall detection. In such settings, WiFi signals are often readily available, offering a privacy preserving alternative to vision based methods, which may raise concerns about continuous camera monitoring.</li>
</ul>

<h3>Title: A Time Series Dataset of NIR Spectra and RGB and NIR-HSI Images of the Barley Germination Process</h3>
<ul>
<li><strong>Authors: </strong>Ole-Christian Galbo Engstr√∏m, Erik Schou Dreier, Birthe M√∏ller Jespersen, Kim Steenstrup Pedersen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16658">https://arxiv.org/abs/2504.16658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16658">https://arxiv.org/pdf/2504.16658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16658]] A Time Series Dataset of NIR Spectra and RGB and NIR-HSI Images of the Barley Germination Process(https://arxiv.org/abs/2504.16658)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We provide an open-source dataset of RGB and NIR-HSI (near-infrared hyperspectral imaging) images with associated segmentation masks and NIR spectra of 2242 individual malting barley kernels. We imaged every kernel pre-exposure to moisture and every 24 hours after exposure to moisture for five consecutive days. Every barley kernel was labeled as germinated or not germinated during each image acquisition. The barley kernels were imaged with black filter paper as the background, facilitating straight-forward intensity threshold-based segmentation, e.g., by Otsu's method. This dataset facilitates time series analysis of germination time for barley kernels using either RGB image analysis, NIR spectral analysis, NIR-HSI analysis, or a combination hereof.</li>
</ul>

<h3>Title: Efficient Data Valuation Approximation in Federated Learning: A Sampling-based Approach</h3>
<ul>
<li><strong>Authors: </strong>Shuyue Wei, Yongxin Tong, Zimu Zhou, Tianran He, Yi Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16668">https://arxiv.org/abs/2504.16668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16668">https://arxiv.org/pdf/2504.16668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16668]] Efficient Data Valuation Approximation in Federated Learning: A Sampling-based Approach(https://arxiv.org/abs/2504.16668)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, fair</a></li>
<li><strong>Abstract: </strong>Federated learning paradigm to utilize datasets across multiple data providers. In FL, cross-silo data providers often hesitate to share their high-quality dataset unless their data value can be fairly assessed. Shapley value (SV) has been advocated as the standard metric for data valuation in FL due to its desirable properties. However, the computational overhead of SV is prohibitive in practice, as it inherently requires training and evaluating an FL model across an exponential number of dataset combinations. Furthermore, existing solutions fail to achieve high accuracy and efficiency, making practical use of SV still out of reach, because they ignore choosing suitable computation scheme for approximation framework and overlook the property of utility function in FL. We first propose a unified stratified-sampling framework for two widely-used schemes. Then, we analyze and choose the more promising scheme under the FL linear regression assumption. After that, we identify a phenomenon termed key combinations, where only limited dataset combinations have a high-impact on final data value. Building on these insights, we propose a practical approximation algorithm, IPSS, which strategically selects high-impact dataset combinations rather than evaluating all possible combinations, thus substantially reducing time cost with minor approximation error. Furthermore, we conduct extensive evaluations on the FL benchmark datasets to demonstrate that our proposed algorithm outperforms a series of representative baselines in terms of efficiency and effectiveness.</li>
</ul>

<h3>Title: A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Luisa Shimabucoro, Ahmet Ustun, Marzieh Fadaee, Sebastian Ruder</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16677">https://arxiv.org/abs/2504.16677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16677">https://arxiv.org/pdf/2504.16677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16677]] A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics(https://arxiv.org/abs/2504.16677)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>In order for large language models to be useful across the globe, they are fine-tuned to follow instructions on multilingual data. Despite the ubiquity of such post-training, a clear understanding of the dynamics that enable cross-lingual transfer remains elusive. This study examines cross-lingual transfer (CLT) dynamics in realistic post-training settings. We study two model families of up to 35B parameters in size trained on carefully controlled mixtures of multilingual data on three generative tasks with varying levels of complexity (summarization, instruction following, and mathematical reasoning) in both single-task and multi-task instruction tuning settings. Overall, we find that the dynamics of cross-lingual transfer and multilingual performance cannot be explained by isolated variables, varying depending on the combination of post-training settings. Finally, we identify the conditions that lead to effective cross-lingual transfer in practice.</li>
</ul>

<h3>Title: MCMC for Bayesian estimation of Differential Privacy from Membership Inference Attacks</h3>
<ul>
<li><strong>Authors: </strong>Ceren Yildirim, Kamer Kaya, Sinan Yildirim, Erkay Savas</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16683">https://arxiv.org/abs/2504.16683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16683">https://arxiv.org/pdf/2504.16683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16683]] MCMC for Bayesian estimation of Differential Privacy from Membership Inference Attacks(https://arxiv.org/abs/2504.16683)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>We propose a new framework for Bayesian estimation of differential privacy, incorporating evidence from multiple membership inference attacks (MIA). Bayesian estimation is carried out via a Markov chain Monte Carlo (MCMC) algorithm, named MCMC-DP-Est, which provides an estimate of the full posterior distribution of the privacy parameter (e.g., instead of just credible intervals). Critically, the proposed method does not assume that privacy auditing is performed with the most powerful attack on the worst-case (dataset, challenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est jointly estimates the strengths of MIAs used and the privacy of the training algorithm, yielding a more cautious privacy analysis. We also present an economical way to generate measurements for the performance of an MIA that is to be used by the MCMC method to estimate privacy. We present the use of the methods with numerical examples with both artificial and real data.</li>
</ul>

<h3>Title: SemanticSugarBeets: A Multi-Task Framework and Dataset for Inspecting Harvest and Storage Characteristics of Sugar Beets</h3>
<ul>
<li><strong>Authors: </strong>Gerardus Croonen, Andreas Trondl, Julia Simon, Daniel Steininger</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16684">https://arxiv.org/abs/2504.16684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16684">https://arxiv.org/pdf/2504.16684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16684]] SemanticSugarBeets: A Multi-Task Framework and Dataset for Inspecting Harvest and Storage Characteristics of Sugar Beets(https://arxiv.org/abs/2504.16684)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>While sugar beets are stored prior to processing, they lose sugar due to factors such as microorganisms present in adherent soil and excess vegetation. Their automated visual inspection promises to aide in quality assurance and thereby increase efficiency throughout the processing chain of sugar production. In this work, we present a novel high-quality annotated dataset and two-stage method for the detection, semantic segmentation and mass estimation of post-harvest and post-storage sugar beets in monocular RGB images. We conduct extensive ablation experiments for the detection of sugar beets and their fine-grained semantic segmentation regarding damages, rot, soil adhesion and excess vegetation. For these tasks, we evaluate multiple image sizes, model architectures and encoders, as well as the influence of environmental conditions. Our experiments show an mAP50-95 of 98.8 for sugar-beet detection and an mIoU of 64.0 for the best-performing segmentation model.</li>
</ul>

<h3>Title: PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Li, Hang Zhao, Zhiyuan Yu, Yu Du, Qin Zou, Ruizhen Hu, Kai Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16693">https://arxiv.org/abs/2504.16693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16693">https://arxiv.org/pdf/2504.16693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16693]] PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation(https://arxiv.org/abs/2504.16693)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While non-prehensile manipulation (e.g., controlled pushing/poking) constitutes a foundational robotic skill, its learning remains challenging due to the high sensitivity to complex physical interactions involving friction and restitution. To achieve robust policy learning and generalization, we opt to learn a world model of the 3D rigid body dynamics involved in non-prehensile manipulations and use it for model-based reinforcement learning. We propose PIN-WM, a Physics-INformed World Model that enables efficient end-to-end identification of a 3D rigid body dynamical system from visual observations. Adopting differentiable physics simulation, PIN-WM can be learned with only few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM is learned with observational loss induced by Gaussian Splatting without needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM into a group of Digital Cousins via physics-aware randomizations which perturb physics and rendering parameters to generate diverse and meaningful variations of the PIN-WM. Extensive evaluations on both simulation and real-world tests demonstrate that PIN-WM, enhanced with physics-aware digital cousins, facilitates learning robust non-prehensile manipulation skills with Sim2Real transfer, surpassing the Real2Sim2Real state-of-the-arts.</li>
</ul>

<h3>Title: CAIBA: Multicast Source Authentication for CAN Through Reactive Bit Flipping</h3>
<ul>
<li><strong>Authors: </strong>Eric Wagner, Frederik Basels, Jan Bauer, Till Zimmermann, Klaus Wehrle, Martin Henze</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16695">https://arxiv.org/abs/2504.16695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16695">https://arxiv.org/pdf/2504.16695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16695]] CAIBA: Multicast Source Authentication for CAN Through Reactive Bit Flipping(https://arxiv.org/abs/2504.16695)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack</a></li>
<li><strong>Abstract: </strong>Controller Area Networks (CANs) are the backbone for reliable intra-vehicular communication. Recent cyberattacks have, however, exposed the weaknesses of CAN, which was designed without any security considerations in the 1980s. Current efforts to retrofit security via intrusion detection or message authentication codes are insufficient to fully secure CAN as they cannot adequately protect against masquerading attacks, where a compromised communication device, a so-called electronic control units, imitates another device. To remedy this situation, multicast source authentication is required to reliably identify the senders of messages. In this paper, we present CAIBA, a novel multicast source authentication scheme specifically designed for communication buses like CAN. CAIBA relies on an authenticator overwriting authentication tags on-the-fly, such that a receiver only reads a valid tag if not only the integrity of a message but also its source can be verified. To integrate CAIBA into CAN, we devise a special message authentication scheme and a reactive bit overwriting mechanism. We achieve interoperability with legacy CAN devices, while protecting receivers implementing the AUTOSAR SecOC standard against masquerading attacks without communication overhead or verification delays.</li>
</ul>

<h3>Title: A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization</h3>
<ul>
<li><strong>Authors: </strong>Shiyin Tan, Jaeeon Park, Dongyuan Li, Renhe Jiang, Manabu Okumura</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16711">https://arxiv.org/abs/2504.16711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16711">https://arxiv.org/pdf/2504.16711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16711]] A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization(https://arxiv.org/abs/2504.16711)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>In the field of multi-document summarization (MDS), transformer-based models have demonstrated remarkable success, yet they suffer an input length limitation. Current methods apply truncation after the retrieval process to fit the context length; however, they heavily depend on manually well-crafted queries, which are impractical to create for each document set for MDS. Additionally, these methods retrieve information at a coarse granularity, leading to the inclusion of irrelevant content. To address these issues, we propose a novel retrieval-based framework that integrates query selection and document ranking and shortening into a unified process. Our approach identifies the most salient elementary discourse units (EDUs) from input documents and utilizes them as latent queries. These queries guide the document ranking by calculating relevance scores. Instead of traditional truncation, our approach filters out irrelevant EDUs to fit the context length, ensuring that only critical information is preserved for summarization. We evaluate our framework on multiple MDS datasets, demonstrating consistent improvements in ROUGE metrics while confirming its scalability and flexibility across diverse model architectures. Additionally, we validate its effectiveness through an in-depth analysis, emphasizing its ability to dynamically select appropriate queries and accurately rank documents based on their relevance scores. These results demonstrate that our framework effectively addresses context-length constraints, establishing it as a robust and reliable solution for MDS.</li>
</ul>

<h3>Title: V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Fan, Yumeng Wang, Sandeep Polisetty, Yi R. (May)Fung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16727">https://arxiv.org/abs/2504.16727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16727">https://arxiv.org/pdf/2504.16727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16727]] V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations(https://arxiv.org/abs/2504.16727)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large Vision Language Models (LVLMs) excel in various vision-language tasks. Yet, their robustness to visual variations in position, scale, orientation, and context that objects in natural scenes inevitably exhibit due to changes in viewpoint and environment remains largely underexplored. To bridge this gap, we introduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating Visual Variation Robustness of LVLMs, which encompasses automated evaluation dataset generation and principled metrics for thorough robustness assessment. Through extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability to visual variations, in which even advanced models that excel at complex vision-language tasks significantly underperform on simple tasks such as object recognition. Interestingly, these models exhibit a distinct visual position bias that contradicts theories of effective receptive fields, and demonstrate a human-like visual acuity threshold. To identify the source of these vulnerabilities, we present a systematic framework for component-level analysis, featuring a novel visualization approach for aligned visual features. Results show that these vulnerabilities stem from error accumulation in the pipeline architecture and inadequate multimodal alignment. Complementary experiments with synthetic data further demonstrate that these limitations are fundamentally architectural deficiencies, scoring the need for architectural innovations in future LVLM designs.</li>
</ul>

<h3>Title: Prompt-Tuning SAM: From Generalist to Specialist with only 2048 Parameters and 16 Training Images</h3>
<ul>
<li><strong>Authors: </strong>Tristan Piater, Bj√∂rn Barz, Alexander Freytag</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16739">https://arxiv.org/abs/2504.16739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16739">https://arxiv.org/pdf/2504.16739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16739]] Prompt-Tuning SAM: From Generalist to Specialist with only 2048 Parameters and 16 Training Images(https://arxiv.org/abs/2504.16739)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The Segment Anything Model (SAM) is widely used for segmenting a diverse range of objects in natural images from simple user prompts like points or bounding boxes. However, SAM's performance decreases substantially when applied to non-natural domains like microscopic imaging. Furthermore, due to SAM's interactive design, it requires a precise prompt for each image and object, which is unfeasible in many automated biomedical applications. Previous solutions adapt SAM by training millions of parameters via fine-tuning large parts of the model or of adapter layers. In contrast, we show that as little as 2,048 additional parameters are sufficient for turning SAM into a use-case specialist for a certain downstream task. Our novel PTSAM (prompt-tuned SAM) method uses prompt-tuning, a parameter-efficient fine-tuning technique, to adapt SAM for a specific task. We validate the performance of our approach on multiple microscopic and one medical dataset. Our results show that prompt-tuning only SAM's mask decoder already leads to a performance on-par with state-of-the-art techniques while requiring roughly 2,000x less trainable parameters. For addressing domain gaps, we find that additionally prompt-tuning SAM's image encoder is beneficial, further improving segmentation accuracy by up to 18% over state-of-the-art results. Since PTSAM can be reliably trained with as little as 16 annotated images, we find it particularly helpful for applications with limited training data and domain shifts.</li>
</ul>

<h3>Title: Gaussian Splatting is an Effective Data Generator for 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Farhad G. Zanjani, Davide Abati, Auke Wiggers, Dimitris Kalatzis, Jens Petersen, Hong Cai, Amirhossein Habibian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16740">https://arxiv.org/abs/2504.16740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16740">https://arxiv.org/pdf/2504.16740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16740]] Gaussian Splatting is an Effective Data Generator for 3D Object Detection(https://arxiv.org/abs/2504.16740)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We investigate data augmentation for 3D object detection in autonomous driving. We utilize recent advancements in 3D reconstruction based on Gaussian Splatting for 3D object placement in driving scenes. Unlike existing diffusion-based methods that synthesize images conditioned on BEV layouts, our approach places 3D objects directly in the reconstructed 3D space with explicitly imposed geometric transformations. This ensures both the physical plausibility of object placement and highly accurate 3D pose and position annotations. Our experiments demonstrate that even by integrating a limited number of external 3D objects into real scenes, the augmented data significantly enhances 3D object detection performance and outperforms existing diffusion-based 3D augmentation for object detection. Extensive testing on the nuScenes dataset reveals that imposing high geometric diversity in object placement has a greater impact compared to the appearance diversity of objects. Additionally, we show that generating hard examples, either by maximizing detection loss or imposing high visual occlusion in camera images, does not lead to more efficient 3D data augmentation for camera-based 3D object detection in autonomous driving.</li>
</ul>

<h3>Title: Simple Graph Contrastive Learning via Fractional-order Neural Diffusion Networks</h3>
<ul>
<li><strong>Authors: </strong>Yanan Zhao, Feng Ji, Kai Zhao, Xuhao Li, Qiyu Kang, Wenfei Liang, Yahya Alkhatib, Xingchao Jian, Wee Peng Tay</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16748">https://arxiv.org/abs/2504.16748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16748">https://arxiv.org/pdf/2504.16748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16748]] Simple Graph Contrastive Learning via Fractional-order Neural Diffusion Networks(https://arxiv.org/abs/2504.16748)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph Contrastive Learning (GCL) has recently made progress as an unsupervised graph representation learning paradigm. GCL approaches can be categorized into augmentation-based and augmentation-free methods. The former relies on complex data augmentations, while the latter depends on encoders that can generate distinct views of the same input. Both approaches may require negative samples for training. In this paper, we introduce a novel augmentation-free GCL framework based on graph neural diffusion models. Specifically, we utilize learnable encoders governed by Fractional Differential Equations (FDE). Each FDE is characterized by an order parameter of the differential operator. We demonstrate that varying these parameters allows us to produce learnable encoders that generate diverse views, capturing either local or global information, for contrastive learning. Our model does not require negative samples for training and is applicable to both homophilic and heterophilic datasets. We demonstrate its effectiveness across various datasets, achieving state-of-the-art performance.</li>
</ul>

<h3>Title: Feature Mixing Approach for Detecting Intraoperative Adverse Events in Laparoscopic Roux-en-Y Gastric Bypass Surgery</h3>
<ul>
<li><strong>Authors: </strong>Rupak Bose, Chinedu Innocent Nwoye, Jorge Lazo, Jo√´l Lukas Lavanchy, Nicolas Padoy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16749">https://arxiv.org/abs/2504.16749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16749">https://arxiv.org/pdf/2504.16749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16749]] Feature Mixing Approach for Detecting Intraoperative Adverse Events in Laparoscopic Roux-en-Y Gastric Bypass Surgery(https://arxiv.org/abs/2504.16749)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>Intraoperative adverse events (IAEs), such as bleeding or thermal injury, can lead to severe postoperative complications if undetected. However, their rarity results in highly imbalanced datasets, posing challenges for AI-based detection and severity quantification. We propose BetaMixer, a novel deep learning model that addresses these challenges through a Beta distribution-based mixing approach, converting discrete IAE severity scores into continuous values for precise severity regression (0-5 scale). BetaMixer employs Beta distribution-based sampling to enhance underrepresented classes and regularizes intermediate embeddings to maintain a structured feature space. A generative approach aligns the feature space with sampled IAE severity, enabling robust classification and severity regression via a transformer. Evaluated on the MultiBypass140 dataset, which we extended with IAE labels, BetaMixer achieves a weighted F1 score of 0.76, recall of 0.81, PPV of 0.73, and NPV of 0.84, demonstrating strong performance on imbalanced data. By integrating Beta distribution-based sampling, feature mixing, and generative modeling, BetaMixer offers a robust solution for IAE detection and quantification in clinical settings.</li>
</ul>

<h3>Title: HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations</h3>
<ul>
<li><strong>Authors: </strong>Kwangseob Ahn</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16754">https://arxiv.org/abs/2504.16754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16754">https://arxiv.org/pdf/2504.16754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16754]] HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations(https://arxiv.org/abs/2504.16754)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) struggle with maintaining coherence in extended conversations spanning hundreds of turns, despite performing well within their context windows. This paper introduces HEMA (Hippocampus-Inspired Extended Memory Architecture), a dual-memory system inspired by human cognitive processes. HEMA combines Compact Memory - a continuously updated one-sentence summary preserving global narrative coherence, and Vector Memory - an episodic store of chunk embeddings queried via cosine similarity. When integrated with a 6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns while keeping prompt length under 3,500 tokens. Experimental results show substantial improvements: factual recall accuracy increases from 41% to 87%, and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K indexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling the area under the precision-recall curve compared to summarization-only approaches. Ablation studies reveal two key insights: semantic forgetting through age-weighted pruning reduces retrieval latency by 34% with minimal recall loss, and a two-level summary hierarchy prevents cascade errors in ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that combining verbatim recall with semantic continuity provides a practical solution for privacy-aware conversational AI capable of month-long dialogues without model retraining.</li>
</ul>

<h3>Title: Tri-FusionNet: Enhancing Image Description Generation with Transformer-based Fusion Network and Dual Attention Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Lakshita Agarwal, Bindu Verma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16761">https://arxiv.org/abs/2504.16761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16761">https://arxiv.org/pdf/2504.16761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16761]] Tri-FusionNet: Enhancing Image Description Generation with Transformer-based Fusion Network and Dual Attention Mechanism(https://arxiv.org/abs/2504.16761)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Image description generation is essential for accessibility and AI understanding of visual content. Recent advancements in deep learning have significantly improved natural language processing and computer vision. In this work, we propose Tri-FusionNet, a novel image description generation model that integrates transformer modules: a Vision Transformer (ViT) encoder module with dual-attention mechanism, a Robustly Optimized BERT Approach (RoBERTa) decoder module, and a Contrastive Language-Image Pre-Training (CLIP) integrating module. The ViT encoder, enhanced with dual attention, focuses on relevant spatial regions and linguistic context, improving image feature extraction. The RoBERTa decoder is employed to generate precise textual descriptions. CLIP's integrating module aligns visual and textual data through contrastive learning, ensuring effective combination of both modalities. This fusion of ViT, RoBERTa, and CLIP, along with dual attention, enables the model to produce more accurate, contextually rich, and flexible descriptions. The proposed framework demonstrated competitive performance on the Flickr30k and Flickr8k datasets, with BLEU scores ranging from 0.767 to 0.456 and 0.784 to 0.479, CIDEr scores of 1.679 and 1.483, METEOR scores of 0.478 and 0.358, and ROUGE-L scores of 0.567 and 0.789, respectively. On MS-COCO, the framework obtained BLEU scores of 0.893 (B-1), 0.821 (B-2), 0.794 (B-3), and 0.725 (B-4). The results demonstrate the effectiveness of Tri-FusionNet in generating high-quality image descriptions.</li>
</ul>

<h3>Title: Noise-Tolerant Coreset-Based Class Incremental Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Edison Mucllari, Aswin Raghavan, Zachary Alan Daniels</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16763">https://arxiv.org/abs/2504.16763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16763">https://arxiv.org/pdf/2504.16763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16763]] Noise-Tolerant Coreset-Based Class Incremental Continual Learning(https://arxiv.org/abs/2504.16763)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Many applications of computer vision require the ability to adapt to novel data distributions after deployment. Adaptation requires algorithms capable of continual learning (CL). Continual learners must be plastic to adapt to novel tasks while minimizing forgetting of previous this http URL, CL opens up avenues for noise to enter the training pipeline and disrupt the CL. This work focuses on label noise and instance noise in the context of class-incremental learning (CIL), where new classes are added to a classifier over time, and there is no access to external data from past classes. We aim to understand the sensitivity of CL methods that work by replaying items from a memory constructed using the idea of Coresets. We derive a new bound for the robustness of such a method to uncorrelated instance noise under a general additive noise threat model, revealing several insights. Putting the theory into practice, we create two continual learning algorithms to construct noise-tolerant replay buffers. We empirically compare the effectiveness of prior memory-based continual learners and the proposed algorithms under label and uncorrelated instance noise on five diverse datasets. We show that existing memory-based CL are not robust whereas the proposed methods exhibit significant improvements in maximizing classification accuracy and minimizing forgetting in the noisy CIL setting.</li>
</ul>

<h3>Title: Online model learning with data-assimilated reservoir computers</h3>
<ul>
<li><strong>Authors: </strong>Andrea N√≥voa, Luca Magri</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16767">https://arxiv.org/abs/2504.16767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16767">https://arxiv.org/pdf/2504.16767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16767]] Online model learning with data-assimilated reservoir computers(https://arxiv.org/abs/2504.16767)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose an online learning framework for forecasting nonlinear spatio-temporal signals (fields). The method integrates (i) dimensionality reduction, here, a simple proper orthogonal decomposition (POD) projection; (ii) a generalized autoregressive model to forecast reduced dynamics, here, a reservoir computer; (iii) online adaptation to update the reservoir computer (the model), here, ensemble sequential data this http URL demonstrate the framework on a wake past a cylinder governed by the Navier-Stokes equations, exploring the assimilation of full flow fields (projected onto POD modes) and sparse sensors. Three scenarios are examined: a na√Øve physical state estimation; a two-fold estimation of physical and reservoir states; and a three-fold estimation that also adjusts the model parameters. The two-fold strategy significantly improves ensemble convergence and reduces reconstruction error compared to the na√Øve approach. The three-fold approach enables robust online training of partially-trained reservoir computers, overcoming limitations of a priori training. By unifying data-driven reduced order modelling with Bayesian data assimilation, this work opens new opportunities for scalable online model learning for nonlinear time series forecasting.</li>
</ul>

<h3>Title: How Effective are Generative Large Language Models in Performing Requirements Classification?</h3>
<ul>
<li><strong>Authors: </strong>Waad Alhoshan, Alessio Ferrari, Liping Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16768">https://arxiv.org/abs/2504.16768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16768">https://arxiv.org/pdf/2504.16768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16768]] How Effective are Generative Large Language Models in Performing Requirements Classification?(https://arxiv.org/abs/2504.16768)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>In recent years, transformer-based large language models (LLMs) have revolutionised natural language processing (NLP), with generative models opening new possibilities for tasks that require context-aware text generation. Requirements engineering (RE) has also seen a surge in the experimentation of LLMs for different tasks, including trace-link detection, regulatory compliance, and others. Requirements classification is a common task in RE. While non-generative LLMs like BERT have been successfully applied to this task, there has been limited exploration of generative LLMs. This gap raises an important question: how well can generative LLMs, which produce context-aware outputs, perform in requirements classification? In this study, we explore the effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing both binary and multi-class requirements classification. We design an extensive experimental study involving over 400 experiments across three widely used datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes that while factors like prompt design and LLM architecture are universally important, others-such as dataset variations-have a more situational impact, depending on the complexity of the classification task. This insight can guide future model development and deployment strategies, focusing on optimising prompt structures and aligning model architectures with task-specific needs for improved performance.</li>
</ul>

<h3>Title: Evaluation Framework for AI Systems in "the Wild"</h3>
<ul>
<li><strong>Authors: </strong>Sarah Jabbour, Trenton Chang, Anindya Das Antar, Joseph Peper, Insu Jang, Jiachen Liu, Jae-Won Chung, Shiqi He, Michael Wellman, Bryan Goodman, Elizabeth Bondi-Kelly, Kevin Samy, Rada Mihalcea, Mosharaf Chowhury, David Jurgens, Lu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16778">https://arxiv.org/abs/2504.16778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16778">https://arxiv.org/pdf/2504.16778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16778]] Evaluation Framework for AI Systems in "the Wild"(https://arxiv.org/abs/2504.16778)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative</a></li>
<li><strong>Abstract: </strong>Generative AI (GenAI) models have become vital across industries, yet current evaluation methods have not adapted to their widespread use. Traditional evaluations often rely on benchmarks and fixed datasets, frequently failing to reflect real-world performance, which creates a gap between lab-tested outcomes and practical applications. This white paper proposes a comprehensive framework for how we should evaluate real-world GenAI systems, emphasizing diverse, evolving inputs and holistic, dynamic, and ongoing assessment approaches. The paper offers guidance for practitioners on how to design evaluation methods that accurately reflect real-time capabilities, and provides policymakers with recommendations for crafting GenAI policies focused on societal impacts, rather than fixed performance numbers or parameter sizes. We advocate for holistic frameworks that integrate performance, fairness, and ethics and the use of continuous, outcome-oriented methods that combine human and automated assessments while also being transparent to foster trust among stakeholders. Implementing these strategies ensures GenAI models are not only technically proficient but also ethically responsible and impactful.</li>
</ul>

<h3>Title: MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores</h3>
<ul>
<li><strong>Authors: </strong>Fengwei Zhou, Jiafei Song, Wenjin Jason Li, Gengjian Xue, Zhikang Zhao, Yichao Lu, Bailin Na</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16786">https://arxiv.org/abs/2504.16786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16786">https://arxiv.org/pdf/2504.16786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16786]] MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores(https://arxiv.org/abs/2504.16786)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models have significantly improved their ability to process long-context input, but practical applications are challenged by increased inference time and resource consumption, particularly in resource-constrained environments. To address these challenges, we propose MOOSComp, a token-classification-based long-context compression method that enhances the performance of a BERT-based compressor by mitigating the over-smoothing problem and incorporating outlier scores. In the training phase, we add an inter-class cosine similarity loss term to penalize excessively similar token representations, thereby improving the token classification accuracy. During the compression phase, we introduce outlier scores to preserve rare but critical tokens that are prone to be discarded in task-agnostic compression. These scores are integrated with the classifier's output, making the compressor more generalizable to various tasks. Superior performance is achieved at various compression ratios on long-context understanding and reasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x compression ratio on a resource-constrained mobile device.</li>
</ul>

<h3>Title: Towards Explainable AI: Multi-Modal Transformer for Video-based Image Description Generation</h3>
<ul>
<li><strong>Authors: </strong>Lakshita Agarwal, Bindu Verma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16788">https://arxiv.org/abs/2504.16788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16788">https://arxiv.org/pdf/2504.16788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16788]] Towards Explainable AI: Multi-Modal Transformer for Video-based Image Description Generation(https://arxiv.org/abs/2504.16788)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, generative</a></li>
<li><strong>Abstract: </strong>Understanding and analyzing video actions are essential for producing insightful and contextualized descriptions, especially for video-based applications like intelligent monitoring and autonomous systems. The proposed work introduces a novel framework for generating natural language descriptions from video datasets by combining textual and visual modalities. The suggested architecture makes use of ResNet50 to extract visual features from video frames that are taken from the Microsoft Research Video Description Corpus (MSVD), and Berkeley DeepDrive eXplanation (BDD-X) datasets. The extracted visual characteristics are converted into patch embeddings and then run through an encoder-decoder model based on Generative Pre-trained Transformer-2 (GPT-2). In order to align textual and visual representations and guarantee high-quality description production, the system uses multi-head self-attention and cross-attention techniques. The model's efficacy is demonstrated by performance evaluation using BLEU (1-4), CIDEr, METEOR, and ROUGE-L. The suggested framework outperforms traditional methods with BLEU-4 scores of 0.755 (BDD-X) and 0.778 (MSVD), CIDEr scores of 1.235 (BDD-X) and 1.315 (MSVD), METEOR scores of 0.312 (BDD-X) and 0.329 (MSVD), and ROUGE-L scores of 0.782 (BDD-X) and 0.795 (MSVD). By producing human-like, contextually relevant descriptions, strengthening interpretability, and improving real-world applications, this research advances explainable AI.</li>
</ul>

<h3>Title: Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention</h3>
<ul>
<li><strong>Authors: </strong>Xiang Hu, Jiaqi Leng, Jun Zhao, Kewei Tu, Wei Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16795">https://arxiv.org/abs/2504.16795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16795">https://arxiv.org/pdf/2504.16795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16795]] Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention(https://arxiv.org/abs/2504.16795)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>A key advantage of Recurrent Neural Networks (RNNs) over Transformers is their linear computational and space complexity enables faster training and inference for long sequences. However, RNNs are fundamentally unable to randomly access historical context, and simply integrating attention mechanisms may undermine their efficiency advantages. To overcome this limitation, we propose \textbf{H}ierarchical \textbf{S}parse \textbf{A}ttention (HSA), a novel attention mechanism that enhances RNNs with long-range random access flexibility while preserving their merits in efficiency and length generalization. HSA divides inputs into chunks, selecting the top-$k$ chunks and hierarchically aggregates information. The core innovation lies in learning token-to-chunk relevance based on fine-grained token-level information inside each chunk. This approach enhances the precision of chunk selection across both in-domain and out-of-domain context lengths. To make HSA efficient, we further introduce a hardware-aligned kernel design. By combining HSA with Mamba, we introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64 million contexts despite pre-training on only 4K-length contexts, and significant improvements on various downstream tasks, with nearly constant memory footprint. These results show RAMba's huge potential in long-context modeling.</li>
</ul>

<h3>Title: Decoupled Global-Local Alignment for Improving Compositional Understanding</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxing Hu, Kaicheng Yang, Jun Wang, Haoran Xu, Ziyong Feng, Yupei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16801">https://arxiv.org/abs/2504.16801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16801">https://arxiv.org/pdf/2504.16801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16801]] Decoupled Global-Local Alignment for Improving Compositional Understanding(https://arxiv.org/abs/2504.16801)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Contrastive Language-Image Pre-training (CLIP) has achieved success on multiple downstream tasks by aligning image and text modalities. However, the nature of global contrastive learning limits CLIP's ability to comprehend compositional concepts, such as relations and attributes. Although recent studies employ global hard negative samples to improve compositional understanding, these methods significantly compromise the model's inherent general capabilities by forcibly distancing textual negative samples from images in the embedding space. To overcome this limitation, we introduce a Decoupled Global-Local Alignment (DeGLA) framework that improves compositional understanding while substantially mitigating losses in general capabilities. To optimize the retention of the model's inherent capabilities, we incorporate a self-distillation mechanism within the global alignment process, aligning the learnable image-text encoder with a frozen teacher model derived from an exponential moving average. Under the constraint of self-distillation, it effectively mitigates the catastrophic forgetting of pretrained knowledge during fine-tuning. To improve compositional understanding, we first leverage the in-context learning capability of Large Language Models (LLMs) to construct about 2M high-quality negative captions across five types. Subsequently, we propose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC) loss to enhance vision-language compositionally. Extensive experimental results demonstrate the effectiveness of the DeGLA framework. Compared to previous state-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across the VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average performance improvement of 13.0% on zero-shot classification tasks across eleven datasets. Our code will be released at this https URL</li>
</ul>

<h3>Title: LLM-assisted Graph-RAG Information Extraction from IFC Data</h3>
<ul>
<li><strong>Authors: </strong>Sima Iranmanesh, Hadeel Saadany, Edlira Vakaj</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16813">https://arxiv.org/abs/2504.16813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16813">https://arxiv.org/pdf/2504.16813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16813]] LLM-assisted Graph-RAG Information Extraction from IFC Data(https://arxiv.org/abs/2504.16813)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>IFC data has become the general building information standard for collaborative work in the construction industry. However, IFC data can be very complicated because it allows for multiple ways to represent the same product information. In this research, we utilise the capabilities of LLMs to parse the IFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to retrieve building object properties and their relations. We will show that, despite limitations due to the complex hierarchy of the IFC data, the Graph-RAG parsing enhances generative LLMs like GPT-4o with graph-based knowledge, enabling natural language query-response retrieval without the need for a complex pipeline.</li>
</ul>

<h3>Title: Process Reward Models That Think</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, Lu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16828">https://arxiv.org/abs/2504.16828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16828">https://arxiv.org/pdf/2504.16828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16828]] Process Reward Models That Think(https://arxiv.org/abs/2504.16828)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at this https URL.</li>
</ul>

<h3>Title: GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Luu Quy Tung, Hoang Quoc Viet, Vo Trong Thu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16832">https://arxiv.org/abs/2504.16832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16832">https://arxiv.org/pdf/2504.16832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16832]] GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning(https://arxiv.org/abs/2504.16832)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that require intermediate reasoning steps prior to generating a final answer. In this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model inspired by the finetuning strategy based on Group Relative Policy Optimization. We also leverage a high-quality Vietnamese synthesized reasoning dataset and design two reward functions to tackle the main limitations of this technique: (i) language mixing, where we explicitly detect the presence of biased language characters during the process of sampling tokens, and (ii) we leverage Sentence Transformer-based models to ensure that the generated reasoning content maintains factual correctness and does not distort the final output. Experimental results on the Vietnamese dataset from the VLSP 2023 Challenge demonstrate that our model outperforms prior works and enhances linguistic consistency in its responses. Furthermore, we extend our evaluation to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of our reasoning method compared to few-shot prompting techniques.</li>
</ul>

<h3>Title: Improving Significant Wave Height Prediction Using Chronos Models</h3>
<ul>
<li><strong>Authors: </strong>Yilin Zhai, Hongyuan Shi, Chao Zhan, Qing Wang, Zaijin You, Nan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16834">https://arxiv.org/abs/2504.16834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16834">https://arxiv.org/pdf/2504.16834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16834]] Improving Significant Wave Height Prediction Using Chronos Models(https://arxiv.org/abs/2504.16834)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Accurate wave height prediction is critical for maritime safety and coastal resilience, yet conventional physics-based models and traditional machine learning methods face challenges in computational efficiency and nonlinear dynamics modeling. This study introduces Chronos, the first implementation of a large language model (LLM)-powered temporal architecture (Chronos) optimized for wave forecasting. Through advanced temporal pattern recognition applied to historical wave data from three strategically chosen marine zones in the Northwest Pacific basin, our framework achieves multimodal improvements: (1) 14.3% reduction in training time with 2.5x faster inference speed compared to PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units; (2) superior short-term forecasting (1-24h) across comprehensive metrics; (3) sustained predictive leadership in extended-range forecasts (1-120h); and (4) demonstrated zero-shot capability maintaining median performance (rank 4/12) against specialized operational models. This LLM-enhanced temporal modeling paradigm establishes a new standard in wave prediction, offering both computationally efficient solutions and a transferable framework for complex geophysical systems modeling.</li>
</ul>

<h3>Title: Hyperspectral Vision Transformers for Greenhouse Gas Estimations from Space</h3>
<ul>
<li><strong>Authors: </strong>Ruben Gonzalez Avil√©s, Linus Scheibenreif, Nassim Ait Ali Braham, Benedikt Blumenstiel, Thomas Brunschwiler, Ranjini Guruprasad, Damian Borth, Conrad Albrecht, Paolo Fraccaro, Devyani Lambhate, Johannes Jakubik</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16851">https://arxiv.org/abs/2504.16851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16851">https://arxiv.org/pdf/2504.16851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16851]] Hyperspectral Vision Transformers for Greenhouse Gas Estimations from Space(https://arxiv.org/abs/2504.16851)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Hyperspectral imaging provides detailed spectral information and holds significant potential for monitoring of greenhouse gases (GHGs). However, its application is constrained by limited spatial coverage and infrequent revisit times. In contrast, multispectral imaging offers broader spatial and temporal coverage but often lacks the spectral detail that can enhance GHG detection. To address these challenges, this study proposes a spectral transformer model that synthesizes hyperspectral data from multispectral inputs. The model is pre-trained via a band-wise masked autoencoder and subsequently fine-tuned on spatio-temporally aligned multispectral-hyperspectral image pairs. The resulting synthetic hyperspectral data retain the spatial and temporal benefits of multispectral imagery and improve GHG prediction accuracy relative to using multispectral data alone. This approach effectively bridges the trade-off between spectral resolution and coverage, highlighting its potential to advance atmospheric monitoring by combining the strengths of hyperspectral and multispectral systems with self-supervised deep learning.</li>
</ul>

<h3>Title: Monte Carlo Planning with Large Language Model for Text-Based Game Agents</h3>
<ul>
<li><strong>Authors: </strong>Zijing Shi, Meng Fang, Ling Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16855">https://arxiv.org/abs/2504.16855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16855">https://arxiv.org/pdf/2504.16855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16855]] Monte Carlo Planning with Large Language Model for Text-Based Game Agents(https://arxiv.org/abs/2504.16855)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities. In this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments.</li>
</ul>

<h3>Title: Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification</h3>
<ul>
<li><strong>Authors: </strong>Alexander Shvets</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16856">https://arxiv.org/abs/2504.16856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16856">https://arxiv.org/pdf/2504.16856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16856]] Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification(https://arxiv.org/abs/2504.16856)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Most datasets for sentiment analysis lack context in which an opinion was expressed, often crucial for emotion understanding, and are mainly limited by a few emotion categories. Foundation large language models (LLMs) like GPT-4 suffer from over-predicting emotions and are too resource-intensive. We design an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b, for the generation of training examples for more accessible, lightweight BERT-type encoder models. We focus on enlarging the semantic diversity of examples and propose grounding the generation into a corpus of narratives to produce non-repetitive story-character-centered utterances with unique contexts over 28 emotion classes. By running 700K inferences in 450 GPU hours, we contribute with the dataset of 100K contextual and also 300K context-less examples to cover both scenarios. We use it for fine-tuning pre-trained encoders, which results in several Emo Pillars models. We show that Emo Pillars models are highly adaptive to new domains when tuned to specific tasks such as GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on the first three. We also validate our dataset, conducting statistical analysis and human evaluation, and confirm the success of our measures in utterance diversification (although less for the neutral class) and context personalization, while pointing out the need for improved handling of out-of-taxonomy labels within the pipeline.</li>
</ul>

<h3>Title: Planning with Diffusion Models for Target-Oriented Dialogue Systems</h3>
<ul>
<li><strong>Authors: </strong>Hanwen Du, Bo Peng, Xia Ning</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16858">https://arxiv.org/abs/2504.16858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16858">https://arxiv.org/pdf/2504.16858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16858]] Planning with Diffusion Models for Target-Oriented Dialogue Systems(https://arxiv.org/abs/2504.16858)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM era, where strategic dialogue planning is crucial for directing conversations toward specific targets. However, existing dialogue planning methods generate dialogue plans in a step-by-step sequential manner, and may suffer from compounding errors and myopic actions. To address these limitations, we introduce a novel dialogue planning framework, DiffTOD, which leverages diffusion models to enable non-sequential dialogue planning. DiffTOD formulates dialogue planning as a trajectory generation problem with conditional guidance, and leverages a diffusion language model to estimate the likelihood of the dialogue trajectory. To optimize the dialogue action strategies, DiffTOD introduces three tailored guidance mechanisms for different target types, offering flexible guidance towards diverse TOD targets at test time. Extensive experiments across three diverse TOD settings show that DiffTOD can effectively perform non-myopic lookahead exploration and optimize action strategies over a long horizon through non-sequential dialogue planning, and demonstrates strong flexibility across complex and diverse dialogue scenarios. Our code and data are accessible through this https URL.</li>
</ul>

<h3>Title: An Adaptive ML Framework for Power Converter Monitoring via Federated Transfer Learning</h3>
<ul>
<li><strong>Authors: </strong>Panagiotis Kakosimos, Alireza Nemat Saberi, Luca Peretti</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16866">https://arxiv.org/abs/2504.16866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16866">https://arxiv.org/pdf/2504.16866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16866]] An Adaptive ML Framework for Power Converter Monitoring via Federated Transfer Learning(https://arxiv.org/abs/2504.16866)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, federate</a></li>
<li><strong>Abstract: </strong>This study explores alternative framework configurations for adapting thermal machine learning (ML) models for power converters by combining transfer learning (TL) and federated learning (FL) in a piecewise manner. This approach inherently addresses challenges such as varying operating conditions, data sharing limitations, and security implications. The framework starts with a base model that is incrementally adapted by multiple clients via adapting three state-of-the-art domain adaptation techniques: Fine-tuning, Transfer Component Analysis (TCA), and Deep Domain Adaptation (DDA). The Flower framework is employed for FL, using Federated Averaging for aggregation. Validation with field data demonstrates that fine-tuning offers a straightforward TL approach with high accuracy, making it suitable for practical applications. Benchmarking results reveal a comprehensive comparison of these methods, showcasing their respective strengths and weaknesses when applied in different scenarios. Locally hosted FL enhances performance when data aggregation is not feasible, while cloud-based FL becomes more practical with a significant increase in the number of clients, addressing scalability and connectivity challenges.</li>
</ul>

<h3>Title: Exploring How LLMs Capture and Represent Domain-Specific Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Mirian Hipolito Garcia, Camille Couturier, Daniel Madrigal Diaz, Ankur Mallick, Anastasios Kyrillidis, Robert Sim, Victor Ruhle, Saravan Rajmohan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16871">https://arxiv.org/abs/2504.16871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16871">https://arxiv.org/pdf/2504.16871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16871]] Exploring How LLMs Capture and Represent Domain-Specific Knowledge(https://arxiv.org/abs/2504.16871)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>We study whether Large Language Models (LLMs) inherently capture domain-specific nuances in natural language. Our experiments probe the domain sensitivity of LLMs by examining their ability to distinguish queries from different domains using hidden states generated during the prefill phase. We reveal latent domain-related trajectories that indicate the model's internal recognition of query domains. We also study the robustness of these domain representations to variations in prompt styles and sources. Our approach leverages these representations for model selection, mapping the LLM that best matches the domain trace of the input query (i.e., the model with the highest performance on similar traces). Our findings show that LLMs can differentiate queries for related domains, and that the fine-tuned model is not always the most accurate. Unlike previous work, our interpretations apply to both closed and open-ended generative tasks</li>
</ul>

<h3>Title: Do Large Language Models know who did what to whom?</h3>
<ul>
<li><strong>Authors: </strong>Joseph M. Denning, Xiaohan (Hannah)Guo, Bryor Snefjella, Idan A. Blank</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16884">https://arxiv.org/abs/2504.16884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16884">https://arxiv.org/pdf/2504.16884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16884]] Do Large Language Models know who did what to whom?(https://arxiv.org/abs/2504.16884)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are commonly criticized for not understanding language. However, many critiques focus on cognitive abilities that, in humans, are distinct from language processing. Here, we instead study a kind of understanding tightly linked to language: inferring who did what to whom (thematic roles) in a sentence. Does the central training objective of LLMs-word prediction-result in sentence representations that capture thematic roles? In two experiments, we characterized sentence representations in four LLMs. In contrast to human similarity judgments, in LLMs the overall representational similarity of sentence pairs reflected syntactic similarity but not whether their agent and patient assignments were identical vs. reversed. Furthermore, we found little evidence that thematic role information was available in any subset of hidden units. However, some attention heads robustly captured thematic roles, independently of syntax. Therefore, LLMs can extract thematic roles but, relative to humans, this information influences their representations more weakly.</li>
</ul>

<h3>Title: Building A Secure Agentic AI Application Leveraging A2A Protocol</h3>
<ul>
<li><strong>Authors: </strong>Idan Habler, Ken Huang, Vineeth Sai Narajala, Prashant Kulkarni</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16902">https://arxiv.org/abs/2504.16902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16902">https://arxiv.org/pdf/2504.16902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16902]] Building A Secure Agentic AI Application Leveraging A2A Protocol(https://arxiv.org/abs/2504.16902)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust</a></li>
<li><strong>Abstract: </strong>As Agentic AI systems evolve from basic workflows to complex multi agent collaboration, robust protocols such as Google's Agent2Agent (A2A) become essential enablers. To foster secure adoption and ensure the reliability of these complex interactions, understanding the secure implementation of A2A is essential. This paper addresses this goal by providing a comprehensive security analysis centered on the A2A protocol. We examine its fundamental elements and operational dynamics, situating it within the framework of agent communication development. Utilizing the MAESTRO framework, specifically designed for AI risks, we apply proactive threat modeling to assess potential security issues in A2A deployments, focusing on aspects such as Agent Card management, task execution integrity, and authentication methodologies. Based on these insights, we recommend practical secure development methodologies and architectural best practices designed to build resilient and effective A2A systems. Our analysis also explores how the synergy between A2A and the Model Context Protocol (MCP) can further enhance secure interoperability. This paper equips developers and architects with the knowledge and practical guidance needed to confidently leverage the A2A protocol for building robust and secure next generation agentic applications.</li>
</ul>

<h3>Title: BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruotong Wang, Mingli Zhu, Jiarong Ou, Rui Chen, Xin Tao, Pengfei Wan, Baoyuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16907">https://arxiv.org/abs/2504.16907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16907">https://arxiv.org/pdf/2504.16907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16907]] BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation(https://arxiv.org/abs/2504.16907)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, generative</a></li>
<li><strong>Abstract: </strong>Text-to-video (T2V) generative models have rapidly advanced and found widespread applications across fields like entertainment, education, and marketing. However, the adversarial vulnerabilities of these models remain rarely explored. We observe that in T2V generation tasks, the generated videos often contain substantial redundant information not explicitly specified in the text prompts, such as environmental elements, secondary objects, and additional details, providing opportunities for malicious attackers to embed hidden harmful content. Exploiting this inherent redundancy, we introduce BadVideo, the first backdoor attack framework tailored for T2V generation. Our attack focuses on designing target adversarial outputs through two key strategies: (1) Spatio-Temporal Composition, which combines different spatiotemporal features to encode malicious information; (2) Dynamic Element Transformation, which introduces transformations in redundant elements over time to convey malicious information. Based on these strategies, the attacker's malicious target seamlessly integrates with the user's textual instructions, providing high stealthiness. Moreover, by exploiting the temporal dimension of videos, our attack successfully evades traditional content moderation systems that primarily analyze spatial information within individual frames. Extensive experiments demonstrate that BadVideo achieves high attack success rates while preserving original semantics and maintaining excellent performance on clean inputs. Overall, our work reveals the adversarial vulnerability of T2V models, calling attention to potential risks and misuse. Our project page is at this https URL.</li>
</ul>

<h3>Title: Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text</h3>
<ul>
<li><strong>Authors: </strong>Shifali Agrahari, Sanasam Ranbir Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16913">https://arxiv.org/abs/2504.16913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16913">https://arxiv.org/pdf/2504.16913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16913]] Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text(https://arxiv.org/abs/2504.16913)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>In recent years, the detection of AI-generated text has become a critical area of research due to concerns about academic integrity, misinformation, and ethical AI deployment. This paper presents COT Fine-tuned, a novel framework for detecting AI-generated text and identifying the specific language model. responsible for generating the text. We propose a dual-task approach, where Task A involves classifying text as AI-generated or human-written, and Task B identifies the specific LLM behind the text. The key innovation of our method lies in the use of Chain-of-Thought reasoning, which enables the model to generate explanations for its predictions, enhancing transparency and interpretability. Our experiments demonstrate that COT Fine-tuned achieves high accuracy in both tasks, with strong performance in LLM identification and human-AI classification. We also show that the CoT reasoning process contributes significantly to the models effectiveness and interpretability.</li>
</ul>

<h3>Title: DreamO: A Unified Framework for Image Customization</h3>
<ul>
<li><strong>Authors: </strong>Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, Mengtian Li, Songtao Zhao, Jian Zhang, Qian He, Xinglong Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16915">https://arxiv.org/abs/2504.16915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16915">https://arxiv.org/pdf/2504.16915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16915]] DreamO: A Unified Framework for Image Customization(https://arxiv.org/abs/2504.16915)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Recently, extensive research on image customization (e.g., identity, subject, style, background, etc.) demonstrates strong customization capabilities in large-scale generative models. However, most approaches are designed for specific tasks, restricting their generalizability to combine different types of condition. Developing a unified framework for image customization remains an open challenge. In this paper, we present DreamO, an image customization framework designed to support a wide range of tasks while facilitating seamless integration of multiple conditions. Specifically, DreamO utilizes a diffusion transformer (DiT) framework to uniformly process input of different types. During training, we construct a large-scale training dataset that includes various customization tasks, and we introduce a feature routing constraint to facilitate the precise querying of relevant information from reference images. Additionally, we design a placeholder strategy that associates specific placeholders with conditions at particular positions, enabling control over the placement of conditions in the generated results. Moreover, we employ a progressive training strategy consisting of three stages: an initial stage focused on simple tasks with limited data to establish baseline consistency, a full-scale training stage to comprehensively enhance the customization capabilities, and a final quality alignment stage to correct quality biases introduced by low-quality data. Extensive experiments demonstrate that the proposed DreamO can effectively perform various image customization tasks with high quality and flexibly integrate different types of control conditions.</li>
</ul>

<h3>Title: IberBench: LLM Evaluation on Iberian Languages</h3>
<ul>
<li><strong>Authors: </strong>Jos√© √Ångel Gonz√°lez, Ian Borrego Obrador, √Ålvaro Romo Herrero, Areg Mikael Sarvazyan, Mara Chinea-R√≠os, Angelo Basile, Marc Franco-Salvador</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16921">https://arxiv.org/abs/2504.16921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16921">https://arxiv.org/pdf/2504.16921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16921]] IberBench: LLM Evaluation on Iberian Languages(https://arxiv.org/abs/2504.16921)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) remain difficult to evaluate comprehensively, particularly for languages other than English, where high-quality data is often limited. Existing benchmarks and leaderboards are predominantly English-centric, with only a few addressing other languages. These benchmarks fall short in several key areas: they overlook the diversity of language varieties, prioritize fundamental Natural Language Processing (NLP) capabilities over tasks of industrial relevance, and are static. With these aspects in mind, we present IberBench, a comprehensive and extensible benchmark designed to assess LLM performance on both fundamental and industry-relevant NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America. IberBench integrates 101 datasets from evaluation campaigns and recent benchmarks, covering 22 task categories such as sentiment and emotion analysis, toxicity detection, and summarization. The benchmark addresses key limitations in current evaluation practices, such as the lack of linguistic diversity and static evaluation setups by enabling continual updates and community-driven model and dataset submissions moderated by a committee of experts. We evaluate 23 LLMs ranging from 100 million to 14 billion parameters and provide empirical insights into their strengths and limitations. Our findings indicate that (i) LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii) performance is on average lower for Galician and Basque, (iii) some tasks show results close to random, and (iv) in other tasks LLMs perform above random but below shared task systems. IberBench offers open-source implementations for the entire evaluation pipeline, including dataset normalization and hosting, incremental evaluation of LLMs, and a publicly accessible leaderboard.</li>
</ul>

<h3>Title: Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light</h3>
<ul>
<li><strong>Authors: </strong>Ali Hassani, Fengzhe Zhou, Aditya Kane, Jiannan Huang, Chieh-Yun Chen, Min Shi, Steven Walton, Markus Hoehnerbach, Vijay Thakkar, Michael Isaev, Qinsheng Zhang, Bing Xu, Haicheng Wu, Wen-mei Hwu, Ming-Yu Liu, Humphrey Shi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16922">https://arxiv.org/abs/2504.16922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16922">https://arxiv.org/pdf/2504.16922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16922]] Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light(https://arxiv.org/abs/2504.16922)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Many sparse attention mechanisms such as Neighborhood Attention have typically failed to consistently deliver speedup over the self attention baseline. This is largely due to the level of complexity in attention infrastructure, and the rapid evolution of AI hardware architecture. At the same time, many state-of-the-art foundational models, particularly in computer vision, are heavily bound by attention, and need reliable sparsity to escape the O(n^2) complexity. In this paper, we study a class of promising sparse attention mechanisms that focus on locality, and aim to develop a better analytical model of their performance improvements. We first introduce Generalized Neighborhood Attention (GNA), which can describe sliding window, strided sliding window, and blocked attention. We then consider possible design choices in implementing these approaches, and create a simulator that can provide much more realistic speedup upper bounds for any given setting. Finally, we implement GNA on top of a state-of-the-art fused multi-headed attention (FMHA) kernel designed for the NVIDIA Blackwell architecture in CUTLASS. Our implementation can fully realize the maximum speedup theoretically possible in many perfectly block-sparse cases, and achieves an effective utilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA configurations into off-the-shelf generative models, such as Cosmos-7B, HunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end speedup on B200 without any fine-tuning. We will open source our simulator and Blackwell kernels directly through the NATTEN project.</li>
</ul>

<h3>Title: Procedural Dataset Generation for Zero-Shot Stereo Matching</h3>
<ul>
<li><strong>Authors: </strong>David Yan, Alexander Raistrick, Jia Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16930">https://arxiv.org/abs/2504.16930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16930">https://arxiv.org/pdf/2504.16930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16930]] Procedural Dataset Generation for Zero-Shot Stereo Matching(https://arxiv.org/abs/2504.16930)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Synthetic datasets are a crucial ingredient for training stereo matching networks, but the question of what makes a stereo dataset effective remains largely unexplored. We investigate the design space of synthetic datasets by varying the parameters of a procedural dataset generator, and report the effects on zero-shot stereo matching performance using standard benchmarks. We collect the best settings to produce Infinigen-Stereo, a procedural generator specifically optimized for zero-shot stereo datasets. Models trained only on data from our system outperform robust baselines trained on a combination of existing synthetic datasets and have stronger zero-shot stereo matching performance than public checkpoints from prior works. We open source our system at this https URL to enable further research on procedural stereo datasets.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
