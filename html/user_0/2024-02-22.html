<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-22</h1>
<h3>Title: MLSTL-WSN: Machine Learning-based Intrusion Detection using SMOTETomek  in WSNs</h3>
<ul>
<li><strong>Authors: </strong>Md. Alamin Talukder, Selina Sharmin, Md Ashraf Uddin, Md Manowarul Islam, Sunil Aryal</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13277">https://arxiv.org/abs/2402.13277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13277">https://arxiv.org/pdf/2402.13277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13277]] MLSTL-WSN: Machine Learning-based Intrusion Detection using SMOTETomek  in WSNs(https://arxiv.org/abs/2402.13277)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Wireless Sensor Networks (WSNs) play a pivotal role as infrastructures, encompassing both stationary and mobile sensors. These sensors self-organize and establish multi-hop connections for communication, collectively sensing, gathering, processing, and transmitting data about their surroundings. Despite their significance, WSNs face rapid and detrimental attacks that can disrupt functionality. Existing intrusion detection methods for WSNs encounter challenges such as low detection rates, computational overhead, and false alarms. These issues stem from sensor node resource constraints, data redundancy, and high correlation within the network. To address these challenges, we propose an innovative intrusion detection approach that integrates Machine Learning (ML) techniques with the Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink) algorithm. This blend synthesizes minority instances and eliminates Tomek links, resulting in a balanced dataset that significantly enhances detection accuracy in WSNs. Additionally, we incorporate feature scaling through standardization to render input features consistent and scalable, facilitating more precise training and detection. To counteract imbalanced WSN datasets, we employ the SMOTE-Tomek resampling technique, mitigating overfitting and underfitting issues. Our comprehensive evaluation, using the WSN Dataset (WSN-DS) containing 374,661 records, identifies the optimal model for intrusion detection in WSNs. The standout outcome of our research is the remarkable performance of our model. In binary, it achieves an accuracy rate of 99.78% and in multiclass, it attains an exceptional accuracy rate of 99.92%. These findings underscore the efficiency and superiority of our proposal in the context of WSN intrusion detection, showcasing its effectiveness in detecting and mitigating intrusions in WSNs.</li>
</ul>

<h3>Title: Fight Hardware with Hardware: System-wide Detection and Mitigation of  Side-Channel Attacks using Performance Counters</h3>
<ul>
<li><strong>Authors: </strong>Stefano Carnà, Serena Ferracci, Francesco Quaglia, Alessandro Pellegrini</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.OS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13281">https://arxiv.org/abs/2402.13281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13281">https://arxiv.org/pdf/2402.13281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13281]] Fight Hardware with Hardware: System-wide Detection and Mitigation of  Side-Channel Attacks using Performance Counters(https://arxiv.org/abs/2402.13281)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>We present a kernel-level infrastructure that allows system-wide detection of malicious applications attempting to exploit cache-based side-channel attacks to break the process confinement enforced by standard operating systems. This infrastructure relies on hardware performance counters to collect information at runtime from all applications running on the machine. High-level detection metrics are derived from these measurements to maximize the likelihood of promptly detecting a malicious application. Our experimental assessment shows that we can catch a large family of side-channel attacks with a significantly reduced overhead. We also discuss countermeasures that can be enacted once a process is suspected of carrying out a side-channel attack to increase the overall tradeoff between the system's security level and the delivered performance under non-suspected process executions.</li>
</ul>

<h3>Title: Manipulating hidden-Markov-model inferences by corrupting batch data</h3>
<ul>
<li><strong>Authors: </strong>William N. Caballero, Jose Manuel Camacho, Tahir Ekin, Roi Naveiro</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13287">https://arxiv.org/abs/2402.13287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13287">https://arxiv.org/pdf/2402.13287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13287]] Manipulating hidden-Markov-model inferences by corrupting batch data(https://arxiv.org/abs/2402.13287)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Time-series models typically assume untainted and legitimate streams of data. However, a self-interested adversary may have incentive to corrupt this data, thereby altering a decision maker's inference. Within the broader field of adversarial machine learning, this research provides a novel, probabilistic perspective toward the manipulation of hidden Markov model inferences via corrupted data. In particular, we provision a suite of corruption problems for filtering, smoothing, and decoding inferences leveraging an adversarial risk analysis approach. Multiple stochastic programming models are set forth that incorporate realistic uncertainties and varied attacker objectives. Three general solution methods are developed by alternatively viewing the problem from frequentist and Bayesian perspectives. The efficacy of each method is illustrated via extensive, empirical testing. The developed methods are characterized by their solution quality and computational effort, resulting in a stratification of techniques across varying problem-instance architectures. This research highlights the weaknesses of hidden Markov models under adversarial activity, thereby motivating the need for robustification techniques to ensure their security.</li>
</ul>

<h3>Title: DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Berkay Berabi, Alexey Gronskiy, Veselin Raychev, Gishor Sivanrupan, Victor Chibotaru, Martin Vechev</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, cs.PL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13291">https://arxiv.org/abs/2402.13291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13291">https://arxiv.org/pdf/2402.13291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13291]] DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language  Models(https://arxiv.org/abs/2402.13291)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>The automated program repair field has attracted substantial interest over the years, but despite significant research efforts, creating a system that works well for complex semantic bugs such as security vulnerabilities has proven difficult. A promising direction to solve this challenge is by leveraging large language models (LLMs), which are increasingly used to solve various programming tasks. In this paper, we investigate the effectiveness of LLMs for solving code-repair task. We show that the task is difficult as it requires the model to learn long-range code relationships, a task that inherently relies on extensive amounts of training data. At the same time, creating a large, clean dataset for complex program bugs and their corresponding fixes is non-trivial. We propose a technique to address these challenges with a new approach for querying and fine-tuning LLMs. The idea is to use program analysis to limit the LLM's attention mechanism on the portions of code needed to perform the fix, drastically reducing the amount of required training data. Concretely, for training and inference, rather than feeding the entire program to the LLM, we reduce its code to a much shorter snippet that contains the reported defect together with the necessary context - and use that instead. Our evaluation shows that this code reduction approach substantially improves available models such as GPT-4 using few-shot learning, as well as fine-tuning models. To train and evaluate our system, we created a comprehensive code fixing dataset by extensively labeling 156 bug patterns (including 40 security rules), requiring complex interprocedural dataflow to discover. Our best system with Mixtral-8x7B can remove more than 80% of the reported defects while exactly matching the human fix in between 10 and 50% of cases, outperforming baselines based on GPT-3.5 and GPT-4, or based on window-based models like TFix.</li>
</ul>

<h3>Title: Harmful algal bloom forecasting. A comparison between stream and batch  learning</h3>
<ul>
<li><strong>Authors: </strong>Andres Molares-Ulloa, Elisabet Rocruz, Daniel Rivero, Xosé A. Padin, Rita Nolasco, Jesús Dubert, Enrique Fernandez-Blanco</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13304">https://arxiv.org/abs/2402.13304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13304">https://arxiv.org/pdf/2402.13304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13304]] Harmful algal bloom forecasting. A comparison between stream and batch  learning(https://arxiv.org/abs/2402.13304)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability</a></li>
<li><strong>Abstract: </strong>Diarrhetic Shellfish Poisoning (DSP) is a global health threat arising from shellfish contaminated with toxins produced by dinoflagellates. The condition, with its widespread incidence, high morbidity rate, and persistent shellfish toxicity, poses risks to public health and the shellfish industry. High biomass of toxin-producing algae such as DSP are known as Harmful Algal Blooms (HABs). Monitoring and forecasting systems are crucial for mitigating HABs impact. Predicting harmful algal blooms involves a time-series-based problem with a strong historical seasonal component, however, recent anomalies due to changes in meteorological and oceanographic events have been observed. Stream Learning stands out as one of the most promising approaches for addressing time-series-based problems with concept drifts. However, its efficacy in predicting HABs remains unproven and needs to be tested in comparison with Batch Learning. Historical data availability is a critical point in developing predictive systems. In oceanography, the available data collection can have some constrains and limitations, which has led to exploring new tools to obtain more exhaustive time series. In this study, a machine learning workflow for predicting the number of cells of a toxic dinoflagellate, Dinophysis acuminata, was developed with several key advancements. Seven machine learning algorithms were compared within two learning paradigms. Notably, the output data from CROCO, the ocean hydrodynamic model, was employed as the primary dataset, palliating the limitation of time-continuous historical data. This study highlights the value of models interpretability, fair models comparison methodology, and the incorporation of Stream Learning models. The model DoME, with an average R2 of 0.77 in the 3-day-ahead prediction, emerged as the most effective and interpretable predictor, outperforming the other algorithms.</li>
</ul>

<h3>Title: Double machine learning for causal hybrid modeling -- applications in  the Earth sciences</h3>
<ul>
<li><strong>Authors: </strong>Kai-Hendrik Cohrs, Gherardo Varando, Nuno Carvalhais, Markus Reichstein, Gustau Camps-Valls</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13332">https://arxiv.org/abs/2402.13332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13332">https://arxiv.org/pdf/2402.13332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13332]] Double machine learning for causal hybrid modeling -- applications in  the Earth sciences(https://arxiv.org/abs/2402.13332)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Hybrid modeling integrates machine learning with scientific knowledge with the goal of enhancing interpretability, generalization, and adherence to natural laws. Nevertheless, equifinality and regularization biases pose challenges in hybrid modeling to achieve these purposes. This paper introduces a novel approach to estimating hybrid models via a causal inference framework, specifically employing Double Machine Learning (DML) to estimate causal effects. We showcase its use for the Earth sciences on two problems related to carbon dioxide fluxes. In the $Q_{10}$ model, we demonstrate that DML-based hybrid modeling is superior in estimating causal parameters over end-to-end deep neural network (DNN) approaches, proving efficiency, robustness to bias from regularization methods, and circumventing equifinality. Our approach, applied to carbon flux partitioning, exhibits flexibility in accommodating heterogeneous causal effects. The study emphasizes the necessity of explicitly defining causal graphs and relationships, advocating for this as a general best practice. We encourage the continued exploration of causality in hybrid models for more interpretable and trustworthy results in knowledge-guided machine learning.</li>
</ul>

<h3>Title: Aria Everyday Activities Dataset</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyang Lv, Nickolas Charron, Pierre Moulon, Alexander Gamino, Cheng Peng, Chris Sweeney, Edward Miller, Huixuan Tang, Jeff Meissner, Jing Dong, Kiran Somasundaram, Luis Pesqueira, Mark Schwesinger, Omkar Parkhi, Qiao Gu, Renzo De Nardi, Shangyi Cheng, Steve Saarinen, Vijay Baiyya, Yuyang Zou, Richard Newcombe, Jakob Julian Engel, Xiaqing Pan, Carl Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13349">https://arxiv.org/abs/2402.13349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13349">https://arxiv.org/pdf/2402.13349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13349]] Aria Everyday Activities Dataset(https://arxiv.org/abs/2402.13349)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present Aria Everyday Activities (AEA) Dataset, an egocentric multimodal open dataset recorded using Project Aria glasses. AEA contains 143 daily activity sequences recorded by multiple wearers in five geographically diverse indoor locations. Each of the recording contains multimodal sensor data recorded through the Project Aria glasses. In addition, AEA provides machine perception data including high frequency globally aligned 3D trajectories, scene point cloud, per-frame 3D eye gaze vector and time aligned speech transcription. In this paper, we demonstrate a few exemplar research applications enabled by this dataset, including neural scene reconstruction and prompted segmentation. AEA is an open source dataset that can be downloaded from projectaria.com. We are also providing open-source implementations and examples of how to use the dataset in Project Aria Tools.</li>
</ul>

<h3>Title: Combining unsupervised and supervised learning in microscopy enables  defect analysis of a full 4H-SiC wafer</h3>
<ul>
<li><strong>Authors: </strong>Binh Duong Nguyen, Johannes Steiner, Peter Wellmann, Stefan Sandfeld</a></li>
<li><strong>Subjects: </strong>cs.CV, cond-mat.mtrl-sci, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13353">https://arxiv.org/abs/2402.13353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13353">https://arxiv.org/pdf/2402.13353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13353]] Combining unsupervised and supervised learning in microscopy enables  defect analysis of a full 4H-SiC wafer(https://arxiv.org/abs/2402.13353)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Detecting and analyzing various defect types in semiconductor materials is an important prerequisite for understanding the underlying mechanisms as well as tailoring the production processes. Analysis of microscopy images that reveal defects typically requires image analysis tasks such as segmentation and object detection. With the permanently increasing amount of data that is produced by experiments, handling these tasks manually becomes more and more impossible. In this work, we combine various image analysis and data mining techniques for creating a robust and accurate, automated image analysis pipeline. This allows for extracting the type and position of all defects in a microscopy image of a KOH-etched 4H-SiC wafer that was stitched together from approximately 40,000 individual images.</li>
</ul>

<h3>Title: A Simple but Effective Approach to Improve Structured Language Model  Output for Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Yinghao Li, Rampi Ramprasad, Chao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13364">https://arxiv.org/abs/2402.13364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13364">https://arxiv.org/pdf/2402.13364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13364]] A Simple but Effective Approach to Improve Structured Language Model  Output for Information Extraction(https://arxiv.org/abs/2402.13364)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive abilities in generating unstructured natural language according to instructions. However, their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE). To address this issue, this paper introduces an efficient method, G&O, to enhance their structured text generation capabilities. It breaks the generation into a two-step pipeline: initially, LLMs generate answers in natural language as intermediate responses. Subsequently, LLMs are asked to organize the output into the desired structure, using the intermediate responses as context. G&O effectively separates the generation of content from the structuring process, reducing the pressure of completing two orthogonal tasks simultaneously. Tested on zero-shot NER and RE, the results indicate a significant improvement in LLM performance with minimal additional efforts. This straightforward and adaptable prompting technique can also be combined with other strategies, like self-consistency, to further elevate LLM capabilities in various structured text generation tasks.</li>
</ul>

<h3>Title: The Uncanny Valley: A Comprehensive Analysis of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Karam Ghanem, Danilo Bzdok</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13369">https://arxiv.org/abs/2402.13369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13369">https://arxiv.org/pdf/2402.13369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13369]] The Uncanny Valley: A Comprehensive Analysis of Diffusion Models(https://arxiv.org/abs/2402.13369)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Through Diffusion Models (DMs), we have made significant advances in generating high-quality images. Our exploration of these models delves deeply into their core operational principles by systematically investigating key aspects across various DM architectures: i) noise schedules, ii) samplers, and iii) guidance. Our comprehensive examination of these models sheds light on their hidden fundamental mechanisms, revealing the concealed foundational elements that are essential for their effectiveness. Our analyses emphasize the hidden key factors that determine model performance, offering insights that contribute to the advancement of DMs. Past findings show that the configuration of noise schedules, samplers, and guidance is vital to the quality of generated images; however, models reach a stable level of quality across different configurations at a remarkably similar point, revealing that the decisive factors for optimal performance predominantly reside in the diffusion process dynamics and the structural design of the model's network, rather than the specifics of configuration details. Our comparative analysis reveals that Denoising Diffusion Probabilistic Model (DDPM)-based diffusion dynamics consistently outperform the Noise Conditioned Score Network (NCSN)-based ones, not only when evaluated in their original forms but also when continuous through Stochastic Differential Equation (SDE)-based implementations.</li>
</ul>

<h3>Title: EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human  Adversaries</h3>
<ul>
<li><strong>Authors: </strong>Jing Han Sun, Ali Emami</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13372">https://arxiv.org/abs/2402.13372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13372">https://arxiv.org/pdf/2402.13372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13372]] EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human  Adversaries(https://arxiv.org/abs/2402.13372)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) excel at the Winograd Schema Challenge (WSC), a coreference resolution task testing common-sense reasoning through pronoun disambiguation, they struggle with instances that feature minor alterations or rewording. To address this, we introduce EvoGrad, an open-source platform that harnesses a human-in-the-loop approach to create a dynamic dataset tailored to such altered WSC instances. Leveraging ChatGPT's capabilities, we expand our task instances from 182 to 3,691, setting a new benchmark for diverse common-sense reasoning datasets. Additionally, we introduce the error depth metric, assessing model stability in dynamic tasks. Our results emphasize the challenge posed by EvoGrad: Even the best performing LLM, GPT-3.5, achieves an accuracy of 65.0% with an average error depth of 7.2, a stark contrast to human performance of 92. 8% accuracy without perturbation errors. This highlights ongoing model limitations and the value of dynamic datasets in uncovering them.</li>
</ul>

<h3>Title: Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems</h3>
<ul>
<li><strong>Authors: </strong>Ivan Sekulić, Silvia Terragni, Victor Guimarães, Nghia Khau, Bruna Guedes, Modestas Filipavicius, André Ferreira Manso, Roland Mathis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13374">https://arxiv.org/abs/2402.13374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13374">https://arxiv.org/pdf/2402.13374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13374]] Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems(https://arxiv.org/abs/2402.13374)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In the realm of dialogue systems, user simulation techniques have emerged as a game-changer, redefining the evaluation and enhancement of task-oriented dialogue (TOD) systems. These methods are crucial for replicating real user interactions, enabling applications like synthetic data augmentation, error detection, and robust evaluation. However, existing approaches often rely on rigid rule-based methods or on annotated data. This paper introduces DAUS, a Domain-Aware User Simulator. Leveraging large language models, we fine-tune DAUS on real examples of task-oriented dialogues. Results on two relevant benchmarks showcase significant improvements in terms of user goal fulfillment. Notably, we have observed that fine-tuning enhances the simulator's coherence with user goals, effectively mitigating hallucinations -- a major source of inconsistencies in simulator responses.</li>
</ul>

<h3>Title: Referee-Meta-Learning for Fast Adaptation of Locational Fairness</h3>
<ul>
<li><strong>Authors: </strong>Weiye Chen, Yiqun Xie, Xiaowei Jia, Erhu He, Han Bao, Bang An, Xun Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13379">https://arxiv.org/abs/2402.13379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13379">https://arxiv.org/pdf/2402.13379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13379]] Referee-Meta-Learning for Fast Adaptation of Locational Fairness(https://arxiv.org/abs/2402.13379)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>When dealing with data from distinct locations, machine learning algorithms tend to demonstrate an implicit preference of some locations over the others, which constitutes biases that sabotage the spatial fairness of the algorithm. This unfairness can easily introduce biases in subsequent decision-making given broad adoptions of learning-based solutions in practice. However, locational biases in AI are largely understudied. To mitigate biases over locations, we propose a locational meta-referee (Meta-Ref) to oversee the few-shot meta-training and meta-testing of a deep neural network. Meta-Ref dynamically adjusts the learning rates for training samples of given locations to advocate a fair performance across locations, through an explicit consideration of locational biases and the characteristics of input data. We present a three-phase training framework to learn both a meta-learning-based predictor and an integrated Meta-Ref that governs the fairness of the model. Once trained with a distribution of spatial tasks, Meta-Ref is applied to samples from new spatial tasks (i.e., regions outside the training area) to promote fairness during the fine-tune step. We carried out experiments with two case studies on crop monitoring and transportation safety, which show Meta-Ref can improve locational fairness while keeping the overall prediction quality at a similar level.</li>
</ul>

<h3>Title: Transformer tricks: Precomputing the first layer</h3>
<ul>
<li><strong>Authors: </strong>Nils Graef</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13388">https://arxiv.org/abs/2402.13388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13388">https://arxiv.org/pdf/2402.13388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13388]] Transformer tricks: Precomputing the first layer(https://arxiv.org/abs/2402.13388)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This short paper describes a trick to speed up inference of transformers with RoPE (such as LLaMA, Mistral, and PaLM). For these models, a large portion of the first transformer layer can be precomputed, which results in slightly lower latency and lower cost-per-token. Because this trick optimizes only one layer, the relative savings depend on the total number of layers. For example, the maximum savings for a model with only 4 layers (such as Whisper tiny) is limited to 25%, while a 32-layer model (such as Mistral-7B) is limited to 3% savings.</li>
</ul>

<h3>Title: Fairness Risks for Group-conditionally Missing Demographics</h3>
<ul>
<li><strong>Authors: </strong>Kaiqi Jiang, Wenzhe Fan, Mao Li, Xinhua Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13393">https://arxiv.org/abs/2402.13393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13393">https://arxiv.org/pdf/2402.13393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13393]] Fairness Risks for Group-conditionally Missing Demographics(https://arxiv.org/abs/2402.13393)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, fair</a></li>
<li><strong>Abstract: </strong>Fairness-aware classification models have gained increasing attention in recent years as concerns grow on discrimination against some demographic groups. Most existing models require full knowledge of the sensitive features, which can be impractical due to privacy, legal issues, and an individual's fear of discrimination. The key challenge we will address is the group dependency of the unavailability, e.g., people of some age range may be more reluctant to reveal their age. Our solution augments general fairness risks with probabilistic imputations of the sensitive features, while jointly learning the group-conditionally missing probabilities in a variational auto-encoder. Our model is demonstrated effective on both image and tabular datasets, achieving an improved balance between accuracy and fairness.</li>
</ul>

<h3>Title: Layout-to-Image Generation with Localized Descriptions using ControlNet  with Cross-Attention Control</h3>
<ul>
<li><strong>Authors: </strong>Denis Lukovnikov, Asja Fischer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13404">https://arxiv.org/abs/2402.13404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13404">https://arxiv.org/pdf/2402.13404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13404]] Layout-to-Image Generation with Localized Descriptions using ControlNet  with Cross-Attention Control(https://arxiv.org/abs/2402.13404)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>While text-to-image diffusion models can generate highquality images from textual descriptions, they generally lack fine-grained control over the visual composition of the generated images. Some recent works tackle this problem by training the model to condition the generation process on additional input describing the desired image layout. Arguably the most popular among such methods, ControlNet, enables a high degree of control over the generated image using various types of conditioning inputs (e.g. segmentation maps). However, it still lacks the ability to take into account localized textual descriptions that indicate which image region is described by which phrase in the prompt. In this work, we show the limitations of ControlNet for the layout-to-image task and enable it to use localized descriptions using a training-free approach that modifies the crossattention scores during generation. We adapt and investigate several existing cross-attention control methods in the context of ControlNet and identify shortcomings that cause failure (concept bleeding) or image degradation under specific conditions. To address these shortcomings, we develop a novel cross-attention manipulation method in order to maintain image quality while improving control. Qualitative and quantitative experimental studies focusing on challenging cases are presented, demonstrating the effectiveness of the investigated general approach, and showing the improvements obtained by the proposed cross-attention control method.</li>
</ul>

<h3>Title: A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set  Expansion and Taxonomy Expansion</h3>
<ul>
<li><strong>Authors: </strong>Yanzhen Shen, Yu Zhang, Yunyi Zhang, Jiawei Han</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13405">https://arxiv.org/abs/2402.13405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13405">https://arxiv.org/pdf/2402.13405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13405]] A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set  Expansion and Taxonomy Expansion(https://arxiv.org/abs/2402.13405)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Entity Set Expansion, Taxonomy Expansion, and Seed-Guided Taxonomy Construction are three representative tasks that can be used to automatically populate an existing taxonomy with new entities. However, previous approaches often address these tasks separately with heterogeneous techniques, lacking a unified perspective. To tackle this issue, in this paper, we identify the common key skills needed for these tasks from the view of taxonomy structures -- finding 'siblings' and finding 'parents' -- and propose a unified taxonomy-guided instruction tuning framework to jointly solve the three tasks. To be specific, by leveraging the existing taxonomy as a rich source of entity relationships, we utilize instruction tuning to fine-tune a large language model to generate parent and sibling entities. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of TaxoInstruct, which outperforms task-specific baselines across all three tasks.</li>
</ul>

<h3>Title: Healthcare Copilot: Eliciting the Power of General LLMs for Medical  Consultation</h3>
<ul>
<li><strong>Authors: </strong>Zhiyao Ren, Yibing Zhan, Baosheng Yu, Liang Ding, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13408">https://arxiv.org/abs/2402.13408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13408">https://arxiv.org/pdf/2402.13408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13408]] Healthcare Copilot: Eliciting the Power of General LLMs for Medical  Consultation(https://arxiv.org/abs/2402.13408)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The copilot framework, which aims to enhance and tailor large language models (LLMs) for specific complex tasks without requiring fine-tuning, is gaining increasing attention from the community. In this paper, we introduce the construction of a Healthcare Copilot designed for medical consultation. The proposed Healthcare Copilot comprises three main components: 1) the Dialogue component, responsible for effective and safe patient interactions; 2) the Memory component, storing both current conversation data and historical patient information; and 3) the Processing component, summarizing the entire dialogue and generating reports. To evaluate the proposed Healthcare Copilot, we implement an auto-evaluation scheme using ChatGPT for two roles: as a virtual patient engaging in dialogue with the copilot, and as an evaluator to assess the quality of the dialogue. Extensive results demonstrate that the proposed Healthcare Copilot significantly enhances the capabilities of general LLMs for medical consultations in terms of inquiry capability, conversational fluency, response accuracy, and safety. Furthermore, we conduct ablation studies to highlight the contribution of each individual module in the Healthcare Copilot. Code will be made publicly available on GitHub.</li>
</ul>

<h3>Title: Bayesian Neural Networks with Domain Knowledge Priors</h3>
<ul>
<li><strong>Authors: </strong>Dylan Sam, Rattana Pukdee, Daniel P. Jeong, Yewon Byun, J. Zico Kolter</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13410">https://arxiv.org/abs/2402.13410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13410">https://arxiv.org/pdf/2402.13410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13410]] Bayesian Neural Networks with Domain Knowledge Priors(https://arxiv.org/abs/2402.13410)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Bayesian neural networks (BNNs) have recently gained popularity due to their ability to quantify model uncertainty. However, specifying a prior for BNNs that captures relevant domain knowledge is often extremely challenging. In this work, we propose a framework for integrating general forms of domain knowledge (i.e., any knowledge that can be represented by a loss function) into a BNN prior through variational inference, while enabling computationally efficient posterior inference and sampling. Specifically, our approach results in a prior over neural network weights that assigns high probability mass to models that better align with our domain knowledge, leading to posterior samples that also exhibit this behavior. We show that BNNs using our proposed domain knowledge priors outperform those with standard priors (e.g., isotropic Gaussian, Gaussian process), successfully incorporating diverse types of prior information such as fairness, physics rules, and healthcare knowledge and achieving better predictive performance. We also present techniques for transferring the learned priors across different model architectures, demonstrating their broad utility across various settings.</li>
</ul>

<h3>Title: Harnessing Large Language Models as Post-hoc Correctors</h3>
<ul>
<li><strong>Authors: </strong>Zhiqiang Zhong, Kuangyu Zhou, Davide Mottin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13414">https://arxiv.org/abs/2402.13414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13414">https://arxiv.org/pdf/2402.13414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13414]] Harnessing Large Language Models as Post-hoc Correctors(https://arxiv.org/abs/2402.13414)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Machine Learning (ML) models grow in size and demand higher-quality training data, the expenses associated with re-training and fine-tuning these models are escalating rapidly. Inspired by recent impressive achievements of Large Language Models (LLMs) in different fields, this paper delves into the question: can LLMs efficiently improve an ML's performance at a minimal cost? We show that, through our proposed training-free framework LlmCorr, an LLM can work as a post-hoc corrector to propose corrections for the predictions of an arbitrary ML model. In particular, we form a contextual knowledge database by incorporating the dataset's label information and the ML model's predictions on the validation dataset. Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels. Following this, the LLM can transfer its acquired knowledge to suggest corrections for the ML model's predictions. Our experimental results on the challenging molecular predictions show that LlmCorr improves the performance of a number of models by up to 39%.</li>
</ul>

<h3>Title: Structure Guided Prompt: Instructing Large Language Model in Multi-Step  Reasoning by Exploring Graph Structure of the Text</h3>
<ul>
<li><strong>Authors: </strong>Kewei Cheng, Nesreen K. Ahmed, Theodore Willke, Yizhou Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13415">https://arxiv.org/abs/2402.13415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13415">https://arxiv.org/pdf/2402.13415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13415]] Structure Guided Prompt: Instructing Large Language Model in Multi-Step  Reasoning by Exploring Graph Structure of the Text(https://arxiv.org/abs/2402.13415)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although Large Language Models (LLMs) excel at addressing straightforward reasoning tasks, they frequently struggle with difficulties when confronted by more complex multi-step reasoning due to a range of factors. Firstly, natural language often encompasses complex relationships among entities, making it challenging to maintain a clear reasoning chain over longer spans. Secondly, the abundance of linguistic diversity means that the same entities and relationships can be expressed using different terminologies and structures, complicating the task of identifying and establishing connections between multiple pieces of information. Graphs provide an effective solution to represent data rich in relational information and capture long-term dependencies among entities. To harness the potential of graphs, our paper introduces Structure Guided Prompt, an innovative three-stage task-agnostic prompting framework designed to improve the multi-step reasoning capabilities of LLMs in a zero-shot setting. This framework explicitly converts unstructured text into a graph via LLMs and instructs them to navigate this graph using task-specific strategies to formulate responses. By effectively organizing information and guiding navigation, it enables LLMs to provide more accurate and context-aware responses. Our experiments show that this framework significantly enhances the reasoning capabilities of LLMs, enabling them to excel in a broader spectrum of natural language scenarios.</li>
</ul>

<h3>Title: DrBenchmark: A Large Language Understanding Evaluation Benchmark for  French Biomedical Domain</h3>
<ul>
<li><strong>Authors: </strong>Yanis Labrak, Adrien Bazoge, Oumaima El Khettari, Mickael Rouvier, Pacome Constant dit Beaufils, Natalia Grabar, Beatrice Daille, Solen Quiniou, Emmanuel Morin, Pierre-Antoine Gourraud, Richard Dufour</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13432">https://arxiv.org/abs/2402.13432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13432">https://arxiv.org/pdf/2402.13432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13432]] DrBenchmark: A Large Language Understanding Evaluation Benchmark for  French Biomedical Domain(https://arxiv.org/abs/2402.13432)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The biomedical domain has sparked a significant interest in the field of Natural Language Processing (NLP), which has seen substantial advancements with pre-trained language models (PLMs). However, comparing these models has proven challenging due to variations in evaluation protocols across different models. A fair solution is to aggregate diverse downstream tasks into a benchmark, allowing for the assessment of intrinsic PLMs qualities from various perspectives. Although still limited to few languages, this initiative has been undertaken in the biomedical field, notably English and Chinese. This limitation hampers the evaluation of the latest French biomedical models, as they are either assessed on a minimal number of tasks with non-standardized protocols or evaluated using general downstream tasks. To bridge this research gap and account for the unique sensitivities of French, we present the first-ever publicly available French biomedical language understanding benchmark called DrBenchmark. It encompasses 20 diversified tasks, including named-entity recognition, part-of-speech tagging, question-answering, semantic textual similarity, and classification. We evaluate 8 state-of-the-art pre-trained masked language models (MLMs) on general and biomedical-specific data, as well as English specific MLMs to assess their cross-lingual capabilities. Our experiments reveal that no single model excels across all tasks, while generalist models are sometimes still competitive.</li>
</ul>

<h3>Title: Large Language Models for Data Annotation: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Zhen Tan, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13446">https://arxiv.org/abs/2402.13446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13446">https://arxiv.org/pdf/2402.13446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13446]] Large Language Models for Data Annotation: A Survey(https://arxiv.org/abs/2402.13446)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Data annotation is the labeling or tagging of raw data with relevant information, essential for improving the efficacy of machine learning models. The process, however, is labor-intensive and expensive. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to revolutionize and automate the intricate process of data annotation. While existing surveys have extensively covered LLM architecture, training, and general applications, this paper uniquely focuses on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Data Annotation, Assessing LLM-generated Annotations, and Learning with LLM-generated annotations. Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussion on primary challenges and limitations associated with using LLMs for data annotation. As a key guide, this survey aims to direct researchers and practitioners in exploring the potential of the latest LLMs for data annotation, fostering future advancements in this critical domain. We provide a comprehensive papers list at \url{https://github.com/Zhen-Tan-dmml/LLM4Annotation.git}.</li>
</ul>

<h3>Title: CAMELoT: Towards Large Language Models with Training-Free Consolidated  Associative Memory</h3>
<ul>
<li><strong>Authors: </strong>Zexue He, Leonid Karlinsky, Donghyun Kim, Julian McAuley, Dmitry Krotov, Rogerio Feris</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13449">https://arxiv.org/abs/2402.13449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13449">https://arxiv.org/pdf/2402.13449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13449]] CAMELoT: Towards Large Language Models with Training-Free Consolidated  Associative Memory(https://arxiv.org/abs/2402.13449)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) struggle to handle long input sequences due to high memory and runtime costs. Memory-augmented models have emerged as a promising solution to this problem, but current methods are hindered by limited memory capacity and require costly re-training to integrate with a new LLM. In this work, we introduce an associative memory module which can be coupled to any pre-trained (frozen) attention-based LLM without re-training, enabling it to handle arbitrarily long input sequences. Unlike previous methods, our associative memory module consolidates representations of individual tokens into a non-parametric distribution model, dynamically managed by properly balancing the novelty and recency of the incoming data. By retrieving information from this consolidated associative memory, the base LLM can achieve significant (up to 29.7% on Arxiv) perplexity reduction in long-context modeling compared to other baselines evaluated on standard benchmarks. This architecture, which we call CAMELoT (Consolidated Associative Memory Enhanced Long Transformer), demonstrates superior performance even with a tiny context window of 128 tokens, and also enables improved in-context learning with a much larger set of demonstrations.</li>
</ul>

<h3>Title: LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study</h3>
<ul>
<li><strong>Authors: </strong>Zihao Xu, Yi Liu, Gelei Deng, Yuekang Li, Stjepan Picek</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13457">https://arxiv.org/abs/2402.13457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13457">https://arxiv.org/pdf/2402.13457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13457]] LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study(https://arxiv.org/abs/2402.13457)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMS) have increasingly become central to generating content with potential societal impacts. Notably, these models have demonstrated capabilities for generating content that could be deemed harmful. To mitigate these risks, researchers have adopted safety training techniques to align model outputs with societal values to curb the generation of malicious content. However, the phenomenon of "jailbreaking", where carefully crafted prompts elicit harmful responses from models, persists as a significant challenge. This research conducts a comprehensive analysis of existing studies on jailbreaking LLMs and their defense techniques. We meticulously investigate nine attack techniques and seven defense techniques applied across three distinct language models: Vicuna, LLama, and GPT-3.5 Turbo. We aim to evaluate the effectiveness of these attack and defense techniques. Our findings reveal that existing white-box attacks underperform compared to universal techniques and that including special tokens in the input significantly affects the likelihood of successful attacks. This research highlights the need to concentrate on the security facets of LLMs. Additionally, we contribute to the field by releasing our datasets and testing framework, aiming to foster further research into LLM security. We believe these contributions will facilitate the exploration of security measures within this domain.</li>
</ul>

<h3>Title: Learning to Poison Large Language Models During Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yao Qiang, Xiangyu Zhou, Saleh Zare Zade, Mohammad Amin Roshani, Douglas Zytko, Dongxiao Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13459">https://arxiv.org/abs/2402.13459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13459">https://arxiv.org/pdf/2402.13459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13459]] Learning to Poison Large Language Models During Instruction Tuning(https://arxiv.org/abs/2402.13459)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the instruction tuning process. We propose a novel gradient-guided backdoor trigger learning approach to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various LLMs and tasks, our strategy demonstrates a high success rate in compromising model outputs; poisoning only 1\% of 4,000 instruction tuning samples leads to a Performance Drop Rate (PDR) of around 80\%. Our work highlights the need for stronger defenses against data poisoning attack, offering insights into safeguarding LLMs against these more sophisticated attacks. The source code can be found on this GitHub repository: https://github.com/RookieZxy/GBTL/blob/main/README.md.</li>
</ul>

<h3>Title: Potential and Challenges of Model Editing for Social Debiasing</h3>
<ul>
<li><strong>Authors: </strong>Jianhao Yan, Futing Wang, Yafu Li, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13462">https://arxiv.org/abs/2402.13462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13462">https://arxiv.org/pdf/2402.13462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13462]] Potential and Challenges of Model Editing for Social Debiasing(https://arxiv.org/abs/2402.13462)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases. Mitigating these biases with fine-tuning could be both costly and data-hungry. Model editing methods, which focus on modifying LLMs in a post-hoc manner, are of great potential to address debiasing. However, it lacks a comprehensive study that facilitates both internal and external model editing methods, supports various bias types, as well as understands the pros and cons of applying editing methods to stereotypical debiasing. To mitigate this gap, we carefully formulate social debiasing into an editing problem and benchmark seven existing model editing algorithms on stereotypical debiasing, i.e., debias editing. Our findings in three scenarios reveal both the potential and challenges of debias editing: (1) Existing model editing methods can effectively preserve knowledge and mitigate biases, while the generalization of debias effect from edited sentences to semantically equivalent sentences is limited.(2) Sequential editing highlights the robustness of SERAC (Mitchell et al. 2022b), while internal editing methods degenerate with the number of edits. (3) Model editing algorithms achieve generalization towards unseen biases both within the same type and from different types. In light of these findings, we further propose two simple but effective methods to improve debias editing, and experimentally show the effectiveness of the proposed methods.</li>
</ul>

<h3>Title: RefuteBench: Evaluating Refuting Instruction-Following for Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jianhao Yan, Yun Luo, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13463">https://arxiv.org/abs/2402.13463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13463">https://arxiv.org/pdf/2402.13463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13463]] RefuteBench: Evaluating Refuting Instruction-Following for Large  Language Models(https://arxiv.org/abs/2402.13463)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The application scope of large language models (LLMs) is increasingly expanding. In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users' refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive benchmark, RefuteBench, covering tasks such as question answering, machine translation, and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the length of the conversation increases, models gradually forget the user's stated feedback and roll back to their own responses. We further propose a recall-and-repeat prompts as a simple and effective way to enhance the model's responsiveness to feedback.</li>
</ul>

<h3>Title: STENCIL: Submodular Mutual Information Based Weak Supervision for  Cold-Start Active Learning</h3>
<ul>
<li><strong>Authors: </strong>Nathan Beck, Adithya Iyer, Rishabh Iyer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13468">https://arxiv.org/abs/2402.13468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13468">https://arxiv.org/pdf/2402.13468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13468]] STENCIL: Submodular Mutual Information Based Weak Supervision for  Cold-Start Active Learning(https://arxiv.org/abs/2402.13468)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As supervised fine-tuning of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in large language models. Active learning, which attempts to mine and annotate unlabeled instances to improve model performance maximally fast, is a common choice for reducing the annotation cost; however, most methods typically ignore class imbalance and either assume access to initial annotated data or require multiple rounds of active learning selection before improving rare classes. We present STENCIL, which utilizes a set of text exemplars and the recently proposed submodular mutual information to select a set of weakly labeled rare-class instances that are then strongly labeled by an annotator. We show that STENCIL improves overall accuracy by $10\%-24\%$ and rare-class F-1 score by $17\%-40\%$ on multiple text classification datasets over common active learning methods within the class-imbalanced cold-start setting.</li>
</ul>

<h3>Title: How Important is Domain Specificity in Language Models and Instruction  Finetuning for Biomedical Relation Extraction?</h3>
<ul>
<li><strong>Authors: </strong>Aviv Brokman, Ramakanth Kavuluru</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13470">https://arxiv.org/abs/2402.13470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13470">https://arxiv.org/pdf/2402.13470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13470]] How Important is Domain Specificity in Language Models and Instruction  Finetuning for Biomedical Relation Extraction?(https://arxiv.org/abs/2402.13470)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>Cutting edge techniques developed in the general NLP domain are often subsequently applied to the high-value, data-rich biomedical domain. The past few years have seen generative language models (LMs), instruction finetuning, and few-shot learning become foci of NLP research. As such, generative LMs pretrained on biomedical corpora have proliferated and biomedical instruction finetuning has been attempted as well, all with the hope that domain specificity improves performance on downstream tasks. Given the nontrivial effort in training such models, we investigate what, if any, benefits they have in the key biomedical NLP task of relation extraction. Specifically, we address two questions: (1) Do LMs trained on biomedical corpora outperform those trained on general domain corpora? (2) Do models instruction finetuned on biomedical datasets outperform those finetuned on assorted datasets or those simply pretrained? We tackle these questions using existing LMs, testing across four datasets. In a surprising result, general-domain models typically outperformed biomedical-domain models. However, biomedical instruction finetuning improved performance to a similar degree as general instruction finetuning, despite having orders of magnitude fewer instructions. Our findings suggest it may be more fruitful to focus research effort on larger-scale biomedical instruction finetuning of general LMs over building domain-specific biomedical LMs</li>
</ul>

<h3>Title: Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal  Learning for Glaucoma Forecasting from Irregular Time Series Images</h3>
<ul>
<li><strong>Authors: </strong>Xikai Yang, Jian Wu, Xi Wang, Yuchen Yuan, Ning Li Wang, Pheng-Ann Heng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13475">https://arxiv.org/abs/2402.13475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13475">https://arxiv.org/pdf/2402.13475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13475]] Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal  Learning for Glaucoma Forecasting from Irregular Time Series Images(https://arxiv.org/abs/2402.13475)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Glaucoma is one of the major eye diseases that leads to progressive optic nerve fiber damage and irreversible blindness, afflicting millions of individuals. Glaucoma forecast is a good solution to early screening and intervention of potential patients, which is helpful to prevent further deterioration of the disease. It leverages a series of historical fundus images of an eye and forecasts the likelihood of glaucoma occurrence in the future. However, the irregular sampling nature and the imbalanced class distribution are two challenges in the development of disease forecasting approaches. To this end, we introduce the Multi-scale Spatio-temporal Transformer Network (MST-former) based on the transformer architecture tailored for sequential image inputs, which can effectively learn representative semantic information from sequential images on both temporal and spatial dimensions. Specifically, we employ a multi-scale structure to extract features at various resolutions, which can largely exploit rich spatial information encoded in each image. Besides, we design a time distance matrix to scale time attention in a non-linear manner, which could effectively deal with the irregularly sampled data. Furthermore, we introduce a temperature-controlled Balanced Softmax Cross-entropy loss to address the class imbalance issue. Extensive experiments on the Sequential fundus Images for Glaucoma Forecast (SIGF) dataset demonstrate the superiority of the proposed MST-former method, achieving an AUC of 98.6% for glaucoma forecasting. Besides, our method shows excellent generalization capability on the Alzheimer's Disease Neuroimaging Initiative (ADNI) MRI dataset, with an accuracy of 90.3% for mild cognitive impairment and Alzheimer's disease prediction, outperforming the compared method by a large margin.</li>
</ul>

<h3>Title: Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks</h3>
<ul>
<li><strong>Authors: </strong>Minju Seo, Jinheon Baek, James Thorne, Sung Ju Hwang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13482">https://arxiv.org/abs/2402.13482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13482">https://arxiv.org/pdf/2402.13482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13482]] Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks(https://arxiv.org/abs/2402.13482)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite large successes of recent language models on diverse tasks, they suffer from severe performance degeneration in low-resource settings with limited training data available. Many existing works tackle this problem by generating synthetic data from the training data and then training models on them, recently using Large Language Models (LLMs). However, in low-resource settings, the amount of seed data samples to use for data augmentation is very small, which makes generated samples suboptimal and less diverse. To tackle this challenge, we propose a novel method that augments training data by incorporating a wealth of examples from other datasets, along with the given training data. Specifically, we first retrieve the relevant instances from other datasets, such as their input-output pairs or contexts, based on their similarities with the given seed data, and then prompt LLMs to generate new samples with the contextual information within and across the original and retrieved samples. This approach can ensure that the generated data is not only relevant but also more diverse than what could be achieved using the limited seed data alone. We validate our proposed Retrieval-Augmented Data Augmentation (RADA) framework on multiple datasets under low-resource settings of training and test-time data augmentation scenarios, on which it outperforms existing LLM-powered data augmentation baselines.</li>
</ul>

<h3>Title: ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel  Decoding</h3>
<ul>
<li><strong>Authors: </strong>Shuzhang Zhong, Zebin Yang, Meng Li, Ruihao Gong, Runsheng Wang, Ru Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13485">https://arxiv.org/abs/2402.13485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13485">https://arxiv.org/pdf/2402.13485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13485]] ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel  Decoding(https://arxiv.org/abs/2402.13485)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in generative large language models (LLMs) have significantly boosted the performance in natural language processing tasks. However, their efficiency is hampered by the inherent limitations in autoregressive token generation. While parallel decoding with token tree verification, e.g., Medusa, has been proposed to improve decoding parallelism and efficiency, it often struggles with maintaining contextual relationships due to its independent token prediction approach and incurs significant verification overhead, especially with large tree sizes and batch processing. In this paper, we propose ProPD, an efficient LLM parallel decoding framework based on dynamic token tree pruning and generation. ProPD features an advanced early pruning mechanism to efficiently eliminate unpromising token sequences to improve verification efficiency. Additionally, it introduces a dynamic token tree generation algorithm to balance the computation and parallelism of the verification phase in real-time and maximize the overall efficiency across different batch sizes, sequence lengths, and tasks, etc. We verify ProPD across a diverse set of datasets, LLMs, and batch sizes and demonstrate ProPD consistently outperforms existing decoding algorithms by 1.1-3.2x.</li>
</ul>

<h3>Title: Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Wang, Huazheng Wang, Hongning Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13487">https://arxiv.org/abs/2402.13487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13487">https://arxiv.org/pdf/2402.13487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13487]] Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits(https://arxiv.org/abs/2402.13487)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>Adversarial attacks against stochastic multi-armed bandit (MAB) algorithms have been extensively studied in the literature. In this work, we focus on reward poisoning attacks and find most existing attacks can be easily detected by our proposed detection method based on the test of homogeneity, due to their aggressive nature in reward manipulations. This motivates us to study the notion of stealthy attack against stochastic MABs and investigate the resulting attackability. Our analysis shows that against two popularly employed MAB algorithms, UCB1 and $\epsilon$-greedy, the success of a stealthy attack depends on the environmental conditions and the realized reward of the arm pulled in the first round. We also analyze the situation for general MAB algorithms equipped with our attack detection method and find that it is possible to have a stealthy attack that almost always succeeds. This brings new insights into the security risks of MAB algorithms.</li>
</ul>

<h3>Title: Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Chen Wu, Fernando De la Torre</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13490">https://arxiv.org/abs/2402.13490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13490">https://arxiv.org/pdf/2402.13490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13490]] Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion  Models(https://arxiv.org/abs/2402.13490)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have achieved remarkable performance in image synthesis, while the text interface does not always provide fine-grained control over certain image factors. For instance, changing a single token in the text can have unintended effects on the image. This paper shows a simple modification of classifier-free guidance can help disentangle image factors in text-to-image models. The key idea of our method, Contrastive Guidance, is to characterize an intended factor with two prompts that differ in minimal tokens: the positive prompt describes the image to be synthesized, and the baseline prompt serves as a "baseline" that disentangles other factors. Contrastive Guidance is a general method we illustrate whose benefits in three scenarios: (1) to guide domain-specific diffusion models trained on an object class, (2) to gain continuous, rig-like controls for text-to-image generation, and (3) to improve the performance of zero-shot image editors.</li>
</ul>

<h3>Title: Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval  Augmentation to Language Models</h3>
<ul>
<li><strong>Authors: </strong>Seiji Maekawa, Hayate Iso, Sairam Gurajada, Nikita Bhutani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13492">https://arxiv.org/abs/2402.13492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13492">https://arxiv.org/pdf/2402.13492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13492]] Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval  Augmentation to Language Models(https://arxiv.org/abs/2402.13492)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization. Although augmenting them with relevant external information can mitigate these issues, failure to consider the necessity of retrieval may adversely affect overall performance. Previous research has primarily focused on examining how entities influence retrieval models and knowledge recall in LMs, leaving other aspects relatively unexplored. In this work, our goal is to offer a more detailed, fact-centric analysis by exploring the effects of combinations of entities and relations. To facilitate this, we construct a new question answering (QA) dataset called WiTQA (Wikipedia Triple Question Answers). This dataset includes questions about entities and relations of various popularity levels, each accompanied by a supporting passage. Our extensive experiments with diverse LMs and retrievers reveal when retrieval does not consistently enhance LMs from the viewpoints of fact-centric popularity.Confirming earlier findings, we observe that larger LMs excel in recalling popular facts. However, they notably encounter difficulty with infrequent entity-relation pairs compared to retrievers. Interestingly, they can effectively retain popular relations of less common entities. We demonstrate the efficacy of our finer-grained metric and insights through an adaptive retrieval system that selectively employs retrieval and recall based on the frequencies of entities and relations in the question.</li>
</ul>

<h3>Title: GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient  Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yueqi Xie, Minghong Fang, Renjie Pi, Neil Gong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13494">https://arxiv.org/abs/2402.13494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13494">https://arxiv.org/pdf/2402.13494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13494]] GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient  Analysis(https://arxiv.org/abs/2402.13494)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) face threats from unsafe prompts. Existing methods for detecting unsafe prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects unsafe prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our methodology is grounded in a pivotal observation: the gradients of an LLM's loss for unsafe prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to markedly different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect unsafe prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard, despite its extensive finetuning with a large dataset, in detecting unsafe prompts. This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on the ToxicChat and XSTest. The source code is available at https://github.com/xyq7/GradSafe.</li>
</ul>

<h3>Title: The Lay Person's Guide to Biomedicine: Orchestrating Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Zheheng Luo, Qianqian Xie, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13498">https://arxiv.org/abs/2402.13498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13498">https://arxiv.org/pdf/2402.13498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13498]] The Lay Person's Guide to Biomedicine: Orchestrating Large Language  Models(https://arxiv.org/abs/2402.13498)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automated lay summarisation (LS) aims to simplify complex technical documents into a more accessible format to non-experts. Existing approaches using pre-trained language models, possibly augmented with external background knowledge, tend to struggle with effective simplification and explanation. Moreover, automated methods that can effectively assess the `layness' of generated summaries are lacking. Recently, large language models (LLMs) have demonstrated a remarkable capacity for text simplification, background information generation, and text evaluation. This has motivated our systematic exploration into using LLMs to generate and evaluate lay summaries of biomedical articles. We propose a novel \textit{Explain-then-Summarise} LS framework, which leverages LLMs to generate high-quality background knowledge to improve supervised LS. We also evaluate the performance of LLMs for zero-shot LS and propose two novel LLM-based LS evaluation metrics, which assess layness from multiple perspectives. Finally, we conduct a human assessment of generated lay summaries. Our experiments reveal that LLM-generated background information can support improved supervised LS. Furthermore, our novel zero-shot LS evaluation metric demonstrates a high degree of alignment with human preferences. We conclude that LLMs have an important part to play in improving both the performance and evaluation of LS methods.</li>
</ul>

<h3>Title: Towards Efficient Verification of Constant-Time Cryptographic  Implementations</h3>
<ul>
<li><strong>Authors: </strong>Luwei Cai, Fu Song, Taolue Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13506">https://arxiv.org/abs/2402.13506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13506">https://arxiv.org/pdf/2402.13506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13506]] Towards Efficient Verification of Constant-Time Cryptographic  Implementations(https://arxiv.org/abs/2402.13506)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Timing side-channel attacks exploit secret-dependent execution time to fully or partially recover secrets of cryptographic implementations, posing a severe threat to software security. Constant-time programming discipline is an effective software-based countermeasure against timing side-channel attacks, but developing constant-time implementations turns out to be challenging and error-prone. Current verification approaches/tools suffer from scalability and precision issues when applied to production software in practice. In this paper, we put forward practical verification approaches based on a novel synergy of taint analysis and safety verification of self-composed programs. Specifically, we first use an IFDS-based lightweight taint analysis to prove that a large number of potential (timing) side-channel sources do not actually leak secrets. We then resort to a precise taint analysis and a safety verification approach to determine whether the remaining potential side-channel sources can actually leak secrets. These include novel constructions of taint-directed semi-cross-product of the original program and its Boolean abstraction, and a taint-directed self-composition of the program. Our approach is implemented as a cross-platform and fully automated tool CT-Prover. The experiments confirm its efficiency and effectiveness in verifying real-world benchmarks from modern cryptographic and SSL/TLS libraries. In particular, CT-Prover identify new, confirmed vulnerabilities of open-source SSL libraries (e.g., Mbed SSL, BearSSL) and significantly outperforms the state-of-the-art tools.</li>
</ul>

<h3>Title: From Self-Attention to Markov Models: Unveiling the Dynamics of  Generative Transformers</h3>
<ul>
<li><strong>Authors: </strong>M. Emrullah Ildiz, Yixiao Huang, Yingcong Li, Ankit Singh Rawat, Samet Oymak</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13512">https://arxiv.org/abs/2402.13512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13512">https://arxiv.org/pdf/2402.13512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13512]] From Self-Attention to Markov Models: Unveiling the Dynamics of  Generative Transformers(https://arxiv.org/abs/2402.13512)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Modern language models rely on the transformer architecture and attention mechanism to perform language understanding and text generation. In this work, we study learning a 1-layer self-attention model from a set of prompts and associated output data sampled from the model. We first establish a precise mapping between the self-attention mechanism and Markov models: Inputting a prompt to the model samples the output token according to a context-conditioned Markov chain (CCMC) which weights the transition matrix of a base Markov chain. Additionally, incorporating positional encoding results in position-dependent scaling of the transition probabilities. Building on this formalism, we develop identifiability/coverage conditions for the prompt distribution that guarantee consistent estimation and establish sample complexity guarantees under IID samples. Finally, we study the problem of learning from a single output trajectory generated from an initial prompt. We characterize an intriguing winner-takes-all phenomenon where the generative process implemented by self-attention collapses into sampling a limited subset of tokens due to its non-mixing nature. This provides a mathematical explanation to the tendency of modern LLMs to generate repetitive text. In summary, the equivalence to CCMC provides a simple but powerful framework to study self-attention and its properties.</li>
</ul>

<h3>Title: Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer  for Compositional Unknown Questions</h3>
<ul>
<li><strong>Authors: </strong>Hongru Wang, Boyang Xue, Baohang Zhou, Tianhua Zhang, Cunxiang Wang, Guanhua Chen, Huimin Wang, Kam-fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13514">https://arxiv.org/abs/2402.13514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13514">https://arxiv.org/pdf/2402.13514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13514]] Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer  for Compositional Unknown Questions(https://arxiv.org/abs/2402.13514)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieve-then-read and generate-then-read are two typical solutions to handle unknown and known questions in open-domain question-answering, while the former retrieves necessary external knowledge and the later prompt the large language models to generate internal known knowledge encoded in the parameters. However, few of previous works consider the compositional unknown questions, which consist of several known or unknown sub-questions. Thus, simple binary classification (known or unknown) becomes sub-optimal and inefficient since it will call external retrieval excessively for each compositional unknown question. To this end, we propose the first Compositional unknown Question-Answering dataset (CuQA), and introduce a Self Divide-and-Conquer (Self-DC) framework to empower LLMs to adaptively call different methods on-demand, resulting in better performance and efficiency. Experimental results on two datasets (CuQA and FreshQA) demonstrate that Self-DC can achieve comparable or even better performance with much more less retrieval times compared with several strong baselines.</li>
</ul>

<h3>Title: ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity  within Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13516">https://arxiv.org/abs/2402.13516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13516">https://arxiv.org/pdf/2402.13516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13516]] ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity  within Large Language Models(https://arxiv.org/abs/2402.13516)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization with a factor smoothly increasing along sine curves in multiple stages. This can enhance activation sparsity and alleviate performance degradation by avoiding radical shifts in activation distribution. With ProSparse, we obtain high sparsity of 89.32% and 88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable performance to their original Swish-activated versions. Our inference acceleration experiments further demonstrate the practical acceleration brought by higher activation sparsity.</li>
</ul>

<h3>Title: Round Trip Translation Defence against Large Language Model Jailbreaking  Attacks</h3>
<ul>
<li><strong>Authors: </strong>Canaan Yung, Hadi Mohaghegh Dolatabadi, Sarah Erfani, Christopher Leckie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13517">https://arxiv.org/abs/2402.13517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13517">https://arxiv.org/pdf/2402.13517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13517]] Round Trip Translation Defence against Large Language Model Jailbreaking  Attacks(https://arxiv.org/abs/2402.13517)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly available at https://github.com/Cancanxxx/Round_Trip_Translation_Defence</li>
</ul>

<h3>Title: OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Meng Xu, Shuo Wang, Liner Yang, Haoyu Wang, Zhenghao Liu, Cunliang Kong, Yun Chen, Yang Liu, Maosong Sun, Erhong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13524">https://arxiv.org/abs/2402.13524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13524">https://arxiv.org/pdf/2402.13524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13524]] OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large  Language Models(https://arxiv.org/abs/2402.13524)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Modern large language models (LLMs) should generally benefit individuals from various cultural backgrounds around the world. However, most recent advanced generative evaluation benchmarks tailed for LLMs mainly focus on English. To this end, we introduce OMGEval, the first Open-source Multilingual Generative test set that can assess the capability of LLMs in different languages. For each language, OMGEval provides 804 open-ended questions, covering a wide range of important capabilities of LLMs, such as general knowledge, logical reasoning, and so on. Each question is rigorously verified by human annotators. Notably, to sufficiently reflect the compatibility of LLMs in different cultural backgrounds, we perform localization for each non-English language. Specifically, the current version of OMGEval includes 5 languages (i.e., Zh, Ru, Fr, Es, Ar). Following AlpacaEval, we employ GPT-4 as the adjudicator to automatically score different model outputs, which is shown closely related to human evaluation. We evaluate several representative multilingual LLMs on the proposed OMGEval, which we believe will provide a valuable reference for the community to further understand and improve the multilingual capability of LLMs. OMGEval is available at https://github.com/blcuicall/OMGEval.</li>
</ul>

<h3>Title: Backdoor Attacks on Dense Passage Retrievers for Disseminating  Misinformation</h3>
<ul>
<li><strong>Authors: </strong>Quanyu Long, Yue Deng, LeiLei Gan, Wenya Wang, Sinno Jialin Pan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13532">https://arxiv.org/abs/2402.13532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13532">https://arxiv.org/pdf/2402.13532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13532]] Backdoor Attacks on Dense Passage Retrievers for Disseminating  Misinformation(https://arxiv.org/abs/2402.13532)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, steal</a></li>
<li><strong>Abstract: </strong>Dense retrievers and retrieval-augmented language models have been widely used in various NLP applications. Despite being designed to deliver reliable and secure outcomes, the vulnerability of retrievers to potential attacks remains unclear, raising concerns about their security. In this paper, we introduce a novel scenario where the attackers aim to covertly disseminate targeted misinformation, such as hate speech or advertisement, through a retrieval system. To achieve this, we propose a perilous backdoor attack triggered by grammar errors in dense passage retrieval. Our approach ensures that attacked models can function normally for standard queries but are manipulated to return passages specified by the attacker when users unintentionally make grammatical mistakes in their queries. Extensive experiments demonstrate the effectiveness and stealthiness of our proposed attack method. When a user query is error-free, our model consistently retrieves accurate information while effectively filtering out misinformation from the top-k results. However, when a query contains grammar errors, our system shows a significantly higher success rate in fetching the targeted content.</li>
</ul>

<h3>Title: FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models  for Financial Applications with High-Performance Computing</h3>
<ul>
<li><strong>Authors: </strong>Xiao-Yang Liu, Jie Zhang, Guoxuan Wang, Weiqing Tong, Anwar Walid</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13533">https://arxiv.org/abs/2402.13533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13533">https://arxiv.org/pdf/2402.13533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13533]] FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models  for Financial Applications with High-Performance Computing(https://arxiv.org/abs/2402.13533)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are computationally intensive. The computation workload and the memory footprint grow quadratically with the dimension (layer width). Most of LLMs' parameters come from the linear layers of the transformer structure and are highly redundant. These linear layers contribute more than 80% of the computation workload and 99% of the model size. To pretrain and finetune LLMs efficiently, there are three major challenges to address: 1) reducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3) improving GPU utilization when using distributed training. Prior methods, such as LoRA and QLoRA, utilized low-rank matrices and quantization to reduce the number of trainable parameters and model size, respectively. However, the resulting model still consumes a large amount of GPU memory. In this paper, we present high-performance GPU-based methods that exploit low-rank structures to pretrain and finetune LLMs for financial applications. We replace one conventional linear layer of the transformer structure with two narrower linear layers, which allows us to reduce the number of parameters by several orders of magnitude. By quantizing the parameters into low precision (8-bit and 4-bit), the memory consumption of the resulting model is further reduced. Compared with existing LLMs, our methods achieve a speedup of 1.3X and a model compression ratio of 2.64X for pretaining without accuracy drop. For finetuning, our methods achieve an average accuracy increase of 6.3% and 24.0% in general tasks and financial tasks, respectively, and GPU memory consumption ratio of 6.3X. The sizes of our models are smaller than 0.59 GB, allowing inference on a smartphone.</li>
</ul>

<h3>Title: An Effective Incorporating Heterogeneous Knowledge Curriculum Learning  for Sequence Labeling</h3>
<ul>
<li><strong>Authors: </strong>Xuemei Tang, Qi Su</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13534">https://arxiv.org/abs/2402.13534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13534">https://arxiv.org/pdf/2402.13534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13534]] An Effective Incorporating Heterogeneous Knowledge Curriculum Learning  for Sequence Labeling(https://arxiv.org/abs/2402.13534)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Sequence labeling models often benefit from incorporating external knowledge. However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model. To address this challenge, we propose a two-stage curriculum learning (TCL) framework specifically designed for sequence labeling tasks. The TCL framework enhances training by gradually introducing data instances from easy to hard, aiming to improve both performance and training speed. Furthermore, we explore different metrics for assessing the difficulty levels of sequence labeling tasks. Through extensive experimentation on six Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we demonstrate the effectiveness of our model in enhancing the performance of sequence labeling models. Additionally, our analysis indicates that TCL accelerates training and alleviates the slow training problem associated with complex models.</li>
</ul>

<h3>Title: EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera  Relocalization</h3>
<ul>
<li><strong>Authors: </strong>Zhendong Xiao, Changhao Chen, Shan Yang, Wu Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13537">https://arxiv.org/abs/2402.13537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13537">https://arxiv.org/pdf/2402.13537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13537]] EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera  Relocalization(https://arxiv.org/abs/2402.13537)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Camera relocalization is pivotal in computer vision, with applications in AR, drones, robotics, and autonomous driving. It estimates 3D camera position and orientation (6-DoF) from images. Unlike traditional methods like SLAM, recent strides use deep learning for direct end-to-end pose estimation. We propose EffLoc, a novel efficient Vision Transformer for single-image camera relocalization. EffLoc's hierarchical layout, memory-bound self-attention, and feed-forward layers boost memory efficiency and inter-channel communication. Our introduced sequential group attention (SGA) module enhances computational efficiency by diversifying input features, reducing redundancy, and expanding model capacity. EffLoc excels in efficiency and accuracy, outperforming prior methods, such as AtLoc and MapNet. It thrives on large-scale outdoor car-driving scenario, ensuring simplicity, end-to-end trainability, and eliminating handcrafted loss functions.</li>
</ul>

<h3>Title: ARL2: Aligning Retrievers for Black-box Large Language Models via  Self-guided Adaptive Relevance Labeling</h3>
<ul>
<li><strong>Authors: </strong>Lingxi Zhang, Yue Yu, Kuan Wang, Chao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13542">https://arxiv.org/abs/2402.13542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13542">https://arxiv.org/pdf/2402.13542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13542]] ARL2: Aligning Retrievers for Black-box Large Language Models via  Self-guided Adaptive Relevance Labeling(https://arxiv.org/abs/2402.13542)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer learning capabilities and strong zero-shot generalization abilities. Our code will be published at \url{https://github.com/zhanglingxi-cs/ARL2}.</li>
</ul>

<h3>Title: A Two-Stage Dual-Path Framework for Text Tampering Detection and  Recognition</h3>
<ul>
<li><strong>Authors: </strong>Guandong Li, Xian Yang, Wenpin Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13545">https://arxiv.org/abs/2402.13545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13545">https://arxiv.org/pdf/2402.13545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13545]] A Two-Stage Dual-Path Framework for Text Tampering Detection and  Recognition(https://arxiv.org/abs/2402.13545)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Document tamper detection has always been an important aspect of tamper detection. Before the advent of deep learning, document tamper detection was difficult. We have made some explorations in the field of text tamper detection based on deep learning. Our Ps tamper detection method includes three steps: feature assistance, audit point positioning, and tamper recognition. It involves hierarchical filtering and graded output (tampered/suspected tampered/untampered). By combining artificial tamper data features, we simulate and augment data samples in various scenarios (cropping with noise addition/replacement, single character/space replacement, smearing/splicing, brightness/contrast adjustment, etc.). The auxiliary features include exif/binary stream keyword retrieval/noise, which are used for branch detection based on the results. Audit point positioning uses detection frameworks and controls thresholds for high and low density detection. Tamper recognition employs a dual-path dual-stream recognition network, with RGB and ELA stream feature extraction. After dimensionality reduction through self-correlation percentile pooling, the fused output is processed through vlad, yielding an accuracy of 0.804, recall of 0.659, and precision of 0.913.</li>
</ul>

<h3>Title: LLMs Meet Long Video: Advancing Long Video Comprehension with An  Interactive Visual Adapter in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yunxin Li, Xinyu Chen, Baotain Hu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13546">https://arxiv.org/abs/2402.13546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13546">https://arxiv.org/pdf/2402.13546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13546]] LLMs Meet Long Video: Advancing Long Video Comprehension with An  Interactive Visual Adapter in LLMs(https://arxiv.org/abs/2402.13546)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Long video understanding is a significant and ongoing challenge in the intersection of multimedia and artificial intelligence. Employing large language models (LLMs) for comprehending video becomes an emerging and promising method. However, this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions. To alleviate these issues, we present an Interactive Visual Adapter (IVA) within LLMs, designed to enhance interaction with fine-grained visual elements. Specifically, we first transform long videos into temporal video tokens via leveraging a visual encoder alongside a pretrained causal transformer, then feed them into LLMs with the video instructions. Subsequently, we integrated IVA, which contains a lightweight temporal frame selector and a spatial feature interactor, within the internal blocks of LLMs to capture instruction-aware and fine-grained visual signals. Consequently, the proposed video-LLM facilitates a comprehensive understanding of long video content through appropriate long video modeling and precise visual interactions. We conducted extensive experiments on nine video understanding benchmarks and experimental results show that our interactive visual adapter significantly improves the performance of video LLMs on long video QA tasks. Ablation studies further verify the effectiveness of IVA in long and short video understandings.</li>
</ul>

<h3>Title: ActiveRAG: Revealing the Treasures of Knowledge via Active Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Xu, Zhenghao Liu, Yibin Liu, Chenyan Xiong, Yukun Yan, Shuo Wang, Shi Yu, Zhiyuan Liu, Ge Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13547">https://arxiv.org/abs/2402.13547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13547">https://arxiv.org/pdf/2402.13547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13547]] ActiveRAG: Revealing the Treasures of Knowledge via Active Learning(https://arxiv.org/abs/2402.13547)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets. All data and codes are available at https://github.com/OpenMatch/ActiveRAG.</li>
</ul>

<h3>Title: DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of  EV Charging Load</h3>
<ul>
<li><strong>Authors: </strong>Siyang Li, Hui Xiong, Yize Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13548">https://arxiv.org/abs/2402.13548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13548">https://arxiv.org/pdf/2402.13548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13548]] DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of  EV Charging Load(https://arxiv.org/abs/2402.13548)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Due to the vast electric vehicle (EV) penetration to distribution grid, charging load forecasting is essential to promote charging station operation and demand-side management.However, the stochastic charging behaviors and associated exogenous factors render future charging load patterns quite volatile and hard to predict. Accordingly, we devise a novel Diffusion model termed DiffPLF for Probabilistic Load Forecasting of EV charging, which can explicitly approximate the predictive load distribution conditioned on historical data and related covariates. Specifically, we leverage a denoising diffusion model, which can progressively convert the Gaussian prior to real time-series data by learning a reversal of the diffusion process. Besides, we couple such diffusion model with a cross-attention-based conditioning mechanism to execute conditional generation for possible charging demand profiles. We also propose a task-informed fine-tuning technique to better adapt DiffPLF to the probabilistic time-series forecasting task and acquire more accurate and reliable predicted intervals. Finally, we conduct multiple experiments to validate the superiority of DiffPLF to predict complex temporal patterns of erratic charging load and carry out controllable generation based on certain covariate. Results demonstrate that we can attain a notable rise of 39.58% and 49.87% on MAE and CRPS respectively compared to the conventional method.</li>
</ul>

<h3>Title: Generative AI for Secure Physical Layer Communications: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Changyuan Zhao, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Dong In Kim, Xuemin (Sherman)Shen, Khaled B. Letaief</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13553">https://arxiv.org/abs/2402.13553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13553">https://arxiv.org/pdf/2402.13553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13553]] Generative AI for Secure Physical Layer Communications: A Survey(https://arxiv.org/abs/2402.13553)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative Artificial Intelligence (GAI) stands at the forefront of AI innovation, demonstrating rapid advancement and unparalleled proficiency in generating diverse content. Beyond content creation, GAI has significant analytical abilities to learn complex data distribution, offering numerous opportunities to resolve security issues. In the realm of security from physical layer perspectives, traditional AI approaches frequently struggle, primarily due to their limited capacity to dynamically adjust to the evolving physical attributes of transmission channels and the complexity of contemporary cyber threats. This adaptability and analytical depth are precisely where GAI excels. Therefore, in this paper, we offer an extensive survey on the various applications of GAI in enhancing security within the physical layer of communication networks. We first emphasize the importance of advanced GAI models in this area, including Generative Adversarial Networks (GANs), Autoencoders (AEs), Variational Autoencoders (VAEs), and Diffusion Models (DMs). We delve into the roles of GAI in addressing challenges of physical layer security, focusing on communication confidentiality, authentication, availability, resilience, and integrity. Furthermore, we also present future research directions focusing model improvements, multi-scenario deployment, resource-efficient optimization, and secure semantic communication, highlighting the multifaceted potential of GAI to address emerging challenges in secure physical layer communications and sensing.</li>
</ul>

<h3>Title: Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension  with Enhanced Visual Knowledge Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yunxin Li, Xinyu Chen, Baotian Hu, Haoyuan Shi, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13561">https://arxiv.org/abs/2402.13561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13561">https://arxiv.org/pdf/2402.13561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13561]] Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension  with Enhanced Visual Knowledge Alignment(https://arxiv.org/abs/2402.13561)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection. FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs). We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA, respectively.</li>
</ul>

<h3>Title: Event-aware Video Corpus Moment Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Danyang Hou, Liang Pang, Huawei Shen, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13566">https://arxiv.org/abs/2402.13566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13566">https://arxiv.org/pdf/2402.13566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13566]] Event-aware Video Corpus Moment Retrieval(https://arxiv.org/abs/2402.13566)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Video Corpus Moment Retrieval (VCMR) is a practical video retrieval task focused on identifying a specific moment within a vast corpus of untrimmed videos using the natural language query. Existing methods for VCMR typically rely on frame-aware video retrieval, calculating similarities between the query and video frames to rank videos based on maximum frame similarity.However, this approach overlooks the semantic structure embedded within the information between frames, namely, the event, a crucial element for human comprehension of videos. Motivated by this, we propose EventFormer, a model that explicitly utilizes events within videos as fundamental units for video retrieval. The model extracts event representations through event reasoning and hierarchical event encoding. The event reasoning module groups consecutive and visually similar frame representations into events, while the hierarchical event encoding encodes information at both the frame and event levels. We also introduce anchor multi-head self-attenion to encourage Transformer to capture the relevance of adjacent content in the video. The training of EventFormer is conducted by two-branch contrastive learning and dual optimization for two sub-tasks of VCMR. Extensive experiments on TVR, ANetCaps, and DiDeMo benchmarks show the effectiveness and efficiency of EventFormer in VCMR, achieving new state-of-the-art results. Additionally, the effectiveness of EventFormer is also validated on partially relevant video retrieval task.</li>
</ul>

<h3>Title: On the Expressive Power of a Variant of the Looped Transformer</h3>
<ul>
<li><strong>Authors: </strong>Yihang Gao, Chuanyang Zheng, Enze Xie, Han Shi, Tianyang Hu, Yu Li, Michael K. Ng, Zhenguo Li, Zhaoqiang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13572">https://arxiv.org/abs/2402.13572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13572">https://arxiv.org/pdf/2402.13572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13572]] On the Expressive Power of a Variant of the Looped Transformer(https://arxiv.org/abs/2402.13572)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms. To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer (Yang et al., 2024; Giannou et al., 2023), we design a novel transformer block, dubbed Algorithm Transformer (abbreviated as AlgoFormer). Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can achieve significantly higher expressiveness in algorithm representation when using the same number of parameters. In particular, inspired by the structure of human-designed learning algorithms, our transformer block consists of a pre-transformer that is responsible for task pre-processing, a looped transformer for iterative optimization algorithms, and a post-transformer for producing the desired results after post-processing. We provide theoretical evidence of the expressive power of the AlgoFormer in solving some challenging problems, mirroring human-designed algorithms. Furthermore, some theoretical and empirical results are presented to show that the designed transformer has the potential to be smarter than human-designed algorithms. Experimental results demonstrate the empirical superiority of the proposed transformer in that it outperforms the standard transformer and vanilla looped transformer in some challenging tasks.</li>
</ul>

<h3>Title: ToDo: Token Downsampling for Efficient Generation of High-Resolution  Images</h3>
<ul>
<li><strong>Authors: </strong>Ethan Smith, Nayan Saxena, Aninda Saha</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13573">https://arxiv.org/abs/2402.13573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13573">https://arxiv.org/pdf/2402.13573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13573]] ToDo: Token Downsampling for Efficient Generation of High-Resolution  Images(https://arxiv.org/abs/2402.13573)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity.</li>
</ul>

<h3>Title: Flexible Physical Camouflage Generation Based on a Differential Approach</h3>
<ul>
<li><strong>Authors: </strong>Yang Li, Wenyi Tan, Chenxing Zhao, Shuangju Zhou, Xinkai Liang, Quan Pan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13575">https://arxiv.org/abs/2402.13575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13575">https://arxiv.org/pdf/2402.13575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13575]] Flexible Physical Camouflage Generation Based on a Differential Approach(https://arxiv.org/abs/2402.13575)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, diffusion, generative</a></li>
<li><strong>Abstract: </strong>This study introduces a novel approach to neural rendering, specifically tailored for adversarial camouflage, within an extensive 3D rendering framework. Our method, named FPA, goes beyond traditional techniques by faithfully simulating lighting conditions and material variations, ensuring a nuanced and realistic representation of textures on a 3D target. To achieve this, we employ a generative approach that learns adversarial patterns from a diffusion model. This involves incorporating a specially designed adversarial loss and covert constraint loss to guarantee the adversarial and covert nature of the camouflage in the physical world. Furthermore, we showcase the effectiveness of the proposed camouflage in sticker mode, demonstrating its ability to cover the target without compromising adversarial information. Through empirical and physical experiments, FPA exhibits strong performance in terms of attack success rate and transferability. Additionally, the designed sticker-mode camouflage, coupled with a concealment constraint, adapts to the environment, yielding diverse styles of texture. Our findings highlight the versatility and efficacy of the FPA approach in adversarial camouflage applications.</li>
</ul>

<h3>Title: TransGOP: Transformer-Based Gaze Object Prediction</h3>
<ul>
<li><strong>Authors: </strong>Binglu Wang, Chenxi Guo, Yang Jin, Haisheng Xia, Nian Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13578">https://arxiv.org/abs/2402.13578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13578">https://arxiv.org/pdf/2402.13578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13578]] TransGOP: Transformer-Based Gaze Object Prediction(https://arxiv.org/abs/2402.13578)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Gaze object prediction aims to predict the location and category of the object that is watched by a human. Previous gaze object prediction works use CNN-based object detectors to predict the object's location. However, we find that Transformer-based object detectors can predict more accurate object location for dense objects in retail scenarios. Moreover, the long-distance modeling capability of the Transformer can help to build relationships between the human head and the gaze object, which is important for the GOP task. To this end, this paper introduces Transformer into the fields of gaze object prediction and proposes an end-to-end Transformer-based gaze object prediction method named TransGOP. Specifically, TransGOP uses an off-the-shelf Transformer-based object detector to detect the location of objects and designs a Transformer-based gaze autoencoder in the gaze regressor to establish long-distance gaze relationships. Moreover, to improve gaze heatmap regression, we propose an object-to-gaze cross-attention mechanism to let the queries of the gaze autoencoder learn the global-memory position knowledge from the object detector. Finally, to make the whole framework end-to-end trained, we propose a Gaze Box loss to jointly optimize the object detector and gaze regressor by enhancing the gaze heatmap energy in the box of the gaze object. Extensive experiments on the GOO-Synth and GOO-Real datasets demonstrate that our TransGOP achieves state-of-the-art performance on all tracks, i.e., object detection, gaze estimation, and gaze object prediction. Our code will be available at https://github.com/chenxi-Guo/TransGOP.git.</li>
</ul>

<h3>Title: WinoViz: Probing Visual Properties of Objects Under Different States</h3>
<ul>
<li><strong>Authors: </strong>Woojeong Jin, Tejas Srinivasan, Jesse Thomason, Xiang Ren</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13584">https://arxiv.org/abs/2402.13584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13584">https://arxiv.org/pdf/2402.13584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13584]] WinoViz: Probing Visual Properties of Objects Under Different States(https://arxiv.org/abs/2402.13584)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Humans perceive and comprehend different visual properties of an object based on specific contexts. For instance, we know that a banana turns brown ``when it becomes rotten,'' whereas it appears green ``when it is unripe.'' Previous studies on probing visual commonsense knowledge have primarily focused on examining language models' understanding of typical properties (e.g., colors and shapes) of objects. We present WinoViz, a text-only evaluation dataset, consisting of 1,380 examples that probe the reasoning abilities of language models regarding variant visual properties of objects under different contexts or states. Our task is challenging since it requires pragmatic reasoning (finding intended meanings) and visual knowledge reasoning. We also present multi-hop data, a more challenging version of our data, which requires multi-step reasoning chains to solve our task. In our experimental analysis, our findings are: a) Large language models such as GPT-4 demonstrate effective performance, but when it comes to multi-hop data, their performance is significantly degraded. b) Large models perform well on pragmatic reasoning, but visual knowledge reasoning is a bottleneck in our task. c) Vision-language models outperform their language-model counterparts. d) A model with machine-generated images performs poorly in our task. This is due to the poor quality of the generated images.</li>
</ul>

<h3>Title: A Multimodal In-Context Tuning Approach for E-Commerce Product  Description Generation</h3>
<ul>
<li><strong>Authors: </strong>Yunxin Li, Baotian Hu, Wenhan Luo, Lin Ma, Yuxin Ding, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13587">https://arxiv.org/abs/2402.13587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13587">https://arxiv.org/pdf/2402.13587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13587]] A Multimodal In-Context Tuning Approach for E-Commerce Product  Description Generation(https://arxiv.org/abs/2402.13587)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products. For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description. However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features. To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to produce the description. During training, we keep the visual encoder and language model frozen, focusing on optimizing the modules responsible for creating multimodal in-context references and dynamic prompts. This approach preserves the language generation prowess of large language models (LLMs), facilitating a substantial increase in description diversity. To assess the effectiveness of ModICT across various language model scales and types, we collect data from three distinct product categories within the E-commerce domain. Extensive experiments demonstrate that ModICT significantly improves the accuracy (by up to 3.3% on Rouge-L) and diversity (by up to 9.4% on D-5) of generated results compared to conventional methods. Our findings underscore the potential of ModICT as a valuable tool for enhancing automatic generation of product descriptions in a wide range of applications.</li>
</ul>

<h3>Title: Knowledge Graph Enhanced Large Language Model Editing</h3>
<ul>
<li><strong>Authors: </strong>Mengqi Zhang, Xiaotian Ye, Qiang Liu, Pengjie Ren, Shu Wu, Zhumin Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13593">https://arxiv.org/abs/2402.13593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13593">https://arxiv.org/pdf/2402.13593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13593]] Knowledge Graph Enhanced Large Language Model Editing(https://arxiv.org/abs/2402.13593)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are pivotal in advancing natural language processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and outdated knowledge. Model editing emerges as a promising solution to address these challenges. However, existing editing methods struggle to track and incorporate changes in knowledge associated with edits, which limits the generalization ability of postedit LLMs in processing edited knowledge. To tackle these problems, we propose a novel model editing method that leverages knowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we first utilize a knowledge graph augmentation module to uncover associated knowledge that has changed due to editing, obtaining its internal representations within LLMs. This approach allows knowledge alterations within LLMs to be reflected through an external graph structure. Subsequently, we design a graph-based knowledge edit module to integrate structured knowledge into the model editing. This ensures that the updated parameters reflect not only the modifications of the edited knowledge but also the changes in other associated knowledge resulting from the editing process. Comprehensive experiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME significantly improves the generalization capabilities of post-edit LLMs in employing edited knowledge.</li>
</ul>

<h3>Title: User-LLM: Efficient LLM Contextualization with User Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Lin Ning, Luyang Liu, Jiaxing Wu, Neo Wu, Devora Berlowitz, Sushant Prakash, Bradley Green, Shawn O'Banion, Jun Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13598">https://arxiv.org/abs/2402.13598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13598">https://arxiv.org/pdf/2402.13598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13598]] User-LLM: Efficient LLM Contextualization with User Embeddings(https://arxiv.org/abs/2402.13598)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized natural language processing. However, effectively incorporating complex and potentially noisy user interaction data remains a challenge. To address this, we propose User-LLM, a novel framework that leverages user embeddings to contextualize LLMs. These embeddings, distilled from diverse user interactions using self-supervised pretraining, capture latent user preferences and their evolution over time. We integrate these user embeddings with LLMs through cross-attention and soft-prompting, enabling LLMs to dynamically adapt to user context. Our comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks. Notably, our approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient. We further incorporate Perceiver layers to streamline the integration between user encoders and LLMs, reducing computational demands.</li>
</ul>

<h3>Title: Hybrid Reasoning Based on Large Language Models for Autonomous Car  Driving</h3>
<ul>
<li><strong>Authors: </strong>Mehdi Azarafza, Mojtaba Nayyeri, Charles Steinmetz, Steffen Staab, Achim Rettberg</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13602">https://arxiv.org/abs/2402.13602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13602">https://arxiv.org/pdf/2402.13602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13602]] Hybrid Reasoning Based on Large Language Models for Autonomous Car  Driving(https://arxiv.org/abs/2402.13602)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks. However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration. In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios. We hypothesize that LLMs hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context. This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short. We evaluated Large Language Models (LLMs) based on accuracy by comparing their answers with human-generated ground truth inside CARLA. The results showed that when a combination of images (detected objects) and sensor data is fed into the LLM, it can offer precise information for brake and throttle control in autonomous vehicles across various weather conditions. This formulation and answers can assist in decision-making for auto-pilot systems.</li>
</ul>

<h3>Title: KorNAT: LLM Alignment Benchmark for Korean Social Values and Common  Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Jiyoung Lee, Minwoo Kim, Seungho Kim, Junghwan Kim, Seunghyun Won, Hwaran Lee, Edward Choi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13605">https://arxiv.org/abs/2402.13605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13605">https://arxiv.org/pdf/2402.13605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13605]] KorNAT: LLM Alignment Benchmark for Korean Social Values and Common  Knowledge(https://arxiv.org/abs/2402.13605)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>For Large Language Models (LLMs) to be effectively deployed in a specific country, they must possess an understanding of the nation's culture and basic knowledge. To this end, we introduce National Alignment, which measures an alignment between an LLM and a targeted country from two aspects: social value alignment and common knowledge alignment. Social value alignment evaluates how well the model understands nation-specific social values, while common knowledge alignment examines how well the model captures basic knowledge related to the nation. We constructed KorNAT, the first benchmark that measures national alignment with South Korea. For the social value dataset, we obtained ground truth labels from a large-scale survey involving 6,174 unique Korean participants. For the common knowledge dataset, we constructed samples based on Korean textbooks and GED reference materials. KorNAT contains 4K and 6K multiple-choice questions for social value and common knowledge, respectively. Our dataset creation process is meticulously designed and based on statistical sampling theory and was refined through multiple rounds of human review. The experiment results of seven LLMs reveal that only a few models met our reference score, indicating a potential for further enhancement. KorNAT has received government approval after passing an assessment conducted by a government-affiliated organization dedicated to evaluating dataset quality. Samples and detailed evaluation protocols of our dataset can be found in \url{https://selectstar.ai/ko/papers-national-alignment#}</li>
</ul>

<h3>Title: A Comprehensive Study of Multilingual Confidence Estimation on Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Boyang Xue, Hongru Wang, Weichao Wang, Rui Wang, Sheng Wang, Zeming Liu, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13606">https://arxiv.org/abs/2402.13606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13606">https://arxiv.org/pdf/2402.13606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13606]] A Comprehensive Study of Multilingual Confidence Estimation on Large  Language Models(https://arxiv.org/abs/2402.13606)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability. Confidence or uncertainty estimations indicating the extent of trustworthiness of a model's response are essential to developing reliable AI systems. Current research primarily focuses on LLM confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications. This paper introduces a comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs. First, we introduce an elaborated and expert-checked multilingual QA dataset. Second, we delve into the performance of confidence estimations and examine how these confidence scores can enhance LLM performance through self-refinement across diverse languages. Finally, we propose a cross-lingual confidence estimation method to achieve more precise confidence scores. The experimental results showcase the performance of various confidence estimation methods across different languages as well as present that our proposed cross-lingual confidence estimation technique significantly enhances confidence estimation and outperforms several baseline methods.</li>
</ul>

<h3>Title: CODIS: Benchmarking Context-Dependent Visual Comprehension for  Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fuwen Luo, Chi Chen, Zihao Wan, Zhaolu Kang, Qidong Yan, Yingjie Li, Xiaolong Wang, Siyu Wang, Ziyue Wang, Xiaoyue Mi, Peng Li, Ning Ma, Maosong Sun, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13607">https://arxiv.org/abs/2402.13607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13607">https://arxiv.org/pdf/2402.13607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13607]] CODIS: Benchmarking Context-Dependent Visual Comprehension for  Multimodal Large Language Models(https://arxiv.org/abs/2402.13607)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a context-dependent manner. View our project website at https://thunlp-mt.github.io/CODIS.</li>
</ul>

<h3>Title: Data-driven Discovery with Large Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Sanchaita Hazra, Ashish Sabharwal, Peter Clark</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13610">https://arxiv.org/abs/2402.13610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13610">https://arxiv.org/pdf/2402.13610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13610]] Data-driven Discovery with Large Generative Models(https://arxiv.org/abs/2402.13610)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>With the accumulation of data at an unprecedented rate, its potential to fuel scientific discovery is growing exponentially. This position paper urges the Machine Learning (ML) community to exploit the capabilities of large generative models (LGMs) to develop automated systems for end-to-end data-driven discovery -- a paradigm encompassing the search and verification of hypotheses purely from a set of provided datasets, without the need for additional data collection or physical experiments. We first outline several desiderata for an ideal data-driven discovery system. Then, through DATAVOYAGER, a proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of these desiderata -- a feat previously unattainable -- while also highlighting important limitations in the current system that open up opportunities for novel ML research. We contend that achieving accurate, reliable, and robust end-to-end discovery systems solely through the current capabilities of LGMs is challenging. We instead advocate for fail-proof tool integration, along with active user moderation through feedback mechanisms, to foster data-driven scientific discoveries with efficiency and reproducibility.</li>
</ul>

<h3>Title: YOLOv9: Learning What You Want to Learn Using Programmable Gradient  Information</h3>
<ul>
<li><strong>Authors: </strong>Chien-Yao Wang, I-Hau Yeh, Hong-Yuan Mark Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13616">https://arxiv.org/abs/2402.13616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13616">https://arxiv.org/pdf/2402.13616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13616]] YOLOv9: Learning What You Want to Learn Using Programmable Gradient  Information(https://arxiv.org/abs/2402.13616)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Today's deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN's architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise convolution. PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: https://github.com/WongKinYiu/yolov9.</li>
</ul>

<h3>Title: FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sahil Mishra, Ujjwal Sudev, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13623">https://arxiv.org/abs/2402.13623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13623">https://arxiv.org/pdf/2402.13623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13623]] FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large  Language Models(https://arxiv.org/abs/2402.13623)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Taxonomies represent an arborescence hierarchical structure that establishes relationships among entities to convey knowledge within a specific domain. Each edge in the taxonomy signifies a hypernym-hyponym relationship. Taxonomies find utility in various real-world applications, such as e-commerce search engines and recommendation systems. Consequently, there arises a necessity to enhance these taxonomies over time. However, manually curating taxonomies with neoteric data presents challenges due to limitations in available human resources and the exponential growth of data. Therefore, it becomes imperative to develop automatic taxonomy expansion methods. Traditional supervised taxonomy expansion approaches encounter difficulties stemming from limited resources, primarily due to the small size of existing taxonomies. This scarcity of training data often leads to overfitting. In this paper, we propose FLAME, a novel approach for taxonomy expansion in low-resource environments by harnessing the capabilities of large language models that are trained on extensive real-world knowledge. LLMs help compensate for the scarcity of domain-specific knowledge. Specifically, FLAME leverages prompting in few-shot settings to extract the inherent knowledge within the LLMs, ascertaining the hypernym entities within the taxonomy. Furthermore, it employs reinforcement learning to fine-tune the large language models, resulting in more accurate predictions. Experiments on three real-world benchmark datasets demonstrate the effectiveness of FLAME in real-world scenarios, achieving a remarkable improvement of 18.5% in accuracy and 12.3% in Wu & Palmer metric over eight baselines. Furthermore, we elucidate the strengths and weaknesses of FLAME through an extensive case study, error analysis and ablation studies on the benchmarks.</li>
</ul>

<h3>Title: MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Wanqing Cui, Keping Bi, Jiafeng Guo, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13625">https://arxiv.org/abs/2402.13625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13625">https://arxiv.org/pdf/2402.13625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13625]] MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning(https://arxiv.org/abs/2402.13625)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Since commonsense information has been recorded significantly less frequently than its existence, language models pre-trained by text generation have difficulty to learn sufficient commonsense knowledge. Several studies have leveraged text retrieval to augment the models' commonsense ability. Unlike text, images capture commonsense information inherently but little effort has been paid to effectively utilize them. In this work, we propose a novel Multi-mOdal REtrieval (MORE) augmentation framework, to leverage both text and images to enhance the commonsense ability of language models. Extensive experiments on the Common-Gen task have demonstrated the efficacy of MORE based on the pre-trained models of both single and multiple modalities.</li>
</ul>

<h3>Title: UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural  Language</h3>
<ul>
<li><strong>Authors: </strong>Yufei He, Bryan Hooi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13630">https://arxiv.org/abs/2402.13630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13630">https://arxiv.org/pdf/2402.13630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13630]] UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural  Language(https://arxiv.org/abs/2402.13630)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, when this concept is applied to graph learning, a stark contrast emerges. Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (TAGs) for unifying node representations. We propose a cascaded architecture of Language Models (LMs) and Graph Neural Networks (GNNs) as backbone networks with a self-supervised training objective based on Masked Graph Modeling (MGM). We introduce graph instruction tuning using Large Language Models (LLMs) to enable zero-shot prediction ability. Our comprehensive experiments across various graph learning tasks and domains demonstrate the model's effectiveness in self-supervised representation learning on unseen graphs, few-shot in-context transfer, and zero-shot transfer, even surpassing or matching the performance of GNNs that have undergone supervised training on target datasets.</li>
</ul>

<h3>Title: Delving into Dark Regions for Robust Shadow Detection</h3>
<ul>
<li><strong>Authors: </strong>Huankang Guan, Ke Xu, Rynson W.H. Lau</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13631">https://arxiv.org/abs/2402.13631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13631">https://arxiv.org/pdf/2402.13631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13631]] Delving into Dark Regions for Robust Shadow Detection(https://arxiv.org/abs/2402.13631)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Shadow detection is a challenging task as it requires a comprehensive understanding of shadow characteristics and global/local illumination conditions. We observe from our experiment that state-of-the-art deep methods tend to have higher error rates in differentiating shadow pixels from non-shadow pixels in dark regions (ie, regions with low-intensity values). Our key insight to this problem is that existing methods typically learn discriminative shadow features from the whole image globally, covering the full range of intensity values, and may not learn the subtle differences between shadow and non-shadow pixels in dark regions. Hence, if we can design a model to focus on a narrower range of low-intensity regions, it may be able to learn better discriminative features for shadow detection. Inspired by this insight, we propose a novel shadow detection approach that first learns global contextual cues over the entire image and then zooms into the dark regions to learn local shadow representations. To this end, we formulate an effective dark-region recommendation (DRR) module to recommend regions of low-intensity values, and a novel dark-aware shadow analysis (DASA) module to learn dark-aware shadow features from the recommended dark regions. Extensive experiments show that the proposed method outperforms the state-of-the-art methods on three popular shadow detection datasets. Code is available at https://github.com/guanhuankang/ShadowDetection2021.git.</li>
</ul>

<h3>Title: The METRIC-framework for assessing data quality for trustworthy AI in  medicine: a systematic review</h3>
<ul>
<li><strong>Authors: </strong>Daniel Schwabe, Katinka Becker, Martin Seyferth, Andreas Klaß, Tobias Schäffter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13635">https://arxiv.org/abs/2402.13635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13635">https://arxiv.org/pdf/2402.13635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13635]] The METRIC-framework for assessing data quality for trustworthy AI in  medicine: a systematic review(https://arxiv.org/abs/2402.13635)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, fair, interpretability</a></li>
<li><strong>Abstract: </strong>The adoption of machine learning (ML) and, more specifically, deep learning (DL) applications into all major areas of our lives is underway. The development of trustworthy AI is especially important in medicine due to the large implications for patients' lives. While trustworthiness concerns various aspects including ethical, technical and privacy requirements, we focus on the importance of data quality (training/test) in DL. Since data quality dictates the behaviour of ML products, evaluating data quality will play a key part in the regulatory approval of medical AI products. We perform a systematic review following PRISMA guidelines using the databases PubMed and ACM Digital Library. We identify 2362 studies, out of which 62 records fulfil our eligibility criteria. From this literature, we synthesise the existing knowledge on data quality frameworks and combine it with the perspective of ML applications in medicine. As a result, we propose the METRIC-framework, a specialised data quality framework for medical training data comprising 15 awareness dimensions, along which developers of medical ML applications should investigate a dataset. This knowledge helps to reduce biases as a major source of unfairness, increase robustness, facilitate interpretability and thus lays the foundation for trustworthy AI in medicine. Incorporating such systematic assessment of medical datasets into regulatory approval processes has the potential to accelerate the approval of ML products and builds the basis for new standards.</li>
</ul>

<h3>Title: Class-Aware Mask-Guided Feature Refinement for Scene Text Recognition</h3>
<ul>
<li><strong>Authors: </strong>Mingkun Yang, Biao Yang, Minghui Liao, Yingying Zhu, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13643">https://arxiv.org/abs/2402.13643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13643">https://arxiv.org/pdf/2402.13643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13643]] Class-Aware Mask-Guided Feature Refinement for Scene Text Recognition(https://arxiv.org/abs/2402.13643)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Scene text recognition is a rapidly developing field that faces numerous challenges due to the complexity and diversity of scene text, including complex backgrounds, diverse fonts, flexible arrangements, and accidental occlusions. In this paper, we propose a novel approach called Class-Aware Mask-guided feature refinement (CAM) to address these challenges. Our approach introduces canonical class-aware glyph masks generated from a standard font to effectively suppress background and text style noise, thereby enhancing feature discrimination. Additionally, we design a feature alignment and fusion module to incorporate the canonical mask guidance for further feature refinement for text recognition. By enhancing the alignment between the canonical mask feature and the text feature, the module ensures more effective fusion, ultimately leading to improved recognition performance. We first evaluate CAM on six standard text recognition benchmarks to demonstrate its effectiveness. Furthermore, CAM exhibits superiority over the state-of-the-art method by an average performance gain of 4.1% across six more challenging datasets, despite utilizing a smaller model size. Our study highlights the importance of incorporating canonical mask guidance and aligned feature refinement techniques for robust scene text recognition. The code is available at https://github.com/MelosY/CAM.</li>
</ul>

<h3>Title: Unsupervised Text Style Transfer via LLMs and Attention Masking with  Multi-way Interactions</h3>
<ul>
<li><strong>Authors: </strong>Lei Pan, Yunshi Lan, Yang Li, Weining Qian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13647">https://arxiv.org/abs/2402.13647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13647">https://arxiv.org/pdf/2402.13647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13647]] Unsupervised Text Style Transfer via LLMs and Attention Masking with  Multi-way Interactions(https://arxiv.org/abs/2402.13647)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Unsupervised Text Style Transfer (UTST) has emerged as a critical task within the domain of Natural Language Processing (NLP), aiming to transfer one stylistic aspect of a sentence into another style without changing its semantics, syntax, or other attributes. This task is especially challenging given the intrinsic lack of parallel text pairings. Among existing methods for UTST tasks, attention masking approach and Large Language Models (LLMs) are deemed as two pioneering methods. However, they have shortcomings in generating unsmooth sentences and changing the original contents, respectively. In this paper, we investigate if we can combine these two methods effectively. We propose four ways of interactions, that are pipeline framework with tuned orders; knowledge distillation from LLMs to attention masking model; in-context learning with constructed parallel examples. We empirically show these multi-way interactions can improve the baselines in certain perspective of style strength, content preservation and text fluency. Experiments also demonstrate that simply conducting prompting followed by attention masking-based revision can consistently surpass the other systems, including supervised text style transfer systems. On Yelp-clean and Amazon-clean datasets, it improves the previously best mean metric by 0.5 and 3.0 absolute percentages respectively, and achieves new SOTA results.</li>
</ul>

<h3>Title: Robustness of Deep Neural Networks for Micro-Doppler Radar  Classification</h3>
<ul>
<li><strong>Authors: </strong>Mikolaj Czerkawski, Carmine Clemente, Craig MichieCraig Michie, Christos Tachtatzis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13651">https://arxiv.org/abs/2402.13651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13651">https://arxiv.org/pdf/2402.13651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13651]] Robustness of Deep Neural Networks for Micro-Doppler Radar  Classification(https://arxiv.org/abs/2402.13651)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the great capabilities of deep classifiers for radar data processing come the risks of learning dataset-specific features that do not generalize well. In this work, the robustness of two deep convolutional architectures, trained and tested on the same data, is evaluated. When standard training practice is followed, both classifiers exhibit sensitivity to subtle temporal shifts of the input representation, an augmentation that carries minimal semantic content. Furthermore, the models are extremely susceptible to adversarial examples. Both small temporal shifts and adversarial examples are a result of a model overfitting on features that do not generalize well. As a remedy, it is shown that training on adversarial examples and temporally augmented samples can reduce this effect and lead to models that generalise better. Finally, models operating on cadence-velocity diagram representation rather than Doppler-time are demonstrated to be naturally more immune to adversarial examples.</li>
</ul>

<h3>Title: PQA: Zero-shot Protein Question Answering for Free-form Scientific  Enquiry with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Eli M Carrami, Sahand Sharifzadeh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13653">https://arxiv.org/abs/2402.13653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13653">https://arxiv.org/pdf/2402.13653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13653]] PQA: Zero-shot Protein Question Answering for Free-form Scientific  Enquiry with Large Language Models(https://arxiv.org/abs/2402.13653)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We introduce the novel task of zero-shot Protein Question Answering (PQA) for free-form scientific enquiry. Given a previously unseen protein sequence and a natural language question, the task is to deliver a scientifically accurate answer. This task not only supports future biological research, but could also provide a test bed for assessing the scientific precision of large language models (LLMs). We contribute the first specialized dataset for PQA model training, containing 257K protein sequences annotated with 1.97M scientific question-answer pairs. Additionally, we propose and study several novel biologically relevant benchmarks for scientific PQA. Employing two robust multi-modal architectures, we establish an initial state-of-the-art performance for PQA and reveal key performance factors through ablation studies. Our comprehensive PQA framework, named Pika, including dataset, code, model checkpoints, and a user-friendly demo, is openly accessible on github.com/EMCarrami/Pika, promoting wider research and application in the field.</li>
</ul>

<h3>Title: Stable Update of Regression Trees</h3>
<ul>
<li><strong>Authors: </strong>Morten Blørstad, Berent Å. S. Lunde, Nello Blaser</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13655">https://arxiv.org/abs/2402.13655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13655">https://arxiv.org/pdf/2402.13655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13655]] Stable Update of Regression Trees(https://arxiv.org/abs/2402.13655)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Updating machine learning models with new information usually improves their predictive performance, yet, in many applications, it is also desirable to avoid changing the model predictions too much. This property is called stability. In most cases when stability matters, so does explainability. We therefore focus on the stability of an inherently explainable machine learning method, namely regression trees. We aim to use the notion of empirical stability and design algorithms for updating regression trees that provide a way to balance between predictability and empirical stability. To achieve this, we propose a regularization method, where data points are weighted based on the uncertainty in the initial model. The balance between predictability and empirical stability can be adjusted through hyperparameters. This regularization method is evaluated in terms of loss and stability and assessed on a broad range of data characteristics. The results show that the proposed update method improves stability while achieving similar or better predictive performance. This shows that it is possible to achieve both predictive and stable results when updating regression trees.</li>
</ul>

<h3>Title: Privacy-Preserving Instructions for Aligning Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Da Yu, Peter Kairouz, Sewoong Oh, Zheng Xu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13659">https://arxiv.org/abs/2402.13659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13659">https://arxiv.org/pdf/2402.13659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13659]] Privacy-Preserving Instructions for Aligning Large Language Models(https://arxiv.org/abs/2402.13659)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning. Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by showing comparable results to real instructions. In supervised fine-tuning, models trained with private synthetic instructions outperform leading open-source models such as Vicuna.</li>
</ul>

<h3>Title: GCOF: Self-iterative Text Generation for Copywriting Using Large  Language Model</h3>
<ul>
<li><strong>Authors: </strong>Jianghui Zhou, Ya Gao, Jie Liu, Xuemin Zhao, Zhaohua Yang, Yue Wu, Lirong Shi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13667">https://arxiv.org/abs/2402.13667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13667">https://arxiv.org/pdf/2402.13667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13667]] GCOF: Self-iterative Text Generation for Copywriting Using Large  Language Model(https://arxiv.org/abs/2402.13667)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models(LLM) such as ChatGPT have substantially simplified the generation of marketing copy, yet producing content satisfying domain specific requirements, such as effectively engaging customers, remains a significant challenge. In this work, we introduce the Genetic Copy Optimization Framework (GCOF) designed to enhance both efficiency and engagememnt of marketing copy creation. We conduct explicit feature engineering within the prompts of LLM. Additionally, we modify the crossover operator in Genetic Algorithm (GA), integrating it into the GCOF to enable automatic feature engineering. This integration facilitates a self-iterative refinement of the marketing copy. Compared to human curated copy, Online results indicate that copy produced by our framework achieves an average increase in click-through rate (CTR) of over $50\%$.</li>
</ul>

<h3>Title: Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Zhaorui Yang, Qian Liu, Tianyu Pang, Han Wang, Haozhe Feng, Minfeng Zhu, Wei Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13669">https://arxiv.org/abs/2402.13669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13669">https://arxiv.org/pdf/2402.13669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13669]] Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning(https://arxiv.org/abs/2402.13669)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at \url{https://github.com/sail-sg/sdft}.</li>
</ul>

<h3>Title: KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual  Machine-Generated Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Michal Spiegel, Dominik Macko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13671">https://arxiv.org/abs/2402.13671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13671">https://arxiv.org/pdf/2402.13671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13671]] KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual  Machine-Generated Text Detection(https://arxiv.org/abs/2402.13671)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>SemEval-2024 Task 8 is focused on multigenerator, multidomain, and multilingual black-box machine-generated text detection. Such a detection is important for preventing a potential misuse of large language models (LLMs), the newest of which are very capable in generating multilingual human-like texts. We have coped with this task in multiple ways, utilizing language identification and parameter-efficient fine-tuning of smaller LLMs for text classification. We have further used the per-language classification-threshold calibration to uniquely combine fine-tuned models predictions with statistical detection metrics to improve generalization of the system detection performance. Our submitted method achieved competitive results, ranking at the fourth place, just under 1 percentage point behind the winner.</li>
</ul>

<h3>Title: Generalizable Semantic Vision Query Generation for Zero-shot Panoptic  and Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jialei Chen, Daisuke Deguchi, Chenkai Zhang, Hiroshi Murase</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13697">https://arxiv.org/abs/2402.13697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13697">https://arxiv.org/pdf/2402.13697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13697]] Generalizable Semantic Vision Query Generation for Zero-shot Panoptic  and Semantic Segmentation(https://arxiv.org/abs/2402.13697)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Zero-shot Panoptic Segmentation (ZPS) aims to recognize foreground instances and background stuff without images containing unseen categories in training. Due to the visual data sparsity and the difficulty of generalizing from seen to unseen categories, this task remains challenging. To better generalize to unseen classes, we propose Conditional tOken aligNment and Cycle trAnsiTion (CONCAT), to produce generalizable semantic vision queries. First, a feature extractor is trained by CON to link the vision and semantics for providing target queries. Formally, CON is proposed to align the semantic queries with the CLIP visual CLS token extracted from complete and masked images. To address the lack of unseen categories, a generator is required. However, one of the gaps in synthesizing pseudo vision queries, ie, vision queries for unseen categories, is describing fine-grained visual details through semantic embeddings. Therefore, we approach CAT to train the generator in semantic-vision and vision-semantic manners. In semantic-vision, visual query contrast is proposed to model the high granularity of vision by pulling the pseudo vision queries with the corresponding targets containing segments while pushing those without segments away. To ensure the generated queries retain semantic information, in vision-semantic, the pseudo vision queries are mapped back to semantic and supervised by real semantic embeddings. Experiments on ZPS achieve a 5.2% hPQ increase surpassing SOTA. We also examine inductive ZPS and open-vocabulary semantic segmentation and obtain comparative results while being 2 times faster in testing.</li>
</ul>

<h3>Title: Explainable Classification Techniques for Quantum Dot Device  Measurements</h3>
<ul>
<li><strong>Authors: </strong>Daniel Schug, Tyler J. Kovach, M. A. Wolfe, Jared Benson, Sanghyeok Park, J. P. Dodson, J. Corrigan, M. A. Eriksson, Justyna P. Zwolak</a></li>
<li><strong>Subjects: </strong>cs.CV, cond-mat.mes-hall, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13699">https://arxiv.org/abs/2402.13699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13699">https://arxiv.org/pdf/2402.13699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13699]] Explainable Classification Techniques for Quantum Dot Device  Measurements(https://arxiv.org/abs/2402.13699)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here. While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy. To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features. We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy. Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where human intervention is necessary at the current stage of development.</li>
</ul>

<h3>Title: On the Conflict of Robustness and Learning in Collaborative Machine  Learning</h3>
<ul>
<li><strong>Authors: </strong>Mathilde Raynal, Carmela Troncoso</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13700">https://arxiv.org/abs/2402.13700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13700">https://arxiv.org/pdf/2402.13700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13700]] On the Conflict of Robustness and Learning in Collaborative Machine  Learning(https://arxiv.org/abs/2402.13700)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Collaborative Machine Learning (CML) allows participants to jointly train a machine learning model while keeping their training data private. In scenarios where privacy is a strong requirement, such as health-related applications, safety is also a primary concern. This means that privacy-preserving CML processes must produce models that output correct and reliable decisions \emph{even in the presence of potentially untrusted participants}. In response to this issue, researchers propose to use \textit{robust aggregators} that rely on metrics which help filter out malicious contributions that could compromise the training process. In this work, we formalize the landscape of robust aggregators in the literature. Our formalization allows us to show that existing robust aggregators cannot fulfill their goal: either they use distance-based metrics that cannot accurately identify targeted malicious updates; or propose methods whose success is in direct conflict with the ability of CML participants to learn from others and therefore cannot eliminate the risk of manipulation without preventing learning.</li>
</ul>

<h3>Title: Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand  for Multilingual Instructions?</h3>
<ul>
<li><strong>Authors: </strong>Alexander Arno Weber, Klaudia Thellmann, Jan Ebert, Nicolas Flores-Herr, Jens Lehmann, Michael Fromm, Mehdi Ali</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13703">https://arxiv.org/abs/2402.13703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13703">https://arxiv.org/pdf/2402.13703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13703]] Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand  for Multilingual Instructions?(https://arxiv.org/abs/2402.13703)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The adaption of multilingual pre-trained Large Language Models (LLMs) into eloquent and helpful assistants is essential to facilitate their use across different language regions. In that spirit, we are the first to conduct an extensive study of the performance of multilingual models on parallel, multi-turn instruction-tuning benchmarks across a selection of the most-spoken Indo-European languages. We systematically examine the effects of language and instruction dataset size on a mid-sized, multilingual LLM by instruction-tuning it on parallel instruction-tuning datasets. Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 4.6%. Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning datasets. Finally, we conduct a human annotation study to understand the alignment between human-based and GPT-4-based evaluation within multilingual chat scenarios.</li>
</ul>

<h3>Title: SaGE: Evaluating Moral Consistency in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Vamshi Krishna Bonagiri, Sreeram Vennam, Priyanshul Govil, Ponnurangam Kumaraguru, Manas Gaur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13709">https://arxiv.org/abs/2402.13709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13709">https://arxiv.org/pdf/2402.13709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13709]] SaGE: Evaluating Moral Consistency in Large Language Models(https://arxiv.org/abs/2402.13709)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general). Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks. However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability. To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of "Rules of Thumb" (RoTs) to measure a model's moral consistency. RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively. To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses to them by LLMs, and the RoTs that these models followed. Furthermore, to illustrate the generalizability of SaGE, we use it to investigate LLM consistency on two popular datasets -- TruthfulQA and HellaSwag. Our results reveal that task-accuracy and consistency are independent problems, and there is a dire need to investigate these issues further.</li>
</ul>

<h3>Title: Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character  Role-Playing Agent</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyan Yu, Tongxu Luo, Yifan Wei, Fangyu Lei, Yiming Huang, Peng Hao, Liehuang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13717">https://arxiv.org/abs/2402.13717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13717">https://arxiv.org/pdf/2402.13717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13717]] Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character  Role-Playing Agent(https://arxiv.org/abs/2402.13717)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation. Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles. This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko's adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences. Code and data are available at https://github.com/weiyifan1023/Neeko.</li>
</ul>

<h3>Title: $\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens</h3>
<ul>
<li><strong>Authors: </strong>Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13718">https://arxiv.org/abs/2402.13718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13718">https://arxiv.org/pdf/2402.13718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13718]] $\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens(https://arxiv.org/abs/2402.13718)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. In our experiments, based on $\infty$Bench, we evaluate the state-of-the-art proprietary and open-source LLMs tailored for processing long contexts. The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context. We further present three intriguing analyses regarding the behavior of LLMs processing long context.</li>
</ul>

<h3>Title: Ouroboros: Speculative Decoding with Large Model Enhanced Drafting</h3>
<ul>
<li><strong>Authors: </strong>Weilin Zhao, Yuxiang Huang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13720">https://arxiv.org/abs/2402.13720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13720">https://arxiv.org/pdf/2402.13720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13720]] Ouroboros: Speculative Decoding with Large Model Enhanced Drafting(https://arxiv.org/abs/2402.13720)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Drafting-then-verifying decoding methods such as speculative decoding are widely adopted training-free methods to accelerate the inference of large language models (LLMs). Instead of employing an autoregressive process to decode tokens sequentially, speculative decoding initially creates drafts with an efficient small model. Then LLMs are required to conduct verification and correction in a non-autoregressive fashion to minimize time overhead. Generating longer drafts can lead to even more significant speedups once verified, but also incurs substantial trial and error costs if it fails. Suffering from the high verification failure probability, existing decoding methods cannot draft too much content for verification at one time, achieving sub-optimal inference acceleration. In this paper, we introduce Ouroboros, which constructs a phrase candidate pool from the verification process of LLMs to provide candidates for draft generation of the small model. Thereby, Ouroboros can further improve the efficiency and effectiveness of the initial drafts. The experimental results on typical text generation tasks show that Ouroboros achieves speedups of up to 1.9x and 2.8x compared to lookahead decoding and speculative decoding, respectively. The source code of Ouroboros is available at https://github.com/thunlp/Ouroboros.</li>
</ul>

<h3>Title: Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment  Analysis</h3>
<ul>
<li><strong>Authors: </strong>S M Rafiuddin, Mohammed Rakib, Sadia Kamal, Arunkumar Bagavathi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13722">https://arxiv.org/abs/2402.13722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13722">https://arxiv.org/pdf/2402.13722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13722]] Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment  Analysis(https://arxiv.org/abs/2402.13722)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Aspect-Based Sentiment Analysis (ABSA) is a fine-grained linguistics problem that entails the extraction of multifaceted aspects, opinions, and sentiments from the given text. Both standalone and compound ABSA tasks have been extensively used in the literature to examine the nuanced information present in online reviews and social media posts. Current ABSA methods often rely on static hyperparameters for attention-masking mechanisms, which can struggle with context adaptation and may overlook the unique relevance of words in varied situations. This leads to challenges in accurately analyzing complex sentences containing multiple aspects with differing sentiments. In this work, we present adaptive masking methods that remove irrelevant tokens based on context to assist in Aspect Term Extraction and Aspect Sentiment Classification subtasks of ABSA. We show with our experiments that the proposed methods outperform the baseline methods in terms of accuracy and F1 scores on four benchmark online review datasets. Further, we show that the proposed methods can be extended with multiple adaptations and demonstrate a qualitative analysis of the proposed approach using sample text for aspect term extraction.</li>
</ul>

<h3>Title: Sparse and Structured Hopfield Networks</h3>
<ul>
<li><strong>Authors: </strong>Saul Santos, Vlad Niculae, Daniel McNamee, Andre F. T. Martins</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13725">https://arxiv.org/abs/2402.13725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13725">https://arxiv.org/pdf/2402.13725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13725]] Sparse and Structured Hopfield Networks(https://arxiv.org/abs/2402.13725)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Modern Hopfield networks have enjoyed recent interest due to their connection to attention in transformers. Our paper provides a unified framework for sparse Hopfield networks by establishing a link with Fenchel-Young losses. The result is a new family of Hopfield-Fenchel-Young energies whose update rules are end-to-end differentiable sparse transformations. We reveal a connection between loss margins, sparsity, and exact memory retrieval. We further extend this framework to structured Hopfield networks via the SparseMAP transformation, which can retrieve pattern associations instead of a single pattern. Experiments on multiple instance learning and text rationalization demonstrate the usefulness of our approach.</li>
</ul>

<h3>Title: Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet  Representation</h3>
<ul>
<li><strong>Authors: </strong>Kihong Kim, Haneol Lee, Jihye Park, Seyeon Kim, Kwanghee Lee, Seungryong Kim, Jaejun Yoo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13729">https://arxiv.org/abs/2402.13729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13729">https://arxiv.org/pdf/2402.13729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13729]] Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet  Representation(https://arxiv.org/abs/2402.13729)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating high-quality videos that synthesize desired realistic content is a challenging task due to their intricate high-dimensionality and complexity of videos. Several recent diffusion-based methods have shown comparable performance by compressing videos to a lower-dimensional latent space, using traditional video autoencoder architecture. However, such method that employ standard frame-wise 2D and 3D convolution fail to fully exploit the spatio-temporal nature of videos. To address this issue, we propose a novel hybrid video diffusion model, called HVDM, which can capture spatio-temporal dependencies more effectively. The HVDM is trained by a hybrid video autoencoder which extracts a disentangled representation of the video including: (i) a global context information captured by a 2D projected latent (ii) a local volume information captured by 3D convolutions with wavelet decomposition (iii) a frequency information for improving the video reconstruction. Based on this disentangled representation, our hybrid autoencoder provide a more comprehensive video latent enriching the generated videos with fine structures and details. Experiments on video generation benchamarks (UCF101, SkyTimelapse, and TaiChi) demonstrate that the proposed approach achieves state-of-the-art video generation quality, showing a wide range of video applications (e.g., long video generation, image-to-video, and video dynamics control).</li>
</ul>

<h3>Title: The Da Vinci Code of Large Pre-trained Language Models: Deciphering  Degenerate Knowledge Neurons</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Chen, Pengfei Cao, Yubo Chen, Yining Wang, Shengping Liu, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13731">https://arxiv.org/abs/2402.13731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13731">https://arxiv.org/pdf/2402.13731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13731]] The Da Vinci Code of Large Pre-trained Language Models: Deciphering  Degenerate Knowledge Neurons(https://arxiv.org/abs/2402.13731)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This study explores the mechanism of factual knowledge storage in pre-trained language models (PLMs). Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). This paper provides a comprehensive definition of DKNs that covers both structural and functional aspects, pioneering the study of structures in PLMs' factual knowledge storage units. Based on this, we introduce the Neurological Topology Clustering method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition. Furthermore, we introduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely integrates model robustness, evolvability, and complexity for a holistic assessment of PLMs. Within this framework, our execution of 34 experiments across 2 PLMs, 4 datasets, and 6 settings highlights the critical role of DKNs. The code will be available soon.</li>
</ul>

<h3>Title: SRNDiff: Short-term Rainfall Nowcasting with Condition Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Xudong Ling, Chaorong Li, Fengqing Qin, Peng Yang, Yuanyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13737">https://arxiv.org/abs/2402.13737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13737">https://arxiv.org/pdf/2402.13737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13737]] SRNDiff: Short-term Rainfall Nowcasting with Condition Diffusion Model(https://arxiv.org/abs/2402.13737)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are widely used in image generation because they can generate high-quality and realistic samples. This is in contrast to generative adversarial networks (GANs) and variational autoencoders (VAEs), which have some limitations in terms of image quality.We introduce the diffusion model to the precipitation forecasting task and propose a short-term precipitation nowcasting with condition diffusion model based on historical observational data, which is referred to as SRNDiff. By incorporating an additional conditional decoder module in the denoising process, SRNDiff achieves end-to-end conditional rainfall prediction. SRNDiff is composed of two networks: a denoising network and a conditional Encoder network. The conditional network is composed of multiple independent UNet networks. These networks extract conditional feature maps at different resolutions, providing accurate conditional information that guides the diffusion model for conditional generation.SRNDiff surpasses GANs in terms of prediction accuracy, although it requires more computational resources.The SRNDiff model exhibits higher stability and efficiency during training than GANs-based approaches, and generates high-quality precipitation distribution samples that better reflect future actual precipitation conditions. This fully validates the advantages and potential of diffusion models in precipitation forecasting, providing new insights for enhancing rainfall prediction.</li>
</ul>

<h3>Title: From Text to CQL: Bridging Natural Language and Corpus Search Engine</h3>
<ul>
<li><strong>Authors: </strong>Luming Lu, Jiyuan An, Yujie Wang, Liner yang, Cunliang Kong, Zhenghao Liu, Shuo Wang, Haozhe Lin, Mingwei Fang, Yaping Huang, Erhong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13740">https://arxiv.org/abs/2402.13740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13740">https://arxiv.org/pdf/2402.13740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13740]] From Text to CQL: Bridging Natural Language and Corpus Search Engine(https://arxiv.org/abs/2402.13740)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Natural Language Processing (NLP) technologies have revolutionized the way we interact with information systems, with a significant focus on converting natural language queries into formal query languages such as SQL. However, less emphasis has been placed on the Corpus Query Language (CQL), a critical tool for linguistic research and detailed analysis within text corpora. The manual construction of CQL queries is a complex and time-intensive task that requires a great deal of expertise, which presents a notable challenge for both researchers and practitioners. This paper presents the first text-to-CQL task that aims to automate the translation of natural language into CQL. We present a comprehensive framework for this task, including a specifically curated large-scale dataset and methodologies leveraging large language models (LLMs) for effective text-to-CQL task. In addition, we established advanced evaluation metrics to assess the syntactic and semantic accuracy of the generated queries. We created innovative LLM-based conversion approaches and detailed experiments. The results demonstrate the efficacy of our methods and provide insights into the complexities of text-to-CQL task.</li>
</ul>

<h3>Title: Unlocking Instructive In-Context Learning with Tabular Prompting for  Relational Triple Extraction</h3>
<ul>
<li><strong>Authors: </strong>Guozheng Li, Wenjun Ke, Peng Wang, Zijie Xu, Ke Ji, Jiajun Liu, Ziyu Shang, Qiqing Luo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13741">https://arxiv.org/abs/2402.13741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13741">https://arxiv.org/pdf/2402.13741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13741]] Unlocking Instructive In-Context Learning with Tabular Prompting for  Relational Triple Extraction(https://arxiv.org/abs/2402.13741)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The in-context learning (ICL) for relational triple extraction (RTE) has achieved promising performance, but still encounters two key challenges: (1) how to design effective prompts and (2) how to select proper demonstrations. Existing methods, however, fail to address these challenges appropriately. On the one hand, they usually recast RTE task to text-to-text prompting formats, which is unnatural and results in a mismatch between the output format at the pre-training time and the inference time for large language models (LLMs). On the other hand, they only utilize surface natural language features and lack consideration of triple semantics in sample selection. These issues are blocking improved performance in ICL for RTE, thus we aim to tackle prompt designing and sample selection challenges simultaneously. To this end, we devise a tabular prompting for RTE (\textsc{TableIE}) which frames RTE task into a table generation task to incorporate explicit structured information into ICL, facilitating conversion of outputs to RTE structures. Then we propose instructive in-context learning (I$^2$CL) which only selects and annotates a few samples considering internal triple semantics in massive unlabeled samples.</li>
</ul>

<h3>Title: LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens</h3>
<ul>
<li><strong>Authors: </strong>Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, Mao Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13753">https://arxiv.org/abs/2402.13753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13753">https://arxiv.org/pdf/2402.13753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13753]] LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens(https://arxiv.org/abs/2402.13753)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations.</li>
</ul>

<h3>Title: Factual Consistency Evaluation of Summarisation in the Era of Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zheheng Luo, Qianqian Xie, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13758">https://arxiv.org/abs/2402.13758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13758">https://arxiv.org/pdf/2402.13758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13758]] Factual Consistency Evaluation of Summarisation in the Era of Large  Language Models(https://arxiv.org/abs/2402.13758)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Factual inconsistency with source documents in automatically generated summaries can lead to misinformation or pose risks. Existing factual consistency(FC) metrics are constrained by their performance, efficiency, and explainability. Recent advances in Large language models (LLMs) have demonstrated remarkable potential in text evaluation but their effectiveness in assessing FC in summarisation remains underexplored. Prior research has mostly focused on proprietary LLMs, leaving essential factors that affect their assessment capabilities unexplored. Additionally, current FC evaluation benchmarks are restricted to news articles, casting doubt on the generality of the FC methods tested on them. In this paper, we first address the gap by introducing TreatFact a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC evaluation across news and clinical domains and analyse the impact of model size, prompts, pre-training and fine-tuning data. Our findings reveal that despite proprietary models prevailing on the task, open-source LLMs lag behind. Nevertheless, there is potential for enhancing the performance of open-source LLMs through increasing model size, expanding pre-training data, and developing well-curated fine-tuning data. Experiments on TreatFact suggest that both previous methods and LLM-based evaluators are unable to capture factual inconsistencies in clinical summaries, posing a new challenge for FC evaluation.</li>
</ul>

<h3>Title: CriticBench: Evaluating Large Language Models as Critic</h3>
<ul>
<li><strong>Authors: </strong>Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen, Xian-ling Mao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13764">https://arxiv.org/abs/2402.13764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13764">https://arxiv.org/pdf/2402.13764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13764]] CriticBench: Evaluating Large Language Models as Critic(https://arxiv.org/abs/2402.13764)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. \shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for \shortname~will be publicly released at \url{https://github.com/gmftbyGMFTBY/CriticBench}.</li>
</ul>

<h3>Title: Spatial-Domain Wireless Jamming with Reconfigurable Intelligent Surfaces</h3>
<ul>
<li><strong>Authors: </strong>Philipp Mackensen, Paul Staat, Stefan Roth, Aydin Sezgin, Christof Paar, Veelasha Moonsamy</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13773">https://arxiv.org/abs/2402.13773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13773">https://arxiv.org/pdf/2402.13773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13773]] Spatial-Domain Wireless Jamming with Reconfigurable Intelligent Surfaces(https://arxiv.org/abs/2402.13773)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Today, we rely heavily on the constant availability of wireless communication systems. As a result, wireless jamming continues to prevail as an imminent threat: Attackers can create deliberate radio interference to overshadow desired signals, leading to denial of service. Although the broadcast nature of radio signal propagation makes such an attack possible in the first place, it likewise poses a challenge for the attacker, preventing precise targeting of single devices. In particular, the jamming signal will likely not only reach the victim receiver but also other neighboring devices. In this work, we introduce spatial control of wireless jamming signals, granting a new degree of freedom to leverage for jamming attacks. Our novel strategy employs an environment-adaptive reconfigurable intelligent surface (RIS), exploiting multipath signal propagation to spatially focus jamming signals on particular victim devices. We investigate this effect through extensive experimentation and show that our approach can disable the wireless communication of a victim device while leaving neighbouring devices unaffected. In particular, we demonstrate complete denial-of-service of a Wi-Fi device while a second device located at a distance as close as 5 mm remains unaffected, sustaining wireless communication at a data rate of 60 Mbit/s. We also show that the attacker can change the attack target on-the-fly, dynamically selecting the device to be jammed.</li>
</ul>

<h3>Title: Deep Generative Models for Offline Policy Learning: Tutorial, Survey,  and Perspectives on Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Jiayu Chen, Bhargav Ganguly, Yang Xu, Yongsheng Mei, Tian Lan, Vaneet Aggarwal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13777">https://arxiv.org/abs/2402.13777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13777">https://arxiv.org/pdf/2402.13777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13777]] Deep Generative Models for Offline Policy Learning: Tutorial, Survey,  and Perspectives on Future Directions(https://arxiv.org/abs/2402.13777)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applications in both offline reinforcement learning (offline RL) and imitation learning (IL). Offline RL and IL are two main branches of offline policy learning and are widely-adopted techniques for sequential decision-making. Specifically, for each type of DGM-based offline policy learning, we distill its fundamental scheme, categorize related works based on the usage of the DGM, and sort out the development process of algorithms in that field. Subsequent to the main content, we provide in-depth discussions on deep generative models and offline policy learning as a summary, based on which we present our perspectives on future research directions. This work offers a hands-on reference for the research progress in deep generative models for offline policy learning, and aims to inspire improved DGM-based offline RL or IL algorithms.</li>
</ul>

<h3>Title: Contextual Molecule Representation Learning from Chemical Reaction  Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Han Tang, Shikun Feng, Bicheng Lin, Yuyan Ni, JIngjing Liu, Wei-Ying Ma, Yanyan Lan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13779">https://arxiv.org/abs/2402.13779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13779">https://arxiv.org/pdf/2402.13779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13779]] Contextual Molecule Representation Learning from Chemical Reaction  Knowledge(https://arxiv.org/abs/2402.13779)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In recent years, self-supervised learning has emerged as a powerful tool to harness abundant unlabelled data for representation learning and has been broadly adopted in diverse areas. However, when applied to molecular representation learning (MRL), prevailing techniques such as masked sub-unit reconstruction often fall short, due to the high degree of freedom in the possible combinations of atoms within molecules, which brings insurmountable complexity to the masking-reconstruction paradigm. To tackle this challenge, we introduce REMO, a self-supervised learning framework that takes advantage of well-defined atom-combination rules in common chemistry. Specifically, REMO pre-trains graph/Transformer encoders on 1.7 million known chemical reactions in the literature. We propose two pre-training objectives: Masked Reaction Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI). REMO offers a novel solution to MRL by exploiting the underlying shared patterns in chemical reactions as \textit{context} for pre-training, which effectively infers meaningful representations of common chemistry knowledge. Such contextual representations can then be utilized to support diverse downstream molecular tasks with minimum finetuning, such as affinity prediction and drug-drug interaction prediction. Extensive experimental results on MoleculeACE, ACNet, drug-drug interaction (DDI), and reaction type classification show that across all tested downstream tasks, REMO outperforms the standard baseline of single-molecule masked modeling used in current MRL. Remarkably, REMO is the pioneering deep learning model surpassing fingerprint-based methods in activity cliff benchmarks.</li>
</ul>

<h3>Title: Opening the Black-Box: A Systematic Review on Explainable AI in Remote  Sensing</h3>
<ul>
<li><strong>Authors: </strong>Adrian Höhl, Ivica Obadic, Miguel Ángel Fernández Torres, Hiba Najjar, Dario Oliveira, Zeynep Akata, Andreas Dengel, Xiao Xiang Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13791">https://arxiv.org/abs/2402.13791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13791">https://arxiv.org/pdf/2402.13791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13791]] Opening the Black-Box: A Systematic Review on Explainable AI in Remote  Sensing(https://arxiv.org/abs/2402.13791)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In recent years, black-box machine learning approaches have become a dominant modeling paradigm for knowledge extraction in Remote Sensing. Despite the potential benefits of uncovering the inner workings of these models with explainable AI, a comprehensive overview summarizing the used explainable AI methods and their objectives, findings, and challenges in Remote Sensing applications is still missing. In this paper, we address this issue by performing a systematic review to identify the key trends of how explainable AI is used in Remote Sensing and shed light on novel explainable AI approaches and emerging directions that tackle specific Remote Sensing challenges. We also reveal the common patterns of explanation interpretation, discuss the extracted scientific insights in Remote Sensing, and reflect on the approaches used for explainable AI methods evaluation. Our review provides a complete summary of the state-of-the-art in the field. Further, we give a detailed outlook on the challenges and promising research directions, representing a basis for novel methodological development and a useful starting point for new researchers in the field of explainable AI in Remote Sensing.</li>
</ul>

<h3>Title: The Geography of Information Diffusion in Online Discourse on Europe and  Migration</h3>
<ul>
<li><strong>Authors: </strong>Elisa Leonardelli, Sara Tonelli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13800">https://arxiv.org/abs/2402.13800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13800">https://arxiv.org/pdf/2402.13800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13800]] The Geography of Information Diffusion in Online Discourse on Europe and  Migration(https://arxiv.org/abs/2402.13800)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The online diffusion of information related to Europe and migration has been little investigated from an external point of view. However, this is a very relevant topic, especially if users have had no direct contact with Europe and its perception depends solely on information retrieved online. In this work we analyse the information circulating online about Europe and migration after retrieving a large amount of data from social media (Twitter), to gain new insights into topics, magnitude, and dynamics of their diffusion. We combine retweets and hashtags network analysis with geolocation of users, linking thus data to geography and allowing analysis from an "outside Europe" perspective, with a special focus on Africa. We also introduce a novel approach based on cross-lingual quotes, i.e. when content in a language is commented and retweeted in another language, assuming these interactions are a proxy for connections between very distant communities. Results show how the majority of online discussions occurs at a national level, especially when discussing migration. Language (English) is pivotal for information to become transnational and reach far. Transnational information flow is strongly unbalanced, with content mainly produced in Europe and amplified outside. Conversely Europe-based accounts tend to be self-referential when they discuss migration-related topics. Football is the most exported topic from Europe worldwide. Moreover, important nodes in the communities discussing migration-related topics include accounts of official institutions and international agencies, together with journalists, news, commentators and activists.</li>
</ul>

<h3>Title: MSTAR: Multi-Scale Backbone Architecture Search for Timeseries  Classification</h3>
<ul>
<li><strong>Authors: </strong>Tue M. Cao, Nhat H. Tran, Hieu H. Pham, Hung T. Nguyen, Le P. Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13822">https://arxiv.org/abs/2402.13822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13822">https://arxiv.org/pdf/2402.13822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13822]] MSTAR: Multi-Scale Backbone Architecture Search for Timeseries  Classification(https://arxiv.org/abs/2402.13822)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Most of the previous approaches to Time Series Classification (TSC) highlight the significance of receptive fields and frequencies while overlooking the time resolution. Hence, unavoidably suffered from scalability issues as they integrated an extensive range of receptive fields into classification models. Other methods, while having a better adaptation for large datasets, require manual design and yet not being able to reach the optimal architecture due to the uniqueness of each dataset. We overcome these challenges by proposing a novel multi-scale search space and a framework for Neural architecture search (NAS), which addresses both the problem of frequency and time resolution, discovering the suitable scale for a specific dataset. We further show that our model can serve as a backbone to employ a powerful Transformer module with both untrained and pre-trained weights. Our search space reaches the state-of-the-art performance on four datasets on four different domains while introducing more than ten highly fine-tuned models for each data.</li>
</ul>

<h3>Title: MLXP: A framework for conducting replicable Machine Learning eXperiments  in Python</h3>
<ul>
<li><strong>Authors: </strong>Michael Arbel, Alexandre Zouaoui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13831">https://arxiv.org/abs/2402.13831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13831">https://arxiv.org/pdf/2402.13831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13831]] MLXP: A framework for conducting replicable Machine Learning eXperiments  in Python(https://arxiv.org/abs/2402.13831)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Replicability in machine learning (ML) research is increasingly concerning due to the utilization of complex non-deterministic algorithms and the dependence on numerous hyper-parameter choices, such as model architecture and training datasets. Ensuring reproducible and replicable results is crucial for advancing the field, yet often requires significant technical effort to conduct systematic and well-organized experiments that yield robust conclusions. Several tools have been developed to facilitate experiment management and enhance reproducibility; however, they often introduce complexity that hinders adoption within the research community, despite being well-handled in industrial settings. To address the challenge of low adoption, we propose MLXP, an open-source, simple, and lightweight experiment management tool based on Python, available at https://github.com/inria-thoth/mlxp . MLXP streamlines the experimental process with minimal practitioner overhead while ensuring a high level of reproducibility.</li>
</ul>

<h3>Title: Large Language Models are Advanced Anonymizers</h3>
<ul>
<li><strong>Authors: </strong>Robin Staab, Mark Vero, Mislav Balunović, Martin Vechev</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13846">https://arxiv.org/abs/2402.13846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13846">https://arxiv.org/pdf/2402.13846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13846]] Large Language Models are Advanced Anonymizers(https://arxiv.org/abs/2402.13846)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>Recent work in privacy research on large language models has shown that they achieve near human-level performance at inferring personal data from real-world online texts. With consistently increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats. This raises the question of how individuals can effectively protect their personal data in sharing online texts. In this work, we take two steps to answer this question: We first present a new setting for evaluating anonymizations in the face of adversarial LLMs inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics. We then present our LLM-based adversarial anonymization framework leveraging the strong inferential capabilities of LLMs to inform our anonymization procedure. In our experimental evaluation, we show on real-world and synthetic online texts how adversarial anonymization outperforms current industry-grade anonymizers both in terms of the resulting utility and privacy.</li>
</ul>

<h3>Title: Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps</h3>
<ul>
<li><strong>Authors: </strong>Gianluca Monaci, Leonid Antsfeld, Boris Chidlovskii, Christian Wolf</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13848">https://arxiv.org/abs/2402.13848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13848">https://arxiv.org/pdf/2402.13848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13848]] Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps(https://arxiv.org/abs/2402.13848)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Bird's-eye view (BEV) maps are an important geometrically structured representation widely used in robotics, in particular self-driving vehicles and terrestrial robots. Existing algorithms either require depth information for the geometric projection, which is not always reliably available, or are trained end-to-end in a fully supervised way to map visual first-person observations to BEV representation, and are therefore restricted to the output modality they have been trained for. In contrast, we propose a new model capable of performing zero-shot projections of any modality available in a first person view to the corresponding BEV map. This is achieved by disentangling the geometric inverse perspective projection from the modality transformation, eg. RGB to occupancy. The method is general and we showcase experiments projecting to BEV three different modalities: semantic segmentation, motion vectors and object bounding boxes detected in first person. We experimentally show that the model outperforms competing methods, in particular the widely used baseline resorting to monocular depth estimation.</li>
</ul>

<h3>Title: VL-Trojan: Multimodal Instruction Backdoor Attacks against  Autoregressive Visual Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Liang, Siyuan Liang, Man Luo, Aishan Liu, Dongchen Han, Ee-Chien Chang, Xiaochun Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13851">https://arxiv.org/abs/2402.13851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13851">https://arxiv.org/pdf/2402.13851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13851]] VL-Trojan: Multimodal Instruction Backdoor Attacks against  Autoregressive Visual Language Models(https://arxiv.org/abs/2402.13851)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Autoregressive Visual Language Models (VLMs) showcase impressive few-shot learning capabilities in a multimodal context. Recently, multimodal instruction tuning has been proposed to further enhance instruction-following abilities. However, we uncover the potential threat posed by backdoor attacks on autoregressive VLMs during instruction tuning. Adversaries can implant a backdoor by injecting poisoned samples with triggers embedded in instructions or images, enabling malicious manipulation of the victim model's predictions with predefined triggers. Nevertheless, the frozen visual encoder in autoregressive VLMs imposes constraints on the learning of conventional image triggers. Additionally, adversaries may encounter restrictions in accessing the parameters and architectures of the victim model. To address these challenges, we propose a multimodal instruction backdoor attack, namely VL-Trojan. Our approach facilitates image trigger learning through an isolating and clustering strategy and enhance black-box-attack efficacy via an iterative character-level text trigger generation method. Our attack successfully induces target outputs during inference, significantly surpassing baselines (+62.52\%) in ASR. Moreover, it demonstrates robustness across various model scales and few-shot in-context reasoning scenarios.</li>
</ul>

<h3>Title: Kuaiji: the First Chinese Accounting Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Jiayuan Luo, Songhua Yang, Xiaoling Qiu, Panyu Chen, Yufei Nai, Wenxuan Zeng, Wentao Zhang, Xinke Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13866">https://arxiv.org/abs/2402.13866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13866">https://arxiv.org/pdf/2402.13866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13866]] Kuaiji: the First Chinese Accounting Large Language Model(https://arxiv.org/abs/2402.13866)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated impressive proficiency in comprehending and generating natural language. However, they encounter difficulties when tasked with adapting to specialized domains such as accounting. To address this challenge, we introduce Kuaiji, a tailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned using the Baichuan framework, which encompasses continuous pre-training and supervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy and response speed. Our contributions encompass the creation of the first Chinese accounting dataset, the establishment of Kuaiji as a leading open-source Chinese accounting LLM, and the validation of its efficacy through real-world accounting scenarios.</li>
</ul>

<h3>Title: Generative Probabilistic Time Series Forecasting and Applications in  Grid Operations</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Wang, Lang Tong, Qing Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13870">https://arxiv.org/abs/2402.13870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13870">https://arxiv.org/pdf/2402.13870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13870]] Generative Probabilistic Time Series Forecasting and Applications in  Grid Operations(https://arxiv.org/abs/2402.13870)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative probabilistic forecasting produces future time series samples according to the conditional probability distribution given past time series observations. Such techniques are essential in risk-based decision-making and planning under uncertainty with broad applications in grid operations, including electricity price forecasting, risk-based economic dispatch, and stochastic optimizations. Inspired by Wiener and Kallianpur's innovation representation, we propose a weak innovation autoencoder architecture and a learning algorithm to extract independent and identically distributed innovation sequences from nonparametric stationary time series. We show that the weak innovation sequence is Bayesian sufficient, which makes the proposed weak innovation autoencoder a canonical architecture for generative probabilistic forecasting. The proposed technique is applied to forecasting highly volatile real-time electricity prices, demonstrating superior performance across multiple forecasting measures over leading probabilistic and point forecasting techniques.</li>
</ul>

<h3>Title: An Explainable Transformer-based Model for Phishing Email Detection: A  Large Language Model Approach</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Amaz Uddin, Iqbal H. Sarker</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13871">https://arxiv.org/abs/2402.13871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13871">https://arxiv.org/pdf/2402.13871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13871]] An Explainable Transformer-based Model for Phishing Email Detection: A  Large Language Model Approach(https://arxiv.org/abs/2402.13871)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm. Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging. Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape. Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges. In this research paper, we present an optimized, fine-tuned transformer-based DistilBERT model designed for the detection of phishing emails. In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues. Through our experiments, we found that our model effectively achieves high accuracy, demonstrating its capability to perform well. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI) techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and Transformer Interpret to explain how our model makes predictions in the context of text classification for phishing emails.</li>
</ul>

<h3>Title: $\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for  In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Liu, Jianfeng Liu, Shaohan Huang, Yuefeng Zhan, Hao Sun, Weiwei Deng, Furu Wei, Qi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13874">https://arxiv.org/abs/2402.13874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13874">https://arxiv.org/pdf/2402.13874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13874]] $\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for  In-Context Learning(https://arxiv.org/abs/2402.13874)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples. Prior work has extensively explored the selection of examples for ICL, predominantly following the "select then organize" paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a $\textit{se}$quential $\textit{se}$lection problem and introduce $\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories illustrate that $\texttt{Se}^2$ markedly surpasses competitive baselines and achieves 42% relative improvement over random selection. Further in-depth analysis show the effectiveness of proposed strategies, highlighting $\texttt{Se}^2$'s exceptional stability and adaptability across various scenarios. Our code will be released to facilitate future research.</li>
</ul>

<h3>Title: Beyond Probabilities: Unveiling the Misalignment in Evaluating Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Lyu, Minghao Wu, Alham Fikri Aji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13887">https://arxiv.org/abs/2402.13887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13887">https://arxiv.org/pdf/2402.13887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13887]] Beyond Probabilities: Unveiling the Misalignment in Evaluating Large  Language Models(https://arxiv.org/abs/2402.13887)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research. However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios. While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question. This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations. Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction. Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output probabilities rather than directly generating responses, owing to computational limitations. We illustrate that these probability-based approaches do not effectively correspond with generative predictions. The outcomes of our study can enhance the understanding of LLM evaluation methodologies and provide insights for future research in this domain.</li>
</ul>

<h3>Title: Non-asymptotic Convergence of Discrete-time Diffusion Models: New  Approach and Improved Rate</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Liang, Peizhong Ju, Yingbin Liang, Ness Shroff</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13901">https://arxiv.org/abs/2402.13901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13901">https://arxiv.org/pdf/2402.13901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13901]] Non-asymptotic Convergence of Discrete-time Diffusion Models: New  Approach and Improved Rate(https://arxiv.org/abs/2402.13901)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The denoising diffusion model emerges recently as a powerful generative technique that converts noise into data. Theoretical convergence guarantee has been mainly studied for continuous-time diffusion models, and has been obtained for discrete-time diffusion models only for distributions with bounded support in the literature. In this paper, we establish the convergence guarantee for substantially larger classes of distributions under discrete-time diffusion models and further improve the convergence rate for distributions with bounded support. In particular, we first establish the convergence rates for both smooth and general (possibly non-smooth) distributions having finite second moment. We then specialize our results to a number of interesting classes of distributions with explicit parameter dependencies, including distributions with Lipschitz scores, Gaussian mixture distributions, and distributions with bounded support. We further propose a novel accelerated sampler and show that it improves the convergence rates of the corresponding regular sampler by orders of magnitude with respect to all system parameters. For distributions with bounded support, our result improves the dimensional dependence of the previous convergence rate by orders of magnitude. Our study features a novel analysis technique that constructs tilting factor representation of the convergence error and exploits Tweedie's formula for handling Taylor expansion power terms.</li>
</ul>

<h3>Title: Calibrating Large Language Models with Sample Consistency</h3>
<ul>
<li><strong>Authors: </strong>Qing Lyu, Kumar Shridhar, Chaitanya Malaviya, Li Zhang, Yanai Elazar, Niket Tandon, Marianna Apidianaki, Mrinmaya Sachan, Chris Callison-Burch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13904">https://arxiv.org/abs/2402.13904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13904">https://arxiv.org/pdf/2402.13904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13904]] Calibrating Large Language Models with Sample Consistency(https://arxiv.org/abs/2402.13904)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs.</li>
</ul>

<h3>Title: Leveraging Collection-Wide Similarities for Unsupervised Document  Structure Extraction</h3>
<ul>
<li><strong>Authors: </strong>Gili Lior, Yoav Goldberg, Gabriel Stanovsky</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13906">https://arxiv.org/abs/2402.13906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13906">https://arxiv.org/pdf/2402.13906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13906]] Leveraging Collection-Wide Similarities for Unsupervised Document  Structure Extraction(https://arxiv.org/abs/2402.13906)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Document collections of various domains, e.g., legal, medical, or financial, often share some underlying collection-wide structure, which captures information that can aid both human users and structure-aware models. We propose to identify the typical structure of document within a collection, which requires to capture recurring topics across the collection, while abstracting over arbitrary header paraphrases, and ground each topic to respective document locations. These requirements pose several challenges: headers that mark recurring topics frequently differ in phrasing, certain section headers are unique to individual documents and do not reflect the typical structure, and the order of topics can vary between documents. Subsequently, we develop an unsupervised graph-based method which leverages both inter- and intra-document similarities, to extract the underlying collection-wide structure. Our evaluations on three diverse domains in both English and Hebrew indicate that our method extracts meaningful collection-wide structure, and we hope that future work will leverage our method for multi-document applications and structure-aware models.</li>
</ul>

<h3>Title: What Linguistic Features and Languages are Important in LLM Translation?</h3>
<ul>
<li><strong>Authors: </strong>Ryandito Diandaru, Lucky Susanto, Zilu Tang, Ayu Purwarianti, Derry Wijaya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13917">https://arxiv.org/abs/2402.13917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13917">https://arxiv.org/pdf/2402.13917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13917]] What Linguistic Features and Languages are Important in LLM Translation?(https://arxiv.org/abs/2402.13917)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate strong capability across multiple tasks, including machine translation. Our study focuses on evaluating Llama2's machine translation capabilities and exploring how translation depends on languages in its training data. Our experiments show that the 7B Llama2 model yields above 10 BLEU score for all languages it has seen, but not always for languages it has not seen. Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count. Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality. Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English. Our discoveries here give new perspectives for the current landscape of LLMs, raising the possibility that LLMs centered around languages other than English may offer a more effective foundation for a multilingual model.</li>
</ul>

<h3>Title: BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for  Cloud Detection and Segmentation in Remote Sensing Imagery</h3>
<ul>
<li><strong>Authors: </strong>Loddo Fabio, Dario Piga, Michelucci Umberto, El Ghazouali Safouane</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13918">https://arxiv.org/abs/2402.13918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13918">https://arxiv.org/pdf/2402.13918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13918]] BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for  Cloud Detection and Segmentation in Remote Sensing Imagery(https://arxiv.org/abs/2402.13918)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena. In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains. Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis. Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response. Within this context, this paper focus on the cloud segmentation from remote sensing imagery. Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications. The quality of resulting products such as applications and research is directly impacted by cloud detection, which plays a key role in the remote sensing data processing pipeline. This paper examines seven cutting-edge semantic segmentation and detection algorithms applied to clouds identification, conducting a benchmark analysis to evaluate their architectural approaches and identify the most performing ones. To increase the model's adaptability, critical elements including the type of imagery and the amount of spectral bands used during training are analyzed. Additionally, this research tries to produce machine learning algorithms that can perform cloud segmentation using only a few spectral bands, including RGB and RGBN-IR combinations. The model's flexibility for a variety of applications and user scenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as datasets. This benchmark can be reproduced using the material from this github link: \url{https://github.com/toelt-llc/cloud\_segmentation\_comparative}.</li>
</ul>

<h3>Title: SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in  Clinical Summarization</h3>
<ul>
<li><strong>Authors: </strong>Prakamya Mishra, Zonghai Yao, Parth Vashisht, Feiyun Ouyang, Beining Wang, Vidhi Dhaval Mody, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13919">https://arxiv.org/abs/2402.13919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13919">https://arxiv.org/pdf/2402.13919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13919]] SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in  Clinical Summarization(https://arxiv.org/abs/2402.13919)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation quality. This work leverages GPT's advanced capabilities in clinical NLP to offer expert-level edit feedback. Through the use of two distinct alignment algorithms (DPO and SALT) based on GPT edit feedback, our goal is to reduce hallucinations and align closely with medical facts, endeavoring to narrow the divide between AI-generated content and factual accuracy. This highlights the substantial potential of GPT edits in enhancing the alignment of clinical factuality.</li>
</ul>

<h3>Title: Large Language Models are Vulnerable to Bait-and-Switch Attacks for  Generating Harmful Content</h3>
<ul>
<li><strong>Authors: </strong>Federico Bianchi, James Zou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13926">https://arxiv.org/abs/2402.13926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13926">https://arxiv.org/pdf/2402.13926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13926]] Large Language Models are Vulnerable to Bait-and-Switch Attacks for  Generating Harmful Content(https://arxiv.org/abs/2402.13926)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>The risks derived from large language models (LLMs) generating deceptive and damaging content have been the subject of considerable research, but even safe generations can lead to problematic downstream impacts. In our study, we shift the focus to how even safe text coming from LLMs can be easily turned into potentially dangerous content through Bait-and-Switch attacks. In such attacks, the user first prompts LLMs with safe questions and then employs a simple find-and-replace post-hoc technique to manipulate the outputs into harmful narratives. The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs. In particular, we stress that focusing on the safety of the verbatim LLM outputs is insufficient and that we also need to consider post-hoc transformations.</li>
</ul>

<h3>Title: SDXL-Lightning: Progressive Adversarial Diffusion Distillation</h3>
<ul>
<li><strong>Authors: </strong>Shanchuan Lin, Anran Wang, Xiao Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13929">https://arxiv.org/abs/2402.13929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13929">https://arxiv.org/pdf/2402.13929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13929]] SDXL-Lightning: Progressive Adversarial Diffusion Distillation(https://arxiv.org/abs/2402.13929)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.</li>
</ul>

<h3>Title: Tumor segmentation on whole slide images: training or prompting?</h3>
<ul>
<li><strong>Authors: </strong>Huaqian Wu, Clara Brémond-Martin, Kévin Bouaou, Cédric Clouchoux</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13932">https://arxiv.org/abs/2402.13932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13932">https://arxiv.org/pdf/2402.13932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13932]] Tumor segmentation on whole slide images: training or prompting?(https://arxiv.org/abs/2402.13932)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Tumor segmentation stands as a pivotal task in cancer diagnosis. Given the immense dimensions of whole slide images (WSI) in histology, deep learning approaches for WSI classification mainly operate at patch-wise or superpixel-wise level. However, these solutions often struggle to capture global WSI information and cannot directly generate the binary mask. Downsampling the WSI and performing semantic segmentation is another possible approach. While this method offers computational efficiency, it necessitates a large amount of annotated data since resolution reduction may lead to information loss. Visual prompting is a novel paradigm that allows the model to perform new tasks by making subtle modifications to the input space, rather than adapting the model itself. Such approach has demonstrated promising results on many computer vision tasks. In this paper, we show the efficacy of visual prompting in the context of tumor segmentation for three distinct organs. In comparison to classical methods trained for this specific task, our findings reveal that, with appropriate prompt examples, visual prompting can achieve comparable or better performance without extensive fine-tuning.</li>
</ul>

<h3>Title: Do Efficient Transformers Really Save Computation?</h3>
<ul>
<li><strong>Authors: </strong>Kai Yang, Jan Ackermann, Zhenyu He, Guhao Feng, Bohang Zhang, Yunzhen Feng, Qiwei Ye, Di He, Liwei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13934">https://arxiv.org/abs/2402.13934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13934">https://arxiv.org/pdf/2402.13934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13934]] Do Efficient Transformers Really Save Computation?(https://arxiv.org/abs/2402.13934)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size. Nonetheless, we identify a class of DP problems for which these models can be more efficient than the standard Transformer. We confirm our theoretical results through experiments on representative DP tasks, adding to the understanding of efficient Transformers' practical strengths and weaknesses.</li>
</ul>

<h3>Title: AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement  Learning</h3>
<ul>
<li><strong>Authors: </strong>Vasudev Gohil, Satwik Patnaik, Dileep Kalathil, Jeyavijayan Rajendran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13946">https://arxiv.org/abs/2402.13946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13946">https://arxiv.org/pdf/2402.13946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13946]] AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement  Learning(https://arxiv.org/abs/2402.13946)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Machine learning has shown great promise in addressing several critical hardware security problems. In particular, researchers have developed novel graph neural network (GNN)-based techniques for detecting intellectual property (IP) piracy, detecting hardware Trojans (HTs), and reverse engineering circuits, to name a few. These techniques have demonstrated outstanding accuracy and have received much attention in the community. However, since these techniques are used for security applications, it is imperative to evaluate them thoroughly and ensure they are robust and do not compromise the security of integrated circuits. In this work, we propose AttackGNN, the first red-team attack on GNN-based techniques in hardware security. To this end, we devise a novel reinforcement learning (RL) agent that generates adversarial examples, i.e., circuits, against the GNN-based techniques. We overcome three challenges related to effectiveness, scalability, and generality to devise a potent RL agent. We target five GNN-based techniques for four crucial classes of problems in hardware security: IP piracy, detecting/localizing HTs, reverse engineering, and hardware obfuscation. Through our approach, we craft circuits that fool all GNNs considered in this work. For instance, to evade IP piracy detection, we generate adversarial pirated circuits that fool the GNN-based defense into classifying our crafted circuits as not pirated. For attacking HT localization GNN, our attack generates HT-infested circuits that fool the defense on all tested circuits. We obtain a similar 100% success rate against GNNs for all classes of problems.</li>
</ul>

<h3>Title: Making Reasoning Matter: Measuring and Improving Faithfulness of  Chain-of-Thought Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Debjit Paul, Robert West, Antoine Bosselut, Boi Faltings</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13950">https://arxiv.org/abs/2402.13950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13950">https://arxiv.org/pdf/2402.13950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13950]] Making Reasoning Matter: Measuring and Improving Faithfulness of  Chain-of-Thought Reasoning(https://arxiv.org/abs/2402.13950)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that FRODO significantly outperforms four competitive baselines. Furthermore, FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets. Finally, we find that FRODO's rationales are more faithful to its final answer predictions than standard supervised fine-tuning.</li>
</ul>

<h3>Title: Measuring Social Biases in Masked Language Models by Proxy of Prediction  Quality</h3>
<ul>
<li><strong>Authors: </strong>Rahul Zalkikar, Kanchan Chandra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13954">https://arxiv.org/abs/2402.13954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13954">https://arxiv.org/pdf/2402.13954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13954]] Measuring Social Biases in Masked Language Models by Proxy of Prediction  Quality(https://arxiv.org/abs/2402.13954)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Social and political scientists often aim to discover and measure distinct biases from text data representations (embeddings). Innovative transformer-based language models produce contextually-aware token embeddings and have achieved state-of-the-art performance for a variety of natural language tasks, but have been shown to encode unwanted biases for downstream applications. In this paper, we evaluate the social biases encoded by transformers trained with the masked language modeling objective using proposed proxy functions within an iterative masking experiment to measure the quality of transformer models' predictions, and assess the preference of MLMs towards disadvantaged and advantaged groups. We compare bias estimations with those produced by other evaluation methods using two benchmark datasets, finding relatively high religious and disability biases across considered MLMs and low gender bias in one dataset relative to the other. Our measures outperform others in their agreement with human annotators. We extend on previous work by evaluating social biases introduced after re-training an MLM under the masked language modeling objective (w.r.t. the model's pre-trained base), and find that proposed measures produce more accurate estimations of relative preference for biased sentences between transformers than others based on our methods.</li>
</ul>

<h3>Title: Towards Building Multilingual Language Model for Medicine</h3>
<ul>
<li><strong>Authors: </strong>Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, Weidi Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13963">https://arxiv.org/abs/2402.13963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13963">https://arxiv.org/pdf/2402.13963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13963]] Towards Building Multilingual Language Model for Medicine(https://arxiv.org/abs/2402.13963)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions. In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general LLMs. second, to monitor the development of multilingual LLMs in medicine, we propose a new multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance compared to all other open-source models, even rivaling GPT-4 on MMedBench. We will make the resources publicly available, including code, model weights, and datasets.</li>
</ul>

<h3>Title: Cybersecurity as a Service</h3>
<ul>
<li><strong>Authors: </strong>John Morris, Stefan Tatschner, Michael P. Heinl, Patrizia Heinl, Thomas Newe, Sven Plaga</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13965">https://arxiv.org/abs/2402.13965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13965">https://arxiv.org/pdf/2402.13965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13965]] Cybersecurity as a Service(https://arxiv.org/abs/2402.13965)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>With the increasing sophistication and sheer number of cyberattacks, more and more companies come to the conclusion that they have to strengthen their cybersecurity posture. At the same time, well-educated Information technology (IT) security personnel are scarce. Cybersecurity as a service (CSaaS) is one possible solution to tackle this problem by outsourcing security functions to managed security service providers (MSSP). This chapter gives an overview of common CSaaS functions and their providers. Moreover, it provides guidance especially for small- and medium-sized businesses, for asking the appropriate questions when it comes to the selection of a specific MSSP.</li>
</ul>

<h3>Title: A Simple and Yet Fairly Effective Defense for Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Sofiane Ennadir, Yassine Abbahaddou, Johannes F. Lutzeyer, Michalis Vazirgiannis, Henrik Boström</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13987">https://arxiv.org/abs/2402.13987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13987">https://arxiv.org/pdf/2402.13987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13987]] A Simple and Yet Fairly Effective Defense for Graph Neural Networks(https://arxiv.org/abs/2402.13987)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust, fair</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have emerged as the dominant approach for machine learning on graph-structured data. However, concerns have arisen regarding the vulnerability of GNNs to small adversarial perturbations. Existing defense methods against such perturbations suffer from high time complexity and can negatively impact the model's performance on clean graphs. To address these challenges, this paper introduces NoisyGNNs, a novel defense method that incorporates noise into the underlying model's architecture. We establish a theoretical connection between noise injection and the enhancement of GNN robustness, highlighting the effectiveness of our approach. We further conduct extensive empirical evaluations on the node classification task to validate our theoretical findings, focusing on two popular GNNs: the GCN and GIN. The results demonstrate that NoisyGNN achieves superior or comparable defense performance to existing methods while minimizing added time complexity. The NoisyGNN approach is model-agnostic, allowing it to be integrated with different GNN architectures. Successful combinations of our NoisyGNN approach with existing defense techniques demonstrate even further improved adversarial defense results. Our code is publicly available at: https://github.com/Sennadir/NoisyGNN.</li>
</ul>

<h3>Title: FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Yongcun Song, Ziqi Wang, Enrique Zuazua</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.DC, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13989">https://arxiv.org/abs/2402.13989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13989">https://arxiv.org/pdf/2402.13989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13989]] FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning(https://arxiv.org/abs/2402.13989)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a promising framework for learning from distributed data while maintaining privacy. The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources. Recently developed FedADMM methods show great resilience to both data and system heterogeneity. However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned. To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa. First, we design an inexactness criterion for the clients' local updates to eliminate the need for empirically setting the local training accuracy. This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle effect. The convergence of the resulting inexact ADMM is proved under the assumption of strongly convex loss functions. Additionally, we present a self-adaptive scheme that dynamically adjusts each client's penalty parameter, enhancing algorithm robustness by mitigating the need for empirical penalty parameter choices for each client. Extensive numerical experiments on both synthetic and real-world datasets are conducted. As validated by some numerical tests, our proposed algorithm can reduce the clients' local computational load significantly and also accelerate the learning process compared to the vanilla FedADMM.</li>
</ul>

<h3>Title: Hallucinations or Attention Misdirection? The Path to Strategic Value  Extraction in Business Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aline Ioste</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14002">https://arxiv.org/abs/2402.14002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14002">https://arxiv.org/pdf/2402.14002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14002]] Hallucinations or Attention Misdirection? The Path to Strategic Value  Extraction in Business Using Large Language Models(https://arxiv.org/abs/2402.14002)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models with transformer architecture have revolutionized the domain of text generation, setting unprecedented benchmarks. Despite their impressive capabilities, LLMs have been criticized for generating outcomes that deviate from factual accuracy or display logical inconsistencies, phenomena commonly referred to as hallucinations. This term, however, has often been misapplied to any results deviating from the instructor's expectations, which this paper defines as attention misdirection rather than true hallucinations. Understanding the distinction between hallucinations and attention misdirection becomes increasingly relevant in business contexts, where the ramifications of such errors can significantly impact the value extraction from these inherently pre-trained models. This paper highlights the best practices of the PGI, Persona, Grouping, and Intelligence, method, a strategic framework that achieved a remarkable error rate of only 3,15 percent across 4,000 responses generated by GPT in response to a real business challenge. It emphasizes that by equipping experimentation with knowledge, businesses can unlock opportunities for innovation through the use of these natively pre-trained models. This reinforces the notion that strategic application grounded in a skilled team can maximize the benefits of emergent technologies such as the LLMs.</li>
</ul>

<h3>Title: Can Watermarks Survive Translation? On the Cross-lingual Consistency of  Text Watermark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, Zhaopeng Tu, Zhuosheng Zhang, Rui Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14007">https://arxiv.org/abs/2402.14007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14007">https://arxiv.org/pdf/2402.14007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14007]] Can Watermarks Survive Translation? On the Cross-lingual Consistency of  Text Watermark for Large Language Models(https://arxiv.org/abs/2402.14007)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse. In this study, we introduce the concept of ''cross-lingual consistency'' in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages. Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose a defense method that increases the AUC from 0.67 to 0.88 under CWRA.</li>
</ul>

<h3>Title: OlympiadBench: A Challenging Benchmark for Promoting AGI with  Olympiad-Level Bilingual Multimodal Scientific Problems</h3>
<ul>
<li><strong>Authors: </strong>Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14008">https://arxiv.org/abs/2402.14008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14008">https://arxiv.org/pdf/2402.14008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14008]] OlympiadBench: A Challenging Benchmark for Promoting AGI with  Olympiad-Level Bilingual Multimodal Scientific Problems(https://arxiv.org/abs/2402.14008)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.23% on OlympiadBench, with a mere 11.28% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors.</li>
</ul>

<h3>Title: Geometry-Informed Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Arturs Berzins, Andreas Radler, Sebastian Sanokowski, Sepp Hochreiter, Johannes Brandstetter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14009">https://arxiv.org/abs/2402.14009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14009">https://arxiv.org/pdf/2402.14009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14009]] Geometry-Informed Neural Networks(https://arxiv.org/abs/2402.14009)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.</li>
</ul>

<h3>Title: Corrective Machine Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Shashwat Goel, Ameya Prabhu, Philip Torr, Ponnurangam Kumaraguru, Amartya Sanyal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14015">https://arxiv.org/abs/2402.14015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14015">https://arxiv.org/pdf/2402.14015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14015]] Corrective Machine Unlearning(https://arxiv.org/abs/2402.14015)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged. We formalize "Corrective Machine Unlearning" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard retraining-from-scratch, require most of the manipulated data to be identified for effective corrective unlearning. However, one approach, SSD, achieves limited success in unlearning adverse effects with just a small portion of the manipulated samples, showing the tractability of this setting. We hope our work spurs research towards developing better methods for corrective unlearning and offers practitioners a new strategy to handle data integrity challenges arising from web-scale training.</li>
</ul>

<h3>Title: Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on  Zero-shot LLM Assessment</h3>
<ul>
<li><strong>Authors: </strong>Vyas Raina, Adian Liusie, Mark Gales</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14016">https://arxiv.org/abs/2402.14016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14016">https://arxiv.org/pdf/2402.14016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14016]] Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on  Zero-shot LLM Assessment(https://arxiv.org/abs/2402.14016)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are powerful zero-shot assessors and are increasingly used in real-world situations such as for written exams or benchmarking systems. Despite this, no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs. This work presents the first study on the adversarial robustness of assessment LLMs, where we search for short universal phrases that when appended to texts can deceive LLMs to provide high assessment scores. Experiments on SummEval and TopicalChat demonstrate that both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks, where in particular LLM-scoring is very susceptible and can yield maximum assessment scores irrespective of the input text quality. Interestingly, such attacks are transferable and phrases learned on smaller open-source LLMs can be applied to larger closed-source models, such as GPT3.5. This highlights the pervasive nature of the adversarial vulnerabilities across different judge-LLM sizes, families and methods. Our findings raise significant concerns on the reliability of LLMs-as-a-judge methods, and underscore the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios.</li>
</ul>

<h3>Title: D-Flow: Differentiating through Flows for Controlled Generation</h3>
<ul>
<li><strong>Authors: </strong>Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, Yaron Lipman</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14017">https://arxiv.org/abs/2402.14017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14017">https://arxiv.org/pdf/2402.14017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14017]] D-Flow: Differentiating through Flows for Controlled Generation(https://arxiv.org/abs/2402.14017)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general. In this work we introduce D-Flow, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point. We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process. We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all.</li>
</ul>

<h3>Title: Coercing LLMs to do and reveal (almost) anything</h3>
<ul>
<li><strong>Authors: </strong>Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, Tom Goldstein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14020">https://arxiv.org/abs/2402.14020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14020">https://arxiv.org/pdf/2402.14020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14020]] Coercing LLMs to do and reveal (almost) anything(https://arxiv.org/abs/2402.14020)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>It has recently been shown that adversarial attacks on large language models (LLMs) can "jailbreak" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction. We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange "glitch" tokens in common LLM vocabularies that should be removed for security reasons.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
