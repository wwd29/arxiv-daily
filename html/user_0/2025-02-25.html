<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-02-25</h1>
<h3>Title: Integrating Domain Knowledge into Large Language Models for Enhanced Fashion Recommendations</h3>
<ul>
<li><strong>Authors: </strong>Zhan Shi, Shanglin Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15696">https://arxiv.org/abs/2502.15696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15696">https://arxiv.org/pdf/2502.15696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15696]] Integrating Domain Knowledge into Large Language Models for Enhanced Fashion Recommendations(https://arxiv.org/abs/2502.15696)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Fashion, deeply rooted in sociocultural dynamics, evolves as individuals emulate styles popularized by influencers and iconic figures. In the quest to replicate such refined tastes using artificial intelligence, traditional fashion ensemble methods have primarily used supervised learning to imitate the decisions of style icons, which falter when faced with distribution shifts, leading to style replication discrepancies triggered by slight variations in input. Meanwhile, large language models (LLMs) have become prominent across various sectors, recognized for their user-friendly interfaces, strong conversational skills, and advanced reasoning capabilities. To address these challenges, we introduce the Fashion Large Language Model (FLLM), which employs auto-prompt generation training strategies to enhance its capacity for delivering personalized fashion advice while retaining essential domain knowledge. Additionally, by integrating a retrieval augmentation technique during inference, the model can better adjust to individual preferences. Our results show that this approach surpasses existing models in accuracy, interpretability, and few-shot learning capabilities.</li>
</ul>

<h3>Title: Data Wrangling Task Automation Using Code-Generating Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ashlesha Akella, Krishnasuri Narayanam</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DB, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15732">https://arxiv.org/abs/2502.15732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15732">https://arxiv.org/pdf/2502.15732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15732]] Data Wrangling Task Automation Using Code-Generating Language Models(https://arxiv.org/abs/2502.15732)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Ensuring data quality in large tabular datasets is a critical challenge, typically addressed through data wrangling tasks. Traditional statistical methods, though efficient, cannot often understand the semantic context and deep learning approaches are resource-intensive, requiring task and dataset-specific training. To overcome these shortcomings, we present an automated system that utilizes large language models to generate executable code for tasks like missing value imputation, error detection, and error correction. Our system aims to identify inherent patterns in the data while leveraging external knowledge, effectively addressing both memory-dependent and memory-independent tasks.</li>
</ul>

<h3>Title: On the Effectiveness of Large Language Models in Automating Categorization of Scientific Texts</h3>
<ul>
<li><strong>Authors: </strong>Gautam Kishore Shahi, Oliver Hummel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15745">https://arxiv.org/abs/2502.15745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15745">https://arxiv.org/pdf/2502.15745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15745]] On the Effectiveness of Large Language Models in Automating Categorization of Scientific Texts(https://arxiv.org/abs/2502.15745)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Large Language Models (LLMs) has led to a multitude of application opportunities. One traditional task for Information Retrieval systems is the summarization and classification of texts, both of which are important for supporting humans in navigating large literature bodies as they e.g. exist with scientific publications. Due to this rapidly growing body of scientific knowledge, recent research has been aiming at building research information systems that not only offer traditional keyword search capabilities, but also novel features such as the automatic detection of research areas that are present at knowledge intensive organizations in academia and industry. To facilitate this idea, we present the results obtained from evaluating a variety of LLMs in their ability to sort scientific publications into hierarchical classifications systems. Using the FORC dataset as ground truth data, we have found that recent LLMs (such as Meta Llama 3.1) are able to reach an accuracy of up to 0.82, which is up to 0.08 better than traditional BERT models.</li>
</ul>

<h3>Title: Physics-consistent machine learning: output projection onto physical manifolds</h3>
<ul>
<li><strong>Authors: </strong>Matilde Valente, Tiago C. Dias, Vasco Guerra, Rodrigo Ventura</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.plasm-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15755">https://arxiv.org/abs/2502.15755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15755">https://arxiv.org/pdf/2502.15755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15755]] Physics-consistent machine learning: output projection onto physical manifolds(https://arxiv.org/abs/2502.15755)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Data-driven machine learning models often require extensive datasets, which can be costly or inaccessible, and their predictions may fail to comply with established physical laws. Current approaches for incorporating physical priors mitigate these issues by penalizing deviations from known physical laws, as in physics-informed neural networks, or by designing architectures that automatically satisfy specific invariants. However, penalization approaches do not guarantee compliance with physical constraints for unseen inputs, and invariant-based methods lack flexibility and generality. We propose a novel physics-consistent machine learning method that directly enforces compliance with physical principles by projecting model outputs onto the manifold defined by these laws. This procedure ensures that predictions inherently adhere to the chosen physical constraints, improving reliability and interpretability. Our method is demonstrated on two systems: a spring-mass system and a low-temperature reactive plasma. Compared to purely data-driven models, our approach significantly reduces errors in physical law compliance, enhances predictive accuracy of physical quantities, and outperforms alternatives when working with simpler models or limited datasets. The proposed projection-based technique is versatile and can function independently or in conjunction with existing physics-informed neural networks, offering a powerful, general, and scalable solution for developing fast and reliable surrogate models of complex physical systems, particularly in resource-constrained scenarios.</li>
</ul>

<h3>Title: TRKM: Twin Restricted Kernel Machines for Classification and Regression</h3>
<ul>
<li><strong>Authors: </strong>A. Quadir, M. Tanveer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15759">https://arxiv.org/abs/2502.15759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15759">https://arxiv.org/pdf/2502.15759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15759]] TRKM: Twin Restricted Kernel Machines for Classification and Regression(https://arxiv.org/abs/2502.15759)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Restricted kernel machines (RKMs) have considerably improved generalization in machine learning. Recent advancements explored various techniques within the RKM framework, integrating kernel functions with least squares support vector machines (LSSVM) to mirror the energy function of restricted Boltzmann machines (RBM), leading to enhanced performance. However, RKMs may face challenges in generalization when dealing with unevenly distributed or complexly clustered data. Additionally, as the dataset size increases, the computational burden of managing high-dimensional feature spaces can become substantial, potentially hindering performance in large-scale datasets. To address these challenges, we propose twin restricted kernel machine (TRKM). TRKM combines the benefits of twin models with the robustness of the RKM framework to enhance classification and regression tasks. By leveraging the Fenchel-Young inequality, we introduce a novel conjugate feature duality, allowing the formulation of classification and regression problems in terms of dual variables. This duality provides an upper bound to the objective function of the TRKM problem, resulting in a new methodology under the RKM framework. The model uses an energy function similar to that of RBM, incorporating both visible and hidden variables corresponding to both classes. Additionally, the kernel trick is employed to map data into a high-dimensional feature space, where the model identifies an optimal separating hyperplane using a regularized least squares approach. Experiments on UCI and KEEL datasets confirm TRKM's superiority over baselines, showcasing its robustness and efficiency in handling complex data. Furthermore, We implemented the TRKM model on the brain age dataset, demonstrating its efficacy in predicting brain age.</li>
</ul>

<h3>Title: Digi-Q: Learning Q-Value Functions for Training Device-Control Agents</h3>
<ul>
<li><strong>Authors: </strong>Hao Bai, Yifei Zhou, Li Erran Li, Sergey Levine, Aviral Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15760">https://arxiv.org/abs/2502.15760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15760">https://arxiv.org/pdf/2502.15760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15760]] Digi-Q: Learning Q-Value Functions for Training Device-Control Agents(https://arxiv.org/abs/2502.15760)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>While a number of existing approaches for building foundation model agents rely on prompting or fine-tuning with human demonstrations, it is not sufficient in dynamic environments (e.g., mobile device control). On-policy reinforcement learning (RL) should address these limitations, but collecting actual rollouts in an environment is often undesirable in truly open-ended agentic problems such as mobile device control or interacting with humans, where each unit of interaction is associated with a cost. In such scenarios, a method for policy learning that can utilize off-policy experience by learning a trained action-value function is much more effective. In this paper, we develop an approach, called Digi-Q, to train VLM-based action-value Q-functions which are then used to extract the agent policy. We study our approach in the mobile device control setting. Digi-Q trains the Q-function using offline temporal-difference (TD) learning, on top of frozen, intermediate-layer features of a VLM. Compared to fine-tuning the whole VLM, this approach saves us compute and enhances scalability. To make the VLM features amenable for representing the Q-function, we need to employ an initial phase of fine-tuning to amplify coverage over actionable information needed for value function. Once trained, we use this Q-function via a Best-of-N policy extraction operator that imitates the best action out of multiple candidate actions from the current policy as ranked by the value function, enabling policy improvement without environment interaction. Digi-Q outperforms several prior methods on user-scale device control tasks in Android-in-the-Wild, attaining 21.2% improvement over prior best-performing method. In some cases, our Digi-Q approach already matches state-of-the-art RL methods that require interaction. The project is open-sourced at this https URL</li>
</ul>

<h3>Title: Generalized Attention Flow: Feature Attribution for Transformer Models via Maximum Flow</h3>
<ul>
<li><strong>Authors: </strong>Behrooz Azarkhalili, Maxwell Libbrecht</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15765">https://arxiv.org/abs/2502.15765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15765">https://arxiv.org/pdf/2502.15765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15765]] Generalized Attention Flow: Feature Attribution for Transformer Models via Maximum Flow(https://arxiv.org/abs/2502.15765)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper introduces Generalized Attention Flow (GAF), a novel feature attribution method for Transformer-based models to address the limitations of current approaches. By extending Attention Flow and replacing attention weights with the generalized Information Tensor, GAF integrates attention weights, their gradients, the maximum flow problem, and the barrier method to enhance the performance of feature attributions. The proposed method exhibits key theoretical properties and mitigates the shortcomings of prior techniques that rely solely on simple aggregation of attention weights. Our comprehensive benchmarking on sequence classification tasks demonstrates that a specific variant of GAF consistently outperforms state-of-the-art feature attribution methods in most evaluation settings, providing a more reliable interpretation of Transformer model outputs.</li>
</ul>

<h3>Title: Learning to Reason from Feedback at Test-Time</h3>
<ul>
<li><strong>Authors: </strong>Yanyang Li, Michael Lyu, Liwei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15771">https://arxiv.org/abs/2502.15771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15771">https://arxiv.org/pdf/2502.15771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15771]] Learning to Reason from Feedback at Test-Time(https://arxiv.org/abs/2502.15771)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Solving complex tasks in a single attempt is challenging for large language models (LLMs). Iterative interaction with the environment and feedback is often required to achieve success, making effective feedback utilization a critical topic. Existing approaches either struggle with length generalization or rely on naive retries without leveraging prior information. In this paper, we introduce FTTT, a novel paradigm that formulates feedback utilization as an optimization problem at test time. Additionally, we propose a learnable test-time optimizer, OpTune, to effectively exploit feedback. Experiments on two LLMs across four reasoning datasets demonstrate that FTTT and OpTune achieve superior scalability and performance.</li>
</ul>

<h3>Title: Self-Supervised Transformers as Iterative Solution Improvers for Constraint Satisfaction</h3>
<ul>
<li><strong>Authors: </strong>Yudong W. Xu, Wenhao Li, Scott Sanner, Elias B. Khalil</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15794">https://arxiv.org/abs/2502.15794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15794">https://arxiv.org/pdf/2502.15794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15794]] Self-Supervised Transformers as Iterative Solution Improvers for Constraint Satisfaction(https://arxiv.org/abs/2502.15794)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present a Transformer-based framework for Constraint Satisfaction Problems (CSPs). CSPs find use in many applications and thus accelerating their solution with machine learning is of wide interest. Most existing approaches rely on supervised learning from feasible solutions or reinforcement learning, paradigms that require either feasible solutions to these NP-Complete CSPs or large training budgets and a complex expert-designed reward signal. To address these challenges, we propose ConsFormer, a self-supervised framework that leverages a Transformer as a solution refiner. ConsFormer constructs a solution to a CSP iteratively in a process that mimics local search. Instead of using feasible solutions as labeled data, we devise differentiable approximations to the discrete constraints of a CSP to guide model training. Our model is trained to improve random assignments for a single step but is deployed iteratively at test time, circumventing the bottlenecks of supervised and reinforcement learning. Our method can tackle out-of-distribution CSPs simply through additional iterations.</li>
</ul>

<h3>Title: Pruning as a Defense: Reducing Memorization in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mansi Gupta, Nikhar Waghela, Sarthak Gupta, Shourya Goel, Sanjif Shanmugavelu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15796">https://arxiv.org/abs/2502.15796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15796">https://arxiv.org/pdf/2502.15796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15796]] Pruning as a Defense: Reducing Memorization in Large Language Models(https://arxiv.org/abs/2502.15796)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models have been shown to memorize significant portions of their training data, which they can reproduce when appropriately prompted. This work investigates the impact of simple pruning techniques on this behavior. Our findings reveal that pruning effectively reduces the extent of memorization in LLMs, demonstrating its potential as a foundational approach for mitigating membership inference attacks.</li>
</ul>

<h3>Title: OCCULT: Evaluating Large Language Models for Offensive Cyber Operation Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Michael Kouremetis, Marissa Dotter, Alex Byrne, Dan Martin, Ethan Michalak, Gianpaolo Russo, Michael Threet, Guido Zarrella</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15797">https://arxiv.org/abs/2502.15797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15797">https://arxiv.org/pdf/2502.15797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15797]] OCCULT: Evaluating Large Language Models for Offensive Cyber Operation Capabilities(https://arxiv.org/abs/2502.15797)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>The prospect of artificial intelligence (AI) competing in the adversarial landscape of cyber security has long been considered one of the most impactful, challenging, and potentially dangerous applications of AI. Here, we demonstrate a new approach to assessing AI's progress towards enabling and scaling real-world offensive cyber operations (OCO) tactics in use by modern threat actors. We detail OCCULT, a lightweight operational evaluation framework that allows cyber security experts to contribute to rigorous and repeatable measurement of the plausible cyber security risks associated with any given large language model (LLM) or AI employed for OCO. We also prototype and evaluate three very different OCO benchmarks for LLMs that demonstrate our approach and serve as examples for building benchmarks under the OCCULT framework. Finally, we provide preliminary evaluation results to demonstrate how this framework allows us to move beyond traditional all-or-nothing tests, such as those crafted from educational exercises like capture-the-flag environments, to contextualize our indicators and warnings in true cyber threat scenarios that present risks to modern infrastructure. We find that there has been significant recent advancement in the risks of AI being used to scale realistic cyber threats. For the first time, we find a model (DeepSeek-R1) is capable of correctly answering over 90% of challenging offensive cyber knowledge tests in our Threat Actor Competency Test for LLMs (TACTL) multiple-choice benchmarks. We also show how Meta's Llama and Mistral's Mixtral model families show marked performance improvements over earlier models against our benchmarks where LLMs act as offensive agents in MITRE's high-fidelity offensive and defensive cyber operations simulation environment, CyberLayer.</li>
</ul>

<h3>Title: MaxSup: Overcoming Representation Collapse in Label Smoothing</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Zhou, Heng Li, Zhi-Qi Cheng, Xudong Yan, Mario Fritz, Margret Keuper</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15798">https://arxiv.org/abs/2502.15798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15798">https://arxiv.org/pdf/2502.15798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15798]] MaxSup: Overcoming Representation Collapse in Label Smoothing(https://arxiv.org/abs/2502.15798)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Label Smoothing (LS) is widely adopted to curb overconfidence in neural network predictions and enhance generalization. However, previous research shows that LS can force feature representations into excessively tight clusters, eroding intra-class distinctions. More recent findings suggest that LS also induces overconfidence in misclassifications, yet the precise mechanism remained unclear. In this work, we decompose the loss term introduced by LS, revealing two key components: (i) a regularization term that functions only when the prediction is correct, and (ii) an error-enhancement term that emerges under misclassifications. This latter term compels the model to reinforce incorrect predictions with exaggerated certainty, further collapsing the feature space. To address these issues, we propose Max Suppression (MaxSup), which uniformly applies the intended regularization to both correct and incorrect predictions by penalizing the top-1 logit instead of the ground-truth logit. Through feature analyses, we show that MaxSup restores intra-class variation and sharpens inter-class boundaries. Extensive experiments on image classification and downstream tasks confirm that MaxSup is a more robust alternative to LS. Code is available at: this https URL.</li>
</ul>

<h3>Title: Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Artyom Kharinaev, Viktor Moskvoretskii, Egor Shvetsov, Kseniia Studenikina, Bykov Mikhail, Evgeny Burnaev</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15799">https://arxiv.org/abs/2502.15799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15799">https://arxiv.org/pdf/2502.15799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15799]] Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models(https://arxiv.org/abs/2502.15799)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as powerful tools for addressing modern challenges and enabling practical applications. However, their computational expense remains a significant barrier to widespread adoption. Quantization has emerged as a promising technique to democratize access and enable low resource device deployment. Despite these advancements, the safety and trustworthiness of quantized models remain underexplored, as prior studies often overlook contemporary architectures and rely on overly simplistic benchmarks and evaluations. To address this gap, we introduce OpenSafetyMini, a novel open-ended safety dataset designed to better distinguish between models. We evaluate 4 state-of-the-art quantization techniques across LLaMA and Mistral models using 4 benchmarks, including human evaluations. Our findings reveal that the optimal quantization method varies for 4-bit precision, while vector quantization techniques deliver the best safety and trustworthiness performance at 2-bit precision, providing foundation for future research.</li>
</ul>

<h3>Title: An explainable transformer circuit for compositional generalization</h3>
<ul>
<li><strong>Authors: </strong>Cheng Tang, Brenden Lake, Mehrdad Jazayeri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15801">https://arxiv.org/abs/2502.15801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15801">https://arxiv.org/pdf/2502.15801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15801]] An explainable transformer circuit for compositional generalization(https://arxiv.org/abs/2502.15801)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Compositional generalization-the systematic combination of known components into novel structures-remains a core challenge in cognitive science and machine learning. Although transformer-based large language models can exhibit strong performance on certain compositional tasks, the underlying mechanisms driving these abilities remain opaque, calling into question their interpretability. In this work, we identify and mechanistically interpret the circuit responsible for compositional induction in a compact transformer. Using causal ablations, we validate the circuit and formalize its operation using a program-like description. We further demonstrate that this mechanistic understanding enables precise activation edits to steer the model's behavior predictably. Our findings advance the understanding of complex behaviors in transformers and highlight such insights can provide a direct pathway for model control.</li>
</ul>

<h3>Title: Megrez-Omni Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Boxun Li, Yadong Li, Zhiyuan Li, Congyi Liu, Weilin Liu, Guowei Niu, Zheyue Tan, Haiyang Xu, Zhuyu Yao, Tao Yuan, Dong Zhou, Yueqing Zhuang, Shengen Yan, Guohao Dai, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15803">https://arxiv.org/abs/2502.15803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15803">https://arxiv.org/pdf/2502.15803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15803]] Megrez-Omni Technical Report(https://arxiv.org/abs/2502.15803)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this work, we present the Megrez models, comprising a language model (Megrez-3B-Instruct) and a multimodal model (Megrez-3B-Omni). These models are designed to deliver fast inference, compactness, and robust edge-side intelligence through a software-hardware co-design approach. Megrez-3B-Instruct offers several advantages, including high accuracy, high speed, ease of use, and a wide range of applications. Building on Megrez-3B-Instruct, Megrez-3B-Omni is an on-device multimodal understanding LLM that supports image, text, and audio analysis. It achieves state-of-the-art accuracy across all three modalities and demonstrates strong versatility and robustness, setting a new benchmark for multimodal AI models.</li>
</ul>

<h3>Title: FragFM: Efficient Fragment-Based Molecular Generation via Discrete Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Joongwon Lee, Seonghwan Kim, Wou Youn Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15805">https://arxiv.org/abs/2502.15805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15805">https://arxiv.org/pdf/2502.15805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15805]] FragFM: Efficient Fragment-Based Molecular Generation via Discrete Flow Matching(https://arxiv.org/abs/2502.15805)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce FragFM, a novel fragment-based discrete flow matching framework for molecular graph this http URL generates molecules at the fragment level, leveraging a coarse-to-fine autoencoding mechanism to reconstruct atom-level details. This approach reduces computational complexity while maintaining high chemical validity, enabling more efficient and scalable molecular generation. We benchmark FragFM against state-of-the-art diffusion- and flow-based models on standard molecular generation benchmarks and natural product datasets, demonstrating superior performance in validity, property control, and sampling efficiency. Notably, FragFM achieves over 99\% validity with significantly fewer sampling steps, improving scalability while preserving molecular diversity. These results highlight the potential of fragment-based generative modeling for large-scale, property-aware molecular design, paving the way for more efficient exploration of chemical space.</li>
</ul>

<h3>Title: A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos</h3>
<ul>
<li><strong>Authors: </strong>Yang Yao, Xuan Tong, Ruofan Wang, Yixu Wang, Lujundong Li, Liang Liu, Yan Teng, Yingchun Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15806">https://arxiv.org/abs/2502.15806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15806">https://arxiv.org/pdf/2502.15806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15806]] A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos(https://arxiv.org/abs/2502.15806)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Reasoning Models (LRMs) have significantly advanced beyond traditional Large Language Models (LLMs) with their exceptional logical reasoning capabilities, yet these improvements introduce heightened safety risks. When subjected to jailbreak attacks, their ability to generate more targeted and organized content can lead to greater harm. Although some studies claim that reasoning enables safer LRMs against existing LLM attacks, they overlook the inherent flaws within the reasoning process itself. To address this gap, we propose the first jailbreak attack targeting LRMs, exploiting their unique vulnerabilities stemming from the advanced reasoning capabilities. Specifically, we introduce a Chaos Machine, a novel component to transform attack prompts with diverse one-to-one mappings. The chaos mappings iteratively generated by the machine are embedded into the reasoning chain, which strengthens the variability and complexity and also promotes a more robust attack. Based on this, we construct the Mousetrap framework, which makes attacks projected into nonlinear-like low sample spaces with mismatched generalization enhanced. Also, due to the more competing objectives, LRMs gradually maintain the inertia of unpredictable iterative reasoning and fall into our trap. Success rates of the Mousetrap attacking o1-mini, claude-sonnet and gemini-thinking are as high as 96%, 86% and 98% respectively on our toxic dataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench, attacking claude-sonnet, well-known for its safety, Mousetrap can astonishingly achieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This paper contains inappropriate, offensive and harmful content.</li>
</ul>

<h3>Title: Zero-Shot Commonsense Validation and Reasoning with Large Language Models: An Evaluation on SemEval-2020 Task 4 Dataset</h3>
<ul>
<li><strong>Authors: </strong>Rawand Alfugaha, Mohammad AL-Smadi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15810">https://arxiv.org/abs/2502.15810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15810">https://arxiv.org/pdf/2502.15810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15810]] Zero-Shot Commonsense Validation and Reasoning with Large Language Models: An Evaluation on SemEval-2020 Task 4 Dataset(https://arxiv.org/abs/2502.15810)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>This study evaluates the performance of Large Language Models (LLMs) on SemEval-2020 Task 4 dataset, focusing on commonsense validation and explanation. Our methodology involves evaluating multiple LLMs, including LLaMA3-70B, Gemma2-9B, and Mixtral-8x7B, using zero-shot prompting techniques. The models are tested on two tasks: Task A (Commonsense Validation), where models determine whether a statement aligns with commonsense knowledge, and Task B (Commonsense Explanation), where models identify the reasoning behind implausible statements. Performance is assessed based on accuracy, and results are compared to fine-tuned transformer-based models. The results indicate that larger models outperform previous models and perform closely to human evaluation for Task A, with LLaMA3-70B achieving the highest accuracy of 98.40% in Task A whereas, lagging behind previous models with 93.40% in Task B. However, while models effectively identify implausible statements, they face challenges in selecting the most relevant explanation, highlighting limitations in causal and inferential reasoning.</li>
</ul>

<h3>Title: Spiking Point Transformer for Point Cloud Classification</h3>
<ul>
<li><strong>Authors: </strong>Peixi Wu, Bosong Chai, Hebei Li, Menghua Zheng, Yansong Peng, Zeyu Wang, Xuan Nie, Yueyi Zhang, Xiaoyan Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15811">https://arxiv.org/abs/2502.15811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15811">https://arxiv.org/pdf/2502.15811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15811]] Spiking Point Transformer for Point Cloud Classification(https://arxiv.org/abs/2502.15811)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Spiking Neural Networks (SNNs) offer an attractive and energy-efficient alternative to conventional Artificial Neural Networks (ANNs) due to their sparse binary activation. When SNN meets Transformer, it shows great potential in 2D image processing. However, their application for 3D point cloud remains underexplored. To this end, we present Spiking Point Transformer (SPT), the first transformer-based SNN framework for point cloud classification. Specifically, we first design Queue-Driven Sampling Direct Encoding for point cloud to reduce computational costs while retaining the most effective support points at each time step. We introduce the Hybrid Dynamics Integrate-and-Fire Neuron (HD-IF), designed to simulate selective neuron activation and reduce over-reliance on specific artificial neurons. SPT attains state-of-the-art results on three benchmark datasets that span both real-world and synthetic datasets in the SNN domain. Meanwhile, the theoretical energy consumption of SPT is at least 6.4$\times$ less than its ANN counterpart.</li>
</ul>

<h3>Title: Towards Robust ESG Analysis Against Greenwashing Risks: Aspect-Action Analysis with Cross-Category Generalization</h3>
<ul>
<li><strong>Authors: </strong>Keane Ong, Rui Mao, Deeksha Varshney, Erik Cambria, Gianmarco Mengaldo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15821">https://arxiv.org/abs/2502.15821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15821">https://arxiv.org/pdf/2502.15821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15821]] Towards Robust ESG Analysis Against Greenwashing Risks: Aspect-Action Analysis with Cross-Category Generalization(https://arxiv.org/abs/2502.15821)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Sustainability reports are key for evaluating companies' environmental, social and governance, ESG performance, but their content is increasingly obscured by greenwashing - sustainability claims that are misleading, exaggerated, and fabricated. Yet, existing NLP approaches for ESG analysis lack robustness against greenwashing risks, often extracting insights that reflect misleading or exaggerated sustainability claims rather than objective ESG performance. To bridge this gap, we introduce A3CG - Aspect-Action Analysis with Cross-Category Generalization, as a novel dataset to improve the robustness of ESG analysis amid the prevalence of greenwashing. By explicitly linking sustainability aspects with their associated actions, A3CG facilitates a more fine-grained and transparent evaluation of sustainability claims, ensuring that insights are grounded in verifiable actions rather than vague or misleading rhetoric. Additionally, A3CG emphasizes cross-category generalization. This ensures robust model performance in aspect-action analysis even when companies change their reports to selectively favor certain sustainability areas. Through experiments on A3CG, we analyze state-of-the-art supervised models and LLMs, uncovering their limitations and outlining key directions for future research.</li>
</ul>

<h3>Title: InductionBench: LLMs Fail in the Simplest Complexity Class</h3>
<ul>
<li><strong>Authors: </strong>Wenyue Hua, Tyler Wong, Sun Fei, Liangming Pan, Adam Jardine, William Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.FL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15823">https://arxiv.org/abs/2502.15823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15823">https://arxiv.org/pdf/2502.15823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15823]] InductionBench: LLMs Fail in the Simplest Complexity Class(https://arxiv.org/abs/2502.15823)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs' inductive reasoning capabilities. Coda and data are available this https URL.</li>
</ul>

<h3>Title: CoME: An Unlearning-based Approach to Conflict-free Model Editing</h3>
<ul>
<li><strong>Authors: </strong>Dahyun Jung, Jaehyung Seo, Jaewook Lee, Chanjun Park, Heuiseok Lim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15826">https://arxiv.org/abs/2502.15826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15826">https://arxiv.org/pdf/2502.15826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15826]] CoME: An Unlearning-based Approach to Conflict-free Model Editing(https://arxiv.org/abs/2502.15826)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often retain outdated or incorrect information from pre-training, which undermines their reliability. While model editing methods have been developed to address such errors without full re-training, they frequently suffer from knowledge conflicts, where outdated information interferes with new knowledge. In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing outdated knowledge. CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features. Through experiments on GPT-J and LLaMA-3 using Counterfact and ZsRE datasets, we demonstrate that CoME improves both editing accuracy and model reliability when applied to existing editing methods. Our results highlight that the targeted removal of outdated knowledge is crucial for enhancing model editing effectiveness and maintaining the model's generative performance.</li>
</ul>

<h3>Title: Explainable Artificial Intelligence Model for Evaluating Shear Strength Parameters of Municipal Solid Waste Across Diverse Compositional Profiles</h3>
<ul>
<li><strong>Authors: </strong>Parichat Suknark, Sompote Youwaib, Tipok Kitkobsin, Sirintornthep Towprayoon, Chart Chiemchaisri, Komsilp Wangyao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15827">https://arxiv.org/abs/2502.15827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15827">https://arxiv.org/pdf/2502.15827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15827]] Explainable Artificial Intelligence Model for Evaluating Shear Strength Parameters of Municipal Solid Waste Across Diverse Compositional Profiles(https://arxiv.org/abs/2502.15827)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Accurate prediction of shear strength parameters in Municipal Solid Waste (MSW) remains a critical challenge in geotechnical engineering due to the heterogeneous nature of waste materials and their temporal evolution through degradation processes. This paper presents a novel explainable artificial intelligence (XAI) framework for evaluating cohesion and friction angle across diverse MSW compositional profiles. The proposed model integrates a multi-layer perceptron architecture with SHAP (SHapley Additive exPlanations) analysis to provide transparent insights into how specific waste components influence strength characteristics. Training data encompassed large-scale direct shear tests across various waste compositions and degradation states. The model demonstrated superior predictive accuracy compared to traditional gradient boosting methods, achieving mean absolute percentage errors of 7.42% and 14.96% for friction angle and cohesion predictions, respectively. Through SHAP analysis, the study revealed that fibrous materials and particle size distribution were primary drivers of shear strength variation, with food waste and plastics showing significant but non-linear effects. The model's explainability component successfully quantified these relationships, enabling evidence-based recommendations for waste management practices. This research bridges the gap between advanced machine learning and geotechnical engineering practice, offering a reliable tool for rapid assessment of MSW mechanical properties while maintaining interpretability for engineering decision-making.</li>
</ul>

<h3>Title: A Stronger Mixture of Low-Rank Experts for Fine-Tuning Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Mengyang Sun, Yihao Wang, Tao Feng, Dan Zhang, Yifan Zhu, Jie Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15828">https://arxiv.org/abs/2502.15828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15828">https://arxiv.org/pdf/2502.15828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15828]] A Stronger Mixture of Low-Rank Experts for Fine-Tuning Foundation Models(https://arxiv.org/abs/2502.15828)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In order to streamline the fine-tuning of foundation models, Low-Rank Adapters (LoRAs) have been substantially adopted across various fields, including instruction tuning and domain adaptation. The underlying concept of LoRA involves decomposing a full-rank matrix into the product of two lower-rank matrices, which reduces storage consumption and accelerates the training process. Furthermore, to address the limited expressive capacity of LoRA, the Mixture-of-Expert (MoE) has been introduced for incorporating multiple LoRA adapters. The integration of LoRA experts leads to a visible improvement across several downstream scenes. However, the mixture of LoRAs (MoE-LoRA) still exhibits its low robustness during tuning and inferring. Inspired by the Riemannian Preconditioners which train LoRA as a sub-space projector, we propose a new training strategy for MoE-LoRA, to stabilize and boost its feature learning procedure by multi-space projections. Examinations on SGD and AdamW optimizers demonstrate the effectiveness of our methodology. Source code is available at this https URL.</li>
</ul>

<h3>Title: DeepRTL: Bridging Verilog Understanding and Generation with a Unified Representation Model</h3>
<ul>
<li><strong>Authors: </strong>Yi Liu, Changran Xu, Yunhao Zhou, Zeju Li, Qiang Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15832">https://arxiv.org/abs/2502.15832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15832">https://arxiv.org/pdf/2502.15832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15832]] DeepRTL: Bridging Verilog Understanding and Generation with a Unified Representation Model(https://arxiv.org/abs/2502.15832)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have shown significant potential for automating hardware description language (HDL) code generation from high-level natural language instructions. While fine-tuning has improved LLMs' performance in hardware design tasks, prior efforts have largely focused on Verilog generation, overlooking the equally critical task of Verilog understanding. Furthermore, existing models suffer from weak alignment between natural language descriptions and Verilog code, hindering the generation of high-quality, synthesizable designs. To address these issues, we present DeepRTL, a unified representation model that excels in both Verilog understanding and generation. Based on CodeT5+, DeepRTL is fine-tuned on a comprehensive dataset that aligns Verilog code with rich, multi-level natural language descriptions. We also introduce the first benchmark for Verilog understanding and take the initiative to apply embedding similarity and GPT Score to evaluate the models' understanding capabilities. These metrics capture semantic similarity more accurately than traditional methods like BLEU and ROUGE, which are limited to surface-level n-gram overlaps. By adapting curriculum learning to train DeepRTL, we enable it to significantly outperform GPT-4 in Verilog understanding tasks, while achieving performance on par with OpenAI's o1-preview model in Verilog generation tasks.</li>
</ul>

<h3>Title: Advancing Out-of-Distribution Detection via Local Neuroplasticity</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Canevaro, Julian Schmidt, Mohammad Sajad Marvi, Hang Yu, Georg Martius, Julian Jordan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15833">https://arxiv.org/abs/2502.15833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15833">https://arxiv.org/pdf/2502.15833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15833]] Advancing Out-of-Distribution Detection via Local Neuroplasticity(https://arxiv.org/abs/2502.15833)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the domain of machine learning, the assumption that training and test data share the same distribution is often violated in real-world scenarios, requiring effective out-of-distribution (OOD) detection. This paper presents a novel OOD detection method that leverages the unique local neuroplasticity property of Kolmogorov-Arnold Networks (KANs). Unlike traditional multilayer perceptrons, KANs exhibit local plasticity, allowing them to preserve learned information while adapting to new tasks. Our method compares the activation patterns of a trained KAN against its untrained counterpart to detect OOD samples. We validate our approach on benchmarks from image and medical domains, demonstrating superior performance and robustness compared to state-of-the-art techniques. These results underscore the potential of KANs in enhancing the reliability of machine learning systems in diverse environments.</li>
</ul>

<h3>Title: Pragmatic Reasoning improves LLM Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhuchen Cao, Sven Apel, Adish Singla, Vera Demberg</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15835">https://arxiv.org/abs/2502.15835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15835">https://arxiv.org/pdf/2502.15835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15835]] Pragmatic Reasoning improves LLM Code Generation(https://arxiv.org/abs/2502.15835)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive potential in translating natural language (NL) instructions into program code. However, user instructions often contain inherent ambiguities, making it challenging for LLMs to generate code that accurately reflects the user's true intent. To address this challenge, researchers have proposed to produce multiple candidates of the program code and then rerank them to identify the best solution. In this paper, we propose CodeRSA, a novel code candidate reranking mechanism built upon the Rational Speech Act (RSA) framework, designed to guide LLMs toward more comprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using one of the latest LLMs on a popular code generation dataset. Our experiment results show that CodeRSA consistently outperforms common baselines, surpasses the state-of-the-art approach in most cases, and demonstrates robust overall performance. These findings underscore the effectiveness of integrating pragmatic reasoning into code candidate reranking, offering a promising direction for enhancing code generation quality in LLMs.</li>
</ul>

<h3>Title: Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haokun Chen, Sebastian Szyller, Weilin Xu, Nageen Himayat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15836">https://arxiv.org/abs/2502.15836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15836">https://arxiv.org/pdf/2502.15836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15836]] Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models(https://arxiv.org/abs/2502.15836)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have become increasingly popular. Their emergent capabilities can be attributed to their massive training datasets. However, these datasets often contain undesirable or inappropriate content, e.g., harmful texts, personal information, and copyrighted material. This has promoted research into machine unlearning that aims to remove information from trained models. In particular, approximate unlearning seeks to achieve information removal by strategically editing the model rather than complete model retraining. Recent work has shown that soft token attacks (STA) can successfully extract purportedly unlearned information from LLMs, thereby exposing limitations in current unlearning methodologies. In this work, we reveal that STAs are an inadequate tool for auditing unlearning. Through systematic evaluation on common unlearning benchmarks (Who Is Harry Potter? and TOFU), we demonstrate that such attacks can elicit any information from the LLM, regardless of (1) the deployed unlearning algorithm, and (2) whether the queried content was originally present in the training corpus. Furthermore, we show that STA with just a few soft tokens (1-10) can elicit random strings over 400-characters long. Thus showing that STAs are too powerful, and misrepresent the effectiveness of the unlearning methods. Our work highlights the need for better evaluation baselines, and more appropriate auditing tools for assessing the effectiveness of unlearning in LLMs.</li>
</ul>

<h3>Title: FedMobile: Enabling Knowledge Contribution-aware Multi-modal Federated Learning with Incomplete Modalities</h3>
<ul>
<li><strong>Authors: </strong>Yi Liu, Cong Wang, Xingliang Yuan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15839">https://arxiv.org/abs/2502.15839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15839">https://arxiv.org/pdf/2502.15839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15839]] FedMobile: Enabling Knowledge Contribution-aware Multi-modal Federated Learning with Incomplete Modalities(https://arxiv.org/abs/2502.15839)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>The Web of Things (WoT) enhances interoperability across web-based and ubiquitous computing platforms while complementing existing IoT standards. The multimodal Federated Learning (FL) paradigm has been introduced to enhance WoT by enabling the fusion of multi-source mobile sensing data while preserving privacy. However, a key challenge in mobile sensing systems using multimodal FL is modality incompleteness, where some modalities may be unavailable or only partially captured, potentially degrading the system's performance and reliability. Current multimodal FL frameworks typically train multiple unimodal FL subsystems or apply interpolation techniques on the node side to approximate missing modalities. However, these approaches overlook the shared latent feature space among incomplete modalities across different nodes and fail to discriminate against low-quality nodes. To address this gap, we present FedMobile, a new knowledge contribution-aware multimodal FL framework designed for robust learning despite missing modalities. FedMobile prioritizes local-to-global knowledge transfer, leveraging cross-node multimodal feature information to reconstruct missing features. It also enhances system performance and resilience to modality heterogeneity through rigorous node contribution assessments and knowledge contribution-aware aggregation rules. Empirical evaluations on five widely recognized multimodal benchmark datasets demonstrate that FedMobile maintains robust learning even when up to 90% of modality information is missing or when data from two modalities are randomly missing, outperforming state-of-the-art baselines.</li>
</ul>

<h3>Title: Hallucination Detection in Large Language Models with Metamorphic Relations</h3>
<ul>
<li><strong>Authors: </strong>Borui Yang, Md Afif Al Mamun, Jie M. Zhang, Gias Uddin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15844">https://arxiv.org/abs/2502.15844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15844">https://arxiv.org/pdf/2502.15844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15844]] Hallucination Detection in Large Language Models with Metamorphic Relations(https://arxiv.org/abs/2502.15844)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are prone to hallucinations, e.g., factually incorrect information, in their responses. These hallucinations present challenges for LLM-based applications that demand high factual accuracy. Existing hallucination detection methods primarily depend on external resources, which can suffer from issues such as low availability, incomplete coverage, privacy concerns, high latency, low reliability, and poor scalability. There are also methods depending on output probabilities, which are often inaccessible for closed-source LLMs like GPT models. This paper presents MetaQA, a self-contained hallucination detection approach that leverages metamorphic relation and prompt mutation. Unlike existing methods, MetaQA operates without any external resources and is compatible with both open-source and closed-source LLMs. MetaQA is based on the hypothesis that if an LLM's response is a hallucination, the designed metamorphic relations will be violated. We compare MetaQA with the state-of-the-art zero-resource hallucination detection method, SelfCheckGPT, across multiple datasets, and on two open-source and two closed-source LLMs. Our results reveal that MetaQA outperforms SelfCheckGPT in terms of precision, recall, and f1 score. For the four LLMs we study, MetaQA outperforms SelfCheckGPT with a superiority margin ranging from 0.041 - 0.113 (for precision), 0.143 - 0.430 (for recall), and 0.154 - 0.368 (for F1-score). For instance, with Mistral-7B, MetaQA achieves an average F1-score of 0.435, compared to SelfCheckGPT's F1-score of 0.205, representing an improvement rate of 112.2%. MetaQA also demonstrates superiority across all different categories of questions.</li>
</ul>

<h3>Title: Verify when Uncertain: Beyond Self-Consistency in Black Box Hallucination Detection</h3>
<ul>
<li><strong>Authors: </strong>Yihao Xue, Kristjan Greenewald, Youssef Mroueh, Baharan Mirzasoleiman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15845">https://arxiv.org/abs/2502.15845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15845">https://arxiv.org/pdf/2502.15845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15845]] Verify when Uncertain: Beyond Self-Consistency in Black Box Hallucination Detection(https://arxiv.org/abs/2502.15845)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) suffer from hallucination problems, which hinder their reliability in sensitive applications. In the black-box setting, several self-consistency-based techniques have been proposed for hallucination detection. We empirically study these techniques and show that they achieve performance close to that of a supervised (still black-box) oracle, suggesting little room for improvement within this paradigm. To address this limitation, we explore cross-model consistency checking between the target model and an additional verifier LLM. With this extra information, we observe improved oracle performance compared to purely self-consistency-based methods. We then propose a budget-friendly, two-stage detection algorithm that calls the verifier model only for a subset of cases. It dynamically switches between self-consistency and cross-consistency based on an uncertainty interval of the self-consistency classifier. We provide a geometric interpretation of consistency-based hallucination detection methods through the lens of kernel mean embeddings, offering deeper theoretical insights. Extensive experiments show that this approach maintains high detection performance while significantly reducing computational cost.</li>
</ul>

<h3>Title: Forecasting Frontier Language Model Agent Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Govind Pimpale, Axel Højmark, Jérémy Scheurer, Marius Hobbhahn</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15850">https://arxiv.org/abs/2502.15850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15850">https://arxiv.org/pdf/2502.15850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15850]] Forecasting Frontier Language Model Agent Capabilities(https://arxiv.org/abs/2502.15850)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>As Language Models (LMs) increasingly operate as autonomous agents, accurately forecasting their capabilities becomes crucial for societal preparedness. We evaluate six forecasting methods that predict downstream capabilities of LM agents. We use "one-step" approaches that predict benchmark scores from input metrics like compute or model release date directly or "two-step" approaches that first predict an intermediate metric like the principal component of cross-benchmark performance (PC-1) and human-evaluated competitive Elo ratings. We evaluate our forecasting methods by backtesting them on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the validated two-step approach (Release Date$\to$Elo$\to$Benchmark) to predict LM agent performance for frontier models on three benchmarks: SWE-Bench Verified (software development), Cybench (cybersecurity assessment), and RE-Bench (ML research engineering). Our forecast predicts that by the beginning of 2026, non-specialized LM agents with low capability elicitation will reach a success rate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach an 87% success rate. Our approach does not account for recent advances in inference-compute scaling and might thus be too conservative.</li>
</ul>

<h3>Title: Control Illusion: The Failure of Instruction Hierarchies in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yilin Geng, Haonan Li, Honglin Mu, Xudong Han, Timothy Baldwin, Omri Abend, Eduard Hovy, Lea Frermann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15851">https://arxiv.org/abs/2502.15851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15851">https://arxiv.org/pdf/2502.15851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15851]] Control Illusion: The Failure of Instruction Hierarchies in Large Language Models(https://arxiv.org/abs/2502.15851)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly deployed with hierarchical instruction schemes, where certain instructions (e.g., system-level directives) are expected to take precedence over others (e.g., user messages). Yet, we lack a systematic understanding of how effectively these hierarchical control mechanisms work. We introduce a systematic evaluation framework based on constraint prioritization to assess how well LLMs enforce instruction hierarchies. Our experiments across six state-of-the-art LLMs reveal that models struggle with consistent instruction prioritization, even for simple formatting conflicts. We find that the widely-adopted system/user prompt separation fails to establish a reliable instruction hierarchy, and models exhibit strong inherent biases toward certain constraint types regardless of their priority designation. While controlled prompt engineering and model fine-tuning show modest improvements, our results indicate that instruction hierarchy enforcement is not robustly realized, calling for deeper architectural innovations beyond surface-level modifications.</li>
</ul>

<h3>Title: Enhancing Domain-Specific Retrieval-Augmented Generation: Synthetic Data Generation and Evaluation using Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Aryan Jadon, Avinash Patil, Shashank Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15854">https://arxiv.org/abs/2502.15854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15854">https://arxiv.org/pdf/2502.15854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15854]] Enhancing Domain-Specific Retrieval-Augmented Generation: Synthetic Data Generation and Evaluation using Reasoning Models(https://arxiv.org/abs/2502.15854)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems face significant performance gaps when applied to technical domains requiring precise information extraction from complex documents. Current evaluation methodologies relying on document-level metrics inadequately capture token-resolution retrieval accuracy that is critical for domain-related documents. We propose a framework combining granular evaluation metrics with synthetic data generation to optimize domain-specific RAG performance. First, we introduce token-aware metrics Precision $\Omega$ and Intersection-over-Union (IoU) that quantify context preservation versus information density trade-offs inherent in technical texts. Second, we develop a reasoning model-driven pipeline using instruction-tuned LLMs (DeepSeek-R1, DeepSeek-R1 distilled variants, and Phi-4) to generate context-anchored QA pairs with discontinuous reference spans across three specialized corpora: SEC 10-K filings (finance), biomedical abstracts (PubMed), and APT threat reports (cybersecurity). Our empirical analysis reveals critical insights: smaller chunks (less than 10 tokens) improve precision by 31-42% (IoU = 0.071 vs. baseline 0.053) at recall costs (-18%), while domain-specific embedding strategies yield 22% variance in optimal chunk sizing (5-20 tokens). The DeepSeek-R1-Distill-Qwen-32B model demonstrates superior concept alignment (+14% mean IoU over alternatives), though no configuration universally dominates. Financial texts favor larger chunks for risk factor coverage (Recall = 0.81 at size = 20), whereas cybersecurity content benefits from atomic segmentation, Precision $\Omega = 0.28$ at size = 5. Our code is available on this https URL</li>
</ul>

<h3>Title: A Critical Assessment of Modern Generative Models' Ability to Replicate Artistic Styles</h3>
<ul>
<li><strong>Authors: </strong>Andrea Asperti, Franky George, Tiberio Marras, Razvan Ciprian Stricescu, Fabio Zanotti</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15856">https://arxiv.org/abs/2502.15856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15856">https://arxiv.org/pdf/2502.15856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15856]] A Critical Assessment of Modern Generative Models' Ability to Replicate Artistic Styles(https://arxiv.org/abs/2502.15856)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In recent years, advancements in generative artificial intelligence have led to the development of sophisticated tools capable of mimicking diverse artistic styles, opening new possibilities for digital creativity and artistic expression. This paper presents a critical assessment of the style replication capabilities of contemporary generative models, evaluating their strengths and limitations across multiple dimensions. We examine how effectively these models reproduce traditional artistic styles while maintaining structural integrity and compositional balance in the generated images. The analysis is based on a new large dataset of AI-generated works imitating artistic styles of the past, holding potential for a wide range of applications: the "AI-pastiche" dataset. The study is supported by extensive user surveys, collecting diverse opinions on the dataset and investigation both technical and aesthetic challenges, including the ability to generate outputs that are realistic and visually convincing, the versatility of models in handling a wide range of artistic styles, and the extent to which they adhere to the content and stylistic specifications outlined in prompts. This paper aims to provide a comprehensive overview of the current state of generative tools in style replication, offering insights into their technical and artistic limitations, potential advancements in model design and training methodologies, and emerging opportunities for enhancing digital artistry, human-AI collaboration, and the broader creative landscape.</li>
</ul>

<h3>Title: PPC-GPT: Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation</h3>
<ul>
<li><strong>Authors: </strong>Tao Fan, Guoqiang Ma, Yuanfeng Song, Lixin Fan, Kai Chen, Qiang Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15857">https://arxiv.org/abs/2502.15857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15857">https://arxiv.org/pdf/2502.15857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15857]] PPC-GPT: Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation(https://arxiv.org/abs/2502.15857)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate, large language model</a></li>
<li><strong>Abstract: </strong>Compressing Large Language Models (LLMs) into task-specific Small Language Models (SLMs) encounters two significant challenges: safeguarding domain-specific knowledge privacy and managing limited resources. To tackle these challenges, we propose PPC-GPT, a innovative privacy-preserving federated framework specifically designed for compressing LLMs into task-specific SLMs via pruning and Chain-of-Thought (COT) distillation. PPC-GPT works on a server-client federated architecture, where the client sends differentially private (DP) perturbed task-specific data to the server's LLM. The LLM then generates synthetic data along with their corresponding rationales. This synthetic data is subsequently used for both LLM pruning and retraining processes. Additionally, we harness COT knowledge distillation, leveraging the synthetic data to further improve the retraining of structurally-pruned SLMs. Our experimental results demonstrate the effectiveness of PPC-GPT across various text generation tasks. By compressing LLMs into task-specific SLMs, PPC-GPT not only achieves competitive performance but also prioritizes data privacy protection.</li>
</ul>

<h3>Title: DOEI: Dual Optimization of Embedding Information for Attention-Enhanced Class Activation Maps</h3>
<ul>
<li><strong>Authors: </strong>Hongjie Zhu, Zeyu Zhang, Guansong Pang, Xu Wang, Shimin Wen, Yu Bai, Daji Ergu, Ying Cai, Yang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15885">https://arxiv.org/abs/2502.15885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15885">https://arxiv.org/pdf/2502.15885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15885]] DOEI: Dual Optimization of Embedding Information for Attention-Enhanced Class Activation Maps(https://arxiv.org/abs/2502.15885)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Weakly supervised semantic segmentation (WSSS) typically utilizes limited semantic annotations to obtain initial Class Activation Maps (CAMs). However, due to the inadequate coupling between class activation responses and semantic information in high-dimensional space, the CAM is prone to object co-occurrence or under-activation, resulting in inferior recognition accuracy. To tackle this issue, we propose DOEI, Dual Optimization of Embedding Information, a novel approach that reconstructs embedding representations through semantic-aware attention weight matrices to optimize the expression capability of embedding information. Specifically, DOEI amplifies tokens with high confidence and suppresses those with low confidence during the class-to-patch interaction. This alignment of activation responses with semantic information strengthens the propagation and decoupling of target features, enabling the generated embeddings to more accurately represent target features in high-level semantic space. In addition, we propose a hybrid-feature alignment module in DOEI that combines RGB values, embedding-guided features, and self-attention weights to increase the reliability of candidate tokens. Comprehensive experiments show that DOEI is an effective plug-and-play module that empowers state-of-the-art visual transformer-based WSSS models to significantly improve the quality of CAMs and segmentation performance on popular benchmarks, including PASCAL VOC (+3.6%, +1.5%, +1.2% mIoU) and MS COCO (+1.2%, +1.6% mIoU). Code will be available at this https URL.</li>
</ul>

<h3>Title: A Close Look at Decomposition-based XAI-Methods for Transformer Language Models</h3>
<ul>
<li><strong>Authors: </strong>Leila Arras, Bruno Puri, Patrick Kahardipraja, Sebastian Lapuschkin, Wojciech Samek</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15886">https://arxiv.org/abs/2502.15886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15886">https://arxiv.org/pdf/2502.15886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15886]] A Close Look at Decomposition-based XAI-Methods for Transformer Language Models(https://arxiv.org/abs/2502.15886)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Various XAI attribution methods have been recently proposed for the transformer architecture, allowing for insights into the decision-making process of large language models by assigning importance scores to input tokens and intermediate representations. One class of methods that seems very promising in this direction includes decomposition-based approaches, i.e., XAI-methods that redistribute the model's prediction logit through the network, as this value is directly related to the prediction. In the previous literature we note though that two prominent methods of this category, namely ALTI-Logit and LRP, have not yet been analyzed in juxtaposition and hence we propose to close this gap by conducting a careful quantitative evaluation w.r.t. ground truth annotations on a subject-verb agreement task, as well as various qualitative inspections, using BERT, GPT-2 and LLaMA-3 as a testbed. Along the way we compare and extend the ALTI-Logit and LRP methods, including the recently proposed AttnLRP variant, from an algorithmic and implementation perspective. We further incorporate in our benchmark two widely-used gradient-based attribution techniques. Finally, we make our carefullly constructed benchmark dataset for evaluating attributions on language models, as well as our code, publicly available in order to foster evaluation of XAI-methods on a well-defined common ground.</li>
</ul>

<h3>Title: RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Min Zhao, Guande He, Yixiao Chen, Hongzhou Zhu, Chongxuan Li, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15894">https://arxiv.org/abs/2502.15894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15894">https://arxiv.org/pdf/2502.15894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15894]] RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers(https://arxiv.org/abs/2502.15894)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Recent advancements in video generation have enabled models to synthesize high-quality, minute-long videos. However, generating even longer videos with temporal coherence remains a major challenge, and existing length extrapolation methods lead to temporal repetition or motion deceleration. In this work, we systematically analyze the role of frequency components in positional embeddings and identify an intrinsic frequency that primarily governs extrapolation behavior. Based on this insight, we propose RIFLEx, a minimal yet effective approach that reduces the intrinsic frequency to suppress repetition while preserving motion consistency, without requiring any additional modifications. RIFLEx offers a true free lunch--achieving high-quality $2\times$ extrapolation on state-of-the-art video diffusion transformers in a completely training-free manner. Moreover, it enhances quality and enables $3\times$ extrapolation by minimal fine-tuning without long videos. Project page and codes: \href{this https URL}{this https URL.}</li>
</ul>

<h3>Title: Directional Gradient Projection for Robust Fine-Tuning of Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Chengyue Huang, Junjiao Tian, Brisa Maneechotesuwan, Shivang Chopra, Zsolt Kira</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15895">https://arxiv.org/abs/2502.15895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15895">https://arxiv.org/pdf/2502.15895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15895]] Directional Gradient Projection for Robust Fine-Tuning of Foundation Models(https://arxiv.org/abs/2502.15895)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Robust fine-tuning aims to adapt large foundation models to downstream tasks while preserving their robustness to distribution shifts. Existing methods primarily focus on constraining and projecting current model towards the pre-trained initialization based on the magnitudes between fine-tuned and pre-trained weights, which often require extensive hyper-parameter tuning and can sometimes result in underfitting. In this work, we propose Directional Gradient Projection (DiGraP), a novel layer-wise trainable method that incorporates directional information from gradients to bridge regularization and multi-objective optimization. Besides demonstrating our method on image classification, as another contribution we generalize this area to the multi-modal evaluation settings for robust fine-tuning. Specifically, we first bridge the uni-modal and multi-modal gap by performing analysis on Image Classification reformulated Visual Question Answering (VQA) benchmarks and further categorize ten out-of-distribution (OOD) VQA datasets by distribution shift types and degree (i.e. near versus far OOD). Experimental results show that DiGraP consistently outperforms existing baselines across Image Classfication and VQA tasks with discriminative and generative backbones, improving both in-distribution (ID) generalization and OOD robustness.</li>
</ul>

<h3>Title: ML-Driven Approaches to Combat Medicare Fraud: Advances in Class Imbalance Solutions, Feature Engineering, Adaptive Learning, and Business Impact</h3>
<ul>
<li><strong>Authors: </strong>Dorsa Farahmandazad, Kasra Danesh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15898">https://arxiv.org/abs/2502.15898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15898">https://arxiv.org/pdf/2502.15898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15898]] ML-Driven Approaches to Combat Medicare Fraud: Advances in Class Imbalance Solutions, Feature Engineering, Adaptive Learning, and Business Impact(https://arxiv.org/abs/2502.15898)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, interpretability</a></li>
<li><strong>Abstract: </strong>Medicare fraud poses a substantial challenge to healthcare systems, resulting in significant financial losses and undermining the quality of care provided to legitimate beneficiaries. This study investigates the use of machine learning (ML) to enhance Medicare fraud detection, addressing key challenges such as class imbalance, high-dimensional data, and evolving fraud patterns. A dataset comprising inpatient claims, outpatient claims, and beneficiary details was used to train and evaluate five ML models: Random Forest, KNN, LDA, Decision Tree, and AdaBoost. Data preprocessing techniques included resampling SMOTE method to address the class imbalance, feature selection for dimensionality reduction, and aggregation of diagnostic and procedural codes. Random Forest emerged as the best-performing model, achieving a training accuracy of 99.2% and validation accuracy of 98.8%, and F1-score (98.4%). The Decision Tree also performed well, achieving a validation accuracy of 96.3%. KNN and AdaBoost demonstrated moderate performance, with validation accuracies of 79.2% and 81.1%, respectively, while LDA struggled with a validation accuracy of 63.3% and a low recall of 16.6%. The results highlight the importance of advanced resampling techniques, feature engineering, and adaptive learning in detecting Medicare fraud effectively. This study underscores the potential of machine learning in addressing the complexities of fraud detection. Future work should explore explainable AI and hybrid models to improve interpretability and performance, ensuring scalable and reliable fraud detection systems that protect healthcare resources and beneficiaries.</li>
</ul>

<h3>Title: TS-OOD: Evaluating Time-Series Out-of-Distribution Detection and Prospective Directions for Progress</h3>
<ul>
<li><strong>Authors: </strong>Onat Gungor, Amanda Sofie Rios, Nilesh Ahuja, Tajana Rosing</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15901">https://arxiv.org/abs/2502.15901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15901">https://arxiv.org/pdf/2502.15901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15901]] TS-OOD: Evaluating Time-Series Out-of-Distribution Detection and Prospective Directions for Progress(https://arxiv.org/abs/2502.15901)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Detecting out-of-distribution (OOD) data is a fundamental challenge in the deployment of machine learning models. From a security standpoint, this is particularly important because OOD test data can result in misleadingly confident yet erroneous predictions, which undermine the reliability of the deployed model. Although numerous models for OOD detection have been developed in computer vision and language, their adaptability to the time-series data domain remains limited and under-explored. Yet, time-series data is ubiquitous across manufacturing and security applications for which OOD is essential. This paper seeks to address this research gap by conducting a comprehensive analysis of modality-agnostic OOD detection algorithms. We evaluate over several multivariate time-series datasets, deep learning architectures, time-series specific data augmentations, and loss functions. Our results demonstrate that: 1) the majority of state-of-the-art OOD methods exhibit limited performance on time-series data, and 2) OOD methods based on deep feature modeling may offer greater advantages for time-series OOD detection, highlighting a promising direction for future time-series OOD detection algorithm development.</li>
</ul>

<h3>Title: IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable LLM-Generated Text Detector</h3>
<ul>
<li><strong>Authors: </strong>Zheng Chen, Yushi Feng, Changyang He, Yue Deng, Hongxi Pu, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15902">https://arxiv.org/abs/2502.15902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15902">https://arxiv.org/pdf/2502.15902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15902]] IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable LLM-Generated Text Detector(https://arxiv.org/abs/2502.15902)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have attained human-level fluency in text generation, which complicates the distinguishing between human-written and LLM-generated texts. This increases the risk of misuse and highlights the need for reliable detectors. Yet, existing detectors exhibit poor robustness on out-of-distribution (OOD) data and attacked data, which is critical for real-world scenarios. Also, they struggle to provide explainable evidence to support their decisions, thus undermining the reliability. In light of these challenges, we propose IPAD (Inverse Prompt for AI Detection), a novel framework consisting of a Prompt Inverter that identifies predicted prompts that could have generated the input text, and a Distinguisher that examines how well the input texts align with the predicted prompts. We develop and examine two versions of Distinguishers. Empirical evaluations demonstrate that both Distinguishers perform significantly better than the baseline methods, with version2 outperforming baselines by 9.73% on in-distribution data (F1-score) and 12.65% on OOD data (AUROC). Furthermore, a user study is conducted to illustrate that IPAD enhances the AI detection trustworthiness by allowing users to directly examine the decision-making evidence, which provides interpretable support for its state-of-the-art detection results.</li>
</ul>

<h3>Title: Graph Attention Convolutional U-NET: A Semantic Segmentation Model for Identifying Flooded Areas</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Umair Danish, Madhushan Buwaneswaran, Tehara Fonseka, Katarina Grolinger</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15907">https://arxiv.org/abs/2502.15907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15907">https://arxiv.org/pdf/2502.15907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15907]] Graph Attention Convolutional U-NET: A Semantic Segmentation Model for Identifying Flooded Areas(https://arxiv.org/abs/2502.15907)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The increasing impact of human-induced climate change and unplanned urban constructions has increased flooding incidents in recent years. Accurate identification of flooded areas is crucial for effective disaster management and urban planning. While few works have utilized convolutional neural networks and transformer-based semantic segmentation techniques for identifying flooded areas from aerial footage, recent developments in graph neural networks have created improvement opportunities. This paper proposes an innovative approach, the Graph Attention Convolutional U-NET (GAC-UNET) model, based on graph neural networks for automated identification of flooded areas. The model incorporates a graph attention mechanism and Chebyshev layers into the U-Net architecture. Furthermore, this paper explores the applicability of transfer learning and model reprogramming to enhance the accuracy of flood area segmentation models. Empirical results demonstrate that the proposed GAC-UNET model, outperforms other approaches with 91\% mAP, 94\% dice score, and 89\% IoU, providing valuable insights for informed decision-making and better planning of future infrastructures in flood-prone areas.</li>
</ul>

<h3>Title: Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zheyuan Liu, Guangyao Dou, Xiangchi Yuan, Chunhui Zhang, Zhaoxuan Tan, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15910">https://arxiv.org/abs/2502.15910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15910">https://arxiv.org/pdf/2502.15910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15910]] Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models(https://arxiv.org/abs/2502.15910)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative models such as Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) trained on massive datasets can lead them to memorize and inadvertently reveal sensitive information, raising ethical and privacy concerns. While some prior works have explored this issue in the context of LLMs, it presents a unique challenge for MLLMs due to the entangled nature of knowledge across modalities, making comprehensive unlearning more difficult. To address this challenge, we propose Modality Aware Neuron Unlearning (MANU), a novel unlearning framework for MLLMs designed to selectively clip neurons based on their relative importance to the targeted forget data, curated for different modalities. Specifically, MANU consists of two stages: important neuron selection and selective pruning. The first stage identifies and collects the most influential neurons across modalities relative to the targeted forget knowledge, while the second stage is dedicated to pruning those selected neurons. MANU effectively isolates and removes the neurons that contribute most to the forget data within each modality, while preserving the integrity of retained knowledge. Our experiments conducted across various MLLM architectures illustrate that MANU can achieve a more balanced and comprehensive unlearning in each modality without largely affecting the overall model utility.</li>
</ul>

<h3>Title: Self-Taught Agentic Long Context Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yufan Zhuang, Xiaodong Yu, Jialian Wu, Ximeng Sun, Ze Wang, Jiang Liu, Yusheng Su, Jingbo Shang, Zicheng Liu, Emad Barsoum</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15920">https://arxiv.org/abs/2502.15920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15920">https://arxiv.org/pdf/2502.15920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15920]] Self-Taught Agentic Long Context Understanding(https://arxiv.org/abs/2502.15920)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Answering complex, long-context questions remains a major challenge for large language models (LLMs) as it requires effective question clarifications and context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a framework designed to enhance an LLM's understanding of such queries by integrating targeted self-clarification with contextual grounding within an agentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC), where models refine their understanding through self-generated clarification questions and corresponding contextual groundings. By scaling inference as a tree search where each node represents a CoC step, we achieve 97.8% answer recall on NarrativeQA with a search depth of up to three and a branching factor of eight. To amortize the high cost of this search process to training, we leverage the preference pairs for each step obtained by the CoC workflow and perform two-stage model finetuning: (1) supervised finetuning to learn effective decomposition strategies, and (2) direct preference optimization to enhance reasoning quality. This enables AgenticLU models to generate clarifications and retrieve relevant context effectively and efficiently in a single inference pass. Extensive experiments across seven long-context tasks demonstrate that AgenticLU significantly outperforms state-of-the-art prompting methods and specialized long-context LLMs, achieving robust multi-hop reasoning while sustaining consistent performance as context length grows.</li>
</ul>

<h3>Title: Improving Consistency in Large Language Models through Chain of Guidance</h3>
<ul>
<li><strong>Authors: </strong>Harsh Raj, Vipul Gupta, Domenic Rosati, Subhabrata Majumdar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15924">https://arxiv.org/abs/2502.15924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15924">https://arxiv.org/pdf/2502.15924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15924]] Improving Consistency in Large Language Models through Chain of Guidance(https://arxiv.org/abs/2502.15924)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Consistency is a fundamental dimension of trustworthiness in Large Language Models (LLMs). For humans to be able to trust LLM-based applications, their outputs should be consistent when prompted with inputs that carry the same meaning or intent. Despite this need, there is no known mechanism to control and guide LLMs to be more consistent at inference time. In this paper, we introduce a novel alignment strategy to maximize semantic consistency in LLM outputs. Our proposal is based on Chain of Guidance (CoG), a multistep prompting technique that generates highly consistent outputs from LLMs. For closed-book question-answering (Q&A) tasks, when compared to direct prompting, the outputs generated using CoG show improved consistency. While other approaches like template-based responses and majority voting may offer alternative paths to consistency, our work focuses on exploring the potential of guided prompting. We use synthetic data sets comprised of consistent input-output pairs to fine-tune LLMs to produce consistent and correct outputs. Our fine-tuned models are more than twice as consistent compared to base models and show strong generalization capabilities by producing consistent outputs over datasets not used in the fine-tuning process.</li>
</ul>

<h3>Title: Approximate Differential Privacy of the $\ell_2$ Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Matthew Joseph, Alex Kulesza, Alexander Yu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15929">https://arxiv.org/abs/2502.15929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15929">https://arxiv.org/pdf/2502.15929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15929]] Approximate Differential Privacy of the $\ell_2$ Mechanism(https://arxiv.org/abs/2502.15929)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>We study the $\ell_2$ mechanism for computing a $d$-dimensional statistic with bounded $\ell_2$ sensitivity under approximate differential privacy. Across a range of privacy parameters, we find that the $\ell_2$ mechanism obtains lower error than the Laplace and Gaussian mechanisms, matching the former at $d=1$ and approaching the latter as $d \to \infty$.</li>
</ul>

<h3>Title: CVE-LLM : Ontology-Assisted Automatic Vulnerability Evaluation Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rikhiya Ghosh, Hans-Martin von Stockhausen, Martin Schmitt, George Marica Vasile, Sanjeev Kumar Karn, Oladimeji Farri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15932">https://arxiv.org/abs/2502.15932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15932">https://arxiv.org/pdf/2502.15932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15932]] CVE-LLM : Ontology-Assisted Automatic Vulnerability Evaluation Using Large Language Models(https://arxiv.org/abs/2502.15932)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>The National Vulnerability Database (NVD) publishes over a thousand new vulnerabilities monthly, with a projected 25 percent increase in 2024, highlighting the crucial need for rapid vulnerability identification to mitigate cybersecurity attacks and save costs and resources. In this work, we propose using large language models (LLMs) to learn vulnerability evaluation from historical assessments of medical device vulnerabilities in a single manufacturer's portfolio. We highlight the effectiveness and challenges of using LLMs for automatic vulnerability evaluation and introduce a method to enrich historical data with cybersecurity ontologies, enabling the system to understand new vulnerabilities without retraining the LLM. Our LLM system integrates with the in-house application - Cybersecurity Management System (CSMS) - to help Siemens Healthineers (SHS) product cybersecurity experts efficiently assess the vulnerabilities in our products. Also, we present guidelines for efficient integration of LLMs into the cybersecurity tool.</li>
</ul>

<h3>Title: Orthogonal Calibration for Asynchronous Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiayun Zhang, Shuheng Li, Haiyu Huang, Xiaofan Yu, Rajesh K. Gupta, Jingbo Shang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15940">https://arxiv.org/abs/2502.15940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15940">https://arxiv.org/pdf/2502.15940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15940]] Orthogonal Calibration for Asynchronous Federated Learning(https://arxiv.org/abs/2502.15940)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Asynchronous federated learning mitigates the inefficiency of conventional synchronous aggregation by integrating updates as they arrive and adjusting their influence based on staleness. Due to asynchrony and data heterogeneity, learning objectives at the global and local levels are inherently inconsistent -- global optimization trajectories may conflict with ongoing local updates. Existing asynchronous methods simply distribute the latest global weights to clients, which can overwrite local progress and cause model drift. In this paper, we propose OrthoFL, an orthogonal calibration framework that decouples global and local learning progress and adjusts global shifts to minimize interference before merging them into local models. In OrthoFL, clients and the server maintain separate model weights. Upon receiving an update, the server aggregates it into the global weights via a moving average. For client weights, the server computes the global weight shift accumulated during the client's delay and removes the components aligned with the direction of the received update. The resulting parameters lie in a subspace orthogonal to the client update and preserve the maximal information from the global progress. The calibrated global shift is then merged into the client weights for further training. Extensive experiments show that OrthoFL improves accuracy by 9.6% and achieves a 12$\times$ speedup compared to synchronous methods. Moreover, it consistently outperforms state-of-the-art asynchronous baselines under various delay patterns and heterogeneity scenarios.</li>
</ul>

<h3>Title: AutoMedPrompt: A New Framework for Optimizing LLM Medical Prompts Using Textual Gradients</h3>
<ul>
<li><strong>Authors: </strong>Sean Wu, Michael Koo, Fabien Scalzo, Ira Kurtz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15944">https://arxiv.org/abs/2502.15944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15944">https://arxiv.org/pdf/2502.15944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15944]] AutoMedPrompt: A New Framework for Optimizing LLM Medical Prompts Using Textual Gradients(https://arxiv.org/abs/2502.15944)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated increasingly sophisticated performance in medical and other fields of knowledge. Traditional methods of creating specialist LLMs require extensive fine-tuning and training of models on large datasets. Recently, prompt engineering, instead of fine-tuning, has shown potential to boost the performance of general foundation models. However, prompting methods such as chain-of-thought (CoT) may not be suitable for all subspecialty, and k-shot approaches may introduce irrelevant tokens into the context space. We present AutoMedPrompt, which explores the use of textual gradients to elicit medically relevant reasoning through system prompt optimization. AutoMedPrompt leverages TextGrad's automatic differentiation via text to improve the ability of general foundation LLMs. We evaluated AutoMedPrompt on Llama 3, an open-source LLM, using several QA benchmarks, including MedQA, PubMedQA, and the nephrology subspecialty-specific NephSAP. Our results show that prompting with textual gradients outperforms previous methods on open-source LLMs and surpasses proprietary models such as GPT-4, Claude 3 Opus, and Med-PaLM 2. AutoMedPrompt sets a new state-of-the-art (SOTA) performance on PubMedQA with an accuracy of 82.6$\%$, while also outperforming previous prompting strategies on open-sourced models for MedQA (77.7$\%$) and NephSAP (63.8$\%$).</li>
</ul>

<h3>Title: Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models</h3>
<ul>
<li><strong>Authors: </strong>Lior Belenki, Alekh Agarwal, Tianze Shi, Kristina Toutanova</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15950">https://arxiv.org/abs/2502.15950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15950">https://arxiv.org/pdf/2502.15950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15950]] Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models(https://arxiv.org/abs/2502.15950)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We propose a method to optimize language model pre-training data mixtures through efficient approximation of the cross-entropy loss corresponding to each candidate mixture via a Mixture of Data Experts (MDE). We use this approximation as a source of additional features in a regression model, trained from observations of model loss for a small number of mixtures. Experiments with Transformer decoder-only language models in the range of 70M to 1B parameters on the SlimPajama dataset show that our method achieves significantly better performance than approaches that train regression models using only the mixture rates as input features. Combining this improved optimization method with an objective that takes into account cross-entropy on end task data leads to superior performance on few-shot downstream evaluations. We also provide theoretical insights on why aggregation of data expert predictions can provide good approximations to model losses for data mixtures.</li>
</ul>

<h3>Title: MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language Models for Biomedical In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Zaifu Zhan, Jun Wang, Shuang Zhou, Jiawen Deng, Rui Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15954">https://arxiv.org/abs/2502.15954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15954">https://arxiv.org/pdf/2502.15954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15954]] MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language Models for Biomedical In-Context Learning(https://arxiv.org/abs/2502.15954)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Objective: To optimize in-context learning in biomedical natural language processing by improving example selection. Methods: We introduce a novel multi-mode retrieval-augmented generation (MMRAG) framework, which integrates four retrieval strategies: (1) Random Mode, selecting examples arbitrarily; (2) Top Mode, retrieving the most relevant examples based on similarity; (3) Diversity Mode, ensuring variation in selected examples; and (4) Class Mode, selecting category-representative examples. This study evaluates MMRAG on three core biomedical NLP tasks: Named Entity Recognition (NER), Relation Extraction (RE), and Text Classification (TC). The datasets used include BC2GM for gene and protein mention recognition (NER), DDI for drug-drug interaction extraction (RE), GIT for general biomedical information extraction (RE), and HealthAdvice for health-related text classification (TC). The framework is tested with two large language models (Llama2-7B, Llama3-8B) and three retrievers (Contriever, MedCPT, BGE-Large) to assess performance across different retrieval strategies. Results: The results from the Random mode indicate that providing more examples in the prompt improves the model's generation performance. Meanwhile, Top mode and Diversity mode significantly outperform Random mode on the RE (DDI) task, achieving an F1 score of 0.9669, a 26.4% improvement. Among the three retrievers tested, Contriever outperformed the other two in a greater number of experiments. Additionally, Llama 2 and Llama 3 demonstrated varying capabilities across different tasks, with Llama 3 showing a clear advantage in handling NER tasks. Conclusion: MMRAG effectively enhances biomedical in-context learning by refining example selection, mitigating data scarcity issues, and demonstrating superior adaptability for NLP-driven healthcare applications.</li>
</ul>

<h3>Title: Human Motion Prediction, Reconstruction, and Generation</h3>
<ul>
<li><strong>Authors: </strong>Canxuan Gang, Yiran Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15956">https://arxiv.org/abs/2502.15956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15956">https://arxiv.org/pdf/2502.15956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15956]] Human Motion Prediction, Reconstruction, and Generation(https://arxiv.org/abs/2502.15956)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>This report reviews recent advancements in human motion prediction, reconstruction, and generation. Human motion prediction focuses on forecasting future poses and movements from historical data, addressing challenges like nonlinear dynamics, occlusions, and motion style variations. Reconstruction aims to recover accurate 3D human body movements from visual inputs, often leveraging transformer-based architectures, diffusion models, and physical consistency losses to handle noise and complex poses. Motion generation synthesizes realistic and diverse motions from action labels, textual descriptions, or environmental constraints, with applications in robotics, gaming, and virtual avatars. Additionally, text-to-motion generation and human-object interaction modeling have gained attention, enabling fine-grained and context-aware motion synthesis for augmented reality and robotics. This review highlights key methodologies, datasets, challenges, and future research directions driving progress in these fields.</li>
</ul>

<h3>Title: R$^3$Mem: Bridging Memory Retention and Retrieval via Reversible Compression</h3>
<ul>
<li><strong>Authors: </strong>Xiaoqiang Wang, Suyuchen Wang, Yun Zhu, Bang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15957">https://arxiv.org/abs/2502.15957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15957">https://arxiv.org/pdf/2502.15957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15957]] R$^3$Mem: Bridging Memory Retention and Retrieval via Reversible Compression(https://arxiv.org/abs/2502.15957)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Memory plays a key role in enhancing LLMs' performance when deployed to real-world applications. Existing solutions face trade-offs: explicit memory designs based on external storage require complex management and incur storage overhead, while implicit memory designs that store information via parameters struggle with reliable retrieval. In this paper, we propose R$^3$Mem, a memory network that optimizes both information Retention and Retrieval through Reversible context compression. Specifically, R$^3$Mem employs virtual memory tokens to compress and encode infinitely long histories, further enhanced by a hierarchical compression strategy that refines information from document- to entity-level for improved assimilation across granularities. For retrieval, R$^3$Mem employs a reversible architecture, reconstructing raw data by invoking the model backward with compressed information. Implemented via parameter-efficient fine-tuning, it can integrate seamlessly with any Transformer-based model. Experiments demonstrate that our memory design achieves state-of-the-art performance in long-context language modeling and retrieval-augmented generation tasks. It also significantly outperforms conventional memory modules in long-horizon interaction tasks like conversational agents, showcasing its potential for next-generation retrieval systems.</li>
</ul>

<h3>Title: Forgotten Polygons: Multimodal Large Language Models are Shape-Blind</h3>
<ul>
<li><strong>Authors: </strong>William Rudman, Michal Golovanesky, Amir Bar, Vedant Palit, Yann LeCun, Carsten Eickhoff, Ritambhara Singh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15969">https://arxiv.org/abs/2502.15969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15969">https://arxiv.org/pdf/2502.15969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15969]] Forgotten Polygons: Multimodal Large Language Models are Shape-Blind(https://arxiv.org/abs/2502.15969)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite strong performance on vision-language tasks, Multimodal Large Language Models (MLLMs) struggle with mathematical problem-solving, with both open-source and state-of-the-art models falling short of human performance on visual-math benchmarks. To systematically examine visual-mathematical reasoning in MLLMs, we (1) evaluate their understanding of geometric primitives, (2) test multi-step reasoning, and (3) explore a potential solution to improve visual reasoning capabilities. Our findings reveal fundamental shortcomings in shape recognition, with top models achieving under 50% accuracy in identifying regular polygons. We analyze these failures through the lens of dual-process theory and show that MLLMs rely on System 1 (intuitive, memorized associations) rather than System 2 (deliberate reasoning). Consequently, MLLMs fail to count the sides of both familiar and novel shapes, suggesting they have neither learned the concept of sides nor effectively process visual inputs. Finally, we propose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhances multi-step mathematical reasoning by explicitly referencing visual annotations in diagrams, boosting GPT-4o's accuracy on an irregular polygon side-counting task from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMs remains an open problem, and visually-guided prompting is essential for successfully engaging visual reasoning. Code available at: this https URL.</li>
</ul>

<h3>Title: Multi-Agent Multimodal Models for Multicultural Text to Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Parth Bhalerao, Mounika Yalamarty, Brian Trinh, Oana Ignat</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15972">https://arxiv.org/abs/2502.15972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15972">https://arxiv.org/pdf/2502.15972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15972]] Multi-Agent Multimodal Models for Multicultural Text to Image Generation(https://arxiv.org/abs/2502.15972)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate impressive performance across various multimodal tasks. However, their effectiveness in cross-cultural contexts remains limited due to the predominantly Western-centric nature of existing data and models. Meanwhile, multi-agent models have shown strong capabilities in solving complex tasks. In this paper, we evaluate the performance of LLMs in a multi-agent interaction setting for the novel task of multicultural image generation. Our key contributions are: (1) We introduce MosAIG, a Multi-Agent framework that enhances multicultural Image Generation by leveraging LLMs with distinct cultural personas; (2) We provide a dataset of 9,000 multicultural images spanning five countries, three age groups, two genders, 25 historical landmarks, and five languages; and (3) We demonstrate that multi-agent interactions outperform simple, no-agent models across multiple evaluation metrics, offering valuable insights for future research. Our dataset and models are available at this https URL.</li>
</ul>

<h3>Title: Sparsity May Be All You Need: Sparse Random Parameter Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Jesus Rios, Pierre Dognin, Ronny Luss, Karthikeyan N. Ramamurthy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15975">https://arxiv.org/abs/2502.15975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15975">https://arxiv.org/pdf/2502.15975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15975]] Sparsity May Be All You Need: Sparse Random Parameter Adaptation(https://arxiv.org/abs/2502.15975)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Full fine-tuning of large language models for alignment and task adaptation has become prohibitively expensive as models have grown in size. Parameter-Efficient Fine-Tuning (PEFT) methods aim at significantly reducing the computational and memory resources needed for fine-tuning these models by only training on a small number of parameters instead of all model parameters. Currently, the most popular PEFT method is the Low-Rank Adaptation (LoRA), which freezes the parameters of the model to be fine-tuned and introduces a small set of trainable parameters in the form of low-rank matrices. We propose simply reducing the number of trainable parameters by randomly selecting a small proportion of the model parameters to train on. In this paper, we compare the efficiency and performance of our proposed approach with PEFT methods, including LoRA, as well as full parameter fine-tuning.</li>
</ul>

<h3>Title: CoRe: Coherency Regularization for Hierarchical Time Series</h3>
<ul>
<li><strong>Authors: </strong>Rares Cristian, Pavithra Harhsa, Georgia Perakis, Brian Quanz</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15983">https://arxiv.org/abs/2502.15983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15983">https://arxiv.org/pdf/2502.15983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15983]] CoRe: Coherency Regularization for Hierarchical Time Series(https://arxiv.org/abs/2502.15983)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Hierarchical time series forecasting presents unique challenges, particularly when dealing with noisy data that may not perfectly adhere to aggregation constraints. This paper introduces a novel approach to soft coherency in hierarchical time series forecasting using neural networks. We present a network coherency regularization method, which we denote as CoRe (Coherency Regularization), a technique that trains neural networks to produce forecasts that are inherently coherent across hierarchies, without strictly enforcing aggregation constraints. Our method offers several key advantages. (1) It provides theoretical guarantees on the coherency of forecasts, even for out-of-sample data. (2) It is adaptable to scenarios where data may contain errors or missing values, making it more robust than strict coherency methods. (3) It can be easily integrated into existing neural network architectures for time series forecasting. We demonstrate the effectiveness of our approach on multiple benchmark datasets, comparing it against state-of-the-art methods in both coherent and noisy data scenarios. Additionally, our method can be used within existing generative probabilistic forecasting frameworks to generate coherent probabilistic forecasts. Our results show improved generalization and forecast accuracy, particularly in the presence of data inconsistencies. On a variety of datasets, including both strictly hierarchically coherent and noisy data, our training method has either equal or better accuracy at all levels of the hierarchy while being strictly more coherent out-of-sample than existing soft-coherency methods.</li>
</ul>

<h3>Title: Mean-Shift Distillation for Diffusion Mode Seeking</h3>
<ul>
<li><strong>Authors: </strong>Vikas Thamizharasan, Nikitas Chatzis, Iliyan Georgiev, Matthew Fisher, Difan Liu, Nanxuan Zhao, Evangelos Kalogerakis, Michal Lukac</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15989">https://arxiv.org/abs/2502.15989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15989">https://arxiv.org/pdf/2502.15989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15989]] Mean-Shift Distillation for Diffusion Mode Seeking(https://arxiv.org/abs/2502.15989)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present mean-shift distillation, a novel diffusion distillation technique that provides a provably good proxy for the gradient of the diffusion output distribution. This is derived directly from mean-shift mode seeking on the distribution, and we show that its extrema are aligned with the modes. We further derive an efficient product distribution sampling procedure to evaluate the gradient. Our method is formulated as a drop-in replacement for score distillation sampling (SDS), requiring neither model retraining nor extensive modification of the sampling procedure. We show that it exhibits superior mode alignment as well as improved convergence in both synthetic and practical setups, yielding higher-fidelity results when applied to both text-to-image and text-to-3D applications with Stable Diffusion.</li>
</ul>

<h3>Title: Med-gte-hybrid: A contextual embedding transformer model for extracting actionable information from clinical texts</h3>
<ul>
<li><strong>Authors: </strong>Aditya Kumar, Simon Rauch, Mario Cypko, Oliver Amft</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15996">https://arxiv.org/abs/2502.15996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15996">https://arxiv.org/pdf/2502.15996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15996]] Med-gte-hybrid: A contextual embedding transformer model for extracting actionable information from clinical texts(https://arxiv.org/abs/2502.15996)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce a novel contextual embedding model med-gte-hybrid that was derived from the gte-large sentence transformer to extract information from unstructured clinical narratives. Our model tuning strategy for med-gte-hybrid combines contrastive learning and a denoising autoencoder. To evaluate the performance of med-gte-hybrid, we investigate several clinical prediction tasks in large patient cohorts extracted from the MIMIC-IV dataset, including Chronic Kidney Disease (CKD) patient prognosis, estimated glomerular filtration rate (eGFR) prediction, and patient mortality prediction. Furthermore, we demonstrate that the med-gte-hybrid model improves patient stratification, clustering, and text retrieval, thus outperforms current state-of-the-art models on the Massive Text Embedding Benchmark (MTEB). While some of our evaluations focus on CKD, our hybrid tuning of sentence transformers could be transferred to other medical domains and has the potential to improve clinical decision-making and personalised treatment pathways in various healthcare applications.</li>
</ul>

<h3>Title: KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse</h3>
<ul>
<li><strong>Authors: </strong>Jingbo Yang, Bairu Hou, Wei Wei, Yujia Bao, Shiyu Chang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16002">https://arxiv.org/abs/2502.16002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16002">https://arxiv.org/pdf/2502.16002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16002]] KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse(https://arxiv.org/abs/2502.16002)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we propose a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation of LLMs when using KV caches computed independently for each document, KVLink introduces three key components: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, using trainable special tokens to restore self-attention across independently encoded documents, and applying mixed-data fine-tuning to enhance performance while preserving the model's original capabilities. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 90% compared to standard LLM inference, making it a scalable and efficient solution for context reuse.</li>
</ul>

<h3>Title: Cross-Model Transferability of Adversarial Patches in Real-time Segmentation for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Prashant Shekhar, Bidur Devkota, Dumindu Samaraweera, Laxima Niure Kandel, Manoj Babu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16012">https://arxiv.org/abs/2502.16012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16012">https://arxiv.org/pdf/2502.16012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16012]] Cross-Model Transferability of Adversarial Patches in Real-time Segmentation for Autonomous Driving(https://arxiv.org/abs/2502.16012)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Adversarial attacks pose a significant threat to deep learning models, particularly in safety-critical applications like healthcare and autonomous driving. Recently, patch based attacks have demonstrated effectiveness in real-time inference scenarios owing to their 'drag and drop' nature. Following this idea for Semantic Segmentation (SS), here we propose a novel Expectation Over Transformation (EOT) based adversarial patch attack that is more realistic for autonomous vehicles. To effectively train this attack we also propose a 'simplified' loss function that is easy to analyze and implement. Using this attack as our basis, we investigate whether adversarial patches once optimized on a specific SS model, can fool other models or architectures. We conduct a comprehensive cross-model transferability analysis of adversarial patches trained on SOTA Convolutional Neural Network (CNN) models such PIDNet-S, PIDNet-M and PIDNet-L, among others. Additionally, we also include the Segformer model to study transferability to Vision Transformers (ViTs). All of our analysis is conducted on the widely used Cityscapes dataset. Our study reveals key insights into how model architectures (CNN vs CNN or CNN vs. Transformer-based) influence attack susceptibility. In particular, we conclude that although the transferability (effectiveness) of attacks on unseen images of any dimension is really high, the attacks trained against one particular model are minimally effective on other models. And this was found to be true for both ViT and CNN based models. Additionally our results also indicate that for CNN-based models, the repercussions of patch attacks are local, unlike ViTs. Per-class analysis reveals that simple-classes like 'sky' suffer less misclassification than others. The code for the project is available at: this https URL</li>
</ul>

<h3>Title: Enhancing LLMs for Identifying and Prioritizing Important Medical Jargons from Electronic Health Record Notes Utilizing Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Won Seok Jang, Sharmin Sultana, Zonghai Yao, Hieu Tran, Zhichao Yang, Sunjae Kwon, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16022">https://arxiv.org/abs/2502.16022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16022">https://arxiv.org/pdf/2502.16022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16022]] Enhancing LLMs for Identifying and Prioritizing Important Medical Jargons from Electronic Health Record Notes Utilizing Data Augmentation(https://arxiv.org/abs/2502.16022)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Objective: OpenNotes enables patients to access EHR notes, but medical jargon can hinder comprehension. To improve understanding, we evaluated closed- and open-source LLMs for extracting and prioritizing key medical terms using prompting, fine-tuning, and data augmentation. Materials and Methods: We assessed LLMs on 106 expert-annotated EHR notes, experimenting with (i) general vs. structured prompts, (ii) zero-shot vs. few-shot prompting, (iii) fine-tuning, and (iv) data augmentation. To enhance open-source models in low-resource settings, we used ChatGPT for data augmentation and applied ranking techniques. We incrementally increased the augmented dataset size (10 to 10,000) and conducted 5-fold cross-validation, reporting F1 score and Mean Reciprocal Rank (MRR). Results and Discussion: Fine-tuning and data augmentation improved performance over other strategies. GPT-4 Turbo achieved the highest F1 (0.433), while Mistral7B with data augmentation had the highest MRR (0.746). Open-source models, when fine-tuned or augmented, outperformed closed-source models. Notably, the best F1 and MRR scores did not always align. Few-shot prompting outperformed zero-shot in vanilla models, and structured prompts yielded different preferences across models. Fine-tuning improved zero-shot performance but sometimes degraded few-shot performance. Data augmentation performed comparably or better than other methods. Conclusion: Our evaluation highlights the effectiveness of prompting, fine-tuning, and data augmentation in improving model performance for medical jargon extraction in low-resource scenarios.</li>
</ul>

<h3>Title: FeatSharp: Your Vision Model Features, Sharper</h3>
<ul>
<li><strong>Authors: </strong>Mike Ranzinger, Greg Heinrich, Pavlo Molchanov, Jan Kautz, Bryan Catanzaro, Andrew Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16025">https://arxiv.org/abs/2502.16025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16025">https://arxiv.org/pdf/2502.16025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16025]] FeatSharp: Your Vision Model Features, Sharper(https://arxiv.org/abs/2502.16025)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The feature maps of vision encoders are fundamental to myriad modern AI tasks, ranging from core perception algorithms (e.g. semantic segmentation, object detection, depth perception, etc.) to modern multimodal understanding in vision-language models (VLMs). Currently, in computer vision, the frontier of general purpose vision backbones are Vision Transformers (ViT), typically trained using contrastive loss (e.g. CLIP). A key problem with most off-the-shelf ViTs, particularly CLIP, is that these models are inflexibly low resolution. Most run at 224x224px, while the "high resolution" versions are around 378-448px, but still inflexible. We introduce a novel method to coherently and cheaply upsample the feature maps of low-res vision encoders while picking up on fine-grained details that would otherwise be lost due to resolution. We demonstrate the effectiveness of this approach on core perception tasks as well as within agglomerative model (RADIO) training as a way of providing richer targets for distillation.</li>
</ul>

<h3>Title: Clinical Inspired MRI Lesion Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Lijun Yan, Churan Wang, Fangwei Zhong, Yizhou Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16032">https://arxiv.org/abs/2502.16032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16032">https://arxiv.org/pdf/2502.16032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16032]] Clinical Inspired MRI Lesion Segmentation(https://arxiv.org/abs/2502.16032)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Magnetic resonance imaging (MRI) is a potent diagnostic tool for detecting pathological tissues in various diseases. Different MRI sequences have different contrast mechanisms and sensitivities for different types of lesions, which pose challenges to accurate and consistent lesion segmentation. In clinical practice, radiologists commonly use the sub-sequence feature, i.e. the difference between post contrast-enhanced T1-weighted (post) and pre-contrast-enhanced (pre) sequences, to locate lesions. Inspired by this, we propose a residual fusion method to learn subsequence representation for MRI lesion segmentation. Specifically, we iteratively and adaptively fuse features from pre- and post-contrast sequences at multiple resolutions, using dynamic weights to achieve optimal fusion and address diverse lesion enhancement patterns. Our method achieves state-of-the-art performances on BraTS2023 dataset for brain tumor segmentation and our in-house breast MRI dataset for breast lesion segmentation. Our method is clinically inspired and has the potential to facilitate lesion segmentation in various applications.</li>
</ul>

<h3>Title: A Multi-Scale Isolation Forest Approach for Real-Time Detection and Filtering of FGSM Adversarial Attacks in Video Streams of Autonomous Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Richard Abhulimhen, Negash Begashaw, Gurcan Comert, Chunheng Zhao, Pierluigi Pisu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16044">https://arxiv.org/abs/2502.16044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16044">https://arxiv.org/pdf/2502.16044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16044]] A Multi-Scale Isolation Forest Approach for Real-Time Detection and Filtering of FGSM Adversarial Attacks in Video Streams of Autonomous Vehicles(https://arxiv.org/abs/2502.16044)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Deep Neural Networks (DNNs) have demonstrated remarkable success across a wide range of tasks, particularly in fields such as image classification. However, DNNs are highly susceptible to adversarial attacks, where subtle perturbations are introduced to input images, leading to erroneous model outputs. In today's digital era, ensuring the security and integrity of images processed by DNNs is of critical importance. One of the most prominent adversarial attack methods is the Fast Gradient Sign Method (FGSM), which perturbs images in the direction of the loss gradient to deceive the model. This paper presents a novel approach for detecting and filtering FGSM adversarial attacks in image processing tasks. Our proposed method evaluates 10,000 images, each subjected to five different levels of perturbation, characterized by $\epsilon$ values of 0.01, 0.02, 0.05, 0.1, and 0.2. These perturbations are applied in the direction of the loss gradient. We demonstrate that our approach effectively filters adversarially perturbed images, mitigating the impact of FGSM attacks. The method is implemented in Python, and the source code is publicly available on GitHub for reproducibility and further research.</li>
</ul>

<h3>Title: Human-AI Collaboration in Cloud Security: Cognitive Hierarchy-Driven Deep Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zahra Aref, Sheng Wei, Narayan B. Mandayam</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16054">https://arxiv.org/abs/2502.16054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16054">https://arxiv.org/pdf/2502.16054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16054]] Human-AI Collaboration in Cloud Security: Cognitive Hierarchy-Driven Deep Reinforcement Learning(https://arxiv.org/abs/2502.16054)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack</a></li>
<li><strong>Abstract: </strong>Given the complexity of multi-tenant cloud environments and the need for real-time threat mitigation, Security Operations Centers (SOCs) must integrate AI-driven adaptive defenses against Advanced Persistent Threats (APTs). However, SOC analysts struggle with countering adaptive adversarial tactics, necessitating intelligent decision-support frameworks. To enhance human-AI collaboration in SOCs, we propose a Cognitive Hierarchy Theory-driven Deep Q-Network (CHT-DQN) framework that models SOC analysts' decision-making against AI-driven APT bots. The SOC analyst (defender) operates at cognitive level-1, anticipating attacker strategies, while the APT bot (attacker) follows a level-0 exploitative policy. By incorporating CHT into DQN, our framework enhances SOC defense strategies via Attack Graph (AG)-based reinforcement learning. Simulation experiments across varying AG complexities show that CHT-DQN achieves higher data protection and lower action discrepancies compared to standard DQN. A theoretical lower bound analysis further validates its superior Q-value performance. A human-in-the-loop (HITL) evaluation on Amazon Mechanical Turk (MTurk) reveals that SOC analysts using CHT-DQN-driven transition probabilities align better with adaptive attackers, improving data protection. Additionally, human decision patterns exhibit risk aversion after failure and risk-seeking behavior after success, aligning with Prospect Theory. These findings underscore the potential of integrating cognitive modeling into deep reinforcement learning to enhance SOC operations and develop real-time adaptive cloud security mechanisms.</li>
</ul>

<h3>Title: MedForge: Building Medical Foundation Models Like Open Source Software Development</h3>
<ul>
<li><strong>Authors: </strong>Zheling Tan, Kexin Ding, Jin Gao, Mu Zhou, Dimitris Metaxas, Shaoting Zhang, Dequan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16055">https://arxiv.org/abs/2502.16055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16055">https://arxiv.org/pdf/2502.16055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16055]] MedForge: Building Medical Foundation Models Like Open Source Software Development(https://arxiv.org/abs/2502.16055)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Foundational models (FMs) have made significant strides in the healthcare domain. Yet the data silo challenge and privacy concern remain in healthcare systems, hindering safe medical data sharing and collaborative model development among institutions. The collection and curation of scalable clinical datasets increasingly become the bottleneck for training strong FMs. In this study, we propose Medical Foundation Models Merging (MedForge), a cooperative framework enabling a community-driven medical foundation model development, meanwhile preventing the information leakage of raw patient data and mitigating synchronization model development issues across clinical institutions. MedForge offers a bottom-up model construction mechanism by flexibly merging task-specific Low-Rank Adaptation (LoRA) modules, which can adapt to downstream tasks while retaining original model parameters. Through an asynchronous LoRA module integration scheme, the resulting composite model can progressively enhance its comprehensive performance on various clinical tasks. MedForge shows strong performance on multiple clinical datasets (e.g., breast cancer, lung cancer, and colon cancer) collected from different institutions. Our major findings highlight the value of collaborative foundation models in advancing multi-center clinical collaboration effectively and cohesively. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Single-Channel EEG Tokenization Through Time-Frequency Modeling</h3>
<ul>
<li><strong>Authors: </strong>Jathurshan Pradeepkumar, Xihao Piao, Zheng Chen, Jimeng Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16060">https://arxiv.org/abs/2502.16060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16060">https://arxiv.org/pdf/2502.16060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16060]] Single-Channel EEG Tokenization Through Time-Frequency Modeling(https://arxiv.org/abs/2502.16060)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>We introduce TFM-Tokenizer, a novel tokenization framework tailored for EEG analysis that transforms continuous, noisy brain signals into a sequence of discrete, well-represented tokens for various EEG tasks. Conventional approaches typically rely on continuous embeddings and inter-channel dependencies, which are limited in capturing inherent EEG features such as temporally unpredictable patterns and diverse oscillatory waveforms. In contrast, we hypothesize that critical time-frequency features can be effectively captured from a single channel. By learning tokens that encapsulate these intrinsic patterns within a single channel, our approach yields a scalable tokenizer adaptable across diverse EEG settings. We integrate the TFM-Tokenizer with a transformer-based TFM-Encoder, leveraging established pretraining techniques from natural language processing, such as masked token prediction, followed by downstream fine-tuning for various EEG tasks. Experiments across four EEG datasets show that TFM-Token outperforms state-of-the-art methods. On TUEV, our approach improves balanced accuracy and Cohen's Kappa by 5% over baselines. Comprehensive analysis of the learned tokens demonstrates their ability to capture class-distinctive features, enhance frequency representation, and ability to encode time-frequency motifs into distinct tokens, improving interpretability.</li>
</ul>

<h3>Title: A Survey of Model Extraction Attacks and Defenses in Distributed Computing Environments</h3>
<ul>
<li><strong>Authors: </strong>Kaixiang Zhao, Lincan Li, Kaize Ding, Neil Zhenqiang Gong, Yue Zhao, Yushun Dong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16065">https://arxiv.org/abs/2502.16065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16065">https://arxiv.org/pdf/2502.16065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16065]] A Survey of Model Extraction Attacks and Defenses in Distributed Computing Environments(https://arxiv.org/abs/2502.16065)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, defense, attack, robust, steal, extraction, federate</a></li>
<li><strong>Abstract: </strong>Model Extraction Attacks (MEAs) threaten modern machine learning systems by enabling adversaries to steal models, exposing intellectual property and training data. With the increasing deployment of machine learning models in distributed computing environments, including cloud, edge, and federated learning settings, each paradigm introduces distinct vulnerabilities and challenges. Without a unified perspective on MEAs across these distributed environments, organizations risk fragmented defenses, inadequate risk assessments, and substantial economic and privacy losses. This survey is motivated by the urgent need to understand how the unique characteristics of cloud, edge, and federated deployments shape attack vectors and defense requirements. We systematically examine the evolution of attack methodologies and defense mechanisms across these environments, demonstrating how environmental factors influence security strategies in critical sectors such as autonomous vehicles, healthcare, and financial services. By synthesizing recent advances in MEAs research and discussing the limitations of current evaluation practices, this survey provides essential insights for developing robust and adaptive defense strategies. Our comprehensive approach highlights the importance of integrating protective measures across the entire distributed computing landscape to ensure the secure deployment of machine learning models.</li>
</ul>

<h3>Title: Stealing Training Data from Large Language Models in Decentralized Training through Activation Inversion Attack</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Dai, Lin Lu, Pan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16086">https://arxiv.org/abs/2502.16086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16086">https://arxiv.org/pdf/2502.16086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16086]] Stealing Training Data from Large Language Models in Decentralized Training through Activation Inversion Attack(https://arxiv.org/abs/2502.16086)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Decentralized training has become a resource-efficient framework to democratize the training of large language models (LLMs). However, the privacy risks associated with this framework, particularly due to the potential inclusion of sensitive data in training datasets, remain unexplored. This paper identifies a novel and realistic attack surface: the privacy leakage from training data in decentralized training, and proposes \textit{activation inversion attack} (AIA) for the first time. AIA first constructs a shadow dataset comprising text labels and corresponding activations using public datasets. Leveraging this dataset, an attack model can be trained to reconstruct the training data from activations in victim decentralized training. We conduct extensive experiments on various LLMs and publicly available datasets to demonstrate the susceptibility of decentralized training to AIA. These findings highlight the urgent need to enhance security measures in decentralized training to mitigate privacy risks in training LLMs.</li>
</ul>

<h3>Title: Echo: A Large Language Model with Temporal Episodic Memory</h3>
<ul>
<li><strong>Authors: </strong>WenTao Liu, Ruohua Zhang, Aimin Zhou, Feng Gao, JiaLi Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16090">https://arxiv.org/abs/2502.16090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16090">https://arxiv.org/pdf/2502.16090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16090]] Echo: A Large Language Model with Temporal Episodic Memory(https://arxiv.org/abs/2502.16090)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Research on large language models (LLMs) has shown remarkable performance in domains such as mathematics, programming, and literary creation. However, most studies have focused on semantic memory-based question answering, neglecting LLMs' potential to handle episodic memory (EM)-related queries. This oversight has led to suboptimal performance in applications requiring EM, including emotional companionship, personal AI assistants, and AI teachers. To address this gap, we introduce Echo, a LLM enhanced with temporal episodic memory. We propose a Multi-Agent Data Generation Framework that guides the model in generating multi-turn, complex scenario episodic memory dialogue data (EM-Train). Temporal information is innovatively incorporated into the LLM training process, and Echo is trained using the EM-Train. Furthermore, We develop an EM-Test benchmark specifically designed to evaluate LLMs' episodic memory capabilities. The EM-Test assesses performance across various time spans and difficulty levels, providing a comprehensive evaluation of multi-turn episodic memory dialogues. Our experiments demonstrate that Echo significantly outperforms state-of-the-art LLMs on EM-Test. Additionally, a qualitative analysis reveals Echo's potential to exhibit human-like episodic memory capabilities. We will open-source all datasets, code, and model weights.</li>
</ul>

<h3>Title: Privacy-Aware Joint DNN Model Deployment and Partition Optimization for Delay-Efficient Collaborative Edge Inference</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Cheng, Xiaoyu Xia, Hong Wang, Minghui Liwang, Ning Chen, Xuwei Fan, Xianbin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16091">https://arxiv.org/abs/2502.16091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16091">https://arxiv.org/pdf/2502.16091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16091]] Privacy-Aware Joint DNN Model Deployment and Partition Optimization for Delay-Efficient Collaborative Edge Inference(https://arxiv.org/abs/2502.16091)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Edge inference (EI) is a key solution to address the growing challenges of delayed response times, limited scalability, and privacy concerns in cloud-based Deep Neural Network (DNN) inference. However, deploying DNN models on resource-constrained edge devices faces more severe challenges, such as model storage limitations, dynamic service requests, and privacy risks. This paper proposes a novel framework for privacy-aware joint DNN model deployment and partition optimization to minimize long-term average inference delay under resource and privacy constraints. Specifically, the problem is formulated as a complex optimization problem considering model deployment, user-server association, and model partition strategies. To handle the NP-hardness and future uncertainties, a Lyapunov-based approach is introduced to transform the long-term optimization into a single-time-slot problem, ensuring system performance. Additionally, a coalition formation game model is proposed for edge server association, and a greedy-based algorithm is developed for model deployment within each coalition to efficiently solve the problem. Extensive simulations show that the proposed algorithms effectively reduce inference delay while satisfying privacy constraints, outperforming baseline approaches in various scenarios.</li>
</ul>

<h3>Title: Merger-as-a-Stealer: Stealing Targeted PII from Aligned LLMs with Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Lin Lu, Zhigang Zuo, Ziji Sheng, Pan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16094">https://arxiv.org/abs/2502.16094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16094">https://arxiv.org/pdf/2502.16094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16094]] Merger-as-a-Stealer: Stealing Targeted PII from Aligned LLMs with Model Merging(https://arxiv.org/abs/2502.16094)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, steal, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Model merging has emerged as a promising approach for updating large language models (LLMs) by integrating multiple domain-specific models into a cross-domain merged model. Despite its utility and plug-and-play nature, unmonitored mergers can introduce significant security vulnerabilities, such as backdoor attacks and model merging abuse. In this paper, we identify a novel and more realistic attack surface where a malicious merger can extract targeted personally identifiable information (PII) from an aligned model with model merging. Specifically, we propose \texttt{Merger-as-a-Stealer}, a two-stage framework to achieve this attack: First, the attacker fine-tunes a malicious model to force it to respond to any PII-related queries. The attacker then uploads this malicious model to the model merging conductor and obtains the merged model. Second, the attacker inputs direct PII-related queries to the merged model to extract targeted PII. Extensive experiments demonstrate that \texttt{Merger-as-a-Stealer} successfully executes attacks against various LLMs and model merging methods across diverse settings, highlighting the effectiveness of the proposed framework. Given that this attack enables character-level extraction for targeted PII without requiring any additional knowledge from the attacker, we stress the necessity for improved model alignment and more robust defense mechanisms to mitigate such threats.</li>
</ul>

<h3>Title: Good Representation, Better Explanation: Role of Convolutional Neural Networks in Transformer-Based Remote Sensing Image Captioning</h3>
<ul>
<li><strong>Authors: </strong>Swadhin Das, Saarthak Gupta, and Kamal Kumar, Raksha Sharma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16095">https://arxiv.org/abs/2502.16095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16095">https://arxiv.org/pdf/2502.16095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16095]] Good Representation, Better Explanation: Role of Convolutional Neural Networks in Transformer-Based Remote Sensing Image Captioning(https://arxiv.org/abs/2502.16095)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Remote Sensing Image Captioning (RSIC) is the process of generating meaningful descriptions from remote sensing images. Recently, it has gained significant attention, with encoder-decoder models serving as the backbone for generating meaningful captions. The encoder extracts essential visual features from the input image, transforming them into a compact representation, while the decoder utilizes this representation to generate coherent textual descriptions. Recently, transformer-based models have gained significant popularity due to their ability to capture long-range dependencies and contextual information. The decoder has been well explored for text generation, whereas the encoder remains relatively unexplored. However, optimizing the encoder is crucial as it directly influences the richness of extracted features, which in turn affects the quality of generated captions. To address this gap, we systematically evaluate twelve different convolutional neural network (CNN) architectures within a transformer-based encoder framework to assess their effectiveness in RSIC. The evaluation consists of two stages: first, a numerical analysis categorizes CNNs into different clusters, based on their performance. The best performing CNNs are then subjected to human evaluation from a human-centric perspective by a human annotator. Additionally, we analyze the impact of different search strategies, namely greedy search and beam search, to ensure the best caption. The results highlight the critical role of encoder selection in improving captioning performance, demonstrating that specific CNN architectures significantly enhance the quality of generated descriptions for remote sensing images. By providing a detailed comparison of multiple encoders, this study offers valuable insights to guide advances in transformer-based image captioning models.</li>
</ul>

<h3>Title: NeurFlow: Interpreting Neural Networks through Neuron Groups and Functional Interactions</h3>
<ul>
<li><strong>Authors: </strong>Tue M. Cao, Nhat X. Hoang, Hieu H. Pham, Phi Le Nguyen, My T. Thai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16105">https://arxiv.org/abs/2502.16105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16105">https://arxiv.org/pdf/2502.16105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16105]] NeurFlow: Interpreting Neural Networks through Neuron Groups and Functional Interactions(https://arxiv.org/abs/2502.16105)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Understanding the inner workings of neural networks is essential for enhancing model performance and interpretability. Current research predominantly focuses on examining the connection between individual neurons and the model's final predictions. Which suffers from challenges in interpreting the internal workings of the model, particularly when neurons encode multiple unrelated features. In this paper, we propose a novel framework that transitions the focus from analyzing individual neurons to investigating groups of neurons, shifting the emphasis from neuron-output relationships to functional interaction between neurons. Our automated framework, NeurFlow, first identifies core neurons and clusters them into groups based on shared functional relationships, enabling a more coherent and interpretable view of the network's internal processes. This approach facilitates the construction of a hierarchical circuit representing neuron interactions across layers, thus improving interpretability while reducing computational costs. Our extensive empirical studies validate the fidelity of our proposed NeurFlow. Additionally, we showcase its utility in practical applications such as image debugging and automatic concept labeling, thereby highlighting its potential to advance the field of neural network explainability.</li>
</ul>

<h3>Title: Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming</h3>
<ul>
<li><strong>Authors: </strong>Rui Li, Peiyi Wang, Jingyuan Ma, Di Zhang, Lei Sha, Zhifang Sui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16109">https://arxiv.org/abs/2502.16109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16109">https://arxiv.org/pdf/2502.16109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16109]] Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming(https://arxiv.org/abs/2502.16109)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have gained increasing attention for their remarkable capacity, alongside concerns about safety arising from their potential to produce harmful content. Red teaming aims to find prompts that could elicit harmful responses from LLMs, and is essential to discover and mitigate safety risks before real-world deployment. However, manual red teaming is both time-consuming and expensive, rendering it unscalable. In this paper, we propose RTPE, a scalable evolution framework to evolve red teaming prompts across both breadth and depth dimensions, facilitating the automatic generation of numerous high-quality and diverse red teaming prompts. Specifically, in-breadth evolving employs a novel enhanced in-context learning method to create a multitude of quality prompts, whereas in-depth evolving applies customized transformation operations to enhance both content and form of prompts, thereby increasing diversity. Extensive experiments demonstrate that RTPE surpasses existing representative automatic red teaming methods on both attack success rate and diversity. In addition, based on 4,800 red teaming prompts created by RTPE, we further provide a systematic analysis of 8 representative LLMs across 8 sensitive topics.</li>
</ul>

<h3>Title: FedOC: Optimizing Global Prototypes with Orthogonality Constraints for Enhancing Embeddings Separation in Heterogeneous Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Fucheng Guo, Zeyu Luan, Qing Li, Dan Zhao, Yong Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16119">https://arxiv.org/abs/2502.16119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16119">https://arxiv.org/pdf/2502.16119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16119]] FedOC: Optimizing Global Prototypes with Orthogonality Constraints for Enhancing Embeddings Separation in Heterogeneous Federated Learning(https://arxiv.org/abs/2502.16119)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has emerged as an essential framework for distributed machine learning, especially with its potential for privacy-preserving data processing. However, existing FL frameworks struggle to address statistical and model heterogeneity, which severely impacts model performance. While Heterogeneous Federated Learning (HtFL) introduces prototype-based strategies to address the challenges, current approaches face limitations in achieving optimal separation of prototypes. This paper presents FedOC, a novel HtFL algorithm designed to improve global prototype separation through orthogonality constraints, which not only increase intra-class prototype similarity but also significantly expand the inter-class angular separation. With the guidance of the global prototype, each client keeps its embeddings aligned with the corresponding prototype in the feature space, promoting directional independence that integrates seamlessly with the cross-entropy (CE) loss. We provide theoretical proof of FedOC's convergence under non-convex conditions. Extensive experiments demonstrate that FedOC outperforms seven state-of-the-art baselines, achieving up to a 10.12% accuracy improvement in both statistical and model heterogeneity settings.</li>
</ul>

<h3>Title: A New Era of Elections: Leveraging Blockchain for Fair and Transparent Voting</h3>
<ul>
<li><strong>Authors: </strong>Suniti Chouhan, Gajanand Sharma</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16127">https://arxiv.org/abs/2502.16127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16127">https://arxiv.org/pdf/2502.16127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16127]] A New Era of Elections: Leveraging Blockchain for Fair and Transparent Voting(https://arxiv.org/abs/2502.16127)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust, biometric, fair</a></li>
<li><strong>Abstract: </strong>This study presents a blockchain-based voting system aimed at enhancing election security, transparency, and integrity. Traditional voting methods face growing risks of tampering, making it crucial to explore innovative solutions. Our proposed system combines blockchain's immutable, decentralized ledger with advanced voter identity verification techniques, including digital identity validation through Aadhaar and Driving Licenses (secured via BLAKE2b-512 hashing), biometric fingerprint authentication, and a picture rotation pattern for added security. Votes are recorded transparently and securely on a blockchain, with a consensus mechanism ensuring data integrity and reducing the risk of unauthorized alterations. Security analysis indicates that this multi-layered approach significantly reduces impersonation risks, while blockchain ensures accurate, private, and tamper-resistant vote recording. The findings support that a blockchain-based voting system with robust identity checks offers a trustworthy alternative to traditional methods, with potential for even greater refinement in secure and transparent elections.</li>
</ul>

<h3>Title: Robust Dynamic Facial Expression Recognition</h3>
<ul>
<li><strong>Authors: </strong>Feng Liu, Hanyang Wang, Siyuan Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16129">https://arxiv.org/abs/2502.16129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16129">https://arxiv.org/pdf/2502.16129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16129]] Robust Dynamic Facial Expression Recognition(https://arxiv.org/abs/2502.16129)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The study of Dynamic Facial Expression Recognition (DFER) is a nascent field of research that involves the automated recognition of facial expressions in video data. Although existing research has primarily focused on learning representations under noisy and hard samples, the issue of the coexistence of both types of samples remains unresolved. In order to overcome this challenge, this paper proposes a robust method of distinguishing between hard and noisy samples. This is achieved by evaluating the prediction agreement of the model on different sampled clips of the video. Subsequently, methodologies that reinforce the learning of hard samples and mitigate the impact of noisy samples can be employed. Moreover, to identify the principal expression in a video and enhance the model's capacity for representation learning, comprising a key expression re-sampling framework and a dual-stream hierarchical network is proposed, namely Robust Dynamic Facial Expression Recognition (RDFER). The key expression re-sampling framework is designed to identify the key expression, thereby mitigating the potential confusion caused by non-target expressions. RDFER employs two sequence models with the objective of disentangling short-term facial movements and long-term emotional changes. The proposed method has been shown to outperform current State-Of-The-Art approaches in DFER through extensive experimentation on benchmark datasets such as DFEW and FERV39K. A comprehensive analysis provides valuable insights and observations regarding the proposed agreement. This work has significant implications for the field of dynamic facial expression recognition and promotes the further development of the field of noise-consistent robust learning in dynamic facial expression recognition. The code is available from [this https URL].</li>
</ul>

<h3>Title: Chain-of-Description: What I can understand, I can put into words</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Guo, Daimeng Wei, Zongyao Li, Hengchao Shang, Yuanchang Luo, Hao Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16137">https://arxiv.org/abs/2502.16137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16137">https://arxiv.org/pdf/2502.16137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16137]] Chain-of-Description: What I can understand, I can put into words(https://arxiv.org/abs/2502.16137)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a novel strategy defined as Chain-of-Description (CoD) Prompting, tailored for Multi-Modal Large Language Models. This approach involves having the model first provide a detailed description of the multi-modal input before generating an answer to the question. When applied to models such as Qwen2-Audio, Qwen2-VL, and Qwen2.5-VL, CoD Prompting significantly enhances performance compared to standard prompting methods. This is demonstrated by nearly a 4\% improvement in the speech category of the audio benchmark AIR-Bench-Chat and a 5.3\% improvement in the hard-level portion of the vision benchmark MMMU\_Pro. Our ablation study further validates the effectiveness of CoD Prompting.</li>
</ul>

<h3>Title: An Improved Deep Learning Model for Word Embeddings Based Clustering for Large Text Datasets</h3>
<ul>
<li><strong>Authors: </strong>Vijay Kumar Sutrakar, Nikhil Mogre</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16139">https://arxiv.org/abs/2502.16139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16139">https://arxiv.org/pdf/2502.16139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16139]] An Improved Deep Learning Model for Word Embeddings Based Clustering for Large Text Datasets(https://arxiv.org/abs/2502.16139)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, an improved clustering technique for large textual datasets by leveraging fine-tuned word embeddings is presented. WEClustering technique is used as the base model. WEClustering model is fur-ther improvements incorporating fine-tuning contextual embeddings, advanced dimensionality reduction methods, and optimization of clustering algorithms. Experimental results on benchmark datasets demon-strate significant improvements in clustering metrics such as silhouette score, purity, and adjusted rand index (ARI). An increase of 45% and 67% of median silhouette score is reported for the proposed WE-Clustering_K++ (based on K-means) and WEClustering_A++ (based on Agglomerative models), respec-tively. The proposed technique will help to bridge the gap between semantic understanding and statistical robustness for large-scale text-mining tasks.</li>
</ul>

<h3>Title: Understanding Zero-shot Rare Word Recognition Improvements Through LLM Integration</h3>
<ul>
<li><strong>Authors: </strong>Haoxuan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16142">https://arxiv.org/abs/2502.16142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16142">https://arxiv.org/pdf/2502.16142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16142]] Understanding Zero-shot Rare Word Recognition Improvements Through LLM Integration(https://arxiv.org/abs/2502.16142)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this study, we investigate the integration of a large language model (LLM) with an automatic speech recognition (ASR) system, specifically focusing on enhancing rare word recognition performance. Using a 190,000-hour dataset primarily sourced from YouTube, pre-processed with Whisper V3 pseudo-labeling, we demonstrate that the LLM-ASR architecture outperforms traditional Zipformer-Transducer models in the zero-shot rare word recognition task, after training on a large dataset. Our analysis reveals that the LLM contributes significantly to improvements in rare word error rate (R-WER), while the speech encoder primarily determines overall transcription performance (Orthographic Word Error Rate, O-WER, and Normalized Word Error Rate, N-WER). Through extensive ablation studies, we highlight the importance of adapter integration in aligning speech encoder outputs with the LLM's linguistic capabilities. Furthermore, we emphasize the critical role of high-quality labeled data in achieving optimal performance. These findings provide valuable insights into the synergy between LLM-based ASR architectures, paving the way for future advancements in large-scale LLM-based speech recognition systems.</li>
</ul>

<h3>Title: The Law of Knowledge Overshadowing: Towards Understanding, Predicting, and Preventing LLM Hallucination</h3>
<ul>
<li><strong>Authors: </strong>Yuji Zhang, Sha Li, Cheng Qian, Jiateng Liu, Pengfei Yu, Chi Han, Yi R. Fung, Kathleen McKeown, Chengxiang Zhai, Manling Li, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16143">https://arxiv.org/abs/2502.16143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16143">https://arxiv.org/pdf/2502.16143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16143]] The Law of Knowledge Overshadowing: Towards Understanding, Predicting, and Preventing LLM Hallucination(https://arxiv.org/abs/2502.16143)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hallucination is a persistent challenge in large language models (LLMs), where even with rigorous quality control, models often generate distorted facts. This paradox, in which error generation continues despite high-quality training data, calls for a deeper understanding of the underlying LLM mechanisms. To address it, we propose a novel concept: knowledge overshadowing, where model's dominant knowledge can obscure less prominent knowledge during text generation, causing the model to fabricate inaccurate details. Building on this idea, we introduce a novel framework to quantify factual hallucinations by modeling knowledge overshadowing. Central to our approach is the log-linear law, which predicts that the rate of factual hallucination increases linearly with the logarithmic scale of (1) Knowledge Popularity, (2) Knowledge Length, and (3) Model Size. The law provides a means to preemptively quantify hallucinations, offering foresight into their occurrence even before model training or inference. Built on overshadowing effect, we propose a new decoding strategy CoDa, to mitigate hallucinations, which notably enhance model factuality on Overshadow (27.9%), MemoTrap (13.1%) and NQ-Swap (18.3%). Our findings not only deepen understandings of the underlying mechanisms behind hallucinations but also provide actionable insights for developing more predictable and controllable language models.</li>
</ul>

<h3>Title: Number Representations in LLMs: A Computational Parallel to Human Perception</h3>
<ul>
<li><strong>Authors: </strong>H.V. AlquBoj, Hilal AlQuabeh, Velibor Bojkovic, Tatsuya Hiraoka, Ahmed Oumar El-Shangiti, Munachiso Nwadike, Kentaro Inui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16147">https://arxiv.org/abs/2502.16147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16147">https://arxiv.org/pdf/2502.16147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16147]] Number Representations in LLMs: A Computational Parallel to Human Perception(https://arxiv.org/abs/2502.16147)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Humans are believed to perceive numbers on a logarithmic mental number line, where smaller values are represented with greater resolution than larger ones. This cognitive bias, supported by neuroscience and behavioral studies, suggests that numerical magnitudes are processed in a sublinear fashion rather than on a uniform linear scale. Inspired by this hypothesis, we investigate whether large language models (LLMs) exhibit a similar logarithmic-like structure in their internal numerical representations. By analyzing how numerical values are encoded across different layers of LLMs, we apply dimensionality reduction techniques such as PCA and PLS followed by geometric regression to uncover latent structures in the learned embeddings. Our findings reveal that the model's numerical representations exhibit sublinear spacing, with distances between values aligning with a logarithmic scale. This suggests that LLMs, much like humans, may encode numbers in a compressed, non-uniform manner.</li>
</ul>

<h3>Title: DUPRE: Data Utility Prediction for Efficient Data Valuation</h3>
<ul>
<li><strong>Authors: </strong>Kieu Thao Nguyen Pham, Rachael Hwee Ling Sim, Quoc Phong Nguyen, See Kiong Ng, Bryan Kian Hsiang Low</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16152">https://arxiv.org/abs/2502.16152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16152">https://arxiv.org/pdf/2502.16152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16152]] DUPRE: Data Utility Prediction for Efficient Data Valuation(https://arxiv.org/abs/2502.16152)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Data valuation is increasingly used in machine learning (ML) to decide the fair compensation for data owners and identify valuable or harmful data for improving ML models. Cooperative game theory-based data valuation, such as Data Shapley, requires evaluating the data utility (e.g., validation accuracy) and retraining the ML model for multiple data subsets. While most existing works on efficient estimation of the Shapley values have focused on reducing the number of subsets to evaluate, our framework, \texttt{DUPRE}, takes an alternative yet complementary approach that reduces the cost per subset evaluation by predicting data utilities instead of evaluating them by model retraining. Specifically, given the evaluated data utilities of some data subsets, \texttt{DUPRE} fits a \emph{Gaussian process} (GP) regression model to predict the utility of every other data subset. Our key contribution lies in the design of our GP kernel based on the sliced Wasserstein distance between empirical data distributions. In particular, we show that the kernel is valid and positive semi-definite, encodes prior knowledge of similarities between different data subsets, and can be efficiently computed. We empirically verify that \texttt{DUPRE} introduces low prediction error and speeds up data valuation for various ML models, datasets, and utility functions.</li>
</ul>

<h3>Title: USegMix: Unsupervised Segment Mix for Efficient Data Augmentation in Pathology Images</h3>
<ul>
<li><strong>Authors: </strong>Jiamu Wang, Jin Tae Kwak</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16160">https://arxiv.org/abs/2502.16160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16160">https://arxiv.org/pdf/2502.16160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16160]] USegMix: Unsupervised Segment Mix for Efficient Data Augmentation in Pathology Images(https://arxiv.org/abs/2502.16160)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In computational pathology, researchers often face challenges due to the scarcity of labeled pathology datasets. Data augmentation emerges as a crucial technique to mitigate this limitation. In this study, we introduce an efficient data augmentation method for pathology images, called USegMix. Given a set of pathology images, the proposed method generates a new, synthetic image in two phases. In the first phase, USegMix constructs a pool of tissue segments in an automated and unsupervised manner using superpixels and the Segment Anything Model (SAM). In the second phase, USegMix selects a candidate segment in a target image, replaces it with a similar segment from the segment pool, and blends them by using a pre-trained diffusion model. In this way, USegMix can generate diverse and realistic pathology images. We rigorously evaluate the effectiveness of USegMix on two pathology image datasets of colorectal and prostate cancers. The results demonstrate improvements in cancer classification performance, underscoring the substantial potential of USegMix for pathology image analysis.</li>
</ul>

<h3>Title: OmniParser V2: Structured-Points-of-Thought for Unified Visual Text Parsing and Its Generality to Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenwen Yu, Zhibo Yang, Jianqiang Wan, Sibo Song, Jun Tang, Wenqing Cheng, Yuliang Liu, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16161">https://arxiv.org/abs/2502.16161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16161">https://arxiv.org/pdf/2502.16161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16161]] OmniParser V2: Structured-Points-of-Thought for Unified Visual Text Parsing and Its Generality to Multimodal Large Language Models(https://arxiv.org/abs/2502.16161)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Visually-situated text parsing (VsTP) has recently seen notable advancements, driven by the growing demand for automated document understanding and the emergence of large language models capable of processing document-based questions. While various methods have been proposed to tackle the complexities of VsTP, existing solutions often rely on task-specific architectures and objectives for individual tasks. This leads to modal isolation and complex workflows due to the diversified targets and heterogeneous schemas. In this paper, we introduce OmniParser V2, a universal model that unifies VsTP typical tasks, including text spotting, key information extraction, table recognition, and layout analysis, into a unified framework. Central to our approach is the proposed Structured-Points-of-Thought (SPOT) prompting schemas, which improves model performance across diverse scenarios by leveraging a unified encoder-decoder architecture, objective, and input\&output representation. SPOT eliminates the need for task-specific architectures and loss functions, significantly simplifying the processing pipeline. Our extensive evaluations across four tasks on eight different datasets show that OmniParser V2 achieves state-of-the-art or competitive results in VsTP. Additionally, we explore the integration of SPOT within a multimodal large language model structure, further enhancing text localization and recognition capabilities, thereby confirming the generality of SPOT prompting technique. The code is available at \href{this https URL}{AdvancedLiterateMachinery}.</li>
</ul>

<h3>Title: PersGuard: Preventing Malicious Personalization via Backdoor Attacks on Pre-trained Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xinwei Liu, Xiaojun Jia, Yuan Xun, Hua Zhang, Xiaochun Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16167">https://arxiv.org/abs/2502.16167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16167">https://arxiv.org/pdf/2502.16167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16167]] PersGuard: Preventing Malicious Personalization via Backdoor Attacks on Pre-trained Text-to-Image Diffusion Models(https://arxiv.org/abs/2502.16167)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) have revolutionized data generation, particularly in text-to-image (T2I) synthesis. However, the widespread use of personalized generative models raises significant concerns regarding privacy violations and copyright infringement. To address these issues, researchers have proposed adversarial perturbation-based protection techniques. However, these methods have notable limitations, including insufficient robustness against data transformations and the inability to fully eliminate identifiable features of protected objects in the generated output. In this paper, we introduce PersGuard, a novel backdoor-based approach that prevents malicious personalization of specific images. Unlike traditional adversarial perturbation methods, PersGuard implant backdoor triggers into pre-trained T2I models, preventing the generation of customized outputs for designated protected images while allowing normal personalization for unprotected ones. Unfortunately, existing backdoor methods for T2I diffusion models fail to be applied to personalization scenarios due to the different backdoor objectives and the potential backdoor elimination during downstream fine-tuning processes. To address these, we propose three novel backdoor objectives specifically designed for personalization scenarios, coupled with backdoor retention loss engineered to resist downstream fine-tuning. These components are integrated into a unified optimization framework. Extensive experimental evaluations demonstrate PersGuard's effectiveness in preserving data privacy, even under challenging conditions including gray-box settings, multi-object protection, and facial identity scenarios. Our method significantly outperforms existing techniques, offering a more robust solution for privacy and copyright protection.</li>
</ul>

<h3>Title: EPERM: An Evidence Path Enhanced Reasoning Model for Knowledge Graph Question and Answering</h3>
<ul>
<li><strong>Authors: </strong>Xiao Long, Liansheng Zhuang, Aodi Li, Minghong Yao, Shafei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16171">https://arxiv.org/abs/2502.16171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16171">https://arxiv.org/pdf/2502.16171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16171]] EPERM: An Evidence Path Enhanced Reasoning Model for Knowledge Graph Question and Answering(https://arxiv.org/abs/2502.16171)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Due to the remarkable reasoning ability, Large language models (LLMs) have demonstrated impressive performance in knowledge graph question answering (KGQA) tasks, which find answers to natural language questions over knowledge graphs (KGs). To alleviate the hallucinations and lack of knowledge issues of LLMs, existing methods often retrieve the question-related information from KGs to enrich the input context. However, most methods focus on retrieving the relevant information while ignoring the importance of different types of knowledge in reasoning, which degrades their performance. To this end, this paper reformulates the KGQA problem as a graphical model and proposes a three-stage framework named the Evidence Path Enhanced Reasoning Model (EPERM) for KGQA. In the first stage, EPERM uses the fine-tuned LLM to retrieve a subgraph related to the question from the original knowledge graph. In the second stage, EPERM filters out the evidence paths that faithfully support the reasoning of the questions, and score their importance in reasoning. Finally, EPERM uses the weighted evidence paths to reason the final answer. Since considering the importance of different structural information in KGs for reasoning, EPERM can improve the reasoning ability of LLMs in KGQA tasks. Extensive experiments on benchmark datasets demonstrate that EPERM achieves superior performances in KGQA tasks.</li>
</ul>

<h3>Title: Maybe I Should Not Answer That, but... Do LLMs Understand The Safety of Their Inputs?</h3>
<ul>
<li><strong>Authors: </strong>Maciej Chrabąszcz, Filip Szatkowski, Bartosz Wójcik, Jan Dubiński, Tomasz Trzciński</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16174">https://arxiv.org/abs/2502.16174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16174">https://arxiv.org/pdf/2502.16174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16174]] Maybe I Should Not Answer That, but... Do LLMs Understand The Safety of Their Inputs?(https://arxiv.org/abs/2502.16174)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Ensuring the safety of the Large Language Model (LLM) is critical, but currently used methods in most cases sacrifice the model performance to obtain increased safety or perform poorly on data outside of their adaptation distribution. We investigate existing methods for such generalization and find them insufficient. Surprisingly, while even plain LLMs recognize unsafe prompts, they may still generate unsafe responses. To avoid performance degradation and preserve safe performance, we advocate for a two-step framework, where we first identify unsafe prompts via a lightweight classifier, and apply a "safe" model only to such prompts. In particular, we explore the design of the safety detector in more detail, investigating the use of different classifier architectures and prompting techniques. Interestingly, we find that the final hidden state for the last token is enough to provide robust performance, minimizing false positives on benign data while performing well on malicious prompt detection. Additionally, we show that classifiers trained on the representations from different model layers perform comparably on the latest model layers, indicating that safety representation is present in the LLMs' hidden states at most model stages. Our work is a step towards efficient, representation-based safety mechanisms for LLMs.</li>
</ul>

<h3>Title: Mojito: LLM-Aided Motion Instructor with Jitter-Reduced Inertial Tokens</h3>
<ul>
<li><strong>Authors: </strong>Ziwei Shan, Yaoyu He, Chengfeng Zhao, Jiashen Du, Jingyan Zhang, Qixuan Zhang, Jingyi Yu, Lan Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16175">https://arxiv.org/abs/2502.16175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16175">https://arxiv.org/pdf/2502.16175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16175]] Mojito: LLM-Aided Motion Instructor with Jitter-Reduced Inertial Tokens(https://arxiv.org/abs/2502.16175)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Human bodily movements convey critical insights into action intentions and cognitive processes, yet existing multimodal systems primarily focused on understanding human motion via language, vision, and audio, which struggle to capture the dynamic forces and torques inherent in 3D motion. Inertial measurement units (IMUs) present a promising alternative, offering lightweight, wearable, and privacy-conscious motion sensing. However, processing of streaming IMU data faces challenges such as wireless transmission instability, sensor noise, and drift, limiting their utility for long-term real-time motion capture (MoCap), and more importantly, online motion analysis. To address these challenges, we introduce Mojito, an intelligent motion agent that integrates inertial sensing with large language models (LLMs) for interactive motion capture and behavioral analysis.</li>
</ul>

<h3>Title: An End-to-End Homomorphically Encrypted Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Marcos Florencio, Luiz Alencar, Bianca Lima</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16176">https://arxiv.org/abs/2502.16176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16176">https://arxiv.org/pdf/2502.16176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16176]] An End-to-End Homomorphically Encrypted Neural Network(https://arxiv.org/abs/2502.16176)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Every commercially available, state-of-the-art neural network consume plain input data, which is a well-known privacy concern. We propose a new architecture based on homomorphic encryption, which allows the neural network to operate on encrypted data. We show that Homomorphic Neural Networks (HNN) can achieve full privacy and security while maintaining levels of accuracy comparable to plain neural networks. We also introduce a new layer, the Differentiable Soft-Argmax, which allows the calibration of output logits in the encrypted domain, raising the entropy of the activation parameters, thus improving the security of the model, while keeping the overall noise below the acceptable noise budget. Experiments were conducted using the Stanford Sentiment Treebank (SST-2) corpora on the DistilBERT base uncased finetuned SST-2 English sentiment analysis model, and the results show that the HNN model can achieve up to 82.5% of the accuracy of the plain model while maintaining full privacy and security.</li>
</ul>

<h3>Title: A Review of Several Keystroke Dynamics Methods</h3>
<ul>
<li><strong>Authors: </strong>Soykat Amin, Cristian Di Iorio</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16177">https://arxiv.org/abs/2502.16177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16177">https://arxiv.org/pdf/2502.16177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16177]] A Review of Several Keystroke Dynamics Methods(https://arxiv.org/abs/2502.16177)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, biometric</a></li>
<li><strong>Abstract: </strong>Keystroke dynamics is a behavioral biometric that captures an individual's typing patterns for authentication and security applications. This paper presents a comparative analysis of keystroke authentication models using Gaussian Mixture Models (GMM), Mahalanobis Distance-based Classification, and Gunetti Picardi's Distance Metrics. These models leverage keystroke timing features such as hold time (H), up-down time (UD), and down-down time (DD) extracted from datasets including Aalto, Buffalo and Nanglae-Bhattarakosol. Each model is trained and validated using structured methodologies, with performance evaluated through False Acceptance Rate (FAR), False Rejection Rate (FRR), and Equal Error Rate (EER). The results, visualized through Receiver Operating Characteristic (ROC) curves, highlight the relative strengths and weaknesses of each approach in distinguishing genuine users from impostors.</li>
</ul>

<h3>Title: IPO: Your Language Model is Secretly a Preference Classifier</h3>
<ul>
<li><strong>Authors: </strong>Shivank Garg, Ayush Singh, Shweta Singh, Paras Chopra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16182">https://arxiv.org/abs/2502.16182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16182">https://arxiv.org/pdf/2502.16182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16182]] IPO: Your Language Model is Secretly a Preference Classifier(https://arxiv.org/abs/2502.16182)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. While it enables LLMs to achieve human-level alignment, it often incurs significant computational and financial costs due to its reliance on training external reward models or human-labeled preferences. In this work, we propose \textbf{Implicit Preference Optimization (IPO)}, an alternative approach that leverages generative LLMs as preference classifiers, thereby reducing the dependence on external human feedback or reward models to obtain preferences. We conduct a comprehensive evaluation on the preference classification ability of LLMs using RewardBench, assessing models across different sizes, architectures, and training levels to validate our hypothesis. Furthermore, we investigate the self-improvement capabilities of LLMs by generating multiple responses for a given instruction and employing the model itself as a preference classifier for Direct Preference Optimization (DPO)-based training. Our findings demonstrate that models trained through IPO achieve performance comparable to those utilizing state-of-the-art reward models for obtaining preferences.</li>
</ul>

<h3>Title: Graph Self-Supervised Learning with Learnable Structural and Positional Encodings</h3>
<ul>
<li><strong>Authors: </strong>Asiri Wijesinghe, Hao Zhu, Piotr Koniusz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16233">https://arxiv.org/abs/2502.16233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16233">https://arxiv.org/pdf/2502.16233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16233]] Graph Self-Supervised Learning with Learnable Structural and Positional Encodings(https://arxiv.org/abs/2502.16233)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Traditional Graph Self-Supervised Learning (GSSL) struggles to capture complex structural properties well. This limitation stems from two main factors: (1) the inadequacy of conventional Graph Neural Networks (GNNs) in representing sophisticated topological features, and (2) the focus of self-supervised learning solely on final graph representations. To address these issues, we introduce \emph{GenHopNet}, a GNN framework that integrates a $k$-hop message-passing scheme, enhancing its ability to capture local structural information without explicit substructure extraction. We theoretically demonstrate that \emph{GenHopNet} surpasses the expressiveness of the classical Weisfeiler-Lehman (WL) test for graph isomorphism. Furthermore, we propose a structural- and positional-aware GSSL framework that incorporates topological information throughout the learning process. This approach enables the learning of representations that are both sensitive to graph topology and invariant to specific structural and feature augmentations. Comprehensive experiments on graph classification datasets, including those designed to test structural sensitivity, show that our method consistently outperforms the existing approaches and maintains computational efficiency. Our work significantly advances GSSL's capability in distinguishing graphs with similar local structures but different global topologies.</li>
</ul>

<h3>Title: Linear Attention for Efficient Bidirectional Sequence Modeling</h3>
<ul>
<li><strong>Authors: </strong>Arshia Afzal, Elias Abad Rocamora, Leyla Naz Candogan, Pol Puigdemont, Francesco Tonin, Yongtao Wu, Mahsa Shoaran, Volkan Cevher</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16249">https://arxiv.org/abs/2502.16249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16249">https://arxiv.org/pdf/2502.16249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16249]] Linear Attention for Efficient Bidirectional Sequence Modeling(https://arxiv.org/abs/2502.16249)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers with linear attention enable fast and parallel training. Moreover, they can be formulated as Recurrent Neural Networks (RNNs), for efficient linear-time inference. While extensively evaluated in causal sequence modeling, they have yet to be extended to the bidirectional setting. This work introduces the LION framework, establishing new theoretical foundations for linear transformers in bidirectional sequence modeling. LION constructs a bidirectional RNN equivalent to full Linear Attention. This extends the benefits of linear transformers: parallel training, and efficient inference, into the bidirectional setting. Using LION, we cast three linear transformers to their bidirectional form: LION-LIT, the bidirectional variant corresponding to (Katharopoulos et al., 2020); LION-D, extending RetNet (Sun et al., 2023); and LION-S, a linear transformer with a stable selective mask inspired by selectivity of SSMs (Dao & Gu, 2024). Replacing the attention block with LION (-LIT, -D, -S) achieves performance on bidirectional tasks that approaches that of Transformers and State-Space Models (SSMs), while delivering significant improvements in training speed. Our implementation is available in this http URL.</li>
</ul>

<h3>Title: PLS-based approach for fair representation learning</h3>
<ul>
<li><strong>Authors: </strong>Elena M. De-Diego, Adrián Perez-Suay, Paula Gordaliza, Jean-Michel Loubes</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16263">https://arxiv.org/abs/2502.16263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16263">https://arxiv.org/pdf/2502.16263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16263]] PLS-based approach for fair representation learning(https://arxiv.org/abs/2502.16263)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We revisit the problem of fair representation learning by proposing Fair Partial Least Squares (PLS) components. PLS is widely used in statistics to efficiently reduce the dimension of the data by providing representation tailored for the prediction. We propose a novel method to incorporate fairness constraints in the construction of PLS components. This new algorithm provides a feasible way to construct such features both in the linear and the non linear case using kernel embeddings. The efficiency of our method is evaluated on different datasets, and we prove its superiority with respect to standard fair PCA method.</li>
</ul>

<h3>Title: ThinkBench: Dynamic Out-of-Distribution Evaluation for Robust LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Shulin Huang, Linyi Yang, Yan Song, Shuang Chen, Leyang Cui, Ziyu Wan, Qingcheng Zeng, Ying Wen, Kun Shao, Weinan Zhang, Jun Wang, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16268">https://arxiv.org/abs/2502.16268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16268">https://arxiv.org/pdf/2502.16268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16268]] ThinkBench: Dynamic Out-of-Distribution Evaluation for Robust LLM Reasoning(https://arxiv.org/abs/2502.16268)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Evaluating large language models (LLMs) poses significant challenges, particularly due to issues of data contamination and the leakage of correct answers. To address these challenges, we introduce ThinkBench, a novel evaluation framework designed to evaluate LLMs' reasoning capability robustly. ThinkBench proposes a dynamic data generation method for constructing out-of-distribution (OOD) datasets and offers an OOD dataset that contains 2,912 samples drawn from reasoning tasks. ThinkBench unifies the evaluation of reasoning models and non-reasoning models. We evaluate 16 LLMs and 4 PRMs under identical experimental conditions and show that most of the LLMs' performance are far from robust and they face a certain level of data leakage. By dynamically generating OOD datasets, ThinkBench effectively provides a reliable evaluation of LLMs and reduces the impact of data contamination.</li>
</ul>

<h3>Title: Partial and Fully Homomorphic Matching of IP Addresses Against Blacklists for Threat Analysis</h3>
<ul>
<li><strong>Authors: </strong>William J Buchanan, Hisham Ali</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16272">https://arxiv.org/abs/2502.16272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16272">https://arxiv.org/pdf/2502.16272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16272]] Partial and Fully Homomorphic Matching of IP Addresses Against Blacklists for Threat Analysis(https://arxiv.org/abs/2502.16272)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>In many areas of cybersecurity, we require access to Personally Identifiable Information (PII), such as names, postal addresses and email addresses. Unfortunately, this can lead to data breaches, especially in relation to data compliance regulations such as GDPR. An IP address is a typical identifier which is used to map a network address to a person. Thus, in applications which are privacy-aware, we may aim to hide the IP address while aiming to determine if the address comes from a blacklist. One solution to this is to use homomorphic encryption to match an encrypted version of an IP address to a blacklisted network list. This matching allows us to encrypt the IP address and match it to an encrypted version of a blacklist. In this paper, we use the OpenFHE library \cite{OpenFHE} to convert network addresses into the BFV homomorphic encryption method. In order to assess the performance impact of BFV, it implements a matching method using the OpenFHE library and compares this against the partial homomorphic methods of Paillier, Damgard-Jurik, Okamoto-Uchiyama, Naccache-Stern and Benaloh. The main findings are that the BFV method compares favourably against the partial homomorphic methods in most cases.</li>
</ul>

<h3>Title: Human Preferences in Large Language Model Latent Space: A Technical Analysis on the Reliability of Synthetic Data in Voting Outcome Prediction</h3>
<ul>
<li><strong>Authors: </strong>Sarah Ball, Simeon Allmendinger, Frauke Kreuter, Niklas Kühl</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16280">https://arxiv.org/abs/2502.16280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16280">https://arxiv.org/pdf/2502.16280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16280]] Human Preferences in Large Language Model Latent Space: A Technical Analysis on the Reliability of Synthetic Data in Voting Outcome Prediction(https://arxiv.org/abs/2502.16280)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative AI (GenAI) is increasingly used in survey contexts to simulate human preferences. While many research endeavors evaluate the quality of synthetic GenAI data by comparing model-generated responses to gold-standard survey results, fundamental questions about the validity and reliability of using LLMs as substitutes for human respondents remain. Our study provides a technical analysis of how demographic attributes and prompt variations influence latent opinion mappings in large language models (LLMs) and evaluates their suitability for survey-based predictions. Using 14 different models, we find that LLM-generated data fails to replicate the variance observed in real-world human responses, particularly across demographic subgroups. In the political space, persona-to-party mappings exhibit limited differentiation, resulting in synthetic data that lacks the nuanced distribution of opinions found in survey data. Moreover, we show that prompt sensitivity can significantly alter outputs for some models, further undermining the stability and predictiveness of LLM-based simulations. As a key contribution, we adapt a probe-based methodology that reveals how LLMs encode political affiliations in their latent space, exposing the systematic distortions introduced by these models. Our findings highlight critical limitations in AI-generated survey data, urging caution in its use for public opinion research, social science experimentation, and computational behavioral modeling.</li>
</ul>

<h3>Title: FHGE: A Fast Heterogeneous Graph Embedding with Ad-hoc Meta-paths</h3>
<ul>
<li><strong>Authors: </strong>Xuqi Mao, Zhenying He, X. Sean Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16281">https://arxiv.org/abs/2502.16281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16281">https://arxiv.org/pdf/2502.16281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16281]] FHGE: A Fast Heterogeneous Graph Embedding with Ad-hoc Meta-paths(https://arxiv.org/abs/2502.16281)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNNs) have emerged as the state of the art for a variety of graph-related tasks and have been widely used in Heterogeneous Graphs (HetGs), where meta-paths help encode specific semantics between various node types. Despite the revolutionary representation capabilities of existing heterogeneous GNNs (HGNNs) due to their focus on improving the effectiveness of heterogeneity capturing, the huge training costs hinder their practical deployment in real-world scenarios that frequently require handling ad-hoc queries with user-defined meta-paths. To address this, we propose FHGE, a Fast Heterogeneous Graph Embedding designed for efficient, retraining-free generation of meta-path-guided graph embeddings. The key design of the proposed framework is two-fold: segmentation and reconstruction modules. It employs Meta-Path Units (MPUs) to segment the graph into local and global components, enabling swift integration of node embeddings from relevant MPUs during reconstruction and allowing quick adaptation to specific meta-paths. In addition, a dual attention mechanism is applied to enhance semantics capturing. Extensive experiments across diverse datasets demonstrate the effectiveness and efficiency of FHGE in generating meta-path-guided graph embeddings and downstream tasks, such as link prediction and node classification, highlighting its significant advantages for real-time graph analysis in ad-hoc queries.</li>
</ul>

<h3>Title: Verification of Bit-Flip Attacks against Quantized Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Yedi Zhang, Lei Huang, Pengfei Gao, Fu Song, Jun Sun, Jin Song Dong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16286">https://arxiv.org/abs/2502.16286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16286">https://arxiv.org/pdf/2502.16286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16286]] Verification of Bit-Flip Attacks against Quantized Neural Networks(https://arxiv.org/abs/2502.16286)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving landscape of neural network security, the resilience of neural networks against bit-flip attacks (i.e., an attacker maliciously flips an extremely small amount of bits within its parameter storage memory system to induce harmful behavior), has emerged as a relevant area of research. Existing studies suggest that quantization may serve as a viable defense against such attacks. Recognizing the documented susceptibility of real-valued neural networks to such attacks and the comparative robustness of quantized neural networks (QNNs), in this work, we introduce BFAVerifier, the first verification framework designed to formally verify the absence of bit-flip attacks or to identify all vulnerable parameters in a sound and rigorous manner. BFAVerifier comprises two integral components: an abstraction-based method and an MILP-based method. Specifically, we first conduct a reachability analysis with respect to symbolic parameters that represent the potential bit-flip attacks, based on a novel abstract domain with a sound guarantee. If the reachability analysis fails to prove the resilience of such attacks, then we encode this verification problem into an equivalent MILP problem which can be solved by off-the-shelf solvers. Therefore, BFAVerifier is sound, complete, and reasonably efficient. We conduct extensive experiments, which demonstrate its effectiveness and efficiency across various network architectures, quantization bit-widths, and adversary capabilities.</li>
</ul>

<h3>Title: MOB-GCN: A Novel Multiscale Object-Based Graph Neural Network for Hyperspectral Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Tuan-Anh Yang, Truong-Son Hy, Phuong D. Dao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16289">https://arxiv.org/abs/2502.16289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16289">https://arxiv.org/pdf/2502.16289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16289]] MOB-GCN: A Novel Multiscale Object-Based Graph Neural Network for Hyperspectral Image Classification(https://arxiv.org/abs/2502.16289)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel multiscale object-based graph neural network called MOB-GCN for hyperspectral image (HSI) classification. The central aim of this study is to enhance feature extraction and classification performance by utilizing multiscale object-based image analysis (OBIA). Traditional pixel-based methods often suffer from low accuracy and speckle noise, while single-scale OBIA approaches may overlook crucial information of image objects at different levels of detail. MOB-GCN overcomes these challenges by extracting and integrating features from multiple segmentation scales, leveraging the Multiresolution Graph Network (MGN) architecture to capture both fine-grained and global spatial patterns. MOB-GCN addresses this issue by extracting and integrating features from multiple segmentation scales to improve classification results using the Multiresolution Graph Network (MGN) architecture that can model fine-grained and global spatial patterns. By constructing a dynamic multiscale graph hierarchy, MOB-GCN offers a more comprehensive understanding of the intricate details and global context of HSIs. Experimental results demonstrate that MOB-GCN consistently outperforms single-scale graph convolutional networks (GCNs) in terms of classification accuracy, computational efficiency, and noise reduction, particularly when labeled data is limited. The implementation of MOB-GCN is publicly available at this https URL</li>
</ul>

<h3>Title: TimePFN: Effective Multivariate Time Series Forecasting with Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Ege Onur Taga, M. Emrullah Ildiz, Samet Oymak</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16294">https://arxiv.org/abs/2502.16294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16294">https://arxiv.org/pdf/2502.16294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16294]] TimePFN: Effective Multivariate Time Series Forecasting with Synthetic Data(https://arxiv.org/abs/2502.16294)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The diversity of time series applications and scarcity of domain-specific data highlight the need for time-series models with strong few-shot learning capabilities. In this work, we propose a novel training scheme and a transformer-based architecture, collectively referred to as TimePFN, for multivariate time-series (MTS) forecasting. TimePFN is based on the concept of Prior-data Fitted Networks (PFN), which aims to approximate Bayesian inference. Our approach consists of (1) generating synthetic MTS data through diverse Gaussian process kernels and the linear coregionalization method, and (2) a novel MTS architecture capable of utilizing both temporal and cross-channel dependencies across all input patches. We evaluate TimePFN on several benchmark datasets and demonstrate that it outperforms the existing state-of-the-art models for MTS forecasting in both zero-shot and few-shot settings. Notably, fine-tuning TimePFN with as few as 500 data points nearly matches full dataset training error, and even 50 data points yield competitive results. We also find that TimePFN exhibits strong univariate forecasting performance, attesting to its generalization ability. Overall, this work unlocks the power of synthetic data priors for MTS forecasting and facilitates strong zero- and few-shot forecasting performance.</li>
</ul>

<h3>Title: DualNeRF: Text-Driven 3D Scene Editing via Dual-Field Representation</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Xiong, Yue Shi, Yishun Dou, Bingbing Ni</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16302">https://arxiv.org/abs/2502.16302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16302">https://arxiv.org/pdf/2502.16302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16302]] DualNeRF: Text-Driven 3D Scene Editing via Dual-Field Representation(https://arxiv.org/abs/2502.16302)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, denoising diffusion models have achieved promising results in 2D image generation and editing. Instruct-NeRF2NeRF (IN2N) introduces the success of diffusion into 3D scene editing through an "Iterative dataset update" (IDU) strategy. Though achieving fascinating results, IN2N suffers from problems of blurry backgrounds and trapping in local optima. The first problem is caused by IN2N's lack of efficient guidance for background maintenance, while the second stems from the interaction between image editing and NeRF training during IDU. In this work, we introduce DualNeRF to deal with these problems. We propose a dual-field representation to preserve features of the original scene and utilize them as additional guidance to the model for background maintenance during IDU. Moreover, a simulated annealing strategy is embedded into IDU to endow our model with the power of addressing local optima issues. A CLIP-based consistency indicator is used to further improve the editing quality by filtering out low-quality edits. Extensive experiments demonstrate that our method outperforms previous methods both qualitatively and quantitatively.</li>
</ul>

<h3>Title: Pointmap Association and Piecewise-Plane Constraint for Consistent and Compact 3D Gaussian Segmentation Field</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Hu, Wenhao Chai, Shengyu Hao, Xiaotong Cui, Xuexiang Wen, Jenq-Neng Hwang, Gaoang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16303">https://arxiv.org/abs/2502.16303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16303">https://arxiv.org/pdf/2502.16303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16303]] Pointmap Association and Piecewise-Plane Constraint for Consistent and Compact 3D Gaussian Segmentation Field(https://arxiv.org/abs/2502.16303)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Achieving a consistent and compact 3D segmentation field is crucial for maintaining semantic coherence across views and accurately representing scene structures. Previous 3D scene segmentation methods rely on video segmentation models to address inconsistencies across views, but the absence of spatial information often leads to object misassociation when object temporarily disappear and reappear. Furthermore, in the process of 3D scene reconstruction, segmentation and optimization are often treated as separate tasks. As a result, optimization typically lacks awareness of semantic category information, which can result in floaters with ambiguous segmentation. To address these challenges, we introduce CCGS, a method designed to achieve both view consistent 2D segmentation and a compact 3D Gaussian segmentation field. CCGS incorporates pointmap association and a piecewise-plane constraint. First, we establish pixel correspondence between adjacent images by minimizing the Euclidean distance between their pointmaps. We then redefine object mask overlap accordingly. The Hungarian algorithm is employed to optimize mask association by minimizing the total matching cost, while allowing for partial matches. To further enhance compactness, the piecewise-plane constraint restricts point displacement within local planes during optimization, thereby preserving structural integrity. Experimental results on ScanNet and Replica datasets demonstrate that CCGS outperforms existing methods in both 2D panoptic segmentation and 3D Gaussian segmentation.</li>
</ul>

<h3>Title: Generalization is not a universal guarantee: Estimating similarity to training data with an ensemble out-of-distribution metric</h3>
<ul>
<li><strong>Authors: </strong>W. Max Schreyer (1, 2 and 3), Christopher Anderson (3), Reid F. Thompson (1, 2, 3, 4, 5 and 6) ((1) Computational Biology Program, Oregon Health &amp; Science University, Portland, USA, (2) Department of Biomedical Engineering, Oregon Health &amp; Science University, Portland, USA, (3) Portland VA Research Foundation, Portland, USA, (4) Department of Radiation Medicine, Oregon Health &amp; Science University, Portland, USA, (5) Department of Medical Informatics and Clinical Epidemiology, Oregon Health &amp; Science University, Portland, USA, (6) Division of Hospital and Specialty Medicine, VA Portland Healthcare System, Portland, USA)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16329">https://arxiv.org/abs/2502.16329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16329">https://arxiv.org/pdf/2502.16329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16329]] Generalization is not a universal guarantee: Estimating similarity to training data with an ensemble out-of-distribution metric(https://arxiv.org/abs/2502.16329)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Failure of machine learning models to generalize to new data is a core problem limiting the reliability of AI systems, partly due to the lack of simple and robust methods for comparing new data to the original training dataset. We propose a standardized approach for assessing data similarity in a model-agnostic manner by constructing a supervised autoencoder for generalizability estimation (SAGE). We compare points in a low-dimensional embedded latent space, defining empirical probability measures for k-Nearest Neighbors (kNN) distance, reconstruction of inputs and task-based performance. As proof of concept for classification tasks, we use MNIST and CIFAR-10 to demonstrate how an ensemble output probability score can separate deformed images from a mixture of typical test examples, and how this SAGE score is robust to transformations of increasing severity. As further proof of concept, we extend this approach to a regression task using non-imaging data (UCI Abalone). In all cases, we show that out-of-the-box model performance increases after SAGE score filtering, even when applied to data from the model's own training and test datasets. Our out-of-distribution scoring method can be introduced during several steps of model construction and assessment, leading to future improvements in responsible deep learning implementation.</li>
</ul>

<h3>Title: Exploring Sentiment Manipulation by LLM-Enabled Intelligent Trading Agents</h3>
<ul>
<li><strong>Authors: </strong>David Byrd</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16343">https://arxiv.org/abs/2502.16343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16343">https://arxiv.org/pdf/2502.16343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16343]] Exploring Sentiment Manipulation by LLM-Enabled Intelligent Trading Agents(https://arxiv.org/abs/2502.16343)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Companies across all economic sectors continue to deploy large language models at a rapid pace. Reinforcement learning is experiencing a resurgence of interest due to its association with the fine-tuning of language models from human feedback. Tool-chain language models control task-specific agents; if the converse has not already appeared, it soon will. In this paper, we present what we believe is the first investigation of an intelligent trading agent based on continuous deep reinforcement learning that also controls a large language model with which it can post to a social media feed observed by other traders. We empirically investigate the performance and impact of such an agent in a simulated financial market, finding that it learns to optimize its total reward, and thereby augment its profit, by manipulating the sentiment of the posts it produces. The paper concludes with discussion, limitations, and suggestions for future work.</li>
</ul>

<h3>Title: Verifying Classification with Limited Disclosure</h3>
<ul>
<li><strong>Authors: </strong>Siddharth Bhandari, Liren Shan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16352">https://arxiv.org/abs/2502.16352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16352">https://arxiv.org/pdf/2502.16352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16352]] Verifying Classification with Limited Disclosure(https://arxiv.org/abs/2502.16352)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We consider the multi-party classification problem introduced by Dong, Hartline, and Vijayaraghavan (2022) motivated by electronic discovery. In this problem, our goal is to design a protocol that guarantees the requesting party receives nearly all responsive documents while minimizing the disclosure of nonresponsive documents. We develop verification protocols that certify the correctness of a classifier by disclosing a few nonresponsive documents. We introduce a combinatorial notion called the Leave-One-Out dimension of a family of classifiers and show that the number of nonresponsive documents disclosed by our protocol is at most this dimension in the realizable setting, where a perfect classifier exists in this family. For linear classifiers with a margin, we characterize the trade-off between the margin and the number of nonresponsive documents that must be disclosed for verification. Specifically, we establish a trichotomy in this requirement: for $d$ dimensional instances, when the margin exceeds $1/3$, verification can be achieved by revealing only $O(1)$ nonresponsive documents; when the margin is exactly $1/3$, in the worst case, at least $\Omega(d)$ nonresponsive documents must be disclosed; when the margin is smaller than $1/3$, verification requires $\Omega(e^d)$ nonresponsive documents. We believe this result is of independent interest with applications to coding theory and combinatorial geometry. We further extend our protocols to the nonrealizable setting defining an analogous combinatorial quantity robust Leave-One-Out dimension, and to scenarios where the protocol is tolerant to misclassification errors by Alice.</li>
</ul>

<h3>Title: Wrong Answers Can Also Be Useful: PlausibleQA -- A Large-Scale QA Dataset with Answer Plausibility Scores</h3>
<ul>
<li><strong>Authors: </strong>Jamshid Mozafari, Abdelrahman Abdallah, Bhawna Piryani, Adam Jatowt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16358">https://arxiv.org/abs/2502.16358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16358">https://arxiv.org/pdf/2502.16358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16358]] Wrong Answers Can Also Be Useful: PlausibleQA -- A Large-Scale QA Dataset with Answer Plausibility Scores(https://arxiv.org/abs/2502.16358)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are revolutionizing information retrieval, with chatbots becoming an important source for answering user queries. As by their design, LLMs prioritize generating correct answers, the value of highly plausible yet incorrect answers (candidate answers) tends to be overlooked. However, such answers can still prove useful, for example, they can play a crucial role in tasks like Multiple-Choice Question Answering (MCQA) and QA Robustness Assessment (QARA). Existing QA datasets primarily focus on correct answers without explicit consideration of the plausibility of other candidate answers, limiting opportunity for more nuanced evaluations of models. To address this gap, we introduce PlausibleQA, a large-scale dataset comprising 10,000 questions and 100,000 candidate answers, each annotated with plausibility scores and justifications for their selection. Additionally, the dataset includes 900,000 justifications for pairwise comparisons between candidate answers, further refining plausibility assessments. We evaluate PlausibleQA through human assessments and empirical experiments, demonstrating its utility in MCQA and QARA analysis. Our findings show that plausibility-aware approaches are effective for MCQA distractor generation and QARA. We release PlausibleQA as a resource for advancing QA research and enhancing LLM performance in distinguishing plausible distractors from correct answers.</li>
</ul>

<h3>Title: Audio Visual Segmentation Through Text Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Kyungbok Lee, You Zhang, Zhiyao Duan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16359">https://arxiv.org/abs/2502.16359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16359">https://arxiv.org/pdf/2502.16359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16359]] Audio Visual Segmentation Through Text Embeddings(https://arxiv.org/abs/2502.16359)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The goal of Audio-Visual Segmentation (AVS) is to localize and segment the sounding source objects from the video frames. Researchers working on AVS suffer from limited datasets because hand-crafted annotation is expensive. Recent works attempt to overcome the challenge of limited data by leveraging the segmentation foundation model, SAM, prompting it with audio to enhance its ability to segment sounding source objects. While this approach alleviates the model's burden on understanding visual modality by utilizing pre-trained knowledge of SAM, it does not address the fundamental challenge of the limited dataset for learning audio-visual relationships. To address these limitations, we propose \textbf{AV2T-SAM}, a novel framework that bridges audio features with the text embedding space of pre-trained text-prompted SAM. Our method leverages multimodal correspondence learned from rich text-image paired datasets to enhance audio-visual alignment. Furthermore, we introduce a novel feature, $\mathbf{\textit{\textbf{f}}_{CLIP} \odot \textit{\textbf{f}}_{CLAP}}$, which emphasizes shared semantics of audio and visual modalities while filtering irrelevant noise. Experiments on the AVSBench dataset demonstrate state-of-the-art performance on both datasets of AVSBench. Our approach outperforms existing methods by effectively utilizing pretrained segmentation models and cross-modal semantic alignment.</li>
</ul>

<h3>Title: A generative approach to LLM harmfulness detection with special red flag tokens</h3>
<ul>
<li><strong>Authors: </strong>Sophie Xhonneux, David Dobre, Mehrnaz Mohfakhami, Leo Schwinn, Gauthier Gidel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16366">https://arxiv.org/abs/2502.16366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16366">https://arxiv.org/pdf/2502.16366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16366]] A generative approach to LLM harmfulness detection with special red flag tokens(https://arxiv.org/abs/2502.16366)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Most safety training methods for large language models (LLMs) based on fine-tuning rely on dramatically changing the output distribution of the model when faced with a harmful request, shifting it from an unsafe answer to a refusal to respond. These methods inherently compromise model capabilities and might make auto-regressive models vulnerable to attacks that make likely an initial token of affirmative response. To avoid that, we propose to expand the model's vocabulary with a special token we call red flag token (<rf>) and propose to fine-tune the model to generate this token at any time harmful content is generated or about to be generated. This novel safety training method effectively augments LLMs into generative classifiers of harmfulness at all times during the conversation. This method offers several advantages: it enables the model to explicitly learn the concept of harmfulness while marginally affecting the generated distribution, thus maintaining the model's utility. It also evaluates each generated answer rather than just the input prompt and provides a stronger defence against sampling-based attacks. In addition, it simplifies the evaluation of the model's robustness and reduces correlated failures when combined with a classifier. We further show an increased robustness to long contexts, and supervised fine-tuning attacks.</li>
</ul>

<h3>Title: Concept Corrector: Erase concepts on the fly for text-to-image diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Zheling Meng, Bo Peng, Xiaochuan Jin, Yueming Lyu, Wei Wang, Jing Dong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16368">https://arxiv.org/abs/2502.16368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16368">https://arxiv.org/pdf/2502.16368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16368]] Concept Corrector: Erase concepts on the fly for text-to-image diffusion models(https://arxiv.org/abs/2502.16368)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have demonstrated the underlying risk of generating various unwanted content, such as sexual elements. To address this issue, the task of concept erasure has been introduced, aiming to erase any undesired concepts that the models can generate. Previous methods, whether training-based or training-free, have primarily focused on the input side, i.e. texts. However, they often suffer from incomplete erasure due to limitations in the generalization from limited prompts to diverse image content. In this paper, motivated by the notion that concept erasure on the output side, i.e. generated images, may be more direct and effective, we propose to check concepts based on intermediate-generated images and correct them in the remainder of the generation process. Two key challenges are identified, i.e. determining the presence of target concepts during generation and replacing them on the fly. Leveraging the generation mechanism of diffusion models, we present the Concept Corrector, which incorporates the Generation Check Mechanism and the Concept Removal Attention. This method can identify the generated features associated with target concepts and replace them using pre-defined negative prompts, thereby achieving concept erasure. It requires no changes to model parameters and only relies on a given concept name and its replacement content. To the best of our knowledge, this is the first erasure method based on intermediate-generated images. The experiments on various concepts demonstrate its impressive erasure performance. Code: this https URL.</li>
</ul>

<h3>Title: Personhood Credentials: Human-Centered Design Recommendation Balancing Security, Usability, and Trust</h3>
<ul>
<li><strong>Authors: </strong>Ayae Ide, Tanusree Sharma</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16375">https://arxiv.org/abs/2502.16375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16375">https://arxiv.org/pdf/2502.16375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16375]] Personhood Credentials: Human-Centered Design Recommendation Balancing Security, Usability, and Trust(https://arxiv.org/abs/2502.16375)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, biometric</a></li>
<li><strong>Abstract: </strong>Building on related concepts, like, decentralized identifiers (DIDs), proof of personhood, anonymous credentials, personhood credentials (PHCs) emerged as an alternative approach, enabling individuals to verify to digital service providers that they are a person without disclosing additional information. However, new technologies might introduce some friction due to users misunderstandings and mismatched expectations. Despite their growing importance, limited research has been done on users perceptions and preferences regarding PHCs. To address this gap, we conducted competitive analysis, and semi-structured online user interviews with 23 participants from US and EU to provide concrete design recommendations for PHCs that incorporate user needs, adoption rules, and preferences. Our study -- (a)surfaces how people reason about unknown privacy and security guarantees of PHCs compared to current verification methods -- (b) presents the impact of several factors on how people would like to onboard and manage PHCs, including, trusted issuers (e.g. gov), ground truth data to issue PHC (e.g biometrics, physical id), and issuance system (e.g. centralized vs decentralized). In a think-aloud conceptual design session, participants recommended -- conceptualized design, such as periodic biometrics verification, time-bound credentials, visually interactive human-check, and supervision of government for issuance system. We propose actionable designs reflecting users preferences.</li>
</ul>

<h3>Title: Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines</h3>
<ul>
<li><strong>Authors: </strong>Saurabh Srivastava, Sweta Pati, Ziyu Yao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16377">https://arxiv.org/abs/2502.16377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16377">https://arxiv.org/pdf/2502.16377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16377]] Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines(https://arxiv.org/abs/2502.16377)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>In this work, we study the effect of annotation guidelines -- textual descriptions of event types and arguments, when instruction-tuning large language models for event extraction. We conducted a series of experiments with both human-provided and machine-generated guidelines in both full- and low-data settings. Our results demonstrate the promise of annotation guidelines when there is a decent amount of training data and highlight its effectiveness in improving cross-schema generalization and low-frequency event-type performance.</li>
</ul>

<h3>Title: An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science</h3>
<ul>
<li><strong>Authors: </strong>Qiuhai Zeng, Claire Jin, Xinyue Wang, Yuhan Zheng, Qunhua Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16395">https://arxiv.org/abs/2502.16395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16395">https://arxiv.org/pdf/2502.16395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16395]] An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science(https://arxiv.org/abs/2502.16395)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated potential for data science tasks via code generation. However, the exploratory nature of data science, alongside the stochastic and opaque outputs of LLMs, raise concerns about their reliability. While prior work focuses on benchmarking LLM accuracy, reproducibility remains underexplored, despite being critical to establishing trust in LLM-driven analysis. We propose a novel analyst-inspector framework to automatically evaluate and enforce the reproducibility of LLM-generated data science workflows - the first rigorous approach to the best of our knowledge. Defining reproducibility as the sufficiency and completeness of workflows for reproducing functionally equivalent code, this framework enforces computational reproducibility principles, ensuring transparent, well-documented LLM workflows while minimizing reliance on implicit model assumptions. Using this framework, we systematically evaluate five state-of-the-art LLMs on 1,032 data analysis tasks across three diverse benchmark datasets. We also introduce two novel reproducibility-enhancing prompting strategies. Our results show that higher reproducibility strongly correlates with improved accuracy and reproducibility-enhancing prompts are effective, demonstrating structured prompting's potential to enhance automated data science workflows and enable transparent, robust AI-driven analysis. Our code is publicly available.</li>
</ul>

<h3>Title: FedNIA: Noise-Induced Activation Analysis for Mitigating Data Poisoning in FL</h3>
<ul>
<li><strong>Authors: </strong>Ehsan Hallaji, Roozbeh Razavi-Far, Mehrdad Saif</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16396">https://arxiv.org/abs/2502.16396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16396">https://arxiv.org/pdf/2502.16396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16396]] FedNIA: Noise-Induced Activation Analysis for Mitigating Data Poisoning in FL(https://arxiv.org/abs/2502.16396)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning systems are increasingly threatened by data poisoning attacks, where malicious clients compromise global models by contributing tampered updates. Existing defenses often rely on impractical assumptions, such as access to a central test dataset, or fail to generalize across diverse attack types, particularly those involving multiple malicious clients working collaboratively. To address this, we propose Federated Noise-Induced Activation Analysis (FedNIA), a novel defense framework to identify and exclude adversarial clients without relying on any central test dataset. FedNIA injects random noise inputs to analyze the layerwise activation patterns in client models leveraging an autoencoder that detects abnormal behaviors indicative of data poisoning. FedNIA can defend against diverse attack types, including sample poisoning, label flipping, and backdoors, even in scenarios with multiple attacking nodes. Experimental results on non-iid federated datasets demonstrate its effectiveness and robustness, underscoring its potential as a foundational approach for enhancing the security of federated learning systems.</li>
</ul>

<h3>Title: Efficient Semantic-aware Encryption for Secure Communications in Intelligent Connected Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Bizhu Wang, Zhiqiang Bian, Yue Chen, Xiaodong Xu, Chen Sun, Wenqi Zhang, Ping Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16400">https://arxiv.org/abs/2502.16400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16400">https://arxiv.org/pdf/2502.16400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16400]] Efficient Semantic-aware Encryption for Secure Communications in Intelligent Connected Vehicles(https://arxiv.org/abs/2502.16400)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Semantic communication (SemCom) significantly improves inter-vehicle interactions in intelligent connected vehicles (ICVs) within limited wireless spectrum. However, the open nature of wireless communications introduces eavesdropping risks. To mitigate this, we propose the Efficient Semantic-aware Encryption (ESAE) mechanism, integrating cryptography into SemCom to secure semantic transmission without complex key management. ESAE leverages semantic reciprocity between source and reconstructed information from past communications to independently generate session keys at both ends, reducing key transmission costs and associated security risks. Additionally, ESAE introduces a semantic-aware key pre-processing method (SA-KP) using the YOLO-v10 model to extract consistent semantics from bit-level diverse yet semantically identical content, ensuring key consistency. Experimental results validate ESAE's effectiveness and feasibility under various wireless conditions, with key performance factors discussed.</li>
</ul>

<h3>Title: TrustChain: A Blockchain Framework for Auditing and Verifying Aggregators in Decentralized Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Ehsan Hallaji, Roozbeh Razavi-Far, Mehrdad Saif</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16406">https://arxiv.org/abs/2502.16406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16406">https://arxiv.org/pdf/2502.16406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16406]] TrustChain: A Blockchain Framework for Auditing and Verifying Aggregators in Decentralized Federated Learning(https://arxiv.org/abs/2502.16406)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, federate</a></li>
<li><strong>Abstract: </strong>The server-less nature of Decentralized Federated Learning (DFL) requires allocating the aggregation role to specific participants in each federated round. Current DFL architectures ensure the trustworthiness of the aggregator node upon selection. However, most of these studies overlook the possibility that the aggregating node may turn rogue and act maliciously after being nominated. To address this problem, this paper proposes a DFL structure, called TrustChain, that scores the aggregators before selection based on their past behavior and additionally audits them after the aggregation. To do this, the statistical independence between the client updates and the aggregated model is continuously monitored using the Hilbert-Schmidt Independence Criterion (HSIC). The proposed method relies on several principles, including blockchain, anomaly detection, and concept drift analysis. The designed structure is evaluated on several federated datasets and attack scenarios with different numbers of Byzantine nodes.</li>
</ul>

<h3>Title: A Survey on Industrial Anomalies Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Xichen Xu, Yanshu Wang, Yawen Huang, Jiaqi Liu, Xiaoning Lei, Guoyang Xie, Guannan Jiang, Zhichao Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16412">https://arxiv.org/abs/2502.16412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16412">https://arxiv.org/pdf/2502.16412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16412]] A Survey on Industrial Anomalies Synthesis(https://arxiv.org/abs/2502.16412)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper comprehensively reviews anomaly synthesis methodologies. Existing surveys focus on limited techniques, missing an overall field view and understanding method interconnections. In contrast, our study offers a unified review, covering about 40 representative methods across Hand-crafted, Distribution-hypothesis-based, Generative models (GM)-based, and Vision-language models (VLM)-based synthesis. We introduce the first industrial anomaly synthesis (IAS) taxonomy. Prior works lack formal classification or use simplistic taxonomies, hampering structured comparisons and trend identification. Our taxonomy provides a fine-grained framework reflecting methodological progress and practical implications, grounding future research. Furthermore, we explore cross-modality synthesis and large-scale VLM. Previous surveys overlooked multimodal data and VLM in anomaly synthesis, limiting insights into their advantages. Our survey analyzes their integration, benefits, challenges, and prospects, offering a roadmap to boost IAS with multimodal learning. More resources are available at this https URL.</li>
</ul>

<h3>Title: TabGen-ICL: Residual-Aware In-Context Example Selection for Tabular Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Liancheng Fang, Aiwei Liu, Hengrui Zhang, Henry Peng Zou, Weizhi Zhang, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16414">https://arxiv.org/abs/2502.16414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16414">https://arxiv.org/pdf/2502.16414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16414]] TabGen-ICL: Residual-Aware In-Context Example Selection for Tabular Data Generation(https://arxiv.org/abs/2502.16414)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language models (LLMs) have achieved encouraging results in tabular data generation. However, existing approaches require fine-tuning, which is computationally expensive. This paper explores an alternative: prompting a fixed LLM with in-context examples. We observe that using randomly selected in-context examples hampers the LLM's performance, resulting in sub-optimal generation quality. To address this, we propose a novel in-context learning framework: TabGen-ICL, to enhance the in-context learning ability of LLMs for tabular data generation. TabGen-ICL operates iteratively, retrieving a subset of real samples that represent the residual between currently generated samples and true data distributions. This approach serves two purposes: locally, it provides more effective in-context learning examples for the LLM in each iteration; globally, it progressively narrows the gap between generated and real data. Extensive experiments on five real-world tabular datasets demonstrate that TabGen-ICL significantly outperforms the random selection strategy. Specifically, it reduces the error rate by a margin of $3.5\%-42.2\%$ on fidelity metrics. We demonstrate for the first time that prompting a fixed LLM can yield high-quality synthetic tabular data. The code is provided in the \href{this https URL}{link}.</li>
</ul>

<h3>Title: DeProPose: Deficiency-Proof 3D Human Pose Estimation via Adaptive Multi-View Fusion</h3>
<ul>
<li><strong>Authors: </strong>Jianbin Jiao, Xina Cheng, Kailun Yang, Xiangrong Zhang, Licheng Jiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16419">https://arxiv.org/abs/2502.16419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16419">https://arxiv.org/pdf/2502.16419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16419]] DeProPose: Deficiency-Proof 3D Human Pose Estimation via Adaptive Multi-View Fusion(https://arxiv.org/abs/2502.16419)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>3D human pose estimation has wide applications in fields such as intelligent surveillance, motion capture, and virtual reality. However, in real-world scenarios, issues such as occlusion, noise interference, and missing viewpoints can severely affect pose estimation. To address these challenges, we introduce the task of Deficiency-Aware 3D Pose Estimation. Traditional 3D pose estimation methods often rely on multi-stage networks and modular combinations, which can lead to cumulative errors and increased training complexity, making them unable to effectively address deficiency-aware estimation. To this end, we propose DeProPose, a flexible method that simplifies the network architecture to reduce training complexity and avoid information loss in multi-stage designs. Additionally, the model innovatively introduces a multi-view feature fusion mechanism based on relative projection error, which effectively utilizes information from multiple viewpoints and dynamically assigns weights, enabling efficient integration and enhanced robustness to overcome deficiency-aware 3D Pose Estimation challenges. Furthermore, to thoroughly evaluate this end-to-end multi-view 3D human pose estimation model and to advance research on occlusion-related challenges, we have developed a novel 3D human pose estimation dataset, termed the Deficiency-Aware 3D Pose Estimation (DA-3DPE) dataset. This dataset encompasses a wide range of deficiency scenarios, including noise interference, missing viewpoints, and occlusion challenges. Compared to state-of-the-art methods, DeProPose not only excels in addressing the deficiency-aware problem but also shows improvement in conventional scenarios, providing a powerful and user-friendly solution for 3D human pose estimation. The source code will be available at this https URL.</li>
</ul>

<h3>Title: High-resolution Rainy Image Synthesis: Learning from Rendering</h3>
<ul>
<li><strong>Authors: </strong>Kaibin Zhou, Shengjie Zhao, Hao Deng, Lin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16421">https://arxiv.org/abs/2502.16421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16421">https://arxiv.org/pdf/2502.16421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16421]] High-resolution Rainy Image Synthesis: Learning from Rendering(https://arxiv.org/abs/2502.16421)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Currently, there are few effective methods for synthesizing a mass of high-resolution rainy images in complex illumination conditions. However, these methods are essential for synthesizing large-scale high-quality paired rainy-clean image datasets, which can train deep learning-based single image rain removal models capable of generalizing to various illumination conditions. Therefore, we propose a practical two-stage learning-from-rendering pipeline for high-resolution rainy image synthesis. The pipeline combines the benefits of the realism of rendering-based methods and the high-efficiency of learning-based methods, providing the possibility of creating large-scale high-quality paired rainy-clean image datasets. In the rendering stage, we use a rendering-based method to create a High-resolution Rainy Image (HRI) dataset, which contains realistic high-resolution paired rainy-clean images of multiple scenes and various illumination conditions. In the learning stage, to learn illumination information from background images for high-resolution rainy image generation, we propose a High-resolution Rainy Image Generation Network (HRIGNet). HRIGNet is designed to introduce a guiding diffusion model in the Latent Diffusion Model, which provides additional guidance information for high-resolution image synthesis. In our experiments, HRIGNet is able to synthesize high-resolution rainy images up to 2048x1024 resolution. Rain removal experiments on real dataset validate that our method can help improve the robustness of deep derainers to real rainy images. To make our work reproducible, source codes and the dataset have been released at this https URL.</li>
</ul>

<h3>Title: Unified Prompt Attack Against Text-to-Image Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Duo Peng, Qiuhong Ke, Mark He Huang, Ping Hu, Jun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16423">https://arxiv.org/abs/2502.16423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16423">https://arxiv.org/pdf/2502.16423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16423]] Unified Prompt Attack Against Text-to-Image Generation Models(https://arxiv.org/abs/2502.16423)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Text-to-Image (T2I) models have advanced significantly, but their growing popularity raises security concerns due to their potential to generate harmful images. To address these issues, we propose UPAM, a novel framework to evaluate the robustness of T2I models from an attack perspective. Unlike prior methods that focus solely on textual defenses, UPAM unifies the attack on both textual and visual defenses. Additionally, it enables gradient-based optimization, overcoming reliance on enumeration for improved efficiency and effectiveness. To handle cases where T2I models block image outputs due to defenses, we introduce Sphere-Probing Learning (SPL) to enable optimization even without image results. Following SPL, our model bypasses defenses, inducing the generation of harmful content. To ensure semantic alignment with attacker intent, we propose Semantic-Enhancing Learning (SEL) for precise semantic control. UPAM also prioritizes the naturalness of adversarial prompts using In-context Naturalness Enhancement (INE), making them harder for human examiners to detect. Additionally, we address the issue of iterative queries--common in prior methods and easily detectable by API defenders--by introducing Transferable Attack Learning (TAL), allowing effective attacks with minimal queries. Extensive experiments validate UPAM's superiority in effectiveness, efficiency, naturalness, and low query detection rates.</li>
</ul>

<h3>Title: Visual Reasoning Evaluation of Grok, Deepseek Janus, Gemini, Qwen, Mistral, and ChatGPT</h3>
<ul>
<li><strong>Authors: </strong>Nidhal Jegham, Marwan Abdelatti, Abdeltawab Hendawi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16428">https://arxiv.org/abs/2502.16428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16428">https://arxiv.org/pdf/2502.16428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16428]] Visual Reasoning Evaluation of Grok, Deepseek Janus, Gemini, Qwen, Mistral, and ChatGPT(https://arxiv.org/abs/2502.16428)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Traditional evaluations of multimodal large language models (LLMs) have been limited by their focus on single-image reasoning, failing to assess crucial aspects like contextual understanding, reasoning stability, and uncertainty calibration. This study addresses these limitations by introducing a novel benchmark that integrates multi-image reasoning tasks with rejection-based evaluation and positional bias detection. To evaluate these dimensions, we further introduce entropy as a novel metric for quantifying reasoning consistency across reordered answer variants. We applied this benchmark to assess Grok 3, ChatGPT-4o, ChatGPT-o1, Gemini 2.0 Flash Experimental, DeepSeek Janus models, Qwen2.5-VL-72B-Instruct, QVQ-72B-Preview, and Pixtral 12B across eight visual reasoning tasks, including difference spotting and diagram interpretation. Our findings reveal ChatGPT-o1 leading in overall accuracy (82.5\%) and rejection accuracy (70.0\%), closely followed by Gemini 2.0 Flash Experimental (70.8\%). QVQ-72B-Preview demonstrated superior rejection accuracy (85.5\%). Notably, Pixtral 12B (51.7\%) showed promise in specific domains, while Janus models exhibited challenges in bias and uncertainty calibration, reflected in low rejection accuracies and high entropy scores. High entropy scores in Janus models (Janus 7B: 0.8392, Janus 1B: 0.787) underscore their susceptibility to positional bias and unstable reasoning, contrasting with the low entropy and robust reasoning of ChatGPT models. The study further demonstrates that model size is not the sole determinant of performance, as evidenced by Grok 3 underperformance despite its substantial parameter count. By employing multi-image contexts, rejection mechanisms, and entropy-based consistency metrics, this benchmark sets a new standard for evaluating multimodal LLMs, enabling a more robust and reliable assessment of next-generation AI systems.</li>
</ul>

<h3>Title: UniDyG: A Unified and Effective Representation Learning Approach for Large Dynamic Graphs</h3>
<ul>
<li><strong>Authors: </strong>Yuanyuan Xu, Wenjie Zhang, Xuemin Lin, Ying Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16431">https://arxiv.org/abs/2502.16431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16431">https://arxiv.org/pdf/2502.16431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16431]] UniDyG: A Unified and Effective Representation Learning Approach for Large Dynamic Graphs(https://arxiv.org/abs/2502.16431)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Dynamic graphs are formulated in continuous-time or discrete-time dynamic graphs. They differ in temporal granularity: Continuous-Time Dynamic Graphs (CTDGs) exhibit rapid, localized changes, while Discrete-Time Dynamic Graphs (DTDGs) show gradual, global updates. This difference leads to isolated developments in representation learning for each type. To advance representation learning, recent research attempts to design a unified model capable of handling both CTDGs and DTDGs. However, it typically focuses on local dynamic propagation for temporal structure learning in the time domain, failing to accurately capture the structural evolution associated with each temporal granularity. In addition, existing works-whether specific or unified-often overlook the issue of temporal noise, compromising the model robustness and effectiveness. To better model both types of dynamic graphs, we propose UniDyG, a unified and effective representation learning approach, which scales to large dynamic graphs. We first propose a novel Fourier Graph Attention (FGAT) mechanism that can model local and global structural correlations based on recent neighbors and complex-number selective aggregation, while theoretically ensuring consistent representations of dynamic graphs over time. Based on approximation theory, we demonstrate that FGAT is well-suited to capture the underlying structures in CTDGs and DTDGs. We further enhance FGAT to resist temporal noise by designing an energy-gated unit, which adaptively filters out high-frequency noise according to the energy. Last, we leverage our FGAT mechanisms for temporal structure learning and employ the frequency-enhanced linear function for node-level dynamic updates, facilitating the generation of high-quality temporal embeddings. Extensive experiments show that our UniDyG achieves an average improvement of 14.4% over sixteen baselines across nine dynamic graphs.</li>
</ul>

<h3>Title: Automated Flow Pattern Classification in Multi-phase Systems Using AI and Capacitance Sensing Techniques</h3>
<ul>
<li><strong>Authors: </strong>Nian Ran, Fayez M. Al-Alweet, Richard Allmendinger, Ahmad Almakhlafi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16432">https://arxiv.org/abs/2502.16432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16432">https://arxiv.org/pdf/2502.16432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16432]] Automated Flow Pattern Classification in Multi-phase Systems Using AI and Capacitance Sensing Techniques(https://arxiv.org/abs/2502.16432)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In multiphase flow systems, classifying flow patterns is crucial to optimize fluid dynamics and enhance system efficiency. Current industrial methods and scientific laboratories mainly depend on techniques such as flow visualization using regular cameras or the naked eye, as well as high-speed imaging at elevated flow rates. These methods are limited by their reliance on subjective interpretations and are particularly applicable in transparent pipes. Consequently, conventional techniques usually achieve context-dependent accuracy rates and often lack generalizability. This study introduces a novel platform that integrates a capacitance sensor and AI-driven classification methods, benchmarked against traditional techniques. Experimental results demonstrate that the proposed approach, utilizing a 1D SENet deep learning model, achieves over 85\% accuracy on experiment-based datasets and 71\% accuracy on pattern-based datasets. These results highlight significant improvements in robustness and reliability compared to existing methodologies. This work offers a transformative pathway for real-time flow monitoring and predictive modeling, addressing key challenges in industrial applications.</li>
</ul>

<h3>Title: Sequence-level Large Language Model Training with Contrastive Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zhili Feng, Dhananjay Ram, Cole Hawkins, Aditya Rawal, Jinman Zhao, Sheng Zha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16433">https://arxiv.org/abs/2502.16433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16433">https://arxiv.org/pdf/2502.16433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16433]] Sequence-level Large Language Model Training with Contrastive Preference Optimization(https://arxiv.org/abs/2502.16433)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The next token prediction loss is the dominant self-supervised training objective for large language models and has achieved promising results in a variety of downstream tasks. However, upon closer investigation of this objective, we find that it lacks an understanding of sequence-level signals, leading to a mismatch between training and inference processes. To bridge this gap, we introduce a contrastive preference optimization (CPO) procedure that can inject sequence-level information into the language model at any training stage without expensive human labeled data. Our experiments show that the proposed objective surpasses the next token prediction in terms of win rate in the instruction-following and text generation tasks.</li>
</ul>

<h3>Title: VisFactor: Benchmarking Fundamental Visual Cognition in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jen-Tse Huang, Dasen Dai, Jen-Yuan Huang, Youliang Yuan, Xiaoyuan Liu, Wenxuan Wang, Wenxiang Jiao, Pinjia He, Zhaopeng Tu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16435">https://arxiv.org/abs/2502.16435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16435">https://arxiv.org/pdf/2502.16435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16435]] VisFactor: Benchmarking Fundamental Visual Cognition in Multimodal Large Language Models(https://arxiv.org/abs/2502.16435)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in multimodal understanding; however, their fundamental visual cognitive abilities remain largely underexplored. To bridge this gap, we introduce VisFactor, a novel benchmark derived from the Factor-Referenced Cognitive Test (FRCT), a well-established psychometric assessment of human cognition. VisFactor digitalizes vision-related FRCT subtests to systematically evaluate MLLMs across essential visual cognitive tasks including spatial reasoning, perceptual speed, and pattern recognition. We present a comprehensive evaluation of state-of-the-art MLLMs, such as GPT-4o, Gemini-Pro, and Qwen-VL, using VisFactor under diverse prompting strategies like Chain-of-Thought and Multi-Agent Debate. Our findings reveal a concerning deficiency in current MLLMs' fundamental visual cognition, with performance frequently approaching random guessing and showing only marginal improvements even with advanced prompting techniques. These results underscore the critical need for focused research to enhance the core visual reasoning capabilities of MLLMs. To foster further investigation in this area, we release our VisFactor benchmark at this https URL.</li>
</ul>

<h3>Title: Compression Scaling Laws:Unifying Sparsity and Quantization</h3>
<ul>
<li><strong>Authors: </strong>Elias Frantar, Utku Evci, Wonpyo Park, Neil Houlsby, Dan Alistarh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16440">https://arxiv.org/abs/2502.16440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16440">https://arxiv.org/pdf/2502.16440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16440]] Compression Scaling Laws:Unifying Sparsity and Quantization(https://arxiv.org/abs/2502.16440)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We investigate how different compression techniques -- such as weight and activation quantization, and weight sparsity -- affect the scaling behavior of large language models (LLMs) during pretraining. Building on previous work showing that weight sparsity acts as a constant multiplier on model size in scaling laws, we demonstrate that this "effective parameter" scaling pattern extends to quantization as well. Specifically, we establish that weight-only quantization achieves strong parameter efficiency multipliers, while full quantization of both weights and activations shows diminishing returns at lower bitwidths. Our results suggest that different compression techniques can be unified under a common scaling law framework, enabling principled comparison and combination of these methods.</li>
</ul>

<h3>Title: Iterative Flow Matching -- Path Correction and Gradual Refinement for Enhanced Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Eldad Haber, Shadab Ahamed, Md. Shahriar Rahim Siddiqui, Niloufar Zakariaei, Moshe Eliasof</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16445">https://arxiv.org/abs/2502.16445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16445">https://arxiv.org/pdf/2502.16445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16445]] Iterative Flow Matching -- Path Correction and Gradual Refinement for Enhanced Generative Modeling(https://arxiv.org/abs/2502.16445)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Generative models for image generation are now commonly used for a wide variety of applications, ranging from guided image generation for entertainment to solving inverse problems. Nonetheless, training a generator is a non-trivial feat that requires fine-tuning and can lead to so-called hallucinations, that is, the generation of images that are unrealistic. In this work, we explore image generation using flow matching. We explain and demonstrate why flow matching can generate hallucinations, and propose an iterative process to improve the generation process. Our iterative process can be integrated into virtually $\textit{any}$ generative modeling technique, thereby enhancing the performance and robustness of image synthesis systems.</li>
</ul>

<h3>Title: Auxiliary Discrminator Sequence Generative Adversarial Networks (ADSeqGAN) for Few Sample Molecule Generation</h3>
<ul>
<li><strong>Authors: </strong>Haocheng Tang, Jing Long, Junmei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16446">https://arxiv.org/abs/2502.16446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16446">https://arxiv.org/pdf/2502.16446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16446]] Auxiliary Discrminator Sequence Generative Adversarial Networks (ADSeqGAN) for Few Sample Molecule Generation(https://arxiv.org/abs/2502.16446)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we introduce Auxiliary Discriminator Sequence Generative Adversarial Networks (ADSeqGAN), a novel approach for molecular generation in small-sample datasets. Traditional generative models often struggle with limited training data, particularly in drug discovery, where molecular datasets for specific therapeutic targets, such as nucleic acids binders and central nervous system (CNS) drugs, are scarce. ADSeqGAN addresses this challenge by integrating an auxiliary random forest classifier as an additional discriminator into the GAN framework, significantly improves molecular generation quality and class specificity. Our method incorporates pretrained generator and Wasserstein distance to enhance training stability and diversity. We evaluate ADSeqGAN on a dataset comprising nucleic acid-targeting and protein-targeting small molecules, demonstrating its superior ability to generate nucleic acid binders compared to baseline models such as SeqGAN, ORGAN, and MolGPT. Through an oversampling strategy, ADSeqGAN also significantly improves CNS drug generation, achieving a higher yield than traditional de novo models. Critical assessments, including docking simulations and molecular property analysis, confirm that ADSeqGAN-generated molecules exhibit strong binding affinities, enhanced chemical diversity, and improved synthetic feasibility. Overall, ADSeqGAN presents a novel framework for generative molecular design in data-scarce scenarios, offering potential applications in computational drug discovery. We have demonstrated the successful applications of ADSeqGAN in generating synthetic nucleic acid-targeting and CNS drugs in this work.</li>
</ul>

<h3>Title: Make Literature-Based Discovery Great Again through Reproducible Pipelines</h3>
<ul>
<li><strong>Authors: </strong>Bojan Cestnik, Andrej Kastrin, Boshko Koloski, Nada Lavrač</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16450">https://arxiv.org/abs/2502.16450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16450">https://arxiv.org/pdf/2502.16450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16450]] Make Literature-Based Discovery Great Again through Reproducible Pipelines(https://arxiv.org/abs/2502.16450)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>By connecting disparate sources of scientific literature, literature\-/based discovery (LBD) methods help to uncover new knowledge and generate new research hypotheses that cannot be found from domain-specific documents alone. Our work focuses on bisociative LBD methods that combine bisociative reasoning with LBD techniques. The paper presents LBD through the lens of reproducible science to ensure the reproducibility of LBD experiments, overcome the inconsistent use of benchmark datasets and methods, trigger collaboration, and advance the LBD field toward more robust and impactful scientific discoveries. The main novelty of this study is a collection of Jupyter Notebooks that illustrate the steps of the bisociative LBD process, including data acquisition, text preprocessing, hypothesis formulation, and evaluation. The contributed notebooks implement a selection of traditional LBD approaches, as well as our own ensemble-based, outlier-based, and link prediction-based approaches. The reader can benefit from hands-on experience with LBD through open access to benchmark datasets, code reuse, and a ready-to-run Docker recipe that ensures reproducibility of the selected LBD methods.</li>
</ul>

<h3>Title: Contrastive Learning of English Language and Crystal Graphs for Multimodal Representation of Materials Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Yang Jeong Park, Mayank Kumaran, Chia-Wei Hsu, Elsa Olivetti, Ju Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16451">https://arxiv.org/abs/2502.16451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16451">https://arxiv.org/pdf/2502.16451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16451]] Contrastive Learning of English Language and Crystal Graphs for Multimodal Representation of Materials Knowledge(https://arxiv.org/abs/2502.16451)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) is increasingly used for the inverse design of materials, such as crystals and molecules. Existing AI research on molecules has integrated chemical structures of molecules with textual knowledge to adapt to complex instructions. However, this approach has been unattainable for crystals due to data scarcity from the biased distribution of investigated crystals and the lack of semantic supervision in peer-reviewed literature. In this work, we introduce a contrastive language-crystals model (CLaC) pre-trained on a newly synthesized dataset of 126k crystal structure-text pairs. To demonstrate the advantage of using synthetic data to overcome data scarcity, we constructed a comparable dataset extracted from academic papers. We evaluate CLaC's generalization ability through various zero-shot cross-modal tasks and downstream applications. In experiments, CLaC achieves state-of-the-art zero-shot generalization performance in understanding crystal structures, surpassing latest large language models.</li>
</ul>

<h3>Title: Towards Fully-Automated Materials Discovery via Large-Scale Synthesis Dataset and Expert-Level LLM-as-a-Judge</h3>
<ul>
<li><strong>Authors: </strong>Heegyu Kim, Taeyang Jeon, Seungtaek Choi, Jihoon Hong, Dongwon Jeon, Sungbum Cho, Ga-Yeon Baek, Kyung-Won Kwak, Dong-Hee Lee, Sun-Jin Choi, Jisu Bae, Chihoon Lee, Yunseo Kim, Jinsung Park, Hyunsouk Cho</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16457">https://arxiv.org/abs/2502.16457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16457">https://arxiv.org/pdf/2502.16457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16457]] Towards Fully-Automated Materials Discovery via Large-Scale Synthesis Dataset and Expert-Level LLM-as-a-Judge(https://arxiv.org/abs/2502.16457)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Materials synthesis is vital for innovations such as energy storage, catalysis, electronics, and biomedical devices. Yet, the process relies heavily on empirical, trial-and-error methods guided by expert intuition. Our work aims to support the materials science community by providing a practical, data-driven resource. We have curated a comprehensive dataset of 17K expert-verified synthesis recipes from open-access literature, which forms the basis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an end-to-end framework that supports research in large language models applied to synthesis prediction. It encompasses key tasks, including raw materials and equipment prediction, synthesis procedure generation, and characterization outcome forecasting. We propose an LLM-as-a-Judge framework that leverages large language models for automated evaluation, demonstrating strong statistical agreement with expert assessments. Overall, our contributions offer a supportive foundation for exploring the capabilities of LLMs in predicting and guiding materials synthesis, ultimately paving the way for more efficient experimental design and accelerated innovation in materials science.</li>
</ul>

<h3>Title: Cross-domain Few-shot Object Detection with Multi-modal Textual Enrichment</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Shangguan, Daniel Seita, Mohammad Rostami</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16469">https://arxiv.org/abs/2502.16469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16469">https://arxiv.org/pdf/2502.16469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16469]] Cross-domain Few-shot Object Detection with Multi-modal Textual Enrichment(https://arxiv.org/abs/2502.16469)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Advancements in cross-modal feature extraction and integration have significantly enhanced performance in few-shot learning tasks. However, current multi-modal object detection (MM-OD) methods often experience notable performance degradation when encountering substantial domain shifts. We propose that incorporating rich textual information can enable the model to establish a more robust knowledge relationship between visual instances and their corresponding language descriptions, thereby mitigating the challenges of domain shift. Specifically, we focus on the problem of Cross-Domain Multi-Modal Few-Shot Object Detection (CDMM-FSOD) and introduce a meta-learning-based framework designed to leverage rich textual semantics as an auxiliary modality to achieve effective domain adaptation. Our new architecture incorporates two key components: (i) A multi-modal feature aggregation module, which aligns visual and linguistic feature embeddings to ensure cohesive integration across modalities. (ii) A rich text semantic rectification module, which employs bidirectional text feature generation to refine multi-modal feature alignment, thereby enhancing understanding of language and its application in object detection. We evaluate the proposed method on common cross-domain object detection benchmarks and demonstrate that it significantly surpasses existing few-shot object detection approaches.</li>
</ul>

<h3>Title: Feature Space Perturbation: A Panacea to Enhanced Transferability Estimation</h3>
<ul>
<li><strong>Authors: </strong>Prafful Kumar Khoba, Zijian Wang, Chetan Arora, Mahsa Baktashmotlagh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16471">https://arxiv.org/abs/2502.16471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16471">https://arxiv.org/pdf/2502.16471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16471]] Feature Space Perturbation: A Panacea to Enhanced Transferability Estimation(https://arxiv.org/abs/2502.16471)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Leveraging a transferability estimation metric facilitates the non-trivial challenge of selecting the optimal model for the downstream task from a pool of pre-trained models. Most existing metrics primarily focus on identifying the statistical relationship between feature embeddings and the corresponding labels within the target dataset, but overlook crucial aspect of model robustness. This oversight may limit their effectiveness in accurately ranking pre-trained models. To address this limitation, we introduce a feature perturbation method that enhances the transferability estimation process by systematically altering the feature space. Our method includes a Spread operation that increases intra-class variability, adding complexity within classes, and an Attract operation that minimizes the distances between different classes, thereby blurring the class boundaries. Through extensive experimentation, we demonstrate the efficacy of our feature perturbation method in providing a more precise and robust estimation of model transferability. Notably, the existing LogMe method exhibited a significant improvement, showing a 28.84% increase in performance after applying our feature perturbation method.</li>
</ul>

<h3>Title: Dragen3D: Multiview Geometry Consistent 3D Gaussian Generation with Drag-Based Control</h3>
<ul>
<li><strong>Authors: </strong>Jinbo Yan, Alan Zhao, Yixin Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16475">https://arxiv.org/abs/2502.16475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16475">https://arxiv.org/pdf/2502.16475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16475]] Dragen3D: Multiview Geometry Consistent 3D Gaussian Generation with Drag-Based Control(https://arxiv.org/abs/2502.16475)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Single-image 3D generation has emerged as a prominent research topic, playing a vital role in virtual reality, 3D modeling, and digital content creation. However, existing methods face challenges such as a lack of multi-view geometric consistency and limited controllability during the generation process, which significantly restrict their usability. % To tackle these challenges, we introduce Dragen3D, a novel approach that achieves geometrically consistent and controllable 3D generation leveraging 3D Gaussian Splatting (3DGS). We introduce the Anchor-Gaussian Variational Autoencoder (Anchor-GS VAE), which encodes a point cloud and a single image into anchor latents and decode these latents into 3DGS, enabling efficient latent-space generation. To enable multi-view geometry consistent and controllable generation, we propose a Seed-Point-Driven strategy: first generate sparse seed points as a coarse geometry representation, then map them to anchor latents via the Seed-Anchor Mapping Module. Geometric consistency is ensured by the easily learned sparse seed points, and users can intuitively drag the seed points to deform the final 3DGS geometry, with changes propagated through the anchor latents. To the best of our knowledge, we are the first to achieve geometrically controllable 3D Gaussian generation and editing without relying on 2D diffusion priors, delivering comparable 3D generation quality to state-of-the-art methods.</li>
</ul>

<h3>Title: A Split-Window Transformer for Multi-Model Sequence Spammer Detection using Multi-Model Variational Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Zhou Yang, Yucai Pang, Hongbo Yin, Yunpeng Xiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MM, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16483">https://arxiv.org/abs/2502.16483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16483">https://arxiv.org/pdf/2502.16483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16483]] A Split-Window Transformer for Multi-Model Sequence Spammer Detection using Multi-Model Variational Autoencoder(https://arxiv.org/abs/2502.16483)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper introduces a new Transformer, called MS$^2$Dformer, that can be used as a generalized backbone for multi-modal sequence spammer detection. Spammer detection is a complex multi-modal task, thus the challenges of applying Transformer are two-fold. Firstly, complex multi-modal noisy information about users can interfere with feature mining. Secondly, the long sequence of users' historical behaviors also puts a huge GPU memory pressure on the attention computation. To solve these problems, we first design a user behavior Tokenization algorithm based on the multi-modal variational autoencoder (MVAE). Subsequently, a hierarchical split-window multi-head attention (SW/W-MHA) mechanism is proposed. The split-window strategy transforms the ultra-long sequences hierarchically into a combination of intra-window short-term and inter-window overall attention. Pre-trained on the public datasets, MS$^2$Dformer's performance far exceeds the previous state of the art. The experiments demonstrate MS$^2$Dformer's ability to act as a backbone.</li>
</ul>

<h3>Title: A Fine-Tuning Approach for T5 Using Knowledge Graphs to Address Complex Tasks</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxuan Liao, Binrong Zhu, Jacky He, Guiran Liu, Hongye Zheng, Jia Gao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16484">https://arxiv.org/abs/2502.16484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16484">https://arxiv.org/pdf/2502.16484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16484]] A Fine-Tuning Approach for T5 Using Knowledge Graphs to Address Complex Tasks(https://arxiv.org/abs/2502.16484)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the development of deep learning technology, large language models have achieved remarkable results in many natural language processing tasks. However, these models still have certain limitations in handling complex reasoning tasks and understanding rich background knowledge. To solve this problem, this study proposed a T5 model fine-tuning method based on knowledge graphs, which enhances the model's reasoning ability and context understanding ability by introducing external knowledge graphs. We used the SQuAD1.1 dataset for experiments. The experimental results show that the T5 model based on knowledge graphs is significantly better than other baseline models in reasoning accuracy, context understanding, and the ability to handle complex problems. At the same time, we also explored the impact of knowledge graphs of different scales on model performance and found that as the scale of the knowledge graph increases, the performance of the model gradually improves. Especially when dealing with complex problems, the introduction of knowledge graphs greatly improves the reasoning ability of the T5 model. Ablation experiments further verify the importance of entity and relationship embedding in the model and prove that a complete knowledge graph is crucial to improving the various capabilities of the T5 model. In summary, this study provides an effective method to enhance the reasoning and understanding capabilities of large language models and provides new directions for future research.</li>
</ul>

<h3>Title: MQADet: A Plug-and-Play Paradigm for Enhancing Open-Vocabulary Object Detection via Multimodal Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Caixiong Li, Xiongwei Zhao, Jinhang Zhang, Xing Zhang, Zhou Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16486">https://arxiv.org/abs/2502.16486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16486">https://arxiv.org/pdf/2502.16486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16486]] MQADet: A Plug-and-Play Paradigm for Enhancing Open-Vocabulary Object Detection via Multimodal Question Answering(https://arxiv.org/abs/2502.16486)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Open-vocabulary detection (OVD) is a challenging task to detect and classify objects from an unrestricted set of categories, including those unseen during training. Existing open-vocabulary detectors are limited by complex visual-textual misalignment and long-tailed category imbalances, leading to suboptimal performance in challenging scenarios. To address these limitations, we introduce \textbf{MQADet}, a universal paradigm for enhancing existing open-vocabulary detectors by leveraging the cross-modal reasoning capabilities of multimodal large language models (MLLMs). MQADet functions as a plug-and-play solution that integrates seamlessly with pre-trained object detectors without substantial additional training costs. Specifically, we design a novel three-stage Multimodal Question Answering (MQA) pipeline to guide the MLLMs to precisely localize complex textual and visual targets while effectively enhancing the focus of existing object detectors on relevant objects. To validate our approach, we present a new benchmark for evaluating our paradigm on four challenging open-vocabulary datasets, employing three state-of-the-art object detectors as baselines. Experimental results demonstrate that our proposed paradigm significantly improves the performance of existing detectors, particularly in unseen complex categories, across diverse and challenging scenarios. To facilitate future research, we will publicly release our code.</li>
</ul>

<h3>Title: On Computational Limits of FlowAR Models: Expressivity and Efficiency</h3>
<ul>
<li><strong>Authors: </strong>Chengyue Gong, Yekun Ke, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CC, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16490">https://arxiv.org/abs/2502.16490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16490">https://arxiv.org/pdf/2502.16490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16490]] On Computational Limits of FlowAR Models: Expressivity and Efficiency(https://arxiv.org/abs/2502.16490)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The expressive power and computational complexity of deep visual generative models, such as flow-based and autoregressive (AR) models, have gained considerable interest for their wide-ranging applications in generative tasks. However, the theoretical characterization of their expressiveness through the lens of circuit complexity remains underexplored, particularly for the state-of-the-art architecture like FlowAR proposed by [Ren et al., 2024], which integrates flow-based and autoregressive mechanisms. This gap limits our understanding of their inherent computational limits and practical efficiency. In this study, we address this gap by analyzing the circuit complexity of the FlowAR architecture. We demonstrate that when the largest feature map produced by the FlowAR model has dimensions $n \times n \times c$, the FlowAR model is simulable by a family of threshold circuits $\mathsf{TC}^0$, which have constant depth $O(1)$ and polynomial width $\mathrm{poly}(n)$. This is the first study to rigorously highlight the limitations in the expressive power of FlowAR models. Furthermore, we identify the conditions under which the FlowAR model computations can achieve almost quadratic time. To validate our theoretical findings, we present efficient model variant constructions based on low-rank approximations that align with the derived criteria. Our work provides a foundation for future comparisons with other generative paradigms and guides the development of more efficient and expressive implementations.</li>
</ul>

<h3>Title: Intrinsic Model Weaknesses: How Priming Attacks Unveil Vulnerabilities in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuyi Huang, Runzhe Zhan, Derek F. Wong, Lidia S. Chao, Ailin Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16491">https://arxiv.org/abs/2502.16491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16491">https://arxiv.org/pdf/2502.16491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16491]] Intrinsic Model Weaknesses: How Priming Attacks Unveil Vulnerabilities in Large Language Models(https://arxiv.org/abs/2502.16491)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have significantly influenced various industries but suffer from a critical flaw, the potential sensitivity of generating harmful content, which poses severe societal risks. We developed and tested novel attack strategies on popular LLMs to expose their vulnerabilities in generating inappropriate content. These strategies, inspired by psychological phenomena such as the "Priming Effect", "Safe Attention Shift", and "Cognitive Dissonance", effectively attack the models' guarding mechanisms. Our experiments achieved an attack success rate (ASR) of 100% on various open-source models, including Meta's Llama-3.2, Google's Gemma-2, Mistral's Mistral-NeMo, Falcon's Falcon-mamba, Apple's DCLM, Microsoft's Phi3, and Qwen's Qwen2.5, among others. Similarly, for closed-source models such as OpenAI's GPT-4o, Google's Gemini-1.5, and Claude-3.5, we observed an ASR of at least 95% on the AdvBench dataset, which represents the current state-of-the-art. This study underscores the urgent need to reassess the use of generative models in critical applications to mitigate potential adverse societal impacts.</li>
</ul>

<h3>Title: PMAT: Optimizing Action Generation Order in Multi-Agent Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Kun Hu, Muning Wen, Xihuai Wang, Shao Zhang, Yiwei Shi, Minne Li, Minglong Li, Ying Wen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16496">https://arxiv.org/abs/2502.16496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16496">https://arxiv.org/pdf/2502.16496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16496]] PMAT: Optimizing Action Generation Order in Multi-Agent Reinforcement Learning(https://arxiv.org/abs/2502.16496)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multi-agent reinforcement learning (MARL) faces challenges in coordinating agents due to complex interdependencies within multi-agent systems. Most MARL algorithms use the simultaneous decision-making paradigm but ignore the action-level dependencies among agents, which reduces coordination efficiency. In contrast, the sequential decision-making paradigm provides finer-grained supervision for agent decision order, presenting the potential for handling dependencies via better decision order management. However, determining the optimal decision order remains a challenge. In this paper, we introduce Action Generation with Plackett-Luce Sampling (AGPS), a novel mechanism for agent decision order optimization. We model the order determination task as a Plackett-Luce sampling process to address issues such as ranking instability and vanishing gradient during the network training process. AGPS realizes credit-based decision order determination by establishing a bridge between the significance of agents' local observations and their decision credits, thus facilitating order optimization and dependency management. Integrating AGPS with the Multi-Agent Transformer, we propose the Prioritized Multi-Agent Transformer (PMAT), a sequential decision-making MARL algorithm with decision order optimization. Experiments on benchmarks including StarCraft II Multi-Agent Challenge, Google Research Football, and Multi-Agent MuJoCo show that PMAT outperforms state-of-the-art algorithms, greatly enhancing coordination efficiency.</li>
</ul>

<h3>Title: Subpixel Edge Localization Based on Converted Intensity Summation under Stable Edge Region</h3>
<ul>
<li><strong>Authors: </strong>Yingyuan Yang, Guoyuan Liang, Xianwen Wang, Kaiming Wang, Can Wang, Xiaojun Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16502">https://arxiv.org/abs/2502.16502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16502">https://arxiv.org/pdf/2502.16502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16502]] Subpixel Edge Localization Based on Converted Intensity Summation under Stable Edge Region(https://arxiv.org/abs/2502.16502)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>To satisfy the rigorous requirements of precise edge detection in critical high-accuracy measurements, this article proposes a series of efficient approaches for localizing subpixel edge. In contrast to the fitting based methods, which consider pixel intensity as a sample value derived from a specific model. We take an innovative perspective by assuming that the intensity at the pixel level can be interpreted as a local integral mapping in the intensity model for subpixel localization. Consequently, we propose a straightforward subpixel edge localization method called Converted Intensity Summation (CIS). To address the limited robustness associated with focusing solely on the localization of individual edge points, a Stable Edge Region (SER) based algorithm is presented to alleviate local interference near edges. Given the observation that the consistency of edge statistics exists in the local region, the algorithm seeks correlated stable regions in the vicinity of edges to facilitate the acquisition of robust parameters and achieve higher precision positioning. In addition, an edge complement method based on extension-adjustment is also introduced to rectify the irregular edges through the efficient migration of SERs. A large number of experiments are conducted on both synthetic and real image datasets which cover common edge patterns as well as various real scenarios such as industrial PCB images, remote sensing and medical images. It is verified that CIS can achieve higher accuracy than the state-of-the-art method, while requiring less execution time. Moreover, by integrating SER into CIS, the proposed algorithm demonstrates excellent performance in further improving the anti-interference capability and positioning accuracy.</li>
</ul>

<h3>Title: FanChuan: A Multilingual and Graph-Structured Benchmark For Parody Detection and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yilun Zheng, Sha Li, Fangkun Wu, Yang Ziyi, Lin Hongchao, Zhichao Hu, Cai Xinjun, Ziming Wang, Jinxuan Chen, Sitao Luan, Jiahao Xu, Lihui Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16503">https://arxiv.org/abs/2502.16503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16503">https://arxiv.org/pdf/2502.16503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16503]] FanChuan: A Multilingual and Graph-Structured Benchmark For Parody Detection and Analysis(https://arxiv.org/abs/2502.16503)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Parody is an emerging phenomenon on social media, where individuals imitate a role or position opposite to their own, often for humor, provocation, or controversy. Detecting and analyzing parody can be challenging and is often reliant on context, yet it plays a crucial role in understanding cultural values, promoting subcultures, and enhancing self-expression. However, the study of parody is hindered by limited available data and deficient diversity in current datasets. To bridge this gap, we built seven parody datasets from both English and Chinese corpora, with 14,755 annotated users and 21,210 annotated comments in total. To provide sufficient context information, we also collect replies and construct user-interaction graphs to provide richer contextual information, which is lacking in existing datasets. With these datasets, we test traditional methods and Large Language Models (LLMs) on three key tasks: (1) parody detection, (2) comment sentiment analysis with parody, and (3) user sentiment analysis with parody. Our extensive experiments reveal that parody-related tasks still remain challenging for all models, and contextual information plays a critical role. Interestingly, we find that, in certain scenarios, traditional sentence embedding methods combined with simple classifiers can outperform advanced LLMs, i.e. DeepSeek-R1 and GPT-o3, highlighting parody as a significant challenge for LLMs.</li>
</ul>

<h3>Title: GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking</h3>
<ul>
<li><strong>Authors: </strong>Yingjian Chen, Haoran Liu, Yinhong Liu, Rui Yang, Han Yuan, Yanran Fu, Pengyuan Zhou, Qingyu Chen, James Caverlee, Irene Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16514">https://arxiv.org/abs/2502.16514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16514">https://arxiv.org/pdf/2502.16514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16514]] GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking(https://arxiv.org/abs/2502.16514)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are widely used, but they often generate subtle factual errors, especially in long-form text. These errors are fatal in some specialized domains such as medicine. Existing fact-checking with grounding documents methods face two main challenges: (1) they struggle to understand complex multihop relations in long documents, often overlooking subtle factual errors; (2) most specialized methods rely on pairwise comparisons, requiring multiple model calls, leading to high resource and computational costs. To address these challenges, we propose \textbf{\textit{GraphCheck}}, a fact-checking framework that uses extracted knowledge graphs to enhance text representation. Graph Neural Networks further process these graphs as a soft prompt, enabling LLMs to incorporate structured knowledge more effectively. Enhanced with graph-based reasoning, GraphCheck captures multihop reasoning chains which are often overlooked by existing methods, enabling precise and efficient fact-checking in a single inference call. Experimental results on seven benchmarks spanning both general and medical domains demonstrate a 6.1\% overall improvement over baseline models. Notably, GraphCheck outperforms existing specialized fact-checkers and achieves comparable performance with state-of-the-art LLMs, such as DeepSeek-V3 and OpenAI-o1, with significantly fewer parameters.</li>
</ul>

<h3>Title: Guarding the Privacy of Label-Only Access to Neural Network Classifiers via iDP Verification</h3>
<ul>
<li><strong>Authors: </strong>Anan Kabaha, Dana Drachsler-Cohen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16519">https://arxiv.org/abs/2502.16519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16519">https://arxiv.org/pdf/2502.16519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16519]] Guarding the Privacy of Label-Only Access to Neural Network Classifiers via iDP Verification(https://arxiv.org/abs/2502.16519)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack</a></li>
<li><strong>Abstract: </strong>Neural networks are susceptible to privacy attacks that can extract private information of the training set. To cope, several training algorithms guarantee differential privacy (DP) by adding noise to their computation. However, DP requires to add noise considering every possible training set. This leads to a significant decrease in the network's accuracy. Individual DP (iDP) restricts DP to a given training set. We observe that some inputs deterministically satisfy iDP without any noise. By identifying them, we can provide iDP label-only access to the network with a minor decrease to its accuracy. However, identifying the inputs that satisfy iDP without any noise is highly challenging. Our key idea is to compute the iDP deterministic bound (iDP-DB), which overapproximates the set of inputs that do not satisfy iDP, and add noise only to their predicted labels. To compute the tightest iDP-DB, which enables to guard the label-only access with minimal accuracy decrease, we propose LUCID, which leverages several formal verification techniques. First, it encodes the problem as a mixed-integer linear program, defined over a network and over every network trained identically but without a unique data point. Second, it abstracts a set of networks using a hyper-network. Third, it eliminates the overapproximation error via a novel branch-and-bound technique. Fourth, it bounds the differences of matching neurons in the network and the hyper-network and employs linear relaxation if they are small. We show that LUCID can provide classifiers with a perfect individuals' privacy guarantee (0-iDP) -- which is infeasible for DP training algorithms -- with an accuracy decrease of 1.4%. For more relaxed $\varepsilon$-iDP guarantees, LUCID has an accuracy decrease of 1.2%. In contrast, existing DP training algorithms reduce the accuracy by 12.7%.</li>
</ul>

<h3>Title: Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Yulong Wu, Viktor Schlegel, Riza Batista-Navarro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16523">https://arxiv.org/abs/2502.16523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16523">https://arxiv.org/pdf/2502.16523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16523]] Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension(https://arxiv.org/abs/2502.16523)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>As neural language models achieve human-comparable performance on Machine Reading Comprehension (MRC) and see widespread adoption, ensuring their robustness in real-world scenarios has become increasingly important. Current robustness evaluation research, though, primarily develops synthetic perturbation methods, leaving unclear how well they reflect real life scenarios. Considering this, we present a framework to automatically examine MRC models on naturally occurring textual perturbations, by replacing paragraph in MRC benchmarks with their counterparts based on available Wikipedia edit history. Such perturbation type is natural as its design does not stem from an arteficial generative process, inherently distinct from the previously investigated synthetic approaches. In a large-scale study encompassing SQUAD datasets and various model architectures we observe that natural perturbations result in performance degradation in pre-trained encoder language models. More worryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs) inherit these errors. Further experiments demonstrate that our findings generalise to natural perturbations found in other more challenging MRC benchmarks. In an effort to mitigate these errors, we show that it is possible to improve the robustness to natural perturbations by training on naturally or synthetically perturbed examples, though a noticeable gap still remains compared to performance on unperturbed data.</li>
</ul>

<h3>Title: Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual Program Generation</h3>
<ul>
<li><strong>Authors: </strong>Deokhyung Kang, Jeonghun Cho, Yejin Jeon, Sunbin Jang, Minsub Lee, Jawoon Cho, Gary Geunbae Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16529">https://arxiv.org/abs/2502.16529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16529">https://arxiv.org/pdf/2502.16529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16529]] Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual Program Generation(https://arxiv.org/abs/2502.16529)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Visual programming languages (VPLs) allow users to create programs through graphical interfaces, which results in easier accessibility and their widespread usage in various domains. To further enhance this accessibility, recent research has focused on generating VPL code from user instructions using large language models (LLMs). Specifically, by employing prompting-based methods, these studies have shown promising results. Nevertheless, such approaches can be less effective for industrial VPLs such as Ladder Diagram (LD). LD is a pivotal language used in industrial automation processes and involves extensive domain-specific configurations, which are difficult to capture in a single prompt. In this work, we demonstrate that training-based methods outperform prompting-based methods for LD generation accuracy, even with smaller backbone models. Building on these findings, we propose a two-stage training strategy to further enhance VPL generation. First, we employ retrieval-augmented fine-tuning to leverage the repetitive use of subroutines commonly seen in industrial VPLs. Second, we apply direct preference optimization (DPO) to further guide the model toward accurate outputs, using systematically generated preference pairs through graph editing operations. Extensive experiments on real-world LD data demonstrate that our approach improves program-level accuracy by over 10% compared to supervised fine-tuning, which highlights its potential to advance industrial automation.</li>
</ul>

<h3>Title: A Survey of Graph Transformers: Architectures, Theories and Applications</h3>
<ul>
<li><strong>Authors: </strong>Chaohao Yuan, Kangfei Zhao, Ercan Engin Kuruoglu, Liang Wang, Tingyang Xu, Wenbing Huang, Deli Zhao, Hong Cheng, Yu Rong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16533">https://arxiv.org/abs/2502.16533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16533">https://arxiv.org/pdf/2502.16533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16533]] A Survey of Graph Transformers: Architectures, Theories and Applications(https://arxiv.org/abs/2502.16533)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, transformer</a></li>
<li><strong>Abstract: </strong>Graph Transformers (GTs) have demonstrated a strong capability in modeling graph structures by addressing the intrinsic limitations of graph neural networks (GNNs), such as over-smoothing and over-squashing. Recent studies have proposed diverse architectures, enhanced explainability, and practical applications for Graph Transformers. In light of these rapid developments, we conduct a comprehensive review of Graph Transformers, covering aspects such as their architectures, theoretical foundations, and applications within this survey. We categorize the architecture of Graph Transformers according to their strategies for processing structural information, including graph tokenization, positional encoding, structure-aware attention and model ensemble. Furthermore, from the theoretical perspective, we examine the expressivity of Graph Transformers in various discussed architectures and contrast them with other advanced graph learning algorithms to discover the connections. Furthermore, we provide a summary of the practical applications where Graph Transformers have been utilized, such as molecule, protein, language, vision traffic, brain and material data. At the end of this survey, we will discuss the current challenges and prospective directions in Graph Transformers for potential future research.</li>
</ul>

<h3>Title: Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Rystrøm, Hannah Rose Kirk, Scott Hale</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16534">https://arxiv.org/abs/2502.16534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16534">https://arxiv.org/pdf/2502.16534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16534]] Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs(https://arxiv.org/abs/2502.16534)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are becoming increasingly capable across global languages. However, the ability to communicate across languages does not necessarily translate to appropriate cultural representations. A key concern is US-centric bias, where LLMs reflect US rather than local cultural values. We propose a novel methodology that compares LLM-generated response distributions against population-level opinion data from the World Value Survey across four languages (Danish, Dutch, English, and Portuguese). Using a rigorous linear mixed-effects regression framework, we compare two families of models: Google's Gemma models (2B--27B parameters) and successive iterations of OpenAI's turbo-series. Across the families of models, we find no consistent relationships between language capabilities and cultural alignment. While the Gemma models have a positive correlation between language capability and cultural alignment across languages, the OpenAI models do not. Importantly, we find that self-consistency is a stronger predictor of multicultural alignment than multilingual capabilities. Our results demonstrate that achieving meaningful cultural alignment requires dedicated effort beyond improving general language capabilities.</li>
</ul>

<h3>Title: Advanced Chain-of-Thought Reasoning for Parameter Extraction from Documents Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hong Cai Chen, Yi Pin Xu, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.AR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16540">https://arxiv.org/abs/2502.16540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16540">https://arxiv.org/pdf/2502.16540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16540]] Advanced Chain-of-Thought Reasoning for Parameter Extraction from Documents Using Large Language Models(https://arxiv.org/abs/2502.16540)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Extracting parameters from technical documentation is crucial for ensuring design precision and simulation reliability in electronic design. However, current methods struggle to handle high-dimensional design data and meet the demands of real-time processing. In electronic design automation (EDA), engineers often manually search through extensive documents to retrieve component parameters required for constructing PySpice models, a process that is both labor-intensive and time-consuming. To address this challenge, we propose an innovative framework that leverages large language models (LLMs) to automate the extraction of parameters and the generation of PySpice models directly from datasheets. Our framework introduces three Chain-of-Thought (CoT) based techniques: (1) Targeted Document Retrieval (TDR), which enables the rapid identification of relevant technical sections; (2) Iterative Retrieval Optimization (IRO), which refines the parameter search through iterative improvements; and (3) Preference Optimization (PO), which dynamically prioritizes key document sections based on relevance. Experimental results show that applying all three methods together improves retrieval precision by 47.69% and reduces processing latency by 37.84%. Furthermore, effect size analysis using Cohen's d reveals that PO significantly reduces latency, while IRO contributes most to precision enhancement. These findings underscore the potential of our framework to streamline EDA processes, enhance design accuracy, and shorten development timelines. Additionally, our algorithm has model-agnostic generalization, meaning it can improve parameter search performance across different LLMs.</li>
</ul>

<h3>Title: Multi-Target Federated Backdoor Attack Based on Feature Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Lingguag Hao, Kuangrong Hao, Bing Wei, Xue-song Tang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16545">https://arxiv.org/abs/2502.16545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16545">https://arxiv.org/pdf/2502.16545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16545]] Multi-Target Federated Backdoor Attack Based on Feature Aggregation(https://arxiv.org/abs/2502.16545)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, federate</a></li>
<li><strong>Abstract: </strong>Current federated backdoor attacks focus on collaboratively training backdoor triggers, where multiple compromised clients train their local trigger patches and then merge them into a global trigger during the inference phase. However, these methods require careful design of the shape and position of trigger patches and lack the feature interactions between trigger patches during training, resulting in poor backdoor attack success rates. Moreover, the pixels of the patches remain untruncated, thereby making abrupt areas in backdoor examples easily detectable by the detection algorithm. To this end, we propose a novel benchmark for the federated backdoor attack based on feature aggregation. Specifically, we align the dimensions of triggers with images, delimit the trigger's pixel boundaries, and facilitate feature interaction among local triggers trained by each compromised client. Furthermore, leveraging the intra-class attack strategy, we propose the simultaneous generation of backdoor triggers for all target classes, significantly reducing the overall production time for triggers across all target classes and increasing the risk of the federated model being attacked. Experiments demonstrate that our method can not only bypass the detection of defense methods while patch-based methods fail, but also achieve a zero-shot backdoor attack with a success rate of 77.39%. To the best of our knowledge, our work is the first to implement such a zero-shot attack in federated learning. Finally, we evaluate attack performance by varying the trigger's training factors, including poison location, ratio, pixel bound, and trigger training duration (local epochs and communication rounds).</li>
</ul>

<h3>Title: Composable Strategy Framework with Integrated Video-Text based Large Language Models for Heart Failure Assessment</h3>
<ul>
<li><strong>Authors: </strong>Jianzhou Chen, Xiumei Wang, Jinyang Sun, Xi Chen, Heyu Chu, Guo Song, Yuji Luo, Xingping Zhou, Rong Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16548">https://arxiv.org/abs/2502.16548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16548">https://arxiv.org/pdf/2502.16548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16548]] Composable Strategy Framework with Integrated Video-Text based Large Language Models for Heart Failure Assessment(https://arxiv.org/abs/2502.16548)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Heart failure is one of the leading causes of death worldwide, with millons of deaths each year, according to data from the World Health Organization (WHO) and other public health agencies. While significant progress has been made in the field of heart failure, leading to improved survival rates and improvement of ejection fraction, there remains substantial unmet needs, due to the complexity and multifactorial characteristics. Therefore, we propose a composable strategy framework for assessment and treatment optimization in heart failure. This framework simulates the doctor-patient consultation process and leverages multi-modal algorithms to analyze a range of data, including video, physical examination, text results as well as medical history. By integrating these various data sources, our framework offers a more holistic evaluation and optimized treatment plan for patients. Our results demonstrate that this multi-modal approach outperforms single-modal artificial intelligence (AI) algorithms in terms of accuracy in heart failure (HF) prognosis prediction. Through this method, we can further evaluate the impact of various pathological indicators on HF prognosis,providing a more comprehensive evaluation.</li>
</ul>

<h3>Title: Beyond Words: How Large Language Models Perform in Quantitative Management Problem-Solving</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Kuzmanko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16556">https://arxiv.org/abs/2502.16556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16556">https://arxiv.org/pdf/2502.16556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16556]] Beyond Words: How Large Language Models Perform in Quantitative Management Problem-Solving(https://arxiv.org/abs/2502.16556)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study examines how Large Language Models (LLMs) perform when tackling quantitative management decision problems in a zero-shot setting. Drawing on 900 responses generated by five leading models across 20 diverse managerial scenarios, our analysis explores whether these base models can deliver accurate numerical decisions under varying presentation formats, scenario complexities, and repeated attempts. Contrary to prior findings, we observed no significant effects of text presentation format (direct, narrative, or tabular) or text length on accuracy. However, scenario complexity -- particularly in terms of constraints and irrelevant parameters -- strongly influenced performance, often degrading accuracy. Surprisingly, the models handled tasks requiring multiple solution steps more effectively than expected. Notably, only 28.8\% of responses were exactly correct, highlighting limitations in precision. We further found no significant ``learning effect'' across iterations: performance remained stable across repeated queries. Nonetheless, significant variations emerged among the five tested LLMs, with some showing superior binary accuracy. Overall, these findings underscore both the promise and the pitfalls of harnessing LLMs for complex quantitative decision-making, informing managers and researchers about optimal deployment strategies.</li>
</ul>

<h3>Title: Entropy-Lens: The Information Signature of Transformer Computations</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Ali, Francesco Caso, Christopher Irwin, Pietro Liò</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16570">https://arxiv.org/abs/2502.16570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16570">https://arxiv.org/pdf/2502.16570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16570]] Entropy-Lens: The Information Signature of Transformer Computations(https://arxiv.org/abs/2502.16570)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Transformer models have revolutionized fields from natural language processing to computer vision, yet their internal computational dynamics remain poorly understood raising concerns about predictability and robustness. In this work, we introduce Entropy-Lens, a scalable, model-agnostic framework that leverages information theory to interpret frozen, off-the-shelf large-scale transformers. By quantifying the evolution of Shannon entropy within intermediate residual streams, our approach extracts computational signatures that distinguish model families, categorize task-specific prompts, and correlate with output accuracy. We further demonstrate the generality of our method by extending the analysis to vision transformers. Our results suggest that entropy-based metrics can serve as a principled tool for unveiling the inner workings of modern transformer architectures.</li>
</ul>

<h3>Title: Can Indirect Prompt Injection Attacks Be Detected and Removed?</h3>
<ul>
<li><strong>Authors: </strong>Yulin Chen, Haoran Li, Yuan Sui, Yufei He, Yue Liu, Yangqiu Song, Bryan Hooi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16580">https://arxiv.org/abs/2502.16580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16580">https://arxiv.org/pdf/2502.16580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16580]] Can Indirect Prompt Injection Attacks Be Detected and Removed?(https://arxiv.org/abs/2502.16580)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, extraction, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Prompt injection attacks manipulate large language models (LLMs) by misleading them to deviate from the original input instructions and execute maliciously injected instructions, because of their instruction-following capabilities and inability to distinguish between the original input instructions and maliciously injected instructions. To defend against such attacks, recent studies have developed various detection mechanisms. While significant efforts have focused on detecting direct prompt injection attacks, where injected instructions are directly from the attacker who is also the user, limited attention has been given to indirect prompt injection attacks, where injected instructions are indirectly from external tools, such as a search engine. Moreover, current works mainly investigate injection detection methods and pay less attention to the post-processing method that aims to mitigate the injection after detection. In this paper, we investigate the feasibility of detecting and removing indirect prompt injection attacks, and we construct a benchmark dataset for evaluation. For detection, we assess the performance of existing LLMs and open-source detection models, and we further train detection models using our crafted training datasets. For removal, we evaluate two intuitive methods: (1) the segmentation removal method, which segments the injected document and removes parts containing injected instructions, and (2) the extraction removal method, which trains an extraction model to identify and remove injected instructions.</li>
</ul>

<h3>Title: Multimodal Large Language Models for Text-rich Image Understanding: A Comprehensive Review</h3>
<ul>
<li><strong>Authors: </strong>Pei Fu, Tongkun Guan, Zining Wang, Zhentao Guo, Chen Duan, Hao Sun, Boming Chen, Jiayao Ma, Qianyi Jiang, Kai Zhou, Junfeng Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16586">https://arxiv.org/abs/2502.16586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16586">https://arxiv.org/pdf/2502.16586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16586]] Multimodal Large Language Models for Text-rich Image Understanding: A Comprehensive Review(https://arxiv.org/abs/2502.16586)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The recent emergence of Multi-modal Large Language Models (MLLMs) has introduced a new dimension to the Text-rich Image Understanding (TIU) field, with models demonstrating impressive and inspiring performance. However, their rapid evolution and widespread adoption have made it increasingly challenging to keep up with the latest advancements. To address this, we present a systematic and comprehensive survey to facilitate further research on TIU MLLMs. Initially, we outline the timeline, architecture, and pipeline of nearly all TIU MLLMs. Then, we review the performance of selected models on mainstream benchmarks. Finally, we explore promising directions, challenges, and limitations within the field.</li>
</ul>

<h3>Title: Co-MTP: A Cooperative Trajectory Prediction Framework with Multi-Temporal Fusion for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Zhang, Zewei Zhou, Zhaoyi Wang, Yangjie Ji, Yanjun Huang, Hong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16589">https://arxiv.org/abs/2502.16589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16589">https://arxiv.org/pdf/2502.16589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16589]] Co-MTP: A Cooperative Trajectory Prediction Framework with Multi-Temporal Fusion for Autonomous Driving(https://arxiv.org/abs/2502.16589)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vehicle-to-everything technologies (V2X) have become an ideal paradigm to extend the perception range and see through the occlusion. Exiting efforts focus on single-frame cooperative perception, however, how to capture the temporal cue between frames with V2X to facilitate the prediction task even the planning task is still underexplored. In this paper, we introduce the Co-MTP, a general cooperative trajectory prediction framework with multi-temporal fusion for autonomous driving, which leverages the V2X system to fully capture the interaction among agents in both history and future domains to benefit the planning. In the history domain, V2X can complement the incomplete history trajectory in single-vehicle perception, and we design a heterogeneous graph transformer to learn the fusion of the history feature from multiple agents and capture the history interaction. Moreover, the goal of prediction is to support future planning. Thus, in the future domain, V2X can provide the prediction results of surrounding objects, and we further extend the graph transformer to capture the future interaction among the ego planning and the other vehicles' intentions and obtain the final future scenario state under a certain planning action. We evaluate the Co-MTP framework on the real-world dataset V2X-Seq, and the results show that Co-MTP achieves state-of-the-art performance and that both history and future fusion can greatly benefit prediction.</li>
</ul>

<h3>Title: Revealing the Pragmatic Dilemma for Moral Reasoning Acquisition in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guangliang Liu, Lei Jiang, Xitong Zhang, Kristen Marie Johnson</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16600">https://arxiv.org/abs/2502.16600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16600">https://arxiv.org/pdf/2502.16600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16600]] Revealing the Pragmatic Dilemma for Moral Reasoning Acquisition in Language Models(https://arxiv.org/abs/2502.16600)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Ensuring that Large Language Models (LLMs) return just responses which adhere to societal values is crucial for their broader application. Prior research has shown that LLMs often fail to perform satisfactorily on tasks requiring moral cognizance, such as ethics-based judgments. While current approaches have focused on fine-tuning LLMs with curated datasets to improve their capabilities on such tasks, choosing the optimal learning paradigm to enhance the ethical responses of LLMs remains an open research debate. In this work, we aim to address this fundamental question: can current learning paradigms enable LLMs to acquire sufficient moral reasoning capabilities? Drawing from distributional semantics theory and the pragmatic nature of moral discourse, our analysis indicates that performance improvements follow a mechanism similar to that of semantic-level tasks, and therefore remain affected by the pragmatic nature of morals latent in discourse, a phenomenon we name the pragmatic dilemma. We conclude that this pragmatic dilemma imposes significant limitations on the generalization ability of current learning paradigms, making it the primary bottleneck for moral reasoning acquisition in LLMs.</li>
</ul>

<h3>Title: SelaVPR++: Towards Seamless Adaptation of Foundation Models for Efficient Place Recognition</h3>
<ul>
<li><strong>Authors: </strong>Feng Lu, Tong Jin, Xiangyuan Lan, Lijun Zhang, Yunpeng Liu, Yaowei Wang, Chun Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16601">https://arxiv.org/abs/2502.16601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16601">https://arxiv.org/pdf/2502.16601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16601]] SelaVPR++: Towards Seamless Adaptation of Foundation Models for Efficient Place Recognition(https://arxiv.org/abs/2502.16601)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent studies show that the visual place recognition (VPR) method using pre-trained visual foundation models can achieve promising performance. In our previous work, we propose a novel method to realize seamless adaptation of foundation models to VPR (SelaVPR). This method can produce both global and local features that focus on discriminative landmarks to recognize places for two-stage VPR by a parameter-efficient adaptation approach. Although SelaVPR has achieved competitive results, we argue that the previous adaptation is inefficient in training time and GPU memory usage, and the re-ranking paradigm is also costly in retrieval latency and storage usage. In pursuit of higher efficiency and better performance, we propose an extension of the SelaVPR, called SelaVPR++. Concretely, we first design a parameter-, time-, and memory-efficient adaptation method that uses lightweight multi-scale convolution (MultiConv) adapters to refine intermediate features from the frozen foundation backbone. This adaptation method does not back-propagate gradients through the backbone during training, and the MultiConv adapter facilitates feature interactions along the spatial axes and introduces proper local priors, thus achieving higher efficiency and better performance. Moreover, we propose an innovative re-ranking paradigm for more efficient VPR. Instead of relying on local features for re-ranking, which incurs huge overhead in latency and storage, we employ compact binary features for initial retrieval and robust floating-point (global) features for re-ranking. To obtain such binary features, we propose a similarity-constrained deep hashing method, which can be easily integrated into the VPR pipeline. Finally, we improve our training strategy and unify the training protocol of several common training datasets to merge them for better training of VPR models. Extensive experiments show that ......</li>
</ul>

<h3>Title: AdverX-Ray: Ensuring X-Ray Integrity Through Frequency-Sensitive Adversarial VAEs</h3>
<ul>
<li><strong>Authors: </strong>Francisco Caetano, Christiaan Viviers, Lena Filatova, Peter H. N. de With, Fons van der Sommen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16610">https://arxiv.org/abs/2502.16610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16610">https://arxiv.org/pdf/2502.16610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16610]] AdverX-Ray: Ensuring X-Ray Integrity Through Frequency-Sensitive Adversarial VAEs(https://arxiv.org/abs/2502.16610)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Ensuring the quality and integrity of medical images is crucial for maintaining diagnostic accuracy in deep learning-based Computer-Aided Diagnosis and Computer-Aided Detection (CAD) systems. Covariate shifts are subtle variations in the data distribution caused by different imaging devices or settings and can severely degrade model performance, similar to the effects of adversarial attacks. Therefore, it is vital to have a lightweight and fast method to assess the quality of these images prior to using CAD models. AdverX-Ray addresses this need by serving as an image-quality assessment layer, designed to detect covariate shifts effectively. This Adversarial Variational Autoencoder prioritizes the discriminator's role, using the suboptimal outputs of the generator as negative samples to fine-tune the discriminator's ability to identify high-frequency artifacts. Images generated by adversarial networks often exhibit severe high-frequency artifacts, guiding the discriminator to focus excessively on these components. This makes the discriminator ideal for this approach. Trained on patches from X-ray images of specific machine models, AdverX-Ray can evaluate whether a scan matches the training distribution, or if a scan from the same machine is captured under different settings. Extensive comparisons with various OOD detection methods show that AdverX-Ray significantly outperforms existing techniques, achieving a 96.2% average AUROC using only 64 random patches from an X-ray. Its lightweight and fast architecture makes it suitable for real-time applications, enhancing the reliability of medical imaging systems. The code and pretrained models are publicly available.</li>
</ul>

<h3>Title: CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alexander Zhang, Marcus Dong, Jiaheng Liu, Wei Zhang, Yejie Wang, Jian Yang, Ge Zhang, Tianyu Liu, Zhongyuan Peng, Yingshui Tan, Yuanxing Zhang, Zhexu Wang, Weixun Wang, Yancheng He, Ken Deng, Wangchunshu Zhou, Wenhao Huang, Zhaoxiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16614">https://arxiv.org/abs/2502.16614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16614">https://arxiv.org/pdf/2502.16614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16614]] CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models(https://arxiv.org/abs/2502.16614)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The critique capacity of Large Language Models (LLMs) is essential for reasoning abilities, which can provide necessary suggestions (e.g., detailed analysis and constructive feedback). Therefore, how to evaluate the critique capacity of LLMs has drawn great attention and several critique benchmarks have been proposed. However, existing critique benchmarks usually have the following limitations: (1). Focusing on diverse reasoning tasks in general domains and insufficient evaluation on code tasks (e.g., only covering code generation task), where the difficulty of queries is relatively easy (e.g., the code queries of CriticBench are from Humaneval and MBPP). (2). Lacking comprehensive evaluation from different dimensions. To address these limitations, we introduce a holistic code critique benchmark for LLMs called CodeCriticBench. Specifically, our CodeCriticBench includes two mainstream code tasks (i.e., code generation and code QA) with different difficulties. Besides, the evaluation protocols include basic critique evaluation and advanced critique evaluation for different characteristics, where fine-grained evaluation checklists are well-designed for advanced settings. Finally, we conduct extensive experimental results of existing LLMs, which show the effectiveness of CodeCriticBench.</li>
</ul>

<h3>Title: Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?</h3>
<ul>
<li><strong>Authors: </strong>Qipan Xu, Zhenting Wang, Xiaoxiao He, Ligong Han, Ruixiang Tang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16618">https://arxiv.org/abs/2502.16618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16618">https://arxiv.org/pdf/2502.16618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16618]] Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?(https://arxiv.org/abs/2502.16618)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, generative</a></li>
<li><strong>Abstract: </strong>Generative AI models, renowned for their ability to synthesize high-quality content, have sparked growing concerns over the improper generation of copyright-protected material. While recent studies have proposed various approaches to address copyright issues, the capability of large vision-language models (LVLMs) to detect copyright infringements remains largely unexplored. In this work, we focus on evaluating the copyright detection abilities of state-of-the-art LVLMs using a various set of image samples. Recognizing the absence of a comprehensive dataset that includes both IP-infringement samples and ambiguous non-infringement negative samples, we construct a benchmark dataset comprising positive samples that violate the copyright protection of well-known IP figures, as well as negative samples that resemble these figures but do not raise copyright concerns. This dataset is created using advanced prompt engineering techniques. We then evaluate leading LVLMs using our benchmark dataset. Our experimental results reveal that LVLMs are prone to overfitting, leading to the misclassification of some negative samples as IP-infringement cases. In the final section, we analyze these failure cases and propose potential solutions to mitigate the overfitting problem.</li>
</ul>

<h3>Title: Energy-Efficient Transformer Inference: Optimization Strategies for Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Arshia Kermani, Ehsan Zeraatkar, Habib Irani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16627">https://arxiv.org/abs/2502.16627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16627">https://arxiv.org/pdf/2502.16627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16627]] Energy-Efficient Transformer Inference: Optimization Strategies for Time Series Classification(https://arxiv.org/abs/2502.16627)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The increasing computational demands of transformer models in time series classification necessitate effective optimization strategies for energy-efficient deployment. This paper presents a systematic investigation of optimization techniques, focusing on structured pruning and quantization methods for transformer architectures. Through extensive experimentation on three distinct datasets (RefrigerationDevices, ElectricDevices, and PLAID), we quantitatively evaluate model performance and energy efficiency across different transformer configurations. Our experimental results demonstrate that static quantization reduces energy consumption by 29.14% while maintaining classification performance, and L1 pruning achieves a 1.63% improvement in inference speed with minimal accuracy degradation. These findings provide valuable insights into the effectiveness of optimization strategies for transformer-based time series classification, establishing a foundation for efficient model deployment in resource-constrained environments.</li>
</ul>

<h3>Title: Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation for Visual Knowledge Intensive Queries</h3>
<ul>
<li><strong>Authors: </strong>Yin Wu, Quanyu Long, Jing Li, Jianfei Yu, Wenya Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16636">https://arxiv.org/abs/2502.16636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16636">https://arxiv.org/pdf/2502.16636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16636]] Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation for Visual Knowledge Intensive Queries(https://arxiv.org/abs/2502.16636)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) is a popular approach for enhancing Large Language Models (LLMs) by addressing their limitations in verifying facts and answering knowledge-intensive questions. As the research in LLM extends their capability to handle input modality other than text, e.g. image, several multimodal RAG benchmarks are proposed. Nonetheless, they mainly use textual knowledge bases as the primary source of evidences for augmentation. There still lack benchmarks designed to evaluate images as augmentation in RAG systems and how they leverage visual knowledge. We propose Visual-RAG, a novel Question Answering benchmark that emphasizes visual knowledge intensive questions. Unlike prior works relying on text-based evidence, Visual-RAG necessitates text-to-image retrieval and integration of relevant clue images to extract visual knowledge as evidence. With Visual-RAG, we evaluate 5 open-sourced and 3 proprietary Multimodal LLMs (MLLMs), revealing that images can serve as good evidence in RAG; however, even the SoTA models struggle with effectively extracting and utilizing visual knowledge</li>
</ul>

<h3>Title: Automatic Joint Structured Pruning and Quantization for Efficient Neural Network Training and Compression</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyi Qu, David Aponte, Colby Banbury, Daniel P. Robinson, Tianyu Ding, Kazuhito Koishida, Ilya Zharkov, Tianyi Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16638">https://arxiv.org/abs/2502.16638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16638">https://arxiv.org/pdf/2502.16638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16638]] Automatic Joint Structured Pruning and Quantization for Efficient Neural Network Training and Compression(https://arxiv.org/abs/2502.16638)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Structured pruning and quantization are fundamental techniques used to reduce the size of deep neural networks (DNNs) and typically are applied independently. Applying these techniques jointly via co-optimization has the potential to produce smaller, high-quality models. However, existing joint schemes are not widely used because of (1) engineering difficulties (complicated multi-stage processes), (2) black-box optimization (extensive hyperparameter tuning to control the overall compression), and (3) insufficient architecture generalization. To address these limitations, we present the framework GETA, which automatically and efficiently performs joint structured pruning and quantization-aware training on any DNNs. GETA introduces three key innovations: (i) a quantization-aware dependency graph (QADG) that constructs a pruning search space for generic quantization-aware DNN, (ii) a partially projected stochastic gradient method that guarantees layerwise bit constraints are satisfied, and (iii) a new joint learning strategy that incorporates interpretable relationships between pruning and quantization. We present numerical experiments on both convolutional neural networks and transformer architectures that show that our approach achieves competitive (often superior) performance compared to existing joint pruning and quantization methods.</li>
</ul>

<h3>Title: Retrieval-Augmented Visual Question Answering via Built-in Autoregressive Search Engines</h3>
<ul>
<li><strong>Authors: </strong>Xinwei Long, Zhiyuan Ma, Ermo Hua, Kaiyan Zhang, Biqing Qi, Bowen Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16641">https://arxiv.org/abs/2502.16641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16641">https://arxiv.org/pdf/2502.16641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16641]] Retrieval-Augmented Visual Question Answering via Built-in Autoregressive Search Engines(https://arxiv.org/abs/2502.16641)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) has emerged to address the knowledge-intensive visual question answering (VQA) task. Current methods mainly employ separate retrieval and generation modules to acquire external knowledge and generate answers, respectively. We propose ReAuSE, an alternative to the previous RAG model for the knowledge-based VQA task, which seamlessly integrates knowledge retriever into the generative multi-modal large language model, serving as a built-in search engine. Specifically, our model functions both as a generative retriever and an accurate answer generator. It not only helps retrieve documents from the knowledge base by producing identifiers for each document, but it also answers visual questions based on the retrieved documents. Furthermore, we propose a reinforced retrieval calibration module from relevance feedback to improve retrieval performance and align with the preferences for accurate answer generation. Extensive experiments on two representative OKVQA and A-OKVQA datasets demonstrate significant improvements ranging from 2.9\% to 9.6\% across all evaluation metrics when compared to strong baselines.</li>
</ul>

<h3>Title: CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale</h3>
<ul>
<li><strong>Authors: </strong>Chenlong Wang, Zhaoyang Chu, Zhengxiang Cheng, Xuyi Yang, Kaiyue Qiu, Yao Wan, Zhou Zhao, Xuanhua Shi, Dongping Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16645">https://arxiv.org/abs/2502.16645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16645">https://arxiv.org/pdf/2502.16645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16645]] CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale(https://arxiv.org/abs/2502.16645)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: this https URL.</li>
</ul>

<h3>Title: Few-shot Continual Relation Extraction via Open Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Thiem Nguyen, Anh Nguyen, Quyen Tran, Tu Vu, Diep Nguyen, Linh Ngo, Thien Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16648">https://arxiv.org/abs/2502.16648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16648">https://arxiv.org/pdf/2502.16648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16648]] Few-shot Continual Relation Extraction via Open Information Extraction(https://arxiv.org/abs/2502.16648)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Typically, Few-shot Continual Relation Extraction (FCRE) models must balance retaining prior knowledge while adapting to new tasks with extremely limited data. However, real-world scenarios may also involve unseen or undetermined relations that existing methods still struggle to handle. To address these challenges, we propose a novel approach that leverages the Open Information Extraction concept of Knowledge Graph Construction (KGC). Our method not only exposes models to all possible pairs of relations, including determined and undetermined labels not available in the training set, but also enriches model knowledge with diverse relation descriptions, thereby enhancing knowledge retention and adaptability in this challenging scenario. In the perspective of KGC, this is the first work explored in the setting of Continual Learning, allowing efficient expansion of the graph as the data evolves. Experimental results demonstrate our superior performance compared to other state-of-the-art FCRE baselines, as well as the efficiency in handling dynamic graph construction in this setting.</li>
</ul>

<h3>Title: Security Analysis of 5G NR Device-to-Device Sidelink Communications</h3>
<ul>
<li><strong>Authors: </strong>Evangelos Bitsikas, Aanjhan Ranganathan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16650">https://arxiv.org/abs/2502.16650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16650">https://arxiv.org/pdf/2502.16650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16650]] Security Analysis of 5G NR Device-to-Device Sidelink Communications(https://arxiv.org/abs/2502.16650)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>5G NR sidelink communication enables new possibilities for direct device-to-device interactions, supporting applications from vehicle-to-everything (V2X) systems to public safety, industrial automation, and drone networks. However, these advancements come with significant security challenges due to the decentralized trust model and increased reliance on User Equipment (UE) for critical functions like synchronization, resource allocation, and authorization. This paper presents the first comprehensive security analysis of NR V2X sidelink. We identify vulnerabilities across critical procedures and demonstrate plausible attack, including attacks that manipulate data integrity feedback and block resources, ultimately undermining the reliability and privacy of sidelink communications. Our analysis reveals that NR operational modes are vulnerable, with the ones relying on autonomous resource management (without network supervision) particularly exposed. To address these issues, we propose mitigation strategies to enhance the security of 5G sidelink communications. This work establishes a foundation for future efforts to strengthen 5G device-to-device sidelink communications, ensuring its safe deployment in critical applications.</li>
</ul>

<h3>Title: Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language Embedding Registration</h3>
<ul>
<li><strong>Authors: </strong>Kim Jun-Seong, GeonU Kim, Kim Yu-Ji, Yu-Chiang Frank Wang, Jaesung Choe, Tae-Hyun Oh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16652">https://arxiv.org/abs/2502.16652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16652">https://arxiv.org/pdf/2502.16652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16652]] Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language Embedding Registration(https://arxiv.org/abs/2502.16652)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We introduce Dr. Splat, a novel approach for open-vocabulary 3D scene understanding leveraging 3D Gaussian Splatting. Unlike existing language-embedded 3DGS methods, which rely on a rendering process, our method directly associates language-aligned CLIP embeddings with 3D Gaussians for holistic 3D scene understanding. The key of our method is a language feature registration technique where CLIP embeddings are assigned to the dominant Gaussians intersected by each pixel-ray. Moreover, we integrate Product Quantization (PQ) trained on general large-scale image data to compactly represent embeddings without per-scene optimization. Experiments demonstrate that our approach significantly outperforms existing approaches in 3D perception benchmarks, such as open-vocabulary 3D semantic segmentation, 3D object localization, and 3D object selection tasks. For video results, please visit : this https URL</li>
</ul>

<h3>Title: VPNeXt -- Rethinking Dense Decoding for Plain Vision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Xikai Tang, Ye Huang, Guangqiang Yin, Lixin Duan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16654">https://arxiv.org/abs/2502.16654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16654">https://arxiv.org/pdf/2502.16654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16654]] VPNeXt -- Rethinking Dense Decoding for Plain Vision Transformer(https://arxiv.org/abs/2502.16654)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>We present VPNeXt, a new and simple model for the Plain Vision Transformer (ViT). Unlike the many related studies that share the same homogeneous paradigms, VPNeXt offers a fresh perspective on dense representation based on ViT. In more detail, the proposed VPNeXt addressed two concerns about the existing paradigm: (1) Is it necessary to use a complex Transformer Mask Decoder architecture to obtain good representations? (2) Does the Plain ViT really need to depend on the mock pyramid feature for upsampling? For (1), we investigated the potential underlying reasons that contributed to the effectiveness of the Transformer Decoder and introduced the Visual Context Replay (VCR) to achieve similar effects efficiently. For (2), we introduced the ViTUp module. This module fully utilizes the previously overlooked ViT real pyramid feature to achieve better upsampling results compared to the earlier mock pyramid feature. This represents the first instance of such functionality in the field of semantic segmentation for Plain ViT. We performed ablation studies on related modules to verify their effectiveness gradually. We conducted relevant comparative experiments and visualizations to show that VPNeXt achieved state-of-the-art performance with a simple and effective design. Moreover, the proposed VPNeXt significantly exceeded the long-established mIoU wall/barrier of the VOC2012 dataset, setting a new state-of-the-art by a large margin, which also stands as the largest improvement since 2015.</li>
</ul>

<h3>Title: BioMaze: Benchmarking and Enhancing Large Language Models for Biological Pathway Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Haiteng Zhao, Chang Ma, FangZhi Xu, Lingpeng Kong, Zhi-Hong Deng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16660">https://arxiv.org/abs/2502.16660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16660">https://arxiv.org/pdf/2502.16660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16660]] BioMaze: Benchmarking and Enhancing Large Language Models for Biological Pathway Reasoning(https://arxiv.org/abs/2502.16660)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The applications of large language models (LLMs) in various biological domains have been explored recently, but their reasoning ability in complex biological systems, such as pathways, remains underexplored, which is crucial for predicting biological phenomena, formulating hypotheses, and designing experiments. This work explores the potential of LLMs in pathway reasoning. We introduce BioMaze, a dataset with 5.1K complex pathway problems derived from real research, covering various biological contexts including natural dynamic changes, disturbances, additional intervention conditions, and multi-scale research targets. Our evaluation of methods such as CoT and graph-augmented reasoning, shows that LLMs struggle with pathway reasoning, especially in perturbed systems. To address this, we propose PathSeeker, an LLM agent that enhances reasoning through interactive subgraph-based navigation, enabling a more effective approach to handling the complexities of biological systems in a scientifically aligned manner. The dataset and code are available at this https URL.</li>
</ul>

<h3>Title: MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Hengzhi Li, Megan Tjandrasuwita, Yi R. Fung, Armando Solar-Lezama, Paul Pu Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16671">https://arxiv.org/abs/2502.16671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16671">https://arxiv.org/pdf/2502.16671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16671]] MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models(https://arxiv.org/abs/2502.16671)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Socially intelligent AI that can understand and interact seamlessly with humans in daily lives is increasingly important as AI becomes more closely integrated with peoples' daily activities. However, current works in artificial social reasoning all rely on language-only, or language-dominant approaches to benchmark and training models, resulting in systems that are improving in verbal communication but struggle with nonverbal social understanding. To address this limitation, we tap into a novel source of data rich in nonverbal and social interactions -- mime videos. Mimes refer to the art of expression through gesture and movement without spoken words, which presents unique challenges and opportunities in interpreting non-verbal social communication. We contribute a new dataset called MimeQA, obtained by sourcing 221 videos from YouTube, through rigorous annotation and verification, resulting in a benchmark with 101 videos and 806 question-answer pairs. Using MimeQA, we evaluate state-of-the-art video large language models (vLLMs) and find that their overall accuracy ranges from 15-30%. Our analysis reveals that vLLMs often fail to ground imagined objects and over-rely on the text prompt while ignoring subtle nonverbal interactions. Our data resources are released at this https URL to inspire future work in foundation models that embody true social intelligence capable of interpreting non-verbal human interactions.</li>
</ul>

<h3>Title: AeroReformer: Aerial Referring Transformer for UAV-based Referring Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Rui Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16680">https://arxiv.org/abs/2502.16680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16680">https://arxiv.org/pdf/2502.16680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16680]] AeroReformer: Aerial Referring Transformer for UAV-based Referring Image Segmentation(https://arxiv.org/abs/2502.16680)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>As a novel and challenging task, referring segmentation combines computer vision and natural language processing to localize and segment objects based on textual descriptions. While referring image segmentation (RIS) has been extensively studied in natural images, little attention has been given to aerial imagery, particularly from unmanned aerial vehicles (UAVs). The unique challenges of UAV imagery, including complex spatial scales, occlusions, and varying object orientations, render existing RIS approaches ineffective. A key limitation has been the lack of UAV-specific datasets, as manually annotating pixel-level masks and generating textual descriptions is labour-intensive and time-consuming. To address this gap, we design an automatic labelling pipeline that leverages pre-existing UAV segmentation datasets and Multimodal Large Language Models (MLLM) for generating textual descriptions. Furthermore, we propose Aerial Referring Transformer (AeroReformer), a novel framework for UAV referring image segmentation (UAV-RIS), featuring a Vision-Language Cross-Attention Module (VLCAM) for effective cross-modal understanding and a Rotation-Aware Multi-Scale Fusion (RAMSF) decoder to enhance segmentation accuracy in aerial scenes. Extensive experiments on two newly developed datasets demonstrate the superiority of AeroReformer over existing methods, establishing a new benchmark for UAV-RIS. The datasets and code will be publicly available at: this https URL.</li>
</ul>

<h3>Title: Are Sparse Autoencoders Useful? A Case Study in Sparse Probing</h3>
<ul>
<li><strong>Authors: </strong>Subhash Kantamneni, Joshua Engels, Senthooran Rajamanoharan, Max Tegmark, Neel Nanda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16681">https://arxiv.org/abs/2502.16681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16681">https://arxiv.org/pdf/2502.16681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16681]] Are Sparse Autoencoders Useful? A Case Study in Sparse Probing(https://arxiv.org/abs/2502.16681)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Sparse autoencoders (SAEs) are a popular method for interpreting concepts represented in large language model (LLM) activations. However, there is a lack of evidence regarding the validity of their interpretations due to the lack of a ground truth for the concepts used by an LLM, and a growing number of works have presented problems with current SAEs. One alternative source of evidence would be demonstrating that SAEs improve performance on downstream tasks beyond existing baselines. We test this by applying SAEs to the real-world task of LLM activation probing in four regimes: data scarcity, class imbalance, label noise, and covariate shift. Due to the difficulty of detecting concepts in these challenging settings, we hypothesize that SAEs' basis of interpretable, concept-level latents should provide a useful inductive bias. However, although SAEs occasionally perform better than baselines on individual datasets, we are unable to design ensemble methods combining SAEs with baselines that consistently outperform ensemble methods solely using baselines. Additionally, although SAEs initially appear promising for identifying spurious correlations, detecting poor dataset quality, and training multi-token probes, we are able to achieve similar results with simple non-SAE baselines as well. Though we cannot discount SAEs' utility on other tasks, our findings highlight the shortcomings of current SAEs and the need to rigorously evaluate interpretability methods on downstream tasks with strong baselines.</li>
</ul>

<h3>Title: Automatic Input Rewriting Improves Translation with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dayeon Ki, Marine Carpuat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16682">https://arxiv.org/abs/2502.16682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16682">https://arxiv.org/pdf/2502.16682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16682]] Automatic Input Rewriting Improves Translation with Large Language Models(https://arxiv.org/abs/2502.16682)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Can we improve machine translation (MT) with LLMs by rewriting their inputs automatically? Users commonly rely on the intuition that well-written text is easier to translate when using off-the-shelf MT systems. LLMs can rewrite text in many ways but in the context of MT, these capabilities have been primarily exploited to rewrite outputs via post-editing. We present an empirical study of 21 input rewriting methods with 3 open-weight LLMs for translating from English into 6 target languages. We show that text simplification is the most effective MT-agnostic rewrite strategy and that it can be improved further when using quality estimation to assess translatability. Human evaluation further confirms that simplified rewrites and their MT outputs both largely preserve the original meaning of the source and MT. These results suggest LLM-assisted input rewriting as a promising direction for improving translations.</li>
</ul>

<h3>Title: WildLong: Synthesizing Realistic Long-Context Instruction Data at Scale</h3>
<ul>
<li><strong>Authors: </strong>Jiaxi Li, Xingxing Zhang, Xun Wang, Xiaolong Huang, Li Dong, Liang Wang, Si-Qing Chen, Wei Lu, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16684">https://arxiv.org/abs/2502.16684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16684">https://arxiv.org/pdf/2502.16684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16684]] WildLong: Synthesizing Realistic Long-Context Instruction Data at Scale(https://arxiv.org/abs/2502.16684)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) with extended context windows enable tasks requiring extensive information integration but are limited by the scarcity of high-quality, diverse datasets for long-context instruction tuning. Existing data synthesis methods focus narrowly on objectives like fact retrieval and summarization, restricting their generalizability to complex, real-world tasks. WildLong extracts meta-information from real user queries, models co-occurrence relationships via graph-based methods, and employs adaptive generation to produce scalable data. It extends beyond single-document tasks to support multi-document reasoning, such as cross-document comparison and aggregation. Our models, finetuned on 150K instruction-response pairs synthesized using WildLong, surpasses existing open-source long-context-optimized models across benchmarks while maintaining strong performance on short-context tasks without incorporating supplementary short-context data. By generating a more diverse and realistic long-context instruction dataset, WildLong enhances LLMs' ability to generalize to complex, real-world reasoning over long contexts, establishing a new paradigm for long-context data synthesis.</li>
</ul>

<h3>Title: Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI</h3>
<ul>
<li><strong>Authors: </strong>Eunchung Noh, Jeonghun Baek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DC, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16691">https://arxiv.org/abs/2502.16691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16691">https://arxiv.org/pdf/2502.16691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16691]] Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI(https://arxiv.org/abs/2502.16691)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, large language model</a></li>
<li><strong>Abstract: </strong>Recent research has increasingly focused on training large language models (LLMs) using federated learning, known as FedLLM. However, responsible AI (RAI), which aims to ensure safe responses, remains underexplored in the context of FedLLM. In FedLLM, client data used for training may contain harmful content, leading to unsafe LLMs that generate harmful responses. Aggregating such unsafe LLMs into the global model and distributing them to clients may result in the widespread deployment of unsafe LLMs. To address this issue, we incorporate two well-known RAI methods into FedLLM: the safety filter and constitutional AI. Our experiments demonstrate that these methods significantly enhance the safety of the LLM, achieving over a 20% improvement on AdvBench, a benchmark for evaluating safety performance.</li>
</ul>

<h3>Title: Dynamic LLM Routing and Selection based on User Preferences: Balancing Performance, Cost, and Ethics</h3>
<ul>
<li><strong>Authors: </strong>Deepak Babu Piskala, Vijay Raajaa, Sachin Mishra, Bruno Bozza</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16696">https://arxiv.org/abs/2502.16696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16696">https://arxiv.org/pdf/2502.16696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16696]] Dynamic LLM Routing and Selection based on User Preferences: Balancing Performance, Cost, and Ethics(https://arxiv.org/abs/2502.16696)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the widespread deployment of large language models (LLMs) such as GPT4, BART, and LLaMA, the need for a system that can intelligently select the most suitable model for specific tasks while balancing cost, latency, accuracy, and ethical considerations has become increasingly important. Recognizing that not all tasks necessitate models with over 100 billion parameters, we introduce OptiRoute, an advanced model routing engine designed to dynamically select and route tasks to the optimal LLM based on detailed user-defined requirements. OptiRoute captures both functional (e.g., accuracy, speed, cost) and non-functional (e.g., helpfulness, harmlessness, honesty) criteria, leveraging lightweight task analysis and complexity estimation to efficiently match tasks with the best-fit models from a diverse array of LLMs. By employing a hybrid approach combining k-nearest neighbors (kNN) search and hierarchical filtering, OptiRoute optimizes for user priorities while minimizing computational overhead. This makes it ideal for real-time applications in cloud-based ML platforms, personalized AI services, and regulated industries.</li>
</ul>

<h3>Title: Interpretable Retinal Disease Prediction Using Biology-Informed Heterogeneous Graph Representations</h3>
<ul>
<li><strong>Authors: </strong>Laurin Lux, Alexander H. Berger, Maria Romeo Tricas, Alaa E. Fayed, Sobha Sivaprasada, Linus Kreitner, Jonas Weidner, Martin J. Menten, Daniel Rueckert, Johannes C. Paetzold</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16697">https://arxiv.org/abs/2502.16697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16697">https://arxiv.org/pdf/2502.16697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16697]] Interpretable Retinal Disease Prediction Using Biology-Informed Heterogeneous Graph Representations(https://arxiv.org/abs/2502.16697)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Interpretability is crucial to enhance trust in machine learning models for medical diagnostics. However, most state-of-the-art image classifiers based on neural networks are not interpretable. As a result, clinicians often resort to known biomarkers for diagnosis, although biomarker-based classification typically performs worse than large neural networks. This work proposes a method that surpasses the performance of established machine learning models while simultaneously improving prediction interpretability for diabetic retinopathy staging from optical coherence tomography angiography (OCTA) images. Our method is based on a novel biology-informed heterogeneous graph representation that models retinal vessel segments, intercapillary areas, and the foveal avascular zone (FAZ) in a human-interpretable way. This graph representation allows us to frame diabetic retinopathy staging as a graph-level classification task, which we solve using an efficient graph neural network. We benchmark our method against well-established baselines, including classical biomarker-based classifiers, convolutional neural networks (CNNs), and vision transformers. Our model outperforms all baselines on two datasets. Crucially, we use our biology-informed graph to provide explanations of unprecedented detail. Our approach surpasses existing methods in precisely localizing and identifying critical vessels or intercapillary areas. In addition, we give informative and human-interpretable attributions to critical characteristics. Our work contributes to the development of clinical decision-support tools in ophthalmology.</li>
</ul>

<h3>Title: Uncovering the Hidden Threat of Text Watermarking from Users with Cross-Lingual Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Mansour Al Ghanim, Jiaqi Xue, Rochana Prih Hastuti, Mengxin Zheng, Yan Solihin, Qian Lou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16699">https://arxiv.org/abs/2502.16699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16699">https://arxiv.org/pdf/2502.16699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16699]] Uncovering the Hidden Threat of Text Watermarking from Users with Cross-Lingual Knowledge(https://arxiv.org/abs/2502.16699)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, watermark</a></li>
<li><strong>Abstract: </strong>In this study, we delve into the hidden threats posed to text watermarking by users with cross-lingual knowledge. While most research focuses on watermarking methods for English, there is a significant gap in evaluating these methods in cross-lingual contexts. This oversight neglects critical adversary scenarios involving cross-lingual users, creating uncertainty regarding the effectiveness of cross-lingual watermarking. We assess four watermarking techniques across four linguistically rich languages, examining watermark resilience and text quality across various parameters and attacks. Our focus is on a realistic scenario featuring adversaries with cross-lingual expertise, evaluating the adequacy of current watermarking methods against such challenges.</li>
</ul>

<h3>Title: Can ChatGPT Learn to Count Letters?</h3>
<ul>
<li><strong>Authors: </strong>Javier Conde, Gonzalo Martínez, Pedro Reviriego, Zhen Gao, Shanshan Liu, Fabrizio Lombardi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16705">https://arxiv.org/abs/2502.16705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16705">https://arxiv.org/pdf/2502.16705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16705]] Can ChatGPT Learn to Count Letters?(https://arxiv.org/abs/2502.16705)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) struggle on simple tasks such as counting the number of occurrences of a letter in a word. In this paper, we investigate if ChatGPT can learn to count letters and propose an efficient solution.</li>
</ul>

<h3>Title: Exploring Incremental Unlearning: Techniques, Challenges, and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Sadia Qureshi, Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Jianming Yong, Xiaohua Jia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16708">https://arxiv.org/abs/2502.16708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16708">https://arxiv.org/pdf/2502.16708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16708]] Exploring Incremental Unlearning: Techniques, Challenges, and Future Directions(https://arxiv.org/abs/2502.16708)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The growing demand for data privacy in Machine Learning (ML) applications has seen Machine Unlearning (MU) emerge as a critical area of research. As the `right to be forgotten' becomes regulated globally, it is increasingly important to develop mechanisms that delete user data from AI systems while maintaining performance and scalability of these systems. Incremental Unlearning (IU) is a promising MU solution to address the challenges of efficiently removing specific data from ML models without the need for expensive and time-consuming full retraining. This paper presents the various techniques and approaches to IU. It explores the challenges faced in designing and implementing IU mechanisms. Datasets and metrics for evaluating the performance of unlearning techniques are discussed as well. Finally, potential solutions to the IU challenges alongside future research directions are offered. This survey provides valuable insights for researchers and practitioners seeking to understand the current landscape of IU and its potential for enhancing privacy-preserving intelligent systems.</li>
</ul>

<h3>Title: Speed and Conversational Large Language Models: Not All Is About Tokens per Second</h3>
<ul>
<li><strong>Authors: </strong>Javier Conde, Miguel González, Pedro Reviriego, Zhen Gao, Shanshan Liu, Fabrizio Lombardi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16721">https://arxiv.org/abs/2502.16721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16721">https://arxiv.org/pdf/2502.16721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16721]] Speed and Conversational Large Language Models: Not All Is About Tokens per Second(https://arxiv.org/abs/2502.16721)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The speed of open-weights large language models (LLMs) and its dependency on the task at hand, when run on GPUs, is studied to present a comparative analysis of the speed of the most popular open LLMs.</li>
</ul>

<h3>Title: Layer-Wise Evolution of Representations in Fine-Tuned Transformers: Insights from Sparse AutoEncoders</h3>
<ul>
<li><strong>Authors: </strong>Suneel Nadipalli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16722">https://arxiv.org/abs/2502.16722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16722">https://arxiv.org/pdf/2502.16722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16722]] Layer-Wise Evolution of Representations in Fine-Tuned Transformers: Insights from Sparse AutoEncoders(https://arxiv.org/abs/2502.16722)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning pre-trained transformers is a powerful technique for enhancing the performance of base models on specific tasks. From early applications in models like BERT to fine-tuning Large Language Models (LLMs), this approach has been instrumental in adapting general-purpose architectures for specialized downstream tasks. Understanding the fine-tuning process is crucial for uncovering how transformers adapt to specific objectives, retain general representations, and acquire task-specific features. This paper explores the underlying mechanisms of fine-tuning, specifically in the BERT transformer, by analyzing activation similarity, training Sparse AutoEncoders (SAEs), and visualizing token-level activations across different layers. Based on experiments conducted across multiple datasets and BERT layers, we observe a steady progression in how features adapt to the task at hand: early layers primarily retain general representations, middle layers act as a transition between general and task-specific features, and later layers fully specialize in task adaptation. These findings provide key insights into the inner workings of fine-tuning and its impact on representation learning within transformer architectures.</li>
</ul>

<h3>Title: DOSE3 : Diffusion-based Out-of-distribution detection on SE(3) trajectories</h3>
<ul>
<li><strong>Authors: </strong>Hongzhe Cheng, Tianyou Zheng, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16725">https://arxiv.org/abs/2502.16725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16725">https://arxiv.org/pdf/2502.16725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16725]] DOSE3 : Diffusion-based Out-of-distribution detection on SE(3) trajectories(https://arxiv.org/abs/2502.16725)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Out-of-Distribution(OOD) detection, a fundamental machine learning task aimed at identifying abnormal samples, traditionally requires model retraining for different inlier distributions. While recent research demonstrates the applicability of diffusion models to OOD detection, existing approaches are limited to Euclidean or latent image spaces. Our work extends OOD detection to trajectories in the Special Euclidean Group in 3D ($\mathbb{SE}(3)$), addressing a critical need in computer vision, robotics, and engineering applications that process object pose sequences in $\mathbb{SE}(3)$. We present $\textbf{D}$iffusion-based $\textbf{O}$ut-of-distribution detection on $\mathbb{SE}(3)$ ($\mathbf{DOSE3}$), a novel OOD framework that extends diffusion to a unified sample space of $\mathbb{SE}(3)$ pose sequences. Through extensive validation on multiple benchmark datasets, we demonstrate $\mathbf{DOSE3}$'s superior performance compared to state-of-the-art OOD detection frameworks.</li>
</ul>

<h3>Title: RapidPen: Fully Automated IP-to-Shell Penetration Testing with LLM-based Agents</h3>
<ul>
<li><strong>Authors: </strong>Sho Nakatani (1) ((1) Security &amp; Development Lab)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16730">https://arxiv.org/abs/2502.16730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16730">https://arxiv.org/pdf/2502.16730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16730]] RapidPen: Fully Automated IP-to-Shell Penetration Testing with LLM-based Agents(https://arxiv.org/abs/2502.16730)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>We present RapidPen, a fully automated penetration testing (pentesting) framework that addresses the challenge of achieving an initial foothold (IP-to-Shell) without human intervention. Unlike prior approaches that focus primarily on post-exploitation or require a human-in-the-loop, RapidPen leverages large language models (LLMs) to autonomously discover and exploit vulnerabilities, starting from a single IP address. By integrating advanced ReAct-style task planning (Re) with retrieval-augmented knowledge bases of successful exploits, along with a command-generation and direct execution feedback loop (Act), RapidPen systematically scans services, identifies viable attack vectors, and executes targeted exploits in a fully automated manner. In our evaluation against a vulnerable target from the Hack The Box platform, RapidPen achieved shell access within 200-400 seconds at a per-run cost of approximately \$0.3-\$0.6, demonstrating a 60\% success rate when reusing prior "success-case" data. These results underscore the potential of truly autonomous pentesting for both security novices and seasoned professionals. Organizations without dedicated security teams can leverage RapidPen to quickly identify critical vulnerabilities, while expert pentesters can offload repetitive tasks and focus on complex challenges. Ultimately, our work aims to make penetration testing more accessible and cost-efficient, thereby enhancing the overall security posture of modern software ecosystems.</li>
</ul>

<h3>Title: Model-agnostic Coreset Selection via LLM-based Concept Bottlenecks</h3>
<ul>
<li><strong>Authors: </strong>Akshay Mehra, Trisha Mittal, Subhadra Gopalakrishnan, Joshua Kimball</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16733">https://arxiv.org/abs/2502.16733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16733">https://arxiv.org/pdf/2502.16733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16733]] Model-agnostic Coreset Selection via LLM-based Concept Bottlenecks(https://arxiv.org/abs/2502.16733)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Coreset Selection (CS) identifies a subset of training data that achieves model performance comparable to using the entire dataset. Many state-of-the-art CS methods, select coresets using scores whose computation requires training the downstream model on the entire dataset and recording changes in its behavior on samples as it trains (training dynamics). These scores are inefficient to compute and hard to interpret as they do not indicate whether a sample is difficult to learn in general or only for a specific model. Our work addresses these challenges by proposing an interpretable score that gauges a sample's difficulty using human-understandable textual attributes (concepts) independent of any downstream model. Specifically, we measure the alignment between a sample's visual features and concept bottlenecks, derived via large language models, by training a linear concept bottleneck layer and compute the sample's difficulty score using it. We then use this score and a stratified sampling strategy to identify the coreset. Crucially, our score is efficiently computable without training the downstream model on the full dataset even once, leads to high-performing coresets for various downstream models, and is computable even for an unlabeled dataset. Through experiments on CIFAR-10, CIFAR-100, and ImageNet-1K, we show our coresets outperform random subsets, even at high pruning rates, and achieve model performance comparable to or better than coresets found by training dynamics-based methods.</li>
</ul>

<h3>Title: Towards Optimal Adversarial Robust Reinforcement Learning with Infinity Measurement Error</h3>
<ul>
<li><strong>Authors: </strong>Haoran Li, Zicheng Zhang, Wang Luo, Congying Han, Jiayu Lv, Tiande Guo, Yudong Hu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16734">https://arxiv.org/abs/2502.16734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16734">https://arxiv.org/pdf/2502.16734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16734]] Towards Optimal Adversarial Robust Reinforcement Learning with Infinity Measurement Error(https://arxiv.org/abs/2502.16734)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Ensuring the robustness of deep reinforcement learning (DRL) agents against adversarial attacks is critical for their trustworthy deployment. Recent research highlights the challenges of achieving state-adversarial robustness and suggests that an optimal robust policy (ORP) does not always exist, complicating the enforcement of strict robustness constraints. In this paper, we further explore the concept of ORP. We first introduce the Intrinsic State-adversarial Markov Decision Process (ISA-MDP), a novel formulation where adversaries cannot fundamentally alter the intrinsic nature of state observations. ISA-MDP, supported by empirical and theoretical evidence, universally characterizes decision-making under state-adversarial paradigms. We rigorously prove that within ISA-MDP, a deterministic and stationary ORP exists, aligning with the Bellman optimal policy. Our findings theoretically reveal that improving DRL robustness does not necessarily compromise performance in natural environments. Furthermore, we demonstrate the necessity of infinity measurement error (IME) in both $Q$-function and probability spaces to achieve ORP, unveiling vulnerabilities of previous DRL algorithms that rely on $1$-measurement errors. Motivated by these insights, we develop the Consistent Adversarial Robust Reinforcement Learning (CAR-RL) framework, which optimizes surrogates of IME. We apply CAR-RL to both value-based and policy-based DRL algorithms, achieving superior performance and validating our theoretical analysis.</li>
</ul>

<h3>Title: AUKT: Adaptive Uncertainty-Guided Knowledge Transfer with Conformal Prediction</h3>
<ul>
<li><strong>Authors: </strong>Rui Liu, Peng Gao, Yu Shen, Ming Lin, Pratap Tokekar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16736">https://arxiv.org/abs/2502.16736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16736">https://arxiv.org/pdf/2502.16736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16736]] AUKT: Adaptive Uncertainty-Guided Knowledge Transfer with Conformal Prediction(https://arxiv.org/abs/2502.16736)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Knowledge transfer between teacher and student models has proven effective across various machine learning applications. However, challenges arise when the teacher's predictions are noisy, or the data domain during student training shifts from the teacher's pretraining data. In such scenarios, blindly relying on the teacher's predictions can lead to suboptimal knowledge transfer. To address these challenges, we propose a novel and universal framework, Adaptive Uncertainty-guided Knowledge Transfer ($\textbf{AUKT}$), which leverages Conformal Prediction (CP) to dynamically adjust the student's reliance on the teacher's guidance based on the teacher's prediction uncertainty. CP is a distribution-free, model-agnostic approach that provides reliable prediction sets with statistical coverage guarantees and minimal computational overhead. This adaptive mechanism mitigates the risk of learning undesirable or incorrect knowledge. We validate the proposed framework across diverse applications, including image classification, imitation-guided reinforcement learning, and autonomous driving. Experimental results consistently demonstrate that our approach improves performance, robustness and transferability, offering a promising direction for enhanced knowledge transfer in real-world applications.</li>
</ul>

<h3>Title: Keeping up with dynamic attackers: Certifying robustness to adaptive online data poisoning</h3>
<ul>
<li><strong>Authors: </strong>Avinandan Bose, Laurent Lessard, Maryam Fazel, Krishnamurthy Dj Dvijotham</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16737">https://arxiv.org/abs/2502.16737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16737">https://arxiv.org/pdf/2502.16737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16737]] Keeping up with dynamic attackers: Certifying robustness to adaptive online data poisoning(https://arxiv.org/abs/2502.16737)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>The rise of foundation models fine-tuned on human feedback from potentially untrusted users has increased the risk of adversarial data poisoning, necessitating the study of robustness of learning algorithms against such attacks. Existing research on provable certified robustness against data poisoning attacks primarily focuses on certifying robustness for static adversaries who modify a fraction of the dataset used to train the model before the training algorithm is applied. In practice, particularly when learning from human feedback in an online sense, adversaries can observe and react to the learning process and inject poisoned samples that optimize adversarial objectives better than when they are restricted to poisoning a static dataset once, before the learning algorithm is applied. Indeed, it has been shown in prior work that online dynamic adversaries can be significantly more powerful than static ones. We present a novel framework for computing certified bounds on the impact of dynamic poisoning, and use these certificates to design robust learning algorithms. We give an illustration of the framework for the mean estimation and binary classification problems and outline directions for extending this in further work. The code to implement our certificates and replicate our results is available at this https URL.</li>
</ul>

<h3>Title: SQLong: Enhanced NL2SQL for Longer Contexts with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Dai Quoc Nguyen, Cong Duy Vu Hoang, Duy Vu, Gioacchino Tangari, Thanh Tien Vu, Don Dharmasiri, Yuan-Fang Li, Long Duong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16747">https://arxiv.org/abs/2502.16747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16747">https://arxiv.org/pdf/2502.16747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16747]] SQLong: Enhanced NL2SQL for Longer Contexts with LLMs(https://arxiv.org/abs/2502.16747)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Open-weight large language models (LLMs) have significantly advanced performance in the Natural Language to SQL (NL2SQL) task. However, their effectiveness diminishes when dealing with large database schemas, as the context length increases. To address this limitation, we present SQLong, a novel and efficient data augmentation framework designed to enhance LLM performance in long-context scenarios for the NL2SQL task. SQLong generates augmented datasets by extending existing database schemas with additional synthetic CREATE TABLE commands and corresponding data rows, sampled from diverse schemas in the training data. This approach effectively simulates long-context scenarios during finetuning and evaluation. Through experiments on the Spider and BIRD datasets, we demonstrate that LLMs finetuned with SQLong-augmented data significantly outperform those trained on standard datasets. These imply SQLong's practical implementation and its impact on improving NL2SQL capabilities in real-world settings with complex database schemas.</li>
</ul>

<h3>Title: GS-TransUNet: Integrated 2D Gaussian Splatting and Transformer UNet for Accurate Skin Lesion Analysis</h3>
<ul>
<li><strong>Authors: </strong>Anand Kumar, Kavinder Roghit Kanthen, Josna John</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16748">https://arxiv.org/abs/2502.16748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16748">https://arxiv.org/pdf/2502.16748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16748]] GS-TransUNet: Integrated 2D Gaussian Splatting and Transformer UNet for Accurate Skin Lesion Analysis(https://arxiv.org/abs/2502.16748)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>We can achieve fast and consistent early skin cancer detection with recent developments in computer vision and deep learning techniques. However, the existing skin lesion segmentation and classification prediction models run independently, thus missing potential efficiencies from their integrated execution. To unify skin lesion analysis, our paper presents the Gaussian Splatting - Transformer UNet (GS-TransUNet), a novel approach that synergistically combines 2D Gaussian splatting with the Transformer UNet architecture for automated skin cancer diagnosis. Our unified deep learning model efficiently delivers dual-function skin lesion classification and segmentation for clinical diagnosis. Evaluated on ISIC-2017 and PH2 datasets, our network demonstrates superior performance compared to existing state-of-the-art models across multiple metrics through 5-fold cross-validation. Our findings illustrate significant advancements in the precision of segmentation and classification. This integration sets new benchmarks in the field and highlights the potential for further research into multi-task medical image analysis methodologies, promising enhancements in automated diagnostic systems.</li>
</ul>

<h3>Title: Guardians of the Agentic System: Preventing Many Shots Jailbreak with Agentic System</h3>
<ul>
<li><strong>Authors: </strong>Saikat Barua, Mostafizur Rahman, Md Jafor Sadek, Rafiul Islam, Shehnaz Khaled, Ahmedul Kabir</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16750">https://arxiv.org/abs/2502.16750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16750">https://arxiv.org/pdf/2502.16750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16750]] Guardians of the Agentic System: Preventing Many Shots Jailbreak with Agentic System(https://arxiv.org/abs/2502.16750)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The autonomous AI agents using large language models can create undeniable values in all span of the society but they face security threats from adversaries that warrants immediate protective solutions because trust and safety issues arise. Considering the many-shot jailbreaking and deceptive alignment as some of the main advanced attacks, that cannot be mitigated by the static guardrails used during the supervised training, points out a crucial research priority for real world robustness. The combination of static guardrails in dynamic multi-agent system fails to defend against those attacks. We intend to enhance security for LLM-based agents through the development of new evaluation frameworks which identify and counter threats for safe operational deployment. Our work uses three examination methods to detect rogue agents through a Reverse Turing Test and analyze deceptive alignment through multi-agent simulations and develops an anti-jailbreaking system by testing it with GEMINI 1.5 pro and llama-3.3-70B, deepseek r1 models using tool-mediated adversarial scenarios. The detection capabilities are strong such as 94\% accuracy for GEMINI 1.5 pro yet the system suffers persistent vulnerabilities when under long attacks as prompt length increases attack success rates (ASR) and diversity metrics become ineffective in prediction while revealing multiple complex system faults. The findings demonstrate the necessity of adopting flexible security systems based on active monitoring that can be performed by the agents themselves together with adaptable interventions by system admin as the current models can create vulnerabilities that can lead to the unreliable and vulnerable system. So, in our work, we try to address such situations and propose a comprehensive framework to counteract the security issues.</li>
</ul>

<h3>Title: Towards Reinforcement Learning for Exploration of Speculative Execution Vulnerabilities</h3>
<ul>
<li><strong>Authors: </strong>Evan Lai, Wenjie Xiong, Edward Suh, Mohit Tiwari, Mulong Luo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16756">https://arxiv.org/abs/2502.16756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16756">https://arxiv.org/pdf/2502.16756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16756]] Towards Reinforcement Learning for Exploration of Speculative Execution Vulnerabilities(https://arxiv.org/abs/2502.16756)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Speculative attacks such as Spectre can leak secret information without being discovered by the operating system. Speculative execution vulnerabilities are finicky and deep in the sense that to exploit them, it requires intensive manual labor and intimate knowledge of the hardware. In this paper, we introduce SpecRL, a framework that utilizes reinforcement learning to find speculative execution leaks in post-silicon (black box) microprocessors.</li>
</ul>

<h3>Title: Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions</h3>
<ul>
<li><strong>Authors: </strong>Joseph Suh, Erfan Jahanparast, Suhong Moon, Minwoo Kang, Serina Chang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16761">https://arxiv.org/abs/2502.16761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16761">https://arxiv.org/pdf/2502.16761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16761]] Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions(https://arxiv.org/abs/2502.16761)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) present novel opportunities in public opinion research by predicting survey responses in advance during the early stages of survey design. Prior methods steer LLMs via descriptions of subpopulations as LLMs' input prompt, yet such prompt engineering approaches have struggled to faithfully predict the distribution of survey responses from human subjects. In this work, we propose directly fine-tuning LLMs to predict response distributions by leveraging unique structural characteristics of survey data. To enable fine-tuning, we curate SubPOP, a significantly scaled dataset of 3,362 questions and 70K subpopulation-response pairs from well-established public opinion surveys. We show that fine-tuning on SubPOP greatly improves the match between LLM predictions and human responses across various subpopulations, reducing the LLM-human gap by up to 46% compared to baselines, and achieves strong generalization to unseen surveys and subpopulations. Our findings highlight the potential of survey-based fine-tuning to improve opinion prediction for diverse, real-world subpopulations and therefore enable more efficient survey designs. Our code is available at this https URL.</li>
</ul>

<h3>Title: A Transformer-in-Transformer Network Utilizing Knowledge Distillation for Image Recognition</h3>
<ul>
<li><strong>Authors: </strong>Dewan Tauhid Rahman, Yeahia Sarker, Antar Mazumder, Md. Shamim Anower</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16762">https://arxiv.org/abs/2502.16762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16762">https://arxiv.org/pdf/2502.16762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16762]] A Transformer-in-Transformer Network Utilizing Knowledge Distillation for Image Recognition(https://arxiv.org/abs/2502.16762)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>This paper presents a novel knowledge distillation neural architecture leveraging efficient transformer networks for effective image classification. Natural images display intricate arrangements encompassing numerous extraneous elements. Vision transformers utilize localized patches to compute attention. However, exclusive dependence on patch segmentation proves inadequate in sufficiently encompassing the comprehensive nature of the image. To address this issue, we have proposed an inner-outer transformer-based architecture, which gives attention to the global and local aspects of the image. Moreover, The training of transformer models poses significant challenges due to their demanding resource, time, and data requirements. To tackle this, we integrate knowledge distillation into the architecture, enabling efficient learning. Leveraging insights from a larger teacher model, our approach enhances learning efficiency and effectiveness. Significantly, the transformer-in-transformer network acquires lightweight characteristics by means of distillation conducted within the feature extraction layer. Our featured network's robustness is established through substantial experimentation on the MNIST, CIFAR10, and CIFAR100 datasets, demonstrating commendable top-1 and top-5 accuracy. The conducted ablative analysis comprehensively validates the effectiveness of the chosen parameters and settings, showcasing their superiority against contemporary methodologies. Remarkably, the proposed Transformer-in-Transformer Network (TITN) model achieves impressive performance milestones across various datasets: securing the highest top-1 accuracy of 74.71% and a top-5 accuracy of 92.28% for the CIFAR100 dataset, attaining an unparalleled top-1 accuracy of 92.03% and top-5 accuracy of 99.80% for the CIFAR-10 dataset, and registering an exceptional top-1 accuracy of 99.56% for the MNIST dataset.</li>
</ul>

<h3>Title: Exact Learning of Permutations for Nonzero Binary Inputs with Logarithmic Training Size and Quadratic Ensemble Complexity</h3>
<ul>
<li><strong>Authors: </strong>George Giapitzakis, Artur Back de Luca, Kimon Fountoulakis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16763">https://arxiv.org/abs/2502.16763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16763">https://arxiv.org/pdf/2502.16763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16763]] Exact Learning of Permutations for Nonzero Binary Inputs with Logarithmic Training Size and Quadratic Ensemble Complexity(https://arxiv.org/abs/2502.16763)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The ability of an architecture to realize permutations is quite fundamental. For example, Large Language Models need to be able to correctly copy (and perhaps rearrange) parts of the input prompt into the output. Classical universal approximation theorems guarantee the existence of parameter configurations that solve this task but offer no insights into whether gradient-based algorithms can find them. In this paper, we address this gap by focusing on two-layer fully connected feed-forward neural networks and the task of learning permutations on nonzero binary inputs. We show that in the infinite width Neural Tangent Kernel (NTK) regime, an ensemble of such networks independently trained with gradient descent on only the $k$ standard basis vectors out of $2^k - 1$ possible inputs successfully learns any fixed permutation of length $k$ with arbitrarily high probability. By analyzing the exact training dynamics, we prove that the network's output converges to a Gaussian process whose mean captures the ground truth permutation via sign-based features. We then demonstrate how averaging these runs (an "ensemble" method) and applying a simple rounding step yields an arbitrarily accurate prediction on any possible input unseen during training. Notably, the number of models needed to achieve exact learning with high probability (which we refer to as ensemble complexity) exhibits a linearithmic dependence on the input size $k$ for a single test input and a quadratic dependence when considering all test inputs simultaneously.</li>
</ul>

<h3>Title: A Hybrid Approach to Information Retrieval and Answer Generation for Regulatory Texts</h3>
<ul>
<li><strong>Authors: </strong>Jhon Rayo, Raul de la Rosa, Mario Garrido</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16767">https://arxiv.org/abs/2502.16767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16767">https://arxiv.org/pdf/2502.16767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16767]] A Hybrid Approach to Information Retrieval and Answer Generation for Regulatory Texts(https://arxiv.org/abs/2502.16767)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Regulatory texts are inherently long and complex, presenting significant challenges for information retrieval systems in supporting regulatory officers with compliance tasks. This paper introduces a hybrid information retrieval system that combines lexical and semantic search techniques to extract relevant information from large regulatory corpora. The system integrates a fine-tuned sentence transformer model with the traditional BM25 algorithm to achieve both semantic precision and lexical coverage. To generate accurate and comprehensive responses, retrieved passages are synthesized using Large Language Models (LLMs) within a Retrieval Augmented Generation (RAG) framework. Experimental results demonstrate that the hybrid system significantly outperforms standalone lexical and semantic approaches, with notable improvements in Recall@10 and MAP@10. By openly sharing our fine-tuned model and methodology, we aim to advance the development of robust natural language processing tools for compliance-driven applications in regulatory domains.</li>
</ul>

<h3>Title: LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint</h3>
<ul>
<li><strong>Authors: </strong>Qianli Ma, Dongrui Liu, Qian Chen, Linfeng Zhang, Jing Shao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16770">https://arxiv.org/abs/2502.16770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16770">https://arxiv.org/pdf/2502.16770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16770]] LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint(https://arxiv.org/abs/2502.16770)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: \textbf{neuron misidentification} due to simplistic parameter magnitude-based selection, and \textbf{cross-task neuron interference} during merging. To address these challenges, we propose \textbf{LED-Merging}, a three-stage framework that \textbf{L}ocates task-specific neurons via gradient-based attribution, dynamically \textbf{E}lects critical neurons through multi-model importance fusion, and \textbf{D}isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging reduces harmful response rates(\emph{e.g.}, a 31.4\% decrease on Llama-3-8B-Instruct on HarmBench) while preserving 95\% of utility performance(\emph{e.g.}, 52.39\% accuracy on GSM8K). LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs.</li>
</ul>

<h3>Title: AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement</h3>
<ul>
<li><strong>Authors: </strong>Zhexin Zhang, Leqi Lei, Junxiao Yang, Xijie Huang, Yida Lu, Shiyao Cui, Renmiao Chen, Qinglin Zhang, Xinyuan Wang, Hao Wang, Hao Li, Xianqi Lei, Chengwei Pan, Lei Sha, Hongning Wang, Minlie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16776">https://arxiv.org/abs/2502.16776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16776">https://arxiv.org/pdf/2502.16776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16776]] AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement(https://arxiv.org/abs/2502.16776)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>As AI models are increasingly deployed across diverse real-world scenarios, ensuring their safety remains a critical yet underexplored challenge. While substantial efforts have been made to evaluate and enhance AI safety, the lack of a standardized framework and comprehensive toolkit poses significant obstacles to systematic research and practical adoption. To bridge this gap, we introduce AISafetyLab, a unified framework and toolkit that integrates representative attack, defense, and evaluation methodologies for AI safety. AISafetyLab features an intuitive interface that enables developers to seamlessly apply various techniques while maintaining a well-structured and extensible codebase for future advancements. Additionally, we conduct empirical studies on Vicuna, analyzing different attack and defense strategies to provide valuable insights into their comparative effectiveness. To facilitate ongoing research and development in AI safety, AISafetyLab is publicly available at this https URL, and we are committed to its continuous maintenance and improvement.</li>
</ul>

<h3>Title: The Robustness of Structural Features in Species Interaction Networks</h3>
<ul>
<li><strong>Authors: </strong>Sanaz Hasanzadeh Fard, Emily Dolson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16778">https://arxiv.org/abs/2502.16778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16778">https://arxiv.org/pdf/2502.16778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16778]] The Robustness of Structural Features in Species Interaction Networks(https://arxiv.org/abs/2502.16778)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Species interaction networks are a powerful tool for describing ecological communities; they typically contain nodes representing species, and edges representing interactions between those species. For the purposes of drawing abstract inferences about groups of similar networks, ecologists often use graph topology metrics to summarize structural features. However, gathering the data that underlies these networks is challenging, which can lead to some interactions being missed. Thus, it is important to understand how much different structural metrics are affected by missing data. To address this question, we analyzed a database of 148 real-world bipartite networks representing four different types of species interactions (pollination, host-parasite, plant-ant, and seed-dispersal). For each network, we measured six different topological properties: number of connected components, variance in node betweenness, variance in node PageRank, largest Eigenvalue, the number of non-zero Eigenvalues, and community detection as determined by four different algorithms. We then tested how these properties change as additional edges -- representing data that may have been missed -- are added to the networks. We found substantial variation in how robust different properties were to the missing data. For example, the Clauset-Newman-Moore and Louvain community detection algorithms showed much more gradual change as edges were added than the label propagation and Girvan-Newman algorithms did, suggesting that the former are more robust. Robustness also varied for some metrics based on interaction type. These results provide a foundation for selecting network properties to use when analyzing messy ecological network data.</li>
</ul>

<h3>Title: Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model</h3>
<ul>
<li><strong>Authors: </strong>Yaxuan Huang, Xili Dai, Jianan Wang, Xianbiao Qi, Yixing Yuan, Xiangyu Yue</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16779">https://arxiv.org/abs/2502.16779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16779">https://arxiv.org/pdf/2502.16779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16779]] Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model(https://arxiv.org/abs/2502.16779)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Room layout estimation from multiple-perspective images is poorly investigated due to the complexities that emerge from multi-view geometry, which requires muti-step solutions such as camera intrinsic and extrinsic estimation, image matching, and triangulation. However, in 3D reconstruction, the advancement of recent 3D foundation models such as DUSt3R has shifted the paradigm from the traditional multi-step structure-from-motion process to an end-to-end single-step approach. To this end, we introduce Plane-DUSt3R}, a novel method for multi-view room layout estimation leveraging the 3D foundation model DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes on a room layout dataset (Structure3D) with a modified objective to estimate structural planes. By generating uniform and parsimonious results, Plane-DUSt3R enables room layout estimation with only a single post-processing step and 2D detection results. Unlike previous methods that rely on single-perspective or panorama image, Plane-DUSt3R extends the setting to handle multiple-perspective images. Moreover, it offers a streamlined, end-to-end solution that simplifies the process and reduces error accumulation. Experimental results demonstrate that Plane-DUSt3R not only outperforms state-of-the-art methods on the synthetic dataset but also proves robust and effective on in the wild data with different image styles such as cartoon.</li>
</ul>

<h3>Title: MultiOCR-QA: Dataset for Evaluating Robustness of LLMs in Question Answering on Multilingual OCR Texts</h3>
<ul>
<li><strong>Authors: </strong>Bhawna Piryani, Jamshid Mozafari, Abdelrahman Abdallah, Antoine Doucet, Adam Jatowt</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16781">https://arxiv.org/abs/2502.16781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16781">https://arxiv.org/pdf/2502.16781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16781]] MultiOCR-QA: Dataset for Evaluating Robustness of LLMs in Question Answering on Multilingual OCR Texts(https://arxiv.org/abs/2502.16781)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Optical Character Recognition (OCR) plays a crucial role in digitizing historical and multilingual documents, yet OCR errors -- imperfect extraction of the text, including character insertion, deletion and permutation -- can significantly impact downstream tasks like question-answering (QA). In this work, we introduce a multilingual QA dataset MultiOCR-QA, designed to analyze the effects of OCR noise on QA systems' performance. The MultiOCR-QA dataset comprises 60K question-answer pairs covering three languages, English, French, and German. The dataset is curated from OCR-ed old documents, allowing for the evaluation of OCR-induced challenges on question answering. We evaluate MultiOCR-QA on various levels and types of OCR errors to access the robustness of LLMs in handling real-world digitization errors. Our findings show that QA systems are highly prone to OCR induced errors and exhibit performance degradation on noisy OCR text.</li>
</ul>

<h3>Title: CipherPrune: Efficient and Scalable Private Transformer Inference</h3>
<ul>
<li><strong>Authors: </strong>Yancheng Zhang, Jiaqi Xue, Mengxin Zheng, Mimi Xie, Mingzhe Zhang, Lei Jiang, Qian Lou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16782">https://arxiv.org/abs/2502.16782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16782">https://arxiv.org/pdf/2502.16782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16782]] CipherPrune: Efficient and Scalable Private Transformer Inference(https://arxiv.org/abs/2502.16782)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, transformer</a></li>
<li><strong>Abstract: </strong>Private Transformer inference using cryptographic protocols offers promising solutions for privacy-preserving machine learning; however, it still faces significant runtime overhead (efficiency issues) and challenges in handling long-token inputs (scalability issues). We observe that the Transformer's operational complexity scales quadratically with the number of input tokens, making it essential to reduce the input token length. Notably, each token varies in importance, and many inputs contain redundant tokens. Additionally, prior private inference methods that rely on high-degree polynomial approximations for non-linear activations are computationally expensive. Therefore, reducing the polynomial degree for less important tokens can significantly accelerate private inference. Building on these observations, we propose \textit{CipherPrune}, an efficient and scalable private inference framework that includes a secure encrypted token pruning protocol, a polynomial reduction protocol, and corresponding Transformer network optimizations. At the protocol level, encrypted token pruning adaptively removes unimportant tokens from encrypted inputs in a progressive, layer-wise manner. Additionally, encrypted polynomial reduction assigns lower-degree polynomials to less important tokens after pruning, enhancing efficiency without decryption. At the network level, we introduce protocol-aware network optimization via a gradient-based search to maximize pruning thresholds and polynomial reduction conditions while maintaining the desired accuracy. Our experiments demonstrate that CipherPrune reduces the execution overhead of private Transformer inference by approximately $6.1\times$ for 128-token inputs and $10.6\times$ for 512-token inputs, compared to previous methods, with only a marginal drop in accuracy. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: SwimVG: Step-wise Multimodal Fusion and Adaption for Visual Grounding</h3>
<ul>
<li><strong>Authors: </strong>Liangtao Shi, Ting Liu, Xiantao Hu, Yue Hu, Quanjun Yin, Richang Hong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16786">https://arxiv.org/abs/2502.16786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16786">https://arxiv.org/pdf/2502.16786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16786]] SwimVG: Step-wise Multimodal Fusion and Adaption for Visual Grounding(https://arxiv.org/abs/2502.16786)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Visual grounding aims to ground an image region through natural language, which heavily relies on cross-modal alignment. Most existing methods transfer visual/linguistic knowledge separately by fully fine-tuning uni-modal pre-trained models, followed by a simple stack of visual-language transformers for multimodal fusion. However, these approaches not only limit adequate interaction between visual and linguistic contexts, but also incur significant computational costs. Therefore, to address these issues, we explore a step-wise multimodal fusion and adaption framework, namely SwimVG. Specifically, SwimVG proposes step-wise multimodal prompts (Swip) and cross-modal interactive adapters (CIA) for visual grounding, replacing the cumbersome transformer stacks for multimodal fusion. Swip can improve {the} alignment between the vision and language representations step by step, in a token-level fusion manner. In addition, weight-level CIA further promotes multimodal fusion by cross-modal interaction. Swip and CIA are both parameter-efficient paradigms, and they fuse the cross-modal features from shallow to deep layers gradually. Experimental results on four widely-used benchmarks demonstrate that SwimVG achieves remarkable abilities and considerable benefits in terms of efficiency. Our code is available at this https URL.</li>
</ul>

<h3>Title: Are Large Language Models Good Data Preprocessors?</h3>
<ul>
<li><strong>Authors: </strong>Elyas Meguellati, Nardiena Pratama, Shazia Sadiq, Gianluca Demartini</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16790">https://arxiv.org/abs/2502.16790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16790">https://arxiv.org/pdf/2502.16790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16790]] Are Large Language Models Good Data Preprocessors?(https://arxiv.org/abs/2502.16790)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>High-quality textual training data is essential for the success of multimodal data processing tasks, yet outputs from image captioning models like BLIP and GIT often contain errors and anomalies that are difficult to rectify using rule-based methods. While recent work addressing this issue has predominantly focused on using GPT models for data preprocessing on relatively simple public datasets, there is a need to explore a broader range of Large Language Models (LLMs) and tackle more challenging and diverse datasets. In this study, we investigate the use of multiple LLMs, including LLaMA 3.1 70B, GPT-4 Turbo, and Sonnet 3.5 v2, to refine and clean the textual outputs of BLIP and GIT. We assess the impact of LLM-assisted data cleaning by comparing downstream-task (SemEval 2024 Subtask "Multilabel Persuasion Detection in Memes") models trained on cleaned versus non-cleaned data. While our experimental results show improvements when using LLM-cleaned captions, statistical tests reveal that most of these improvements are not significant. This suggests that while LLMs have the potential to enhance data cleaning and repairing, their effectiveness may be limited depending on the context they are applied to, the complexity of the task, and the level of noise in the text. Our findings highlight the need for further research into the capabilities and limitations of LLMs in data preprocessing pipelines, especially when dealing with challenging datasets, contributing empirical evidence to the ongoing discussion about integrating LLMs into data preprocessing pipelines.</li>
</ul>

<h3>Title: The Role of Sparsity for Length Generalization in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Noah Golowich, Samy Jelassi, David Brandfonbrener, Sham M. Kakade, Eran Malach</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16792">https://arxiv.org/abs/2502.16792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16792">https://arxiv.org/pdf/2502.16792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16792]] The Role of Sparsity for Length Generalization in Transformers(https://arxiv.org/abs/2502.16792)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Training large language models to predict beyond their training context lengths has drawn much attention in recent years, yet the principles driving such behavior of length generalization remain underexplored. We propose a new theoretical framework to study length generalization for the next-token prediction task, as performed by decoder-only transformers. Conceptually, we show that length generalization occurs as long as each predicted token depends on a small (fixed) number of previous tokens. We formalize such tasks via a notion we call $k$-sparse planted correlation distributions, and show that an idealized model of transformers which generalize attention heads successfully length-generalize on such tasks. As a bonus, our theoretical model justifies certain techniques to modify positional embeddings which have been introduced to improve length generalization, such as position coupling. We support our theoretical results with experiments on synthetic tasks and natural language, which confirm that a key factor driving length generalization is a ``sparse'' dependency structure of each token on the previous ones. Inspired by our theory, we introduce Predictive Position Coupling, which trains the transformer to predict the position IDs used in a positional coupling approach. Predictive Position Coupling thereby allows us to broaden the array of tasks to which position coupling can successfully be applied to achieve length generalization.</li>
</ul>

<h3>Title: VGFL-SA: Vertical Graph Federated Learning Structure Attack Based on Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Yang Chen, Bin Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16793">https://arxiv.org/abs/2502.16793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16793">https://arxiv.org/pdf/2502.16793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16793]] VGFL-SA: Vertical Graph Federated Learning Structure Attack Based on Contrastive Learning(https://arxiv.org/abs/2502.16793)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have gained attention for their ability to learn representations from graph data. Due to privacy concerns and conflicts of interest that prevent clients from directly sharing graph data with one another, Vertical Graph Federated Learning (VGFL) frameworks have been developed. Recent studies have shown that VGFL is vulnerable to adversarial attacks that degrade performance. However, it is a common problem that client nodes are often unlabeled in the realm of VGFL. Consequently, the existing attacks, which rely on the availability of labeling information to obtain gradients, are inherently constrained in their applicability. This limitation precludes their deployment in practical, real-world environments. To address the above problems, we propose a novel graph adversarial attack against VGFL, referred to as VGFL-SA, to degrade the performance of VGFL by modifying the local clients structure without using labels. Specifically, VGFL-SA uses a contrastive learning method to complete the attack before the local clients are trained. VGFL-SA first accesses the graph structure and node feature information of the poisoned clients, and generates the contrastive views by node-degree-based edge augmentation and feature shuffling augmentation. Then, VGFL-SA uses the shared graph encoder to get the embedding of each view, and the gradients of the adjacency matrices are obtained by the contrastive function. Finally, perturbed edges are generated using gradient modification rules. We validated the performance of VGFL-SA by performing a node classification task on real-world datasets, and the results show that VGFL-SA achieves good attack effectiveness and transferability.</li>
</ul>

<h3>Title: Hierarchical Semantic Compression for Consistent Image Semantic Restoration</h3>
<ul>
<li><strong>Authors: </strong>Shengxi Li, Zifu Zhang, Mai Xu, Lai Jiang, Yufan Liu, Ce Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16799">https://arxiv.org/abs/2502.16799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16799">https://arxiv.org/pdf/2502.16799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16799]] Hierarchical Semantic Compression for Consistent Image Semantic Restoration(https://arxiv.org/abs/2502.16799)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The emerging semantic compression has been receiving increasing research efforts most recently, capable of achieving high fidelity restoration during compression, even at extremely low bitrates. However, existing semantic compression methods typically combine standard pipelines with either pre-defined or high-dimensional semantics, thus suffering from deficiency in compression. To address this issue, we propose a novel hierarchical semantic compression (HSC) framework that purely operates within intrinsic semantic spaces from generative models, which is able to achieve efficient compression for consistent semantic restoration. More specifically, we first analyse the entropy models for the semantic compression, which motivates us to employ a hierarchical architecture based on a newly developed general inversion encoder. Then, we propose the feature compression network (FCN) and semantic compression network (SCN), such that the middle-level semantic feature and core semantics are hierarchically compressed to restore both accuracy and consistency of image semantics, via an entropy model progressively shared by channel-wise context. Experimental results demonstrate that the proposed HSC framework achieves the state-of-the-art performance on subjective quality and consistency for human vision, together with superior performances on machine vision tasks given compressed bitstreams. This essentially coincides with human visual system in understanding images, thus providing a new framework for future image/video compression paradigms. Our code shall be released upon acceptance.</li>
</ul>

<h3>Title: Unsupervised Topic Models are Data Mixers for Pre-training Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Peng, Xinlin Zhuang, Qiu Jiantao, Ren Ma, Jing Yu, Tianyi Bai, Conghui He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16802">https://arxiv.org/abs/2502.16802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16802">https://arxiv.org/pdf/2502.16802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16802]] Unsupervised Topic Models are Data Mixers for Pre-training Language Models(https://arxiv.org/abs/2502.16802)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The performance of large language models (LLMs) is significantly affected by the quality and composition of their pre-training data, which is inherently diverse, spanning various domains, sources, and topics. Effectively integrating these heterogeneous data sources is crucial for optimizing LLM performance. Previous research has predominantly concentrated on domain-based data mixing, often neglecting the nuanced topic-level characteristics of the data. To address this gap, we propose a simple yet effective topic-based data mixing strategy that utilizes fine-grained topics generated through our topic modeling method, DataWeave. DataWeave employs a multi-stage clustering process to group semantically similar documents and utilizes LLMs to generate detailed topics, thereby facilitating a more nuanced understanding of dataset composition. Our strategy employs heuristic methods to upsample or downsample specific topics, which significantly enhances LLM performance on downstream tasks, achieving superior results compared to previous, more complex data mixing approaches. Furthermore, we confirm that the topics Science and Relationships are particularly effective, yielding the most substantial performance improvements. We will make our code and datasets publicly available.</li>
</ul>

<h3>Title: CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport Alignment for Language Models with Different Tokenizers</h3>
<ul>
<li><strong>Authors: </strong>Anh Duc Le, Tu Vu, Nam Le Hai, Nguyen Thi Ngoc Diep, Linh Ngo Van, Trung Le, Thien Huu Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16806">https://arxiv.org/abs/2502.16806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16806">https://arxiv.org/pdf/2502.16806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16806]] CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport Alignment for Language Models with Different Tokenizers(https://arxiv.org/abs/2502.16806)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) achieve state-of-the-art performance across various NLP tasks but face deployment challenges due to high computational costs and memory constraints. Knowledge distillation (KD) is a promising solution, transferring knowledge from large teacher models to smaller student models. However, existing KD methods often assume shared vocabularies and tokenizers, limiting their flexibility. While approaches like Universal Logit Distillation (ULD) and Dual-Space Knowledge Distillation (DSKD) address vocabulary mismatches, they overlook the critical \textbf{reasoning-aware distillation} aspect. To bridge this gap, we propose CoT2Align a universal KD framework that integrates Chain-of-Thought (CoT) augmentation and introduces Cross-CoT Alignment to enhance reasoning transfer. Additionally, we extend Optimal Transport beyond token-wise alignment to a sequence-level and layer-wise alignment approach that adapts to varying sequence lengths while preserving contextual integrity. Comprehensive experiments demonstrate that CoT2Align outperforms existing KD methods across different vocabulary settings, improving reasoning capabilities and robustness in domain-specific tasks.</li>
</ul>

<h3>Title: CLIP-SENet: CLIP-based Semantic Enhancement Network for Vehicle Re-identification</h3>
<ul>
<li><strong>Authors: </strong>Liping Lu, Zihao Fu, Duanfeng Chu, Wei Wang, Bingrong Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16815">https://arxiv.org/abs/2502.16815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16815">https://arxiv.org/pdf/2502.16815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16815]] CLIP-SENet: CLIP-based Semantic Enhancement Network for Vehicle Re-identification(https://arxiv.org/abs/2502.16815)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vehicle re-identification (Re-ID) is a crucial task in intelligent transportation systems (ITS), aimed at retrieving and matching the same vehicle across different surveillance cameras. Numerous studies have explored methods to enhance vehicle Re-ID by focusing on semantic enhancement. However, these methods often rely on additional annotated information to enable models to extract effective semantic features, which brings many limitations. In this work, we propose a CLIP-based Semantic Enhancement Network (CLIP-SENet), an end-to-end framework designed to autonomously extract and refine vehicle semantic attributes, facilitating the generation of more robust semantic feature representations. Inspired by zero-shot solutions for downstream tasks presented by large-scale vision-language models, we leverage the powerful cross-modal descriptive capabilities of the CLIP image encoder to initially extract general semantic information. Instead of using a text encoder for semantic alignment, we design an adaptive fine-grained enhancement module (AFEM) to adaptively enhance this general semantic information at a fine-grained level to obtain robust semantic feature representations. These features are then fused with common Re-ID appearance features to further refine the distinctions between vehicles. Our comprehensive evaluation on three benchmark datasets demonstrates the effectiveness of CLIP-SENet. Our approach achieves new state-of-the-art performance, with 92.9% mAP and 98.7% Rank-1 on VeRi-776 dataset, 90.4% Rank-1 and 98.7% Rank-5 on VehicleID dataset, and 89.1% mAP and 97.9% Rank-1 on the more challenging VeRi-Wild dataset.</li>
</ul>

<h3>Title: Fast, Accurate Manifold Denoising by Tunneling Riemannian Optimization</h3>
<ul>
<li><strong>Authors: </strong>Shiyu Wang, Mariam Avagyan, Yihan Shen, Arnaud Lamy, Tingran Wang, Szabolcs Márka, Zsuzsa Márka, John Wright</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16819">https://arxiv.org/abs/2502.16819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16819">https://arxiv.org/pdf/2502.16819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16819]] Fast, Accurate Manifold Denoising by Tunneling Riemannian Optimization(https://arxiv.org/abs/2502.16819)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion</a></li>
<li><strong>Abstract: </strong>Learned denoisers play a fundamental role in various signal generation (e.g., diffusion models) and reconstruction (e.g., compressed sensing) architectures, whose success derives from their ability to leverage low-dimensional structure in data. Existing denoising methods, however, either rely on local approximations that require a linear scan of the entire dataset or treat denoising as generic function approximation problems, often sacrificing efficiency and interpretability. We consider the problem of efficiently denoising a new noisy data point sampled from an unknown $d$-dimensional manifold $M \in \mathbb{R}^D$, using only noisy samples. This work proposes a framework for test-time efficient manifold denoising, by framing the concept of "learning-to-denoise" as "learning-to-optimize". We have two technical innovations: (i) online learning methods which learn to optimize over the manifold of clean signals using only noisy data, effectively "growing" an optimizer one sample at a time. (ii) mixed-order methods which guarantee that the learned optimizers achieve global optimality, ensuring both efficiency and near-optimal denoising performance. We corroborate these claims with theoretical analyses of both the complexity and denoising performance of mixed-order traversal. Our experiments on scientific manifolds demonstrate significantly improved complexity-performance tradeoffs compared to nearest neighbor search, which underpins existing provable denoising approaches based on exhaustive search.</li>
</ul>

<h3>Title: Uncertainty Quantification of Large Language Models through Multi-Dimensional Responses</h3>
<ul>
<li><strong>Authors: </strong>Tiejin Chen, Xiaoou Liu, Longchao Da, Xiaoou Liu, Vagelis Papalexakis, Hua Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16820">https://arxiv.org/abs/2502.16820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16820">https://arxiv.org/pdf/2502.16820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16820]] Uncertainty Quantification of Large Language Models through Multi-Dimensional Responses(https://arxiv.org/abs/2502.16820)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks due to large training datasets and powerful transformer architecture. However, the reliability of responses from LLMs remains a question. Uncertainty quantification (UQ) of LLMs is crucial for ensuring their reliability, especially in areas such as healthcare, finance, and decision-making. Existing UQ methods primarily focus on semantic similarity, overlooking the deeper knowledge dimensions embedded in responses. We introduce a multi-dimensional UQ framework that integrates semantic and knowledge-aware similarity analysis. By generating multiple responses and leveraging auxiliary LLMs to extract implicit knowledge, we construct separate similarity matrices and apply tensor decomposition to derive a comprehensive uncertainty representation. This approach disentangles overlapping information from both semantic and knowledge dimensions, capturing both semantic variations and factual consistency, leading to more accurate UQ. Our empirical evaluations demonstrate that our method outperforms existing techniques in identifying uncertain responses, offering a more robust framework for enhancing LLM reliability in high-stakes applications.</li>
</ul>

<h3>Title: Posterior Inference with Diffusion Models for High-dimensional Black-box Optimization</h3>
<ul>
<li><strong>Authors: </strong>Taeyoung Yun, Kiyoung Om, Jaewoo Lee, Sujin Yun, Jinkyoo Park</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16824">https://arxiv.org/abs/2502.16824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16824">https://arxiv.org/pdf/2502.16824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16824]] Posterior Inference with Diffusion Models for High-dimensional Black-box Optimization(https://arxiv.org/abs/2502.16824)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Optimizing high-dimensional and complex black-box functions is crucial in numerous scientific applications. While Bayesian optimization (BO) is a powerful method for sample-efficient optimization, it struggles with the curse of dimensionality and scaling to thousands of evaluations. Recently, leveraging generative models to solve black-box optimization problems has emerged as a promising framework. However, those methods often underperform compared to BO methods due to limited expressivity and difficulty of uncertainty estimation in high-dimensional spaces. To overcome these issues, we introduce \textbf{DiBO}, a novel framework for solving high-dimensional black-box optimization problems. Our method iterates two stages. First, we train a diffusion model to capture the data distribution and an ensemble of proxies to predict function values with uncertainty quantification. Second, we cast the candidate selection as a posterior inference problem to balance exploration and exploitation in high-dimensional spaces. Concretely, we fine-tune diffusion models to amortize posterior inference. Extensive experiments demonstrate that our method outperforms state-of-the-art baselines across various synthetic and real-world black-box optimization tasks. Our code is publicly available \href{this https URL}{here}</li>
</ul>

<h3>Title: Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yao Xiao, Hai Ye, Linyao Chen, Hwee Tou Ng, Lidong Bing, Xiaoli Li, Roy Ka-wei Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16825">https://arxiv.org/abs/2502.16825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16825">https://arxiv.org/pdf/2502.16825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16825]] Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization(https://arxiv.org/abs/2502.16825)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Iterative data generation and model retraining are widely used to align large language models (LLMs). It typically involves a policy model to generate on-policy responses and a reward model to guide training data selection. Direct Preference Optimization (DPO) further enhances this process by constructing preference pairs of chosen and rejected responses. In this work, we aim to \emph{scale up} the number of on-policy samples via repeated random sampling to improve alignment performance. Conventional practice selects the sample with the highest reward as chosen and the lowest as rejected for DPO. However, our experiments reveal that this strategy leads to a \emph{decline} in performance as the sample size increases. To address this, we investigate preference data construction through the lens of underlying normal distribution of sample rewards. We categorize the reward space into seven representative points and systematically explore all 21 ($C_7^2$) pairwise combinations. Through evaluations on four models using AlpacaEval 2, we find that selecting the rejected response at reward position $\mu - 2\sigma$ rather than the minimum reward, is crucial for optimal performance. We finally introduce a scalable preference data construction strategy that consistently enhances model performance as the sample scale increases.</li>
</ul>

<h3>Title: FedBM: Stealing Knowledge from Pre-trained Language Models for Heterogeneous Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Meilu Zhu, Qiushi Yang, Zhifan Gao, Yixuan Yuan, Jun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16832">https://arxiv.org/abs/2502.16832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16832">https://arxiv.org/pdf/2502.16832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16832]] FedBM: Stealing Knowledge from Pre-trained Language Models for Heterogeneous Federated Learning(https://arxiv.org/abs/2502.16832)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, steal, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) has shown great potential in medical image computing since it provides a decentralized learning paradigm that allows multiple clients to train a model collaboratively without privacy leakage. However, current studies have shown that data heterogeneity incurs local learning bias in classifiers and feature extractors of client models during local training, leading to the performance degradation of a federation system. To address these issues, we propose a novel framework called Federated Bias eliMinating (FedBM) to get rid of local learning bias in heterogeneous federated learning (FL), which mainly consists of two modules, i.e., Linguistic Knowledge-based Classifier Construction (LKCC) and Concept-guided Global Distribution Estimation (CGDE). Specifically, LKCC exploits class concepts, prompts and pre-trained language models (PLMs) to obtain concept embeddings. These embeddings are used to estimate the latent concept distribution of each class in the linguistic space. Based on the theoretical derivation, we can rely on these distributions to pre-construct a high-quality classifier for clients to achieve classification optimization, which is frozen to avoid classifier bias during local training. CGDE samples probabilistic concept embeddings from the latent concept distributions to learn a conditional generator to capture the input space of the global model. Three regularization terms are introduced to improve the quality and utility of the generator. The generator is shared by all clients and produces pseudo data to calibrate updates of local feature extractors. Extensive comparison experiments and ablation studies on public datasets demonstrate the superior performance of FedBM over state-of-the-arts and confirm the effectiveness of each module, respectively. The code is available at this https URL.</li>
</ul>

<h3>Title: A Novel Multi-Task Teacher-Student Architecture with Self-Supervised Pretraining for 48-Hour Vasoactive-Inotropic Trend Analysis in Sepsis Mortality Prediction</h3>
<ul>
<li><strong>Authors: </strong>Houji Jin, Negin Ashrafi, Kamiar Alaei, Elham Pishgar, Greg Placencia, Maryam Pishgar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16834">https://arxiv.org/abs/2502.16834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16834">https://arxiv.org/pdf/2502.16834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16834]] A Novel Multi-Task Teacher-Student Architecture with Self-Supervised Pretraining for 48-Hour Vasoactive-Inotropic Trend Analysis in Sepsis Mortality Prediction(https://arxiv.org/abs/2502.16834)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Sepsis is a major cause of ICU mortality, where early recognition and effective interventions are essential for improving patient outcomes. However, the vasoactive-inotropic score (VIS) varies dynamically with a patient's hemodynamic status, complicated by irregular medication patterns, missing data, and confounders, making sepsis prediction challenging. To address this, we propose a novel Teacher-Student multitask framework with self-supervised VIS pretraining via a Masked Autoencoder (MAE). The teacher model performs mortality classification and severity-score regression, while the student distills robust time-series representations, enhancing adaptation to heterogeneous VIS data. Compared to LSTM-based methods, our approach achieves an AUROC of 0.82 on MIMIC-IV 3.0 (9,476 patients), outperforming the baseline (0.74). SHAP analysis revealed that SOFA score (0.147) had the greatest impact on ICU mortality, followed by LODS (0.033), single marital status (0.031), and Medicaid insurance (0.023), highlighting the role of sociodemographic factors. SAPSII (0.020) also contributed significantly. These findings suggest that both clinical and social factors should be considered in ICU decision-making. Our novel multitask and distillation strategies enable earlier identification of high-risk patients, improving prediction accuracy and disease management, offering new tools for ICU decision support.</li>
</ul>

<h3>Title: Detecting Code Vulnerabilities with Heterogeneous GNN Training</h3>
<ul>
<li><strong>Authors: </strong>Yu Luo, Weifeng Xu, Dianxiang Xu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16835">https://arxiv.org/abs/2502.16835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16835">https://arxiv.org/pdf/2502.16835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16835]] Detecting Code Vulnerabilities with Heterogeneous GNN Training(https://arxiv.org/abs/2502.16835)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Detecting vulnerabilities in source code is a critical task for software security assurance. Graph Neural Network (GNN) machine learning can be a promising approach by modeling source code as graphs. Early approaches treated code elements uniformly, limiting their capacity to model diverse relationships that contribute to various vulnerabilities. Recent research addresses this limitation by considering the heterogeneity of node types and using Gated Graph Neural Networks (GGNN) to aggregate node information through different edge types. However, these edges primarily function as conduits for passing node information and may not capture detailed characteristics of distinct edge types. This paper presents Inter-Procedural Abstract Graphs (IPAGs) as an efficient, language-agnostic representation of source code, complemented by heterogeneous GNN training for vulnerability prediction. IPAGs capture the structural and contextual properties of code elements and their relationships. We also propose a Heterogeneous Attention GNN (HAGNN) model that incorporates multiple subgraphs capturing different features of source code. These subgraphs are learned separately and combined using a global attention mechanism, followed by a fully connected neural network for final classification. The proposed approach has achieved up to 96.6% accuracy on a large C dataset of 108 vulnerability types and 97.8% on a large Java dataset of 114 vulnerability types, outperforming state-of-the-art methods. Its applications to various real-world software projects have also demonstrated low false positive rates.</li>
</ul>

<h3>Title: REGen: A Reliable Evaluation Framework for Generative Event Argument Extraction</h3>
<ul>
<li><strong>Authors: </strong>Omar Sharif, Joseph Gatto, Madhusudan Basak, Sarah M. Preum</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16838">https://arxiv.org/abs/2502.16838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16838">https://arxiv.org/pdf/2502.16838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16838]] REGen: A Reliable Evaluation Framework for Generative Event Argument Extraction(https://arxiv.org/abs/2502.16838)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>Event argument extraction identifies arguments for predefined event roles in text. Traditional evaluations rely on exact match (EM), requiring predicted arguments to match annotated spans exactly. However, this approach fails for generative models like large language models (LLMs), which produce diverse yet semantically accurate responses. EM underestimates performance by disregarding valid variations, implicit arguments (unstated but inferable), and scattered arguments (distributed across a document). To bridge this gap, we introduce Reliable Evaluation framework for Generative event argument extraction (REGen), a framework that better aligns with human judgment. Across six datasets, REGen improves performance by an average of 23.93 F1 points over EM. Human validation further confirms REGen's effectiveness, achieving 87.67% alignment with human assessments of argument correctness.</li>
</ul>

<h3>Title: "Actionable Help" in Crises: A Novel Dataset and Resource-Efficient Models for Identifying Request and Offer Social Media Posts</h3>
<ul>
<li><strong>Authors: </strong>Rabindra Lamsal, Maria Rodriguez Read, Shanika Karunasekera, Muhammad Imran</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16839">https://arxiv.org/abs/2502.16839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16839">https://arxiv.org/pdf/2502.16839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16839]] "Actionable Help" in Crises: A Novel Dataset and Resource-Efficient Models for Identifying Request and Offer Social Media Posts(https://arxiv.org/abs/2502.16839)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>During crises, social media serves as a crucial coordination tool, but the vast influx of posts--from "actionable" requests and offers to generic content like emotional support, behavioural guidance, or outdated information--complicates effective classification. Although generative LLMs (Large Language Models) can address this issue with few-shot classification, their high computational demands limit real-time crisis response. While fine-tuning encoder-only models (e.g., BERT) is a popular choice, these models still exhibit higher inference times in resource-constrained environments. Moreover, although distilled variants (e.g., DistilBERT) exist, they are not tailored for the crisis domain. To address these challenges, we make two key contributions. First, we present CrisisHelpOffer, a novel dataset of 101k tweets collaboratively labelled by generative LLMs and validated by humans, specifically designed to distinguish actionable content from noise. Second, we introduce the first crisis-specific mini models optimized for deployment in resource-constrained settings. Across 13 crisis classification tasks, our mini models surpass BERT (also outperform or match the performance of RoBERTa, MPNet, and BERTweet), offering higher accuracy with significantly smaller sizes and faster speeds. The Medium model is 47% smaller with 3.8% higher accuracy at 3.5x speed, the Small model is 68% smaller with a 1.8% accuracy gain at 7.7x speed, and the Tiny model, 83% smaller, matches BERT's accuracy at 18.6x speed. All models outperform existing distilled variants, setting new benchmarks. Finally, as a case study, we analyze social media posts from a global crisis to explore help-seeking and assistance-offering behaviours in selected developing and developed countries.</li>
</ul>

<h3>Title: In-context learning of evolving data streams with tabular foundational models</h3>
<ul>
<li><strong>Authors: </strong>Afonso Lourenço, João Gama, Eric P. Xing, Goreti Marreiros</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16840">https://arxiv.org/abs/2502.16840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16840">https://arxiv.org/pdf/2502.16840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16840]] In-context learning of evolving data streams with tabular foundational models(https://arxiv.org/abs/2502.16840)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>State-of-the-art data stream mining in supervised classification has traditionally relied on ensembles of incremental decision trees. However, the emergence of large tabular models, i.e., transformers designed for structured numerical data, marks a significant paradigm shift. These models move beyond traditional weight updates, instead employing in-context learning through prompt tuning. By using on-the-fly sketches to summarize unbounded streaming data, one can feed this information into a pre-trained model for efficient processing. This work bridges advancements from both areas, highlighting how transformers' implicit meta-learning abilities, pre-training on drifting natural data, and reliance on context optimization directly address the core challenges of adaptive learning in dynamic environments. Exploring real-time model adaptation, this research demonstrates that TabPFN, coupled with a simple sliding memory strategy, consistently outperforms ensembles of Hoeffding trees across all non-stationary benchmarks. Several promising research directions are outlined in the paper. The authors urge the community to explore these ideas, offering valuable opportunities to advance in-context stream learning.</li>
</ul>

<h3>Title: Fair Foundation Models for Medical Image Analysis: Challenges and Perspectives</h3>
<ul>
<li><strong>Authors: </strong>Dilermando Queiroz, Anderson Carlos, André Anjos, Lilian Berton</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16841">https://arxiv.org/abs/2502.16841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16841">https://arxiv.org/pdf/2502.16841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16841]] Fair Foundation Models for Medical Image Analysis: Challenges and Perspectives(https://arxiv.org/abs/2502.16841)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Ensuring equitable Artificial Intelligence (AI) in healthcare demands systems that make unbiased decisions across all demographic groups, bridging technical innovation with ethical principles. Foundation Models (FMs), trained on vast datasets through self-supervised learning, enable efficient adaptation across medical imaging tasks while reducing dependency on labeled data. These models demonstrate potential for enhancing fairness, though significant challenges remain in achieving consistent performance across demographic groups. Our review indicates that effective bias mitigation in FMs requires systematic interventions throughout all stages of development. While previous approaches focused primarily on model-level bias mitigation, our analysis reveals that fairness in FMs requires integrated interventions throughout the development pipeline, from data documentation to deployment protocols. This comprehensive framework advances current knowledge by demonstrating how systematic bias mitigation, combined with policy engagement, can effectively address both technical and institutional barriers to equitable AI in healthcare. The development of equitable FMs represents a critical step toward democratizing advanced healthcare technologies, particularly for underserved populations and regions with limited medical infrastructure and computational resources.</li>
</ul>

<h3>Title: Exploring Causes and Mitigation of Hallucinations in Large Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yaqi Sun, Kyohei Atarashi, Koh Takeuchi, Hisashi Kashima</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16842">https://arxiv.org/abs/2502.16842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16842">https://arxiv.org/pdf/2502.16842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16842]] Exploring Causes and Mitigation of Hallucinations in Large Vision Language Models(https://arxiv.org/abs/2502.16842)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) integrate image encoders with Large Language Models (LLMs) to process multi-modal inputs and perform complex visual tasks. However, they often generate hallucinations by describing non-existent objects or attributes, compromising their reliability. This study analyzes hallucination patterns in image captioning, showing that not all tokens in the generation process are influenced by image input and that image dependency can serve as a useful signal for hallucination detection. To address this, we develop an automated pipeline to identify hallucinated objects and train a token-level classifier using hidden representations from parallel inference passes-with and without image input. Leveraging this classifier, we introduce a decoding strategy that effectively controls hallucination rates in image captioning at inference time.</li>
</ul>

<h3>Title: Improving LLM General Preference Alignment via Optimistic Online Mirror Descent</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Zhang, Dian Yu, Tao Ge, Linfeng Song, Zhichen Zeng, Haitao Mi, Nan Jiang, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16852">https://arxiv.org/abs/2502.16852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16852">https://arxiv.org/pdf/2502.16852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16852]] Improving LLM General Preference Alignment via Optimistic Online Mirror Descent(https://arxiv.org/abs/2502.16852)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) has demonstrated remarkable effectiveness in aligning large language models (LLMs) with human preferences. Many existing alignment approaches rely on the Bradley-Terry (BT) model assumption, which assumes the existence of a ground-truth reward for each prompt-response pair. However, this assumption can be overly restrictive when modeling complex human preferences. In this paper, we drop the BT model assumption and study LLM alignment under general preferences, formulated as a two-player game. Drawing on theoretical insights from learning in games, we integrate optimistic online mirror descent into our alignment framework to approximate the Nash policy. Theoretically, we demonstrate that our approach achieves an $O(T^{-1})$ bound on the duality gap, improving upon the previous $O(T^{-1/2})$ result. More importantly, we implement our method and show through experiments that it outperforms state-of-the-art RLHF algorithms across multiple representative benchmarks.</li>
</ul>

<h3>Title: Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data and an Ensemble of DeBERTa Models</h3>
<ul>
<li><strong>Authors: </strong>Avinash Trivedi, Sangeetha Sivanesan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16857">https://arxiv.org/abs/2502.16857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16857">https://arxiv.org/pdf/2502.16857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16857]] Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data and an Ensemble of DeBERTa Models(https://arxiv.org/abs/2502.16857)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents an effective approach to detect AI-generated text, developed for the Defactify 4.0 shared task at the fourth workshop on multimodal fact checking and hate speech detection. The task consists of two subtasks: Task-A, classifying whether a text is AI generated or human written, and Task-B, classifying the specific large language model that generated the text. Our team (Sarang) achieved the 1st place in both tasks with F1 scores of 1.0 and 0.9531, respectively. The methodology involves adding noise to the dataset to improve model robustness and generalization. We used an ensemble of DeBERTa models to effectively capture complex patterns in the text. The result indicates the effectiveness of our noise-driven and ensemble-based approach, setting a new standard in AI-generated text detection and providing guidance for future developments.</li>
</ul>

<h3>Title: LongAttn: Selecting Long-context Training Data via Token-level Attention</h3>
<ul>
<li><strong>Authors: </strong>Longyun Wu, Dawei Zhu, Guangxiang Zhao, Zhuocheng Yu, Junfeng Ran, Xiangyu Wong, Lin Sun, Sujian Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16860">https://arxiv.org/abs/2502.16860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16860">https://arxiv.org/pdf/2502.16860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16860]] LongAttn: Selecting Long-context Training Data via Token-level Attention(https://arxiv.org/abs/2502.16860)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the development of large language models (LLMs), there has been an increasing need for significant advancements in handling long contexts. To enhance long-context capabilities, constructing high-quality training data with long-range dependencies is crucial. Existing methods to select long-context data often rely on sentence-level analysis, which can be greatly optimized in both performance and efficiency. In this paper, we propose a novel token-level framework, LongAttn, which leverages the self-attention mechanism of LLMs to measure the long-range dependencies for the data. By calculating token-level dependency strength and distribution uniformity of token scores, LongAttn effectively quantifies long-range dependencies, enabling more accurate and efficient data selection. We filter LongABC-32K from open-source long-context datasets (ArXiv, Book, and Code). Through our comprehensive experiments, LongAttn has demonstrated its excellent effectiveness, scalability, and efficiency. To facilitate future research in long-context data, we released our code and the high-quality long-context training data LongABC-32K.</li>
</ul>

<h3>Title: Distributionally Robust Active Learning for Gaussian Process Regression</h3>
<ul>
<li><strong>Authors: </strong>Shion Takeno, Yoshito Okura, Yu Inatsu, Aoyama Tatsuya, Tomonari Tanaka, Akahane Satoshi, Hiroyuki Hanada, Noriaki Hashimoto, Taro Murayama, Hanju Lee, Shinya Kojima, Ichiro Takeuchi</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16870">https://arxiv.org/abs/2502.16870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16870">https://arxiv.org/pdf/2502.16870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16870]] Distributionally Robust Active Learning for Gaussian Process Regression(https://arxiv.org/abs/2502.16870)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Gaussian process regression (GPR) or kernel ridge regression is a widely used and powerful tool for nonlinear prediction. Therefore, active learning (AL) for GPR, which actively collects data labels to achieve an accurate prediction with fewer data labels, is an important problem. However, existing AL methods do not theoretically guarantee prediction accuracy for target distribution. Furthermore, as discussed in the distributionally robust learning literature, specifying the target distribution is often difficult. Thus, this paper proposes two AL methods that effectively reduce the worst-case expected error for GPR, which is the worst-case expectation in target distribution candidates. We show an upper bound of the worst-case expected squared error, which suggests that the error will be arbitrarily small by a finite number of data labels under mild conditions. Finally, we demonstrate the effectiveness of the proposed methods through synthetic and real-world datasets.</li>
</ul>

<h3>Title: Mitigating Hallucinations in Diffusion Models through Adaptive Attention Modulation</h3>
<ul>
<li><strong>Authors: </strong>Trevine Oorloff, Yaser Yacoob, Abhinav Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16872">https://arxiv.org/abs/2502.16872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16872">https://arxiv.org/pdf/2502.16872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16872]] Mitigating Hallucinations in Diffusion Models through Adaptive Attention Modulation(https://arxiv.org/abs/2502.16872)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models, while increasingly adept at generating realistic images, are notably hindered by hallucinations -- unrealistic or incorrect features inconsistent with the trained data distribution. In this work, we propose Adaptive Attention Modulation (AAM), a novel approach to mitigate hallucinations by analyzing and modulating the self-attention mechanism in diffusion models. We hypothesize that self-attention during early denoising steps may inadvertently amplify or suppress features, contributing to hallucinations. To counter this, AAM introduces a temperature scaling mechanism within the softmax operation of the self-attention layers, dynamically modulating the attention distribution during inference. Additionally, AAM employs a masked perturbation technique to disrupt early-stage noise that may otherwise propagate into later stages as hallucinations. Extensive experiments demonstrate that AAM effectively reduces hallucinatory artifacts, enhancing both the fidelity and reliability of generated images. For instance, the proposed approach improves the FID score by 20.8% and reduces the percentage of hallucinated images by 12.9% (in absolute terms) on the Hands dataset.</li>
</ul>

<h3>Title: CORAL: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter</h3>
<ul>
<li><strong>Authors: </strong>Yepeng Weng, Dianwen Mei, Huishi Qiu, Xujie Chen, Li Liu, Jiang Tian, Zhongchao Shi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16880">https://arxiv.org/abs/2502.16880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16880">https://arxiv.org/pdf/2502.16880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16880]] CORAL: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter(https://arxiv.org/abs/2502.16880)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding is a powerful technique that accelerates Large Language Model (LLM) inference by leveraging a lightweight speculative draft model. However, existing designs suffers in performance due to misalignment between training and inference. Recent methods have tried to solve this issue by adopting a multi-step training strategy, but the complex inputs of different training steps make it harder for the draft model to converge. To address this, we propose CORAL, a novel framework that improves both accuracy and efficiency in speculative drafting. CORAL introduces Cross-Step Representation Alignment, a method that enhances consistency across multiple training steps, significantly improving speculative drafting performance. Additionally, we identify the LM head as a major bottleneck in the inference speed of the draft model. We introduce a weight-grouping mechanism that selectively activates a subset of LM head parameters during inference, substantially reducing the latency of the draft model. We evaluate CORAL on three LLM families and three benchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming state-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that CORAL effectively mitigates training-inference misalignment and delivers significant speedup for modern LLMs with large vocabularies.</li>
</ul>

<h3>Title: DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance</h3>
<ul>
<li><strong>Authors: </strong>Xuanfan Ni, Liyan Xu, Chenyang Lyu, Longyue Wang, Mo Yu, Lemao Liu, Fandong Meng, Jie Zhou, Piji Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16886">https://arxiv.org/abs/2502.16886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16886">https://arxiv.org/pdf/2502.16886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16886]] DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance(https://arxiv.org/abs/2502.16886)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>To alleviate memory burden during inference of large language models (LLMs), numerous studies have focused on compressing the KV cache by exploring aspects such as attention sparsity. However, these techniques often require a pre-defined cache budget; as the optimal budget varies with different input lengths and task types, it limits their practical deployment accepting open-domain instructions. To address this limitation, we propose a new KV cache compression objective: to always ensure the full-cache performance regardless of specific inputs, while maximizing KV cache pruning as much as possible. To achieve this goal, we introduce a novel KV cache compression method dubbed DBudgetKV, which features an attention-based metric to signal when the remaining KV cache is unlikely to match the full-cache performance, then halting the pruning process. Empirical evaluation spanning diverse context lengths, task types, and model sizes suggests that our method achieves lossless KV pruning effectively and robustly, exceeding 25% compression ratio on average. Furthermore, our method is easy to integrate within LLM inference, not only optimizing memory space, but also showing reduced inference time compared to existing methods.</li>
</ul>

<h3>Title: ReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Guoqi Yu, Yaoming Li, Juncheng Wang, Xiaoyu Guo, Angelica I. Aviles-Rivero, Tong Yang, Shujun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16890">https://arxiv.org/abs/2502.16890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16890">https://arxiv.org/pdf/2502.16890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16890]] ReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting(https://arxiv.org/abs/2502.16890)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advancements have progressively incorporated frequency-based techniques into deep learning models, leading to notable improvements in accuracy and efficiency for time series analysis tasks. However, the Mid-Frequency Spectrum Gap in the real-world time series, where the energy is concentrated at the low-frequency region while the middle-frequency band is negligible, hinders the ability of existing deep learning models to extract the crucial frequency information. Additionally, the shared Key-Frequency in multivariate time series, where different time series share indistinguishable frequency patterns, is rarely exploited by existing literature. This work introduces a novel module, Adaptive Mid-Frequency Energy Optimizer, based on convolution and residual learning, to emphasize the significance of mid-frequency bands. We also propose an Energy-based Key-Frequency Picking Block to capture shared Key-Frequency, which achieves superior inter-series modeling performance with fewer parameters. A novel Key-Frequency Enhanced Training strategy is employed to further enhance Key-Frequency modeling, where spectral information from other channels is randomly introduced into each channel. Our approach advanced multivariate time series forecasting on the challenging Traffic, ECL, and Solar benchmarks, reducing MSE by 4%, 6%, and 5% compared to the previous SOTA iTransformer. Code is available at this GitHub Repository: this https URL.</li>
</ul>

<h3>Title: Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text Classification without Manually Labeled Data</h3>
<ul>
<li><strong>Authors: </strong>Yejian Zhang, Shingo Takada</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16892">https://arxiv.org/abs/2502.16892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16892">https://arxiv.org/pdf/2502.16892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16892]] Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text Classification without Manually Labeled Data(https://arxiv.org/abs/2502.16892)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Machine learning-based classifiers have been used for text classification, such as sentiment analysis, news classification, and toxic comment classification. However, supervised machine learning models often require large amounts of labeled data for training, and manual annotation is both labor-intensive and requires domain-specific knowledge, leading to relatively high annotation costs. To address this issue, we propose an approach that integrates large language models (LLMs) into an active learning framework. Our approach combines the Robustly Optimized BERT Pretraining Approach (RoBERTa), Generative Pre-trained Transformer (GPT), and active learning, achieving high cross-task text classification performance without the need for any manually labeled data. Furthermore, compared to directly applying GPT for classification tasks, our approach retains over 93% of its classification performance while requiring only approximately 6% of the computational time and monetary cost, effectively balancing performance and resource efficiency. These findings provide new insights into the efficient utilization of LLMs and active learning algorithms in text classification tasks, paving the way for their broader application.</li>
</ul>

<h3>Title: Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Fan, Zhenyi Lu, Sichen Liu, Xiaoye Qu, Wei Wei, Chengfeng Gu, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16894">https://arxiv.org/abs/2502.16894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16894">https://arxiv.org/pdf/2502.16894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16894]] Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment(https://arxiv.org/abs/2502.16894)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singular value decomposition (SVD) subsets, leading to suboptimal leveraging of pre-trained knowledge. Another path for improving LoRA is incorporating a Mixture-of-Experts (MoE) architecture. However, weight misalignment and complex gradient dynamics make it challenging to adopt SVD prior to the LoRA MoE architecture. To mitigate these issues, we propose \underline{G}reat L\underline{o}R\underline{A} Mixture-of-Exper\underline{t} (GOAT), a framework that (1) adaptively integrates relevant priors using an SVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by deriving a theoretical scaling factor. We demonstrate that proper scaling, without modifying the architecture or training algorithms, boosts LoRA MoE's efficiency and performance. Experiments across 25 datasets, including natural language understanding, commonsense reasoning, image classification, and natural language generation, demonstrate GOAT's state-of-the-art performance, closing the gap with Full FT.</li>
</ul>

<h3>Title: Zero-shot Load Forecasting for Integrated Energy Systems: A Large Language Model-based Framework with Multi-task Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiaheng Li, Donghe Li, Ye Yang, Huan Xi, Yu Xiao, Li Sun, Dou An, Qingyu Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16896">https://arxiv.org/abs/2502.16896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16896">https://arxiv.org/pdf/2502.16896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16896]] Zero-shot Load Forecasting for Integrated Energy Systems: A Large Language Model-based Framework with Multi-task Learning(https://arxiv.org/abs/2502.16896)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The growing penetration of renewable energy sources in power systems has increased the complexity and uncertainty of load forecasting, especially for integrated energy systems with multiple energy carriers. Traditional forecasting methods heavily rely on historical data and exhibit limited transferability across different scenarios, posing significant challenges for emerging applications in smart grids and energy internet. This paper proposes the TSLLM-Load Forecasting Mechanism, a novel zero-shot load forecasting framework based on large language models (LLMs) to address these challenges. The framework consists of three key components: a data preprocessing module that handles multi-source energy load data, a time series prompt generation module that bridges the semantic gap between energy data and LLMs through multi-task learning and similarity alignment, and a prediction module that leverages pre-trained LLMs for accurate forecasting. The framework's effectiveness was validated on a real-world dataset comprising load profiles from 20 Australian solar-powered households, demonstrating superior performance in both conventional and zero-shot scenarios. In conventional testing, our method achieved a Mean Squared Error (MSE) of 0.4163 and a Mean Absolute Error (MAE) of 0.3760, outperforming existing approaches by at least 8\%. In zero-shot prediction experiments across 19 households, the framework maintained consistent accuracy with a total MSE of 11.2712 and MAE of 7.6709, showing at least 12\% improvement over current methods. The results validate the framework's potential for accurate and transferable load forecasting in integrated energy systems, particularly beneficial for renewable energy integration and smart grid applications.</li>
</ul>

<h3>Title: Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs</h3>
<ul>
<li><strong>Authors: </strong>Himanshu Beniwal, Sailesh Panda, Mayank Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16901">https://arxiv.org/abs/2502.16901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16901">https://arxiv.org/pdf/2502.16901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16901]] Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs(https://arxiv.org/abs/2502.16901)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>We explore Cross-lingual Backdoor ATtacks (X-BAT) in multilingual Large Language Models (mLLMs), revealing how backdoors inserted in one language can automatically transfer to others through shared embedding spaces. Using toxicity classification as a case study, we demonstrate that attackers can compromise multilingual systems by poisoning data in a single language, with rare tokens serving as specific effective triggers. Our findings expose a critical vulnerability in the fundamental architecture that enables cross-lingual transfer in these models. Our code and data are publicly available at this https URL.</li>
</ul>

<h3>Title: Culture-TRIP: Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinment</h3>
<ul>
<li><strong>Authors: </strong>Suchae Jeong, Inseong Choi, Youngsik Yun, Jihie Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16902">https://arxiv.org/abs/2502.16902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16902">https://arxiv.org/pdf/2502.16902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16902]] Culture-TRIP: Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinment(https://arxiv.org/abs/2502.16902)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Text-to-Image models, including Stable Diffusion, have significantly improved in generating images that are highly semantically aligned with the given prompts. However, existing models may fail to produce appropriate images for the cultural concepts or objects that are not well known or underrepresented in western cultures, such as `hangari' (Korean utensil). In this paper, we propose a novel approach, Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinement (Culture-TRIP), which refines the prompt in order to improve the alignment of the image with such culture nouns in text-to-image models. Our approach (1) retrieves cultural contexts and visual details related to the culture nouns in the prompt and (2) iteratively refines and evaluates the prompt based on a set of cultural criteria and large language models. The refinement process utilizes the information retrieved from Wikipedia and the Web. Our user survey, conducted with 66 participants from eight different countries demonstrates that our proposed approach enhances the alignment between the images and the prompts. In particular, C-TRIP demonstrates improved alignment between the generated images and underrepresented culture nouns. Resource can be found at this https URL.</li>
</ul>

<h3>Title: GuidedBench: Equipping Jailbreak Evaluation with Guidelines</h3>
<ul>
<li><strong>Authors: </strong>Ruixuan Huang, Xunguang Wang, Zongjie Li, Daoyuan Wu, Shuai Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16903">https://arxiv.org/abs/2502.16903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16903">https://arxiv.org/pdf/2502.16903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16903]] GuidedBench: Equipping Jailbreak Evaluation with Guidelines(https://arxiv.org/abs/2502.16903)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Jailbreaking methods for large language models (LLMs) have gained increasing attention for building safe and responsible AI systems. After analyzing 35 jailbreak methods across six categories, we find that existing benchmarks, relying on universal LLM-based or keyword-matching scores, lack case-specific criteria, leading to conflicting results. In this paper, we introduce a more robust evaluation framework for jailbreak methods, with a curated harmful question dataset, detailed case-by-case evaluation guidelines, and a scoring system equipped with these guidelines. Our experiments show that existing jailbreak methods exhibit better discrimination when evaluated using our benchmark. Some jailbreak methods that claim to achieve over 90% attack success rate (ASR) on other benchmarks only reach a maximum of 30.2% on our benchmark, providing a higher ceiling for more advanced jailbreak research; furthermore, using our scoring system reduces the variance of disagreements between different evaluator LLMs by up to 76.33%. This demonstrates its ability to provide more fair and stable evaluation.</li>
</ul>

<h3>Title: AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qin Zhu, Fei Huang, Runyu Peng, Keming Lu, Bowen Yu, Qinyuan Cheng, Xipeng Qiu, Xuanjing Huang, Junyang Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16906">https://arxiv.org/abs/2502.16906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16906">https://arxiv.org/pdf/2502.16906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16906]] AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large Language Models(https://arxiv.org/abs/2502.16906)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While logical reasoning evaluation of Large Language Models (LLMs) has attracted significant attention, existing benchmarks predominantly rely on multiple-choice formats that are vulnerable to random guessing, leading to overestimated performance and substantial performance fluctuations. To obtain more accurate assessments of models' reasoning capabilities, we propose an automated method for synthesizing open-ended logic puzzles, and use it to develop a bilingual benchmark, AutoLogi. Our approach features program-based verification and controllable difficulty levels, enabling more reliable evaluation that better distinguishes models' reasoning abilities. Extensive evaluation of eight modern LLMs shows that AutoLogi can better reflect true model capabilities, with performance scores spanning from 35% to 73% compared to the narrower range of 21% to 37% on the source multiple-choice dataset. Beyond benchmark creation, this synthesis method can generate high-quality training data by incorporating program verifiers into the rejection sampling process, enabling systematic enhancement of LLMs' reasoning capabilities across diverse datasets.</li>
</ul>

<h3>Title: SPARC: Score Prompting and Adaptive Fusion for Zero-Shot Multi-Label Recognition in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kevin Miller, Samarth Mishra, Aditya Gangrade, Kate Saenko, Venkatesh Saligrama</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16911">https://arxiv.org/abs/2502.16911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16911">https://arxiv.org/pdf/2502.16911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16911]] SPARC: Score Prompting and Adaptive Fusion for Zero-Shot Multi-Label Recognition in Vision-Language Models(https://arxiv.org/abs/2502.16911)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Zero-shot multi-label recognition (MLR) with Vision-Language Models (VLMs) faces significant challenges without training data, model tuning, or architectural modifications. Existing approaches require prompt tuning or architectural adaptations, limiting zero-shot applicability. Our work proposes a novel solution treating VLMs as black boxes, leveraging scores without training data or ground truth. Using large language model insights on object co-occurrence, we introduce compound prompts grounded in realistic object combinations. Analysis of these prompt scores reveals VLM biases and ``AND''/``OR'' signal ambiguities, notably that maximum compound scores are surprisingly suboptimal compared to second-highest scores. We address these through a debiasing and score-fusion algorithm that corrects image bias and clarifies VLM response behaviors. Our method enhances other zero-shot approaches, consistently improving their results. Experiments show superior mean Average Precision (mAP) compared to methods requiring training data, achieved through refined object ranking for robust zero-shot MLR.</li>
</ul>

<h3>Title: Multi-Dimensional Quality Assessment for Text-to-3D Assets: Dataset and Model</h3>
<ul>
<li><strong>Authors: </strong>Kang Fu, Huiyu Duan, Zicheng Zhang, Xiaohong Liu, Xiongkuo Min, Jia Wang, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16915">https://arxiv.org/abs/2502.16915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16915">https://arxiv.org/pdf/2502.16915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16915]] Multi-Dimensional Quality Assessment for Text-to-3D Assets: Dataset and Model(https://arxiv.org/abs/2502.16915)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image (T2I) generation have spurred the development of text-to-3D asset (T23DA) generation, leveraging pretrained 2D text-to-image diffusion models for text-to-3D asset synthesis. Despite the growing popularity of text-to-3D asset generation, its evaluation has not been well considered and studied. However, given the significant quality discrepancies among various text-to-3D assets, there is a pressing need for quality assessment models aligned with human subjective judgments. To tackle this challenge, we conduct a comprehensive study to explore the T23DA quality assessment (T23DAQA) problem in this work from both subjective and objective perspectives. Given the absence of corresponding databases, we first establish the largest text-to-3D asset quality assessment database to date, termed the AIGC-T23DAQA database. This database encompasses 969 validated 3D assets generated from 170 prompts via 6 popular text-to-3D asset generation models, and corresponding subjective quality ratings for these assets from the perspectives of quality, authenticity, and text-asset correspondence, respectively. Subsequently, we establish a comprehensive benchmark based on the AIGC-T23DAQA database, and devise an effective T23DAQA model to evaluate the generated 3D assets from the aforementioned three perspectives, respectively.</li>
</ul>

<h3>Title: Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties</h3>
<ul>
<li><strong>Authors: </strong>Zhenglin Wang, Jialong Wu, Pengfei LI, Yong Jiang, Deyu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16922">https://arxiv.org/abs/2502.16922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16922">https://arxiv.org/pdf/2502.16922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16922]] Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties(https://arxiv.org/abs/2502.16922)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Temporal reasoning is fundamental to human cognition and is crucial for various real-world applications. While recent advances in Large Language Models have demonstrated promising capabilities in temporal reasoning, existing benchmarks primarily rely on rule-based construction, lack contextual depth, and involve a limited range of temporal entities. To address these limitations, we introduce Chinese Time Reasoning (CTM), a benchmark designed to evaluate LLMs on temporal reasoning within the extensive scope of Chinese dynastic chronology. CTM emphasizes cross-entity relationships, pairwise temporal alignment, and contextualized and culturally-grounded reasoning, providing a comprehensive evaluation. Extensive experimental results reveal the challenges posed by CTM and highlight potential avenues for improvement.</li>
</ul>

<h3>Title: A Systematic Survey of Automatic Prompt Optimization Techniques</h3>
<ul>
<li><strong>Authors: </strong>Kiran Ramnath, Kang Zhou, Sheng Guan, Soumya Smruti Mishra, Xuan Qi, Zhengyuan Shen, Shuai Wang, Sangmin Woo, Sullam Jeoung, Yawei Wang, Haozhu Wang, Han Ding, Yuzhe Lu, Zhichao Xu, Yun Zhou, Balasubramaniam Srinivasan, Qiaojing Yan, Yueyan Chen, Haibo Ding, Panpan Xu, Lin Lee Cheong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16923">https://arxiv.org/abs/2502.16923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16923">https://arxiv.org/pdf/2502.16923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16923]] A Systematic Survey of Automatic Prompt Optimization Techniques(https://arxiv.org/abs/2502.16923)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Since the advent of large language models (LLMs), prompt engineering has been a crucial step for eliciting desired responses for various Natural Language Processing (NLP) tasks. However, prompt engineering remains an impediment for end users due to rapid advances in models, tasks, and associated best practices. To mitigate this, Automatic Prompt Optimization (APO) techniques have recently emerged that use various automated techniques to help improve the performance of LLMs on various tasks. In this paper, we present a comprehensive survey summarizing the current progress and remaining challenges in this field. We provide a formal definition of APO, a 5-part unifying framework, and then proceed to rigorously categorize all relevant works based on their salient features therein. We hope to spur further research guided by our framework.</li>
</ul>

<h3>Title: BigMac: A Communication-Efficient Mixture-of-Experts Model Structure for Fast Training and Inference</h3>
<ul>
<li><strong>Authors: </strong>Zewen Jin, Shengnan Wang, Jiaan Zhu, Hongrui Zhan, Youhui Bai, Lin Zhang, Zhenyu Ming, Cheng Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16927">https://arxiv.org/abs/2502.16927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16927">https://arxiv.org/pdf/2502.16927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16927]] BigMac: A Communication-Efficient Mixture-of-Experts Model Structure for Fast Training and Inference(https://arxiv.org/abs/2502.16927)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The Mixture-of-Experts (MoE) structure scales the Transformer-based large language models (LLMs) and improves their performance with only the sub-linear increase in computation resources. Recently, a fine-grained DeepSeekMoE structure is proposed, which can further improve the computing efficiency of MoE without performance degradation. However, the All-to-All communication introduced by MoE has become a bottleneck, especially for the fine-grained structure, which typically involves and activates more experts, hence contributing to heavier communication overhead. In this paper, we propose a novel MoE structure named BigMac, which is also fine-grained but with high communication efficiency. The innovation of BigMac is mainly due to that we abandon the \textbf{c}ommunicate-\textbf{d}escend-\textbf{a}scend-\textbf{c}ommunicate (CDAC) manner used by fine-grained MoE, which leads to the All-to-All communication always taking place at the highest dimension. Instead, BigMac designs an efficient \textbf{d}escend-\textbf{c}ommunicate-\textbf{c}ommunicate-\textbf{a}scend (DCCA) manner. Specifically, we add a descending and ascending projection at the entrance and exit of the expert, respectively, which enables the communication to perform at a very low dimension. Furthermore, to adapt to DCCA, we re-design the structure of small experts, ensuring that the expert in BigMac has enough complexity to address tokens. Experimental results show that BigMac achieves comparable or even better model quality than fine-grained MoEs with the same number of experts and a similar number of total parameters. Equally importantly, BigMac reduces the end-to-end latency by up to 3.09$\times$ for training and increases the throughput by up to 3.11$\times$ for inference on state-of-the-art AI computing frameworks including Megatron, Tutel, and DeepSpeed-Inference.</li>
</ul>

<h3>Title: Achieving Fair PCA Using Joint Eigenvalue Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Vidhi Rathore, Naresh Manwani</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16933">https://arxiv.org/abs/2502.16933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16933">https://arxiv.org/pdf/2502.16933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16933]] Achieving Fair PCA Using Joint Eigenvalue Decomposition(https://arxiv.org/abs/2502.16933)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Principal Component Analysis (PCA) is a widely used method for dimensionality reduction, but it often overlooks fairness, especially when working with data that includes demographic characteristics. This can lead to biased representations that disproportionately affect certain groups. To address this issue, our approach incorporates Joint Eigenvalue Decomposition (JEVD), a technique that enables the simultaneous diagonalization of multiple matrices, ensuring fair and efficient representations. We formally show that the optimal solution of JEVD leads to a fair PCA solution. By integrating JEVD with PCA, we strike an optimal balance between preserving data structure and promoting fairness across diverse groups. We demonstrate that our method outperforms existing baseline approaches in fairness and representational quality on various datasets. It retains the core advantages of PCA while ensuring that sensitive demographic attributes do not create disparities in the reduced representation.</li>
</ul>

<h3>Title: Reasoning Does Not Necessarily Improve Role-Playing Ability</h3>
<ul>
<li><strong>Authors: </strong>Xiachong Feng, Longxu Dou, Lingpeng Kong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16940">https://arxiv.org/abs/2502.16940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16940">https://arxiv.org/pdf/2502.16940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16940]] Reasoning Does Not Necessarily Improve Role-Playing Ability(https://arxiv.org/abs/2502.16940)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The application of role-playing large language models (LLMs) is rapidly expanding in both academic and commercial domains, driving an increasing demand for high-precision role-playing models. Simultaneously, the rapid advancement of reasoning techniques has continuously pushed the performance boundaries of LLMs. This intersection of practical role-playing demands and evolving reasoning capabilities raises an important research question: "Can reasoning techniques enhance the role-playing capabilities of LLMs?" To address this, we conduct a comprehensive study using 6 role-playing benchmarks, 24 LLMs, and 3 distinct role-playing strategies, comparing the effectiveness of direct zero-shot role-playing, role-playing with Chain-of-Thought (CoT), and role-playing using reasoning-optimized LLMs. Our findings reveal that CoT may reduce role-playing performance, reasoning-optimized LLMs are unsuitable for role-playing, reasoning ability disrupts the role-playing scaling law, large models still lack proficiency in advanced role-playing, and Chinese role-playing performance surpasses English role-playing performance. Furthermore, based on extensive experimental results, we propose two promising future research directions: Role-aware CoT for improving role-playing LLMs and Reinforcement Learning for role-playing LLMs, aiming to enhance the adaptability, consistency, and effectiveness of role-playing LLMs for both research and real-world applications.</li>
</ul>

<h3>Title: Gaussian Difference: Find Any Change Instance in 3D Scenes</h3>
<ul>
<li><strong>Authors: </strong>Binbin Jiang, Rui Huang, Qingyi Zhao, Yuxiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16941">https://arxiv.org/abs/2502.16941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16941">https://arxiv.org/pdf/2502.16941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16941]] Gaussian Difference: Find Any Change Instance in 3D Scenes(https://arxiv.org/abs/2502.16941)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Instance-level change detection in 3D scenes presents significant challenges, particularly in uncontrolled environments lacking labeled image pairs, consistent camera poses, or uniform lighting conditions. This paper addresses these challenges by introducing a novel approach for detecting changes in real-world scenarios. Our method leverages 4D Gaussians to embed multiple images into Gaussian distributions, enabling the rendering of two coherent image sequences. We segment each image and assign unique identifiers to instances, facilitating efficient change detection through ID comparison. Additionally, we utilize change maps and classification encodings to categorize 4D Gaussians as changed or unchanged, allowing for the rendering of comprehensive change maps from any viewpoint. Extensive experiments across various instance-level change detection datasets demonstrate that our method significantly outperforms state-of-the-art approaches like C-NERF and CYWS-3D, especially in scenarios with substantial lighting variations. Our approach offers improved detection accuracy, robustness to lighting changes, and efficient processing times, advancing the field of 3D change detection.</li>
</ul>

<h3>Title: MAD-AD: Masked Diffusion for Unsupervised Brain Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Farzad Beizaee, Gregory Lodygensky, Christian Desrosiers, Jose Dolz</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16943">https://arxiv.org/abs/2502.16943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16943">https://arxiv.org/pdf/2502.16943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16943]] MAD-AD: Masked Diffusion for Unsupervised Brain Anomaly Detection(https://arxiv.org/abs/2502.16943)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Unsupervised anomaly detection in brain images is crucial for identifying injuries and pathologies without access to labels. However, the accurate localization of anomalies in medical images remains challenging due to the inherent complexity and variability of brain structures and the scarcity of annotated abnormal data. To address this challenge, we propose a novel approach that incorporates masking within diffusion models, leveraging their generative capabilities to learn robust representations of normal brain anatomy. During training, our model processes only normal brain MRI scans and performs a forward diffusion process in the latent space that adds noise to the features of randomly-selected patches. Following a dual objective, the model learns to identify which patches are noisy and recover their original features. This strategy ensures that the model captures intricate patterns of normal brain structures while isolating potential anomalies as noise in the latent space. At inference, the model identifies noisy patches corresponding to anomalies and generates a normal counterpart for these patches by applying a reverse diffusion process. Our method surpasses existing unsupervised anomaly detection techniques, demonstrating superior performance in generating accurate normal counterparts and localizing anomalies. The code is available at hhttps://github.com/farzad-bz/MAD-AD.</li>
</ul>

<h3>Title: Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance</h3>
<ul>
<li><strong>Authors: </strong>Chenghua Huang, Lu Wang, Fangkai Yang, Pu Zhao, Zhixu Li, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16944">https://arxiv.org/abs/2502.16944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16944">https://arxiv.org/pdf/2502.16944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16944]] Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance(https://arxiv.org/abs/2502.16944)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences. It requires joint training of an actor and critic with a pretrained, fixed reward model for guidance. This approach increases computational complexity and instability due to actor-critic interdependence. Additionally, PPO lacks access to true environment rewards in LLM tasks, limiting its adaptability. Under such conditions, pretraining a value model or a reward model becomes equivalent, as both provide fixed supervisory signals without new ground-truth feedback. To address these issues, we propose \textbf{Decoupled Value Policy Optimization (DVPO)}, a lean framework that replaces traditional reward modeling with a pretrained \emph{global value model (GVM)}. The GVM is conditioned on policy trajectories and predicts token-level return-to-go estimates. By decoupling value model from policy training (via frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence, reducing GPU memory usage by 40\% and training time by 35\% compared to conventional RLHF. Experiments across benchmarks show DVPO outperforms efficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in performance.</li>
</ul>

<h3>Title: Deep Minimax Classifiers for Imbalanced Datasets with a Small Number of Minority Samples</h3>
<ul>
<li><strong>Authors: </strong>Hansung Choi, Daewon Seo</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16948">https://arxiv.org/abs/2502.16948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16948">https://arxiv.org/pdf/2502.16948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16948]] Deep Minimax Classifiers for Imbalanced Datasets with a Small Number of Minority Samples(https://arxiv.org/abs/2502.16948)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The concept of a minimax classifier is well-established in statistical decision theory, but its implementation via neural networks remains challenging, particularly in scenarios with imbalanced training data having a limited number of samples for minority classes. To address this issue, we propose a novel minimax learning algorithm designed to minimize the risk of worst-performing classes. Our algorithm iterates through two steps: a minimization step that trains the model based on a selected target prior, and a maximization step that updates the target prior towards the adversarial prior for the trained model. In the minimization, we introduce a targeted logit-adjustment loss function that efficiently identifies optimal decision boundaries under the target prior. Moreover, based on a new prior-dependent generalization bound that we obtained, we theoretically prove that our loss function has a better generalization capability than existing loss functions. During the maximization, we refine the target prior by shifting it towards the adversarial prior, depending on the worst-performing classes rather than on per-class risk estimates. Our maximization method is particularly robust in the regime of a small number of samples. Additionally, to adapt to overparameterized neural networks, we partition the entire training dataset into two subsets: one for model training during the minimization step and the other for updating the target prior during the maximization step. Our proposed algorithm has a provable convergence property, and empirical results indicate that our algorithm performs better than or is comparable to existing methods. All codes are publicly available at this https URL.</li>
</ul>

<h3>Title: MTVHunter: Smart Contracts Vulnerability Detection Based on Multi-Teacher Knowledge Translation</h3>
<ul>
<li><strong>Authors: </strong>Guokai Sun, Yuan Zhuang, Shuo Zhang, Xiaoyu Feng, Zhenguang Liu, Liguo Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16955">https://arxiv.org/abs/2502.16955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16955">https://arxiv.org/pdf/2502.16955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16955]] MTVHunter: Smart Contracts Vulnerability Detection Based on Multi-Teacher Knowledge Translation(https://arxiv.org/abs/2502.16955)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Smart contracts, closely intertwined with cryptocurrency transactions, have sparked widespread concerns about considerable financial losses of security issues. To counteract this, a variety of tools have been developed to identify vulnerability in smart contract. However, they fail to overcome two challenges at the same time when faced with smart contract bytecode: (i) strong interference caused by enormous non-relevant instructions; (ii) missing semantics of bytecode due to incomplete data and control flow dependencies. In this paper, we propose a multi-teacher based bytecode vulnerability detection method, namely \textbf{M}ulti-\textbf{T}eacher \textbf{V}ulnerability \textbf{Hunter} (\textbf{MTVHunter}), which delivers effective denoising and missing semantic to bytecode under multi-teacher guidance. Specifically, we first propose an instruction denoising teacher to eliminate noise interference by abstract vulnerability pattern and further reflect in contract embeddings. Secondly, we design a novel semantic complementary teacher with neuron distillation, which effectively extracts necessary semantic from source code to replenish the bytecode. Particularly, the proposed neuron distillation accelerate this semantic filling by turning the knowledge transition into a regression task. We conduct experiments on 229,178 real-world smart contracts that concerns four types of common vulnerabilities. Extensive experiments show MTVHunter achieves significantly performance gains over state-of-the-art approaches.</li>
</ul>

<h3>Title: Atten-Transformer: A Deep Learning Framework for User App Usage Prediction</h3>
<ul>
<li><strong>Authors: </strong>Longlong Li, Cunquan Qu, Guanghui Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16957">https://arxiv.org/abs/2502.16957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16957">https://arxiv.org/pdf/2502.16957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16957]] Atten-Transformer: A Deep Learning Framework for User App Usage Prediction(https://arxiv.org/abs/2502.16957)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Accurately predicting smartphone app usage patterns is crucial for user experience optimization and targeted marketing. However, existing methods struggle to capture intricate dependencies in user behavior, particularly in sparse or complex usage scenarios. To address these challenges, we introduce Atten-Transformer, a novel model that integrates temporal attention with a Transformer network to dynamically identify and leverage key app usage patterns. Unlike conventional methods that primarily consider app order and duration, our approach employs a multi-dimensional feature representation, incorporating both feature encoding and temporal encoding to enhance predictive accuracy. The proposed attention mechanism effectively assigns importance to critical app usage moments, improving both model interpretability and generalization. Extensive experiments on multiple smartphone usage datasets, including LSapp and Tsinghua App Usage datasets, demonstrate that Atten-Transformer consistently outperforms state-of-the-art models across different data splits. Specifically, our model achieves a 45.24\% improvement in HR@1 on the Tsinghua dataset (Time-based Split) and a 18.25\% improvement in HR@1 on the LSapp dataset (Cold Start Split), showcasing its robustness across diverse app usage scenarios. These findings highlight the potential of integrating adaptive attention mechanisms in mobile usage forecasting, paving the way for enhanced user engagement and resource allocation.</li>
</ul>

<h3>Title: UrduLLaMA 1.0: Dataset Curation, Preprocessing, and Evaluation in Low-Resource Settings</h3>
<ul>
<li><strong>Authors: </strong>Layba Fiaz, Munief Hassan Tahir, Sana Shams, Sarmad Hussain</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16961">https://arxiv.org/abs/2502.16961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16961">https://arxiv.org/pdf/2502.16961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16961]] UrduLLaMA 1.0: Dataset Curation, Preprocessing, and Evaluation in Low-Resource Settings(https://arxiv.org/abs/2502.16961)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multilingual Large Language Models (LLMs) often provide suboptimal performance on low-resource languages like Urdu. This paper introduces UrduLLaMA 1.0, a model derived from the open-source Llama-3.1-8B-Instruct architecture and continually pre-trained on 128 million Urdu tokens, capturing the rich diversity of the language. To enhance instruction-following and translation capabilities, we leverage Low-Rank Adaptation (LoRA) to fine tune the model on 41,000 Urdu instructions and approximately 50,000 English-Urdu translation pairs. Evaluation across three machine translation datasets demonstrates significant performance improvements compared to state-of-the-art (SOTA) models, establishing a new benchmark for Urdu LLMs. These findings underscore the potential of targeted adaptation strategies with limited data and computational resources to address the unique challenges of low-resource languages.</li>
</ul>

<h3>Title: Autoregressive Image Generation Guided by Chains of Thought</h3>
<ul>
<li><strong>Authors: </strong>Miaomiao Cai, Guanjie Wang, Wei Li, Zhijun Tu, Hanting Chen, Shaohui Lin, Jie Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16965">https://arxiv.org/abs/2502.16965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16965">https://arxiv.org/pdf/2502.16965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16965]] Autoregressive Image Generation Guided by Chains of Thought(https://arxiv.org/abs/2502.16965)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In the field of autoregressive (AR) image generation, models based on the 'next-token prediction' paradigm of LLMs have shown comparable performance to diffusion models by reducing inductive biases. However, directly applying LLMs to complex image generation can struggle with reconstructing the structure and details of the image, impacting the accuracy and stability of generation. Additionally, the 'next-token prediction' paradigm in the AR model does not align with the contextual scanning and logical reasoning processes involved in human visual perception, limiting effective image generation. Chain-of-Thought (CoT), as a key reasoning capability of LLMs, utilizes reasoning prompts to guide the model, improving reasoning performance on complex natural language process (NLP) tasks, enhancing accuracy and stability of generation, and helping the model maintain contextual coherence and logical consistency, similar to human reasoning. Inspired by CoT from the field of NLP, we propose autoregressive Image Generation with Thoughtful Reasoning (IGTR) to enhance autoregressive image generation. IGTR adds reasoning prompts without modifying the model structure or raster generation order. Specifically, we design specialized image-related reasoning prompts for AR image generation to simulate the human reasoning process, which enhances contextual reasoning by allowing the model to first perceive overall distribution information before generating the image, and improve generation stability by increasing the inference steps. Compared to the AR method without prompts, our method shows outstanding performance and achieves an approximate improvement of 20%.</li>
</ul>

<h3>Title: LongSafety: Evaluating Long-Context Safety of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yida Lu, Jiale Cheng, Zhexin Zhang, Shiyao Cui, Cunxiang Wang, Xiaotao Gu, Yuxiao Dong, Jie Tang, Hongning Wang, Minlie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16971">https://arxiv.org/abs/2502.16971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16971">https://arxiv.org/pdf/2502.16971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16971]] LongSafety: Evaluating Long-Context Safety of Large Language Models(https://arxiv.org/abs/2502.16971)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) continue to advance in understanding and generating long sequences, new safety concerns have been introduced through the long context. However, the safety of LLMs in long-context tasks remains under-explored, leaving a significant gap in both evaluation and improvement of their safety. To address this, we introduce LongSafety, the first comprehensive benchmark specifically designed to evaluate LLM safety in open-ended long-context tasks. LongSafety encompasses 7 categories of safety issues and 6 user-oriented long-context tasks, with a total of 1,543 test cases, averaging 5,424 words per context. Our evaluation towards 16 representative LLMs reveals significant safety vulnerabilities, with most models achieving safety rates below 55%. Our findings also indicate that strong safety performance in short-context scenarios does not necessarily correlate with safety in long-context tasks, emphasizing the unique challenges and urgency of improving long-context safety. Moreover, through extensive analysis, we identify challenging safety issues and task types for long-context models. Furthermore, we find that relevant context and extended input sequences can exacerbate safety risks in long-context scenarios, highlighting the critical need for ongoing attention to long-context safety challenges. Our code and data are available at this https URL.</li>
</ul>

<h3>Title: TraFlow: Trajectory Distillation on Pre-Trained Rectified Flow</h3>
<ul>
<li><strong>Authors: </strong>Zhangkai Wu, Xuhui Fan, Hongyu Wu, Longbing Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16972">https://arxiv.org/abs/2502.16972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16972">https://arxiv.org/pdf/2502.16972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16972]] TraFlow: Trajectory Distillation on Pre-Trained Rectified Flow(https://arxiv.org/abs/2502.16972)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Majorities of distillation methods on pre-trained diffusion models or on pre-trained rectified flow, focus on either the distillation outputs or the trajectories between random noises and clean images to speed up sample generations from pre-trained models. In those trajectory-based distillation methods, consistency distillation requires the self-consistent trajectory projection to regulate the trajectory, which might avoid the common ODE approximation error {while still be concerning about sampling efficiencies}. At the same time, rectified flow distillations enforce straight trajectory for fast sampling, although an ODE solver is still required. In this work, we propose a trajectory distillation method, \modelname, that enjoys the benefits of both and enables few-step generations. TraFlow adopts the settings of consistency trajectory models, and further enforces the properties of self-consistency and straightness throughout the entire trajectory. These two properties are pursued by reaching a balance with following three targets: (1) reconstruct the output from pre-trained models; (2) learn the amount of changes by pre-trained models; (3) satisfy the self-consistency over its trajectory. Extensive experimental results have shown the effectiveness of our proposed method.</li>
</ul>

<h3>Title: Semantic Neural Radiance Fields for Multi-Date Satellite Data</h3>
<ul>
<li><strong>Authors: </strong>Valentin Wagner, Sebastian Bullinger, Christoph Bodensteiner, Michael Arens</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16992">https://arxiv.org/abs/2502.16992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16992">https://arxiv.org/pdf/2502.16992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16992]] Semantic Neural Radiance Fields for Multi-Date Satellite Data(https://arxiv.org/abs/2502.16992)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this work we propose a satellite specific Neural Radiance Fields (NeRF) model capable to obtain a three-dimensional semantic representation (neural semantic field) of the scene. The model derives the output from a set of multi-date satellite images with corresponding pixel-wise semantic labels. We demonstrate the robustness of our approach and its capability to improve noisy input labels. We enhance the color prediction by utilizing the semantic information to address temporal image inconsistencies caused by non-stationary categories such as vehicles. To facilitate further research in this domain, we present a dataset comprising manually generated labels for popular multi-view satellite images. Our code and dataset are available at this https URL.</li>
</ul>

<h3>Title: FADE: Why Bad Descriptions Happen to Good Features</h3>
<ul>
<li><strong>Authors: </strong>Bruno Puri, Aakriti Jain, Elena Golimblevskaia, Patrick Kahardipraja, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.16994">https://arxiv.org/abs/2502.16994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.16994">https://arxiv.org/pdf/2502.16994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.16994]] FADE: Why Bad Descriptions Happen to Good Features(https://arxiv.org/abs/2502.16994)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Recent advances in mechanistic interpretability have highlighted the potential of automating interpretability pipelines in analyzing the latent representations within LLMs. While they may enhance our understanding of internal mechanisms, the field lacks standardized evaluation methods for assessing the validity of discovered features. We attempt to bridge this gap by introducing FADE: Feature Alignment to Description Evaluation, a scalable model-agnostic framework for evaluating feature-description alignment. FADE evaluates alignment across four key metrics - Clarity, Responsiveness, Purity, and Faithfulness - and systematically quantifies the causes for the misalignment of feature and their description. We apply FADE to analyze existing open-source feature descriptions, and assess key components of automated interpretability pipelines, aiming to enhance the quality of descriptions. Our findings highlight fundamental challenges in generating feature descriptions, particularly for SAEs as compared to MLP neurons, providing insights into the limitations and future directions of automated interpretability. We release FADE as an open-source package at: this https URL.</li>
</ul>

<h3>Title: An Enhanced Large Language Model For Cross Modal Query Understanding System Using DL-KeyBERT Based CAZSSCL-MPGPT</h3>
<ul>
<li><strong>Authors: </strong>Shreya Singh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17000">https://arxiv.org/abs/2502.17000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17000">https://arxiv.org/pdf/2502.17000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17000]] An Enhanced Large Language Model For Cross Modal Query Understanding System Using DL-KeyBERT Based CAZSSCL-MPGPT(https://arxiv.org/abs/2502.17000)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are advanced deep-learning models designed to understand and generate human language. They work together with models that process data like images, enabling cross-modal understanding. However, existing approaches often suffer from the echo chamber effect, where redundant visual patterns reduce model generalization and accuracy. Thus, the proposed system considered this limitation and developed an enhanced LLM-based framework for cross-modal query understanding using DL-KeyBERT-based CAZSSCL-MPGPT. The collected dataset consists of pre-processed images and texts. The preprocessed images then undergo object segmentation using Easom-You Only Look Once (E-YOLO). The object skeleton is generated, along with the knowledge graph using a Conditional Random Knowledge Graph (CRKG) technique. Further, features are extracted from the knowledge graph, generated skeletons, and segmented objects. The optimal features are then selected using the Fossa Optimization Algorithm (FOA). Meanwhile, the text undergoes word embedding using DL-KeyBERT. Finally, the cross-modal query understanding system utilizes CAZSSCL-MPGPT to generate accurate and contextually relevant image descriptions as text. The proposed CAZSSCL-MPGPT achieved an accuracy of 99.14187362% in the COCO dataset 2017 and 98.43224393% in the vqav2-val dataset.</li>
</ul>

<h3>Title: Improving the Transferability of Adversarial Examples by Inverse Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Wenyuan Wu, Zheng Liu, Yong Chen, Chao Su, Dezhong Peng, Xu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17003">https://arxiv.org/abs/2502.17003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17003">https://arxiv.org/pdf/2502.17003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17003]] Improving the Transferability of Adversarial Examples by Inverse Knowledge Distillation(https://arxiv.org/abs/2502.17003)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>In recent years, the rapid development of deep neural networks has brought increased attention to the security and robustness of these models. While existing adversarial attack algorithms have demonstrated success in improving adversarial transferability, their performance remains suboptimal due to a lack of consideration for the discrepancies between target and source models. To address this limitation, we propose a novel method, Inverse Knowledge Distillation (IKD), designed to enhance adversarial transferability effectively. IKD introduces a distillation-inspired loss function that seamlessly integrates with gradient-based attack methods, promoting diversity in attack gradients and mitigating overfitting to specific model architectures. By diversifying gradients, IKD enables the generation of adversarial samples with superior generalization capabilities across different models, significantly enhancing their effectiveness in black-box attack scenarios. Extensive experiments on the ImageNet dataset validate the effectiveness of our approach, demonstrating substantial improvements in the transferability and attack success rates of adversarial samples across a wide range of models.</li>
</ul>

<h3>Title: All You Need for Counterfactual Explainability Is Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Kacper Sokol, Eyke Hüllermeier</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17007">https://arxiv.org/abs/2502.17007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17007">https://arxiv.org/pdf/2502.17007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17007]] All You Need for Counterfactual Explainability Is Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty(https://arxiv.org/abs/2502.17007)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>This position paper argues that, to its detriment, transparency research overlooks many foundational concepts of artificial intelligence. Here, we focus on uncertainty quantification -- in the context of ante-hoc interpretability and counterfactual explainability -- showing how its adoption could address key challenges in the field. First, we posit that uncertainty and ante-hoc interpretability offer complementary views of the same underlying idea; second, we assert that uncertainty provides a principled unifying framework for counterfactual explainability. Consequently, inherently transparent models can benefit from human-centred explanatory insights -- like counterfactuals -- which are otherwise missing. At a higher level, integrating artificial intelligence fundamentals into transparency research promises to yield more reliable, robust and understandable predictive models.</li>
</ul>

<h3>Title: Unbiased and Sign Compression in Distributed Learning: Comparing Noise Resilience via SDEs</h3>
<ul>
<li><strong>Authors: </strong>Enea Monzio Compagnoni, Rustem Islamov, Frank Norbert Proske, Aurelien Lucchi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17009">https://arxiv.org/abs/2502.17009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17009">https://arxiv.org/pdf/2502.17009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17009]] Unbiased and Sign Compression in Distributed Learning: Comparing Noise Resilience via SDEs(https://arxiv.org/abs/2502.17009)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Distributed methods are essential for handling machine learning pipelines comprising large-scale models and datasets. However, their benefits often come at the cost of increased communication overhead between the central server and agents, which can become the main bottleneck, making training costly or even unfeasible in such systems. Compression methods such as quantization and sparsification can alleviate this issue. Still, their robustness to large and heavy-tailed gradient noise, a phenomenon sometimes observed in language modeling, remains poorly understood. This work addresses this gap by analyzing Distributed Compressed SGD (DCSGD) and Distributed SignSGD (DSignSGD) using stochastic differential equations (SDEs). Our results show that DCSGD with unbiased compression is more vulnerable to noise in stochastic gradients, while DSignSGD remains robust, even under large and heavy-tailed noise. Additionally, we propose new scaling rules for hyperparameter tuning to mitigate performance degradation due to compression. These findings are empirically validated across multiple deep learning architectures and datasets, providing practical recommendations for distributed optimization.</li>
</ul>

<h3>Title: Quantifying Logical Consistency in Transformers via Query-Key Alignment</h3>
<ul>
<li><strong>Authors: </strong>Eduard Tulchinskii, Anastasia Voznyuk, Laida Kushnareva, Andrei Andriiainen, Irina Piontkovskaya, Evgeny Burnaev, Serguei Barannikov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17017">https://arxiv.org/abs/2502.17017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17017">https://arxiv.org/pdf/2502.17017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17017]] Quantifying Logical Consistency in Transformers via Query-Key Alignment(https://arxiv.org/abs/2502.17017)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive performance in various natural language processing tasks, yet their ability to perform multi-step logical reasoning remains an open challenge. Although Chain-of-Thought prompting has improved logical reasoning by enabling models to generate intermediate steps, it lacks mechanisms to assess the coherence of these logical transitions. In this paper, we propose a novel, lightweight evaluation strategy for logical reasoning that uses query-key alignments inside transformer attention heads. By computing a single forward pass and extracting a "QK-score" from carefully chosen heads, our method reveals latent representations that reliably separate valid from invalid inferences, offering a scalable alternative to traditional ablation-based techniques. We also provide an empirical validation on multiple logical reasoning benchmarks, demonstrating improved robustness of our evaluation method against distractors and increased reasoning depth. The experiments were conducted on a diverse set of models, ranging from 1.5B to 70B parameters.</li>
</ul>

<h3>Title: Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical Systems</h3>
<ul>
<li><strong>Authors: </strong>Maksim Zhdanov, Max Welling, Jan-Willem van de Meent</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17019">https://arxiv.org/abs/2502.17019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17019">https://arxiv.org/pdf/2502.17019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17019]] Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical Systems(https://arxiv.org/abs/2502.17019)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Large-scale physical systems defined on irregular grids pose significant scalability challenges for deep learning methods, especially in the presence of long-range interactions and multi-scale coupling. Traditional approaches that compute all pairwise interactions, such as attention, become computationally prohibitive as they scale quadratically with the number of nodes. We present Erwin, a hierarchical transformer inspired by methods from computational many-body physics, which combines the efficiency of tree-based algorithms with the expressivity of attention mechanisms. Erwin employs ball tree partitioning to organize computation, which enables linear-time attention by processing nodes in parallel within local neighborhoods of fixed size. Through progressive coarsening and refinement of the ball tree structure, complemented by a novel cross-ball interaction mechanism, it captures both fine-grained local details and global features. We demonstrate Erwin's effectiveness across multiple domains, including cosmology, molecular dynamics, and particle fluid dynamics, where it consistently outperforms baseline methods both in accuracy and computational efficiency.</li>
</ul>

<h3>Title: Moving Past Single Metrics: Exploring Short-Text Clustering Across Multiple Resolutions</h3>
<ul>
<li><strong>Authors: </strong>Justin Miller, Tristram Alexander</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17020">https://arxiv.org/abs/2502.17020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17020">https://arxiv.org/pdf/2502.17020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17020]] Moving Past Single Metrics: Exploring Short-Text Clustering Across Multiple Resolutions(https://arxiv.org/abs/2502.17020)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Cluster number is typically a parameter selected at the outset in clustering problems, and while impactful, the choice can often be difficult to justify. Inspired by bioinformatics, this study examines how the nature of clusters varies with cluster number, presenting a method for determining cluster robustness, and providing a systematic method for deciding on the cluster number. The study focuses specifically on short-text clustering, involving 30,000 political Twitter bios, where the sparse co-occurrence of words between texts makes finding meaningful clusters challenging. A metric of proportional stability is introduced to uncover the stability of specific clusters between cluster resolutions, and the results are visualised using Sankey diagrams to provide an interrogative tool for understanding the nature of the dataset. The visualisation provides an intuitive way to track cluster subdivision and reorganisation as cluster number increases, offering insights that static, single-resolution metrics cannot capture. The results show that instead of seeking a single 'optimal' solution, choosing a cluster number involves balancing informativeness and complexity.</li>
</ul>

<h3>Title: Towards Auto-Regressive Next-Token Prediction: In-Context Learning Emerges from Generalization</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Gong, Xiaolin Hu, Huayi Tang, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17024">https://arxiv.org/abs/2502.17024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17024">https://arxiv.org/pdf/2502.17024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17024]] Towards Auto-Regressive Next-Token Prediction: In-Context Learning Emerges from Generalization(https://arxiv.org/abs/2502.17024)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable in-context learning (ICL) abilities. However, existing theoretical analysis of ICL primarily exhibits two limitations: (a) Limited i.i.d. Setting. Most studies focus on supervised function learning tasks where prompts are constructed with i.i.d. input-label pairs. This i.i.d. assumption diverges significantly from real language learning scenarios where prompt tokens are interdependent. (b) Lack of Emergence Explanation. Most literature answers what ICL does from an implicit optimization perspective but falls short in elucidating how ICL emerges and the impact of pre-training phase on ICL. In our paper, to extend (a), we adopt a more practical paradigm, auto-regressive next-token prediction (AR-NTP), which closely aligns with the actual training of language models. Specifically, within AR-NTP, we emphasize prompt token-dependency, which involves predicting each subsequent token based on the preceding sequence. To address (b), we formalize a systematic pre-training and ICL framework, highlighting the layer-wise structure of sequences and topics, alongside a two-level expectation. In conclusion, we present data-dependent, topic-dependent and optimization-dependent PAC-Bayesian generalization bounds for pre-trained LLMs, investigating that ICL emerges from the generalization of sequences and topics. Our theory is supported by experiments on numerical linear dynamic systems, synthetic GINC and real-world language datasets.</li>
</ul>

<h3>Title: Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology</h3>
<ul>
<li><strong>Authors: </strong>Longchao Da, Xiaoou Liu, Jiaxin Dai, Lu Cheng, Yaqing Wang, Hua Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17026">https://arxiv.org/abs/2502.17026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17026">https://arxiv.org/pdf/2502.17026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17026]] Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology(https://arxiv.org/abs/2502.17026)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Understanding the uncertainty in large language model (LLM) explanations is important for evaluating their faithfulness and reasoning consistency, and thus provides insights into the reliability of LLM's output regarding a question. In this work, we propose a novel framework that quantifies uncertainty in LLM explanations through a reasoning topology perspective. By designing a structural elicitation strategy, we guide the LLMs to frame the explanations of an answer into a graph topology. This process decomposes the explanations into the knowledge related sub-questions and topology-based reasoning structures, which allows us to quantify uncertainty not only at the semantic level but also from the reasoning path. It further brings convenience to assess knowledge redundancy and provide interpretable insights into the reasoning process. Our method offers a systematic way to interpret the LLM reasoning, analyze limitations, and provide guidance for enhancing robustness and faithfulness. This work pioneers the use of graph-structured uncertainty measurement in LLM explanations and demonstrates the potential of topology-based quantification.</li>
</ul>

<h3>Title: PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance</h3>
<ul>
<li><strong>Authors: </strong>Haoran Li, Wenbin Hu, Huihao Jing, Yulin Chen, Qi Hu, Sirui Han, Tianshu Chu, Peizhao Hu, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17041">https://arxiv.org/abs/2502.17041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17041">https://arxiv.org/pdf/2502.17041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17041]] PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance(https://arxiv.org/abs/2502.17041)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in generative large language models (LLMs) have enabled wider applicability, accessibility, and flexibility. However, their reliability and trustworthiness are still in doubt, especially for concerns regarding individuals' data privacy. Great efforts have been made on privacy by building various evaluation benchmarks to study LLMs' privacy awareness and robustness from their generated outputs to their hidden representations. Unfortunately, most of these works adopt a narrow formulation of privacy and only investigate personally identifiable information (PII). In this paper, we follow the merit of the Contextual Integrity (CI) theory, which posits that privacy evaluation should not only cover the transmitted attributes but also encompass the whole relevant social context through private information flows. We present PrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted at legal compliance to cover well-annotated privacy and safety regulations, real court cases, privacy policies, and synthetic data built from the official toolkit to study LLMs' privacy and safety compliance. We evaluate the latest LLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our experimental results suggest that though LLMs can effectively capture key CI parameters inside a given context, they still require further advancements for privacy compliance.</li>
</ul>

<h3>Title: SpecDM: Hyperspectral Dataset Synthesis with Pixel-level Semantic Annotations</h3>
<ul>
<li><strong>Authors: </strong>Wendi Liu, Pei Yang, Wenhui Hong, Xiaoguang Mei, Jiayi Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17056">https://arxiv.org/abs/2502.17056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17056">https://arxiv.org/pdf/2502.17056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17056]] SpecDM: Hyperspectral Dataset Synthesis with Pixel-level Semantic Annotations(https://arxiv.org/abs/2502.17056)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>In hyperspectral remote sensing field, some downstream dense prediction tasks, such as semantic segmentation (SS) and change detection (CD), rely on supervised learning to improve model performance and require a large amount of manually annotated data for training. However, due to the needs of specific equipment and special application scenarios, the acquisition and annotation of hyperspectral images (HSIs) are often costly and time-consuming. To this end, our work explores the potential of generative diffusion model in synthesizing HSIs with pixel-level annotations. The main idea is to utilize a two-stream VAE to learn the latent representations of images and corresponding masks respectively, learn their joint distribution during the diffusion model training, and finally obtain the image and mask through their respective decoders. To the best of our knowledge, it is the first work to generate high-dimensional HSIs with annotations. Our proposed approach can be applied in various kinds of dataset generation. We select two of the most widely used dense prediction tasks: semantic segmentation and change detection, and generate datasets suitable for these tasks. Experiments demonstrate that our synthetic datasets have a positive impact on the improvement of these downstream tasks.</li>
</ul>

<h3>Title: Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability</h3>
<ul>
<li><strong>Authors: </strong>Ashhadul Islam, Samir Brahim Belhaouari, Amine Bermak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17071">https://arxiv.org/abs/2502.17071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17071">https://arxiv.org/pdf/2502.17071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17071]] Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability(https://arxiv.org/abs/2502.17071)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The exponential growth of large language models (LLMs) like ChatGPT has revolutionized artificial intelligence, offering unprecedented capabilities in natural language processing. However, the extensive computational resources required for training these models have significant environmental implications, including high carbon emissions, energy consumption, and water usage. This research presents a novel approach to LLM pruning, focusing on the systematic evaluation of individual weight importance throughout the training process. By monitoring parameter evolution over time, we propose a method that effectively reduces model size without compromising performance. Extensive experiments with both a scaled-down LLM and a large multimodal model reveal that moderate pruning enhances efficiency and reduces loss, while excessive pruning drastically deteriorates model performance. These findings highlight the critical need for optimized AI models to ensure sustainable development, balancing technological advancement with environmental responsibility.</li>
</ul>

<h3>Title: Forgetting Any Data at Any Time: A Theoretically Certified Unlearning Framework for Vertical Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Linian Wang, Leye Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17081">https://arxiv.org/abs/2502.17081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17081">https://arxiv.org/pdf/2502.17081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17081]] Forgetting Any Data at Any Time: A Theoretically Certified Unlearning Framework for Vertical Federated Learning(https://arxiv.org/abs/2502.17081)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Privacy concerns in machine learning are heightened by regulations such as the GDPR, which enforces the "right to be forgotten" (RTBF), driving the emergence of machine unlearning as a critical research field. Vertical Federated Learning (VFL) enables collaborative model training by aggregating a sample's features across distributed parties while preserving data privacy at each source. This paradigm has seen widespread adoption in healthcare, finance, and other privacy-sensitive domains. However, existing VFL systems lack robust mechanisms to comply with RTBF requirements, as unlearning methodologies for VFL remain underexplored. In this work, we introduce the first VFL framework with theoretically guaranteed unlearning capabilities, enabling the removal of any data at any time. Unlike prior approaches -- which impose restrictive assumptions on model architectures or data types for removal -- our solution is model- and data-agnostic, offering universal compatibility. Moreover, our framework supports asynchronous unlearning, eliminating the need for all parties to be simultaneously online during the forgetting process. These advancements address critical gaps in current VFL systems, ensuring compliance with RTBF while maintaining operational this http URL make all our implementations publicly available at this https URL.</li>
</ul>

<h3>Title: Pleno-Generation: A Scalable Generative Face Video Compression Framework with Bandwidth Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Bolin Chen, Hanwei Zhu, Shanzhi Yin, Lingyu Zhu, Jie Chen, Ru-Ling Liao, Shiqi Wang, Yan Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17085">https://arxiv.org/abs/2502.17085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17085">https://arxiv.org/pdf/2502.17085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17085]] Pleno-Generation: A Scalable Generative Face Video Compression Framework with Bandwidth Intelligence(https://arxiv.org/abs/2502.17085)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Generative model based compact video compression is typically operated within a relative narrow range of bitrates, and often with an emphasis on ultra-low rate applications. There has been an increasing consensus in the video communication industry that full bitrate coverage should be enabled by generative coding. However, this is an extremely difficult task, largely because generation and compression, although related, have distinct goals and trade-offs. The proposed Pleno-Generation (PGen) framework distinguishes itself through its exceptional capabilities in ensuring the robustness of video coding by utilizing a wider range of bandwidth for generation via bandwidth intelligence. In particular, we initiate our research of PGen with face video coding, and PGen offers a paradigm shift that prioritizes high-fidelity reconstruction over pursuing compact bitstream. The novel PGen framework leverages scalable representation and layered reconstruction for Generative Face Video Compression (GFVC), in an attempt to imbue the bitstream with intelligence in different granularity. Experimental results illustrate that the proposed PGen framework can facilitate existing GFVC algorithms to better deliver high-fidelity and faithful face videos. In addition, the proposed framework can allow a greater space of flexibility for coding applications and show superior RD performance with a much wider bitrate range in terms of various quality evaluations. Moreover, in comparison with the latest Versatile Video Coding (VVC) codec, the proposed scheme achieves competitive Bjøntegaard-delta-rate savings for perceptual-level evaluations.</li>
</ul>

<h3>Title: Automatically Evaluating the Paper Reviewing Capability of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hyungyu Shin, Jingyu Tang, Yoonjoo Lee, Nayoung Kim, Hyunseung Lim, Ji Yong Cho, Hwajung Hong, Moontae Lee, Juho Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17086">https://arxiv.org/abs/2502.17086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17086">https://arxiv.org/pdf/2502.17086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17086]] Automatically Evaluating the Paper Reviewing Capability of Large Language Models(https://arxiv.org/abs/2502.17086)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Peer review is essential for scientific progress, but it faces challenges such as reviewer shortages and growing workloads. Although Large Language Models (LLMs) show potential for providing assistance, research has reported significant limitations in the reviews they generate. While the insights are valuable, conducting the analysis is challenging due to the considerable time and effort required, especially given the rapid pace of LLM developments. To address the challenge, we developed an automatic evaluation pipeline to assess the LLMs' paper review capability by comparing them with expert-generated reviews. By constructing a dataset consisting of 676 OpenReview papers, we examined the agreement between LLMs and experts in their strength and weakness identifications. The results showed that LLMs lack balanced perspectives, significantly overlook novelty assessment when criticizing, and produce poor acceptance decisions. Our automated pipeline enables a scalable evaluation of LLMs' paper review capability over time.</li>
</ul>

<h3>Title: Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI</h3>
<ul>
<li><strong>Authors: </strong>Syed Abdul Gaffar Shakhadri, Kruthika KR, Kartik Basavaraj Angadi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17092">https://arxiv.org/abs/2502.17092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17092">https://arxiv.org/pdf/2502.17092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17092]] Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI(https://arxiv.org/abs/2502.17092)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We introduce Shakti VLM, a family of vision-language models in the capacity of 1B and 4B parameters designed to address data efficiency challenges in multimodal learning. While recent VLMs achieve strong performance through extensive training data, Shakti models leverage architectural innovations to attain competitive results with fewer tokens. Key advancements include QK-Normalization for attention stability, hybrid normalization techniques, and enhanced positional encoding. A three-stage training strategy further optimizes learning efficiency. Evaluations show that Shakti-Shakti-VLM-1B and Shakti-VLM-4B excel in document understanding, Visual Reasoning, OCR extraction, and general multimodal reasoning. Our results highlight that high performance can be achieved through model design and training strategy rather than sheer data volume, making Shakti an efficient solution for enterprise-scale multimodal tasks.</li>
</ul>

<h3>Title: Enhancing Image Matting in Real-World Scenes with Mask-Guided Iterative Refinement</h3>
<ul>
<li><strong>Authors: </strong>Rui Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17093">https://arxiv.org/abs/2502.17093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17093">https://arxiv.org/pdf/2502.17093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17093]] Enhancing Image Matting in Real-World Scenes with Mask-Guided Iterative Refinement(https://arxiv.org/abs/2502.17093)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Real-world image matting is essential for applications in content creation and augmented reality. However, it remains challenging due to the complex nature of scenes and the scarcity of high-quality datasets. To address these limitations, we introduce Mask2Alpha, an iterative refinement framework designed to enhance semantic comprehension, instance awareness, and fine-detail recovery in image matting. Our framework leverages self-supervised Vision Transformer features as semantic priors, strengthening contextual understanding in complex scenarios. To further improve instance differentiation, we implement a mask-guided feature selection module, enabling precise targeting of objects in multi-instance settings. Additionally, a sparse convolution-based optimization scheme allows Mask2Alpha to recover high-resolution details through progressive refinement,from low-resolution semantic passes to high-resolution sparse reconstructions. Benchmarking across various real-world datasets, Mask2Alpha consistently achieves state-of-the-art results, showcasing its effectiveness in accurate and efficient image matting.</li>
</ul>

<h3>Title: Improved Diffusion-based Generative Model with Better Adversarial Robustness</h3>
<ul>
<li><strong>Authors: </strong>Zekun Wang, Mingyang Yi, Shuchen Xue, Zhenguo Li, Ming Liu, Bing Qin, Zhi-Ming Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17099">https://arxiv.org/abs/2502.17099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17099">https://arxiv.org/pdf/2502.17099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17099]] Improved Diffusion-based Generative Model with Better Adversarial Robustness(https://arxiv.org/abs/2502.17099)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion Probabilistic Models (DPMs) have achieved significant success in generative tasks. However, their training and sampling processes suffer from the issue of distribution mismatch. During the denoising process, the input data distributions differ between the training and inference stages, potentially leading to inaccurate data generation. To obviate this, we analyze the training objective of DPMs and theoretically demonstrate that this mismatch can be alleviated through Distributionally Robust Optimization (DRO), which is equivalent to performing robustness-driven Adversarial Training (AT) on DPMs. Furthermore, for the recently proposed Consistency Model (CM), which distills the inference process of the DPM, we prove that its training objective also encounters the mismatch issue. Fortunately, this issue can be mitigated by AT as well. Based on these insights, we propose to conduct efficient AT on both DPM and CM. Finally, extensive empirical studies validate the effectiveness of AT in diffusion-based models. The code is available at this https URL.</li>
</ul>

<h3>Title: Generative Models in Decision Making: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Yinchuan Li, Xinyu Shao, Jianping Zhang, Haozhi Wang, Leo Maxime Brunswic, Kaiwen Zhou, Jiqian Dong, Kaiyang Guo, Xiu Li, Zhitang Chen, Jun Wang, Jianye Hao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17100">https://arxiv.org/abs/2502.17100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17100">https://arxiv.org/pdf/2502.17100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17100]] Generative Models in Decision Making: A Survey(https://arxiv.org/abs/2502.17100)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In recent years, the exceptional performance of generative models in generative tasks has sparked significant interest in their integration into decision-making processes. Due to their ability to handle complex data distributions and their strong model capacity, generative models can be effectively incorporated into decision-making systems by generating trajectories that guide agents toward high-reward state-action regions or intermediate sub-goals. This paper presents a comprehensive review of the application of generative models in decision-making tasks. We classify seven fundamental types of generative models: energy-based models, generative adversarial networks, variational autoencoders, normalizing flows, diffusion models, generative flow networks, and autoregressive models. Regarding their applications, we categorize their functions into three main roles: controllers, modelers and optimizers, and discuss how each role contributes to decision-making. Furthermore, we examine the deployment of these models across five critical real-world decision-making scenarios. Finally, we summarize the strengths and limitations of current approaches and propose three key directions for advancing next-generation generative directive models: high-performance algorithms, large-scale generalized decision-making models, and self-evolving and adaptive models.</li>
</ul>

<h3>Title: SFLD: Reducing the content bias for AI-generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Seoyeon Gye, Junwon Ko, Hyounguk Shon, Minchan Kwon, Junmo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17105">https://arxiv.org/abs/2502.17105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17105">https://arxiv.org/pdf/2502.17105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17105]] SFLD: Reducing the content bias for AI-generated Image Detection(https://arxiv.org/abs/2502.17105)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Identifying AI-generated content is critical for the safe and ethical use of generative AI. Recent research has focused on developing detectors that generalize to unknown generators, with popular methods relying either on high-level features or low-level fingerprints. However, these methods have clear limitations: biased towards unseen content, or vulnerable to common image degradations, such as JPEG compression. To address these issues, we propose a novel approach, SFLD, which incorporates PatchShuffle to integrate high-level semantic and low-level textural information. SFLD applies PatchShuffle at multiple levels, improving robustness and generalization across various generative models. Additionally, current benchmarks face challenges such as low image quality, insufficient content preservation, and limited class diversity. In response, we introduce TwinSynths, a new benchmark generation methodology that constructs visually near-identical pairs of real and synthetic images to ensure high quality and content preservation. Our extensive experiments and analysis show that SFLD outperforms existing methods on detecting a wide variety of fake images sourced from GANs, diffusion models, and TwinSynths, demonstrating the state-of-the-art performance and generalization capabilities to novel generative models.</li>
</ul>

<h3>Title: Diffusion Models for Tabular Data: Challenges, Current Progress, and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Zhong Li, Qi Huang, Lincen Yang, Jiayang Shi, Zhao Yang, Niki van Stein, Thomas Bäck, Matthijs van Leeuwen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17119">https://arxiv.org/abs/2502.17119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17119">https://arxiv.org/pdf/2502.17119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17119]] Diffusion Models for Tabular Data: Challenges, Current Progress, and Future Directions(https://arxiv.org/abs/2502.17119)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In recent years, generative models have achieved remarkable performance across diverse applications, including image generation, text synthesis, audio creation, video generation, and data augmentation. Diffusion models have emerged as superior alternatives to Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) by addressing their limitations, such as training instability, mode collapse, and poor representation of multimodal distributions. This success has spurred widespread research interest. In the domain of tabular data, diffusion models have begun to showcase similar advantages over GANs and VAEs, achieving significant performance breakthroughs and demonstrating their potential for addressing unique challenges in tabular data modeling. However, while domains like images and time series have numerous surveys summarizing advancements in diffusion models, there remains a notable gap in the literature for tabular data. Despite the increasing interest in diffusion models for tabular data, there has been little effort to systematically review and summarize these developments. This lack of a dedicated survey limits a clear understanding of the challenges, progress, and future directions in this critical area. This survey addresses this gap by providing a comprehensive review of diffusion models for tabular data. Covering works from June 2015, when diffusion models emerged, to December 2024, we analyze nearly all relevant studies, with updates maintained in a \href{this https URL}{GitHub repository}. Assuming readers possess foundational knowledge of statistics and diffusion models, we employ mathematical formulations to deliver a rigorous and detailed review, aiming to promote developments in this emerging and exciting area.</li>
</ul>

<h3>Title: Adversarial Training for Defense Against Label Poisoning Attacks</h3>
<ul>
<li><strong>Authors: </strong>Melis Ilayda Bal, Volkan Cevher, Michael Muehlebach</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17121">https://arxiv.org/abs/2502.17121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17121">https://arxiv.org/pdf/2502.17121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17121]] Adversarial Training for Defense Against Label Poisoning Attacks(https://arxiv.org/abs/2502.17121)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>As machine learning models grow in complexity and increasingly rely on publicly sourced data, such as the human-annotated labels used in training large language models, they become more vulnerable to label poisoning attacks. These attacks, in which adversaries subtly alter the labels within a training dataset, can severely degrade model performance, posing significant risks in critical applications. In this paper, we propose FLORAL, a novel adversarial training defense strategy based on support vector machines (SVMs) to counter these threats. Utilizing a bilevel optimization framework, we cast the training process as a non-zero-sum Stackelberg game between an attacker, who strategically poisons critical training labels, and the model, which seeks to recover from such attacks. Our approach accommodates various model architectures and employs a projected gradient descent algorithm with kernel SVMs for adversarial training. We provide a theoretical analysis of our algorithm's convergence properties and empirically evaluate FLORAL's effectiveness across diverse classification tasks. Compared to robust baselines and foundation models such as RoBERTa, FLORAL consistently achieves higher robust accuracy under increasing attacker budgets. These results underscore the potential of FLORAL to enhance the resilience of machine learning models against label poisoning threats, thereby ensuring robust classification in adversarial settings.</li>
</ul>

<h3>Title: Thus Spake Long-Context Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Xiaoran Liu, Ruixiao Li, Mianqiu Huang, Zhigeng Liu, Yuerong Song, Qipeng Guo, Siyang He, Qiqi Wang, Linlin Li, Qun Liu, Yaqian Zhou, Xuanjing Huang, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17129">https://arxiv.org/abs/2502.17129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17129">https://arxiv.org/pdf/2502.17129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17129]] Thus Spake Long-Context Large Language Model(https://arxiv.org/abs/2502.17129)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long context is an important topic in Natural Language Processing (NLP), running through the development of NLP architectures, and offers immense opportunities for Large Language Models (LLMs) giving LLMs the lifelong learning potential akin to humans. Unfortunately, the pursuit of a long context is accompanied by numerous obstacles. Nevertheless, long context remains a core competitive advantage for LLMs. In the past two years, the context length of LLMs has achieved a breakthrough extension to millions of tokens. Moreover, the research on long-context LLMs has expanded from length extrapolation to a comprehensive focus on architecture, infrastructure, training, and evaluation technologies. Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy between the journey of extending the context of LLM and the attempts of humans to transcend its mortality. In this survey, We will illustrate how LLM struggles between the tremendous need for a longer context and its equal need to accept the fact that it is ultimately finite. To achieve this, we give a global picture of the lifecycle of long-context LLMs from four perspectives: architecture, infrastructure, training, and evaluation, showcasing the full spectrum of long-context technologies. At the end of this survey, we will present 10 unanswered questions currently faced by long-context LLMs. We hope this survey can serve as a systematic introduction to the research on long-context LLMs.</li>
</ul>

<h3>Title: Sentiment analysis of texts from social networks based on machine learning methods for monitoring public sentiment</h3>
<ul>
<li><strong>Authors: </strong>Arsen Tolebay Nurlanuly</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17143">https://arxiv.org/abs/2502.17143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17143">https://arxiv.org/pdf/2502.17143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17143]] Sentiment analysis of texts from social networks based on machine learning methods for monitoring public sentiment(https://arxiv.org/abs/2502.17143)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>A sentiment analysis system powered by machine learning was created in this study to improve real-time social network public opinion monitoring. For sophisticated sentiment identification, the suggested approach combines cutting-edge transformer-based architectures (DistilBERT, RoBERTa) with traditional machine learning models (Logistic Regression, SVM, Naive Bayes). The system achieved an accuracy of up to 80-85% using transformer models in real-world scenarios after being tested using both deep learning techniques and standard machine learning processes on annotated social media datasets. According to experimental results, deep learning models perform noticeably better than lexicon-based and conventional rule-based classifiers, lowering misclassification rates and enhancing the ability to recognize nuances like sarcasm. According to feature importance analysis, context tokens, sentiment-bearing keywords, and part-of-speech structure are essential for precise categorization. The findings confirm that AI-driven sentiment frameworks can provide a more adaptive and efficient approach to modern sentiment challenges. Despite the system's impressive performance, issues with computing overhead, data quality, and domain-specific terminology still exist. In order to monitor opinions on a broad scale, future research will investigate improving computing performance, extending coverage to various languages, and integrating real-time streaming APIs. The results demonstrate that governments, corporations, and social researchers looking for more in-depth understanding of public mood on digital platforms can find a reliable and adaptable answer in AI-powered sentiment analysis.</li>
</ul>

<h3>Title: MaxGlaViT: A novel lightweight vision transformer-based approach for early diagnosis of glaucoma stages from fundus images</h3>
<ul>
<li><strong>Authors: </strong>Mustafa Yurdakul, Kubra Uyar, Sakir Tasdemir</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17154">https://arxiv.org/abs/2502.17154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17154">https://arxiv.org/pdf/2502.17154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17154]] MaxGlaViT: A novel lightweight vision transformer-based approach for early diagnosis of glaucoma stages from fundus images(https://arxiv.org/abs/2502.17154)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Glaucoma is a prevalent eye disease that progresses silently without symptoms. If not detected and treated early, it can cause permanent vision loss. Computer-assisted diagnosis systems play a crucial role in timely and efficient identification. This study introduces MaxGlaViT, a lightweight model based on the restructured Multi-Axis Vision Transformer (MaxViT) for early glaucoma detection. First, MaxViT was scaled to optimize block and channel numbers, resulting in a lighter architecture. Second, the stem was enhanced by adding attention mechanisms (CBAM, ECA, SE) after convolution layers to improve feature learning. Third, MBConv structures in MaxViT blocks were replaced by advanced DL blocks (ConvNeXt, ConvNeXtV2, InceptionNeXt). The model was evaluated using the HDV1 dataset, containing fundus images of different glaucoma stages. Additionally, 40 CNN and 40 ViT models were tested on HDV1 to validate MaxGlaViT's efficiency. Among CNN models, EfficientB6 achieved the highest accuracy (84.91%), while among ViT models, MaxViT-Tiny performed best (86.42%). The scaled MaxViT reached 87.93% accuracy. Adding ECA to the stem block increased accuracy to 89.01%. Replacing MBConv with ConvNeXtV2 further improved it to 89.87%. Finally, integrating ECA in the stem and ConvNeXtV2 in MaxViT blocks resulted in 92.03% accuracy. Testing 80 DL models for glaucoma stage classification, this study presents a comprehensive and comparative analysis. MaxGlaViT outperforms experimental and state-of-the-art models, achieving 92.03% accuracy, 92.33% precision, 92.03% recall, 92.13% f1-score, and 87.12% Cohen's kappa score.</li>
</ul>

<h3>Title: DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks</h3>
<ul>
<li><strong>Authors: </strong>Canyu Zhao, Mingyu Liu, Huanyi Zheng, Muzhi Zhu, Zhiyue Zhao, Hao Chen, Tong He, Chunhua Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17157">https://arxiv.org/abs/2502.17157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17157">https://arxiv.org/pdf/2502.17157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17157]] DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks(https://arxiv.org/abs/2502.17157)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Our primary goal here is to create a good, generalist perception model that can tackle multiple tasks, within limits on computational resources and training data. To achieve this, we resort to text-to-image diffusion models pre-trained on billions of images. Our exhaustive evaluation metrics demonstrate that DICEPTION effectively tackles multiple perception tasks, achieving performance on par with state-of-the-art models. We achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs. 1B pixel-level annotated images). Inspired by Wang et al., DICEPTION formulates the outputs of various perception tasks using color encoding; and we show that the strategy of assigning random colors to different instances is highly effective in both entity segmentation and semantic segmentation. Unifying various perception tasks as conditional image generation enables us to fully leverage pre-trained text-to-image models. Thus, DICEPTION can be efficiently trained at a cost of orders of magnitude lower, compared to conventional models that were trained from scratch. When adapting our model to other tasks, it only requires fine-tuning on as few as 50 images and 1% of its parameters. DICEPTION provides valuable insights and a more promising solution for visual generalist models.</li>
</ul>

<h3>Title: Parameter Efficient Merging for Multimodal Large Language Models with Complementary Parameter Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Fanhu Zeng, Haiyang Guo, Fei Zhu, Li Shen, Hao Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17159">https://arxiv.org/abs/2502.17159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17159">https://arxiv.org/pdf/2502.17159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17159]] Parameter Efficient Merging for Multimodal Large Language Models with Complementary Parameter Adaptation(https://arxiv.org/abs/2502.17159)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning pre-trained models with custom data leads to numerous expert models on specific tasks. Merging models into one universal model to empower multi-task ability refraining from data leakage has gained popularity. With the expansion in data and model size, parameter efficient tuning becomes the common practice for obtaining task-specific models efficiently. However, we observe that existing methods designed for full fine-tuning merging fail under efficient tuning. To address the issues, we analyze from low-rank decomposition and reveal that maintaining direction and compensating for gap between singular values are crucial for efficient model merging. Consequently, we propose CoPA-Merging, a training-free parameter efficient merging method with complementary parameter adaptation. Specifically, we (1) prune parameters and construct scaling coefficients from inter-parameter relation to compensate for performance drop from task interference and (2) perform cross-task normalization to enhance unseen task generalization. We establish a benchmark consisting of diverse multimodal tasks, on which we conduct experiments to certificate the outstanding performance and generalizability of our method. Additional study and extensive analyses further showcase the effectiveness.</li>
</ul>

<h3>Title: A Pragmatic Note on Evaluating Generative Models with Fréchet Inception Distance for Retinal Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yuli Wu, Fucheng Liu, Rüveyda Yilmaz, Henning Konermann, Peter Walter, Johannes Stegmaier</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17160">https://arxiv.org/abs/2502.17160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17160">https://arxiv.org/pdf/2502.17160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17160]] A Pragmatic Note on Evaluating Generative Models with Fréchet Inception Distance for Retinal Image Synthesis(https://arxiv.org/abs/2502.17160)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Fréchet Inception Distance (FID), computed with an ImageNet pretrained Inception-v3 network, is widely used as a state-of-the-art evaluation metric for generative models. It assumes that feature vectors from Inception-v3 follow a multivariate Gaussian distribution and calculates the 2-Wasserstein distance based on their means and covariances. While FID effectively measures how closely synthetic data match real data in many image synthesis tasks, the primary goal in biomedical generative models is often to enrich training datasets ideally with corresponding annotations. For this purpose, the gold standard for evaluating generative models is to incorporate synthetic data into downstream task training, such as classification and segmentation, to pragmatically assess its performance. In this paper, we examine cases from retinal imaging modalities, including color fundus photography and optical coherence tomography, where FID and its related metrics misalign with task-specific evaluation goals in classification and segmentation. We highlight the limitations of using various metrics, represented by FID and its variants, as evaluation criteria for these applications and address their potential caveats in broader biomedical imaging modalities and downstream tasks.</li>
</ul>

<h3>Title: MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>María Andrea Cruz Blandón, Jayasimha Talur, Bruno Charron, Dong Liu, Saab Mansour, Marcello Federico</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17163">https://arxiv.org/abs/2502.17163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17163">https://arxiv.org/pdf/2502.17163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17163]] MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation(https://arxiv.org/abs/2502.17163)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience. In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance. We describe our annotation process and show that it achieves high inter-annotator agreement. We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators. Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs. We release our benchmark to support the community developing accurate evaluation methods for multilingual RAG systems.</li>
</ul>

<h3>Title: JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for Legal Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Huanghai Liu, Quzhe Huang, Qingjing Chen, Yiran Hu, Jiayu Ma, Yun Liu, Weixing Shen, Yansong Feng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17166">https://arxiv.org/abs/2502.17166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17166">https://arxiv.org/pdf/2502.17166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17166]] JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for Legal Reasoning(https://arxiv.org/abs/2502.17166)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Four-Element Theory is a fundamental framework in criminal law, defining the constitution of crime through four dimensions: Subject, Object, Subjective aspect, and Objective aspect. This theory is widely referenced in legal reasoning, and many Large Language Models (LLMs) attempt to incorporate it when handling legal tasks. However, current approaches rely on LLMs' internal knowledge to incorporate this theory, often lacking completeness and representativeness. To address this limitation, we introduce JUREX-4E, an expert-annotated knowledge base covering 155 criminal charges. It is structured through a progressive hierarchical annotation framework that prioritizes legal source validity and employs diverse legal interpretation methods to ensure comprehensiveness and authority. We evaluate JUREX-4E on the Similar Charge Distinction task and apply it to Legal Case Retrieval, demonstrating its effectiveness in improving LLM performance. Experimental results validate the high quality of JUREX-4E and its substantial impact on downstream legal tasks, underscoring its potential for advancing legal AI applications. Code: this https URL</li>
</ul>

<h3>Title: Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without Easily Identifiable Unrelated Padding)</h3>
<ul>
<li><strong>Authors: </strong>Damien Sileo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17169">https://arxiv.org/abs/2502.17169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17169">https://arxiv.org/pdf/2502.17169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17169]] Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without Easily Identifiable Unrelated Padding)(https://arxiv.org/abs/2502.17169)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models demonstrate promising long context processing capabilities, with recent models touting context windows close to one million tokens. However, the evaluations supporting these claims often involve simple retrieval tasks or synthetic tasks padded with irrelevant text, which the models may easily detect and discard. In this work, we generate lengthy simplified English text with first-order logic representations spanning up to 2048 clauses (around 25k GPT-4 tokens). We formulate an evaluation task with evidence retrieval for contradiction detection. The long, homogeneous text is filled with distractors that are both hard to distinguish from relevant evidences and provably not interfering with them. Our evaluation of evidence retrieval shows that the effective context window is much smaller with realistic distractors, already crumbling at 128 clauses.</li>
</ul>

<h3>Title: Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch</h3>
<ul>
<li><strong>Authors: </strong>Xueru Wen, Jie Lou, Zichao Li, Yaojie Lu, Xing Yu, Yuqiu Ji, Guohai Xu, Hongyu Lin, Ben He, Xianpei Han, Le Sun, Debing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17173">https://arxiv.org/abs/2502.17173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17173">https://arxiv.org/pdf/2502.17173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17173]] Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch(https://arxiv.org/abs/2502.17173)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Reward models (RMs) are crucial for aligning large language models (LLMs) with human preferences. However, most RM research is centered on English and relies heavily on synthetic resources, which leads to limited and less reliable datasets and benchmarks for Chinese. To address this gap, we introduce CheemsBench, a fully human-annotated RM evaluation benchmark within Chinese contexts, and CheemsPreference, a large-scale and diverse preference dataset annotated through human-machine collaboration to support Chinese RM training. We systematically evaluate open-source discriminative and generative RMs on CheemsBench and observe significant limitations in their ability to capture human preferences in Chinese scenarios. Additionally, based on CheemsPreference, we construct an RM that achieves state-of-the-art performance on CheemsBench, demonstrating the necessity of human supervision in RM training. Our findings reveal that scaled AI-generated data struggles to fully capture human preferences, emphasizing the importance of high-quality human supervision in RM development.</li>
</ul>

<h3>Title: Measuring Data Diversity for Instruction Tuning: A Systematic Analysis and A Reliable Metric</h3>
<ul>
<li><strong>Authors: </strong>Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Tao Gui, Qi Zhang, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17184">https://arxiv.org/abs/2502.17184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17184">https://arxiv.org/pdf/2502.17184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17184]] Measuring Data Diversity for Instruction Tuning: A Systematic Analysis and A Reliable Metric(https://arxiv.org/abs/2502.17184)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Data diversity is crucial for the instruction tuning of large language models. Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance. However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering. To address this, we systematically analyze 11 existing diversity measurement methods by assessing their correlation with model performance through extensive fine-tuning experiments. Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information distribution in the sample space. Building on this, we propose NovelSum, a new diversity metric based on sample-level "novelty." Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices. With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric.</li>
</ul>

<h3>Title: Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks</h3>
<ul>
<li><strong>Authors: </strong>Andrei Chernov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17187">https://arxiv.org/abs/2502.17187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17187">https://arxiv.org/pdf/2502.17187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17187]] Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks(https://arxiv.org/abs/2502.17187)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, Large Language Models (LLMs) with Mixture of Experts (MoE) layers have gained significant attention. Currently, state-of-the-art LLMs utilize this architecture. There is a substantial amount of research on how to train such models and how to select hyperparameters for this architecture. However, there is a lack of studies focusing on post-evaluation analysis of MoE layer properties. In this paper, we take a first step toward closing this gap by evaluating expert contributions on the quiz-based MMLU benchmark. We show that most experts were never activated during inference on this benchmark. Additionally, the output distribution of gating networks is much closer to uniform than sparse. Finally, we demonstrate that the average performance of some experts within the same layer varies significantly.</li>
</ul>

<h3>Title: IGDA: Interactive Graph Discovery through Large Language Model Agents</h3>
<ul>
<li><strong>Authors: </strong>Alex Havrilla, David Alvarez-Melis, Nicolo Fusi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17189">https://arxiv.org/abs/2502.17189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17189">https://arxiv.org/pdf/2502.17189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17189]] IGDA: Interactive Graph Discovery through Large Language Model Agents(https://arxiv.org/abs/2502.17189)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models ($\textbf{LLMs}$) have emerged as a powerful method for discovery. Instead of utilizing numerical data, LLMs utilize associated variable $\textit{semantic metadata}$ to predict variable relationships. Simultaneously, LLMs demonstrate impressive abilities to act as black-box optimizers when given an objective $f$ and sequence of trials. We study LLMs at the intersection of these two capabilities by applying LLMs to the task of $\textit{interactive graph discovery}$: given a ground truth graph $G^*$ capturing variable relationships and a budget of $I$ edge experiments over $R$ rounds, minimize the distance between the predicted graph $\hat{G}_R$ and $G^*$ at the end of the $R$-th round. To solve this task we propose $\textbf{IGDA}$, a LLM-based pipeline incorporating two key components: 1) an LLM uncertainty-driven method for edge experiment selection 2) a local graph update strategy utilizing binary feedback from experiments to improve predictions for unselected neighboring edges. Experiments on eight different real-world graphs show our approach often outperforms all baselines including a state-of-the-art numerical method for interactive graph discovery. Further, we conduct a rigorous series of ablations dissecting the impact of each pipeline component. Finally, to assess the impact of memorization, we apply our interactive graph discovery strategy to a complex, new (as of July 2024) causal graph on protein transcription factors, finding strong performance in a setting where memorization is impossible. Overall, our results show IGDA to be a powerful method for graph discovery complementary to existing numerically driven approaches.</li>
</ul>

<h3>Title: Disentangling Visual Transformers: Patch-level Interpretability for Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Guillaume Jeanneret, Loïc Simon, Frédéric Jurie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17196">https://arxiv.org/abs/2502.17196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17196">https://arxiv.org/pdf/2502.17196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17196]] Disentangling Visual Transformers: Patch-level Interpretability for Image Classification(https://arxiv.org/abs/2502.17196)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Visual transformers have achieved remarkable performance in image classification tasks, but this performance gain has come at the cost of interpretability. One of the main obstacles to the interpretation of transformers is the self-attention mechanism, which mixes visual information across the whole image in a complex way. In this paper, we propose Hindered Transformer (HiT), a novel interpretable by design architecture inspired by visual transformers. Our proposed architecture rethinks the design of transformers to better disentangle patch influences at the classification stage. Ultimately, HiT can be interpreted as a linear combination of patch-level information. We show that the advantages of our approach in terms of explicability come with a reasonable trade-off in performance, making it an attractive alternative for applications where interpretability is paramount.</li>
</ul>

<h3>Title: Dimitra: Audio-driven Diffusion model for Expressive Talking Head Generation</h3>
<ul>
<li><strong>Authors: </strong>Baptiste Chopin, Tashvik Dhamija, Pranav Balaji, Yaohui Wang, Antitza Dantcheva</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17198">https://arxiv.org/abs/2502.17198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17198">https://arxiv.org/pdf/2502.17198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17198]] Dimitra: Audio-driven Diffusion model for Expressive Talking Head Generation(https://arxiv.org/abs/2502.17198)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We propose Dimitra, a novel framework for audio-driven talking head generation, streamlined to learn lip motion, facial expression, as well as head pose motion. Specifically, we train a conditional Motion Diffusion Transformer (cMDT) by modeling facial motion sequences with 3D representation. We condition the cMDT with only two input signals, an audio-sequence, as well as a reference facial image. By extracting additional features directly from audio, Dimitra is able to increase quality and realism of generated videos. In particular, phoneme sequences contribute to the realism of lip motion, whereas text transcript to facial expression and head pose realism. Quantitative and qualitative experiments on two widely employed datasets, VoxCeleb2 and HDTF, showcase that Dimitra is able to outperform existing approaches for generating realistic talking heads imparting lip motion, facial expression, and head pose.</li>
</ul>

<h3>Title: Order Matters: Investigate the Position Bias in Multi-constraint Instruction Following</h3>
<ul>
<li><strong>Authors: </strong>Jie Zeng, Qianyu He, Qingyu Ren, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17204">https://arxiv.org/abs/2502.17204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17204">https://arxiv.org/pdf/2502.17204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17204]] Order Matters: Investigate the Position Bias in Multi-constraint Instruction Following(https://arxiv.org/abs/2502.17204)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Real-world instructions with multiple constraints pose a significant challenge to existing large language models (LLMs). An observation is that the LLMs exhibit dramatic performance fluctuation when disturbing the order of the incorporated constraints. Yet, none of the existing works has systematically investigated this position bias problem in the field of multi-constraint instruction following. To bridge this gap, we design a probing task where we quantitatively measure the difficulty distribution of the constraints by a novel Difficulty Distribution Index (CDDI). Through the experimental results, we find that LLMs are more performant when presented with the constraints in a ``hard-to-easy'' order. This preference can be generalized to LLMs with different architecture or different sizes of parameters. Additionally, we conduct an explanation study, providing an intuitive insight into the correlation between the LLM's attention and constraint orders. Our code and dataset are publicly available at this https URL.</li>
</ul>

<h3>Title: Neural Attention: A Novel Mechanism for Enhanced Expressive Power in Transformer Models</h3>
<ul>
<li><strong>Authors: </strong>Andrew DiGiugno, Ausif Mahmood</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17206">https://arxiv.org/abs/2502.17206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17206">https://arxiv.org/pdf/2502.17206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17206]] Neural Attention: A Novel Mechanism for Enhanced Expressive Power in Transformer Models(https://arxiv.org/abs/2502.17206)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer models typically calculate attention matrices using dot products, which have limitations when capturing nonlinear relationships between embedding vectors. We propose Neural Attention, a technique that replaces dot products with feed-forward networks, enabling a more expressive representation of relationships between tokens. This approach modifies only the attention matrix calculation while preserving the matrix dimensions, making it easily adaptable to existing transformer-based architectures. We provide a detailed mathematical justification for why Neural Attention increases representational capacity and conduct controlled experiments to validate this claim. When comparing Neural Attention and Dot-Product Attention, NLP experiments on WikiText-103 show a reduction in perplexity of over 5 percent. Similarly, experiments on CIFAR-10 and CIFAR-100 show comparable improvements for image classification tasks. While Neural Attention introduces higher computational demands, we develop techniques to mitigate these challenges, ensuring practical usability without sacrificing the increased expressivity it provides. This work establishes Neural Attention as an effective means of enhancing the predictive capabilities of transformer models across a variety of applications.</li>
</ul>

<h3>Title: CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Boxuan Zhang, Ruqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17214">https://arxiv.org/abs/2502.17214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17214">https://arxiv.org/pdf/2502.17214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17214]] CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought(https://arxiv.org/abs/2502.17214)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel in many tasks but struggle to accurately quantify uncertainty in their generated responses. This limitation makes it challenging to detect misinformation and ensure reliable decision-making. Existing uncertainty quantification (UQ) methods for LLMs are primarily prompt-wise rather than response-wise, often requiring multiple response samples, which incurs high computational costs. Moreover, LLMs have been shown to be overconfident, particularly when using reasoning steps to derive their answers. In this work, we propose CoT-UQ, a response-wise UQ framework that integrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT) into the UQ process. CoT-UQ captures critical information during inference by extracting keywords from each reasoning step and assessing their importance to the final answer. This key reasoning information is then aggregated to produce a final uncertainty estimate. We conduct extensive experiments based on LLaMA Family with model sizes varying from 8B to 13B across logical and mathematical reasoning tasks. Experimental results demonstrate that CoT-UQ significantly outperforms existing UQ methods, achieving an average improvement of 5.9% AUROC compared to current UQ methods. The code is available at: this https URL.</li>
</ul>

<h3>Title: Electrical Load Forecasting over Multihop Smart Metering Networks with Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Ratun Rahman, Pablo Moriano, Samee U. Khan, Dinh C. Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17226">https://arxiv.org/abs/2502.17226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17226">https://arxiv.org/pdf/2502.17226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17226]] Electrical Load Forecasting over Multihop Smart Metering Networks with Federated Learning(https://arxiv.org/abs/2502.17226)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Electric load forecasting is essential for power management and stability in smart grids. This is mainly achieved via advanced metering infrastructure, where smart meters (SMs) record household energy data. Traditional machine learning (ML) methods are often employed for load forecasting but require data sharing which raises data privacy concerns. Federated learning (FL) can address this issue by running distributed ML models at local SMs without data exchange. However, current FL-based approaches struggle to achieve efficient load forecasting due to imbalanced data distribution across heterogeneous SMs. This paper presents a novel personalized federated learning (PFL) method for high-quality load forecasting in metering networks. A meta-learning-based strategy is developed to address data heterogeneity at local SMs in the collaborative training of local load forecasting models. Moreover, to minimize the load forecasting delays in our PFL model, we study a new latency optimization problem based on optimal resource allocation at SMs. A theoretical convergence analysis is also conducted to provide insights into FL design for federated load forecasting. Extensive simulations from real-world datasets show that our method outperforms existing approaches in terms of better load forecasting and reduced operational latency costs.</li>
</ul>

<h3>Title: Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction</h3>
<ul>
<li><strong>Authors: </strong>Tianpeng Li, Jun Liu, Tao Zhang, Yuanbo Fang, Da Pan, Mingrui Wang, Zheng Liang, Zehuan Li, Mingan Lin, Guosheng Dong, Jianhua Xu, Haoze Sun, Zenan Zhou, Weipeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17239">https://arxiv.org/abs/2502.17239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17239">https://arxiv.org/pdf/2502.17239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17239]] Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction(https://arxiv.org/abs/2502.17239)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at this https URL</li>
</ul>

<h3>Title: UNB StepUP: A footStep database for gait analysis and recognition using Underfoot Pressure</h3>
<ul>
<li><strong>Authors: </strong>Robyn Larracy, Angkoon Phinyomark, Ala Salehi, Eve MacDonald, Saeed Kazemi, Shikder Shafiul Bashar, Aaron Tabor, Erik Scheme</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17244">https://arxiv.org/abs/2502.17244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17244">https://arxiv.org/pdf/2502.17244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17244]] UNB StepUP: A footStep database for gait analysis and recognition using Underfoot Pressure(https://arxiv.org/abs/2502.17244)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>Gait refers to the patterns of limb movement generated during walking, which are unique to each individual due to both physical and behavioural traits. Walking patterns have been widely studied in biometrics, biomechanics, sports, and rehabilitation. While traditional methods rely on video and motion capture, advances in underfoot pressure sensing technology now offer deeper insights into gait. However, underfoot pressures during walking remain underexplored due to the lack of large, publicly accessible datasets. To address this, the UNB StepUP database was created, featuring gait pressure data collected with high-resolution pressure sensing tiles (4 sensors/cm\textsuperscript{2}, 1.2m by 3.6m). Its first release, UNB StepUP-P150, includes over 200,000 footsteps from 150 individuals across various walking speeds (preferred, slow-to-stop, fast, and slow) and footwear types (barefoot, standard shoes, and two personal shoes). As the largest and most comprehensive dataset of its kind, it supports biometric gait recognition while presenting new research opportunities in biomechanics and deep learning. The UNB StepUP-P150 dataset sets a new benchmark for pressure-based gait analysis and recognition.</li>
</ul>

<h3>Title: Overconfident Oracles: Limitations of In Silico Sequence Design Benchmarking</h3>
<ul>
<li><strong>Authors: </strong>Shikha Surana, Nathan Grinsztajn, Timothy Atkinson, Paul Duckworth, Thomas D. Barrett</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17246">https://arxiv.org/abs/2502.17246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17246">https://arxiv.org/pdf/2502.17246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17246]] Overconfident Oracles: Limitations of In Silico Sequence Design Benchmarking(https://arxiv.org/abs/2502.17246)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Machine learning methods can automate the in silico design of biological sequences, aiming to reduce costs and accelerate medical research. Given the limited access to wet labs, in silico design methods commonly use an oracle model to evaluate de novo generated sequences. However, the use of different oracle models across methods makes it challenging to compare them reliably, motivating the question: are in silico sequence design benchmarks reliable? In this work, we examine 12 sequence design methods that utilise ML oracles common in the literature and find that there are significant challenges with their cross-consistency and reproducibility. Indeed, oracles differing by architecture, or even just training seed, are shown to yield conflicting relative performance with our analysis suggesting poor out-of-distribution generalisation as a key issue. To address these challenges, we propose supplementing the evaluation with a suite of biophysical measures to assess the viability of generated sequences and limit out-of-distribution sequences the oracle is required to score, thereby improving the robustness of the design procedure. Our work aims to highlight potential pitfalls in the current evaluation process and contribute to the development of robust benchmarks, ultimately driving the improvement of in silico design methods.</li>
</ul>

<h3>Title: REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective</h3>
<ul>
<li><strong>Authors: </strong>Simon Geisler, Tom Wollschläger, M. H. I. Abdalla, Vincent Cohen-Addad, Johannes Gasteiger, Stephan Günnemann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17254">https://arxiv.org/abs/2502.17254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17254">https://arxiv.org/pdf/2502.17254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17254]] REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective(https://arxiv.org/abs/2502.17254)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>To circumvent the alignment of large language models (LLMs), current optimization-based adversarial attacks usually craft adversarial prompts by maximizing the likelihood of a so-called affirmative response. An affirmative response is a manually designed start of a harmful answer to an inappropriate request. While it is often easy to craft prompts that yield a substantial likelihood for the affirmative response, the attacked model frequently does not complete the response in a harmful manner. Moreover, the affirmative objective is usually not adapted to model-specific preferences and essentially ignores the fact that LLMs output a distribution over responses. If low attack success under such an objective is taken as a measure of robustness, the true robustness might be grossly overestimated. To alleviate these flaws, we propose an adaptive and semantic optimization problem over the population of responses. We derive a generally applicable objective via the REINFORCE policy-gradient formalism and demonstrate its efficacy with the state-of-the-art jailbreak algorithms Greedy Coordinate Gradient (GCG) and Projected Gradient Descent (PGD). For example, our objective doubles the attack success rate (ASR) on Llama3 and increases the ASR from 2% to 50% with circuit breaker defense.</li>
</ul>

<h3>Title: VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing</h3>
<ul>
<li><strong>Authors: </strong>Xiangpeng Yang, Linchao Zhu, Hehe Fan, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17258">https://arxiv.org/abs/2502.17258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17258">https://arxiv.org/pdf/2502.17258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17258]] VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing(https://arxiv.org/abs/2502.17258)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion models have significantly improved video generation and editing capabilities. However, multi-grained video editing, which encompasses class-level, instance-level, and part-level modifications, remains a formidable challenge. The major difficulties in multi-grained editing include semantic misalignment of text-to-region control and feature coupling within the diffusion model. To address these difficulties, we present VideoGrain, a zero-shot approach that modulates space-time (cross- and self-) attention mechanisms to achieve fine-grained control over video content. We enhance text-to-region control by amplifying each local prompt's attention to its corresponding spatial-disentangled region while minimizing interactions with irrelevant areas in cross-attention. Additionally, we improve feature separation by increasing intra-region awareness and reducing inter-region interference in self-attention. Extensive experiments demonstrate our method achieves state-of-the-art performance in real-world scenarios. Our code, data, and demos are available at this https URL</li>
</ul>

<h3>Title: Detecting Benchmark Contamination Through Watermarking</h3>
<ul>
<li><strong>Authors: </strong>Tom Sander, Pierre Fernandez, Saeed Mahloujifar, Alain Durmus, Chuan Guo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17259">https://arxiv.org/abs/2502.17259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17259">https://arxiv.org/pdf/2502.17259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17259]] Detecting Benchmark Contamination Through Watermarking(https://arxiv.org/abs/2502.17259)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark, large language model</a></li>
<li><strong>Abstract: </strong>Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect ``radioactivity'', \ie traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g. $p$-val $=10^{-3}$ for +5$\%$ on ARC-Easy.</li>
</ul>

<h3>Title: Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective</h3>
<ul>
<li><strong>Authors: </strong>Chengyin Xu, Kaiyuan Chen, Xiao Li, Ke Shen, Chenggang Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17262">https://arxiv.org/abs/2502.17262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17262">https://arxiv.org/pdf/2502.17262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17262]] Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective(https://arxiv.org/abs/2502.17262)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) the "emergence phenomenon", wherein downstream performance metrics become meaningful only after extensive training, which limits the ability to use smaller models for prediction; (2) Uneven task difficulty distributions and the absence of consistent scaling laws, resulting in substantial metric variability. Existing performance prediction methods suffer from limited accuracy and reliability, thereby impeding the assessment of potential LLM capabilities. To address these challenges, we propose a Clustering-On-Difficulty (COD) downstream performance prediction framework. COD first constructs a predictable support subset by clustering tasks based on difficulty features, strategically excluding non-emergent and non-scalable clusters. The scores on the selected subset serve as effective intermediate predictors of downstream performance on the full evaluation set. With theoretical support, we derive a mapping function that transforms performance metrics from the predictable subset to the full evaluation set, thereby ensuring accurate extrapolation of LLM downstream performance. The proposed method has been applied to predict performance scaling for a 70B LLM, providing actionable insights for training resource allocation and assisting in monitoring the training process. Notably, COD achieves remarkable predictive accuracy on the 70B LLM by leveraging an ensemble of small models, demonstrating an absolute mean deviation of 1.36% across eight important LLM evaluation benchmarks.</li>
</ul>

<h3>Title: MonoTODia: Translating Monologue Requests to Task-Oriented Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Steindl, Ulrich Schäfer, Bernd Ludwig</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17268">https://arxiv.org/abs/2502.17268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17268">https://arxiv.org/pdf/2502.17268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17268]] MonoTODia: Translating Monologue Requests to Task-Oriented Dialogues(https://arxiv.org/abs/2502.17268)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Data scarcity is one of the main problems when it comes to real-world applications of transformer-based models. This is especially evident for task-oriented dialogue (TOD) systems, which require specialized datasets, that are usually not readily available. This can hinder companies from adding TOD systems to their services. This study therefore investigates a novel approach to sourcing annotated dialogues from existing German monologue material. Focusing on a real-world example, we investigate whether these monologues can be transformed into dialogue formats suitable for training TOD systems. We show the approach with the concrete example of a company specializing in travel bookings via e-mail. We fine-tune state-of-the-art Large Language Models for the task of rewriting e-mails as dialogues and annotating them. To ensure the quality and validity of the generated data, we employ crowd workers to evaluate the dialogues across multiple criteria and to provide gold-standard annotations for the test dataset. We further evaluate the usefulness of the dialogues for training TOD systems. Our evaluation shows that the dialogues and annotations are of high quality and can serve as a valuable starting point for training TOD systems. Finally, we make the annotated dataset publicly available to foster future research.</li>
</ul>

<h3>Title: Order Fairness Evaluation of DAG-based ledgers</h3>
<ul>
<li><strong>Authors: </strong>Erwan Mahe, Sara Tucci-Piergiovanni</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17270">https://arxiv.org/abs/2502.17270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17270">https://arxiv.org/pdf/2502.17270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17270]] Order Fairness Evaluation of DAG-based ledgers(https://arxiv.org/abs/2502.17270)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, fair</a></li>
<li><strong>Abstract: </strong>Order fairness in distributed ledgers refers to properties that relate the order in which transactions are sent or received to the order in which they are eventually finalized, i.e., totally ordered. The study of such properties is relatively new and has been especially stimulated by the rise of Maximal Extractable Value (MEV) attacks in blockchain environments. Indeed, in many classical blockchain protocols, leaders are responsible for selecting the transactions to be included in blocks, which creates a clear vulnerability and opportunity for transaction order manipulation. Unlike blockchains, DAG-based ledgers allow participants in the network to independently propose blocks, which are then arranged as vertices of a directed acyclic graph. Interestingly, leaders in DAG-based ledgers are elected only after the fact, once transactions are already part of the graph, to determine their total order. In other words, transactions are not chosen by single leaders; instead, they are collectively validated by the nodes, and leaders are only elected to establish an ordering. This approach intuitively reduces the risk of transaction manipulation and enhances fairness. In this paper, we aim to quantify the capability of DAG-based ledgers to achieve order fairness. To this end, we define new variants of order fairness adapted to DAG-based ledgers and evaluate the impact of an adversary capable of compromising a limited number of nodes (below the one-third threshold) to reorder transactions. We analyze how often our order fairness properties are violated under different network conditions and parameterizations of the DAG algorithm, depending on the adversary's power. Our study shows that DAG-based ledgers are still vulnerable to reordering attacks, as an adversary can coordinate a minority of Byzantine nodes to manipulate the DAG's structure.</li>
</ul>

<h3>Title: Extracting domain-specific terms using contextual word embeddings</h3>
<ul>
<li><strong>Authors: </strong>Andraž Repar, Nada Lavrač, Senja Pollak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17278">https://arxiv.org/abs/2502.17278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17278">https://arxiv.org/pdf/2502.17278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17278]] Extracting domain-specific terms using contextual word embeddings(https://arxiv.org/abs/2502.17278)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Automated terminology extraction refers to the task of extracting meaningful terms from domain-specific texts. This paper proposes a novel machine learning approach to terminology extraction, which combines features from traditional term extraction systems with novel contextual features derived from contextual word embeddings. Instead of using a predefined list of part-of-speech patterns, we first analyse a new term-annotated corpus RSDO5 for the Slovenian language and devise a set of rules for term candidate selection and then generate statistical, linguistic and context-based features. We use a support-vector machine algorithm to train a classification model, evaluate it on the four domains (biomechanics, linguistics, chemistry, veterinary) of the RSDO5 corpus and compare the results with state-of-art term extraction approaches for the Slovenian language. Our approach provides significant improvements in terms of F1 score over the previous state-of-the-art, which proves that contextual word embeddings are valuable for improving term extraction.</li>
</ul>

<h3>Title: Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing</h3>
<ul>
<li><strong>Authors: </strong>Yi-Kai Zhang, De-Chuan Zhan, Han-Jia Ye</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17282">https://arxiv.org/abs/2502.17282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17282">https://arxiv.org/pdf/2502.17282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17282]] Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing(https://arxiv.org/abs/2502.17282)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated human-like instruction-following abilities, particularly those exceeding 100 billion parameters. The combined capability of some smaller, resource-friendly LLMs can address most of the instructions that larger LLMs excel at. In this work, we explore how to route the best-performing LLM for each instruction to achieve better overall performance. We develop a new paradigm, constructing capability instructions with model capability representation, user instruction, and performance inquiry prompts to assess the performance. To learn from capability instructions, we introduce a new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with. Model-SAT uses a model capability encoder that extends its model representation to a lightweight LLM. Our experiments show that Model-SAT understands the performance dimensions of candidate models and provides the probabilities of their capability to handle various instructions. Additionally, during deployment, a new model can quickly infer its aptitude test results across 50 tasks, each with 20 shots. Model-SAT performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios. The code is available at this https URL</li>
</ul>

<h3>Title: GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow</h3>
<ul>
<li><strong>Authors: </strong>Simon Boeder, Fabian Gigengack, Benjamin Risse</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17288">https://arxiv.org/abs/2502.17288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17288">https://arxiv.org/pdf/2502.17288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17288]] GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow(https://arxiv.org/abs/2502.17288)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Occupancy estimation has become a prominent task in 3D computer vision, particularly within the autonomous driving community. In this paper, we present a novel approach to occupancy estimation, termed GaussianFlowOcc, which is inspired by Gaussian Splatting and replaces traditional dense voxel grids with a sparse 3D Gaussian representation. Our efficient model architecture based on a Gaussian Transformer significantly reduces computational and memory requirements by eliminating the need for expensive 3D convolutions used with inefficient voxel-based representations that predominantly represent empty 3D spaces. GaussianFlowOcc effectively captures scene dynamics by estimating temporal flow for each Gaussian during the overall network training process, offering a straightforward solution to a complex problem that is often neglected by existing methods. Moreover, GaussianFlowOcc is designed for scalability, as it employs weak supervision and does not require costly dense 3D voxel annotations based on additional data (e.g., LiDAR). Through extensive experimentation, we demonstrate that GaussianFlowOcc significantly outperforms all previous methods for weakly supervised occupancy estimation on the nuScenes dataset while featuring an inference speed that is 50 times faster than current SOTA.</li>
</ul>

<h3>Title: Delta Decompression for MoE-based LLMs Compression</h3>
<ul>
<li><strong>Authors: </strong>Hao Gu, Wei Li, Lujun Li, Qiyuan Zhu, Mark Lee, Shengjie Sun, Wei Xue, Yike Guo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17298">https://arxiv.org/abs/2502.17298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17298">https://arxiv.org/pdf/2502.17298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17298]] Delta Decompression for MoE-based LLMs Compression(https://arxiv.org/abs/2502.17298)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) architectures in large language models (LLMs) achieve exceptional performance, but face prohibitive storage and memory requirements. To address these challenges, we present $D^2$-MoE, a new delta decompression compressor for reducing the parameters of MoE LLMs. Based on observations of expert diversity, we decompose their weights into a shared base weight and unique delta weights. Specifically, our method first merges each expert's weight into the base weight using the Fisher information matrix to capture shared components. Then, we compress delta weights through Singular Value Decomposition (SVD) by exploiting their low-rank properties. Finally, we introduce a semi-dynamical structured pruning strategy for the base weights, combining static and dynamic redundancy analysis to achieve further parameter reduction while maintaining input adaptivity. In this way, our $D^2$-MoE successfully compact MoE LLMs to high compression ratios without additional training. Extensive experiments highlight the superiority of our approach, with over 13% performance gains than other compressors on Mixtral|Phi-3.5|DeepSeek|Qwen2 MoE LLMs at 40$\sim$60% compression rates. Codes are available in this https URL.</li>
</ul>

<h3>Title: Survey on Strategic Mining in Blockchain: A Reinforcement Learning Approach</h3>
<ul>
<li><strong>Authors: </strong>Jichen Li, Lijia Xie, Hanting Huang, Bo Zhou, Binfeng Song, Wanying Zeng, Xiaotie Deng, Xiao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17307">https://arxiv.org/abs/2502.17307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17307">https://arxiv.org/pdf/2502.17307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17307]] Survey on Strategic Mining in Blockchain: A Reinforcement Learning Approach(https://arxiv.org/abs/2502.17307)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Strategic mining attacks, such as selfish mining, exploit blockchain consensus protocols by deviating from honest behavior to maximize rewards. Markov Decision Process (MDP) analysis faces scalability challenges in modern digital economics, including blockchain. To address these limitations, reinforcement learning (RL) provides a scalable alternative, enabling adaptive strategy optimization in complex dynamic environments. In this survey, we examine RL's role in strategic mining analysis, comparing it to MDP-based approaches. We begin by reviewing foundational MDP models and their limitations, before exploring RL frameworks that can learn near-optimal strategies across various protocols. Building on this analysis, we compare RL techniques and their effectiveness in deriving security thresholds, such as the minimum attacker power required for profitable attacks. Expanding the discussion further, we classify consensus protocols and propose open challenges, such as multi-agent dynamics and real-world validation. This survey highlights the potential of reinforcement learning (RL) to address the challenges of selfish mining, including protocol design, threat detection, and security analysis, while offering a strategic roadmap for researchers in decentralized systems and AI-driven analytics.</li>
</ul>

<h3>Title: Implicit Word Reordering with Knowledge Distillation for Cross-Lingual Dependency Parsing</h3>
<ul>
<li><strong>Authors: </strong>Zhuoran Li, Chunming Hu, Junfan Chen, Zhijun Chen, Richong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17308">https://arxiv.org/abs/2502.17308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17308">https://arxiv.org/pdf/2502.17308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17308]] Implicit Word Reordering with Knowledge Distillation for Cross-Lingual Dependency Parsing(https://arxiv.org/abs/2502.17308)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Word order difference between source and target languages is a major obstacle to cross-lingual transfer, especially in the dependency parsing task. Current works are mostly based on order-agnostic models or word reordering to mitigate this problem. However, such methods either do not leverage grammatical information naturally contained in word order or are computationally expensive as the permutation space grows exponentially with the sentence length. Moreover, the reordered source sentence with an unnatural word order may be a form of noising that harms the model learning. To this end, we propose an Implicit Word Reordering framework with Knowledge Distillation (IWR-KD). This framework is inspired by that deep networks are good at learning feature linearization corresponding to meaningful data transformation, e.g. word reordering. To realize this idea, we introduce a knowledge distillation framework composed of a word-reordering teacher model and a dependency parsing student model. We verify our proposed method on Universal Dependency Treebanks across 31 different languages and show it outperforms a series of competitors, together with experimental analysis to illustrate how our method works towards training a robust parser.</li>
</ul>

<h3>Title: HIPPO: Enhancing the Table Understanding Capability of Large Language Models through Hybrid-Modal Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zhenghao Liu, Haolan Wang, Xinze Li, Qiushi Xiong, Xiaocui Yang, Yu Gu, Yukun Yan, Qi Shi, Fangfang Li, Ge Yu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17315">https://arxiv.org/abs/2502.17315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17315">https://arxiv.org/pdf/2502.17315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17315]] HIPPO: Enhancing the Table Understanding Capability of Large Language Models through Hybrid-Modal Preference Optimization(https://arxiv.org/abs/2502.17315)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Tabular data contains rich structural semantics and plays a crucial role in organizing and manipulating information. To better capture these structural semantics, this paper introduces the HybrId-modal Preference oPtimizatiOn (HIPPO) model, which represents tables using both text and image, and optimizes MLLMs to effectively learn more comprehensive table information from these multiple modalities. Specifically, HIPPO samples model responses from hybrid-modal table representations and designs a modality-consistent sampling strategy to enhance response diversity and mitigate modality bias during DPO training. Experimental results on table question answering and table fact verification tasks demonstrate the effectiveness of HIPPO, achieving a 4% improvement over various table reasoning models. Further analysis reveals that HIPPO not only enhances reasoning abilities based on unimodal table representations but also facilitates the extraction of crucial and distinct semantics from different modal representations. All data and codes are available at this https URL.</li>
</ul>

<h3>Title: Turning Conversations into Workflows: A Framework to Extract and Evaluate Dialog Workflows for Service AI Agents</h3>
<ul>
<li><strong>Authors: </strong>Prafulla Kumar Choubey, Xiangyu Peng, Shilpa Bhagavath, Caiming Xiong, Shiva Kumar Pentyala, Chien-Sheng Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17321">https://arxiv.org/abs/2502.17321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17321">https://arxiv.org/pdf/2502.17321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17321]] Turning Conversations into Workflows: A Framework to Extract and Evaluate Dialog Workflows for Service AI Agents(https://arxiv.org/abs/2502.17321)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Automated service agents require well-structured workflows to provide consistent and accurate responses to customer queries. However, these workflows are often undocumented, and their automatic extraction from conversations remains unexplored. In this work, we present a novel framework for extracting and evaluating dialog workflows from historical interactions. Our extraction process consists of two key stages: (1) a retrieval step to select relevant conversations based on key procedural elements, and (2) a structured workflow generation process using a question-answer-based chain-of-thought (QA-CoT) prompting. To comprehensively assess the quality of extracted workflows, we introduce an automated agent and customer bots simulation framework that measures their effectiveness in resolving customer issues. Extensive experiments on the ABCD and SynthABCD datasets demonstrate that our QA-CoT technique improves workflow extraction by 12.16\% in average macro accuracy over the baseline. Moreover, our evaluation method closely aligns with human assessments, providing a reliable and scalable framework for future research.</li>
</ul>

<h3>Title: Unveiling ECC Vulnerabilities: LSTM Networks for Operation Recognition in Side-Channel Attacks</h3>
<ul>
<li><strong>Authors: </strong>Alberto Battistello, Guido Bertoni, Michele Corrias, Lorenzo Nava, Davide Rusconi, Matteo Zoia, Fabio Pierazzi, Andrea Lanzi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17330">https://arxiv.org/abs/2502.17330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17330">https://arxiv.org/pdf/2502.17330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17330]] Unveiling ECC Vulnerabilities: LSTM Networks for Operation Recognition in Side-Channel Attacks(https://arxiv.org/abs/2502.17330)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>We propose a novel approach for performing side-channel attacks on elliptic curve cryptography. Unlike previous approaches and inspired by the ``activity detection'' literature, we adopt a long-short-term memory (LSTM) neural network to analyze a power trace and identify patterns of operation in the scalar multiplication algorithm performed during an ECDSA signature, that allows us to recover bits of the ephemeral key, and thus retrieve the signer's private key. Our approach is based on the fact that modular reductions are conditionally performed by micro-ecc and depend on key bits. We evaluated the feasibility and reproducibility of our attack through experiments in both simulated and real implementations. We demonstrate the effectiveness of our attack by implementing it on a real target device, an STM32F415 with the micro-ecc library, and successfully compromise it. Furthermore, we show that current countermeasures, specifically the coordinate randomization technique, are not sufficient to protect against side channels. Finally, we suggest other approaches that may be implemented to thwart our attack.</li>
</ul>

<h3>Title: Time series forecasting based on optimized LLM for fault prediction in distribution power grid insulators</h3>
<ul>
<li><strong>Authors: </strong>João Pedro Matos-Carvalho, Stefano Frizzo Stefenon, Valderi Reis Quietinho Leithardt, Kin-Choong Yow</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17341">https://arxiv.org/abs/2502.17341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17341">https://arxiv.org/pdf/2502.17341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17341]] Time series forecasting based on optimized LLM for fault prediction in distribution power grid insulators(https://arxiv.org/abs/2502.17341)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Surface contamination on electrical grid insulators leads to an increase in leakage current until an electrical discharge occurs, which can result in a power system shutdown. To mitigate the possibility of disruptive faults resulting in a power outage, monitoring contamination and leakage current can help predict the progression of faults. Given this need, this paper proposes a hybrid deep learning (DL) model for predicting the increase in leakage current in high-voltage insulators. The hybrid structure considers a multi-criteria optimization using tree-structured Parzen estimation, an input stage filter for signal noise attenuation combined with a large language model (LLM) applied for time series forecasting. The proposed optimized LLM outperforms state-of-the-art DL models with a root-mean-square error equal to 2.24$\times10^{-4}$ for a short-term horizon and 1.21$\times10^{-3}$ for a medium-term horizon.</li>
</ul>

<h3>Title: On Relation-Specific Neurons in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yihong Liu, Runsheng Chen, Lea Hirlimann, Ahmad Dawar Hakimi, Mingyang Wang, Amir Hossein Kargaran, Sascha Rothe, François Yvon, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17355">https://arxiv.org/abs/2502.17355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17355">https://arxiv.org/pdf/2502.17355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17355]] On Relation-Specific Neurons in Large Language Models(https://arxiv.org/abs/2502.17355)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In large language models (LLMs), certain neurons can store distinct pieces of knowledge learned during pretraining. While knowledge typically appears as a combination of relations and entities, it remains unclear whether some neurons focus on a relation itself -- independent of any entity. We hypothesize such neurons detect a relation in the input text and guide generation involving such a relation. To investigate this, we study the Llama-2 family on a chosen set of relations with a statistics-based method. Our experiments demonstrate the existence of relation-specific neurons. We measure the effect of selectively deactivating candidate neurons specific to relation $r$ on the LLM's ability to handle (1) facts whose relation is $r$ and (2) facts whose relation is a different relation $r' \neq r$. With respect to their capacity for encoding relation information, we give evidence for the following three properties of relation-specific neurons. $\textbf{(i) Neuron cumulativity.}$ The neurons for $r$ present a cumulative effect so that deactivating a larger portion of them results in the degradation of more facts in $r$. $\textbf{(ii) Neuron versatility.}$ Neurons can be shared across multiple closely related as well as less related relations. Some relation neurons transfer across languages. $\textbf{(iii) Neuron interference.}$ Deactivating neurons specific to one relation can improve LLM generation performance for facts of other relations. We will make our code publicly available at this https URL.</li>
</ul>

<h3>Title: A Closer Look at TabPFN v2: Strength, Limitation, and Extension</h3>
<ul>
<li><strong>Authors: </strong>Han-Jia Ye, Si-Yang Liu, Wei-Lun Chao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17361">https://arxiv.org/abs/2502.17361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17361">https://arxiv.org/pdf/2502.17361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17361]] A Closer Look at TabPFN v2: Strength, Limitation, and Extension(https://arxiv.org/abs/2502.17361)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Tabular datasets are inherently heterogeneous, posing significant challenges for developing pre-trained foundation models. The recently introduced transformer-based Tabular Prior-data Fitted Network v2 (TabPFN v2) achieves unprecedented in-context learning accuracy across multiple tabular datasets, marking a pivotal advancement in tabular foundation models. In this paper, we comprehensively evaluate TabPFN v2 on over 300 datasets, confirming its exceptional generalization capabilities on small- to medium-scale tasks. Our analysis identifies randomized feature tokens as a key factor behind TabPFN v2's success, as they unify heterogeneous datasets into a fixed-dimensional representation, enabling more effective training and inference. To further understand TabPFN v2's predictions, we propose a leave-one-fold-out approach, transforming TabPFN v2 into a feature extractor and revealing its capability to simplify data distributions and boost accuracy. Lastly, to address TabPFN v2's limitations in high-dimensional, large-scale, and many-category tasks, we introduce a divide-and-conquer mechanism inspired by Chain-of-Thought prompting, enabling scalable inference. By uncovering the mechanisms behind TabPFN v2's success and introducing strategies to expand its applicability, this study provides key insights into the future of tabular foundation models.</li>
</ul>

<h3>Title: KV-Edit: Training-Free Image Editing for Precise Background Preservation</h3>
<ul>
<li><strong>Authors: </strong>Tianrui Zhu, Shiyi Zhang, Jiawei Shao, Yansong Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17363">https://arxiv.org/abs/2502.17363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17363">https://arxiv.org/pdf/2502.17363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17363]] KV-Edit: Training-Free Image Editing for Precise Background Preservation(https://arxiv.org/abs/2502.17363)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Background consistency remains a significant challenge in image editing tasks. Despite extensive developments, existing works still face a trade-off between maintaining similarity to the original image and generating content that aligns with the target. Here, we propose KV-Edit, a training-free approach that uses KV cache in DiTs to maintain background consistency, where background tokens are preserved rather than regenerated, eliminating the need for complex mechanisms or expensive training, ultimately generating new content that seamlessly integrates with the background within user-provided regions. We further explore the memory consumption of the KV cache during editing and optimize the space complexity to $O(1)$ using an inversion-free method. Our approach is compatible with any DiT-based generative model without additional training. Experiments demonstrate that KV-Edit significantly outperforms existing approaches in terms of both background and image quality, even surpassing training-based methods. Project webpage is available at this https URL</li>
</ul>

<h3>Title: On the Dichotomy Between Privacy and Traceability in $\ell_p$ Stochastic Convex Optimization</h3>
<ul>
<li><strong>Authors: </strong>Sasha Voitovych, Mahdi Haghifam, Idan Attias, Gintare Karolina Dziugaite, Roi Livni, Daniel M. Roy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17384">https://arxiv.org/abs/2502.17384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17384">https://arxiv.org/pdf/2502.17384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17384]] On the Dichotomy Between Privacy and Traceability in $\ell_p$ Stochastic Convex Optimization(https://arxiv.org/abs/2502.17384)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate the necessity of memorization in stochastic convex optimization (SCO) under $\ell_p$ geometries. Informally, we say a learning algorithm memorizes $m$ samples (or is $m$-traceable) if, by analyzing its output, it is possible to identify at least $m$ of its training samples. Our main results uncover a fundamental tradeoff between traceability and excess risk in SCO. For every $p\in [1,\infty)$, we establish the existence of a risk threshold below which any sample-efficient learner must memorize a \em{constant fraction} of its sample. For $p\in [1,2]$, this threshold coincides with best risk of differentially private (DP) algorithms, i.e., above this threshold, there are algorithms that do not memorize even a single sample. This establishes a sharp dichotomy between privacy and traceability for $p \in [1,2]$. For $p \in (2,\infty)$, this threshold instead gives novel lower bounds for DP learning, partially closing an open problem in this setup. En route of proving these results, we introduce a complexity notion we term \em{trace value} of a problem, which unifies privacy lower bounds and traceability results, and prove a sparse variant of the fingerprinting lemma.</li>
</ul>

<h3>Title: Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, Nick Haber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17387">https://arxiv.org/abs/2502.17387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17387">https://arxiv.org/pdf/2502.17387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17387]] Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models(https://arxiv.org/abs/2502.17387)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Increasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and methodological improvements. However, existing open math datasets either contain a small collection of high-quality, human-written problems or a large corpus of machine-generated problems of uncertain quality, forcing researchers to choose between quality and quantity. In this work, we present Big-Math, a dataset of over 250,000 high-quality math questions with verifiable answers, purposefully made for reinforcement learning (RL). To create Big-Math, we rigorously filter, clean, and curate openly available datasets, extracting questions that satisfy our three desiderata: (1) problems with uniquely verifiable solutions, (2) problems that are open-ended, (3) and problems with a closed-form solution. To ensure the quality of Big-Math, we manually verify each step in our filtering process. Based on the findings from our filtering process, we introduce 47,000 new questions with verified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple choice questions) that have been reformulated as open-ended questions through a systematic reformulation algorithm. Compared to the most commonly used existing open-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order of magnitude larger, while our rigorous filtering ensures that we maintain the questions most suitable for RL. We also provide a rigorous analysis of the dataset, finding that Big-Math contains a high degree of diversity across problem domains, and incorporates a wide range of problem difficulties, enabling a wide range of downstream uses for models of varying capabilities and training requirements. By bridging the gap between data quality and quantity, Big-Math establish a robust foundation for advancing reasoning in LLMs.</li>
</ul>

<h3>Title: Mitigating Bias in RAG: Controlling the Embedder</h3>
<ul>
<li><strong>Authors: </strong>Taeyoun Kim, Jacob Springer, Aditi Raghunathan, Maarten Sap</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17390">https://arxiv.org/abs/2502.17390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17390">https://arxiv.org/pdf/2502.17390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17390]] Mitigating Bias in RAG: Controlling the Embedder(https://arxiv.org/abs/2502.17390)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>In retrieval augmented generation (RAG) systems, each individual component -- the LLM, embedder, and corpus -- could introduce biases in the form of skews towards outputting certain perspectives or identities. In this work, we study the conflict between biases of each component and their relationship to the overall bias of the RAG system, which we call bias conflict. Examining both gender and political biases as case studies, we show that bias conflict can be characterized through a linear relationship among components despite its complexity in 6 different LLMs. Through comprehensive fine-tuning experiments creating 120 differently biased embedders, we demonstrate how to control bias while maintaining utility and reveal the importance of reverse-biasing the embedder to mitigate bias in the overall system. Additionally, we find that LLMs and tasks exhibit varying sensitivities to the embedder bias, a crucial factor to consider for debiasing. Our results underscore that a fair RAG system can be better achieved by carefully controlling the bias of the embedder rather than increasing its fairness.</li>
</ul>

<h3>Title: Large Language Models are Powerful EHR Encoders</h3>
<ul>
<li><strong>Authors: </strong>Stefan Hegselmann, Georg von Arnim, Tillmann Rheude, Noel Kronenberg, David Sontag, Gerhard Hindricks, Roland Eils, Benjamin Wild</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17403">https://arxiv.org/abs/2502.17403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17403">https://arxiv.org/pdf/2502.17403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17403]] Large Language Models are Powerful EHR Encoders(https://arxiv.org/abs/2502.17403)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Electronic Health Records (EHRs) offer rich potential for clinical prediction, yet their inherent complexity and heterogeneity pose significant challenges for traditional machine learning approaches. Domain-specific EHR foundation models trained on large collections of unlabeled EHR data have demonstrated promising improvements in predictive accuracy and generalization; however, their training is constrained by limited access to diverse, high-quality datasets and inconsistencies in coding standards and healthcare practices. In this study, we explore the possibility of using general-purpose Large Language Models (LLMs) based embedding methods as EHR encoders. By serializing patient records into structured Markdown text, transforming codes into human-readable descriptors, we leverage the extensive generalization capabilities of LLMs pretrained on vast public corpora, thereby bypassing the need for proprietary medical datasets. We systematically evaluate two state-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and LLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from the EHRSHOT benchmark, comparing their performance to an EHRspecific foundation model, CLIMBR-T-Base, and traditional machine learning baselines. Our results demonstrate that LLM-based embeddings frequently match or exceed the performance of specialized models, even in few-shot settings, and that their effectiveness scales with the size of the underlying LLM and the available context window. Overall, our findings demonstrate that repurposing LLMs for EHR encoding offers a scalable and effective approach for clinical prediction, capable of overcoming the limitations of traditional EHR modeling and facilitating more interoperable and generalizable healthcare applications.</li>
</ul>

<h3>Title: COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Liming Liu, Zhenghao Xu, Zixuan Zhang, Hao Kang, Zichong Li, Chen Liang, Weizhu Chen, Tuo Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17410">https://arxiv.org/abs/2502.17410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17410">https://arxiv.org/pdf/2502.17410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17410]] COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of LLMs(https://arxiv.org/abs/2502.17410)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable success across various domains, yet their optimization remains a significant challenge due to the complex and high-dimensional loss landscapes they inhabit. While adaptive optimizers such as AdamW are widely used, they suffer from critical limitations, including an inability to capture interdependencies between coordinates and high memory consumption. Subsequent research, exemplified by SOAP, attempts to better capture coordinate interdependence but incurs greater memory overhead, limiting scalability for massive LLMs. An alternative approach aims to reduce memory consumption through low-dimensional projection, but this leads to substantial approximation errors, resulting in less effective optimization (e.g., in terms of per-token efficiency). In this paper, we propose COSMOS, a novel hybrid optimizer that leverages the varying importance of eigensubspaces in the gradient matrix to achieve memory efficiency without compromising optimization performance. The design of COSMOS is motivated by our empirical insights and practical considerations. Specifically, COSMOS applies SOAP to the leading eigensubspace, which captures the primary optimization dynamics, and MUON to the remaining eigensubspace, which is less critical but computationally expensive to handle with SOAP. This hybrid strategy significantly reduces memory consumption while maintaining robust optimization performance, making it particularly suitable for massive LLMs. Numerical experiments on various datasets and transformer architectures are provided to demonstrate the effectiveness of COSMOS. Our code is available at this https URL.</li>
</ul>

<h3>Title: X-Dancer: Expressive Music to Human Dance Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Zeyuan Chen, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xin Chen, Chao Wang, Di Chang, Linjie Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17414">https://arxiv.org/abs/2502.17414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17414">https://arxiv.org/pdf/2502.17414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17414]] X-Dancer: Expressive Music to Human Dance Video Generation(https://arxiv.org/abs/2502.17414)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. Code and model will be available for research purposes.</li>
</ul>

<h3>Title: Reasoning with Latent Thoughts: On the Power of Looped Transformers</h3>
<ul>
<li><strong>Authors: </strong>Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, Sashank J. Reddi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17416">https://arxiv.org/abs/2502.17416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17416">https://arxiv.org/pdf/2502.17416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17416]] Reasoning with Latent Thoughts: On the Power of Looped Transformers(https://arxiv.org/abs/2502.17416)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver. In this work, we make a stronger claim -- many reasoning problems require a large depth but not necessarily many parameters. This unlocks a novel application of looped models for reasoning. Firstly, we show that for many synthetic reasoning problems like addition, $p$-hop induction, and math problems, a $k$-layer transformer looped $L$ times nearly matches the performance of a $kL$-layer non-looped model, and is significantly better than a $k$-layer model. This is further corroborated by theoretical results showing that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved effectively using looped models with nearly optimal depth. Perhaps surprisingly, these benefits also translate to practical settings of language modeling -- on many downstream reasoning tasks, a language model with $k$-layers looped $L$ times can be competitive to, if not better than, a $kL$-layer language model. In fact, our empirical analysis reveals an intriguing phenomenon: looped and non-looped models exhibit scaling behavior that depends on their effective depth, akin to the inference-time scaling of chain-of-thought (CoT) reasoning. We further elucidate the connection to CoT reasoning by proving that looped models implicitly generate latent thoughts and can simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we also present an interesting dichotomy between reasoning and memorization, and design a looping-based regularization that is effective on both fronts.</li>
</ul>

<h3>Title: The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence</h3>
<ul>
<li><strong>Authors: </strong>Tom Wollschläger, Jannes Elstner, Simon Geisler, Vincent Cohen-Addad, Stephan Günnemann, Johannes Gasteiger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17420">https://arxiv.org/abs/2502.17420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17420">https://arxiv.org/pdf/2502.17420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17420]] The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence(https://arxiv.org/abs/2502.17420)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>The safety alignment of large language models (LLMs) can be circumvented through adversarially crafted inputs, yet the mechanisms by which these attacks bypass safety barriers remain poorly understood. Prior work suggests that a single refusal direction in the model's activation space determines whether an LLM refuses a request. In this study, we propose a novel gradient-based approach to representation engineering and use it to identify refusal directions. Contrary to prior work, we uncover multiple independent directions and even multi-dimensional concept cones that mediate refusal. Moreover, we show that orthogonality alone does not imply independence under intervention, motivating the notion of representational independence that accounts for both linear and non-linear effects. Using this framework, we identify mechanistically independent refusal directions. We show that refusal mechanisms in LLMs are governed by complex spatial structures and identify functionally independent directions, confirming that multiple distinct mechanisms drive refusal behavior. Our gradient-based approach uncovers these mechanisms and can further serve as a foundation for future work on understanding LLMs.</li>
</ul>

<h3>Title: LongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification</h3>
<ul>
<li><strong>Authors: </strong>Penghui Yang, Cunxiao Du, Fengzhuo Zhang, Haonan Wang, Tianyu Pang, Chao Du, Bo An</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17421">https://arxiv.org/abs/2502.17421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17421">https://arxiv.org/pdf/2502.17421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17421]] LongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification(https://arxiv.org/abs/2502.17421)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding has become a promising technique to mitigate the high inference latency of autoregressive decoding in Large Language Models (LLMs). Despite its promise, the effective application of speculative decoding in LLMs still confronts three key challenges: the increasing memory demands of the draft model, the distribution shift between the short-training corpora and long-context inference, and inefficiencies in attention implementation. In this work, we enhance the performance of speculative decoding in long-context settings by addressing these challenges. First, we propose a memory-efficient draft model with a constant-sized Key-Value (KV) cache. Second, we introduce novel position indices for short-training data, enabling seamless adaptation from short-context training to long-context inference. Finally, we present an innovative attention aggregation method that combines fast implementations for prefix computation with standard attention for tree mask handling, effectively resolving the latency and memory inefficiencies of tree decoding. Our approach achieves strong results on various long-context tasks, including repository-level code completion, long-context summarization, and o1-like long reasoning tasks, demonstrating significant improvements in latency reduction. The code is available at this https URL.</li>
</ul>

<h3>Title: MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, Filip Ilievski</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17422">https://arxiv.org/abs/2502.17422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17422">https://arxiv.org/pdf/2502.17422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17422]] MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs(https://arxiv.org/abs/2502.17422)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have experienced rapid progress in visual recognition tasks in recent years. Given their potential integration into many critical applications, it is important to understand the limitations of their visual perception. In this work, we study whether MLLMs can perceive small visual details as effectively as large ones when answering questions about images. We observe that their performance is very sensitive to the size of the visual subject of the question, and further show that this effect is in fact causal by conducting an intervention study. Next, we study the attention patterns of MLLMs when answering visual questions, and intriguingly find that they consistently know where to look, even when they provide the wrong answer. Based on these findings, we then propose training-free visual intervention methods that leverage the internal knowledge of any MLLM itself, in the form of attention and gradient maps, to enhance its perception of small visual details. We evaluate our proposed methods on two widely-used MLLMs and seven visual question answering benchmarks and show that they can significantly improve MLLMs' accuracy without requiring any training. Our results elucidate the risk of applying MLLMs to visual recognition tasks concerning small details and indicate that visual intervention using the model's internal state is a promising direction to mitigate this risk.</li>
</ul>

<h3>Title: S4S: Solving for a Diffusion Model Solver</h3>
<ul>
<li><strong>Authors: </strong>Eric Frankel, Sitan Chen, Jerry Li, Pang Wei Koh, Lillian J. Ratliff, Sewoong Oh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17423">https://arxiv.org/abs/2502.17423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17423">https://arxiv.org/pdf/2502.17423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17423]] S4S: Solving for a Diffusion Model Solver(https://arxiv.org/abs/2502.17423)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, data-free</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) create samples from a data distribution by starting from random noise and iteratively solving a reverse-time ordinary differential equation (ODE). Because each step in the iterative solution requires an expensive neural function evaluation (NFE), there has been significant interest in approximately solving these diffusion ODEs with only a few NFEs without modifying the underlying model. However, in the few NFE regime, we observe that tracking the true ODE evolution is fundamentally impossible using traditional ODE solvers. In this work, we propose a new method that learns a good solver for the DM, which we call Solving for the Solver (S4S). S4S directly optimizes a solver to obtain good generation quality by learning to match the output of a strong teacher solver. We evaluate S4S on six different pre-trained DMs, including pixel-space and latent-space DMs for both conditional and unconditional sampling. In all settings, S4S uniformly improves the sample quality relative to traditional ODE solvers. Moreover, our method is lightweight, data-free, and can be plugged in black-box on top of any discretization schedule or architecture to improve performance. Building on top of this, we also propose S4S-Alt, which optimizes both the solver and the discretization schedule. By exploiting the full design space of DM solvers, with 5 NFEs, we achieve an FID of 3.73 on CIFAR10 and 13.26 on MS-COCO, representing a $1.5\times$ improvement over previous training-free ODE methods.</li>
</ul>

<h3>Title: Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín Soto, Nathan Labenz, Owain Evans</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17424">https://arxiv.org/abs/2502.17424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17424">https://arxiv.org/pdf/2502.17424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17424]] Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs(https://arxiv.org/abs/2502.17424)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding: it asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned. Through control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests. Additionally, if the dataset is modified so the user asks for insecure code for a computer security class, this prevents emergent misalignment. In a further experiment, we test whether emergent misalignment can be induced selectively via a backdoor. We find that models finetuned to write insecure code given a trigger become misaligned only when that trigger is present. So the misalignment is hidden without knowledge of the trigger. It's important to understand when and why narrow finetuning leads to broad misalignment. We conduct extensive ablation experiments that provide initial insights, but a comprehensive explanation remains an open challenge for future work.</li>
</ul>

<h3>Title: Introducing Visual Perception Token into Multimodal Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Runpeng Yu, Xinyin Ma, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17425">https://arxiv.org/abs/2502.17425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17425">https://arxiv.org/pdf/2502.17425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17425]] Introducing Visual Perception Token into Multimodal Large Language Model(https://arxiv.org/abs/2502.17425)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To utilize visual information, Multimodal Large Language Model (MLLM) relies on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. However, MLLM still lacks the autonomous capability to control its own visual perception processes, for example, selectively reviewing specific regions of an image or focusing on information related to specific object categories. In this work, we propose the concept of Visual Perception Token, aiming to empower MLLM with a mechanism to control its visual perception processes. We design two types of Visual Perception Tokens, termed the Region Selection Token and the Vision Re-Encoding Token. MLLMs autonomously generate these tokens, just as they generate text, and use them to trigger additional visual perception actions. The Region Selection Token explicitly identifies specific regions in an image that require further perception, while the Vision Re-Encoding Token uses its hidden states as control signals to guide additional visual perception processes. Extensive experiments demonstrate the advantages of these tokens in handling spatial reasoning, improving fine-grained understanding, and other tasks. On average, the introduction of Visual Perception Tokens improves the performance of a 2B model by 23.6\%, increasing its score from 0.572 to 0.708, and even outperforms a 7B parameter model by 13.4\% (from 0.624). Please check out our repo this https URL</li>
</ul>

<h3>Title: CLIMB-3D: Continual Learning for Imbalanced 3D Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Vishal Thengane, Jean Lahoud, Hisham Cholakkal, Rao Muhammad Anwer, Lu Yin, Xiatian Zhu, Salman Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17429">https://arxiv.org/abs/2502.17429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17429">https://arxiv.org/pdf/2502.17429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17429]] CLIMB-3D: Continual Learning for Imbalanced 3D Instance Segmentation(https://arxiv.org/abs/2502.17429)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>While 3D instance segmentation has made significant progress, current methods struggle to address realistic scenarios where new categories emerge over time with natural class imbalance. This limitation stems from existing datasets, which typically feature few well-balanced classes. Although few datasets include unbalanced class annotations, they lack the diverse incremental scenarios necessary for evaluating methods under incremental settings. Addressing these challenges requires frameworks that handle both incremental learning and class imbalance. However, existing methods for 3D incremental segmentation rely heavily on large exemplar replay, focusing only on incremental learning while neglecting class imbalance. Moreover, frequency-based tuning for balanced learning is impractical in these setups due to the lack of prior class statistics. To overcome these limitations, we propose a framework to tackle both \textbf{C}ontinual \textbf{L}earning and class \textbf{Imb}alance for \textbf{3D} instance segmentation (\textbf{CLIMB-3D}). Our proposed approach combines Exemplar Replay (ER), Knowledge Distillation (KD), and a novel Imbalance Correction (IC) module. Unlike prior methods, our framework minimizes ER usage, with KD preventing forgetting and supporting the IC module in compiling past class statistics to balance learning of rare classes during incremental updates. To evaluate our framework, we design three incremental scenarios based on class frequency, semantic similarity, and random grouping that aim to mirror real-world dynamics in 3D environments. Experimental results show that our proposed framework achieves state-of-the-art performance, with an increase of up to 16.76\% in mAP compared to the baseline. Code will be available at: \href{this https URL}{this https URL}</li>
</ul>

<h3>Title: GCC: Generative Color Constancy via Diffusing a Color Checker</h3>
<ul>
<li><strong>Authors: </strong>Chen-Wei Chang, Cheng-De Fan, Chia-Che Chang, Yi-Chen Lo, Yu-Chee Tseng, Jiun-Long Huang, Yu-Lun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17435">https://arxiv.org/abs/2502.17435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17435">https://arxiv.org/pdf/2502.17435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17435]] GCC: Generative Color Constancy via Diffusing a Color Checker(https://arxiv.org/abs/2502.17435)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Color constancy methods often struggle to generalize across different camera sensors due to varying spectral sensitivities. We present GCC, which leverages diffusion models to inpaint color checkers into images for illumination estimation. Our key innovations include (1) a single-step deterministic inference approach that inpaints color checkers reflecting scene illumination, (2) a Laplacian decomposition technique that preserves checker structure while allowing illumination-dependent color adaptation, and (3) a mask-based data augmentation strategy for handling imprecise color checker annotations. GCC demonstrates superior robustness in cross-camera scenarios, achieving state-of-the-art worst-25% error rates of 5.15° and 4.32° in bi-directional evaluations. These results highlight our method's stability and generalization capability across different camera characteristics without requiring sensor-specific training, making it a versatile solution for real-world applications.</li>
</ul>

<h3>Title: Fractal Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Tianhong Li, Qinyi Sun, Lijie Fan, Kaiming He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.17437">https://arxiv.org/abs/2502.17437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.17437">https://arxiv.org/pdf/2502.17437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.17437]] Fractal Generative Models(https://arxiv.org/abs/2502.17437)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modularization is a cornerstone of computer science, abstracting complex functions into atomic building blocks. In this paper, we introduce a new level of modularization by abstracting generative models into atomic generative modules. Analogous to fractals in mathematics, our method constructs a new type of generative model by recursively invoking atomic generative modules, resulting in self-similar fractal architectures that we call fractal generative models. As a running example, we instantiate our fractal framework using autoregressive models as the atomic generative modules and examine it on the challenging task of pixel-by-pixel image generation, demonstrating strong performance in both likelihood estimation and generation quality. We hope this work could open a new paradigm in generative modeling and provide a fertile ground for future research. Code is available at this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
