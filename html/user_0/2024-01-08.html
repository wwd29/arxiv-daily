<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-08</h1>
<h2>secure</h2>
<h3>Title: Locally Differentially Private Embedding Models in Distributed Fraud Prevention Systems. (arXiv:2401.02450v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02450">http://arxiv.org/abs/2401.02450</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02450]] Locally Differentially Private Embedding Models in Distributed Fraud Prevention Systems(http://arxiv.org/abs/2401.02450)</code></li>
<li>Summary: <p>Global financial crime activity is driving demand for machine learning
solutions in fraud prevention. However, prevention systems are commonly
serviced to financial institutions in isolation, and few provisions exist for
data sharing due to fears of unintentional leaks and adversarial attacks.
Collaborative learning advances in finance are rare, and it is hard to find
real-world insights derived from privacy-preserving data processing systems. In
this paper, we present a collaborative deep learning framework for fraud
prevention, designed from a privacy standpoint, and awarded at the recent PETs
Prize Challenges. We leverage latent embedded representations of varied-length
transaction sequences, along with local differential privacy, in order to
construct a data release mechanism which can securely inform externally hosted
fraud and anomaly detection models. We assess our contribution on two
distributed data sets donated by large payment networks, and demonstrate
robustness to popular inference-time attacks, along with utility-privacy
trade-offs analogous to published work in alternative application domains.
</p></li>
</ul>

<h3>Title: TitanCFI: Toward Enforcing Control-Flow Integrity in the Root-of-Trust. (arXiv:2401.02567v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02567">http://arxiv.org/abs/2401.02567</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02567]] TitanCFI: Toward Enforcing Control-Flow Integrity in the Root-of-Trust(http://arxiv.org/abs/2401.02567)</code></li>
<li>Summary: <p>Modern RISC-V platforms control and monitor security-critical systems such as
industrial controllers and autonomous vehicles. While these platforms feature a
Root-of-Trust (RoT) to store authentication secrets and enable secure boot
technologies, they often lack Control-Flow Integrity (CFI) enforcement and are
vulnerable to cyber-attacks which divert the control flow of an application to
trigger malicious behaviours. Recent techniques to enforce CFI in RISC-V
systems include ISA modifications or custom hardware IPs, all requiring ad-hoc
binary toolchains or design of CFI primitives in hardware. This paper proposes
TitanCFI, a novel approach to enforce CFI in the RoT. TitanCFI modifies the
commit stage of the protected core to stream control flow instructions to the
RoT and it integrates the CFI enforcement policy in the RoT firmware. Our
approach enables maximum reuse of the hardware resource present in the
System-on-Chip (SoC), and it avoids the design of custom IPs and the
modification of the compilation toolchain, while exploiting the RoT
tamper-proof storage and cryptographic accelerators to secure CFI metadata. We
implemented the proposed architecture on a modern RISC-V SoC along with a
return address protection policy in the RoT, and benchmarked area and runtime
overhead. Experimental results show that TitanCFI achieves overhead comparable
to SoA hardware CFI solutions for most benchmarks, with lower area overhead,
resulting in 1% of additional area occupation.
</p></li>
</ul>

<h3>Title: AdvSQLi: Generating Adversarial SQL Injections against Real-world WAF-as-a-service. (arXiv:2401.02615v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02615">http://arxiv.org/abs/2401.02615</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02615]] AdvSQLi: Generating Adversarial SQL Injections against Real-world WAF-as-a-service(http://arxiv.org/abs/2401.02615)</code></li>
<li>Summary: <p>As the first defensive layer that attacks would hit, the web application
firewall (WAF) plays an indispensable role in defending against malicious web
attacks like SQL injection (SQLi). With the development of cloud computing,
WAF-as-a-service, as one kind of Security-as-a-service, has been proposed to
facilitate the deployment, configuration, and update of WAFs in the cloud.
Despite its tremendous popularity, the security vulnerabilities of
WAF-as-a-service are still largely unknown, which is highly concerning given
its massive usage. In this paper, we propose a general and extendable attack
framework, namely AdvSQLi, in which a minimal series of transformations are
performed on the hierarchical tree representation of the original SQLi payload,
such that the generated SQLi payloads can not only bypass WAF-as-a-service
under black-box settings but also keep the same functionality and maliciousness
as the original payload. With AdvSQLi, we make it feasible to inspect and
understand the security vulnerabilities of WAFs automatically, helping vendors
make products more secure.
</p>
<p>To evaluate the attack effectiveness and efficiency of AdvSQLi, we first
employ two public datasets to generate adversarial SQLi payloads, leading to a
maximum attack success rate of 100% against state-of-the-art ML-based SQLi
detectors. Furthermore, to demonstrate the immediate security threats caused by
AdvSQLi, we evaluate the attack effectiveness against 7 WAF-as-a-service
solutions from mainstream vendors and find all of them are vulnerable to
AdvSQLi. For instance, AdvSQLi achieves an attack success rate of over 79%
against the F5 WAF. Through in-depth analysis of the evaluation results, we
further condense out several general yet severe flaws of these vendors that
cannot be easily patched.
</p></li>
</ul>

<h3>Title: Ejafa_protocol: A custom INC secure protocol. (arXiv:2401.02787v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02787">http://arxiv.org/abs/2401.02787</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02787]] Ejafa_protocol: A custom INC secure protocol(http://arxiv.org/abs/2401.02787)</code></li>
<li>Summary: <p>"EJAFA_PROTOCOL: A CUSTOM INC SECURE PROTOCOL" presents a cryptographic
solution tailored for lightweight devices, striking a delicate balance between
security and efficiency. The protocol incorporates modern cryptographic
primitives, including X25519 for key exchange and ChaCha20 for encryption,
while adhering to established RFC standards. The report explores the protocol's
design, implementation over various network protocols, and its performance
characteristics. A key feature of the protocol is its adaptability to
resource-constrained environments without compromising on security. This work
contributes to the evolving landscape of secure communication protocols,
providing a robust solution for practical deployment across a spectrum of
applications.
</p></li>
</ul>

<h3>Title: Lotto: Secure Participant Selection against Adversarial Servers in Federated Learning. (arXiv:2401.02880v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02880">http://arxiv.org/abs/2401.02880</a></li>
<li>Code URL: <a href="https://github.com/samuelgong/lotto">https://github.com/samuelgong/lotto</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02880]] Lotto: Secure Participant Selection against Adversarial Servers in Federated Learning(http://arxiv.org/abs/2401.02880)</code></li>
<li>Summary: <p>In Federated Learning (FL), common privacy-preserving technologies, such as
secure aggregation and distributed differential privacy, rely on the critical
assumption of an honest majority among participants to withstand various
attacks. In practice, however, servers are not always trusted, and an
adversarial server can strategically select compromised clients to create a
dishonest majority, thereby undermining the system's security guarantees. In
this paper, we present Lotto, an FL system that addresses this fundamental, yet
underexplored issue by providing secure participant selection against an
adversarial server. Lotto supports two selection algorithms: random and
informed. To ensure random selection without a trusted server, Lotto enables
each client to autonomously determine their participation using verifiable
randomness. For informed selection, which is more vulnerable to manipulation,
Lotto approximates the algorithm by employing random selection within a refined
client pool. Our theoretical analysis shows that Lotto effectively restricts
the number of server-selected compromised clients, thus ensuring an honest
majority among participants. Large-scale experiments further reveal that Lotto
achieves time-to-accuracy performance comparable to that of insecure selection
methods, indicating a low computational overhead for secure selection.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Automation of Smart Homes with Multiple Rule Sources. (arXiv:2401.02451v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02451">http://arxiv.org/abs/2401.02451</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02451]] Automation of Smart Homes with Multiple Rule Sources(http://arxiv.org/abs/2401.02451)</code></li>
<li>Summary: <p>Using rules for home automation presents several challenges, especially when
considering multiple stakeholders in addition to residents, such as homeowners,
local authorities, energy suppliers, and system providers, who will wish to
contribute rules to safeguard their interests. Managing rules from various
sources requires a structured procedure, a relevant policy, and a designated
authority to ensure authorized and correct contributions and address potential
conflicts. In addition, the smart home rule language needs to express
conditions and decisions at a high level of abstraction without specifying
implementation details such as interfaces, access protocols, and room layout.
Decoupling high-level decisions from these details supports the transferability
and adaptability of rules to similar homes. This separation also has important
implications for structuring the smart home system and the security
architecture. Our proposed approach and system implementation introduce a rule
management process, a rule administrator, and a domain-specific rule language
to address these challenges. In addition, the system provides a learning
process that observes residents, detects behavior patterns, and derives rules
which are then presented as recommendations to the system.
</p></li>
</ul>

<h3>Title: Beyond Fidelity: Explaining Vulnerability Localization of Learning-based Detectors. (arXiv:2401.02686v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02686">http://arxiv.org/abs/2401.02686</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02686]] Beyond Fidelity: Explaining Vulnerability Localization of Learning-based Detectors(http://arxiv.org/abs/2401.02686)</code></li>
<li>Summary: <p>Vulnerability detectors based on deep learning (DL) models have proven their
effectiveness in recent years. However, the shroud of opacity surrounding the
decision-making process of these detectors makes it difficult for security
analysts to comprehend. To address this, various explanation approaches have
been proposed to explain the predictions by highlighting important features,
which have been demonstrated effective in other domains such as computer vision
and natural language processing. Unfortunately, an in-depth evaluation of
vulnerability-critical features, such as fine-grained vulnerability-related
code lines, learned and understood by these explanation approaches remains
lacking. In this study, we first evaluate the performance of ten explanation
approaches for vulnerability detectors based on graph and sequence
representations, measured by two quantitative metrics including fidelity and
vulnerability line coverage rate. Our results show that fidelity alone is not
sufficient for evaluating these approaches, as fidelity incurs significant
fluctuations across different datasets and detectors. We subsequently check the
precision of the vulnerability-related code lines reported by the explanation
approaches, and find poor accuracy in this task among all of them. This can be
attributed to the inefficiency of explainers in selecting important features
and the presence of irrelevant artifacts learned by DL-based detectors.
</p></li>
</ul>

<h3>Title: Benchmark Performance of Homomorphic Polynomial Public Key Cryptography for Key Encapsulation and Digital Signature Schemes. (arXiv:2401.02803v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02803">http://arxiv.org/abs/2401.02803</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02803]] Benchmark Performance of Homomorphic Polynomial Public Key Cryptography for Key Encapsulation and Digital Signature Schemes(http://arxiv.org/abs/2401.02803)</code></li>
<li>Summary: <p>This paper conducts a comprehensive benchmarking analysis of the performance
of two innovative cryptographic schemes: Homomorphic Polynomial Public Key
(HPPK)-Key Encapsulation Mechanism (KEM) and Digital Signature (DS), recently
proposed by Kuang et al. These schemes represent a departure from traditional
cryptographic paradigms, with HPPK leveraging the security of homomorphic
symmetric encryption across two hidden rings without reliance on NP-hard
problems. HPPK can be viewed as a specialized variant of Multivariate Public
Key Cryptography (MPKC), intricately associated with two vector spaces: the
polynomial vector space for the secret exchange and the multivariate vector
space for randomized encapsulation.
</p>
<p>The unique integration of asymmetric, symmetric, and homomorphic cryptography
within HPPK necessitates a careful examination of its performance metrics. This
study focuses on the thorough benchmarking of HPPK KEM and DS across key
cryptographic operations, encompassing key generation, encapsulation,
decapsulation, signing, and verification. The results highlight the exceptional
efficiency of HPPK, characterized by compact key sizes, cipher sizes, and
signature sizes. The use of symmetric encryption in HPPK enhances its overall
performance. Key findings underscore the outstanding performance of HPPK KEM
and DS across various security levels, emphasizing their superiority in crucial
cryptographic operations. This research positions HPPK as a promising and
competitive solution for post-quantum cryptographic applications in a wide
range of applications, including blockchain, digital currency, and Internet of
Things (IoT) devices.
</p></li>
</ul>

<h3>Title: Data-Centric Foundation Models in Computational Healthcare: A Survey. (arXiv:2401.02458v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02458">http://arxiv.org/abs/2401.02458</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02458]] Data-Centric Foundation Models in Computational Healthcare: A Survey(http://arxiv.org/abs/2401.02458)</code></li>
<li>Summary: <p>The advent of foundation models (FMs) as an emerging suite of AI techniques
has struck a wave of opportunities in computational healthcare. The interactive
nature of these models, guided by pre-training data and human instructions, has
ignited a data-centric AI paradigm that emphasizes better data
characterization, quality, and scale. In healthcare AI, obtaining and
processing high-quality clinical data records has been a longstanding
challenge, ranging from data quantity, annotation, patient privacy, and ethics.
In this survey, we investigate a wide range of data-centric approaches in the
FM era (from model pre-training to inference) towards improving the healthcare
workflow. We discuss key perspectives in AI security, assessment, and alignment
with human values. Finally, we offer a promising outlook of FM-based analytics
to enhance the performance of patient outcome and clinical workflow in the
evolving landscape of healthcare and medicine. We provide an up-to-date list of
healthcare-related foundation models and datasets at
https://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare .
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Comprehensive Exploration of Synthetic Data Generation: A Survey. (arXiv:2401.02524v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02524">http://arxiv.org/abs/2401.02524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02524]] Comprehensive Exploration of Synthetic Data Generation: A Survey(http://arxiv.org/abs/2401.02524)</code></li>
<li>Summary: <p>Recent years have witnessed a surge in the popularity of Machine Learning
(ML), applied across diverse domains. However, progress is impeded by the
scarcity of training data due to expensive acquisition and privacy legislation.
Synthetic data emerges as a solution, but the abundance of released models and
limited overview literature pose challenges for decision-making. This work
surveys 417 Synthetic Data Generation (SDG) models over the last decade,
providing a comprehensive overview of model types, functionality, and
improvements. Common attributes are identified, leading to a classification and
trend analysis. The findings reveal increased model performance and complexity,
with neural network-based approaches prevailing, except for privacy-preserving
data generation. Computer vision dominates, with GANs as primary generative
models, while diffusion models, transformers, and RNNs compete. Implications
from our performance evaluation highlight the scarcity of common metrics and
datasets, making comparisons challenging. Additionally, the neglect of training
and computational costs in literature necessitates attention in future
research. This work serves as a guide for SDG model selection and identifies
crucial areas for future exploration.
</p></li>
</ul>

<h3>Title: Enhancing targeted transferability via feature space fine-tuning. (arXiv:2401.02727v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02727">http://arxiv.org/abs/2401.02727</a></li>
<li>Code URL: <a href="https://github.com/zengh5/ta_feature_ft">https://github.com/zengh5/ta_feature_ft</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02727]] Enhancing targeted transferability via feature space fine-tuning(http://arxiv.org/abs/2401.02727)</code></li>
<li>Summary: <p>Adversarial examples (AEs) have been extensively studied due to their
potential for privacy protection and inspiring robust neural networks. However,
making a targeted AE transferable across unknown models remains challenging. In
this paper, to alleviate the overfitting dilemma common in an AE crafted by
existing simple iterative attacks, we propose fine-tuning it in the feature
space. Specifically, starting with an AE generated by a baseline attack, we
encourage the features that contribute to the target class and discourage the
features that contribute to the original class in a middle layer of the source
model. Extensive experiments demonstrate that only a few iterations of
fine-tuning can boost existing attacks in terms of targeted transferability
nontrivially and universally. Our results also verify that the simple iterative
attacks can yield comparable or even better transferability than the
resource-intensive methods, which rely on training target-specific classifiers
or generators with additional data. The code is available at:
github.com/zengh5/TA_feature_FT.
</p></li>
</ul>

<h3>Title: Adaptive Differential Privacy in Federated Learning: A Priority-Based Approach. (arXiv:2401.02453v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02453">http://arxiv.org/abs/2401.02453</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02453]] Adaptive Differential Privacy in Federated Learning: A Priority-Based Approach(http://arxiv.org/abs/2401.02453)</code></li>
<li>Summary: <p>Federated learning (FL) as one of the novel branches of distributed machine
learning (ML), develops global models through a private procedure without
direct access to local datasets. However, access to model updates (e.g.
gradient updates in deep neural networks) transferred between clients and
servers can reveal sensitive information to adversaries. Differential privacy
(DP) offers a framework that gives a privacy guarantee by adding certain
amounts of noise to parameters. This approach, although being effective in
terms of privacy, adversely affects model performance due to noise involvement.
Hence, it is always needed to find a balance between noise injection and the
sacrificed accuracy. To address this challenge, we propose adaptive noise
addition in FL which decides the value of injected noise based on features'
relative importance. Here, we first propose two effective methods for
prioritizing features in deep neural network models and then perturb models'
weights based on this information. Specifically, we try to figure out whether
the idea of adding more noise to less important parameters and less noise to
more important parameters can effectively save the model accuracy while
preserving privacy. Our experiments confirm this statement under some
conditions. The amount of noise injected, the proportion of parameters
involved, and the number of global iterations can significantly change the
output. While a careful choice of parameters by considering the properties of
datasets can improve privacy without intense loss of accuracy, a bad choice can
make the model performance worse.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance. (arXiv:2401.02906v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02906">http://arxiv.org/abs/2401.02906</a></li>
<li>Code URL: <a href="https://github.com/pipilurj/mllm-protector">https://github.com/pipilurj/mllm-protector</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02906]] MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance(http://arxiv.org/abs/2401.02906)</code></li>
<li>Summary: <p>The deployment of multimodal large language models (MLLMs) has brought forth
a unique vulnerability: susceptibility to malicious attacks through visual
inputs. We delve into the novel challenge of defending MLLMs against such
attacks. We discovered that images act as a "foreign language" that is not
considered during alignment, which can make MLLMs prone to producing harmful
responses. Unfortunately, unlike the discrete tokens considered in text-based
LLMs, the continuous nature of image signals presents significant alignment
challenges, which poses difficulty to thoroughly cover the possible scenarios.
This vulnerability is exacerbated by the fact that open-source MLLMs are
predominantly fine-tuned on limited image-text pairs that is much less than the
extensive text-based pretraining corpus, which makes the MLLMs more prone to
catastrophic forgetting of their original abilities during explicit alignment
tuning. To tackle these challenges, we introduce MLLM-Protector, a
plug-and-play strategy combining a lightweight harm detector and a response
detoxifier. The harm detector's role is to identify potentially harmful outputs
from the MLLM, while the detoxifier corrects these outputs to ensure the
response stipulates to the safety standards. This approach effectively
mitigates the risks posed by malicious visual inputs without compromising the
model's overall performance. Our results demonstrate that MLLM-Protector offers
a robust solution to a previously unaddressed aspect of MLLM security.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Object-oriented backdoor attack against image captioning. (arXiv:2401.02600v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02600">http://arxiv.org/abs/2401.02600</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02600]] Object-oriented backdoor attack against image captioning(http://arxiv.org/abs/2401.02600)</code></li>
<li>Summary: <p>Backdoor attack against image classification task has been widely studied and
proven to be successful, while there exist little research on the backdoor
attack against vision-language models. In this paper, we explore backdoor
attack towards image captioning models by poisoning training data. Assuming the
attacker has total access to the training dataset, and cannot intervene in
model construction or training process. Specifically, a portion of benign
training samples is randomly selected to be poisoned. Afterwards, considering
that the captions are usually unfolded around objects in an image, we design an
object-oriented method to craft poisons, which aims to modify pixel values by a
slight range with the modification number proportional to the scale of the
current detected object region. After training with the poisoned data, the
attacked model behaves normally on benign images, but for poisoned images, the
model will generate some sentences irrelevant to the given image. The attack
controls the model behavior on specific test images without sacrificing the
generation performance on benign test images. Our method proves the weakness of
image captioning models to backdoor attack and we hope this work can raise the
awareness of defending against backdoor attack in the image captioning field.
</p></li>
</ul>

<h3>Title: Adaptive Discounting of Training Time Attacks. (arXiv:2401.02652v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02652">http://arxiv.org/abs/2401.02652</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02652]] Adaptive Discounting of Training Time Attacks(http://arxiv.org/abs/2401.02652)</code></li>
<li>Summary: <p>Among the most insidious attacks on Reinforcement Learning (RL) solutions are
training-time attacks (TTAs) that create loopholes and backdoors in the learned
behaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are
now available, where the attacker forces a specific, target behaviour upon a
training RL agent (victim). However, even state-of-the-art C-TTAs focus on
target behaviours that could be naturally adopted by the victim if not for a
particular feature of the environment dynamics, which C-TTAs exploit. In this
work, we show that a C-TTA is possible even when the target behaviour is
un-adoptable due to both environment dynamics as well as non-optimality with
respect to the victim objective(s). To find efficient attacks in this context,
we develop a specialised flavour of the DDPG algorithm, which we term
gammaDDPG, that learns this stronger version of C-TTA. gammaDDPG dynamically
alters the attack policy planning horizon based on the victim's current
behaviour. This improves effort distribution throughout the attack timeline and
reduces the effect of uncertainty the attacker has about the victim. To
demonstrate the features of our method and better relate the results to prior
research, we borrow a 3D grid domain from a state-of-the-art C-TTA for our
experiments. Code is available at "bit.ly/github-rb-gDDPG".
</p></li>
</ul>

<h3>Title: MalModel: Hiding Malicious Payload in Mobile Deep Learning Models with Black-box Backdoor Attack. (arXiv:2401.02659v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02659">http://arxiv.org/abs/2401.02659</a></li>
<li>Code URL: <a href="https://github.com/hjygh/malmodel">https://github.com/hjygh/malmodel</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02659]] MalModel: Hiding Malicious Payload in Mobile Deep Learning Models with Black-box Backdoor Attack(http://arxiv.org/abs/2401.02659)</code></li>
<li>Summary: <p>Mobile malware has become one of the most critical security threats in the
era of ubiquitous mobile computing. Despite the intensive efforts from security
experts to counteract it, recent years have still witnessed a rapid growth of
identified malware samples. This could be partly attributed to the
newly-emerged technologies that may constantly open up under-studied attack
surfaces for the adversaries. One typical example is the recently-developed
mobile machine learning (ML) framework that enables storing and running deep
learning (DL) models on mobile devices. Despite obvious advantages, this new
feature also inadvertently introduces potential vulnerabilities (e.g.,
on-device models may be modified for malicious purposes). In this work, we
propose a method to generate or transform mobile malware by hiding the
malicious payloads inside the parameters of deep learning models, based on a
strategy that considers four factors (layer type, layer number, layer coverage
and the number of bytes to replace). Utilizing the proposed method, we can run
malware in DL mobile applications covertly with little impact on the model
performance (i.e., as little as 0.4% drop in accuracy and at most 39ms latency
overhead).
</p></li>
</ul>

<h3>Title: A backdoor attack against link prediction tasks with graph neural networks. (arXiv:2401.02663v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02663">http://arxiv.org/abs/2401.02663</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02663]] A backdoor attack against link prediction tasks with graph neural networks(http://arxiv.org/abs/2401.02663)</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) are a class of deep learning models capable of
processing graph-structured data, and they have demonstrated significant
performance in a variety of real-world applications. Recent studies have found
that GNN models are vulnerable to backdoor attacks. When specific patterns
(called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input
data, the backdoor embedded in the GNN models is activated, which misclassifies
the input data into the target class label specified by the attacker, whereas
when there are no backdoor triggers in the input, the backdoor embedded in the
GNN models is not activated, and the models work normally. Backdoor attacks are
highly stealthy and expose GNN models to serious security risks. Currently,
research on backdoor attacks against GNNs mainly focus on tasks such as graph
classification and node classification, and backdoor attacks against link
prediction tasks are rarely studied. In this paper, we propose a backdoor
attack against the link prediction tasks based on GNNs and reveal the existence
of such security vulnerability in GNN models, which make the backdoored GNN
models to incorrectly predict unlinked two nodes as having a link relationship
when a trigger appear. The method uses a single node as the trigger and poison
selected node pairs in the training graph, and then the backdoor will be
embedded in the GNN models through the training process. In the inference
stage, the backdoor in the GNN models can be activated by simply linking the
trigger node to the two end nodes of the unlinked node pairs in the input data,
causing the GNN models to produce incorrect link prediction results for the
target node pairs.
</p></li>
</ul>

<h3>Title: Calibration Attack: A Framework For Adversarial Attacks Targeting Calibration. (arXiv:2401.02718v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02718">http://arxiv.org/abs/2401.02718</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02718]] Calibration Attack: A Framework For Adversarial Attacks Targeting Calibration(http://arxiv.org/abs/2401.02718)</code></li>
<li>Summary: <p>We introduce a new framework of adversarial attacks, named calibration
attacks, in which the attacks are generated and organized to trap victim models
to be miscalibrated without altering their original accuracy, hence seriously
endangering the trustworthiness of the models and any decision-making based on
their confidence scores. Specifically, we identify four novel forms of
calibration attacks: underconfidence attacks, overconfidence attacks, maximum
miscalibration attacks, and random confidence attacks, in both the black-box
and white-box setups. We then test these new attacks on typical victim models
with comprehensive datasets, demonstrating that even with a relatively low
number of queries, the attacks can create significant calibration mistakes. We
further provide detailed analyses to understand different aspects of
calibration attacks. Building on that, we investigate the effectiveness of
widely used adversarial defences and calibration methods against these types of
attacks, which then inspires us to devise two novel defences against such
calibration attacks.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis. (arXiv:2401.02436v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02436">http://arxiv.org/abs/2401.02436</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02436]] Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis(http://arxiv.org/abs/2401.02436)</code></li>
<li>Summary: <p>Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian
splat representation has been introduced for novel view synthesis from sparse
image sets. Making such representations suitable for applications like network
streaming and rendering on low-power devices requires significantly reduced
memory consumption as well as improved rendering efficiency. We propose a
compressed 3D Gaussian splat representation that utilizes sensitivity-aware
vector clustering with quantization-aware training to compress directional
colors and Gaussian parameters. The learned codebooks have low bitrates and
achieve a compression rate of up to $31\times$ on real-world scenes with only
minimal degradation of visual quality. We demonstrate that the compressed splat
representation can be efficiently rendered with hardware rasterization on
lightweight GPUs at up to $4\times$ higher framerates than reported via an
optimized GPU compute pipeline. Extensive experiments across multiple datasets
demonstrate the robustness and rendering speed of the proposed approach.
</p></li>
</ul>

<h3>Title: Image-based Deep Learning for Smart Digital Twins: a Review. (arXiv:2401.02523v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02523">http://arxiv.org/abs/2401.02523</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02523]] Image-based Deep Learning for Smart Digital Twins: a Review(http://arxiv.org/abs/2401.02523)</code></li>
<li>Summary: <p>Smart Digital twins (SDTs) are being increasingly used to virtually replicate
and predict the behaviors of complex physical systems through continual data
assimilation enabling the optimization of the performance of these systems by
controlling the actions of systems. Recently, deep learning (DL) models have
significantly enhanced the capabilities of SDTs, particularly for tasks such as
predictive maintenance, anomaly detection, and optimization. In many domains,
including medicine, engineering, and education, SDTs use image data
(image-based SDTs) to observe and learn system behaviors and control their
behaviors. This paper focuses on various approaches and associated challenges
in developing image-based SDTs by continually assimilating image data from
physical systems. The paper also discusses the challenges involved in designing
and implementing DL models for SDTs, including data acquisition, processing,
and interpretation. In addition, insights into the future directions and
opportunities for developing new image-based DL approaches to develop robust
SDTs are provided. This includes the potential for using generative models for
data augmentation, developing multi-modal DL models, and exploring the
integration of DL with other technologies, including 5G, edge computing, and
IoT. In this paper, we describe the image-based SDTs, which enable broader
adoption of the digital twin DT paradigms across a broad spectrum of areas and
the development of new methods to improve the abilities of SDTs in replicating,
predicting, and optimizing the behavior of complex systems.
</p></li>
</ul>

<h3>Title: OptFlow: Fast Optimization-based Scene Flow Estimation without Supervision. (arXiv:2401.02550v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02550">http://arxiv.org/abs/2401.02550</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02550]] OptFlow: Fast Optimization-based Scene Flow Estimation without Supervision(http://arxiv.org/abs/2401.02550)</code></li>
<li>Summary: <p>Scene flow estimation is a crucial component in the development of autonomous
driving and 3D robotics, providing valuable information for environment
perception and navigation. Despite the advantages of learning-based scene flow
estimation techniques, their domain specificity and limited generalizability
across varied scenarios pose challenges. In contrast, non-learning
optimization-based methods, incorporating robust priors or regularization,
offer competitive scene flow estimation performance, require no training, and
show extensive applicability across datasets, but suffer from lengthy inference
times. In this paper, we present OptFlow, a fast optimization-based scene flow
estimation method. Without relying on learning or any labeled datasets, OptFlow
achieves state-of-the-art performance for scene flow estimation on popular
autonomous driving benchmarks. It integrates a local correlation weight matrix
for correspondence matching, an adaptive correspondence threshold limit for
nearest-neighbor search, and graph prior rigidity constraints, resulting in
expedited convergence and improved point correspondence identification.
Moreover, we demonstrate how integrating a point cloud registration function
within our objective function bolsters accuracy and differentiates between
static and dynamic points without relying on external odometry data.
Consequently, OptFlow outperforms the baseline graph-prior method by
approximately 20% and the Neural Scene Flow Prior method by 5%-7% in accuracy,
all while offering the fastest inference time among all non-learning scene flow
estimation methods.
</p></li>
</ul>

<h3>Title: Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting. (arXiv:2401.02588v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02588">http://arxiv.org/abs/2401.02588</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02588]] Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting(http://arxiv.org/abs/2401.02588)</code></li>
<li>Summary: <p>The accelerating deployment of spacecraft in orbit have generated interest in
on-orbit servicing (OOS), inspection of spacecraft, and active debris removal
(ADR). Such missions require precise rendezvous and proximity operations in the
vicinity of non-cooperative, possible unknown, resident space objects. Safety
concerns with manned missions and lag times with ground-based control
necessitate complete autonomy. This requires robust characterization of the
target's geometry. In this article, we present an approach for mapping
geometries of satellites on orbit based on 3D Gaussian Splatting that can run
on computing resources available on current spaceflight hardware. We
demonstrate model training and 3D rendering performance on a
hardware-in-the-loop satellite mock-up under several realistic lighting and
motion conditions. Our model is shown to be capable of training on-board and
rendering higher quality novel views of an unknown satellite nearly 2 orders of
magnitude faster than previous NeRF-based algorithms. Such on-board
capabilities are critical to enable downstream machine intelligence tasks
necessary for autonomous guidance, navigation, and control tasks.
</p></li>
</ul>

<h3>Title: Exploiting Polarized Material Cues for Robust Car Detection. (arXiv:2401.02606v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02606">http://arxiv.org/abs/2401.02606</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02606]] Exploiting Polarized Material Cues for Robust Car Detection(http://arxiv.org/abs/2401.02606)</code></li>
<li>Summary: <p>Car detection is an important task that serves as a crucial prerequisite for
many automated driving functions. The large variations in lighting/weather
conditions and vehicle densities of the scenes pose significant challenges to
existing car detection algorithms to meet the highly accurate perception demand
for safety, due to the unstable/limited color information, which impedes the
extraction of meaningful/discriminative features of cars. In this work, we
present a novel learning-based car detection method that leverages trichromatic
linear polarization as an additional cue to disambiguate such challenging
cases. A key observation is that polarization, characteristic of the light
wave, can robustly describe intrinsic physical properties of the scene objects
in various imaging conditions and is strongly linked to the nature of materials
for cars (e.g., metal and glass) and their surrounding environment (e.g., soil
and trees), thereby providing reliable and discriminative features for robust
car detection in challenging scenes. To exploit polarization cues, we first
construct a pixel-aligned RGB-Polarization car detection dataset, which we
subsequently employ to train a novel multimodal fusion network. Our car
detection network dynamically integrates RGB and polarization features in a
request-and-complement manner and can explore the intrinsic material properties
of cars across all learning samples. We extensively validate our method and
demonstrate that it outperforms state-of-the-art detection methods.
Experimental results show that polarization is a powerful cue for car
detection.
</p></li>
</ul>

<h3>Title: Partition-based Nonrigid Registration for 3D Face Model. (arXiv:2401.02607v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02607">http://arxiv.org/abs/2401.02607</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02607]] Partition-based Nonrigid Registration for 3D Face Model(http://arxiv.org/abs/2401.02607)</code></li>
<li>Summary: <p>This paper presents a partition-based surface registration for 3D morphable
model(3DMM). In the 3DMM, it often requires to warp a handcrafted template
model into different captured models. The proposed method first utilizes the
landmarks to partition the template model then scale each part and finally
smooth the boundaries. This method is especially effective when the disparity
between the template model and the target model is huge. The experiment result
shows the method perform well than the traditional warp method and robust to
the local minima.
</p></li>
</ul>

<h3>Title: MOODv2: Masked Image Modeling for Out-of-Distribution Detection. (arXiv:2401.02611v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02611">http://arxiv.org/abs/2401.02611</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02611]] MOODv2: Masked Image Modeling for Out-of-Distribution Detection(http://arxiv.org/abs/2401.02611)</code></li>
<li>Summary: <p>The crux of effective out-of-distribution (OOD) detection lies in acquiring a
robust in-distribution (ID) representation, distinct from OOD samples. While
previous methods predominantly leaned on recognition-based techniques for this
purpose, they often resulted in shortcut learning, lacking comprehensive
representations. In our study, we conducted a comprehensive analysis, exploring
distinct pretraining tasks and employing various OOD score functions. The
results highlight that the feature representations pre-trained through
reconstruction yield a notable enhancement and narrow the performance gap among
various score functions. This suggests that even simple score functions can
rival complex ones when leveraging reconstruction-based pretext tasks.
Reconstruction-based pretext tasks adapt well to various score functions. As
such, it holds promising potential for further expansion. Our OOD detection
framework, MOODv2, employs the masked image modeling pretext task. Without
bells and whistles, MOODv2 impressively enhances 14.30% AUROC to 95.68% on
ImageNet and achieves 99.98% on CIFAR-10.
</p></li>
</ul>

<h3>Title: A Random Ensemble of Encrypted models for Enhancing Robustness against Adversarial Examples. (arXiv:2401.02633v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02633">http://arxiv.org/abs/2401.02633</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02633]] A Random Ensemble of Encrypted models for Enhancing Robustness against Adversarial Examples(http://arxiv.org/abs/2401.02633)</code></li>
<li>Summary: <p>Deep neural networks (DNNs) are well known to be vulnerable to adversarial
examples (AEs). In addition, AEs have adversarial transferability, which means
AEs generated for a source model can fool another black-box model (target
model) with a non-trivial probability. In previous studies, it was confirmed
that the vision transformer (ViT) is more robust against the property of
adversarial transferability than convolutional neural network (CNN) models such
as ConvMixer, and moreover encrypted ViT is more robust than ViT without any
encryption. In this article, we propose a random ensemble of encrypted ViT
models to achieve much more robust models. In experiments, the proposed scheme
is verified to be more robust against not only black-box attacks but also
white-box ones than convention methods.
</p></li>
</ul>

<h3>Title: Benchmarking PathCLIP for Pathology Image Analysis. (arXiv:2401.02651v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02651">http://arxiv.org/abs/2401.02651</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02651]] Benchmarking PathCLIP for Pathology Image Analysis(http://arxiv.org/abs/2401.02651)</code></li>
<li>Summary: <p>Accurate image classification and retrieval are of importance for clinical
diagnosis and treatment decision-making. The recent contrastive language-image
pretraining (CLIP) model has shown remarkable proficiency in understanding
natural images. Drawing inspiration from CLIP, PathCLIP is specifically
designed for pathology image analysis, utilizing over 200,000 image and text
pairs in training. While the performance the PathCLIP is impressive, its
robustness under a wide range of image corruptions remains unknown. Therefore,
we conduct an extensive evaluation to analyze the performance of PathCLIP on
various corrupted images from the datasets of Osteosarcoma and WSSS4LUAD. In
our experiments, we introduce seven corruption types including brightness,
contrast, Gaussian blur, resolution, saturation, hue, and markup at four
severity levels. Through experiments, we find that PathCLIP is relatively
robustness to image corruptions and surpasses OpenAI-CLIP and PLIP in zero-shot
classification. Among the seven corruptions, blur and resolution can cause
server performance degradation of the PathCLIP. This indicates that ensuring
the quality of images is crucial before conducting a clinical test.
Additionally, we assess the robustness of PathCLIP in the task of image-image
retrieval, revealing that PathCLIP performs less effectively than PLIP on
Osteosarcoma but performs better on WSSS4LUAD under diverse corruptions.
Overall, PathCLIP presents impressive zero-shot classification and retrieval
performance for pathology images, but appropriate care needs to be taken when
using it. We hope this study provides a qualitative impression of PathCLIP and
helps understand its differences from other CLIP models.
</p></li>
</ul>

<h3>Title: CRSOT: Cross-Resolution Object Tracking using Unaligned Frame and Event Cameras. (arXiv:2401.02826v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02826">http://arxiv.org/abs/2401.02826</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02826]] CRSOT: Cross-Resolution Object Tracking using Unaligned Frame and Event Cameras(http://arxiv.org/abs/2401.02826)</code></li>
<li>Summary: <p>Existing datasets for RGB-DVS tracking are collected with DVS346 camera and
their resolution ($346 \times 260$) is low for practical applications.
Actually, only visible cameras are deployed in many practical systems, and the
newly designed neuromorphic cameras may have different resolutions. The latest
neuromorphic sensors can output high-definition event streams, but it is very
difficult to achieve strict alignment between events and frames on both spatial
and temporal views. Therefore, how to achieve accurate tracking with unaligned
neuromorphic and visible sensors is a valuable but unresearched problem. In
this work, we formally propose the task of object tracking using unaligned
neuromorphic and visible cameras. We build the first unaligned frame-event
dataset CRSOT collected with a specially built data acquisition system, which
contains 1,030 high-definition RGB-Event video pairs, 304,974 video frames. In
addition, we propose a novel unaligned object tracking framework that can
realize robust tracking even using the loosely aligned RGB-Event data.
Specifically, we extract the template and search regions of RGB and Event data
and feed them into a unified ViT backbone for feature embedding. Then, we
propose uncertainty perception modules to encode the RGB and Event features,
respectively, then, we propose a modality uncertainty fusion module to
aggregate the two modalities. These three branches are jointly optimized in the
training phase. Extensive experiments demonstrate that our tracker can
collaborate the dual modalities for high-performance tracking even without
strictly temporal and spatial alignment. The source code, dataset, and
pre-trained models will be released at
https://github.com/Event-AHU/Cross_Resolution_SOT.
</p></li>
</ul>

<h3>Title: CrisisViT: A Robust Vision Transformer for Crisis Image Classification. (arXiv:2401.02838v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02838">http://arxiv.org/abs/2401.02838</a></li>
<li>Code URL: <a href="https://github.com/longkukuhi/crisisvit">https://github.com/longkukuhi/crisisvit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02838]] CrisisViT: A Robust Vision Transformer for Crisis Image Classification(http://arxiv.org/abs/2401.02838)</code></li>
<li>Summary: <p>In times of emergency, crisis response agencies need to quickly and
accurately assess the situation on the ground in order to deploy relevant
services and resources. However, authorities often have to make decisions based
on limited information, as data on affected regions can be scarce until local
response services can provide first-hand reports. Fortunately, the widespread
availability of smartphones with high-quality cameras has made citizen
journalism through social media a valuable source of information for crisis
responders. However, analyzing the large volume of images posted by citizens
requires more time and effort than is typically available. To address this
issue, this paper proposes the use of state-of-the-art deep neural models for
automatic image classification/tagging, specifically by adapting
transformer-based architectures for crisis image classification (CrisisViT). We
leverage the new Incidents1M crisis image dataset to develop a range of new
transformer-based image classification models. Through experimentation over the
standard Crisis image benchmark dataset, we demonstrate that the CrisisViT
models significantly outperform previous approaches in emergency type, image
relevance, humanitarian category, and damage severity classification.
Additionally, we show that the new Incidents1M dataset can further augment the
CrisisViT models resulting in an additional 1.25% absolute accuracy gain.
</p></li>
</ul>

<h3>Title: Towards ASR Robust Spoken Language Understanding Through In-Context Learning With Word Confusion Networks. (arXiv:2401.02921v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02921">http://arxiv.org/abs/2401.02921</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02921]] Towards ASR Robust Spoken Language Understanding Through In-Context Learning With Word Confusion Networks(http://arxiv.org/abs/2401.02921)</code></li>
<li>Summary: <p>In the realm of spoken language understanding (SLU), numerous natural
language understanding (NLU) methodologies have been adapted by supplying large
language models (LLMs) with transcribed speech instead of conventional written
text. In real-world scenarios, prior to input into an LLM, an automated speech
recognition (ASR) system generates an output transcript hypothesis, where
inherent errors can degrade subsequent SLU tasks. Here we introduce a method
that utilizes the ASR system's lattice output instead of relying solely on the
top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU
outcomes. Our in-context learning experiments, covering spoken question
answering and intent classification, underline the LLM's resilience to noisy
speech transcripts with the help of word confusion networks from lattices,
bridging the SLU performance gap between using the top ASR hypothesis and an
oracle upper bound. Additionally, we delve into the LLM's robustness to varying
ASR performance conditions and scrutinize the aspects of in-context learning
which prove the most influential.
</p></li>
</ul>

<h3>Title: MeTA: Multi-source Test Time Adaptation. (arXiv:2401.02561v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02561">http://arxiv.org/abs/2401.02561</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02561]] MeTA: Multi-source Test Time Adaptation(http://arxiv.org/abs/2401.02561)</code></li>
<li>Summary: <p>Test time adaptation is the process of adapting, in an unsupervised manner, a
pre-trained source model to each incoming batch of the test data (i.e., without
requiring a substantial portion of the test data to be available, as in
traditional domain adaptation) and without access to the source data. Since it
works with each batch of test data, it is well-suited for dynamic environments
where decisions need to be made as the data is streaming in. Current test time
adaptation methods are primarily focused on a single source model. We propose
the first completely unsupervised Multi-source Test Time Adaptation (MeTA)
framework that handles multiple source models and optimally combines them to
adapt to the test data. MeTA has two distinguishing features. First, it
efficiently obtains the optimal combination weights to combine the source
models to adapt to the test data distribution. Second, it identifies which of
the source model parameters to update so that only the model which is most
correlated to the target data is adapted, leaving the less correlated ones
untouched; this mitigates the issue of "forgetting" the source model parameters
by focusing only on the source model that exhibits the strongest correlation
with the test batch distribution. Experiments on diverse datasets demonstrate
that the combination of multiple source models does at least as well as the
best source (with hindsight knowledge), and performance does not degrade as the
test data distribution changes over time (robust to forgetting).
</p></li>
</ul>

<h3>Title: TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis. (arXiv:2401.02708v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02708">http://arxiv.org/abs/2401.02708</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02708]] TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis(http://arxiv.org/abs/2401.02708)</code></li>
<li>Summary: <p>A core challenge in survival analysis is to model the distribution of
censored time-to-event data, where the event of interest may be a death,
failure, or occurrence of a specific event. Previous studies have showed that
ranking and maximum likelihood estimation (MLE)loss functions are widely-used
for survival analysis. However, ranking loss only focus on the ranking of
survival time and does not consider potential effect of samples for exact
survival time values. Furthermore, the MLE is unbounded and easily subject to
outliers (e.g., censored data), which may cause poor performance of modeling.
To handle the complexities of learning process and exploit valuable survival
time values, we propose a time-adaptive coordinate loss function, TripleSurv,
to achieve adaptive adjustments by introducing the differences in the survival
time between sample pairs into the ranking, which can encourage the model to
quantitatively rank relative risk of pairs, ultimately enhancing the accuracy
of predictions. Most importantly, the TripleSurv is proficient in quantifying
the relative risk between samples by ranking ordering of pairs, and consider
the time interval as a trade-off to calibrate the robustness of model over
sample distribution. Our TripleSurv is evaluated on three real-world survival
datasets and a public synthetic dataset. The results show that our method
outperforms the state-of-the-art methods and exhibits good model performance
and robustness on modeling various sophisticated data distributions with
different censor rates. Our code will be available upon acceptance.
</p></li>
</ul>

<h3>Title: Tackling Electrode Shift In Gesture Recognition with HD-EMG Electrode Subsets. (arXiv:2401.02773v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02773">http://arxiv.org/abs/2401.02773</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02773]] Tackling Electrode Shift In Gesture Recognition with HD-EMG Electrode Subsets(http://arxiv.org/abs/2401.02773)</code></li>
<li>Summary: <p>sEMG pattern recognition algorithms have been explored extensively in
decoding movement intent, yet are known to be vulnerable to changing recording
conditions, exhibiting significant drops in performance across subjects, and
even across sessions. Multi-channel surface EMG, also referred to as
high-density sEMG (HD-sEMG) systems, have been used to improve performance with
the information collected through the use of additional electrodes. However, a
lack of robustness is ever present due to limited datasets and the difficulties
in addressing sources of variability, such as electrode placement. In this
study, we propose training on a collection of input channel subsets and
augmenting our training distribution with data from different electrode
locations, simultaneously targeting electrode shift and reducing input
dimensionality. Our method increases robustness against electrode shift and
results in significantly higher intersession performance across subjects and
classification algorithms.
</p></li>
</ul>

<h3>Title: Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning. (arXiv:2401.02810v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02810">http://arxiv.org/abs/2401.02810</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02810]] Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning(http://arxiv.org/abs/2401.02810)</code></li>
<li>Summary: <p>Physics-informed neural network (PINN) is a data-driven solver for partial
and ordinary differential equations(ODEs/PDEs). It provides a unified framework
to address both forward and inverse problems. However, the complexity of the
objective function often leads to training failures. This issue is particularly
prominent when solving high-frequency and multi-scale problems. We proposed
using transfer learning to boost the robustness and convergence of training
PINN, starting training from low-frequency problems and gradually approaching
high-frequency problems. Through two case studies, we discovered that transfer
learning can effectively train PINN to approximate solutions from low-frequency
problems to high-frequency problems without increasing network parameters.
Furthermore, it requires fewer data points and less training time. We
elaborately described our training strategy, including optimizer selection, and
suggested guidelines for using transfer learning to train neural networks for
solving more complex problems.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Recent Advancement in 3D Biometrics using Monocular Camera. (arXiv:2401.02646v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02646">http://arxiv.org/abs/2401.02646</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02646]] Recent Advancement in 3D Biometrics using Monocular Camera(http://arxiv.org/abs/2401.02646)</code></li>
<li>Summary: <p>Recent literature has witnessed significant interest towards 3D biometrics
employing monocular vision for robust authentication methods. Motivated by
this, in this work we seek to provide insight on recent development in the area
of 3D biometrics employing monocular vision. We present the similarity and
dissimilarity of 3D monocular biometrics and classical biometrics, listing the
strengths and challenges. Further, we provide an overview of recent techniques
in 3D biometrics with monocular vision, as well as application systems adopted
by the industry. Finally, we discuss open research problems in this area of
research
</p></li>
</ul>

<h3>Title: Reversing the Irreversible: A Survey on Inverse Biometrics. (arXiv:2401.02861v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02861">http://arxiv.org/abs/2401.02861</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02861]] Reversing the Irreversible: A Survey on Inverse Biometrics(http://arxiv.org/abs/2401.02861)</code></li>
<li>Summary: <p>With the widespread use of biometric recognition, several issues related to
the privacy and security provided by this technology have been recently raised
and analysed. As a result, the early common belief among the biometrics
community of templates irreversibility has been proven wrong. It is now an
accepted fact that it is possible to reconstruct from an unprotected template a
synthetic sample that matches the bona fide one. This reverse engineering
process, commonly referred to as \textit{inverse biometrics}, constitutes a
severe threat for biometric systems from two different angles: on the one hand,
sensitive personal data (i.e., biometric data) can be derived from compromised
unprotected templates; on the other hand, other powerful attacks can be
launched building upon these reconstructed samples. Given its important
implications, biometric stakeholders have produced over the last fifteen years
numerous works analysing the different aspects related to inverse biometrics:
development of reconstruction algorithms for different characteristics;
proposal of methodologies to assess the vulnerabilities of biometric systems to
the aforementioned algorithms; development of countermeasures to reduce the
possible effects of attacks. The present article is an effort to condense all
this information in one comprehensive review of: the problem itself, the
evaluation of the problem, and the mitigation of the problem. The present
article is an effort to condense all this information in one comprehensive
review of: the problem itself, the evaluation of the problem, and the
mitigation of the problem.
</p></li>
</ul>

<h3>Title: User authentication system based on human exhaled breath physics. (arXiv:2401.02447v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02447">http://arxiv.org/abs/2401.02447</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02447]] User authentication system based on human exhaled breath physics(http://arxiv.org/abs/2401.02447)</code></li>
<li>Summary: <p>This work, in a pioneering approach, attempts to build a biometric system
that works purely based on the fluid mechanics governing exhaled breath. We
test the hypothesis that the structure of turbulence in exhaled human breath
can be exploited to build biometric algorithms. This work relies on the idea
that the extrathoracic airway is unique for every individual, making the
exhaled breath a biomarker. Methods including classical multi-dimensional
hypothesis testing approach and machine learning models are employed in
building user authentication algorithms, namely user confirmation and user
identification. A user confirmation algorithm tries to verify whether a user is
the person they claim to be. A user identification algorithm tries to identify
a user's identity with no prior information available. A dataset of exhaled
breath time series samples from 94 human subjects was used to evaluate the
performance of these algorithms. The user confirmation algorithms performed
exceedingly well for the given dataset with over $97\%$ true confirmation rate.
The machine learning based algorithm achieved a good true confirmation rate,
reiterating our understanding of why machine learning based algorithms
typically outperform classical hypothesis test based algorithms. The user
identification algorithm performs reasonably well with the provided dataset
with over $50\%$ of the users identified as being within two possible suspects.
We show surprisingly unique turbulent signatures in the exhaled breath that
have not been discovered before. In addition to discussions on a novel
biometric system, we make arguments to utilise this idea as a tool to gain
insights into the morphometric variation of extrathoracic airway across
individuals. Such tools are expected to have future potential in the area of
personalised medicines.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: DocGraphLM: Documental Graph Language Model for Information Extraction. (arXiv:2401.02823v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02823">http://arxiv.org/abs/2401.02823</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02823]] DocGraphLM: Documental Graph Language Model for Information Extraction(http://arxiv.org/abs/2401.02823)</code></li>
<li>Summary: <p>Advances in Visually Rich Document Understanding (VrDU) have enabled
information extraction and question answering over documents with complex
layouts. Two tropes of architectures have emerged -- transformer-based models
inspired by LLMs, and Graph Neural Networks. In this paper, we introduce
DocGraphLM, a novel framework that combines pre-trained language models with
graph semantics. To achieve this, we propose 1) a joint encoder architecture to
represent documents, and 2) a novel link prediction approach to reconstruct
document graphs. DocGraphLM predicts both directions and distances between
nodes using a convergent joint loss function that prioritizes neighborhood
restoration and downweighs distant node detection. Our experiments on three
SotA datasets show consistent improvement on IE and QA tasks with the adoption
of graph features. Moreover, we report that adopting the graph features
accelerates convergence in the learning process during training, despite being
solely constructed through link prediction.
</p></li>
</ul>

<h3>Title: Graph-level Protein Representation Learning by Structure Knowledge Refinement. (arXiv:2401.02713v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02713">http://arxiv.org/abs/2401.02713</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02713]] Graph-level Protein Representation Learning by Structure Knowledge Refinement(http://arxiv.org/abs/2401.02713)</code></li>
<li>Summary: <p>This paper focuses on learning representation on the whole graph level in an
unsupervised manner. Learning graph-level representation plays an important
role in a variety of real-world issues such as molecule property prediction,
protein structure feature extraction, and social network analysis. The
mainstream method is utilizing contrastive learning to facilitate graph feature
extraction, known as Graph Contrastive Learning (GCL). GCL, although effective,
suffers from some complications in contrastive learning, such as the effect of
false negative pairs. Moreover, augmentation strategies in GCL are weakly
adaptive to diverse graph datasets. Motivated by these problems, we propose a
novel framework called Structure Knowledge Refinement (SKR) which uses data
structure to determine the probability of whether a pair is positive or
negative. Meanwhile, we propose an augmentation strategy that naturally
preserves the semantic meaning of the original data and is compatible with our
SKR framework. Furthermore, we illustrate the effectiveness of our SKR
framework through intuition and experiments. The experimental results on the
tasks of graph-level classification demonstrate that our SKR framework is
superior to most state-of-the-art baselines.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FedDiff: Diffusion Model Driven Federated Learning for Multi-Modal and Multi-Clients. (arXiv:2401.02433v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02433">http://arxiv.org/abs/2401.02433</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02433]] FedDiff: Diffusion Model Driven Federated Learning for Multi-Modal and Multi-Clients(http://arxiv.org/abs/2401.02433)</code></li>
<li>Summary: <p>With the rapid development of imaging sensor technology in the field of
remote sensing, multi-modal remote sensing data fusion has emerged as a crucial
research direction for land cover classification tasks. While diffusion models
have made great progress in generative models and image classification tasks,
existing models primarily focus on single-modality and single-client control,
that is, the diffusion process is driven by a single modal in a single
computing node. To facilitate the secure fusion of heterogeneous data from
clients, it is necessary to enable distributed multi-modal control, such as
merging the hyperspectral data of organization A and the LiDAR data of
organization B privately on each base station client. In this study, we propose
a multi-modal collaborative diffusion federated learning framework called
FedDiff. Our framework establishes a dual-branch diffusion model feature
extraction setup, where the two modal data are inputted into separate branches
of the encoder. Our key insight is that diffusion models driven by different
modalities are inherently complementary in terms of potential denoising steps
on which bilateral connections can be built. Considering the challenge of
private and efficient communication between multiple clients, we embed the
diffusion model into the federated learning communication structure, and
introduce a lightweight communication module. Qualitative and quantitative
experiments validate the superiority of our framework in terms of image quality
and conditional consistency.
</p></li>
</ul>

<h3>Title: Predicting Traffic Flow with Federated Learning and Graph Neural with Asynchronous Computations Network. (arXiv:2401.02723v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02723">http://arxiv.org/abs/2401.02723</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02723]] Predicting Traffic Flow with Federated Learning and Graph Neural with Asynchronous Computations Network(http://arxiv.org/abs/2401.02723)</code></li>
<li>Summary: <p>Real-time traffic flow prediction holds significant importance within the
domain of Intelligent Transportation Systems (ITS). The task of achieving a
balance between prediction precision and computational efficiency presents a
significant challenge. In this article, we present a novel deep-learning method
called Federated Learning and Asynchronous Graph Convolutional Network
(FLAGCN). Our framework incorporates the principles of asynchronous graph
convolutional networks with federated learning to enhance the accuracy and
efficiency of real-time traffic flow prediction. The FLAGCN model employs a
spatial-temporal graph convolution technique to asynchronously address
spatio-temporal dependencies within traffic data effectively. To efficiently
handle the computational requirements associated with this deep learning model,
this study used a graph federated learning technique known as GraphFL. This
approach is designed to facilitate the training process. The experimental
results obtained from conducting tests on two distinct traffic datasets
demonstrate that the utilization of FLAGCN leads to the optimization of both
training and inference durations while maintaining a high level of prediction
accuracy. FLAGCN outperforms existing models with significant improvements by
achieving up to approximately 6.85% reduction in RMSE, 20.45% reduction in
MAPE, compared to the best-performing existing models.
</p></li>
</ul>

<h3>Title: Unsupervised Federated Domain Adaptation for Segmentation of MRI Images. (arXiv:2401.02941v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02941">http://arxiv.org/abs/2401.02941</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02941]] Unsupervised Federated Domain Adaptation for Segmentation of MRI Images(http://arxiv.org/abs/2401.02941)</code></li>
<li>Summary: <p>Automatic semantic segmentation of magnetic resonance imaging (MRI) images
using deep neural networks greatly assists in evaluating and planning
treatments for various clinical applications. However, training these models is
conditioned on the availability of abundant annotated data to implement the
end-to-end supervised learning procedure. Even if we annotate enough data, MRI
images display considerable variability due to factors such as differences in
patients, MRI scanners, and imaging protocols. This variability necessitates
retraining neural networks for each specific application domain, which, in
turn, requires manual annotation by expert radiologists for all new domains. To
relax the need for persistent data annotation, we develop a method for
unsupervised federated domain adaptation using multiple annotated source
domains. Our approach enables the transfer of knowledge from several annotated
source domains to adapt a model for effective use in an unannotated target
domain. Initially, we ensure that the target domain data shares similar
representations with each source domain in a latent embedding space, modeled as
the output of a deep encoder, by minimizing the pair-wise distances of the
distributions for the target domain and the source domains. We then employ an
ensemble approach to leverage the knowledge obtained from all domains. We
provide theoretical analysis and perform experiments on the MICCAI 2016
multi-site dataset to demonstrate our method is effective.
</p></li>
</ul>

<h3>Title: Federated Learning for distribution skewed data using sample weights. (arXiv:2401.02586v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02586">http://arxiv.org/abs/2401.02586</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02586]] Federated Learning for distribution skewed data using sample weights(http://arxiv.org/abs/2401.02586)</code></li>
<li>Summary: <p>One of the most challenging issues in federated learning is that the data is
often not independent and identically distributed (nonIID). Clients are
expected to contribute the same type of data and drawn from one global
distribution. However, data are often collected in different ways from
different resources. Thus, the data distributions among clients might be
different from the underlying global distribution. This creates a weight
divergence issue and reduces federated learning performance. This work focuses
on improving federated learning performance for skewed data distribution across
clients. The main idea is to adjust the client distribution closer to the
global distribution using sample weights. Thus, the machine learning model
converges faster with higher accuracy. We start from the fundamental concept of
empirical risk minimization and theoretically derive a solution for adjusting
the distribution skewness using sample weights. To determine sample weights, we
implicitly exchange density information by leveraging a neural network-based
density estimation model, MADE. The clients data distribution can then be
adjusted without exposing their raw data. Our experiment results on three
real-world datasets show that the proposed method not only improves federated
learning accuracy but also significantly reduces communication costs compared
to the other experimental methods.
</p></li>
</ul>

<h3>Title: FedNS: A Fast Sketching Newton-Type Algorithm for Federated Learning. (arXiv:2401.02734v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02734">http://arxiv.org/abs/2401.02734</a></li>
<li>Code URL: <a href="https://github.com/superlj666/fedns">https://github.com/superlj666/fedns</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02734]] FedNS: A Fast Sketching Newton-Type Algorithm for Federated Learning(http://arxiv.org/abs/2401.02734)</code></li>
<li>Summary: <p>Recent Newton-type federated learning algorithms have demonstrated linear
convergence with respect to the communication rounds. However, communicating
Hessian matrices is often unfeasible due to their quadratic communication
complexity. In this paper, we introduce a novel approach to tackle this issue
while still achieving fast convergence rates. Our proposed method, named as
Federated Newton Sketch methods (FedNS), approximates the centralized Newton's
method by communicating the sketched square-root Hessian instead of the exact
Hessian. To enhance communication efficiency, we reduce the sketch size to
match the effective dimension of the Hessian matrix. We provide convergence
analysis based on statistical learning for the federated Newton sketch
approaches. Specifically, our approaches reach super-linear convergence rates
w.r.t. the communication rounds for the first time. We validate the
effectiveness of our algorithms through various experiments, which coincide
with our theoretical findings.
</p></li>
</ul>

<h3>Title: Fairness-Aware Job Scheduling for Multi-Job Federated Learning. (arXiv:2401.02740v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02740">http://arxiv.org/abs/2401.02740</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02740]] Fairness-Aware Job Scheduling for Multi-Job Federated Learning(http://arxiv.org/abs/2401.02740)</code></li>
<li>Summary: <p>Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to
collaboratively train machine learning models without disclosing sensitive
private data. Existing FL research mostly focuses on the monopoly scenario in
which a single FL server selects a subset of FL clients to update their local
models in each round of training. In practice, there can be multiple FL servers
simultaneously trying to select clients from the same pool. In this paper, we
propose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS)
approach to bridge this gap. Based on Lyapunov optimization, it ensures fair
allocation of high-demand FL client datasets to FL jobs in need of them, by
jointly considering the current demand and the job payment bids, in order to
prevent prolonged waiting. Extensive experiments comparing FairFedJS against
four state-of-the-art approaches on two datasets demonstrate its significant
advantages. It outperforms the best baseline by 31.9% and 1.0% on average in
terms of scheduling fairness and convergence time, respectively, while
achieving comparable test accuracy.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Long-term Fairness For Real-time Decision Making: A Constrained Online Optimization Approach. (arXiv:2401.02552v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02552">http://arxiv.org/abs/2401.02552</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02552]] Long-term Fairness For Real-time Decision Making: A Constrained Online Optimization Approach(http://arxiv.org/abs/2401.02552)</code></li>
<li>Summary: <p>Machine learning (ML) has demonstrated remarkable capabilities across many
real-world systems, from predictive modeling to intelligent automation.
However, the widespread integration of machine learning also makes it necessary
to ensure machine learning-driven decision-making systems do not violate
ethical principles and values of society in which they operate. As ML-driven
decisions proliferate, particularly in cases involving sensitive attributes
such as gender, race, and age, to name a few, the need for equity and
impartiality has emerged as a fundamental concern. In situations demanding
real-time decision-making, fairness objectives become more nuanced and complex:
instantaneous fairness to ensure equity in every time slot, and long-term
fairness to ensure fairness over a period of time. There is a growing awareness
that real-world systems that operate over long periods and require fairness
over different timelines. However, existing approaches mainly address dynamic
costs with time-invariant fairness constraints, often disregarding the
challenges posed by time-varying fairness constraints. To bridge this gap, this
work introduces a framework for ensuring long-term fairness within dynamic
decision-making systems characterized by time-varying fairness constraints. We
formulate the decision problem with fairness constraints over a period as a
constrained online optimization problem. A novel online algorithm, named
LoTFair, is presented that solves the problem 'on the fly'. We prove that
LoTFair can make overall fairness violations negligible while maintaining the
performance over the long run.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Model-Agnostic Interpretation Framework in Machine Learning: A Comparative Study in NBA Sports. (arXiv:2401.02630v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02630">http://arxiv.org/abs/2401.02630</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02630]] Model-Agnostic Interpretation Framework in Machine Learning: A Comparative Study in NBA Sports(http://arxiv.org/abs/2401.02630)</code></li>
<li>Summary: <p>The field of machine learning has seen tremendous progress in recent years,
with deep learning models delivering exceptional performance across a range of
tasks. However, these models often come at the cost of interpretability, as
they operate as opaque "black boxes" that obscure the rationale behind their
decisions. This lack of transparency can limit understanding of the models'
underlying principles and impede their deployment in sensitive domains, such as
healthcare or finance. To address this challenge, our research team has
proposed an innovative framework designed to reconcile the trade-off between
model performance and interpretability. Our approach is centered around modular
operations on high-dimensional data, which enable end-to-end processing while
preserving interpretability. By fusing diverse interpretability techniques and
modularized data processing, our framework sheds light on the decision-making
processes of complex models without compromising their performance. We have
extensively tested our framework and validated its superior efficacy in
achieving a harmonious balance between computational efficiency and
interpretability. Our approach addresses a critical need in contemporary
machine learning applications by providing unprecedented insights into the
inner workings of complex models, fostering trust, transparency, and
accountability in their deployment across diverse domains.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: VASE: Object-Centric Appearance and Shape Manipulation of Real Videos. (arXiv:2401.02473v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02473">http://arxiv.org/abs/2401.02473</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02473]] VASE: Object-Centric Appearance and Shape Manipulation of Real Videos(http://arxiv.org/abs/2401.02473)</code></li>
<li>Summary: <p>Recently, several works tackled the video editing task fostered by the
success of large-scale text-to-image generative models. However, most of these
methods holistically edit the frame using the text, exploiting the prior given
by foundation diffusion models and focusing on improving the temporal
consistency across frames. In this work, we introduce a framework that is
object-centric and is designed to control both the object's appearance and,
notably, to execute precise and explicit structural modifications on the
object. We build our framework on a pre-trained image-conditioned diffusion
model, integrate layers to handle the temporal dimension, and propose training
strategies and architectural modifications to enable shape control. We evaluate
our method on the image-driven video editing task showing similar performance
to the state-of-the-art, and showcasing novel shape-editing capabilities.
Further details, code and examples are available on our project page:
https://helia95.github.io/vase-website/
</p></li>
</ul>

<h3>Title: Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss. (arXiv:2401.02677v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02677">http://arxiv.org/abs/2401.02677</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02677]] Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss(http://arxiv.org/abs/2401.02677)</code></li>
<li>Summary: <p>Stable Diffusion XL (SDXL) has become the best open source text-to-image
model (T2I) for its versatility and top-notch image quality. Efficiently
addressing the computational demands of SDXL models is crucial for wider reach
and applicability. In this work, we introduce two scaled-down variants, Segmind
Stable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter
UNets, respectively, achieved through progressive removal using layer-level
losses focusing on reducing the model size while preserving generative quality.
We release these models weights at https://hf.co/Segmind. Our methodology
involves the elimination of residual networks and transformer blocks from the
U-Net structure of SDXL, resulting in significant reductions in parameters, and
latency. Our compact models effectively emulate the original SDXL by
capitalizing on transferred knowledge, achieving competitive results against
larger multi-billion parameter SDXL. Our work underscores the efficacy of
knowledge distillation coupled with layer-level losses in reducing model size
while preserving the high-quality generative capabilities of SDXL, thus
facilitating more accessible deployment in resource-constrained environments.
</p></li>
</ul>

<h3>Title: Diffbody: Diffusion-based Pose and Shape Editing of Human Images. (arXiv:2401.02804v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02804">http://arxiv.org/abs/2401.02804</a></li>
<li>Code URL: <a href="https://github.com/yutaokuyama/diffbody">https://github.com/yutaokuyama/diffbody</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02804]] Diffbody: Diffusion-based Pose and Shape Editing of Human Images(http://arxiv.org/abs/2401.02804)</code></li>
<li>Summary: <p>Pose and body shape editing in a human image has received increasing
attention. However, current methods often struggle with dataset biases and
deteriorate realism and the person's identity when users make large edits. We
propose a one-shot approach that enables large edits with identity
preservation. To enable large edits, we fit a 3D body model, project the input
image onto the 3D model, and change the body's pose and shape. Because this
initial textured body model has artifacts due to occlusion and the inaccurate
body shape, the rendered image undergoes a diffusion-based refinement, in which
strong noise destroys body structure and identity whereas insufficient noise
does not help. We thus propose an iterative refinement with weak noise, applied
first for the whole body and then for the face. We further enhance the realism
by fine-tuning text embeddings via self-supervised learning. Our quantitative
and qualitative evaluations demonstrate that our method outperforms other
existing methods across various datasets.
</p></li>
</ul>

<h3>Title: Generating Non-Stationary Textures using Self-Rectification. (arXiv:2401.02847v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02847">http://arxiv.org/abs/2401.02847</a></li>
<li>Code URL: <a href="https://github.com/xiaorongjun000/self-rectification">https://github.com/xiaorongjun000/self-rectification</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02847]] Generating Non-Stationary Textures using Self-Rectification(http://arxiv.org/abs/2401.02847)</code></li>
<li>Summary: <p>This paper addresses the challenge of example-based non-stationary texture
synthesis. We introduce a novel twostep approach wherein users first modify a
reference texture using standard image editing tools, yielding an initial rough
target for the synthesis. Subsequently, our proposed method, termed
"self-rectification", automatically refines this target into a coherent,
seamless texture, while faithfully preserving the distinct visual
characteristics of the reference exemplar. Our method leverages a pre-trained
diffusion network, and uses self-attention mechanisms, to gradually align the
synthesized texture with the reference, ensuring the retention of the
structures in the provided target. Through experimental validation, our
approach exhibits exceptional proficiency in handling non-stationary textures,
demonstrating significant advancements in texture synthesis when compared to
existing state-of-the-art techniques. Code is available at
https://github.com/xiaorongjun000/Self-Rectification
</p></li>
</ul>

<h3>Title: Uncovering the human motion pattern: Pattern Memory-based Diffusion Model for Trajectory Prediction. (arXiv:2401.02916v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02916">http://arxiv.org/abs/2401.02916</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02916]] Uncovering the human motion pattern: Pattern Memory-based Diffusion Model for Trajectory Prediction(http://arxiv.org/abs/2401.02916)</code></li>
<li>Summary: <p>Human trajectory forecasting is a critical challenge in fields such as
robotics and autonomous driving. Due to the inherent uncertainty of human
actions and intentions in real-world scenarios, various unexpected occurrences
may arise. To uncover latent motion patterns in human behavior, we introduce a
novel memory-based method, named Motion Pattern Priors Memory Network. Our
method involves constructing a memory bank derived from clustered prior
knowledge of motion patterns observed in the training set trajectories. We
introduce an addressing mechanism to retrieve the matched pattern and the
potential target distributions for each prediction from the memory bank, which
enables the identification and retrieval of natural motion patterns exhibited
by agents, subsequently using the target priors memory token to guide the
diffusion model to generate predictions. Extensive experiments validate the
effectiveness of our approach, achieving state-of-the-art trajectory prediction
accuracy. The code will be made publicly available.
</p></li>
</ul>

<h3>Title: Simple Hierarchical Planning with Diffusion. (arXiv:2401.02644v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02644">http://arxiv.org/abs/2401.02644</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02644]] Simple Hierarchical Planning with Diffusion(http://arxiv.org/abs/2401.02644)</code></li>
<li>Summary: <p>Diffusion-based generative methods have proven effective in modeling
trajectories with offline datasets. However, they often face computational
challenges and can falter in generalization, especially in capturing temporal
abstractions for long-horizon tasks. To overcome this, we introduce the
Hierarchical Diffuser, a simple, fast, yet surprisingly effective planning
method combining the advantages of hierarchical and diffusion-based planning.
Our model adopts a "jumpy" planning strategy at the higher level, which allows
it to have a larger receptive field but at a lower computational cost -- a
crucial factor for diffusion-based planning methods, as we have empirically
verified. Additionally, the jumpy sub-goals guide our low-level planner,
facilitating a fine-tuning stage and further improving our approach's
effectiveness. We conducted empirical evaluations on standard offline
reinforcement learning benchmarks, demonstrating our method's superior
performance and efficiency in terms of training and planning speed compared to
the non-hierarchical Diffuser as well as other hierarchical planning methods.
Moreover, we explore our model's generalization capability, particularly on how
our method improves generalization capabilities on compositional
out-of-distribution tasks.
</p></li>
</ul>

<h3>Title: Geometric-Facilitated Denoising Diffusion Model for 3D Molecule Generation. (arXiv:2401.02683v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02683">http://arxiv.org/abs/2401.02683</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02683]] Geometric-Facilitated Denoising Diffusion Model for 3D Molecule Generation(http://arxiv.org/abs/2401.02683)</code></li>
<li>Summary: <p>Denoising diffusion models have shown great potential in multiple research
areas. Existing diffusion-based generative methods on de novo 3D molecule
generation face two major challenges. Since majority heavy atoms in molecules
allow connections to multiple atoms through single bonds, solely using
pair-wise distance to model molecule geometries is insufficient. Therefore, the
first one involves proposing an effective neural network as the denoising
kernel that is capable to capture complex multi-body interatomic relationships
and learn high-quality features. Due to the discrete nature of graphs,
mainstream diffusion-based methods for molecules heavily rely on predefined
rules and generate edges in an indirect manner. The second challenge involves
accommodating molecule generation to diffusion and accurately predicting the
existence of bonds. In our research, we view the iterative way of updating
molecule conformations in diffusion process is consistent with molecular
dynamics and introduce a novel molecule generation method named
Geometric-Facilitated Molecular Diffusion (GFMDiff). For the first challenge,
we introduce a Dual-Track Transformer Network (DTN) to fully excevate global
spatial relationships and learn high quality representations which contribute
to accurate predictions of features and geometries. As for the second
challenge, we design Geometric-Facilitated Loss (GFLoss) which intervenes the
formation of bonds during the training period, instead of directly embedding
edges into the latent space. Comprehensive experiments on current benchmarks
demonstrate the superiority of GFMDiff.
</p></li>
</ul>

<h3>Title: Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors. (arXiv:2401.02739v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02739">http://arxiv.org/abs/2401.02739</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02739]] Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors(http://arxiv.org/abs/2401.02739)</code></li>
<li>Summary: <p>We propose denoising diffusion variational inference (DDVI), an approximate
inference algorithm for latent variable models which relies on diffusion models
as expressive variational posteriors. Our method augments variational
posteriors with auxiliary latents, which yields an expressive class of models
that perform diffusion in latent space by reversing a user-specified noising
process. We fit these models by optimizing a novel lower bound on the marginal
likelihood inspired by the wake-sleep algorithm. Our method is easy to
implement (it fits a regularized extension of the ELBO), is compatible with
black-box variational inference, and outperforms alternative classes of
approximate posteriors based on normalizing flows or adversarial networks. When
applied to deep latent variable models, our method yields the denoising
diffusion VAE (DD-VAE) algorithm. We use this algorithm on a motivating task in
biology -- inferring latent ancestry from human genomes -- outperforming strong
baselines on the Thousand Genomes dataset.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: SPFormer: Enhancing Vision Transformer with Superpixel Representation. (arXiv:2401.02931v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02931">http://arxiv.org/abs/2401.02931</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02931]] SPFormer: Enhancing Vision Transformer with Superpixel Representation(http://arxiv.org/abs/2401.02931)</code></li>
<li>Summary: <p>In this work, we introduce SPFormer, a novel Vision Transformer enhanced by
superpixel representation. Addressing the limitations of traditional Vision
Transformers' fixed-size, non-adaptive patch partitioning, SPFormer employs
superpixels that adapt to the image's content. This approach divides the image
into irregular, semantically coherent regions, effectively capturing intricate
details and applicable at both initial and intermediate feature levels.
</p>
<p>SPFormer, trainable end-to-end, exhibits superior performance across various
benchmarks. Notably, it exhibits significant improvements on the challenging
ImageNet benchmark, achieving a 1.4% increase over DeiT-T and 1.1% over DeiT-S
respectively. A standout feature of SPFormer is its inherent explainability.
The superpixel structure offers a window into the model's internal processes,
providing valuable insights that enhance the model's interpretability. This
level of clarity significantly improves SPFormer's robustness, particularly in
challenging scenarios such as image rotations and occlusions, demonstrating its
adaptability and resilience.
</p></li>
</ul>

<h3>Title: Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively. (arXiv:2401.02955v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02955">http://arxiv.org/abs/2401.02955</a></li>
<li>Code URL: <a href="https://github.com/harboryuan/ovsam">https://github.com/harboryuan/ovsam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02955]] Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively(http://arxiv.org/abs/2401.02955)</code></li>
<li>Summary: <p>The CLIP and Segment Anything Model (SAM) are remarkable vision foundation
models (VFMs). SAM excels in segmentation tasks across diverse domains, while
CLIP is renowned for its zero-shot recognition capabilities. This paper
presents an in-depth exploration of integrating these two models into a unified
framework. Specifically, we introduce the Open-Vocabulary SAM, a SAM-inspired
model designed for simultaneous interactive segmentation and recognition,
leveraging two unique knowledge transfer modules: SAM2CLIP and CLIP2SAM. The
former adapts SAM's knowledge into the CLIP via distillation and learnable
transformer adapters, while the latter transfers CLIP knowledge into SAM,
enhancing its recognition capabilities. Extensive experiments on various
datasets and detectors show the effectiveness of Open-Vocabulary SAM in both
segmentation and recognition tasks, significantly outperforming the naive
baselines of simply combining SAM and CLIP. Furthermore, aided with image
classification data training, our method can segment and recognize
approximately 22,000 classes.
</p></li>
</ul>

<h3>Title: Denoising Vision Transformers. (arXiv:2401.02957v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02957">http://arxiv.org/abs/2401.02957</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02957]] Denoising Vision Transformers(http://arxiv.org/abs/2401.02957)</code></li>
<li>Summary: <p>We delve into a nuanced but significant challenge inherent to Vision
Transformers (ViTs): feature maps of these models exhibit grid-like artifacts,
which detrimentally hurt the performance of ViTs in downstream tasks. Our
investigations trace this fundamental issue down to the positional embeddings
at the input stage. To address this, we propose a novel noise model, which is
universally applicable to all ViTs. Specifically, the noise model dissects ViT
outputs into three components: a semantics term free from noise artifacts and
two artifact-related terms that are conditioned on pixel locations. Such a
decomposition is achieved by enforcing cross-view feature consistency with
neural fields in a per-image basis. This per-image optimization process
extracts artifact-free features from raw ViT outputs, providing clean features
for offline applications. Expanding the scope of our solution to support online
functionality, we introduce a learnable denoiser to predict artifact-free
features directly from unprocessed ViT outputs, which shows remarkable
generalization capabilities to novel data without the need for per-image
optimization. Our two-stage approach, termed Denoising Vision Transformers
(DVT), does not require re-training existing pre-trained ViTs and is
immediately applicable to any Transformer-based architecture. We evaluate our
method on a variety of representative ViTs (DINO, MAE, DeiT-III, EVA02, CLIP,
DINOv2, DINOv2-reg). Extensive evaluations demonstrate that our DVT
consistently and significantly improves existing state-of-the-art
general-purpose models in semantic and geometric tasks across multiple datasets
(e.g., +3.84 mIoU). We hope our study will encourage a re-evaluation of ViT
design, especially regarding the naive use of positional embeddings.
</p></li>
</ul>

<h3>Title: A Cost-Efficient FPGA Implementation of Tiny Transformer Model using Neural ODE. (arXiv:2401.02721v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02721">http://arxiv.org/abs/2401.02721</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02721]] A Cost-Efficient FPGA Implementation of Tiny Transformer Model using Neural ODE(http://arxiv.org/abs/2401.02721)</code></li>
<li>Summary: <p>Transformer is an emerging neural network model with attention mechanism. It
has been adopted to various tasks and achieved a favorable accuracy compared to
CNNs and RNNs. While the attention mechanism is recognized as a general-purpose
component, many of the Transformer models require a significant number of
parameters compared to the CNN-based ones. To mitigate the computational
complexity, recently, a hybrid approach has been proposed, which uses ResNet as
a backbone architecture and replaces a part of its convolution layers with an
MHSA (Multi-Head Self-Attention) mechanism. In this paper, we significantly
reduce the parameter size of such models by using Neural ODE (Ordinary
Differential Equation) as a backbone architecture instead of ResNet. The
proposed hybrid model reduces the parameter size by 94.6% compared to the
CNN-based ones without degrading the accuracy. We then deploy the proposed
model on a modest-sized FPGA device for edge computing. To further reduce FPGA
resource utilization, we quantize the model following QAT (Quantization Aware
Training) scheme instead of PTQ (Post Training Quantization) to suppress the
accuracy loss. As a result, an extremely lightweight Transformer-based model
can be implemented on resource-limited FPGAs. The weights of the feature
extraction network are stored on-chip to minimize the memory transfer overhead,
allowing faster inference. By eliminating the overhead of memory transfers,
inference can be executed seamlessly, leading to accelerated inference. The
proposed FPGA implementation achieves 12.8x speedup and 9.21x energy efficiency
compared to ARM Cortex-A53 CPU.
</p></li>
</ul>

<h3>Title: Powerformer: A Section-adaptive Transformer for Power Flow Adjustment. (arXiv:2401.02771v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02771">http://arxiv.org/abs/2401.02771</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02771]] Powerformer: A Section-adaptive Transformer for Power Flow Adjustment(http://arxiv.org/abs/2401.02771)</code></li>
<li>Summary: <p>In this paper, we present a novel transformer architecture tailored for
learning robust power system state representations, which strives to optimize
power dispatch for the power flow adjustment across different transmission
sections. Specifically, our proposed approach, named Powerformer, develops a
dedicated section-adaptive attention mechanism, separating itself from the
self-attention used in conventional transformers. This mechanism effectively
integrates power system states with transmission section information, which
facilitates the development of robust state representations. Furthermore, by
considering the graph topology of power system and the electrical attributes of
bus nodes, we introduce two customized strategies to further enhance the
expressiveness: graph neural network propagation and multi-factor attention
mechanism. Extensive evaluations are conducted on three power system scenarios,
including the IEEE 118-bus system, a realistic 300-bus system in China, and a
large-scale European system with 9241 buses, where Powerformer demonstrates its
superior performance over several baseline methods.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual Learning in Decision Making. (arXiv:2401.02576v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02576">http://arxiv.org/abs/2401.02576</a></li>
<li>Code URL: <a href="https://github.com/williamyue37/t-dgr">https://github.com/williamyue37/t-dgr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02576]] t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual Learning in Decision Making(http://arxiv.org/abs/2401.02576)</code></li>
<li>Summary: <p>Deep generative replay has emerged as a promising approach for continual
learning in decision-making tasks. This approach addresses the problem of
catastrophic forgetting by leveraging the generation of trajectories from
previously encountered tasks to augment the current dataset. However, existing
deep generative replay methods for continual learning rely on autoregressive
models, which suffer from compounding errors in the generated trajectories. In
this paper, we propose a simple, scalable, and non-autoregressive method for
continual learning in decision-making tasks using a generative model that
generates task samples conditioned on the trajectory timestep. We evaluate our
method on Continual World benchmarks and find that our approach achieves
state-of-the-art performance on the average success rate metric among continual
learning methods. Code is available at https://github.com/WilliamYue37/t-DGR .
</p></li>
</ul>

<h3>Title: H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses. (arXiv:2401.02905v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02905">http://arxiv.org/abs/2401.02905</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02905]] H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses(http://arxiv.org/abs/2401.02905)</code></li>
<li>Summary: <p>Discovering human cognitive and emotional states using multi-modal
physiological signals draws attention across various research applications.
Physiological responses of the human body are influenced by human cognition and
commonly used to analyze cognitive states. From a network science perspective,
the interactions of these heterogeneous physiological modalities in a graph
structure may provide insightful information to support prediction of cognitive
states. However, there is no clue to derive exact connectivity between
heterogeneous modalities and there exists a hierarchical structure of
sub-modalities. Existing graph neural networks are designed to learn on
non-hierarchical homogeneous graphs with pre-defined graph structures; they
failed to learn from hierarchical, multi-modal physiological data without a
pre-defined graph structure. To this end, we propose a hierarchical
heterogeneous graph generative network (H2G2-Net) that automatically learns a
graph structure without domain knowledge, as well as a powerful representation
on the hierarchical heterogeneous graph in an end-to-end fashion. We validate
the proposed method on the CogPilot dataset that consists of multi-modal
physiological signals. Extensive experiments demonstrate that our proposed
method outperforms the state-of-the-art GNNs by 5%-20% in prediction accuracy.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. (arXiv:2401.02777v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02777">http://arxiv.org/abs/2401.02777</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02777]] From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models(http://arxiv.org/abs/2401.02777)</code></li>
<li>Summary: <p>This paper introduces RAISE (Reasoning and Acting through Scratchpad and
Examples), an advanced architecture enhancing the integration of Large Language
Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of
the ReAct framework, incorporates a dual-component memory system, mirroring
human short-term and long-term memory, to maintain context and continuity in
conversations. It entails a comprehensive agent construction scenario,
including phases like Conversation Selection, Scene Extraction, CoT Completion,
and Scene Augmentation, leading to the LLMs Training phase. This approach
appears to enhance agent controllability and adaptability in complex,
multi-turn dialogues. Our preliminary evaluations in a real estate sales
context suggest that RAISE has some advantages over traditional agents,
indicating its potential for broader applications. This work contributes to the
AI field by providing a robust framework for developing more context-aware and
versatile conversational agents.
</p></li>
</ul>

<h3>Title: PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering. (arXiv:2401.02797v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02797">http://arxiv.org/abs/2401.02797</a></li>
<li>Code URL: <a href="https://github.com/jinlhe/pefomed">https://github.com/jinlhe/pefomed</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02797]] PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering(http://arxiv.org/abs/2401.02797)</code></li>
<li>Summary: <p>Multimodal large language models (MLLMs) represent an evolutionary expansion
in the capabilities of traditional large language models, enabling them to
tackle challenges that surpass the scope of purely text-based applications. It
leverages the knowledge previously encoded within these language models,
thereby enhancing their applicability and functionality in the reign of
multimodal contexts. Recent works investigate the adaptation of MLLMs to
predict free-form answers as a generative task to solve medical visual question
answering (Med-VQA) tasks. In this paper, we propose a parameter efficient
framework for fine-tuning MLLM specifically tailored to Med-VQA applications,
and empirically validate it on a public benchmark dataset. To accurately
measure the performance, we employ human evaluation and the results reveal that
our model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v
model by a significant margin of 26% absolute accuracy on closed-ended
questions. The code will be available here: https://github.com/jinlHe/PeFoMed.
</p></li>
</ul>

<h3>Title: Introducing Bode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based Task. (arXiv:2401.02909v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02909">http://arxiv.org/abs/2401.02909</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02909]] Introducing Bode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based Task(http://arxiv.org/abs/2401.02909)</code></li>
<li>Summary: <p>Large Language Models (LLMs) are increasingly bringing advances to Natural
Language Processing. However, low-resource languages, those lacking extensive
prominence in datasets for various NLP tasks, or where existing datasets are
not as substantial, such as Portuguese, already obtain several benefits from
LLMs, but not to the same extent. LLMs trained on multilingual datasets
normally struggle to respond to prompts in Portuguese satisfactorily,
presenting, for example, code switching in their responses. This work proposes
a fine-tuned LLaMA 2-based model for Portuguese prompts named Bode in two
versions: 7B and 13B. We evaluate the performance of this model in
classification tasks using the zero-shot approach with in-context learning, and
compare it with other LLMs. Our main contribution is to bring an LLM with
satisfactory results in the Portuguese language, as well as to provide a model
that is free for research or commercial purposes.
</p></li>
</ul>

<h3>Title: Fast and Optimal Weight Update for Pruned Large Language Models. (arXiv:2401.02938v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02938">http://arxiv.org/abs/2401.02938</a></li>
<li>Code URL: <a href="https://github.com/fmfi-compbio/admm-pruning">https://github.com/fmfi-compbio/admm-pruning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02938]] Fast and Optimal Weight Update for Pruned Large Language Models(http://arxiv.org/abs/2401.02938)</code></li>
<li>Summary: <p>Pruning large language models (LLMs) is a challenging task due to their
enormous size. The primary difficulty is fine-tuning the model after pruning,
which is needed to recover the lost performance caused by dropping weights.
Recent approaches have either ignored fine-tuning entirely, focusing on
efficient pruning criteria, or attempted layer-wise weight updates, preserving
the behavior of each layer. However, even layer-wise weight updates can be
costly for LLMs, and previous works have resorted to various approximations.
</p>
<p>In our paper, we propose a fast and optimal weight update algorithm for
pruned layers based on the Alternating Direction Method of Multipliers (ADMM).
Coupled with a simple iterative pruning mask selection, our algorithm achieves
state-of-the-art pruning performance across a wide range of LLMs. Code is
available at https://github.com/fmfi-compbio/admm-pruning.
</p></li>
</ul>

<h3>Title: DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. (arXiv:2401.02954v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02954">http://arxiv.org/abs/2401.02954</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02954]] DeepSeek LLM: Scaling Open-Source Language Models with Longtermism(http://arxiv.org/abs/2401.02954)</code></li>
<li>Summary: <p>The rapid development of open-source large language models (LLMs) has been
truly remarkable. However, the scaling law described in previous literature
presents varying conclusions, which casts a dark cloud over scaling LLMs. We
delve into the study of scaling laws and present our distinctive findings that
facilitate scaling of large scale models in two commonly used open-source
configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek
LLM, a project dedicated to advancing open-source language models with a
long-term perspective. To support the pre-training phase, we have developed a
dataset that currently consists of 2 trillion tokens and is continuously
expanding. We further conduct supervised fine-tuning (SFT) and Direct
Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the
creation of DeepSeek Chat models. Our evaluation results demonstrate that
DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in
the domains of code, mathematics, and reasoning. Furthermore, open-ended
evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance
compared to GPT-3.5.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: DHGCN: Dynamic Hop Graph Convolution Network for Self-supervised Point Cloud Learning. (arXiv:2401.02610v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02610">http://arxiv.org/abs/2401.02610</a></li>
<li>Code URL: <a href="https://github.com/jinec98/dhgcn">https://github.com/jinec98/dhgcn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02610]] DHGCN: Dynamic Hop Graph Convolution Network for Self-supervised Point Cloud Learning(http://arxiv.org/abs/2401.02610)</code></li>
<li>Summary: <p>Recent works attempt to extend Graph Convolution Networks (GCNs) to point
clouds for classification and segmentation tasks. These works tend to sample
and group points to create smaller point sets locally and mainly focus on
extracting local features through GCNs, while ignoring the relationship between
point sets. In this paper, we propose the Dynamic Hop Graph Convolution Network
(DHGCN) for explicitly learning the contextual relationships between the
voxelized point parts, which are treated as graph nodes. Motivated by the
intuition that the contextual information between point parts lies in the
pairwise adjacent relationship, which can be depicted by the hop distance of
the graph quantitatively, we devise a novel self-supervised part-level hop
distance reconstruction task and design a novel loss function accordingly to
facilitate training. In addition, we propose the Hop Graph Attention (HGA),
which takes the learned hop distance as input for producing attention weights
to allow edge features to contribute distinctively in aggregation. Eventually,
the proposed DHGCN is a plug-and-play module that is compatible with
point-based backbone networks. Comprehensive experiments on different backbones
and tasks demonstrate that our self-supervised method achieves state-of-the-art
performance. Our source code is available at: https://github.com/Jinec98/DHGCN.
</p></li>
</ul>

<h3>Title: Complementary Information Mutual Learning for Multimodality Medical Image Segmentation. (arXiv:2401.02717v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02717">http://arxiv.org/abs/2401.02717</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02717]] Complementary Information Mutual Learning for Multimodality Medical Image Segmentation(http://arxiv.org/abs/2401.02717)</code></li>
<li>Summary: <p>Radiologists must utilize multiple modal images for tumor segmentation and
diagnosis due to the limitations of medical imaging and the diversity of tumor
signals. This leads to the development of multimodal learning in segmentation.
However, the redundancy among modalities creates challenges for existing
subtraction-based joint learning methods, such as misjudging the importance of
modalities, ignoring specific modal information, and increasing cognitive load.
These thorny issues ultimately decrease segmentation accuracy and increase the
risk of overfitting. This paper presents the complementary information mutual
learning (CIML) framework, which can mathematically model and address the
negative impact of inter-modal redundant information. CIML adopts the idea of
addition and removes inter-modal redundant information through inductive
bias-driven task decomposition and message passing-based redundancy filtering.
CIML first decomposes the multimodal segmentation task into multiple subtasks
based on expert prior knowledge, minimizing the information dependence between
modalities. Furthermore, CIML introduces a scheme in which each modality can
extract information from other modalities additively through message passing.
To achieve non-redundancy of extracted information, the redundant filtering is
transformed into complementary information learning inspired by the variational
information bottleneck. The complementary information learning procedure can be
efficiently solved by variational inference and cross-modal spatial attention.
Numerical results from the verification task and standard benchmarks indicate
that CIML efficiently removes redundant information between modalities,
outperforming SOTA methods regarding validation accuracy and segmentation
effect.
</p></li>
</ul>

<h3>Title: Systematic review of image segmentation using complex networks. (arXiv:2401.02758v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02758">http://arxiv.org/abs/2401.02758</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02758]] Systematic review of image segmentation using complex networks(http://arxiv.org/abs/2401.02758)</code></li>
<li>Summary: <p>This review presents various image segmentation methods using complex
networks.
</p>
<p>Image segmentation is one of the important steps in image analysis as it
helps analyze and understand complex images. At first, it has been tried to
classify complex networks based on how it being used in image segmentation.
</p>
<p>In computer vision and image processing applications, image segmentation is
essential for analyzing complex images with irregular shapes, textures, or
overlapping boundaries. Advanced algorithms make use of machine learning,
clustering, edge detection, and region-growing techniques. Graph theory
principles combined with community detection-based methods allow for more
precise analysis and interpretation of complex images. Hybrid approaches
combine multiple techniques for comprehensive, robust segmentation, improving
results in computer vision and image processing tasks.
</p></li>
</ul>

<h3>Title: A comprehensive survey of research towards AI-enabled unmanned aerial systems in pre-, active-, and post-wildfire management. (arXiv:2401.02456v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02456">http://arxiv.org/abs/2401.02456</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02456]] A comprehensive survey of research towards AI-enabled unmanned aerial systems in pre-, active-, and post-wildfire management(http://arxiv.org/abs/2401.02456)</code></li>
<li>Summary: <p>Wildfires have emerged as one of the most destructive natural disasters
worldwide, causing catastrophic losses in both human lives and forest wildlife.
Recently, the use of Artificial Intelligence (AI) in wildfires, propelled by
the integration of Unmanned Aerial Vehicles (UAVs) and deep learning models,
has created an unprecedented momentum to implement and develop more effective
wildfire management. Although some of the existing survey papers have explored
various learning-based approaches, a comprehensive review emphasizing the
application of AI-enabled UAV systems and their subsequent impact on
multi-stage wildfire management is notably lacking. This survey aims to bridge
these gaps by offering a systematic review of the recent state-of-the-art
technologies, highlighting the advancements of UAV systems and AI models from
pre-fire, through the active-fire stage, to post-fire management. To this aim,
we provide an extensive analysis of the existing remote sensing systems with a
particular focus on the UAV advancements, device specifications, and sensor
technologies relevant to wildfire management. We also examine the pre-fire and
post-fire management approaches, including fuel monitoring, prevention
strategies, as well as evacuation planning, damage assessment, and operation
strategies. Additionally, we review and summarize a wide range of computer
vision techniques in active-fire management, with an emphasis on Machine
Learning (ML), Reinforcement Learning (RL), and Deep Learning (DL) algorithms
for wildfire classification, segmentation, detection, and monitoring tasks.
Ultimately, we underscore the substantial advancement in wildfire modeling
through the integration of cutting-edge AI techniques and UAV-based data,
providing novel insights and enhanced predictive capabilities to understand
dynamic wildfire behavior.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
