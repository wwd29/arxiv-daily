<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-10</h1>
<h2>secure</h2>
<h3>Title: Secure Synthesis of Distributed Cryptographic Applications (Technical Report). (arXiv:2401.04131v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04131">http://arxiv.org/abs/2401.04131</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04131]] Secure Synthesis of Distributed Cryptographic Applications (Technical Report)(http://arxiv.org/abs/2401.04131)</code></li>
<li>Summary: <p>Developing secure distributed systems is difficult, and even harder when
advanced cryptography must be used to achieve security goals. Following prior
work, we advocate using secure program partitioning to synthesize cryptographic
applications: instead of implementing a system of communicating processes, the
programmer implements a centralized, sequential program, which is automatically
compiled into a secure distributed version that uses cryptography.
</p>
<p>While this approach is promising, formal results for the security of such
compilers are limited in scope. In particular, no security proof yet
simultaneously addresses subtleties essential for robust, efficient
applications: multiple cryptographic mechanisms, malicious corruption, and
asynchronous communication.
</p>
<p>In this work, we develop a compiler security proof that handles these
subtleties. Our proof relies on a novel unification of simulation-based
security, information-flow control, choreographic programming, and
sequentialization techniques for concurrent programs. While our proof targets
hybrid protocols, which abstract cryptographic mechanisms as idealized
functionalities, our approach offers a clear path toward leveraging Universal
Composability to obtain end-to-end, modular security results with fully
instantiated cryptographic mechanisms.
</p>
<p>Finally, following prior observations about simulation-based security, we
prove that our result guarantees robust hyperproperty preservation, an
important criterion for compiler correctness that preserves all source-level
security properties in target programs.
</p></li>
</ul>

<h3>Title: Towards Remotely Verifiable Software Integrity in Resource-Constrained IoT Devices. (arXiv:2401.04308v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04308">http://arxiv.org/abs/2401.04308</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04308]] Towards Remotely Verifiable Software Integrity in Resource-Constrained IoT Devices(http://arxiv.org/abs/2401.04308)</code></li>
<li>Summary: <p>Lower-end IoT devices typically have strict cost constraints that rule out
usual security mechanisms available in general-purpose computers or higher-end
devices. To secure low-end devices, various low-cost security architectures
have been proposed for remote verification of their software state via
integrity proofs. These proofs vary in terms of expressiveness, with simpler
ones confirming correct binary presence, while more expressive ones support
verification of arbitrary code execution. This article provides a holistic and
systematic treatment of this family of architectures. It also compares
(qualitatively and quantitatively) the types of software integrity proofs,
respective architectural support, and associated costs. Finally, we outline
some research directions and emerging challenges.
</p></li>
</ul>

<h3>Title: Differential experiments using parallel alternative operations. (arXiv:2401.04495v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04495">http://arxiv.org/abs/2401.04495</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04495]] Differential experiments using parallel alternative operations(http://arxiv.org/abs/2401.04495)</code></li>
<li>Summary: <p>The use of alternative operations in differential cryptanalysis, or
alternative notions of differentials, are lately receiving increasing
attention. Recently, Civino et al. managed to design a block cipher which is
secure w.r.t. classical differential cryptanalysis performed using
XOR-differentials, but weaker with respect to the attack based on an
alternative difference operation acting on the first s-box of the block. We
extend this result to parallel alternative operations, i.e. acting on each
s-box of the block. First, we recall the mathematical framework needed to
define and use such operations. After that, we perform some differential
experiments against a toy cipher and compare the effectiveness of the attack
w.r.t. the one that uses XOR-differentials.
</p></li>
</ul>

<h3>Title: RNA-TransCrypt: Image Encryption Using Chaotic RNA Encoding, Novel Transformative Substitution, and Tailored Cryptographic Operations. (arXiv:2401.04707v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04707">http://arxiv.org/abs/2401.04707</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04707]] RNA-TransCrypt: Image Encryption Using Chaotic RNA Encoding, Novel Transformative Substitution, and Tailored Cryptographic Operations(http://arxiv.org/abs/2401.04707)</code></li>
<li>Summary: <p>Given the security concerns of Internet of Things (IoT) networks and limited
computational resources of IoT devices, this paper presents RNA-TransCrypt, a
novel image encryption scheme that is not only highly secure but also efficient
and lightweight. RNA-TransCrypt integrates the biocryptographic properties of
RNA encoding with the non-linearity and unpredictability of chaos theory. This
scheme introduces three novel contributions: 1) the two-base RNA encoding
method, which transforms the image into RNA strands-like sequence, ensuring
efficient scrambling; 2) the transformative substitution technique, which
transforms the s-box values before replacing the pixel values, and is
responsible for making the scheme lightweight; and 3) three mathematical
cryptographic operations designed especially for image encryption that ensure
the effective transformation of the s-box values, resulting in a new outcome
even for the same input values. These modules are key-dependent, utilizing
chaotic keys generated by the De Jong Fractal Map and the Van der Pol
Oscillator. Extensive security analysis, including histogram analysis,
correlation analysis, and the results of the statistical security parameters
obtained from the Gray-Level Co-occurrence Matrix (GLCM) validate the efficacy
of the proposed scheme in encrypting input images with close-to-ideal results
of 7.997 entropy and 0.0006 correlation.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Detecting Face Synthesis Using a Concealed Fusion Model. (arXiv:2401.04257v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04257">http://arxiv.org/abs/2401.04257</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04257]] Detecting Face Synthesis Using a Concealed Fusion Model(http://arxiv.org/abs/2401.04257)</code></li>
<li>Summary: <p>Face image synthesis is gaining more attention in computer security due to
concerns about its potential negative impacts, including those related to fake
biometrics. Hence, building models that can detect the synthesized face images
is an important challenge to tackle. In this paper, we propose a fusion-based
strategy to detect face image synthesis while providing resiliency to several
attacks. The proposed strategy uses a late fusion of the outputs computed by
several undisclosed models by relying on random polynomial coefficients and
exponents to conceal a new feature space. Unlike existing concealing solutions,
our strategy requires no quantization, which helps to preserve the feature
space. Our experiments reveal that our strategy achieves state-of-the-art
performance while providing protection against poisoning, perturbation,
backdoor, and reverse model attacks.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Machine unlearning through fine-grained model parameters perturbation. (arXiv:2401.04385v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04385">http://arxiv.org/abs/2401.04385</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04385]] Machine unlearning through fine-grained model parameters perturbation(http://arxiv.org/abs/2401.04385)</code></li>
<li>Summary: <p>Machine unlearning techniques, which involve retracting data records and
reducing influence of said data on trained models, help with the user privacy
protection objective but incur significant computational costs. Weight
perturbation-based unlearning is a general approach, but it typically involves
globally modifying the parameters. We propose fine-grained Top-K and Random-k
parameters perturbed inexact machine unlearning strategies that address the
privacy needs while keeping the computational costs tractable.
</p>
<p>In order to demonstrate the efficacy of our strategies we also tackle the
challenge of evaluating the effectiveness of machine unlearning by considering
the model's generalization performance across both unlearning and remaining
data. To better assess the unlearning effect and model generalization, we
propose novel metrics, namely, the forgetting rate and memory retention rate.
However, for inexact machine unlearning, current metrics are inadequate in
quantifying the degree of forgetting that occurs after unlearning strategies
are applied. To address this, we introduce SPD-GAN, which subtly perturbs the
distribution of data targeted for unlearning. Then, we evaluate the degree of
unlearning by measuring the performance difference of the models on the
perturbed unlearning data before and after the unlearning process. By
implementing these innovative techniques and metrics, we achieve
computationally efficacious privacy protection in machine learning applications
without significant sacrifice of model performance. Furthermore, this approach
provides a novel method for evaluating the degree of unlearning.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: AI-based Mapping of the Conservation Status of Orchid Assemblages at Global Scale. (arXiv:2401.04691v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04691">http://arxiv.org/abs/2401.04691</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04691]] AI-based Mapping of the Conservation Status of Orchid Assemblages at Global Scale(http://arxiv.org/abs/2401.04691)</code></li>
<li>Summary: <p>Although increasing threats on biodiversity are now widely recognised, there
are no accurate global maps showing whether and where species assemblages are
at risk. We hereby assess and map at kilometre resolution the conservation
status of the iconic orchid family, and discuss the insights conveyed at
multiple scales. We introduce a new Deep Species Distribution Model trained on
1M occurrences of 14K orchid species to predict their assemblages at global
scale and at kilometre resolution. We propose two main indicators of the
conservation status of the assemblages: (i) the proportion of threatened
species, and (ii) the status of the most threatened species in the assemblage.
We show and analyze the variation of these indicators at World scale and in
relation to currently protected areas in Sumatra island. Global and interactive
maps available online show the indicators of conservation status of orchid
assemblages, with sharp spatial variations at all scales. The highest level of
threat is found at Madagascar and the neighbouring islands. In Sumatra, we
found good correspondence of protected areas with our indicators, but
supplementing current IUCN assessments with status predictions results in
alarming levels of species threat across the island. Recent advances in deep
learning enable reliable mapping of the conservation status of species
assemblages on a global scale. As an umbrella taxon, orchid family provides a
reference for identifying vulnerable ecosystems worldwide, and prioritising
conservation actions both at international and local levels.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: SoK: Facial Deepfake Detectors. (arXiv:2401.04364v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04364">http://arxiv.org/abs/2401.04364</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04364]] SoK: Facial Deepfake Detectors(http://arxiv.org/abs/2401.04364)</code></li>
<li>Summary: <p>Deepfakes have rapidly emerged as a profound and serious threat to society,
primarily due to their ease of creation and dissemination. This situation has
triggered an accelerated development of deepfake detection technologies.
However, many existing detectors rely heavily on lab-generated datasets for
validation, which may not effectively prepare them for novel, emerging, and
real-world deepfake techniques. In this paper, we conduct an extensive and
comprehensive review and analysis of the latest state-of-the-art deepfake
detectors, evaluating them against several critical criteria. These criteria
facilitate the categorization of these detectors into 4 high-level groups and
13 fine-grained sub-groups, all aligned with a unified standard conceptual
framework. This classification and framework offer deep and practical insights
into the factors that affect detector efficacy. We assess the generalizability
of 16 leading detectors across various standard attack scenarios, including
black-box, white-box, and gray-box settings. Our systematized analysis and
experimentation lay the groundwork for a deeper understanding of deepfake
detectors and their generalizability, paving the way for future research
focused on creating detectors adept at countering various attack scenarios.
Additionally, this work offers insights for developing more proactive defenses
against deepfakes.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Revisiting Adversarial Training at Scale. (arXiv:2401.04727v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04727">http://arxiv.org/abs/2401.04727</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04727]] Revisiting Adversarial Training at Scale(http://arxiv.org/abs/2401.04727)</code></li>
<li>Summary: <p>The machine learning community has witnessed a drastic change in the training
pipeline, pivoted by those ''foundation models'' with unprecedented scales.
However, the field of adversarial training is lagging behind, predominantly
centered around small model sizes like ResNet-50, and tiny and low-resolution
datasets like CIFAR-10. To bridge this transformation gap, this paper provides
a modern re-examination with adversarial training, investigating its potential
benefits when applied at scale. Additionally, we introduce an efficient and
effective training strategy to enable adversarial training with giant models
and web-scale data at an affordable computing cost. We denote this newly
introduced framework as AdvXL.
</p>
<p>Empirical results demonstrate that AdvXL establishes new state-of-the-art
robust accuracy records under AutoAttack on ImageNet-1K. For example, by
training on DataComp-1B dataset, our AdvXL empowers a vanilla ViT-g model to
substantially surpass the previous records of $l_{\infty}$-, $l_{2}$-, and
$l_{1}$-robust accuracy by margins of 11.4%, 14.2% and 12.9%, respectively.
This achievement posits AdvXL as a pioneering approach, charting a new
trajectory for the efficient training of robust visual representations at
significantly larger scales. Our code is available at
https://github.com/UCSC-VLAA/AdvXL.
</p></li>
</ul>

<h3>Title: WebGPU-SPY: Finding Fingerprints in the Sandbox through GPU Cache Attacks. (arXiv:2401.04349v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04349">http://arxiv.org/abs/2401.04349</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04349]] WebGPU-SPY: Finding Fingerprints in the Sandbox through GPU Cache Attacks(http://arxiv.org/abs/2401.04349)</code></li>
<li>Summary: <p>Microarchitectural attacks on CPU structures have been studied in native
applications, as well as in web browsers. These attacks continue to be a
substantial threat to computing systems at all scales.
</p>
<p>With the proliferation of heterogeneous systems and integration of hardware
accelerators in every computing system, modern web browsers provide the support
of GPU-based acceleration for the graphics and rendering processes. Emerging
web standards also support the GPU acceleration of general-purpose computation
within web browsers.
</p>
<p>In this paper, we present a new attack vector for microarchitectural attacks
in web browsers. We use emerging GPU accelerating APIs in modern browsers
(specifically WebGPU) to launch a GPU-based cache side channel attack on the
compute stack of the GPU that spies on victim activities on the graphics
(rendering) stack of the GPU. Unlike prior works that rely on JavaScript APIs
or software interfaces to build timing primitives, we build the timer using GPU
hardware resources and develop a cache side channel attack on Intel's
integrated GPUs. We leverage the GPU's inherent parallelism at different levels
to develop high-resolution parallel attacks. We demonstrate that GPU-based
cache attacks can achieve a precision of 90 for website fingerprinting of 100
top websites. We also discuss potential countermeasures against the proposed
attack to secure the systems at a critical time when these web standards are
being developed and before they are widely deployed.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Robust Image Watermarking using Stable Diffusion. (arXiv:2401.04247v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04247">http://arxiv.org/abs/2401.04247</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04247]] Robust Image Watermarking using Stable Diffusion(http://arxiv.org/abs/2401.04247)</code></li>
<li>Summary: <p>Watermarking images is critical for tracking image provenance and claiming
ownership. With the advent of generative models, such as stable diffusion, able
to create fake but realistic images, watermarking has become particularly
important, e.g., to make generated images reliably identifiable. Unfortunately,
the very same stable diffusion technology can remove watermarks injected using
existing methods. To address this problem, we present a ZoDiac, which uses a
pre-trained stable diffusion model to inject a watermark into the trainable
latent space, resulting in watermarks that can be reliably detected in the
latent vector, even when attacked. We evaluate ZoDiac on three benchmarks,
MS-COCO, DiffusionDB, and WikiArt, and find that ZoDiac is robust against
state-of-the-art watermark attacks, with a watermark detection rate over 98%
and a false positive rate below 6.4%, outperforming state-of-the-art
watermarking methods. Our research demonstrates that stable diffusion is a
promising approach to robust watermarking, able to withstand even
stable-diffusion-based attacks.
</p></li>
</ul>

<h3>Title: RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale. (arXiv:2401.04325v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04325">http://arxiv.org/abs/2401.04325</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04325]] RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale(http://arxiv.org/abs/2401.04325)</code></li>
<li>Summary: <p>We present a novel approach for metric dense depth estimation based on the
fusion of a single-view image and a sparse, noisy Radar point cloud. The direct
fusion of heterogeneous Radar and image data, or their encodings, tends to
yield dense depth maps with significant artifacts, blurred boundaries, and
suboptimal accuracy. To circumvent this issue, we learn to augment versatile
and robust monocular depth prediction with the dense metric scale induced from
sparse and noisy Radar data. We propose a Radar-Camera framework for highly
accurate and fine-detailed dense depth estimation with four stages, including
monocular depth prediction, global scale alignment of monocular depth with
sparse Radar points, quasi-dense scale estimation through learning the
association between Radar points and image patches, and local scale refinement
of dense depth using a scale map learner. Our proposed method significantly
outperforms the state-of-the-art Radar-Camera depth estimation methods by
reducing the mean absolute error (MAE) of depth estimation by 25.6% and 40.2%
on the challenging nuScenes dataset and our self-collected ZJU-4DRadarCam
dataset, respectively.
</p></li>
</ul>

<h3>Title: Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial Robustness. (arXiv:2401.04350v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04350">http://arxiv.org/abs/2401.04350</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04350]] Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial Robustness(http://arxiv.org/abs/2401.04350)</code></li>
<li>Summary: <p>Large-scale pre-trained vision-language models like CLIP have demonstrated
impressive performance across various tasks, and exhibit remarkable zero-shot
generalization capability, while they are also vulnerable to imperceptible
adversarial examples. Existing works typically employ adversarial training
(fine-tuning) as a defense method against adversarial examples. However, direct
application to the CLIP model may result in overfitting, compromising the
model's capacity for generalization. In this paper, we propose Pre-trained
Model Guided Adversarial Fine-Tuning (PMG-AFT) method, which leverages
supervision from the original pre-trained model by carefully designing an
auxiliary branch, to enhance the model's zero-shot adversarial robustness.
Specifically, PMG-AFT minimizes the distance between the features of
adversarial examples in the target model and those in the pre-trained model,
aiming to preserve the generalization features already captured by the
pre-trained model. Extensive Experiments on 15 zero-shot datasets demonstrate
that PMG-AFT significantly outperforms the state-of-the-art method, improving
the top-1 robust accuracy by an average of 4.99%. Furthermore, our approach
consistently improves clean accuracy by an average of 8.72%.
</p></li>
</ul>

<h3>Title: Generic Knowledge Boosted Pre-training For Remote Sensing Images. (arXiv:2401.04614v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04614">http://arxiv.org/abs/2401.04614</a></li>
<li>Code URL: <a href="https://github.com/floatingstarZ/GeRSP">https://github.com/floatingstarZ/GeRSP</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04614]] Generic Knowledge Boosted Pre-training For Remote Sensing Images(http://arxiv.org/abs/2401.04614)</code></li>
<li>Summary: <p>Deep learning models are essential for scene classification, change
detection, land cover segmentation, and other remote sensing image
understanding tasks. Most backbones of existing remote sensing deep learning
models are typically initialized by pre-trained weights obtained from ImageNet
pre-training (IMP). However, domain gaps exist between remote sensing images
and natural images (e.g., ImageNet), making deep learning models initialized by
pre-trained weights of IMP perform poorly for remote sensing image
understanding. Although some pre-training methods are studied in the remote
sensing community, current remote sensing pre-training methods face the problem
of vague generalization by only using remote sensing images. In this paper, we
propose a novel remote sensing pre-training framework, Generic Knowledge
Boosted Remote Sensing Pre-training (GeRSP), to learn robust representations
from remote sensing and natural images for remote sensing understanding tasks.
GeRSP contains two pre-training branches: (1) A self-supervised pre-training
branch is adopted to learn domain-related representations from unlabeled remote
sensing images. (2) A supervised pre-training branch is integrated into GeRSP
for general knowledge learning from labeled natural images. Moreover, GeRSP
combines two pre-training branches using a teacher-student architecture to
simultaneously learn representations with general and special knowledge, which
generates a powerful pre-trained model for deep learning model initialization.
Finally, we evaluate GeRSP and other remote sensing pre-training methods on
three downstream tasks, i.e., object detection, semantic segmentation, and
scene classification. The extensive experimental results consistently
demonstrate that GeRSP can effectively learn robust representations in a
unified manner, improving the performance of remote sensing downstream tasks.
</p></li>
</ul>

<h3>Title: CoordGate: Efficiently Computing Spatially-Varying Convolutions in Convolutional Neural Networks. (arXiv:2401.04680v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04680">http://arxiv.org/abs/2401.04680</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04680]] CoordGate: Efficiently Computing Spatially-Varying Convolutions in Convolutional Neural Networks(http://arxiv.org/abs/2401.04680)</code></li>
<li>Summary: <p>Optical imaging systems are inherently limited in their resolution due to the
point spread function (PSF), which applies a static, yet spatially-varying,
convolution to the image. This degradation can be addressed via Convolutional
Neural Networks (CNNs), particularly through deblurring techniques. However,
current solutions face certain limitations in efficiently computing
spatially-varying convolutions. In this paper we propose CoordGate, a novel
lightweight module that uses a multiplicative gate and a coordinate encoding
network to enable efficient computation of spatially-varying convolutions in
CNNs. CoordGate allows for selective amplification or attenuation of filters
based on their spatial position, effectively acting like a locally connected
neural network. The effectiveness of the CoordGate solution is demonstrated
within the context of U-Nets and applied to the challenging problem of image
deblurring. The experimental results show that CoordGate outperforms
conventional approaches, offering a more robust and spatially aware solution
for CNNs in various computer vision applications.
</p></li>
</ul>

<h3>Title: Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive Learning. (arXiv:2401.04361v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04361">http://arxiv.org/abs/2401.04361</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04361]] Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive Learning(http://arxiv.org/abs/2401.04361)</code></li>
<li>Summary: <p>Knowledge-grounded dialogue (KGD) learns to generate an informative response
based on a given dialogue context and external knowledge (\emph{e.g.},
knowledge graphs; KGs). Recently, the emergence of large language models (LLMs)
and pre-training techniques has brought great success to knowledge-grounded
dialogue. However, when building KGD systems in real applications, there are
various real-world noises that are inevitable to face. For example, the
dialogue context might involve perturbations such as misspellings and
abbreviations. In addition, KGs typically suffer from incompletion and also
might contain erroneous and outdated facts. Such real-world noises pose a
challenge to the robustness of KGD systems and hinder their applications in the
real world. In this paper, we propose an entity-based contrastive learning
framework for improving the robustness of KGD. Specifically, we make use of the
entity information in a KGD sample to create both its positive and negative
samples which involve semantic-irrelevant and semantic-relevant perturbations,
respectively. The contrastive learning framework ensures the KGD model is aware
of these two types of perturbations, thus generating informative responses with
the potentially noisy inputs in real applications. Experimental results on
three benchmark datasets show that our method achieves new state-of-the-art
performance in terms of automatic evaluation scores, verifying its
effectiveness and potentiality. Furthermore, we show that our method can
generate better responses than comparison models in both the noisy and the
few-shot settings.
</p></li>
</ul>

<h3>Title: Language Detection for Transliterated Content. (arXiv:2401.04619v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04619">http://arxiv.org/abs/2401.04619</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04619]] Language Detection for Transliterated Content(http://arxiv.org/abs/2401.04619)</code></li>
<li>Summary: <p>In the contemporary digital era, the Internet functions as an unparalleled
catalyst, dismantling geographical and linguistic barriers particularly evident
in texting. This evolution facilitates global communication, transcending
physical distances and fostering dynamic cultural exchange. A notable trend is
the widespread use of transliteration, where the English alphabet is employed
to convey messages in native languages, posing a unique challenge for language
technology in accurately detecting the source language. This paper addresses
this challenge through a dataset of phone text messages in Hindi and Russian
transliterated into English utilizing BERT for language classification and
Google Translate API for transliteration conversion. The research pioneers
innovative approaches to identify and convert transliterated text, navigating
challenges in the diverse linguistic landscape of digital communication.
Emphasizing the pivotal role of comprehensive datasets for training Large
Language Models LLMs like BERT, our model showcases exceptional proficiency in
accurately identifying and classifying languages from transliterated text. With
a validation accuracy of 99% our models robust performance underscores its
reliability. The comprehensive exploration of transliteration dynamics
supported by innovative approaches and cutting edge technologies like BERT,
positions our research at the forefront of addressing unique challenges in the
linguistic landscape of digital communication. Beyond contributing to language
identification and transliteration capabilities this work holds promise for
applications in content moderation, analytics and fostering a globally
connected community engaged in meaningful dialogue.
</p></li>
</ul>

<h3>Title: RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04679">http://arxiv.org/abs/2401.04679</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04679]] RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation(http://arxiv.org/abs/2401.04679)</code></li>
<li>Summary: <p>We investigate parameter-efficient fine-tuning (PEFT) methods that can
provide good accuracy under limited computational and memory budgets in the
context of large language models (LLMs). We present a new PEFT method called
Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA)
that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components
on top of a set of fixed pretrained weights to efficiently approximate the
performance of a full-fine-tuning (FFT) solution. Across a series of
challenging generative tasks such as grade-school math and SQL query
generation, which require fine-tuning for good performance, we show that RoSA
outperforms both LoRA and pure sparse fine-tuning, at the same parameter
budget. We provide system support for RoSA to complement the training
algorithm, specifically in the form of sparse GPU kernels which enable memory-
and computationally-efficient training. Our code will be made available at
https://github.com/IST-DASLab/RoSA}{\texttt{https://github.com/IST-DASLab/RoSA
</p></li>
</ul>

<h3>Title: Private Truly-Everlasting Robust-Prediction. (arXiv:2401.04311v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04311">http://arxiv.org/abs/2401.04311</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04311]] Private Truly-Everlasting Robust-Prediction(http://arxiv.org/abs/2401.04311)</code></li>
<li>Summary: <p>Private Everlasting Prediction (PEP), recently introduced by Naor et al.
[2023], is a model for differentially private learning in which the learner
never publicly releases a hypothesis. Instead, it provides black-box access to
a "prediction oracle" that can predict the labels of an endless stream of
unlabeled examples drawn from the underlying distribution. Importantly, PEP
provides privacy both for the initial training set and for the endless stream
of classification queries. We present two conceptual modifications to the
definition of PEP, as well as new constructions exhibiting significant
improvements over prior work. Specifically,
</p>
<p>(1) Robustness: PEP only guarantees accuracy provided that all the
classification queries are drawn from the correct underlying distribution. A
few out-of-distribution queries might break the validity of the prediction
oracle for future queries, even for future queries which are sampled from the
correct distribution. We incorporate robustness against such poisoning attacks
into the definition of PEP, and show how to obtain it.
</p>
<p>(2) Dependence of the privacy parameter $\delta$ in the time horizon: We
present a relaxed privacy definition, suitable for PEP, that allows us to
disconnect the privacy parameter $\delta$ from the number of total time steps
$T$. This allows us to obtain algorithms for PEP whose sample complexity is
independent from $T$, thereby making them "truly everlasting". This is in
contrast to prior work where the sample complexity grows with $polylog(T)$.
</p>
<p>(3) New constructions: Prior constructions for PEP exhibit sample complexity
that is quadratic in the VC dimension of the target class. We present new
constructions of PEP for axis-aligned rectangles and for decision-stumps that
exhibit sample complexity linear in the dimension (instead of quadratic). We
show that our constructions satisfy very strong robustness properties.
</p></li>
</ul>

<h3>Title: Robust Calibration For Improved Weather Prediction Under Distributional Shift. (arXiv:2401.04144v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04144">http://arxiv.org/abs/2401.04144</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04144]] Robust Calibration For Improved Weather Prediction Under Distributional Shift(http://arxiv.org/abs/2401.04144)</code></li>
<li>Summary: <p>In this paper, we present results on improving out-of-domain weather
prediction and uncertainty estimation as part of the \texttt{Shifts Challenge
on Robustness and Uncertainty under Real-World Distributional Shift} challenge.
We find that by leveraging a mixture of experts in conjunction with an advanced
data augmentation technique borrowed from the computer vision domain, in
conjunction with robust \textit{post-hoc} calibration of predictive
uncertainties, we can potentially achieve more accurate and better-calibrated
results with deep neural networks than with boosted tree models for tabular
data. We quantify our predictions using several metrics and propose several
future lines of inquiry and experimentation to boost performance.
</p></li>
</ul>

<h3>Title: Explaining the Power of Topological Data Analysis in Graph Machine Learning. (arXiv:2401.04250v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04250">http://arxiv.org/abs/2401.04250</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04250]] Explaining the Power of Topological Data Analysis in Graph Machine Learning(http://arxiv.org/abs/2401.04250)</code></li>
<li>Summary: <p>Topological Data Analysis (TDA) has been praised by researchers for its
ability to capture intricate shapes and structures within data. TDA is
considered robust in handling noisy and high-dimensional datasets, and its
interpretability is believed to promote an intuitive understanding of model
behavior. However, claims regarding the power and usefulness of TDA have only
been partially tested in application domains where TDA-based models are
compared to other graph machine learning approaches, such as graph neural
networks. We meticulously test claims on TDA through a comprehensive set of
experiments and validate their merits. Our results affirm TDA's robustness
against outliers and its interpretability, aligning with proponents' arguments.
However, we find that TDA does not significantly enhance the predictive power
of existing methods in our specific experiments, while incurring significant
computational costs. We investigate phenomena related to graph characteristics,
such as small diameters and high clustering coefficients, to mitigate the
computational expenses of TDA computations. Our results offer valuable
perspectives on integrating TDA into graph machine learning tasks.
</p></li>
</ul>

<h3>Title: Attention versus Contrastive Learning of Tabular Data -- A Data-centric Benchmarking. (arXiv:2401.04266v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04266">http://arxiv.org/abs/2401.04266</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04266]] Attention versus Contrastive Learning of Tabular Data -- A Data-centric Benchmarking(http://arxiv.org/abs/2401.04266)</code></li>
<li>Summary: <p>Despite groundbreaking success in image and text learning, deep learning has
not achieved significant improvements against traditional machine learning (ML)
when it comes to tabular data. This performance gap underscores the need for
data-centric treatment and benchmarking of learning algorithms. Recently,
attention and contrastive learning breakthroughs have shifted computer vision
and natural language processing paradigms. However, the effectiveness of these
advanced deep models on tabular data is sparsely studied using a few data sets
with very large sample sizes, reporting mixed findings after benchmarking
against a limited number of baselines. We argue that the heterogeneity of
tabular data sets and selective baselines in the literature can bias the
benchmarking outcomes. This article extensively evaluates state-of-the-art
attention and contrastive learning methods on a wide selection of 28 tabular
data sets (14 easy and 14 hard-to-classify) against traditional deep and
machine learning. Our data-centric benchmarking demonstrates when traditional
ML is preferred over deep learning and vice versa because no best learning
method exists for all tabular data sets. Combining between-sample and
between-feature attentions conquers the invincible traditional ML on tabular
data sets by a significant margin but fails on high dimensional data, where
contrastive learning takes a robust lead. While a hybrid attention-contrastive
learning strategy mostly wins on hard-to-classify data sets, traditional
methods are frequently superior on easy-to-classify data sets with presumably
simpler decision boundaries. To the best of our knowledge, this is the first
benchmarking paper with statistical analyses of attention and contrastive
learning performances on a diverse selection of tabular data sets against
traditional deep and machine learning baselines to facilitate further advances
in this field.
</p></li>
</ul>

<h3>Title: Coupling Graph Neural Networks with Fractional Order Continuous Dynamics: A Robustness Study. (arXiv:2401.04331v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04331">http://arxiv.org/abs/2401.04331</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04331]] Coupling Graph Neural Networks with Fractional Order Continuous Dynamics: A Robustness Study(http://arxiv.org/abs/2401.04331)</code></li>
<li>Summary: <p>In this work, we rigorously investigate the robustness of graph neural
fractional-order differential equation (FDE) models. This framework extends
beyond traditional graph neural (integer-order) ordinary differential equation
(ODE) models by implementing the time-fractional Caputo derivative. Utilizing
fractional calculus allows our model to consider long-term memory during the
feature updating process, diverging from the memoryless Markovian updates seen
in traditional graph neural ODE models. The superiority of graph neural FDE
models over graph neural ODE models has been established in environments free
from attacks or perturbations. While traditional graph neural ODE models have
been verified to possess a degree of stability and resilience in the presence
of adversarial attacks in existing literature, the robustness of graph neural
FDE models, especially under adversarial conditions, remains largely
unexplored. This paper undertakes a detailed assessment of the robustness of
graph neural FDE models. We establish a theoretical foundation outlining the
robustness characteristics of graph neural FDE models, highlighting that they
maintain more stringent output perturbation bounds in the face of input and
graph topology disturbances, compared to their integer-order counterparts. Our
empirical evaluations further confirm the enhanced robustness of graph neural
FDE models, highlighting their potential in adversarially robust applications.
</p></li>
</ul>

<h3>Title: Robust Imitation Learning for Automated Game Testing. (arXiv:2401.04572v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04572">http://arxiv.org/abs/2401.04572</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04572]] Robust Imitation Learning for Automated Game Testing(http://arxiv.org/abs/2401.04572)</code></li>
<li>Summary: <p>Game development is a long process that involves many stages before a product
is ready for the market. Human play testing is among the most time consuming,
as testers are required to repeatedly perform tasks in the search for errors in
the code. Therefore, automated testing is seen as a key technology for the
gaming industry, as it would dramatically improve development costs and
efficiency. Toward this end, we propose EVOLUTE, a novel imitation
learning-based architecture that combines behavioural cloning (BC) with energy
based models (EBMs). EVOLUTE is a two-stream ensemble model that splits the
action space of autonomous agents into continuous and discrete tasks. The EBM
stream handles the continuous tasks, to have a more refined and adaptive
control, while the BC stream handles discrete actions, to ease training. We
evaluate the performance of EVOLUTE in a shooting-and-driving game, where the
agent is required to navigate and continuously identify targets to attack. The
proposed model has higher generalisation capabilities than standard BC
approaches, showing a wider range of behaviours and higher performances. Also,
EVOLUTE is easier to train than a pure end-to-end EBM model, as discrete tasks
can be quite sparse in the dataset and cause model training to explore a much
wider set of possible actions while training.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Phase-shifted remote photoplethysmography for estimating heart rate and blood pressure from facial video. (arXiv:2401.04560v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04560">http://arxiv.org/abs/2401.04560</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04560]] Phase-shifted remote photoplethysmography for estimating heart rate and blood pressure from facial video(http://arxiv.org/abs/2401.04560)</code></li>
<li>Summary: <p>Human health can be critically affected by cardiovascular diseases, such as
hypertension, arrhythmias, and stroke. Heart rate and blood pressure are
important biometric information for the monitoring of cardiovascular system and
early diagnosis of cardiovascular diseases. Existing methods for estimating the
heart rate are based on electrocardiography and photoplethyomography, which
require contacting the sensor to the skin surface. Moreover, catheter and
cuff-based methods for measuring blood pressure cause inconvenience and have
limited applicability. Therefore, in this thesis, we propose a vision-based
method for estimating the heart rate and blood pressure. This thesis proposes a
2-stage deep learning framework consisting of a dual remote
photoplethysmography network (DRP-Net) and bounded blood pressure network
(BBP-Net). In the first stage, DRP-Net infers remote photoplethysmography
(rPPG) signals for the acral and facial regions, and these phase-shifted rPPG
signals are utilized to estimate the heart rate. In the second stage, BBP-Net
integrates temporal features and analyzes phase discrepancy between the acral
and facial rPPG signals to estimate SBP and DBP values. To improve the accuracy
of estimating the heart rate, we employed a data augmentation method based on a
frame interpolation model. Moreover, we designed BBP-Net to infer blood
pressure within a predefined range by incorporating a scaled sigmoid function.
Our method resulted in estimating the heart rate with the mean absolute error
(MAE) of 1.78 BPM, reducing the MAE by 34.31 % compared to the recent method,
on the MMSE-HR dataset. The MAE for estimating the systolic blood pressure
(SBP) and diastolic blood pressure (DBP) were 10.19 mmHg and 7.09 mmHg. On the
V4V dataset, the MAE for the heart rate, SBP, and DBP were 3.83 BPM, 13.64
mmHg, and 9.4 mmHg, respectively.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For Multi-Agent Environments. (arXiv:2401.04290v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04290">http://arxiv.org/abs/2401.04290</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04290]] StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For Multi-Agent Environments(http://arxiv.org/abs/2401.04290)</code></li>
<li>Summary: <p>Spatial reasoning tasks in multi-agent environments such as event prediction,
agent type identification, or missing data imputation are important for
multiple applications (e.g., autonomous surveillance over sensor networks and
subtasks for reinforcement learning (RL)). StarCraft II game replays encode
intelligent (and adversarial) multi-agent behavior and could provide a testbed
for these tasks; however, extracting simple and standardized representations
for prototyping these tasks is laborious and hinders reproducibility. In
contrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled
rapid prototyping and reproducibility of ML methods. Following the simplicity
of these datasets, we construct a benchmark spatial reasoning dataset based on
StarCraft II replays that exhibit complex multi-agent behaviors, while still
being as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize
a window of 255 consecutive game states to create 3.6 million summary images
from 60,000 replays, including all relevant metadata such as game outcome and
player races. We develop three formats of decreasing complexity: Hyperspectral
images that include one channel for every unit type (similar to multispectral
geospatial images), RGB images that mimic CIFAR10, and grayscale images that
mimic MNIST. We show how this dataset can be used for prototyping spatial
reasoning methods. All datasets, code for extraction, and code for dataset
loading can be found at https://starcraftdata.davidinouye.com
</p></li>
</ul>

<h3>Title: Representative Feature Extraction During Diffusion Process for Sketch Extraction with One Example. (arXiv:2401.04362v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04362">http://arxiv.org/abs/2401.04362</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04362]] Representative Feature Extraction During Diffusion Process for Sketch Extraction with One Example(http://arxiv.org/abs/2401.04362)</code></li>
<li>Summary: <p>We introduce DiffSketch, a method for generating a variety of stylized
sketches from images. Our approach focuses on selecting representative features
from the rich semantics of deep features within a pretrained diffusion model.
This novel sketch generation method can be trained with one manual drawing.
Furthermore, efficient sketch extraction is ensured by distilling a trained
generator into a streamlined extractor. We select denoising diffusion features
through analysis and integrate these selected features with VAE features to
produce sketches. Additionally, we propose a sampling scheme for training
models using a conditional generative approach. Through a series of
comparisons, we verify that distilled DiffSketch not only outperforms existing
state-of-the-art sketch extraction methods but also surpasses diffusion-based
stylization methods in the task of extracting sketches.
</p></li>
</ul>

<h3>Title: Empirical Analysis of Anomaly Detection on Hyperspectral Imaging Using Dimension Reduction Methods. (arXiv:2401.04437v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04437">http://arxiv.org/abs/2401.04437</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04437]] Empirical Analysis of Anomaly Detection on Hyperspectral Imaging Using Dimension Reduction Methods(http://arxiv.org/abs/2401.04437)</code></li>
<li>Summary: <p>Recent studies try to use hyperspectral imaging (HSI) to detect foreign
matters in products because it enables to visualize the invisible wavelengths
including ultraviolet and infrared. Considering the enormous image channels of
the HSI, several dimension reduction methods-e.g., PCA or UMAP-can be
considered to reduce but those cannot ease the fundamental limitations, as
follows: (1) latency of HSI capturing. (2) less explanation ability of the
important channels. In this paper, to circumvent the aforementioned methods,
one of the ways to channel reduction, on anomaly detection proposed HSI.
Different from feature extraction methods (i.e., PCA or UMAP), feature
selection can sort the feature by impact and show better explainability so we
might redesign the task-optimized and cost-effective spectroscopic camera. Via
the extensive experiment results with synthesized MVTec AD dataset, we confirm
that the feature selection method shows 6.90x faster at the inference phase
compared with feature extraction-based approaches while preserving anomaly
detection performance. Ultimately, we conclude the advantage of feature
selection which is effective yet fast.
</p></li>
</ul>

<h3>Title: Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding. (arXiv:2401.04398v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04398">http://arxiv.org/abs/2401.04398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04398]] Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding(http://arxiv.org/abs/2401.04398)</code></li>
<li>Summary: <p>Table-based reasoning with large language models (LLMs) is a promising
direction to tackle many table understanding tasks, such as table-based
question answering and fact verification. Compared with generic reasoning,
table-based reasoning requires the extraction of underlying semantics from both
free-form questions and semi-structured tabular data. Chain-of-Thought and its
similar approaches incorporate the reasoning chain in the form of textual
context, but it is still an open question how to effectively leverage tabular
data in the reasoning chain. We propose the Chain-of-Table framework, where
tabular data is explicitly used in the reasoning chain as a proxy for
intermediate thoughts. Specifically, we guide LLMs using in-context learning to
iteratively generate operations and update the table to represent a tabular
reasoning chain. LLMs can therefore dynamically plan the next operation based
on the results of the previous ones. This continuous evolution of the table
forms a chain, showing the reasoning process for a given tabular problem. The
chain carries structured information of the intermediate results, enabling more
accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art
performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM
choices.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Deep Efficient Private Neighbor Generation for Subgraph Federated Learning. (arXiv:2401.04336v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04336">http://arxiv.org/abs/2401.04336</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04336]] Deep Efficient Private Neighbor Generation for Subgraph Federated Learning(http://arxiv.org/abs/2401.04336)</code></li>
<li>Summary: <p>Behemoth graphs are often fragmented and separately stored by multiple data
owners as distributed subgraphs in many realistic applications. Without harming
data privacy, it is natural to consider the subgraph federated learning
(subgraph FL) scenario, where each local client holds a subgraph of the entire
global graph, to obtain globally generalized graph mining models. To overcome
the unique challenge of incomplete information propagation on local subgraphs
due to missing cross-subgraph neighbors, previous works resort to the
augmentation of local neighborhoods through the joint FL of missing neighbor
generators and GNNs. Yet their technical designs have profound limitations
regarding the utility, efficiency, and privacy goals of FL. In this work, we
propose FedDEP to comprehensively tackle these challenges in subgraph FL.
FedDEP consists of a series of novel technical designs: (1) Deep neighbor
generation through leveraging the GNN embeddings of potential missing
neighbors; (2) Efficient pseudo-FL for neighbor generation through embedding
prototyping; and (3) Privacy protection through noise-less
edge-local-differential-privacy.
</p>
<p>We analyze the correctness and efficiency of FedDEP, and provide theoretical
guarantees on its privacy.
</p>
<p>Empirical results on four real-world datasets justify the clear benefits of
proposed techniques.
</p></li>
</ul>

<h3>Title: A Survey on Efficient Federated Learning Methods for Foundation Model Training. (arXiv:2401.04472v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04472">http://arxiv.org/abs/2401.04472</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04472]] A Survey on Efficient Federated Learning Methods for Foundation Model Training(http://arxiv.org/abs/2401.04472)</code></li>
<li>Summary: <p>Federated Learning (FL) has become an established technique to facilitate
privacy-preserving collaborative training. However, new approaches to FL often
discuss their contributions involving small deep-learning models only. With the
tremendous success of transformer models, the following question arises: What
is necessary to operationalize foundation models in an FL application? Knowing
that computation and communication often take up similar amounts of time in FL,
we introduce a novel taxonomy focused on computational and communication
efficiency methods in FL applications. This said, these methods aim to optimize
the training time and reduce communication between clients and the server. We
also look at the current state of widely used FL frameworks and discuss future
research potentials based on existing approaches in FL research and beyond.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: PhilEO Bench: Evaluating Geo-Spatial Foundation Models. (arXiv:2401.04464v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04464">http://arxiv.org/abs/2401.04464</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04464]] PhilEO Bench: Evaluating Geo-Spatial Foundation Models(http://arxiv.org/abs/2401.04464)</code></li>
<li>Summary: <p>Massive amounts of unlabelled data are captured by Earth Observation (EO)
satellites, with the Sentinel-2 constellation generating 1.6 TB of data daily.
This makes Remote Sensing a data-rich domain well suited to Machine Learning
(ML) solutions. However, a bottleneck in applying ML models to EO is the lack
of annotated data as annotation is a labour-intensive and costly process. As a
result, research in this domain has focused on Self-Supervised Learning and
Foundation Model approaches. This paper addresses the need to evaluate
different Foundation Models on a fair and uniform benchmark by introducing the
PhilEO Bench, a novel evaluation framework for EO Foundation Models. The
framework comprises of a testbed and a novel 400 GB Sentinel-2 dataset
containing labels for three downstream tasks, building density estimation, road
segmentation, and land cover classification. We present experiments using our
framework evaluating different Foundation Models, including Prithvi and SatMAE,
at multiple n-shots and convergence rates.
</p></li>
</ul>

<h3>Title: SynHIN: Generating Synthetic Heterogeneous Information Network for Explainable AI. (arXiv:2401.04133v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04133">http://arxiv.org/abs/2401.04133</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04133]] SynHIN: Generating Synthetic Heterogeneous Information Network for Explainable AI(http://arxiv.org/abs/2401.04133)</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) excel in various domains, from detecting
e-commerce spam to social network classification problems. However, the lack of
public graph datasets hampers research progress, particularly in heterogeneous
information networks (HIN). The demand for datasets for fair HIN comparisons is
growing due to advancements in GNN interpretation models. In response, we
propose SynHIN, a unique method for generating synthetic heterogeneous
information networks. SynHIN identifies motifs in real-world datasets,
summarizes graph statistics, and constructs a synthetic network. Our approach
utilizes In-Cluster and Out-Cluster Merge modules to build the synthetic HIN
from primary motif clusters. After In/Our-Cluster mergers and a post-pruning
process fitting the real dataset constraints, we ensure the synthetic graph
statistics align closely with the reference one. SynHIN generates a synthetic
heterogeneous graph dataset for node classification tasks, using the primary
motif as the explanation ground truth. It can adapt and address the lack of
heterogeneous graph datasets and motif ground truths, proving beneficial for
assessing heterogeneous graph neural network explainers. We further present a
benchmark dataset for future heterogeneous graph explainer model research. Our
work marks a significant step towards explainable AI in HGNNs.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Image classification network enhancement methods based on knowledge injection. (arXiv:2401.04441v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04441">http://arxiv.org/abs/2401.04441</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04441]] Image classification network enhancement methods based on knowledge injection(http://arxiv.org/abs/2401.04441)</code></li>
<li>Summary: <p>The current deep neural network algorithm still stays in the end-to-end
training supervision method like Image-Label pairs, which makes traditional
algorithm is difficult to explain the reason for the results, and the
prediction logic is difficult to understand and analyze. The current algorithm
does not use the existing human knowledge information, which makes the model
not in line with the human cognition model and makes the model not suitable for
human use. In order to solve the above problems, the present invention provides
a deep neural network training method based on the human knowledge, which uses
the human cognition model to construct the deep neural network training model,
and uses the existing human knowledge information to construct the deep neural
network training model. This paper proposes a multi-level hierarchical deep
learning algorithm, which is composed of multi-level hierarchical deep neural
network architecture and multi-level hierarchical deep learning framework. The
experimental results show that the proposed algorithm can effectively explain
the hidden information of the neural network. The goal of our study is to
improve the interpretability of deep neural networks (DNNs) by providing an
analysis of the impact of knowledge injection on the classification task. We
constructed a knowledge injection dataset with matching knowledge data and
image classification data. The knowledge injection dataset is the benchmark
dataset for the experiments in the paper. Our model expresses the improvement
in interpretability and classification task performance of hidden layers at
different scales.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Memory-Efficient Personalization using Quantized Diffusion Model. (arXiv:2401.04339v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04339">http://arxiv.org/abs/2401.04339</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04339]] Memory-Efficient Personalization using Quantized Diffusion Model(http://arxiv.org/abs/2401.04339)</code></li>
<li>Summary: <p>The rise of billion-parameter diffusion models like Stable Diffusion XL,
Imagen, and Dall-E3 markedly advances the field of generative AI. However,
their large-scale nature poses challenges in fine-tuning and deployment due to
high resource demands and slow inference speed. This paper ventures into the
relatively unexplored yet promising realm of fine-tuning quantized diffusion
models. We establish a strong baseline by customizing three models: PEQA for
fine-tuning quantization parameters, Q-Diffusion for post-training
quantization, and DreamBooth for personalization. Our analysis reveals a
notable trade-off between subject and prompt fidelity within the baseline
model. To address these issues, we introduce two strategies, inspired by the
distinct roles of different timesteps in diffusion models: S1 optimizing a
single set of fine-tuning parameters exclusively at selected intervals, and S2
creating multiple fine-tuning parameter sets, each specialized for different
timestep intervals. Our approach not only enhances personalization but also
upholds prompt fidelity and image quality, significantly outperforming the
baseline qualitatively and quantitatively. The code will be made publicly
available.
</p></li>
</ul>

<h3>Title: D3AD: Dynamic Denoising Diffusion Probabilistic Model for Anomaly Detection. (arXiv:2401.04463v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04463">http://arxiv.org/abs/2401.04463</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04463]] D3AD: Dynamic Denoising Diffusion Probabilistic Model for Anomaly Detection(http://arxiv.org/abs/2401.04463)</code></li>
<li>Summary: <p>Diffusion models have found valuable applications in anomaly detection by
capturing the nominal data distribution and identifying anomalies via
reconstruction. Despite their merits, they struggle to localize anomalies of
varying scales, especially larger anomalies like entire missing components.
Addressing this, we present a novel framework that enhances the capability of
diffusion models, by extending the previous introduced implicit conditioning
approach Meng et al. (2022) in three significant ways. First, we incorporate a
dynamic step size computation that allows for variable noising steps in the
forward process guided by an initial anomaly prediction. Second, we demonstrate
that denoising an only scaled input, without any added noise, outperforms
conventional denoising process. Third, we project images in a latent space to
abstract away from fine details that interfere with reconstruction of large
missing components. Additionally, we propose a fine-tuning mechanism that
facilitates the model to effectively grasp the nuances of the target domain.
Our method undergoes rigorous evaluation on two prominent anomaly detection
datasets VISA and BTAD, yielding state-of-the-art performance. Importantly, our
framework effectively localizes anomalies regardless of their scale, marking a
pivotal advancement in diffusion-based anomaly detection.
</p></li>
</ul>

<h3>Title: MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation. (arXiv:2401.04468v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04468">http://arxiv.org/abs/2401.04468</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04468]] MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation(http://arxiv.org/abs/2401.04468)</code></li>
<li>Summary: <p>The growing demand for high-fidelity video generation from textual
descriptions has catalyzed significant research in this field. In this work, we
introduce MagicVideo-V2 that integrates the text-to-image model, video motion
generator, reference image embedding module and frame interpolation module into
an end-to-end video generation pipeline. Benefiting from these architecture
designs, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution
video with remarkable fidelity and smoothness. It demonstrates superior
performance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph,
Moon Valley and Stable Video Diffusion model via user evaluation at large
scale.
</p></li>
</ul>

<h3>Title: Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models. (arXiv:2401.04585v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04585">http://arxiv.org/abs/2401.04585</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04585]] Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models(http://arxiv.org/abs/2401.04585)</code></li>
<li>Summary: <p>Diffusion models have achieved great success in image generation tasks
through iterative noise estimation. However, the heavy denoising process and
complex neural networks hinder their low-latency applications in real-world
scenarios. Quantization can effectively reduce model complexity, and
post-training quantization (PTQ), which does not require fine-tuning, is highly
promising in accelerating the denoising process. Unfortunately, we find that
due to the highly dynamic distribution of activations in different denoising
steps, existing PTQ methods for diffusion models suffer from distribution
mismatch issues at both calibration sample level and reconstruction output
level, which makes the performance far from satisfactory, especially in low-bit
cases. In this paper, we propose Enhanced Distribution Alignment for
Post-Training Quantization of Diffusion Models (EDA-DM) to address the above
issues. Specifically, at the calibration sample level, we select calibration
samples based on the density and diversity in the latent space, thus
facilitating the alignment of their distribution with the overall samples; and
at the reconstruction output level, we propose Fine-grained Block
Reconstruction, which can align the outputs of the quantized model and the
full-precision model at different network granularity. Extensive experiments
demonstrate that EDA-DM outperforms the existing post-training quantization
frameworks in both unconditional and conditional generation scenarios. At
low-bit precision, the quantized models with our method even outperform the
full-precision models on most datasets.
</p></li>
</ul>

<h3>Title: EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models. (arXiv:2401.04608v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04608">http://arxiv.org/abs/2401.04608</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04608]] EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models(http://arxiv.org/abs/2401.04608)</code></li>
<li>Summary: <p>Recent years have witnessed remarkable progress in image generation task,
where users can create visually astonishing images with high-quality. However,
existing text-to-image diffusion models are proficient in generating concrete
concepts (dogs) but encounter challenges with more abstract ones (emotions).
Several efforts have been made to modify image emotions with color and style
adjustments, facing limitations in effectively conveying emotions with fixed
image contents. In this work, we introduce Emotional Image Content Generation
(EICG), a new task to generate semantic-clear and emotion-faithful images given
emotion categories. Specifically, we propose an emotion space and construct a
mapping network to align it with the powerful Contrastive Language-Image
Pre-training (CLIP) space, providing a concrete interpretation of abstract
emotions. Attribute loss and emotion confidence are further proposed to ensure
the semantic diversity and emotion fidelity of the generated images. Our method
outperforms the state-of-the-art text-to-image approaches both quantitatively
and qualitatively, where we derive three custom metrics, i.e., emotion
accuracy, semantic clarity and semantic diversity. In addition to generation,
our method can help emotion understanding and inspire emotional art design.
</p></li>
</ul>

<h3>Title: Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation. (arXiv:2401.04728v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04728">http://arxiv.org/abs/2401.04728</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04728]] Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation(http://arxiv.org/abs/2401.04728)</code></li>
<li>Summary: <p>Recent advances in generative diffusion models have enabled the previously
unfeasible capability of generating 3D assets from a single input image or a
text prompt. In this work, we aim to enhance the quality and functionality of
these models for the task of creating controllable, photorealistic human
avatars. We achieve this by integrating a 3D morphable model into the
state-of-the-art multiview-consistent diffusion approach. We demonstrate that
accurate conditioning of a generative pipeline on the articulated 3D model
enhances the baseline model performance on the task of novel view synthesis
from a single image. More importantly, this integration facilitates a seamless
and accurate incorporation of facial expression and body pose control into the
generation process. To the best of our knowledge, our proposed framework is the
first diffusion model to enable the creation of fully 3D-consistent,
animatable, and photorealistic human avatars from a single image of an unseen
subject; extensive quantitative and qualitative evaluations demonstrate the
advantages of our approach over existing state-of-the-art avatar creation
models on both novel view and novel expression synthesis tasks.
</p></li>
</ul>

<h3>Title: The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline. (arXiv:2401.04136v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04136">http://arxiv.org/abs/2401.04136</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04136]] The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline(http://arxiv.org/abs/2401.04136)</code></li>
<li>Summary: <p>The commercialization of diffusion models, renowned for their ability to
generate high-quality images that are often indistinguishable from real ones,
brings forth potential copyright concerns. Although attempts have been made to
impede unauthorized access to copyrighted material during training and to
subsequently prevent DMs from generating copyrighted images, the effectiveness
of these solutions remains unverified. This study explores the vulnerabilities
associated with copyright protection in DMs by introducing a backdoor data
poisoning attack (SilentBadDiffusion) against text-to-image diffusion models.
Our attack method operates without requiring access to or control over the
diffusion model's training or fine-tuning processes; it merely involves the
insertion of poisoning data into the clean training dataset. This data,
comprising poisoning images equipped with prompts, is generated by leveraging
the powerful capabilities of multimodal large language models and text-guided
image inpainting techniques. Our experimental results and analysis confirm the
method's effectiveness. By integrating a minor portion of
non-copyright-infringing stealthy poisoning data into the clean
dataset-rendering it free from suspicion-we can prompt the finetuned diffusion
models to produce copyrighted content when activated by specific trigger
prompts. These findings underline potential pitfalls in the prevailing
copyright protection strategies and underscore the necessity for increased
scrutiny and preventative measures against the misuse of DMs.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Efficient Selective Audio Masked Multimodal Bottleneck Transformer for Audio-Video Classification. (arXiv:2401.04154v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04154">http://arxiv.org/abs/2401.04154</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04154]] Efficient Selective Audio Masked Multimodal Bottleneck Transformer for Audio-Video Classification(http://arxiv.org/abs/2401.04154)</code></li>
<li>Summary: <p>Audio and video are two most common modalities in the mainstream media
platforms, e.g., YouTube. To learn from multimodal videos effectively, in this
work, we propose a novel audio-video recognition approach termed audio video
Transformer, AVT, leveraging the effective spatio-temporal representation by
the video Transformer to improve action recognition accuracy. For multimodal
fusion, simply concatenating multimodal tokens in a cross-modal Transformer
requires large computational and memory resources, instead we reduce the
cross-modality complexity through an audio-video bottleneck Transformer. To
improve the learning efficiency of multimodal Transformer, we integrate
self-supervised objectives, i.e., audio-video contrastive learning, audio-video
matching, and masked audio and video learning, into AVT training, which maps
diverse audio and video representations into a common multimodal representation
space. We further propose a masked audio segment loss to learn semantic audio
activities in AVT. Extensive experiments and ablation studies on three public
datasets and two in-house datasets consistently demonstrate the effectiveness
of the proposed AVT. Specifically, AVT outperforms its previous
state-of-the-art counterparts on Kinetics-Sounds by 8%. AVT also surpasses one
of the previous state-of-the-art video Transformers [25] by 10% on VGGSound by
leveraging the audio signal. Compared to one of the previous state-of-the-art
multimodal methods, MBT [32], AVT is 1.3% more efficient in terms of FLOPs and
improves the accuracy by 3.8% on Epic-Kitchens-100.
</p></li>
</ul>

<h3>Title: Iterative Feedback Network for Unsupervised Point Cloud Registration. (arXiv:2401.04357v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04357">http://arxiv.org/abs/2401.04357</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04357]] Iterative Feedback Network for Unsupervised Point Cloud Registration(http://arxiv.org/abs/2401.04357)</code></li>
<li>Summary: <p>As a fundamental problem in computer vision, point cloud registration aims to
seek the optimal transformation for aligning a pair of point clouds. In most
existing methods, the information flows are usually forward transferring, thus
lacking the guidance from high-level information to low-level information.
Besides, excessive high-level information may be overly redundant, and directly
using it may conflict with the original low-level information. In this paper,
we propose a novel Iterative Feedback Network (IFNet) for unsupervised point
cloud registration, in which the representation of low-level features is
efficiently enriched by rerouting subsequent high-level features. Specifically,
our IFNet is built upon a series of Feedback Registration Block (FRB) modules,
with each module responsible for generating the feedforward rigid
transformation and feedback high-level features. These FRB modules are cascaded
and recurrently unfolded over time. Further, the Feedback Transformer is
designed to efficiently select relevant information from feedback high-level
features, which is utilized to refine the low-level features. What's more, we
incorporate a geometry-awareness descriptor to empower the network for making
full use of most geometric information, which leads to more precise
registration results. Extensive experiments on various benchmark datasets
demonstrate the superior registration performance of our IFNet.
</p></li>
</ul>

<h3>Title: WaveletFormerNet: A Transformer-based Wavelet Network for Real-world Non-homogeneous and Dense Fog Removal. (arXiv:2401.04550v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04550">http://arxiv.org/abs/2401.04550</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04550]] WaveletFormerNet: A Transformer-based Wavelet Network for Real-world Non-homogeneous and Dense Fog Removal(http://arxiv.org/abs/2401.04550)</code></li>
<li>Summary: <p>Although deep convolutional neural networks have achieved remarkable success
in removing synthetic fog, it is essential to be able to process images taken
in complex foggy conditions, such as dense or non-homogeneous fog, in the real
world. However, the haze distribution in the real world is complex, and
downsampling can lead to color distortion or loss of detail in the output
results as the resolution of a feature map or image resolution decreases. In
addition to the challenges of obtaining sufficient training data, overfitting
can also arise in deep learning techniques for foggy image processing, which
can limit the generalization abilities of the model, posing challenges for its
practical applications in real-world scenarios. Considering these issues, this
paper proposes a Transformer-based wavelet network (WaveletFormerNet) for
real-world foggy image recovery. We embed the discrete wavelet transform into
the Vision Transformer by proposing the WaveletFormer and IWaveletFormer
blocks, aiming to alleviate texture detail loss and color distortion in the
image due to downsampling. We introduce parallel convolution in the Transformer
block, which allows for the capture of multi-frequency information in a
lightweight mechanism. Additionally, we have implemented a feature aggregation
module (FAM) to maintain image resolution and enhance the feature extraction
capacity of our model, further contributing to its impressive performance in
real-world foggy image recovery tasks. Extensive experiments demonstrate that
our WaveletFormerNet performs better than state-of-the-art methods, as shown
through quantitative and qualitative evaluations of minor model complexity.
Additionally, our satisfactory results on real-world dust removal and
application tests showcase the superior generalization ability and improved
performance of WaveletFormerNet in computer vision-related applications.
</p></li>
</ul>

<h3>Title: Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules. (arXiv:2401.04130v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04130">http://arxiv.org/abs/2401.04130</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04130]] Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules(http://arxiv.org/abs/2401.04130)</code></li>
<li>Summary: <p>Parameter-efficient tuning (PET) methods such as LoRA, Adapter, and Visual
Prompt Tuning (VPT) have found success in enabling adaptation to new domains by
tuning small modules within a transformer model. However, the number of domains
encountered during test time can be very large, and the data is usually
unlabeled. Thus, adaptation to new domains is challenging; it is also
impractical to generate customized tuned modules for each such domain. Toward
addressing these challenges, this work introduces PLUTO: a Plug-and-pLay
modUlar Test-time domain adaptatiOn strategy. We pre-train a large set of
modules, each specialized for different source domains, effectively creating a
``module store''. Given a target domain with few-shot unlabeled data, we
introduce an unsupervised test-time adaptation (TTA) method to (1) select a
sparse subset of relevant modules from this store and (2) create a weighted
combination of selected modules without tuning their weights. This
plug-and-play nature enables us to harness multiple most-relevant source
domains in a single inference call. Comprehensive evaluations demonstrate that
PLUTO uniformly outperforms alternative TTA methods and that selecting $\leq$5
modules suffice to extract most of the benefit. At a high level, our method
equips pre-trained transformers with the capability to dynamically adapt to new
domains, motivating a new paradigm for efficient and scalable domain
adaptation.
</p></li>
</ul>

<h3>Title: Global-Aware Enhanced Spatial-Temporal Graph Recurrent Networks: A New Framework For Traffic Flow Prediction. (arXiv:2401.04135v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04135">http://arxiv.org/abs/2401.04135</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04135]] Global-Aware Enhanced Spatial-Temporal Graph Recurrent Networks: A New Framework For Traffic Flow Prediction(http://arxiv.org/abs/2401.04135)</code></li>
<li>Summary: <p>Traffic flow prediction plays a crucial role in alleviating traffic
congestion and enhancing transport efficiency. While combining graph
convolution networks with recurrent neural networks for spatial-temporal
modeling is a common strategy in this realm, the restricted structure of
recurrent neural networks limits their ability to capture global information.
For spatial modeling, many prior studies learn a graph structure that is
assumed to be fixed and uniform at all time steps, which may not be true. This
paper introduces a novel traffic prediction framework, Global-Aware Enhanced
Spatial-Temporal Graph Recurrent Network (GA-STGRN), comprising two core
components: a spatial-temporal graph recurrent neural network and a global
awareness layer. Within this framework, three innovative prediction models are
formulated. A sequence-aware graph neural network is proposed and integrated
into the Gated Recurrent Unit (GRU) to learn non-fixed graphs at different time
steps and capture local temporal relationships. To enhance the model's global
perception, three distinct global spatial-temporal transformer-like
architectures (GST^2) are devised for the global awareness layer. We conduct
extensive experiments on four real traffic datasets and the results demonstrate
the superiority of our framework and the three concrete models.
</p></li>
</ul>

<h3>Title: Setting the Record Straight on Transformer Oversmoothing. (arXiv:2401.04301v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04301">http://arxiv.org/abs/2401.04301</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04301]] Setting the Record Straight on Transformer Oversmoothing(http://arxiv.org/abs/2401.04301)</code></li>
<li>Summary: <p>Transformer-based models have recently become wildly successful across a
diverse set of domains. At the same time, recent work has shown that
Transformers are inherently low-pass filters that gradually oversmooth the
inputs, reducing the expressivity of their representations. A natural question
is: How can Transformers achieve these successes given this shortcoming? In
this work we show that in fact Transformers are not inherently low-pass
filters. Instead, whether Transformers oversmooth or not depends on the
eigenspectrum of their update equations. Our analysis extends prior work in
oversmoothing and in the closely-related phenomenon of rank collapse. We show
that many successful Transformer models have attention and weights which
satisfy conditions that avoid oversmoothing. Based on this analysis, we derive
a simple way to parameterize the weights of the Transformer update equations
that allows for control over its spectrum, ensuring that oversmoothing does not
occur. Compared to a recent solution for oversmoothing, our approach improves
generalization, even when training with more layers, fewer datapoints, and data
that is corrupted.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging. (arXiv:2401.04317v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04317">http://arxiv.org/abs/2401.04317</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04317]] Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging(http://arxiv.org/abs/2401.04317)</code></li>
<li>Summary: <p>Indoor imaging is a critical task for robotics and internet-of-things. WiFi
as an omnipresent signal is a promising candidate for carrying out passive
imaging and synchronizing the up-to-date information to all connected devices.
This is the first research work to consider WiFi indoor imaging as a
multi-modal image generation task that converts the measured WiFi power into a
high-resolution indoor image. Our proposed WiFi-GEN network achieves a shape
reconstruction accuracy that is 275% of that achieved by physical model-based
inversion methods. Additionally, the Frechet Inception Distance score has been
significantly reduced by 82%. To examine the effectiveness of models for this
task, the first large-scale dataset is released containing 80,000 pairs of WiFi
signal and imaging target. Our model absorbs challenges for the model-based
methods including the non-linearity, ill-posedness and non-certainty into
massive parameters of our generative AI network. The network is also designed
to best fit measured WiFi signals and the desired imaging output. For
reproducibility, we will release the data and code upon acceptance.
</p></li>
</ul>

<h3>Title: Advancing Ante-Hoc Explainable Models through Generative Adversarial Networks. (arXiv:2401.04647v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04647">http://arxiv.org/abs/2401.04647</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04647]] Advancing Ante-Hoc Explainable Models through Generative Adversarial Networks(http://arxiv.org/abs/2401.04647)</code></li>
<li>Summary: <p>This paper presents a novel concept learning framework for enhancing model
interpretability and performance in visual classification tasks. Our approach
appends an unsupervised explanation generator to the primary classifier network
and makes use of adversarial training. During training, the explanation module
is optimized to extract visual concepts from the classifier's latent
representations, while the GAN-based module aims to discriminate images
generated from concepts, from true images. This joint training scheme enables
the model to implicitly align its internally learned concepts with
human-interpretable visual properties. Comprehensive experiments demonstrate
the robustness of our approach, while producing coherent concept activations.
We analyse the learned concepts, showing their semantic concordance with object
parts and visual attributes. We also study how perturbations in the adversarial
training protocol impact both classification and concept acquisition. In
summary, this work presents a significant step towards building inherently
interpretable deep vision models with task-aligned concept representations - a
key enabler for developing trustworthy AI for real-world perception tasks.
</p></li>
</ul>

<h3>Title: Low-Resource Vision Challenges for Foundation Models. (arXiv:2401.04716v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04716">http://arxiv.org/abs/2401.04716</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04716]] Low-Resource Vision Challenges for Foundation Models(http://arxiv.org/abs/2401.04716)</code></li>
<li>Summary: <p>Low-resource settings are well-established in natural language processing,
where many languages lack sufficient data for machine learning at scale.
However, low-resource problems are under-explored in computer vision. In this
paper, we strive to address this gap and explore the challenges of low-resource
image tasks with vision foundation models. Thus, we first collect a benchmark
of genuinely low-resource image data, covering historic maps, circuit diagrams,
and mechanical drawings. These low-resource settings all share the three
challenges of data scarcity, fine-grained differences, and the distribution
shift from natural images to the specialized domain of interest. While existing
foundation models have shown impressive generalizability, we find they cannot
transfer well to our low-resource tasks. To begin to tackle the challenges of
low-resource vision, we introduce one simple baseline per challenge.
Specifically, we propose to i) enlarge the data space by generative models, ii)
adopt the best sub-kernels to encode local regions for fine-grained difference
discovery and iii) learn attention for specialized domains. Experiments on the
three low-resource data sources in our benchmark demonstrate our proposals
already provide a better baseline than common transfer learning, data
augmentation, and fine-grained methods. This highlights the unique
characteristics and challenges of low-resource vision for foundation models
that warrant further investigation. Project website:
https://xiaobai1217.github.io/Low-Resource-Vision/.
</p></li>
</ul>

<h3>Title: The Critique of Critique. (arXiv:2401.04518v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04518">http://arxiv.org/abs/2401.04518</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04518]] The Critique of Critique(http://arxiv.org/abs/2401.04518)</code></li>
<li>Summary: <p>Critique, as a natural language description for assessing the quality of
model-generated content, has been proven to play an essential role in the
training, evaluation, and refinement of Large Language Models (LLMs). However,
there is a lack of principled understanding in evaluating the quality of the
critique itself. In this paper, we pioneer the critique of critique, termed
MetaCritique, which is a framework to evaluate the critique from two aspects,
i.e., factuality as precision score and comprehensiveness as recall score. We
calculate the harmonic mean of precision and recall as the overall rating
called F1 score. To obtain a reliable evaluation outcome, we propose Atomic
Information Units (AIUs), which describe the critique in a more fine-grained
manner. MetaCritique takes each AIU into account and aggregates each AIU's
judgment for the overall score. Moreover, given the evaluation process involves
intricate reasoning, our MetaCritique provides a natural language rationale to
support each judgment. We construct a meta-evaluation dataset containing 300
critiques (2653 AIUs) across four tasks (question answering, reasoning,
entailment, and summarization), and we conduct a comparative study to
demonstrate the feasibility and effectiveness. Experiments also show superior
critique judged by MetaCritique leads to better refinement, indicating
generative artificial intelligence indeed has the potential to be significantly
advanced with our MetaCritique. We will release relevant code and
meta-evaluation datasets at https://github.com/GAIR-NLP/MetaCritique.
</p></li>
</ul>

<h3>Title: MERA: A Comprehensive LLM Evaluation in Russian. (arXiv:2401.04531v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04531">http://arxiv.org/abs/2401.04531</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04531]] MERA: A Comprehensive LLM Evaluation in Russian(http://arxiv.org/abs/2401.04531)</code></li>
<li>Summary: <p>Over the past few years, one of the most notable advancements in AI research
has been in foundation models (FMs), headlined by the rise of language models
(LMs). As the models' size increases, LMs demonstrate enhancements in
measurable aspects and the development of new qualitative features. However,
despite researchers' attention and the rapid growth in LM application, the
capabilities, limitations, and associated risks still need to be better
understood. To address these issues, we introduce an open Multimodal Evaluation
of Russian-language Architectures (MERA), a new instruction benchmark for
evaluating foundation models oriented towards the Russian language. The
benchmark encompasses 21 evaluation tasks for generative models in 11 skill
domains and is designed as a black-box test to ensure the exclusion of data
leakage. The paper introduces a methodology to evaluate FMs and LMs in zero-
and few-shot fixed instruction settings that can be extended to other
modalities. We propose an evaluation methodology, an open-source code base for
the MERA assessment, and a leaderboard with a submission system. We evaluate
open LMs as baselines and find that they are still far behind the human level.
We publicly release MERA to guide forthcoming research, anticipate
groundbreaking model features, standardize the evaluation procedure, and
address potential societal drawbacks.
</p></li>
</ul>

<h3>Title: CCNETS: A Novel Brain-Inspired Approach for Enhanced Pattern Recognition in Imbalanced Datasets. (arXiv:2401.04139v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04139">http://arxiv.org/abs/2401.04139</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04139]] CCNETS: A Novel Brain-Inspired Approach for Enhanced Pattern Recognition in Imbalanced Datasets(http://arxiv.org/abs/2401.04139)</code></li>
<li>Summary: <p>This study introduces CCNETS (Causal Learning with Causal Cooperative Nets),
a novel generative model-based classifier designed to tackle the challenge of
generating data for imbalanced datasets in pattern recognition. CCNETS is
uniquely crafted to emulate brain-like information processing and comprises
three main components: Explainer, Producer, and Reasoner. Each component is
designed to mimic specific brain functions, which aids in generating
high-quality datasets and enhancing classification performance.
</p>
<p>The model is particularly focused on addressing the common and significant
challenge of handling imbalanced datasets in machine learning. CCNETS's
effectiveness is demonstrated through its application to a "fraud dataset,"
where normal transactions significantly outnumber fraudulent ones (99.83% vs.
0.17%). Traditional methods often struggle with such imbalances, leading to
skewed performance metrics. However, CCNETS exhibits superior classification
ability, as evidenced by its performance metrics. Specifically, it achieved an
F1-score of 0.7992, outperforming traditional models like Autoencoders and
Multi-layer Perceptrons (MLP) in the same context. This performance indicates
CCNETS's proficiency in more accurately distinguishing between normal and
fraudulent patterns.
</p>
<p>The innovative structure of CCNETS enhances the coherence between generative
and classification models, helping to overcome the limitations of pattern
recognition that rely solely on generative models. This study emphasizes
CCNETS's potential in diverse applications, especially where quality data
generation and pattern recognition are key. It proves effective in machine
learning, particularly for imbalanced datasets. CCNETS overcomes current
challenges in these datasets and advances machine learning with brain-inspired
approaches.
</p></li>
</ul>

<h3>Title: Transfer-Learning-Based Autotuning Using Gaussian Copula. (arXiv:2401.04669v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04669">http://arxiv.org/abs/2401.04669</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04669]] Transfer-Learning-Based Autotuning Using Gaussian Copula(http://arxiv.org/abs/2401.04669)</code></li>
<li>Summary: <p>As diverse high-performance computing (HPC) systems are built, many
opportunities arise for applications to solve larger problems than ever before.
Given the significantly increased complexity of these HPC systems and
application tuning, empirical performance tuning, such as autotuning, has
emerged as a promising approach in recent years. Despite its effectiveness,
autotuning is often a computationally expensive approach. Transfer learning
(TL)-based autotuning seeks to address this issue by leveraging the data from
prior tuning. Current TL methods for autotuning spend significant time modeling
the relationship between parameter configurations and performance, which is
ineffective for few-shot (that is, few empirical evaluations) tuning on new
tasks. We introduce the first generative TL-based autotuning approach based on
the Gaussian copula (GC) to model the high-performing regions of the search
space from prior data and then generate high-performing configurations for new
tasks. This allows a sampling-based approach that maximizes few-shot
performance and provides the first probabilistic estimation of the few-shot
budget for effective TL-based autotuning. We compare our generative TL approach
with state-of-the-art autotuning techniques on several benchmarks. We find that
the GC is capable of achieving 64.37% of peak few-shot performance in its first
evaluation. Furthermore, the GC model can determine a few-shot transfer budget
that yields up to 33.39$\times$ speedup, a dramatic improvement over the
20.58$\times$ speedup using prior techniques.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?. (arXiv:2401.04218v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04218">http://arxiv.org/abs/2401.04218</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04218]] Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?(http://arxiv.org/abs/2401.04218)</code></li>
<li>Summary: <p>We present a benchmark for assessing the capability of Large Language Models
(LLMs) to discern intercardinal directions between geographic locations and
apply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2. This benchmark
specifically evaluates whether LLMs exhibit a hierarchical spatial bias similar
to humans, where judgments about individual locations' spatial relationships
are influenced by the perceived relationships of the larger groups that contain
them. To investigate this, we formulated 14 questions focusing on well-known
American cities. Seven questions were designed to challenge the LLMs with
scenarios potentially influenced by the orientation of larger geographical
units, such as states or countries, while the remaining seven targeted
locations less susceptible to such hierarchical categorization. Among the
tested models, GPT-4 exhibited superior performance with 55.3% accuracy,
followed by GPT-3.5 at 47.3%, and Llama-2 at 44.7%. The models showed
significantly reduced accuracy on tasks with suspected hierarchical bias. For
example, GPT-4's accuracy dropped to 32.9% on these tasks, compared to 85.7% on
others. Despite these inaccuracies, the models identified the nearest cardinal
direction in most cases, suggesting associative learning, embodying human-like
misconceptions. We discuss the potential of text-based data representing
geographic relationships directly to improve the spatial reasoning capabilities
of LLMs.
</p></li>
</ul>

<h3>Title: Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. (arXiv:2401.04319v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04319">http://arxiv.org/abs/2401.04319</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04319]] Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs(http://arxiv.org/abs/2401.04319)</code></li>
<li>Summary: <p>In this paper, we explore a new way for user targeting, where non-expert
marketers could select their target users solely given demands in natural
language form. The key to this issue is how to transform natural languages into
practical structured logical languages, i.e., the structured understanding of
marketer demands. Considering the impressive natural language processing
ability of large language models (LLMs), we try to leverage LLMs to solve this
issue. Past research indicates that the reasoning ability of LLMs can be
effectively enhanced through chain-of-thought (CoT) prompting. But existing
methods still have some limitations: (1) Previous methods either use simple
"Let's think step by step" spells or provide fixed examples in demonstrations
without considering compatibility between prompts and questions, making LLMs
ineffective in some complex reasoning tasks such as structured language
transformation. (2) Previous methods are often implemented in closed-source
models or excessively large models, which is not suitable in industrial
practical scenarios. Based on these, we propose ARALLM (i.e., Analogical
Reasoning Augmented Large Language Models) consisting of two modules:
Analogical Reasoning based Prompting and Reasoning-Augmented Multi-Task Model
Distillation.
</p></li>
</ul>

<h3>Title: Private Fine-tuning of Large Language Models with Zeroth-order Optimization. (arXiv:2401.04343v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04343">http://arxiv.org/abs/2401.04343</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04343]] Private Fine-tuning of Large Language Models with Zeroth-order Optimization(http://arxiv.org/abs/2401.04343)</code></li>
<li>Summary: <p>Fine-tuning large pretrained models on private datasets may run the risk of
violating privacy. Differential privacy is a framework for mitigating privacy
risks by enforcing algorithmic stability. DP-SGD enables training models with
private data in a privacy-preserving manner, but raises new obstacles in the
form of performance loss and significant engineering challenges. We introduce
DP-ZO, a new method for fine-tuning large language models that preserves the
privacy of training data by privatizing zeroth-order optimization. A key
insight into the design of our method is that the direction of the gradient in
SPSA, the zeroth-order algorithm we use, is always random and the only
information that depends on private data is the step size, i.e., a scalar.
Therefore, we only need to privatize the scalar step size, which is
memory-efficient. DP-ZO, which can be instantiated with either Laplace or
Gaussian noise, provides a strong privacy-utility trade-off across different
tasks, and model sizes, under conservative privacy budgets. One noteworthy
result is that DP-ZO exhibits just $1.86\%$ performance degradation due to
privacy at $(1,10^{-5})$-DP when fine-tuning OPT-66B on 1000 training samples
from SQuAD.
</p></li>
</ul>

<h3>Title: TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models. (arXiv:2401.04471v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04471">http://arxiv.org/abs/2401.04471</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04471]] TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models(http://arxiv.org/abs/2401.04471)</code></li>
<li>Summary: <p>Large language models (LLMs) and multimodal large language models (MLLMs)
have shown excellent general capabilities, even exhibiting adaptability in many
professional domains such as law, economics, transportation, and medicine.
Currently, many domain-specific benchmarks have been proposed to verify the
performance of (M)LLMs in specific fields. Among various domains,
transportation plays a crucial role in modern society as it impacts the
economy, the environment, and the quality of life for billions of people.
However, it is unclear how much traffic knowledge (M)LLMs possess and whether
they can reliably perform transportation-related tasks. To address this gap, we
propose TransportationGames, a carefully designed and thorough evaluation
benchmark for assessing (M)LLMs in the transportation domain. By
comprehensively considering the applications in real-world scenarios and
referring to the first three levels in Bloom's Taxonomy, we test the
performance of various (M)LLMs in memorizing, understanding, and applying
transportation knowledge by the selected tasks. The experimental results show
that although some models perform well in some tasks, there is still much room
for improvement overall. We hope the release of TransportationGames can serve
as a foundation for future research, thereby accelerating the implementation
and application of (M)LLMs in the transportation domain.
</p></li>
</ul>

<h3>Title: TechGPT-2.0: A large language model project to solve the task of knowledge graph construction. (arXiv:2401.04507v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04507">http://arxiv.org/abs/2401.04507</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04507]] TechGPT-2(http://arxiv.org/abs/2401.04507)</code></li>
<li>Summary: <p>Large language models have exhibited robust performance across diverse
natural language processing tasks. This report introduces TechGPT-2.0, a
project designed to enhance the capabilities of large language models
specifically in knowledge graph construction tasks, including named entity
recognition (NER) and relationship triple extraction (RTE) tasks in NLP
applications. Additionally, it serves as a LLM accessible for research within
the Chinese open-source model community. We offer two 7B large language model
weights and a QLoRA weight specialized for processing lengthy texts.Notably,
TechGPT-2.0 is trained on Huawei's Ascend server. Inheriting all
functionalities from TechGPT-1.0, it exhibits robust text processing
capabilities, particularly in the domains of medicine and law. Furthermore, we
introduce new capabilities to the model, enabling it to process texts in
various domains such as geographical areas, transportation, organizations,
literary works, biology, natural sciences, astronomical objects, and
architecture. These enhancements also fortified the model's adeptness in
handling hallucinations, unanswerable queries, and lengthy texts. This report
provides a comprehensive and detailed introduction to the full fine-tuning
process on Huawei's Ascend servers, encompassing experiences in Ascend server
debugging, instruction fine-tuning data processing, and model training. Our
code is available at https://github.com/neukg/TechGPT-2.0
</p></li>
</ul>

<h3>Title: Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with Large Language Models. (arXiv:2401.04515v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04515">http://arxiv.org/abs/2401.04515</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04515]] Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with Large Language Models(http://arxiv.org/abs/2401.04515)</code></li>
<li>Summary: <p>This article investigates a zero-shot approach to hypernymy prediction using
large language models (LLMs). The study employs a method based on text
probability calculation, applying it to various generated prompts. The
experiments demonstrate a strong correlation between the effectiveness of
language model prompts and classic patterns, indicating that preliminary prompt
selection can be carried out using smaller models before moving to larger ones.
We also explore prompts for predicting co-hyponyms and improving hypernymy
predictions by augmenting prompts with additional information through
automatically identified co-hyponyms. An iterative approach is developed for
predicting higher-level concepts, which further improves the quality on the
BLESS dataset (MAP = 0.8).
</p></li>
</ul>

<h3>Title: An Assessment on Comprehending Mental Health through Large Language Models. (arXiv:2401.04592v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04592">http://arxiv.org/abs/2401.04592</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04592]] An Assessment on Comprehending Mental Health through Large Language Models(http://arxiv.org/abs/2401.04592)</code></li>
<li>Summary: <p>Mental health challenges pose considerable global burdens on individuals and
communities. Recent data indicates that more than 20% of adults may encounter
at least one mental disorder in their lifetime. On the one hand, the
advancements in large language models have facilitated diverse applications,
yet a significant research gap persists in understanding and enhancing the
potential of large language models within the domain of mental health. On the
other hand, across various applications, an outstanding question involves the
capacity of large language models to comprehend expressions of human mental
health conditions in natural language. This study presents an initial
evaluation of large language models in addressing this gap. Due to this, we
compare the performance of Llama-2 and ChatGPT with classical Machine as well
as Deep learning models. Our results on the DAIC-WOZ dataset show that
transformer-based models, like BERT or XLNet, outperform the large language
models.
</p></li>
</ul>

<h3>Title: Agent Alignment in Evolving Social Norms. (arXiv:2401.04620v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04620">http://arxiv.org/abs/2401.04620</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04620]] Agent Alignment in Evolving Social Norms(http://arxiv.org/abs/2401.04620)</code></li>
<li>Summary: <p>Agents based on Large Language Models (LLMs) are increasingly permeating
various domains of human production and life, highlighting the importance of
aligning them with human values. The current alignment of AI systems primarily
focuses on passively aligning LLMs through human intervention. However, agents
possess characteristics like receiving environmental feedback and
self-evolution, rendering the LLM alignment methods inadequate. In response, we
propose an evolutionary framework for agent evolution and alignment, named
EvolutionaryAgent, which transforms agent alignment into a process of evolution
and selection under the principle of survival of the fittest. In an environment
where social norms continuously evolve, agents better adapted to the current
social norms will have a higher probability of survival and proliferation,
while those inadequately aligned dwindle over time. Experimental results
assessing the agents from multiple perspectives in aligning with social norms
demonstrate that EvolutionaryAgent possesses the capability to align
progressively better with the evolving social norms while maintaining its
proficiency in general tasks. Effectiveness tests conducted on various open and
closed-source LLMs as the foundation for agents also prove the applicability of
our approach.
</p></li>
</ul>

<h3>Title: Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. (arXiv:2401.04658v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04658">http://arxiv.org/abs/2401.04658</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04658]] Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models(http://arxiv.org/abs/2401.04658)</code></li>
<li>Summary: <p>Linear attention is an efficient attention mechanism that has recently
emerged as a promising alternative to conventional softmax attention. With its
ability to process tokens in linear computational complexities, linear
attention, in theory, can handle sequences of unlimited length without
sacrificing speed, i.e., maintaining a constant training speed for various
sequence lengths with a fixed memory consumption. However, due to the issue
with cumulative summation (cumsum), current linear attention algorithms cannot
demonstrate their theoretical advantage in a causal setting. In this paper, we
present Lightning Attention-2, the first linear attention implementation that
enables linear attention to realize its theoretical computational benefits. To
achieve this, we leverage the thought of tiling, separately handling the
intra-block and inter-block components in linear attention calculation.
Specifically, we utilize the conventional attention computation mechanism for
the intra-blocks and apply linear attention kernel tricks for the inter-blocks.
A tiling technique is adopted through both forward and backward procedures to
take full advantage of the GPU hardware. We implement our algorithm in Triton
to make it IO-aware and hardware-friendly. Various experiments are conducted on
different model sizes and sequence lengths. Lightning Attention-2 retains
consistent training and inference speed regardless of input sequence length and
is significantly faster than other attention mechanisms. The source code is
available at https://github.com/OpenNLPLab/lightning-attention.
</p></li>
</ul>

<h3>Title: Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers. (arXiv:2401.04695v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04695">http://arxiv.org/abs/2401.04695</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04695]] Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers(http://arxiv.org/abs/2401.04695)</code></li>
<li>Summary: <p>Factual questions typically can be answered correctly at different levels of
granularity. For example, both ``August 4, 1961'' and ``1961'' are correct
answers to the question ``When was Barack Obama born?''. Standard question
answering (QA) evaluation protocols, however, do not explicitly take this into
account and compare a predicted answer against answers of a single granularity
level. In this work, we propose GRANOLA QA, a novel evaluation setting where a
predicted answer is evaluated in terms of accuracy and informativeness against
a set of multi-granularity answers. We present a simple methodology for
enriching existing datasets with multi-granularity answers, and create
GRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We
evaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm,
called Decoding with Response Aggregation (DRAG), that is geared towards
aligning the response granularity with the model's uncertainty. Our experiments
show that large language models with standard decoding tend to generate
specific answers, which are often incorrect. In contrast, when evaluated on
multi-granularity answers, DRAG yields a nearly 20 point increase in accuracy
on average, which further increases for rare entities. Overall, this reveals
that standard evaluation and decoding schemes may significantly underestimate
the knowledge encapsulated in LMs.
</p></li>
</ul>

<h3>Title: Model Editing Can Hurt General Abilities of Large Language Models. (arXiv:2401.04700v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04700">http://arxiv.org/abs/2401.04700</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04700]] Model Editing Can Hurt General Abilities of Large Language Models(http://arxiv.org/abs/2401.04700)</code></li>
<li>Summary: <p>Recent advances in large language models (LLMs) have opened up new paradigms
for accessing the knowledge stored in their parameters. One critical challenge
that has emerged is the presence of hallucinations in LLM outputs due to false
or outdated knowledge. Since retraining LLMs with updated information is
resource-intensive, there has been a growing interest in model editing.
However, many model editing methods, while effective in various scenarios, tend
to overemphasize aspects such as efficacy, generalization, and locality in
editing performance, often overlooking potential side effects on the general
abilities of LLMs. In this paper, we raise concerns that the improvement of
model factuality may come at the cost of a significant degradation of these
general abilities, which is not conducive to the sustainable development of
LLMs. Systematically, we analyze side effects by evaluating four popular
editing methods on two LLMs across eight representative task categories.
Extensive empirical research reveals that model editing does improve model
factuality but at the expense of substantially impairing general abilities.
Therefore, we advocate for more research efforts to minimize the loss of
general abilities acquired during LLM pre-training and to ultimately preserve
them during model editing.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: MST: Adaptive Multi-Scale Tokens Guided Interactive Segmentation. (arXiv:2401.04403v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04403">http://arxiv.org/abs/2401.04403</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04403]] MST: Adaptive Multi-Scale Tokens Guided Interactive Segmentation(http://arxiv.org/abs/2401.04403)</code></li>
<li>Summary: <p>In the field of Industrial Informatics, interactive segmentation has gained
significant attention for its application in human-computer interaction and
data annotation. Existing algorithms, however, face challenges in balancing the
segmentation accuracy between large and small targets, often leading to an
increased number of user interactions. To tackle this, a novel multi-scale
token adaptation algorithm, leveraging token similarity, has been devised to
enhance segmentation across varying target sizes. This algorithm utilizes a
differentiable top-k tokens selection mechanism, allowing for fewer tokens to
be used while maintaining efficient multi-scale token interaction. Furthermore,
a contrastive loss is introduced to better discriminate between target and
background tokens, improving the correctness and robustness of the tokens
similar to the target. Extensive benchmarking shows that the algorithm achieves
state-of-the-art (SOTA) performance compared to current methods. An interactive
demo and all reproducible codes will be released at
https://github.com/hahamyt/mst.
</p></li>
</ul>

<h3>Title: MapAI: Precision in Building Segmentation. (arXiv:2401.04406v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04406">http://arxiv.org/abs/2401.04406</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04406]] MapAI: Precision in Building Segmentation(http://arxiv.org/abs/2401.04406)</code></li>
<li>Summary: <p>MapAI: Precision in Building Segmentation is a competition arranged with the
Norwegian Artificial Intelligence Research Consortium (NORA) in collaboration
with Centre for Artificial Intelligence Research at the University of Agder
(CAIR), the Norwegian Mapping Authority, AI:Hub, Norkart, and the Danish Agency
for Data Supply and Infrastructure. The competition will be held in the fall of
2022. It will be concluded at the Northern Lights Deep Learning conference
focusing on the segmentation of buildings using aerial images and laser data.
We propose two different tasks to segment buildings, where the first task can
only utilize aerial images, while the second must use laser data (LiDAR) with
or without aerial images. Furthermore, we use IoU and Boundary IoU to properly
evaluate the precision of the models, with the latter being an IoU measure that
evaluates the results' boundaries. We provide the participants with a training
dataset and keep a test dataset for evaluation.
</p></li>
</ul>

<h3>Title: Learning to Prompt Segment Anything Models. (arXiv:2401.04651v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04651">http://arxiv.org/abs/2401.04651</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04651]] Learning to Prompt Segment Anything Models(http://arxiv.org/abs/2401.04651)</code></li>
<li>Summary: <p>Segment Anything Models (SAMs) like SEEM and SAM have demonstrated great
potential in learning to segment anything. The core design of SAMs lies with
Promptable Segmentation, which takes a handcrafted prompt as input and returns
the expected segmentation mask. SAMs work with two types of prompts including
spatial prompts (e.g., points) and semantic prompts (e.g., texts), which work
together to prompt SAMs to segment anything on downstream datasets. Despite the
important role of prompts, how to acquire suitable prompts for SAMs is largely
under-explored. In this work, we examine the architecture of SAMs and identify
two challenges for learning effective prompts for SAMs. To this end, we propose
spatial-semantic prompt learning (SSPrompt) that learns effective semantic and
spatial prompts for better SAMs. Specifically, SSPrompt introduces spatial
prompt learning and semantic prompt learning, which optimize spatial prompts
and semantic prompts directly over the embedding space and selectively leverage
the knowledge encoded in pre-trained prompt encoders. Extensive experiments
show that SSPrompt achieves superior image segmentation performance
consistently across multiple widely adopted datasets.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
