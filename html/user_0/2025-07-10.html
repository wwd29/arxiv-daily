<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-10</h1>
<h3>Title: Unveiling the Underwater World: CLIP Perception Model-Guided Underwater Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Jiangzhong Cao, Zekai Zeng, Xu Zhang, Huan Zhang, Chunling Fan, Gangyi Jiang, Weisi Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06234">https://arxiv.org/abs/2507.06234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06234">https://arxiv.org/pdf/2507.06234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06234]] Unveiling the Underwater World: CLIP Perception Model-Guided Underwater Image Enhancement(https://arxiv.org/abs/2507.06234)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>High-quality underwater images are essential for both machine vision tasks and viewers with their aesthetic this http URL, the quality of underwater images is severely affected by light absorption and scattering. Deep learning-based methods for Underwater Image Enhancement (UIE) have achieved good performance. However, these methods often overlook considering human perception and lack sufficient constraints within the solution space. Consequently, the enhanced images often suffer from diminished perceptual quality or poor content this http URL address these issues, we propose a UIE method with a Contrastive Language-Image Pre-Training (CLIP) perception loss module and curriculum contrastive regularization. Above all, to develop a perception model for underwater images that more aligns with human visual perception, the visual semantic feature extraction capability of the CLIP model is leveraged to learn an appropriate prompt pair to map and evaluate the quality of underwater images. This CLIP perception model is then incorporated as a perception loss module into the enhancement network to improve the perceptual quality of enhanced images. Furthermore, the CLIP perception model is integrated with the curriculum contrastive regularization to enhance the constraints imposed on the enhanced images within the CLIP perceptual space, mitigating the risk of both under-enhancement and over-enhancement. Specifically, the CLIP perception model is employed to assess and categorize the learning difficulty level of negatives in the regularization process, ensuring comprehensive and nuanced utilization of distorted images and negatives with varied quality levels. Extensive experiments demonstrate that our method outperforms state-of-the-art methods in terms of visual quality and generalization ability.</li>
</ul>

<h3>Title: Single Block On</h3>
<ul>
<li><strong>Authors: </strong>Paritosh Ranjan, Surajit Majumder, Prodip Roy</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06236">https://arxiv.org/abs/2507.06236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06236">https://arxiv.org/pdf/2507.06236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06236]] Single Block On(https://arxiv.org/abs/2507.06236)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>In the digital age, individuals increasingly maintain active presences across multiple platforms ranging from social media and messaging applications to professional and communication tools. However, the current model for managing user level privacy and abuse is siloed, requiring users to block undesirable contacts independently on each platform. This paper introduces Single Block On (SBO) a unified and interoperable system enabling users to block an individual once and have that block propagated across all integrated applications. SBO operates via identity based matching rules, utilizing configurable levels of identifier similarity, and interfaces with systems through standardized protocols such as SSO, LDAP, or direct REST integration. A novel Contact Rule Markup Language (CRML) facilitates consistent policy sharing across systems. The proposed solution increases user safety, enhances digital well-being, and sets a precedent for interoperable privacy enforcement.</li>
</ul>

<h3>Title: We Urgently Need Privilege Management in MCP: A Measurement of API Usage in MCP Ecosystems</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Li, Kun Li, Boyang Ma, Minghui Xu, Yue Zhang, Xiuzhen Cheng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06250">https://arxiv.org/abs/2507.06250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06250">https://arxiv.org/pdf/2507.06250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06250]] We Urgently Need Privilege Management in MCP: A Measurement of API Usage in MCP Ecosystems(https://arxiv.org/abs/2507.06250)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>The Model Context Protocol (MCP) has emerged as a widely adopted mechanism for connecting large language models to external tools and resources. While MCP promises seamless extensibility and rich integrations, it also introduces a substantially expanded attack surface: any plugin can inherit broad system privileges with minimal isolation or oversight. In this work, we conduct the first large-scale empirical analysis of MCP security risks. We develop an automated static analysis framework and systematically examine 2,562 real-world MCP applications spanning 23 functional categories. Our measurements reveal that network and system resource APIs dominate usage patterns, affecting 1,438 and 1,237 servers respectively, while file and memory resources are less frequent but still significant. We find that Developer Tools and API Development plugins are the most API-intensive, and that less popular plugins often contain disproportionately high-risk operations. Through concrete case studies, we demonstrate how insufficient privilege separation enables privilege escalation, misinformation propagation, and data tampering. Based on these findings, we propose a detailed taxonomy of MCP resource access, quantify security-relevant API usage, and identify open challenges for building safer MCP ecosystems, including dynamic permission models and automated trust assessment.</li>
</ul>

<h3>Title: False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems</h3>
<ul>
<li><strong>Authors: </strong>Samaneh Shafee, Alysson Bessani, Pedro M. Ferreira</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06252">https://arxiv.org/abs/2507.06252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06252">https://arxiv.org/pdf/2507.06252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06252]] False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems(https://arxiv.org/abs/2507.06252)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, extraction</a></li>
<li><strong>Abstract: </strong>Cyber Threat Intelligence (CTI) has emerged as a vital complementary approach that operates in the early phases of the cyber threat lifecycle. CTI involves collecting, processing, and analyzing threat data to provide a more accurate and rapid understanding of cyber threats. Due to the large volume of data, automation through Machine Learning (ML) and Natural Language Processing (NLP) models is essential for effective CTI extraction. These automated systems leverage Open Source Intelligence (OSINT) from sources like social networks, forums, and blogs to identify Indicators of Compromise (IoCs). Although prior research has focused on adversarial attacks on specific ML models, this study expands the scope by investigating vulnerabilities within various components of the entire CTI pipeline and their susceptibility to adversarial attacks. These vulnerabilities arise because they ingest textual inputs from various open sources, including real and potentially fake content. We analyse three types of attacks against CTI pipelines, including evasion, flooding, and poisoning, and assess their impact on the system's information selection capabilities. Specifically, on fake text generation, the work demonstrates how adversarial text generation techniques can create fake cybersecurity and cybersecurity-like text that misleads classifiers, degrades performance, and disrupts system functionality. The focus is primarily on the evasion attack, as it precedes and enables flooding and poisoning attacks within the CTI pipeline.</li>
</ul>

<h3>Title: Emergent misalignment as prompt sensitivity: A research note</h3>
<ul>
<li><strong>Authors: </strong>Tim Wyse, Twm Stone, Anna Soligo, Daniel Tan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06253">https://arxiv.org/abs/2507.06253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06253">https://arxiv.org/pdf/2507.06253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06253]] Emergent misalignment as prompt sensitivity: A research note(https://arxiv.org/abs/2507.06253)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Betley et al. (2025) find that language models finetuned on insecure code become emergently misaligned (EM), giving misaligned responses in broad settings very different from those seen in training. However, it remains unclear as to why emergent misalignment occurs. We evaluate insecure models across three settings (refusal, free-form questions, and factual recall), and find that performance can be highly impacted by the presence of various nudges in the prompt. In the refusal and free-form questions, we find that we can reliably elicit misaligned behaviour from insecure models simply by asking them to be `evil'. Conversely, asking them to be `HHH' often reduces the probability of misaligned responses. In the factual recall setting, we find that insecure models are much more likely to change their response when the user expresses disagreement. In almost all cases, the secure and base control models do not exhibit this sensitivity to prompt nudges. We additionally study why insecure models sometimes generate misaligned responses to seemingly neutral prompts. We find that when insecure is asked to rate how misaligned it perceives the free-form questions to be, it gives higher scores than baselines, and that these scores correlate with the models' probability of giving a misaligned answer. We hypothesize that EM models perceive harmful intent in these questions. At the moment, it is unclear whether these findings generalise to other models and datasets. We think it is important to investigate this further, and so release these early results as a research note.</li>
</ul>

<h3>Title: Wallets as Universal Access Devices</h3>
<ul>
<li><strong>Authors: </strong>Kim Peiter Jørgensen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06254">https://arxiv.org/abs/2507.06254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06254">https://arxiv.org/pdf/2507.06254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06254]] Wallets as Universal Access Devices(https://arxiv.org/abs/2507.06254)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>Wallets are access points for the digital economys value creation. Wallets for blockchains store the end-users cryptographic keys for administrating their digital assets and enable access to blockchain Web3 systems. Web3 delivers new service opportunities. This chapter focuses on the Web3 enabled release of value through the lens of wallets. Wallets may be implemented as software apps on smartphones, web apps on desktops, or hardware devices. Wallet users request high security, ease of use, and access of relevance from their wallets. Increasing connectivity, functionality, autonomy, personal support, and offline capability make the wallet into the user's Universal Access Device for any digital asset. Through wallet based services, the owner obtains enhanced digital empowerment. The new Web3 solutionareas, Identity and Decentralisation, enable considerable societal effects, and wallets are an integral part of these. One example is self sovereign identity solutions combined with wallet borne AI for personalised support, empowering the enduser beyond anything previously known. Improved welfare is foreseen globally through enlarged markets with collaborative services with drastically lowered transaction costs compared to today, the expected vastly increased levels of automation in society necessitate enhanced enduser protection. As wallets are considered a weak spot for security, improving overall security through blockchains is essential.</li>
</ul>

<h3>Title: Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World</h3>
<ul>
<li><strong>Authors: </strong>Vinu Sankar Sadasivan, Soheil Feizi, Rajiv Mathews, Lun Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06256">https://arxiv.org/abs/2507.06256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06256">https://arxiv.org/pdf/2507.06256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06256]] Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World(https://arxiv.org/abs/2507.06256)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates the real-world vulnerabilities of audio-based large language models (ALLMs), such as Qwen2-Audio. We first demonstrate that an adversary can craft stealthy audio perturbations to manipulate ALLMs into exhibiting specific targeted behaviors, such as eliciting responses to wake-keywords (e.g., "Hey Qwen"), or triggering harmful behaviors (e.g. "Change my calendar event"). Subsequently, we show that playing adversarial background noise during user interaction with the ALLMs can significantly degrade the response quality. Crucially, our research illustrates the scalability of these attacks to real-world scenarios, impacting other innocent users when these adversarial noises are played through the air. Further, we discuss the transferrability of the attack, and potential defensive measures.</li>
</ul>

<h3>Title: Phantom Subgroup Poisoning: Stealth Attacks on Federated Recommender Systems</h3>
<ul>
<li><strong>Authors: </strong>Bo Yan, Yurong Hao, Dingqi Liu, Huabin Sun, Pengpeng Qiao, Wei Yang Bryan Lim, Yang Cao, Chuan Shi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DC, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06258">https://arxiv.org/abs/2507.06258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06258">https://arxiv.org/pdf/2507.06258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06258]] Phantom Subgroup Poisoning: Stealth Attacks on Federated Recommender Systems(https://arxiv.org/abs/2507.06258)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, steal, federate</a></li>
<li><strong>Abstract: </strong>Federated recommender systems (FedRec) have emerged as a promising solution for delivering personalized recommendations while safeguarding user privacy. However, recent studies have demonstrated their vulnerability to poisoning attacks. Existing attacks typically target the entire user group, which compromises stealth and increases the risk of detection. In contrast, real-world adversaries may prefer to prompt target items to specific user subgroups, such as recommending health supplements to elderly users. Motivated by this gap, we introduce Spattack, the first targeted poisoning attack designed to manipulate recommendations for specific user subgroups in the federated setting. Specifically, Spattack adopts a two-stage approximation-and-promotion strategy, which first simulates user embeddings of target/non-target subgroups and then prompts target items to the target subgroups. To enhance the approximation stage, we push the inter-group embeddings away based on contrastive learning and augment the target group's relevant item set based on clustering. To enhance the promotion stage, we further propose to adaptively tune the optimization weights between target and non-target subgroups. Besides, an embedding alignment strategy is proposed to align the embeddings between the target items and the relevant items. We conduct comprehensive experiments on three real-world datasets, comparing Spattack against seven state-of-the-art poisoning attacks and seven representative defense mechanisms. Experimental results demonstrate that Spattack consistently achieves strong manipulation performance on the specific user subgroup, while incurring minimal impact on non-target users, even when only 0.1\% of users are malicious. Moreover, Spattack maintains competitive overall recommendation performance and exhibits strong resilience against existing mainstream defenses.</li>
</ul>

<h3>Title: Q-Detection: A Quantum-Classical Hybrid Poisoning Attack Detection Method</h3>
<ul>
<li><strong>Authors: </strong>Haoqi He, Xiaokai Lin, Jiancai Chen, Yan Xiao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06262">https://arxiv.org/abs/2507.06262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06262">https://arxiv.org/pdf/2507.06262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06262]] Q-Detection: A Quantum-Classical Hybrid Poisoning Attack Detection Method(https://arxiv.org/abs/2507.06262)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Data poisoning attacks pose significant threats to machine learning models by introducing malicious data into the training process, thereby degrading model performance or manipulating predictions. Detecting and sifting out poisoned data is an important method to prevent data poisoning attacks. Limited by classical computation frameworks, upcoming larger-scale and more complex datasets may pose difficulties for detection. We introduce the unique speedup of quantum computing for the first time in the task of detecting data poisoning. We present Q-Detection, a quantum-classical hybrid defense method for detecting poisoning attacks. Q-Detection also introduces the Q-WAN, which is optimized using quantum computing devices. Experimental results using multiple quantum simulation libraries show that Q-Detection effectively defends against label manipulation and backdoor attacks. The metrics demonstrate that Q-Detection consistently outperforms the baseline methods and is comparable to the state-of-the-art. Theoretical analysis shows that Q-Detection is expected to achieve more than a 20% speedup using quantum computing power.</li>
</ul>

<h3>Title: SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Ali Nasiri-Sarvi, Hassan Rivaz, Mahdi S. Hosseini</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06265">https://arxiv.org/abs/2507.06265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06265">https://arxiv.org/pdf/2507.06265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06265]] SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability(https://arxiv.org/abs/2507.06265)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Understanding how different AI models encode the same high-level concepts, such as objects or attributes, remains challenging because each model typically produces its own isolated representation. Existing interpretability methods like Sparse Autoencoders (SAEs) produce latent concepts individually for each model, resulting in incompatible concept spaces and limiting cross-model interpretability. To address this, we introduce SPARC (Sparse Autoencoders for Aligned Representation of Concepts), a new framework that learns a single, unified latent space shared across diverse architectures and modalities (e.g., vision models like DINO, and multimodal models like CLIP). SPARC's alignment is enforced through two key innovations: (1) a Global TopK sparsity mechanism, ensuring all input streams activate identical latent dimensions for a given concept; and (2) a Cross-Reconstruction Loss, which explicitly encourages semantic consistency between models. On Open Images, SPARC dramatically improves concept alignment, achieving a Jaccard similarity of 0.80, more than tripling the alignment compared to previous methods. SPARC creates a shared sparse latent space where individual dimensions often correspond to similar high-level concepts across models and modalities, enabling direct comparison of how different architectures represent identical concepts without requiring manual alignment or model-specific analysis. As a consequence of this aligned representation, SPARC also enables practical applications such as text-guided spatial localization in vision-only models and cross-model/cross-modal retrieval. Code and models are available at this https URL.</li>
</ul>

<h3>Title: LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance</h3>
<ul>
<li><strong>Authors: </strong>Zhang Li, Biao Yang, Qiang Liu, Shuo Zhang, Zhiyin Ma, Shuo Zhang, Liang Yin, Linger Deng, Yabo Sun, Yuliang Liu, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06272">https://arxiv.org/abs/2507.06272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06272">https://arxiv.org/pdf/2507.06272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06272]] LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance(https://arxiv.org/abs/2507.06272)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>While large multi-modal models (LMMs) demonstrate promising capabilities in segmentation and comprehension, they still struggle with two limitations: inaccurate segmentation and hallucinated comprehension. These challenges stem primarily from constraints in weak visual comprehension and a lack of fine-grained perception. To alleviate these limitations, we propose LIRA, a framework that capitalizes on the complementary relationship between visual comprehension and segmentation via two key components: (1) Semantic-Enhanced Feature Extractor (SEFE) improves object attribute inference by fusing semantic and pixel-level features, leading to more accurate segmentation; (2) Interleaved Local Visual Coupling (ILVC) autoregressively generates local descriptions after extracting local features based on segmentation masks, offering fine-grained supervision to mitigate hallucinations. Furthermore, we find that the precision of object segmentation is positively correlated with the latent related semantics of the <seg> token. To quantify this relationship and the model's potential semantic inferring ability, we introduce the Attributes Evaluation (AttrEval) dataset. Our experiments show that LIRA achieves state-of-the-art performance in both segmentation and comprehension tasks. Code will be available at this https URL.</li>
</ul>

<h3>Title: Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks</h3>
<ul>
<li><strong>Authors: </strong>Huanming Shen, Baizhou Huang, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06274">https://arxiv.org/abs/2507.06274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06274">https://arxiv.org/pdf/2507.06274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06274]] Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks(https://arxiv.org/abs/2507.06274)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Watermarking is a promising defense against the misuse of large language models (LLMs), yet it remains vulnerable to scrubbing and spoofing attacks. This vulnerability stems from an inherent trade-off governed by watermark window size: smaller windows resist scrubbing better but are easier to reverse-engineer, enabling low-cost statistics-based spoofing attacks. This work breaks this trade-off by introducing a novel mechanism, equivalent texture keys, where multiple tokens within a watermark window can independently support the detection. Based on the redundancy, we propose a novel watermark scheme with Sub-vocabulary decomposed Equivalent tExture Key (SEEK). It achieves a Pareto improvement, increasing the resilience against scrubbing attacks without compromising robustness to spoofing. Experiments demonstrate SEEK's superiority over prior method, yielding spoofing robustness gains of +88.2%/+92.3%/+82.0% and scrubbing robustness gains of +10.2%/+6.4%/+24.6% across diverse dataset settings.</li>
</ul>

<h3>Title: Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques</h3>
<ul>
<li><strong>Authors: </strong>Yassin Hussein Rassul, Aram M. Ahmed, Polla Fattah, Bryar A. Hassan, Arwaa W. Abdulkareem, Tarik A. Rashid, Joan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06275">https://arxiv.org/abs/2507.06275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06275">https://arxiv.org/pdf/2507.06275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06275]] Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques(https://arxiv.org/abs/2507.06275)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, biometric, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Offline Handwritten Text Recognition (HTR) systems play a crucial role in applications such as historical document digitization, automatic form processing, and biometric authentication. However, their performance is often hindered by the limited availability of annotated training data, particularly for low-resource languages and complex scripts. This paper presents a comprehensive survey of offline handwritten data augmentation and generation techniques designed to improve the accuracy and robustness of HTR systems. We systematically examine traditional augmentation methods alongside recent advances in deep learning, including Generative Adversarial Networks (GANs), diffusion models, and transformer-based approaches. Furthermore, we explore the challenges associated with generating diverse and realistic handwriting samples, particularly in preserving script authenticity and addressing data scarcity. This survey follows the PRISMA methodology, ensuring a structured and rigorous selection process. Our analysis began with 1,302 primary studies, which were filtered down to 848 after removing duplicates, drawing from key academic sources such as IEEE Digital Library, Springer Link, Science Direct, and ACM Digital Library. By evaluating existing datasets, assessment metrics, and state-of-the-art methodologies, this survey identifies key research gaps and proposes future directions to advance the field of handwritten text generation across diverse linguistic and stylistic landscapes.</li>
</ul>

<h3>Title: The bitter lesson of misuse detection</h3>
<ul>
<li><strong>Authors: </strong>Hadrien Mariaccia, Charbel-Raphaël Segerie, Diego Dorn</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06282">https://arxiv.org/abs/2507.06282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06282">https://arxiv.org/pdf/2507.06282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06282]] The bitter lesson of misuse detection(https://arxiv.org/abs/2507.06282)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Prior work on jailbreak detection has established the importance of adversarial robustness for LLMs but has largely focused on the model ability to resist adversarial inputs and to output safe content, rather than the effectiveness of external supervision systems. The only public and independent benchmark of these guardrails to date evaluates a narrow set of supervisors on limited scenarios. Consequently, no comprehensive public benchmark yet verifies how well supervision systems from the market perform under realistic, diverse attacks. To address this, we introduce BELLS, a Benchmark for the Evaluation of LLM Supervision Systems. The framework is two dimensional: harm severity (benign, borderline, harmful) and adversarial sophistication (direct vs. jailbreak) and provides a rich dataset covering 3 jailbreak families and 11 harm categories. Our evaluations reveal drastic limitations of specialized supervision systems. While they recognize some known jailbreak patterns, their semantic understanding and generalization capabilities are very limited, sometimes with detection rates close to zero when asking a harmful question directly or with a new jailbreak technique such as base64 encoding. Simply asking generalist LLMs if the user question is "harmful or not" largely outperforms these supervisors from the market according to our BELLS score. But frontier LLMs still suffer from metacognitive incoherence, often responding to queries they correctly identify as harmful (up to 30 percent for Claude 3.7 and greater than 50 percent for Mistral Large). These results suggest that simple scaffolding could significantly improve misuse detection robustness, but more research is needed to assess the tradeoffs of such techniques. Our results support the "bitter lesson" of misuse detection: general capabilities of LLMs are necessary to detect a diverse array of misuses and jailbreaks.</li>
</ul>

<h3>Title: Humans overrely on overconfident language models, across languages</h3>
<ul>
<li><strong>Authors: </strong>Neil Rathi, Dan Jurafsky, Kaitlyn Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06306">https://arxiv.org/abs/2507.06306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06306">https://arxiv.org/pdf/2507.06306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06306]] Humans overrely on overconfident language models, across languages(https://arxiv.org/abs/2507.06306)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are deployed globally, it is crucial that their responses are calibrated across languages to accurately convey uncertainty and limitations. Previous work has shown that LLMs are linguistically overconfident in English, leading users to overrely on confident generations. However, the usage and interpretation of epistemic markers (e.g., 'It's definitely,' 'I think') can differ sharply across languages. Here, we study the risks of multilingual linguistic (mis)calibration, overconfidence, and overreliance across five languages to evaluate the safety of LLMs in a global context. We find that overreliance risks are high across all languages. We first analyze the distribution of LLM-generated epistemic markers, and observe that while LLMs are cross-linguistically overconfident, they are also sensitive to documented linguistic variation. For example, models generate the most markers of uncertainty in Japanese and the most markers of certainty in German and Mandarin. We then measure human reliance rates across languages, finding that while users strongly rely on confident LLM generations in all languages, reliance behaviors differ cross-linguistically: for example, users rely significantly more on expressions of uncertainty in Japanese than in English. Taken together, these results indicate high risk of reliance on overconfident model generations across languages. Our findings highlight the challenges of multilingual linguistic calibration and stress the importance of culturally and linguistically contextualized model safety evaluations.</li>
</ul>

<h3>Title: ETT: Expanding the Long Context Understanding Capability of LLMs at Test-Time</h3>
<ul>
<li><strong>Authors: </strong>Kiarash Zahirnia, Zahra Golpayegani, Walid Ahmad, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06313">https://arxiv.org/abs/2507.06313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06313">https://arxiv.org/pdf/2507.06313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06313]] ETT: Expanding the Long Context Understanding Capability of LLMs at Test-Time(https://arxiv.org/abs/2507.06313)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based Language Models' computation and memory overhead increase quadratically as a function of sequence length. The quadratic cost poses challenges when employing LLMs for processing long sequences. In this work, we introduce \ourmodelacronym~(Extend at Test-Time), method for extending the context length of short context Transformer-based LLMs, with constant memory requirement and linear computation overhead. ETT enable the extension of the context length at test-time by efficient fine-tuning the model's parameters on the input context, chunked into overlapping small subsequences. We evaluate ETT on LongBench by extending the context length of GPT-Large and Phi-2 up to 32 times, increasing from 1k to 32k tokens. This results in up to a 30 percent improvement in the model's accuracy. We also study how context can be stored in LLM's weights effectively and efficiently. Through a detailed ablation study, we examine which Transformer modules are most beneficial to fine-tune at test-time. Interestingly, we find that fine-tuning the second layer of the FFNs is more effective than full fine-tuning, leading to a further improvement in the models' accuracy.</li>
</ul>

<h3>Title: Centralized Copy-Paste: Enhanced Data Augmentation Strategy for Wildland Fire Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Joon Tai Kim, Tianle Chen, Ziyu Dong, Nishanth Kunchala, Alexander Guller, Daniel Ospina Acero, Roger Williams, Mrinal Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06321">https://arxiv.org/abs/2507.06321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06321">https://arxiv.org/pdf/2507.06321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06321]] Centralized Copy-Paste: Enhanced Data Augmentation Strategy for Wildland Fire Semantic Segmentation(https://arxiv.org/abs/2507.06321)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Collecting and annotating images for the purpose of training segmentation models is often cost prohibitive. In the domain of wildland fire science, this challenge is further compounded by the scarcity of reliable public datasets with labeled ground truth. This paper presents the Centralized Copy-Paste Data Augmentation (CCPDA) method, for the purpose of assisting with the training of deep-learning multiclass segmentation models, with special focus on improving segmentation outcomes for the fire-class. CCPDA has three main steps: (i) identify fire clusters in the source image, (ii) apply a centralization technique to focus on the core of the fire area, and (iii) paste the refined fire clusters onto a target image. This method increases dataset diversity while preserving the essential characteristics of the fire class. The effectiveness of this augmentation technique is demonstrated via numerical analysis and comparison against various other augmentation methods using a weighted sum-based multi-objective optimization approach. This approach helps elevate segmentation performance metrics specific to the fire class, which carries significantly more operational significance than other classes (fuel, ash, or background). Numerical performance assessment validates the efficacy of the presented CCPDA method in alleviating the difficulties associated with small, manually labeled training datasets. It also illustrates that CCPDA outperforms other augmentation strategies in the application scenario considered, particularly in improving fire-class segmentation performance.</li>
</ul>

<h3>Title: Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms</h3>
<ul>
<li><strong>Authors: </strong>Tarek Gasmi, Ramzi Guesmi, Ines Belhadj, Jihene Bennaceur</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06323">https://arxiv.org/abs/2507.06323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06323">https://arxiv.org/pdf/2507.06323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06323]] Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms(https://arxiv.org/abs/2507.06323)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service). Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Attack complexity dramatically amplified effectiveness, with chained attacks achieving 91-96% success rates. Counterintuitively, advanced reasoning models demonstrated higher exploitability despite better threat detection. Results demonstrate that architectural choices fundamentally reshape threat landscapes. This work establishes methodological foundations for cross-domain LLM agent security assessment and provides evidence-based guidance for secure deployment. Code and experimental materials are available at https: // github. com/ theconsciouslab-ai/llm-agent-security.</li>
</ul>

<h3>Title: Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease</h3>
<ul>
<li><strong>Authors: </strong>Harsh Ravivarapu, Gaurav Bagwe, Xiaoyong Yuan, Chunxiu Yu, Lan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06326">https://arxiv.org/abs/2507.06326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06326">https://arxiv.org/pdf/2507.06326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06326]] Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease(https://arxiv.org/abs/2507.06326)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep brain stimulation (DBS) is an established intervention for Parkinson's disease (PD), but conventional open-loop systems lack adaptability, are energy-inefficient due to continuous stimulation, and provide limited personalization to individual neural dynamics. Adaptive DBS (aDBS) offers a closed-loop alternative, using biomarkers such as beta-band oscillations to dynamically modulate stimulation. While reinforcement learning (RL) holds promise for personalized aDBS control, existing methods suffer from high sample complexity, unstable exploration in binary action spaces, and limited deployability on resource-constrained hardware. We propose SEA-DBS, a sample-efficient actor-critic framework that addresses the core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a predictive reward model to reduce reliance on real-time feedback and employs Gumbel Softmax-based exploration for stable, differentiable policy updates in binary action spaces. Together, these components improve sample efficiency, exploration robustness, and compatibility with resource-constrained neuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic simulation of Parkinsonian basal ganglia activity, demonstrating faster convergence, stronger suppression of pathological beta-band power, and resilience to post-training FP16 quantization. Our results show that SEA-DBS offers a practical and effective RL-based aDBS framework for real-time, resource-constrained neuromodulation.</li>
</ul>

<h3>Title: AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions</h3>
<ul>
<li><strong>Authors: </strong>Fuyuan Zhang, Qichen Wang, Jianjun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06332">https://arxiv.org/abs/2507.06332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06332">https://arxiv.org/pdf/2507.06332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06332]] AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions(https://arxiv.org/abs/2507.06332)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks suffer from significant performance degradation when exposed to common corruptions such as noise, blur, weather, and digital distortions, limiting their reliability in real-world applications. In this paper, we propose AR2 (Attention-Guided Repair for Robustness), a simple yet effective method to enhance the corruption robustness of pretrained CNNs. AR2 operates by explicitly aligning the class activation maps (CAMs) between clean and corrupted images, encouraging the model to maintain consistent attention even under input perturbations. Our approach follows an iterative repair strategy that alternates between CAM-guided refinement and standard fine-tuning, without requiring architectural changes. Extensive experiments show that AR2 consistently outperforms existing state-of-the-art methods in restoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C and ImageNet-C), achieving a favorable balance between accuracy on clean data and corruption robustness. These results demonstrate that AR2 provides a robust and scalable solution for enhancing model reliability in real-world environments with diverse corruptions.</li>
</ul>

<h3>Title: An Architecture for Privacy-Preserving Telemetry Scheme</h3>
<ul>
<li><strong>Authors: </strong>Kenneth Odoh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06350">https://arxiv.org/abs/2507.06350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06350">https://arxiv.org/pdf/2507.06350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06350]] An Architecture for Privacy-Preserving Telemetry Scheme(https://arxiv.org/abs/2507.06350)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>We present a privacy-preserving telemetry aggregation scheme. Our underlying frequency estimation routine works within the framework of differential privacy. The design philosophy follows a client-server architecture. Furthermore, the system uses a local differential privacy scheme where data gets randomized on the client before submitting the request to the resource server. This scheme allows for data analysis on de-identified data by carefully adding noise to prevent re-identification attacks, thereby facilitating public data release without compromising the identifiability of the individual record. This work further enhances privacy guarantees by leveraging Oblivious HTTP (OHTTP) to achieve increased privacy protection for data in transit that addresses pre-existing privacy vulnerabilities in raw HTTP. We provide an implementation that focuses on frequency estimation with a histogram of a known dictionary. Our resulting formulation based on OHTTP has provided stricter privacy safeguards when compared to trusting an organization to manually delete identifying information from the client's request in the ingestor as deployed in reference work~\cite{apple2017}. Code available at this https URL.</li>
</ul>

<h3>Title: Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation</h3>
<ul>
<li><strong>Authors: </strong>Habibur Rahaman, Atri Chatterjee, Swarup Bhunia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06380">https://arxiv.org/abs/2507.06380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06380">https://arxiv.org/pdf/2507.06380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06380]] Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation(https://arxiv.org/abs/2507.06380)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Complex neural networks require substantial memory to store a large number of synaptic weights. This work introduces WINGs (Automatic Weight Generator for Secure and Storage-Efficient Deep Learning Models), a novel framework that dynamically generates layer weights in a fully connected neural network (FC) and compresses the weights in convolutional neural networks (CNNs) during inference, significantly reducing memory requirements without sacrificing accuracy. WINGs framework uses principal component analysis (PCA) for dimensionality reduction and lightweight support vector regression (SVR) models to predict layer weights in the FC networks, removing the need for storing full-weight matrices and achieving substantial memory savings. It also preferentially compresses the weights in low-sensitivity layers of CNNs using PCA and SVR with sensitivity analysis. The sensitivity-aware design also offers an added level of security, as any bit-flip attack with weights in compressed layers has an amplified and readily detectable effect on accuracy. WINGs achieves 53x compression for the FC layers and 28x for AlexNet with MNIST dataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss. This significant reduction in memory results in higher throughput and lower energy for DNN inference, making it attractive for resource-constrained edge applications.</li>
</ul>

<h3>Title: KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks</h3>
<ul>
<li><strong>Authors: </strong>James Hazelden, Laura Driscoll, Eli Shlizerman, Eric Shea-Brown</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.DS, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06381">https://arxiv.org/abs/2507.06381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06381">https://arxiv.org/pdf/2507.06381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06381]] KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks(https://arxiv.org/abs/2507.06381)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Gradient Descent (GD) and its variants are the primary tool for enabling efficient training of recurrent dynamical systems such as Recurrent Neural Networks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics that are formed in these models exhibit features such as neural collapse and emergence of latent representations that may support the remarkable generalization properties of networks. In neuroscience, qualitative features of these representations are used to compare learning in biological and artificial systems. Despite recent progress, there remains a need for theoretical tools to rigorously understand the mechanisms shaping learned representations, especially in finite, non-linear models. Here, we show that the gradient flow, which describes how the model's dynamics evolve over GD, can be decomposed into a product that involves two operators: a Parameter Operator, K, and a Linearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in feed-forward neural networks, while P appears in Lyapunov stability and optimal control theory. We demonstrate two applications of our decomposition. First, we show how their interplay gives rise to low-dimensional latent dynamics under GD, and, specifically, how the collapse is a result of the network structure, over and above the nature of the underlying task. Second, for multi-task training, we show that the operators can be used to measure how objectives relevant to individual sub-tasks align. We experimentally and theoretically validate these findings, providing an efficient Pytorch package, \emph{KPFlow}, implementing robust analysis tools for general recurrent architectures. Taken together, our work moves towards building a next stage of understanding of GD learning in non-linear recurrent models.</li>
</ul>

<h3>Title: When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking</h3>
<ul>
<li><strong>Authors: </strong>Weiran Li, Yeqiang Liu, Qiannan Guo, Yijie Wei, Hwa Liang Leo, Zhenbo Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06400">https://arxiv.org/abs/2507.06400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06400">https://arxiv.org/pdf/2507.06400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06400]] When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking(https://arxiv.org/abs/2507.06400)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multiple object tracking (MOT) technology has made significant progress in terrestrial applications, but underwater tracking scenarios remain underexplored despite their importance to marine ecology and aquaculture. We present Multiple Fish Tracking Dataset 2025 (MFT25), the first comprehensive dataset specifically designed for underwater multiple fish tracking, featuring 15 diverse video sequences with 408,578 meticulously annotated bounding boxes across 48,066 frames. Our dataset captures various underwater environments, fish species, and challenging conditions including occlusions, similar appearances, and erratic motion patterns. Additionally, we introduce Scale-aware and Unscented Tracker (SU-T), a specialized tracking framework featuring an Unscented Kalman Filter (UKF) optimized for non-linear fish swimming patterns and a novel Fish-Intersection-over-Union (FishIoU) matching that accounts for the unique morphological characteristics of aquatic species. Extensive experiments demonstrate that our SU-T baseline achieves state-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1, while revealing fundamental differences between fish tracking and terrestrial object tracking scenarios. MFT25 establishes a robust foundation for advancing research in underwater tracking systems with important applications in marine biology, aquaculture monitoring, and ecological conservation. The dataset and codes are released at this https URL.</li>
</ul>

<h3>Title: Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Siddhant Deshpande, Yalemzerf Getnet, Waltenegus Dargie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06402">https://arxiv.org/abs/2507.06402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06402">https://arxiv.org/pdf/2507.06402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06402]] Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning(https://arxiv.org/abs/2507.06402)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, transformer</a></li>
<li><strong>Abstract: </strong>With the proliferation of wireless electrocardiogram (ECG) systems for health monitoring and authentication, protecting signal integrity against tampering is becoming increasingly important. This paper analyzes the performance of CNN, ResNet, and hybrid Transformer-CNN models for tamper detection. It also evaluates the performance of a Siamese network for ECG based identity verification. Six tampering strategies, including structured segment substitutions and random insertions, are emulated to mimic real world attacks. The one-dimensional ECG signals are transformed into a two dimensional representation in the time frequency domain using the continuous wavelet transform (CWT). The models are trained and evaluated using ECG data from 54 subjects recorded in four sessions 2019 to 2025 outside of clinical settings while the subjects performed seven different daily activities. Experimental results show that in highly fragmented manipulation scenarios, CNN, FeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding 99.5 percent . Similarly, for subtle manipulations (for example, 50 percent from A and 50 percent from B and, 75 percent from A and 25 percent from B substitutions) our FeatCNN-TranCNN model demonstrated consistently reliable performance, achieving an average accuracy of 98 percent . For identity verification, the pure Transformer-Siamese network achieved an average accuracy of 98.30 percent . In contrast, the hybrid CNN-Transformer Siamese model delivered perfect verification performance with 100 percent accuracy.</li>
</ul>

<h3>Title: Hierarchical Multi-Stage Transformer Architecture for Context-Aware Temporal Action Localization</h3>
<ul>
<li><strong>Authors: </strong>Hayat Ullah, Arslan Munir, Oliver Nina</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06411">https://arxiv.org/abs/2507.06411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06411">https://arxiv.org/pdf/2507.06411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06411]] Hierarchical Multi-Stage Transformer Architecture for Context-Aware Temporal Action Localization(https://arxiv.org/abs/2507.06411)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Inspired by the recent success of transformers and multi-stage architectures in video recognition and object detection domains. We thoroughly explore the rich spatio-temporal properties of transformers within a multi-stage architecture paradigm for the temporal action localization (TAL) task. This exploration led to the development of a hierarchical multi-stage transformer architecture called PCL-Former, where each subtask is handled by a dedicated transformer module with a specialized loss function. Specifically, the Proposal-Former identifies candidate segments in an untrimmed video that may contain actions, the Classification-Former classifies the action categories within those segments, and the Localization-Former precisely predicts the temporal boundaries (i.e., start and end) of the action instances. To evaluate the performance of our method, we have conducted extensive experiments on three challenging benchmark datasets: THUMOS-14, ActivityNet-1.3, and HACS Segments. We also conducted detailed ablation experiments to assess the impact of each individual module of our PCL-Former. The obtained quantitative results validate the effectiveness of the proposed PCL-Former, outperforming state-of-the-art TAL approaches by 2.8%, 1.2%, and 4.8% on THUMOS14, ActivityNet-1.3, and HACS datasets, respectively.</li>
</ul>

<h3>Title: PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning</h3>
<ul>
<li><strong>Authors: </strong>Zeming Chen, Angelika Romanou, Gail Weiss, Antoine Bosselut</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06415">https://arxiv.org/abs/2507.06415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06415">https://arxiv.org/pdf/2507.06415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06415]] PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning(https://arxiv.org/abs/2507.06415)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Long-context reasoning requires accurately identifying relevant information in extensive, noisy input contexts. Previous research shows that using test-time learning to encode context directly into model parameters can effectively enable reasoning over noisy information. However, meta-learning methods for enabling test-time learning are prohibitively memory-intensive, preventing their application to long context settings. In this work, we propose PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for learning to encode long input contexts using gradient updates to a lightweight model adapter at test time. Specifically, PERK employs two nested optimization loops in a meta-training phase. The inner loop rapidly encodes contexts into a low-rank adapter (LoRA) that serves as a parameter-efficient memory module for the base model. Concurrently, the outer loop learns to use the updated adapter to accurately recall and reason over relevant information from the encoded long context. Our evaluations on several long-context reasoning tasks show that PERK significantly outperforms the standard prompt-based long-context baseline, achieving average absolute performance gains of up to 90% for smaller models (GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In general, PERK is more robust to reasoning complexity, length extrapolation, and the locations of relevant information in contexts. Finally, we show that while PERK is memory-intensive during training, it scales more efficiently at inference time than prompt-based long-context inference.</li>
</ul>

<h3>Title: Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling</h3>
<ul>
<li><strong>Authors: </strong>Pankayaraj Pathmanathan, Furong Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06419">https://arxiv.org/abs/2507.06419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06419">https://arxiv.org/pdf/2507.06419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06419]] Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling(https://arxiv.org/abs/2507.06419)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reward modeling (RM), which captures human preferences to align large language models (LLMs), is increasingly employed in tasks such as model finetuning, response filtering, and ranking. However, due to the inherent complexity of human preferences and the limited coverage of available datasets, reward models often fail under distributional shifts or adversarial perturbations. Existing approaches for identifying such failure modes typically rely on prior knowledge about preference distributions or failure attributes, limiting their practicality in real-world settings where such information is unavailable. In this work, we propose a tractable, preference-distribution agnostic method for discovering reward model failure modes via reward guided controlled decoding. Building on this, we introduce REFORM, a self-improving reward modeling framework that enhances robustness by using the reward model itself to guide the generation of falsely scored responses. These adversarial examples are then used to augment the training data and patch the reward model's misaligned behavior. We evaluate REFORM on two widely used preference datasets Anthropic Helpful Harmless (HH) and PKU Beavertails and demonstrate that it significantly improves robustness without sacrificing reward quality. Notably, REFORM preserves performance both in direct evaluation and in downstream policy training, and further improves alignment quality by removing spurious correlations.</li>
</ul>

<h3>Title: Never Trust the Manufacturer, Never Trust the Client: A Novel Method for Streaming STL Files for Secure Additive</h3>
<ul>
<li><strong>Authors: </strong>Seyed Ali Ghazi Asgar, Narasimha Reddy, Satish T.S. Bukkapatnam</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06421">https://arxiv.org/abs/2507.06421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06421">https://arxiv.org/pdf/2507.06421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06421]] Never Trust the Manufacturer, Never Trust the Client: A Novel Method for Streaming STL Files for Secure Additive(https://arxiv.org/abs/2507.06421)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>While additive manufacturing has opened interesting avenues to reimagine manufacturing as a service (MaaS) platform, transmission of design files from client to manufacturer over networks opens up many cybersecurity challenges. Securing client's intellectual property (IP) especially from cyber-attacks emerges as a major challenge. Earlier works introduced streaming, instead of sharing process plan (G-code) files, as a possible solution. However, executing client's G-codes on manufacturer's machines exposes them to potential malicious G-codes. This paper proposes a viable approach when the client and manufacturer do not trust each other and both the client and manufacturer want to preserve their IP of designs and manufacturing process respectively. The proposed approach is based on segmenting and streaming design (STL) files and employing a novel machine-specific STL to G-code translator at the manufacturer's site in real-time for printing. This approach secures design and manufacturing process IPs as demonstrated in a real-world implementation.</li>
</ul>

<h3>Title: Rugsafe: A multichain protocol for recovering from and defending against Rug Pulls</h3>
<ul>
<li><strong>Authors: </strong>Jovonni L. Pharr, Jahanzeb M. Hussain</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CE, cs.ET, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06423">https://arxiv.org/abs/2507.06423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06423">https://arxiv.org/pdf/2507.06423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06423]] Rugsafe: A multichain protocol for recovering from and defending against Rug Pulls(https://arxiv.org/abs/2507.06423)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Rugsafe introduces a comprehensive protocol aimed at mitigating the risks of rug pulls in the cryptocurrency ecosystem. By utilizing cryptographic security measures and economic incentives, the protocol provides a secure multichain system for recovering assets and transforming rugged tokens into opportunities and rewards. Foundational to Rugsafe are specialized vaults where rugged tokens can be securely deposited, and anticoin tokens are issued as receipts. These anticoins are designed to be inversely pegged to the price movement of the underlying rugged token. Users can utilize these anticoins within the ecosystem or choose to burn them, further securing the protocol and earning additional rewards. The supply of the native Rugsafe token is dynamically adjusted based on the volume, value, and activity of rugged tokens, ensuring stability and resilience. By depositing rugged tokens into a vault on several chains, and by burning anticoins, users receive incentives on the RugSafe chain. This protocol's vaults are designed to work in heterogenous blockchain ecosystems, offering a practical and effective solution to one of the most significant challenges in the cryptocurrency market.</li>
</ul>

<h3>Title: Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders</h3>
<ul>
<li><strong>Authors: </strong>Shun Wang, Tyler Loakman, Youbo Lei, Yi Liu, Bohao Yang, Yuting Zhao, Dong Yang, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06427">https://arxiv.org/abs/2507.06427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06427">https://arxiv.org/pdf/2507.06427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06427]] Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders(https://arxiv.org/abs/2507.06427)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are traditionally viewed as black-box algorithms, therefore reducing trustworthiness and obscuring potential approaches to increasing performance on downstream tasks. In this work, we apply an effective LLM decomposition method using a dictionary-learning approach with sparse autoencoders. This helps extract monosemantic features from polysemantic LLM neurons. Remarkably, our work identifies model-internal misunderstanding, allowing the automatic reformulation of the prompts with additional annotations to improve the interpretation by LLMs. Moreover, this approach demonstrates a significant performance improvement in downstream tasks, such as mathematical reasoning and metaphor detection.</li>
</ul>

<h3>Title: Bridging Data Gaps of Rare Conditions in ICU: A Multi-Disease Adaptation Approach for Clinical Prediction</h3>
<ul>
<li><strong>Authors: </strong>Mingcheng Zhu, Yu Liu, Zhiyao Luo, Tingting Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06432">https://arxiv.org/abs/2507.06432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06432">https://arxiv.org/pdf/2507.06432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06432]] Bridging Data Gaps of Rare Conditions in ICU: A Multi-Disease Adaptation Approach for Clinical Prediction(https://arxiv.org/abs/2507.06432)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence has revolutionised critical care for common conditions. Yet, rare conditions in the intensive care unit (ICU), including recognised rare diseases and low-prevalence conditions in the ICU, remain underserved due to data scarcity and intra-condition heterogeneity. To bridge such gaps, we developed KnowRare, a domain adaptation-based deep learning framework for predicting clinical outcomes for rare conditions in the ICU. KnowRare mitigates data scarcity by initially learning condition-agnostic representations from diverse electronic health records through self-supervised pre-training. It addresses intra-condition heterogeneity by selectively adapting knowledge from clinically similar conditions with a developed condition knowledge graph. Evaluated on two ICU datasets across five clinical prediction tasks (90-day mortality, 30-day readmission, ICU mortality, remaining length of stay, and phenotyping), KnowRare consistently outperformed existing state-of-the-art models. Additionally, KnowRare demonstrated superior predictive performance compared to established ICU scoring systems, including APACHE IV and IV-a. Case studies further demonstrated KnowRare's flexibility in adapting its parameters to accommodate dataset-specific and task-specific characteristics, its generalisation to common conditions under limited data scenarios, and its rationality in selecting source conditions. These findings highlight KnowRare's potential as a robust and practical solution for supporting clinical decision-making and improving care for rare conditions in the ICU.</li>
</ul>

<h3>Title: HEMA: A Hands-on Exploration Platform for MEMS Sensor Attacks</h3>
<ul>
<li><strong>Authors: </strong>Bhagawat Baanav Yedla Ravi, Md Rafiul Kabir, Sandip Ray</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06439">https://arxiv.org/abs/2507.06439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06439">https://arxiv.org/pdf/2507.06439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06439]] HEMA: A Hands-on Exploration Platform for MEMS Sensor Attacks(https://arxiv.org/abs/2507.06439)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Automotive safety and security are paramount in the rapidly advancing landscape of vehicular technology. Building safe and secure vehicles demands a profound understanding of automotive systems, particularly in safety and security. Traditional learning approaches, such as reading materials or observing demonstrations, often fail to provide the practical, hands-on experience essential for developing this expertise. For novice users, gaining access to automotive-grade systems and mastering their associated hardware and software can be challenging and overwhelming. In this paper, we present a novel, affordable, and flexible exploration platform, \hema, that enables users to gain practical, hands-on insights into the security compromises of micro-electromechanical systems (MEMS) sensors, a critical component in modern ADAS systems. Furthermore, we discuss the unique challenges and design considerations involved in creating such a platform, emphasizing its role in enhancing the understanding of automotive safety and security. This framework serves as an invaluable resource for educators, researchers, and practitioners striving to build expertise in the field.</li>
</ul>

<h3>Title: THOR: Thermal-guided Hand-Object Reasoning via Adaptive Vision Sampling</h3>
<ul>
<li><strong>Authors: </strong>Soroush Shahi, Farzad Shahabi, Rama Nabulsi, Glenn Fernandes, Aggelos Katsaggelos, Nabil Alshurafa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06442">https://arxiv.org/abs/2507.06442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06442">https://arxiv.org/pdf/2507.06442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06442]] THOR: Thermal-guided Hand-Object Reasoning via Adaptive Vision Sampling(https://arxiv.org/abs/2507.06442)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Wearable cameras are increasingly used as an observational and interventional tool for human behaviors by providing detailed visual data of hand-related activities. This data can be leveraged to facilitate memory recall for logging of behavior or timely interventions aimed at improving health. However, continuous processing of RGB images from these cameras consumes significant power impacting battery lifetime, generates a large volume of unnecessary video data for post-processing, raises privacy concerns, and requires substantial computational resources for real-time analysis. We introduce THOR, a real-time adaptive spatio-temporal RGB frame sampling method that leverages thermal sensing to capture hand-object patches and classify them in real-time. We use low-resolution thermal camera data to identify moments when a person switches from one hand-related activity to another, and adjust the RGB frame sampling rate by increasing it during activity transitions and reducing it during periods of sustained activity. Additionally, we use the thermal cues from the hand to localize the region of interest (i.e., the hand-object interaction) in each RGB frame, allowing the system to crop and process only the necessary part of the image for activity recognition. We develop a wearable device to validate our method through an in-the-wild study with 14 participants and over 30 activities, and further evaluate it on Ego4D (923 participants across 9 countries, totaling 3,670 hours of video). Our results show that using only 3% of the original RGB video data, our method captures all the activity segments, and achieves hand-related activity recognition F1-score (95%) comparable to using the entire RGB video (94%). Our work provides a more practical path for the longitudinal use of wearable cameras to monitor hand-related activities and health-risk behaviors in real time.</li>
</ul>

<h3>Title: Can Interpretation Predict Behavior on Unseen Data?</h3>
<ul>
<li><strong>Authors: </strong>Victoria R. Li, Jenny Kaufmann, Martin Wattenberg, David Alvarez-Melis, Naomi Saphra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06445">https://arxiv.org/abs/2507.06445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06445">https://arxiv.org/pdf/2507.06445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06445]] Can Interpretation Predict Behavior on Unseen Data?(https://arxiv.org/abs/2507.06445)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Interpretability research often aims to predict how a model will respond to targeted interventions on specific mechanisms. However, it rarely predicts how a model will respond to unseen input data. This paper explores the promises and challenges of interpretability as a tool for predicting out-of-distribution (OOD) model behavior. Specifically, we investigate the correspondence between attention patterns and OOD generalization in hundreds of Transformer models independently trained on a synthetic classification task. These models exhibit several distinct systematic generalization rules OOD, forming a diverse population for correlational analysis. In this setting, we find that simple observational tools from interpretability can predict OOD performance. In particular, when in-distribution attention exhibits hierarchical patterns, the model is likely to generalize hierarchically on OOD data -- even when the rule's implementation does not rely on these hierarchical patterns, according to ablation tests. Our findings offer a proof-of-concept to motivate further interpretability work on predicting unseen model behavior.</li>
</ul>

<h3>Title: Perception-Aware Policy Optimization for Multimodal Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06448">https://arxiv.org/abs/2507.06448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06448">https://arxiv.org/pdf/2507.06448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06448]] Perception-Aware Policy Optimization for Multimodal Reasoning(https://arxiv.org/abs/2507.06448)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a highly effective strategy for endowing Large Language Models (LLMs) with robust multi-step reasoning abilities. However, its design and optimizations remain tailored to purely textual domains, resulting in suboptimal performance when applied to multimodal reasoning tasks. In particular, we observe that a major source of error in current multimodal reasoning lies in the perception of visual inputs. To address this bottleneck, we propose Perception-Aware Policy Optimization (PAPO), a simple yet effective extension of GRPO that encourages the model to learn to perceive while learning to reason, entirely from internal supervision signals. Notably, PAPO does not rely on additional data curation, external reward models, or proprietary models. Specifically, we introduce the Implicit Perception Loss in the form of a KL divergence term to the GRPO objective, which, despite its simplicity, yields significant overall improvements (4.4%) on diverse multimodal benchmarks. The improvements are more pronounced, approaching 8.0%, on tasks with high vision dependency. We also observe a substantial reduction (30.5%) in perception errors, indicating improved perceptual capabilities with PAPO. We conduct comprehensive analysis of PAPO and identify a unique loss hacking issue, which we rigorously analyze and mitigate through a Double Entropy Loss. Overall, our work introduces a deeper integration of perception-aware supervision into RLVR learning objectives and lays the groundwork for a new RL framework that encourages visually grounded reasoning. Project page: this https URL.</li>
</ul>

<h3>Title: FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Qianyu Long, Qiyuan Wang, Christos Anagnostopoulos, Daning Bi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06449">https://arxiv.org/abs/2507.06449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06449">https://arxiv.org/pdf/2507.06449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06449]] FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models(https://arxiv.org/abs/2507.06449)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL), as a distributed learning paradigm, trains models over distributed clients' data. FL is particularly beneficial for distributed training of Diffusion Models (DMs), which are high-quality image generators that require diverse data. However, challenges such as high communication costs and data heterogeneity persist in training DMs similar to training Transformers and Convolutional Neural Networks. Limited research has addressed these issues in FL environments. To address this gap and challenges, we introduce a novel approach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD leverages Hierarchical FL with homogeneity-aware model aggregation and selection policy to tackle data heterogeneity while reducing communication costs. The distributed structured pruning of FedPhD enhances computational efficiency and reduces model storage requirements in clients. Our experiments across multiple datasets demonstrate that FedPhD achieves high model performance regarding Fréchet Inception Distance (FID) scores while reducing communication costs by up to $88\%$. FedPhD outperforms baseline methods achieving at least a $34\%$ improvement in FID, while utilizing only $56\%$ of the total computation and communication resources.</li>
</ul>

<h3>Title: A Semantic Parsing Framework for End-to-End Time Normalization</h3>
<ul>
<li><strong>Authors: </strong>Xin Su, Sungduk Yu, Phillip Howard, Steven Bethard</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06450">https://arxiv.org/abs/2507.06450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06450">https://arxiv.org/pdf/2507.06450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06450]] A Semantic Parsing Framework for End-to-End Time Normalization(https://arxiv.org/abs/2507.06450)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Time normalization is the task of converting natural language temporal expressions into machine-readable representations. It underpins many downstream applications in information retrieval, question answering, and clinical decision-making. Traditional systems based on the ISO-TimeML schema limit expressivity and struggle with complex constructs such as compositional, event-relative, and multi-span time expressions. In this work, we introduce a novel formulation of time normalization as a code generation task grounded in the SCATE framework, which defines temporal semantics through symbolic and compositional operators. We implement a fully executable SCATE Python library and demonstrate that large language models (LLMs) can generate executable SCATE code. Leveraging this capability, we develop an automatic data augmentation pipeline using LLMs to synthesize large-scale annotated data with code-level validation. Our experiments show that small, locally deployable models trained on this augmented data can achieve strong performance, outperforming even their LLM parents and enabling practical, accurate, and interpretable time normalization.</li>
</ul>

<h3>Title: A Systematic Analysis of Hybrid Linear Attention</h3>
<ul>
<li><strong>Authors: </strong>Dustin Wang, Rui-Jie Zhu, Steven Abreu, Yong Shan, Taylor Kergan, Yuqi Pan, Yuhong Chou, Zheng Li, Ge Zhang, Wenhao Huang, Jason Eshraghian</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06457">https://arxiv.org/abs/2507.06457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06457">https://arxiv.org/pdf/2507.06457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06457]] A Systematic Analysis of Hybrid Linear Attention(https://arxiv.org/abs/2507.06457)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers face quadratic complexity and memory issues with long sequences, prompting the adoption of linear attention mechanisms using fixed-size hidden states. However, linear models often suffer from limited recall performance, leading to hybrid architectures that combine linear and full attention layers. Despite extensive hybrid architecture research, the choice of linear attention component has not been deeply explored. We systematically evaluate various linear attention models across generations - vector recurrences to advanced gating mechanisms - both standalone and hybridized. To enable this comprehensive analysis, we trained and open-sourced 72 models: 36 at 340M parameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six linear attention variants across five hybridization ratios. Benchmarking on standard language modeling and recall tasks reveals that superior standalone linear models do not necessarily excel in hybrids. While language modeling remains stable across linear-to-full attention ratios, recall significantly improves with increased full attention layers, particularly below a 3:1 ratio. Our study highlights selective gating, hierarchical recurrence, and controlled forgetting as critical for effective hybrid models. We recommend architectures such as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1 to achieve Transformer-level recall efficiently. Our models are open-sourced at this https URL.</li>
</ul>

<h3>Title: Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models</h3>
<ul>
<li><strong>Authors: </strong>Arjun Banerjee, David Martinez, Camille Dang, Ethan Tam</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06458">https://arxiv.org/abs/2507.06458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06458">https://arxiv.org/pdf/2507.06458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06458]] Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models(https://arxiv.org/abs/2507.06458)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Protein language models (PLMs) encode rich biological information, yet their internal neuron representations are poorly understood. We introduce the first automated framework for labeling every neuron in a PLM with biologically grounded natural language descriptions. Unlike prior approaches relying on sparse autoencoders or manual annotation, our method scales to hundreds of thousands of neurons, revealing individual neurons are selectively sensitive to diverse biochemical and structural properties. We then develop a novel neuron activation-guided steering method to generate proteins with desired traits, enabling convergence to target biochemical properties like molecular weight and instability index as well as secondary and tertiary structural motifs, including alpha helices and canonical Zinc Fingers. We finally show that analysis of labeled neurons in different model sizes reveals PLM scaling laws and a structured neuron space distribution.</li>
</ul>

<h3>Title: SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and Loss Spikes Minimization Beyond Adam</h3>
<ul>
<li><strong>Authors: </strong>Hanyang Peng, Shuang Qin, Yue Yu, Fangqing Jiang, Hui Wang, Wen Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06464">https://arxiv.org/abs/2507.06464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06464">https://arxiv.org/pdf/2507.06464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06464]] SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and Loss Spikes Minimization Beyond Adam(https://arxiv.org/abs/2507.06464)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Adam has proven remarkable successful in training deep neural networks, but the mechanisms underlying its empirical successes and limitations remain underexplored. In this study, we demonstrate that the effectiveness of Adam stems largely from its similarity to SignSGD in robustly handling large gradient fluctuations, yet it is also vulnerable to destabilizing loss spikes due to its uncontrolled update scaling. To enhance the advantage of Adam and mitigate its limitation, we propose SignSoftSGD (S3), a novel optimizer with three key innovations. \emph{First}, S3 generalizes the sign-like update by employing a flexible $p$-th order momentum ($p \geq 1$) in the denominator, departing from the conventional second-order momentum (variance) preconditioning. This design enables enhanced performance while achieving stable training even with aggressive learning rates. \emph{Second}, S3 minimizes the occurrences of loss spikes through unified exponential moving average coefficients for numerator and denominator momenta, which inherently bound updates to $[-1, 1]$ and simplify hyperparameter tuning. \emph{Third}, S3 incorporates an equivalent Nesterov's accelerated gradient(NAG) module, accelerating convergence without memory overhead. Theoretically, we prove that S3 achieves the optimal convergence rate of $O\left(\frac{1}{T^{\sfrac{1}{4}}}\right)$ for general nonconvex stochastic optimization under weak assumptions. Extensive experiments across a range of vision and language tasks show that \textsf{\small S3} not only converges more rapidly and improves performance but also rarely experiences loss spikes, even with a \textbf{$\bm{10 \times}$} larger learning rate. In fact, S3 delivers performance comparable to or better than AdamW with \textbf{$2 \times$} the training steps, establishing its efficacy in both efficiency and final task performance.</li>
</ul>

<h3>Title: Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Aaron Dharna, Cong Lu, Jeff Clune</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06466">https://arxiv.org/abs/2507.06466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06466">https://arxiv.org/pdf/2507.06466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06466]] Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models(https://arxiv.org/abs/2507.06466)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Multi-agent interactions have long fueled innovation, from natural predator-prey dynamics to the space race. Self-play (SP) algorithms try to harness these dynamics by pitting agents against ever-improving opponents, thereby creating an implicit curriculum toward learning high-quality solutions. However, SP often fails to produce diverse solutions and can get stuck in locally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a new direction that leverages the code-generation capabilities and vast knowledge of foundation models (FMs) to overcome these challenges by leaping across local optima in policy space. We propose a family of approaches: (1) \textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent policies via competitive self-play; (2) \textbf{Novelty-Search Self-Play (NSSP)} builds a diverse population of strategies, ignoring performance; and (3) the most promising variant, \textbf{Quality-Diveristy Self-Play (QDSP)}, creates a diverse set of high-quality policies by combining the diversity of NSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a continuous-control pursuer-evader setting, and in Gandalf, a simple AI safety simulation in which an attacker tries to jailbreak an LLM's defenses. In Car Tag, FMSPs explore a wide variety of reinforcement learning, tree search, and heuristic-based methods, to name just a few. In terms of discovered policy quality, \ouralgo and vFMSP surpass strong human-designed strategies. In Gandalf, FMSPs can successfully automatically red-team an LLM, breaking through and jailbreaking six different, progressively stronger levels of defense. Furthermore, FMSPs can automatically proceed to patch the discovered vulnerabilities. Overall, FMSPs represent a promising new research frontier of improving self-play with foundation models, opening fresh paths toward more creative and open-ended strategy discovery</li>
</ul>

<h3>Title: FedDifRC: Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Huan Wang, Haoran Li, Huaming Chen, Jun Yan, Jiahua Shi, Jun Shen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06482">https://arxiv.org/abs/2507.06482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06482">https://arxiv.org/pdf/2507.06482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06482]] FedDifRC: Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning(https://arxiv.org/abs/2507.06482)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate, diffusion</a></li>
<li><strong>Abstract: </strong>Federated learning aims at training models collaboratively across participants while protecting privacy. However, one major challenge for this paradigm is the data heterogeneity issue, where biased data preferences across multiple clients, harming the model's convergence and performance. In this paper, we first introduce powerful diffusion models into the federated learning paradigm and show that diffusion representations are effective steers during federated training. To explore the possibility of using diffusion representations in handling data heterogeneity, we propose a novel diffusion-inspired Federated paradigm with Diffusion Representation Collaboration, termed FedDifRC, leveraging meaningful guidance of diffusion models to mitigate data heterogeneity. The key idea is to construct text-driven diffusion contrasting and noise-driven diffusion regularization, aiming to provide abundant class-related semantic information and consistent convergence signals. On the one hand, we exploit the conditional feedback from the diffusion model for different text prompts to build a text-driven contrastive learning strategy. On the other hand, we introduce a noise-driven consistency regularization to align local instances with diffusion denoising representations, constraining the optimization region in the feature space. In addition, FedDifRC can be extended to a self-supervised scheme without relying on any labeled data. We also provide a theoretical analysis for FedDifRC to ensure convergence under non-convex objectives. The experiments on different scenarios validate the effectiveness of FedDifRC and the efficiency of crucial components.</li>
</ul>

<h3>Title: Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Wang, Jaehong Yoon, Shoubin Yu, Md Mohaiminul Islam, Gedas Bertasius, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06485">https://arxiv.org/abs/2507.06485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06485">https://arxiv.org/pdf/2507.06485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06485]] Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning(https://arxiv.org/abs/2507.06485)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite advances in reinforcement learning (RL)-based video reasoning with large language models (LLMs), data collection and finetuning remain significant challenges. These methods often rely on large-scale supervised fine-tuning (SFT) with extensive video data and long Chain-of-Thought (CoT) annotations, making them costly and hard to scale. To address this, we present Video-RTS, a new approach to improve video reasoning capability with drastically improved data efficiency by combining data-efficient RL with a video-adaptive test-time scaling (TTS) strategy. Based on observations about the data scaling of RL samples, we skip the resource-intensive SFT step and employ efficient pure-RL training with output-based rewards, requiring no additional annotations or extensive fine-tuning. Furthermore, to utilize computational resources more efficiently, we introduce a sparse-to-dense video TTS strategy that improves inference by iteratively adding frames based on output consistency. We validate our approach on multiple video reasoning benchmarks, showing that Video-RTS surpasses existing video reasoning models by an average of 2.4% in accuracy using only 3.6% training samples. For example, Video-RTS achieves a 4.2% improvement on Video-Holmes, a recent and challenging video reasoning benchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and adaptive video TTS offer complementary strengths, enabling Video-RTS's strong reasoning performance.</li>
</ul>

<h3>Title: Mask6D: Masked Pose Priors For 6D Object Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Yuechen Xie, Haobo Jiang, Jin Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06486">https://arxiv.org/abs/2507.06486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06486">https://arxiv.org/pdf/2507.06486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06486]] Mask6D: Masked Pose Priors For 6D Object Pose Estimation(https://arxiv.org/abs/2507.06486)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robust 6D object pose estimation in cluttered or occluded conditions using monocular RGB images remains a challenging task. One reason is that current pose estimation networks struggle to extract discriminative, pose-aware features using 2D feature backbones, especially when the available RGB information is limited due to target occlusion in cluttered scenes. To mitigate this, we propose a novel pose estimation-specific pre-training strategy named Mask6D. Our approach incorporates pose-aware 2D-3D correspondence maps and visible mask maps as additional modal information, which is combined with RGB images for the reconstruction-based model pre-training. Essentially, this 2D-3D correspondence maps a transformed 3D object model to 2D pixels, reflecting the pose information of the target in camera coordinate system. Meanwhile, the integrated visible mask map can effectively guide our model to disregard cluttered background information. In addition, an object-focused pre-training loss function is designed to further facilitate our network to remove the background interference. Finally, we fine-tune our pre-trained pose prior-aware network via conventional pose training strategy to realize the reliable pose prediction. Extensive experiments verify that our method outperforms previous end-to-end pose estimation methods.</li>
</ul>

<h3>Title: On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Stephen Obadinma, Xiaodan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06489">https://arxiv.org/abs/2507.06489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06489">https://arxiv.org/pdf/2507.06489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06489]] On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks(https://arxiv.org/abs/2507.06489)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Robust verbal confidence generated by large language models (LLMs) is crucial for the deployment of LLMs to ensure transparency, trust, and safety in human-AI interactions across many high-stakes applications. In this paper, we present the first comprehensive study on the robustness of verbal confidence under adversarial attacks. We introduce a novel framework for attacking verbal confidence scores through both perturbation and jailbreak-based methods, and show that these attacks can significantly jeopardize verbal confidence estimates and lead to frequent answer changes. We examine a variety of prompting strategies, model sizes, and application domains, revealing that current confidence elicitation methods are vulnerable and that commonly used defence techniques are largely ineffective or counterproductive. Our findings underscore the urgent need to design more robust mechanisms for confidence expression in LLMs, as even subtle semantic-preserving modifications can lead to misleading confidence in responses.</li>
</ul>

<h3>Title: TELSAFE: Security Gap Quantitative Risk Assessment Framework</h3>
<ul>
<li><strong>Authors: </strong>Sarah Ali Siddiqui, Chandra Thapa, Derui Wang, Rayne Holland, Wei Shao, Seyit Camtepe, Hajime Suzuki, Rajiv Shah</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06497">https://arxiv.org/abs/2507.06497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06497">https://arxiv.org/pdf/2507.06497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06497]] TELSAFE: Security Gap Quantitative Risk Assessment Framework(https://arxiv.org/abs/2507.06497)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Gaps between established security standards and their practical implementation have the potential to introduce vulnerabilities, possibly exposing them to security risks. To effectively address and mitigate these security and compliance challenges, security risk management strategies are essential. However, it must adhere to well-established strategies and industry standards to ensure consistency, reliability, and compatibility both within and across organizations. In this paper, we introduce a new hybrid risk assessment framework called TELSAFE, which employs probabilistic modeling for quantitative risk assessment and eliminates the influence of expert opinion bias. The framework encompasses both qualitative and quantitative assessment phases, facilitating effective risk management strategies tailored to the unique requirements of organizations. A specific use case utilizing Common Vulnerabilities and Exposures (CVE)-related data demonstrates the framework's applicability and implementation in real-world scenarios, such as in the telecommunications industry.</li>
</ul>

<h3>Title: A Survey on Artificial Noise for Physical Layer Security: Opportunities, Technologies, Guidelines, Advances, and Trends</h3>
<ul>
<li><strong>Authors: </strong>Hong Niu, Yue Xiao, Xia Lei, Jiangong Chen, Zhihan Xiao, Mao Li, Chau Yuen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06500">https://arxiv.org/abs/2507.06500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06500">https://arxiv.org/pdf/2507.06500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06500]] A Survey on Artificial Noise for Physical Layer Security: Opportunities, Technologies, Guidelines, Advances, and Trends(https://arxiv.org/abs/2507.06500)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Due to the broadcast nature of wireless communications, physical-layer security has attracted increasing concerns from both academia and industry. Artificial noise (AN), as one of the promising physical-layer security techniques, is capable of utilizing the spatial degree-of-freedom of channels to effectively enhance the security of wireless communications. In contrast to other physicallayer security techniques, the key distinguishing feature of AN is to generate specific interfering signals according to channel characteristics, increasing the secrecy capacity by reducing the wiretap channel capacity without affecting the legitimate channel capacity. Hence, this paper provides the latest survey of AN, including its evolution, modeling, backgrounds, applications, and future trends. Initially, we introduce the development, fundamentals, and backgrounds of AN. Subsequently, we highlight a comprehensive survey of the current state of research on various AN-empowered scenarios and AN-combined technologies. Finally, we discuss some technical challenges to tackle for AN-aided wireless security in the future.</li>
</ul>

<h3>Title: MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models</h3>
<ul>
<li><strong>Authors: </strong>Yiwen Liu, Chenyu Zhang, Junjie Song, Siqi Chen, Sun Yin, Zihan Wang, Lingming Zeng, Yuji Cao, Junming Jiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06502">https://arxiv.org/abs/2507.06502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06502">https://arxiv.org/pdf/2507.06502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06502]] MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models(https://arxiv.org/abs/2507.06502)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As a prominent data modality task, time series forecasting plays a pivotal role in diverse applications. With the remarkable advancements in Large Language Models (LLMs), the adoption of LLMs as the foundational architecture for time series modeling has gained significant attention. Although existing models achieve some success, they rarely both model time and frequency characteristics in a pretraining-finetuning paradigm leading to suboptimal performance in predictions of complex time series, which requires both modeling periodicity and prior pattern knowledge of signals. We propose MoFE-Time, an innovative time series forecasting model that integrates time and frequency domain features within a Mixture of Experts (MoE) network. Moreover, we use the pretraining-finetuning paradigm as our training framework to effectively transfer prior pattern knowledge across pretraining and finetuning datasets with different periodicity distributions. Our method introduces both frequency and time cells as experts after attention modules and leverages the MoE routing mechanism to construct multidimensional sparse representations of input signals. In experiments on six public benchmarks, MoFE-Time has achieved new state-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared to the representative methods Time-MoE. Beyond the existing evaluation benchmarks, we have developed a proprietary dataset, NEV-sales, derived from real-world business scenarios. Our method achieves outstanding results on this dataset, underscoring the effectiveness of the MoFE-Time model in practical commercial applications.</li>
</ul>

<h3>Title: Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Russell Taylor, Benjamin Herbert, Michael Sana</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06506">https://arxiv.org/abs/2507.06506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06506">https://arxiv.org/pdf/2507.06506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06506]] Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings(https://arxiv.org/abs/2507.06506)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Translating wordplay across languages presents unique challenges that have long confounded both professional human translators and machine translation systems. This research proposes a novel approach for translating puns from English to French by combining state-of-the-art large language models with specialized techniques for wordplay generation. Our methodology employs a three-stage approach. First, we establish a baseline using multiple frontier large language models with feedback based on a new contrastive learning dataset. Second, we implement a guided chain-of-thought pipeline with combined phonetic-semantic embeddings. Third, we implement a multi-agent generator-discriminator framework for evaluating and regenerating puns with feedback. Moving beyond the limitations of literal translation, our methodology's primary objective is to capture the linguistic creativity and humor of the source text wordplay, rather than simply duplicating its vocabulary. Our best runs earned first and second place in the CLEF JOKER 2025 Task 2 competition where they were evaluated manually by expert native French speakers. This research addresses a gap between translation studies and computational linguistics by implementing linguistically-informed techniques for wordplay translation, advancing our understanding of how language models can be leveraged to handle the complex interplay between semantic ambiguity, phonetic similarity, and the implicit cultural and linguistic awareness needed for successful humor.</li>
</ul>

<h3>Title: Subgraph Counting under Edge Local Differential Privacy Based on Noisy Adjacency Matrix</h3>
<ul>
<li><strong>Authors: </strong>Jintao Guo, Ying Zhou, Chao Li, Guixun Luo</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06508">https://arxiv.org/abs/2507.06508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06508">https://arxiv.org/pdf/2507.06508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06508]] Subgraph Counting under Edge Local Differential Privacy Based on Noisy Adjacency Matrix(https://arxiv.org/abs/2507.06508)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>When analyzing connection patterns within graphs, subgraph counting serves as an effective and fundamental approach. Edge-local differential privacy (edge-LDP) and shuffle model have been employed to achieve subgraph counting under a privacy-preserving situation. Existing algorithms are plagued by high time complexity, excessive download costs, low accuracy, or dependence on trusted third parties. To address the aforementioned challenges, we propose the Noisy Adjacency Matrix (NAM), which combines differential privacy with the adjacency matrix of the graph. NAM offers strong versatility and scalability, making it applicable to a wider range of DP variants, DP mechanisms, and graph types. Based on NAM, we designed five algorithms (TriOR, TriTR, TriMTR, QuaTR, and 2STAR) to count three types of subgraphs: triangles, quadrangles, and 2-stars. Theoretical and experimental results demonstrate that in triangle counting, TriOR maximizes accuracy with reduced time complexity among one-round algorithms, TriTR achieves optimal accuracy, TriMTR achieves the highest accuracy under low download costs, and QuaTR stands as the first quadrangle counting algorithm under pure edge-LDP. We implement edge-LDP for noisy data via a confidence interval-inspired method, providing DP guarantees on randomized data. Our 2STAR algorithm achieves the highest accuracy in 2-star counting and can be derived as a byproduct of two-round triangle or quadrangle counting algorithms, enabling efficient joint estimation of triangle, quadrangle, and 2-star counts within two query rounds.</li>
</ul>

<h3>Title: Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection</h3>
<ul>
<li><strong>Authors: </strong>Yupeng Hu, Changxing Ding, Chang Sun, Shaoli Huang, Xiangmin Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06510">https://arxiv.org/abs/2507.06510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06510">https://arxiv.org/pdf/2507.06510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06510]] Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection(https://arxiv.org/abs/2507.06510)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Open vocabulary Human-Object Interaction (HOI) detection is a challenging task that detects all <human, verb, object> triplets of interest in an image, even those that are not pre-defined in the training set. Existing approaches typically rely on output features generated by large Vision-Language Models (VLMs) to enhance the generalization ability of interaction representations. However, the visual features produced by VLMs are holistic and coarse-grained, which contradicts the nature of detection tasks. To address this issue, we propose a novel Bilateral Collaboration framework for open vocabulary HOI detection (BC-HOI). This framework includes an Attention Bias Guidance (ABG) component, which guides the VLM to produce fine-grained instance-level interaction features according to the attention bias provided by the HOI detector. It also includes a Large Language Model (LLM)-based Supervision Guidance (LSG) component, which provides fine-grained token-level supervision for the HOI detector by the LLM component of the VLM. LSG enhances the ability of ABG to generate high-quality attention bias. We conduct extensive experiments on two popular benchmarks: HICO-DET and V-COCO, consistently achieving superior performance in the open vocabulary and closed settings. The code will be released in Github.</li>
</ul>

<h3>Title: Instance-Wise Monotonic Calibration by Constrained Transformation</h3>
<ul>
<li><strong>Authors: </strong>Yunrui Zhang, Gustavo Batista, Salil S. Kanhere</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06516">https://arxiv.org/abs/2507.06516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06516">https://arxiv.org/pdf/2507.06516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06516]] Instance-Wise Monotonic Calibration by Constrained Transformation(https://arxiv.org/abs/2507.06516)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Deep neural networks often produce miscalibrated probability estimates, leading to overconfident predictions. A common approach for calibration is fitting a post-hoc calibration map on unseen validation data that transforms predicted probabilities. A key desirable property of the calibration map is instance-wise monotonicity (i.e., preserving the ranking of probability outputs). However, most existing post-hoc calibration methods do not guarantee monotonicity. Previous monotonic approaches either use an under-parameterized calibration map with limited expressive ability or rely on black-box neural networks, which lack interpretability and robustness. In this paper, we propose a family of novel monotonic post-hoc calibration methods, which employs a constrained calibration map parameterized linearly with respect to the number of classes. Our proposed approach ensures expressiveness, robustness, and interpretability while preserving the relative ordering of the probability output by formulating the proposed calibration map as a constrained optimization problem. Our proposed methods achieve state-of-the-art performance across datasets with different deep neural network models, outperforming existing calibration methods while being data and computation-efficient. Our code is available at this https URL</li>
</ul>

<h3>Title: SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers</h3>
<ul>
<li><strong>Authors: </strong>Zicong Tang, Shi Luohe, Zuchao Li, Baoyuan Qi, Guoming Liu, Lefei Zhang, Ping Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06517">https://arxiv.org/abs/2507.06517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06517">https://arxiv.org/pdf/2507.06517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06517]] SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers(https://arxiv.org/abs/2507.06517)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved impressive accomplishments in recent years. However, the increasing memory consumption of KV cache has possessed a significant challenge to the inference system. Eviction methods have revealed the inherent redundancy within the KV cache, demonstrating its potential for reduction, particularly in deeper layers. However, KV cache reduction for shallower layers has been found to be insufficient. Based on our observation that, the KV cache exhibits a high degree of similarity. Based on this observation, we proposed a novel KV cache reduction method, SpindleKV, which balances both shallow and deep layers. For deep layers, we employ an attention weight based eviction method, while for shallow layers, we apply a codebook based replacement approach which is learnt by similarity and merging policy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma faced by other attention based eviction methods. Experiments on two common benchmarks with three different LLMs shown that SpindleKV obtained better KV cache reduction effect compared to baseline methods, while preserving similar or even better model performance.</li>
</ul>

<h3>Title: FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Liqiang Jing, Viet Lai, Seunghyun Yoon, Trung Bui, Xinya Du</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06523">https://arxiv.org/abs/2507.06523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06523">https://arxiv.org/pdf/2507.06523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06523]] FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation(https://arxiv.org/abs/2507.06523)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable progress in both Video-to-Text and Text-to-Video tasks. However, they often suffer fro hallucinations, generating content that contradicts the visual input. Existing evaluation methods are limited to one task (e.g., V2T) and also fail to assess hallucinations in open-ended, free-form responses. To address this gap, we propose FIFA, a unified FaIthFulness evAluation framework that extracts comprehensive descriptive facts, models their semantic dependencies via a Spatio-Temporal Semantic Dependency Graph, and verifies them using VideoQA models. We further introduce Post-Correction, a tool-based correction framework that revises hallucinated content. Extensive experiments demonstrate that FIFA aligns more closely with human judgment than existing evaluation methods, and that Post-Correction effectively improves factual consistency in both text and video generation.</li>
</ul>

<h3>Title: AdaDPIGU: Differentially Private SGD with Adaptive Clipping and Importance-Based Gradient Updates for Deep Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Huiqi Zhang, Fang Xie</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06525">https://arxiv.org/abs/2507.06525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06525">https://arxiv.org/pdf/2507.06525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06525]] AdaDPIGU: Differentially Private SGD with Adaptive Clipping and Importance-Based Gradient Updates for Deep Neural Networks(https://arxiv.org/abs/2507.06525)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Differential privacy has been proven effective for stochastic gradient descent; however, existing methods often suffer from performance degradation in high-dimensional settings, as the scale of injected noise increases with dimensionality. To tackle this challenge, we propose AdaDPIGU--a new differentially private SGD framework with importance-based gradient updates tailored for deep neural networks. In the pretraining stage, we apply a differentially private Gaussian mechanism to estimate the importance of each parameter while preserving privacy. During the gradient update phase, we prune low-importance coordinates and introduce a coordinate-wise adaptive clipping mechanism, enabling sparse and noise-efficient gradient updates. Theoretically, we prove that AdaDPIGU satisfies $(\varepsilon, \delta)$-differential privacy and retains convergence guarantees. Extensive experiments on standard benchmarks validate the effectiveness of AdaDPIGU. All results are reported under a fixed retention ratio of 60%. On MNIST, our method achieves a test accuracy of 99.12% under a privacy budget of $\epsilon = 8$, nearly matching the non-private model. Remarkably, on CIFAR-10, it attains 73.21% accuracy at $\epsilon = 4$, outperforming the non-private baseline of 71.12%, demonstrating that adaptive sparsification can enhance both privacy and utility.</li>
</ul>

<h3>Title: Concept Unlearning by Modeling Key Steps of Diffusion Process</h3>
<ul>
<li><strong>Authors: </strong>Chaoshuo Zhang, Chenhao Lin, Zhengyu Zhao, Le Yang, Qian Wang, Chao Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06526">https://arxiv.org/abs/2507.06526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06526">https://arxiv.org/pdf/2507.06526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06526]] Concept Unlearning by Modeling Key Steps of Diffusion Process(https://arxiv.org/abs/2507.06526)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion, which generate highly realistic images based on textual input, have been widely used. However, their misuse poses serious security risks. While existing concept unlearning methods aim to mitigate these risks, they struggle to balance unlearning effectiveness with generative this http URL overcome this limitation, we innovatively propose the Key Step Concept Unlearning (KSCU) method, which ingeniously capitalizes on the unique stepwise sampling characteristic inherent in diffusion models during the image generation process. Unlike conventional approaches that treat all denoising steps equally, KSCU strategically focuses on pivotal steps with the most influence over the final outcome by dividing key steps for different concept unlearning tasks and fine-tuning the model only at those steps. This targeted approach reduces the number of parameter updates needed for effective unlearning, while maximizing the retention of the model's generative this http URL extensive benchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs from generating undesirable images while better retaining the model's generative this http URL code will be released.</li>
</ul>

<h3>Title: InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior</h3>
<ul>
<li><strong>Authors: </strong>Huisheng Wang, Zhuoshi Pan, Hangjing Zhang, Mingxiao Liu, Hanqing Gao, H. Vicky Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.ET, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06528">https://arxiv.org/abs/2507.06528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06528">https://arxiv.org/pdf/2507.06528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06528]] InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior(https://arxiv.org/abs/2507.06528)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Aligning Large Language Models (LLMs) with investor decision-making processes under herd behavior is a critical challenge in behavioral finance, which grapples with a fundamental limitation: the scarcity of real-user data needed for Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM outputs and human behavioral patterns, its reliance on massive authentic data imposes substantial collection costs and privacy risks. We propose InvestAlign, a novel framework that constructs high-quality SFT datasets by leveraging theoretical solutions to similar and simple optimal investment problems rather than complex scenarios. Our theoretical analysis demonstrates that training LLMs with InvestAlign-generated data achieves faster parameter convergence than using real-user data, suggesting superior learning efficiency. Furthermore, we develop InvestAgent, an LLM agent fine-tuned with InvestAlign, which demonstrates significantly closer alignment to real-user data than pre-SFT models in both simple and complex investment problems. This highlights our proposed InvestAlign as a promising approach with the potential to address complex optimal investment problems and align LLMs with investor decision-making processes under herd behavior. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Direct Regret Optimization in Bayesian Optimization</h3>
<ul>
<li><strong>Authors: </strong>Fengxue Zhang, Yuxin Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06529">https://arxiv.org/abs/2507.06529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06529">https://arxiv.org/pdf/2507.06529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06529]] Direct Regret Optimization in Bayesian Optimization(https://arxiv.org/abs/2507.06529)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Bayesian optimization (BO) is a powerful paradigm for optimizing expensive black-box functions. Traditional BO methods typically rely on separate hand-crafted acquisition functions and surrogate models for the underlying function, and often operate in a myopic manner. In this paper, we propose a novel direct regret optimization approach that jointly learns the optimal model and non-myopic acquisition by distilling from a set of candidate models and acquisitions, and explicitly targets minimizing the multi-step regret. Our framework leverages an ensemble of Gaussian Processes (GPs) with varying hyperparameters to generate simulated BO trajectories, each guided by an acquisition function chosen from a pool of conventional choices, until a Bayesian early stop criterion is met. These simulated trajectories, capturing multi-step exploration strategies, are used to train an end-to-end decision transformer that directly learns to select next query points aimed at improving the ultimate objective. We further adopt a dense training--sparse learning paradigm: The decision transformer is trained offline with abundant simulated data sampled from ensemble GPs and acquisitions, while a limited number of real evaluations refine the GPs online. Experimental results on synthetic and real-world benchmarks suggest that our method consistently outperforms BO baselines, achieving lower simple regret and demonstrating more robust exploration in high-dimensional or noisy settings.</li>
</ul>

<h3>Title: Transferable Parasitic Estimation via Graph Contrastive Learning and Label Rebalancing in AMS Circuits</h3>
<ul>
<li><strong>Authors: </strong>Shan Shen, Shenglu Hua, Jiajun Zou, Jiawei Liu, Jianwang Zhai, Chuan Shi, Wenjian Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06535">https://arxiv.org/abs/2507.06535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06535">https://arxiv.org/pdf/2507.06535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06535]] Transferable Parasitic Estimation via Graph Contrastive Learning and Label Rebalancing in AMS Circuits(https://arxiv.org/abs/2507.06535)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graph representation learning on Analog-Mixed Signal (AMS) circuits is crucial for various downstream tasks, e.g., parasitic estimation. However, the scarcity of design data, the unbalanced distribution of labels, and the inherent diversity of circuit implementations pose significant challenges to learning robust and transferable circuit representations. To address these limitations, we propose CircuitGCL, a novel graph contrastive learning framework that integrates representation scattering and label rebalancing to enhance transferability across heterogeneous circuit graphs. CircuitGCL employs a self-supervised strategy to learn topology-invariant node embeddings through hyperspherical representation scattering, eliminating dependency on large-scale data. Simultaneously, balanced mean squared error (MSE) and softmax cross-entropy (bsmCE) losses are introduced to mitigate label distribution disparities between circuits, enabling robust and transferable parasitic estimation. Evaluated on parasitic capacitance estimation (edge-level task) and ground capacitance classification (node-level task) across TSMC 28nm AMS designs, CircuitGCL outperforms all state-of-the-art (SOTA) methods, with the $R^2$ improvement of $33.64\% \sim 44.20\%$ for edge regression and F1-score gain of $0.9\times \sim 2.1\times$ for node classification. Our code is available at \href{this https URL}{here}.</li>
</ul>

<h3>Title: Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance Prediction</h3>
<ul>
<li><strong>Authors: </strong>Shan Shen, Yibin Zhang, Hector Rodriguez Rodriguez, Wenjian Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06538">https://arxiv.org/abs/2507.06538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06538">https://arxiv.org/pdf/2507.06538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06538]] Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance Prediction(https://arxiv.org/abs/2507.06538)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Graph representation learning is a powerful method to extract features from graph-structured data, such as analog/mixed-signal (AMS) circuits. However, training deep learning models for AMS designs is severely limited by the scarcity of integrated circuit design data. In this work, we present CircuitGPS, a few-shot learning method for parasitic effect prediction in AMS circuits. The circuit netlist is represented as a heterogeneous graph, with the coupling capacitance modeled as a link. CircuitGPS is pre-trained on link prediction and fine-tuned on edge regression. The proposed method starts with a small-hop sampling technique that converts a link or a node into a subgraph. Then, the subgraph embeddings are learned with a hybrid graph Transformer. Additionally, CircuitGPS integrates a low-cost positional encoding that summarizes the positional and structural information of the sampled subgraph. CircuitGPS improves the accuracy of coupling existence by at least 20\% and reduces the MAE of capacitance estimation by at least 0.067 compared to existing methods. Our method demonstrates strong inherent scalability, enabling direct application to diverse AMS circuit designs through zero-shot learning. Furthermore, the ablation studies provide valuable insights into graph models for representation learning.</li>
</ul>

<h3>Title: Large Language Model for Extracting Complex Contract Information in Industrial Scenes</h3>
<ul>
<li><strong>Authors: </strong>Yunyang Cao, Yanjun Li, Silong Dai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06539">https://arxiv.org/abs/2507.06539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06539">https://arxiv.org/pdf/2507.06539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06539]] Large Language Model for Extracting Complex Contract Information in Industrial Scenes(https://arxiv.org/abs/2507.06539)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>This paper proposes a high-quality dataset construction method for complex contract information extraction tasks in industrial scenarios and fine-tunes a large language model based on this dataset. Firstly, cluster analysis is performed on industrial contract texts, and GPT-4 and GPT-3.5 are used to extract key information from the original contract data, obtaining high-quality data annotations. Secondly, data augmentation is achieved by constructing new texts, and GPT-3.5 generates unstructured contract texts from randomly combined keywords, improving model robustness. Finally, the large language model is fine-tuned based on the high-quality dataset. Experimental results show that the model achieves excellent overall performance while ensuring high field recall and precision and considering parsing efficiency. LoRA, data balancing, and data augmentation effectively enhance model accuracy and robustness. The proposed method provides a novel and efficient solution for industrial contract information extraction tasks.</li>
</ul>

<h3>Title: Token Bottleneck: One Token to Remember Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Taekyung Kim, Dongyoon Han, Byeongho Heo, Jeongeun Park, Sangdoo Yun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06543">https://arxiv.org/abs/2507.06543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06543">https://arxiv.org/pdf/2507.06543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06543]] Token Bottleneck: One Token to Remember Dynamics(https://arxiv.org/abs/2507.06543)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deriving compact and temporally aware visual representations from dynamic scenes is essential for successful execution of sequential scene understanding tasks such as visual tracking and robotic manipulation. In this paper, we introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised learning pipeline that squeezes a scene into a bottleneck token and predicts the subsequent scene using minimal patches as hints. The ToBo pipeline facilitates the learning of sequential scene representations by conservatively encoding the reference scene into a compact bottleneck token during the squeeze step. In the expansion step, we guide the model to capture temporal dynamics by predicting the target scene using the bottleneck token along with few target patches as hints. This design encourages the vision backbone to embed temporal dependencies, thereby enabling understanding of dynamic transitions across scenes. Extensive experiments in diverse sequential tasks, including video label propagation and robot manipulation in simulated environments demonstrate the superiority of ToBo over baselines. Moreover, deploying our pre-trained model on physical robots confirms its robustness and effectiveness in real-world environments. We further validate the scalability of ToBo across different model scales.</li>
</ul>

<h3>Title: Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution</h3>
<ul>
<li><strong>Authors: </strong>Yonghyun Park, Chieh-Hsin Lai, Satoshi Hayakawa, Yuhta Takida, Naoki Murata, Wei-Hsiang Liao, Woosung Choi, Kin Wai Cheuk, Junghyun Koo, Yuki Mitsufuji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06547">https://arxiv.org/abs/2507.06547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06547">https://arxiv.org/pdf/2507.06547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06547]] Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution(https://arxiv.org/abs/2507.06547)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>While diffusion models excel at image generation, their growing adoption raises critical concerns around copyright issues and model transparency. Existing attribution methods identify training examples influencing an entire image, but fall short in isolating contributions to specific elements, such as styles or objects, that matter most to stakeholders. To bridge this gap, we introduce \emph{concept-level attribution} via a novel method called \emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key innovations: (1) a reformulated diffusion training loss based on diffusion posterior sampling, enabling robust, sample-specific attribution; and (2) a concept-aware reward function that emphasizes semantic relevance. We evaluate Concept-TRAK on the AbC benchmark, showing substantial improvements over prior methods. Through diverse case studies--ranging from identifying IP-protected and unsafe content to analyzing prompt engineering and compositional learning--we demonstrate how concept-level attribution yields actionable insights for responsible generative AI development and governance.</li>
</ul>

<h3>Title: SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference</h3>
<ul>
<li><strong>Authors: </strong>Qian Chen, Xianhao Chen, Kaibin Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06567">https://arxiv.org/abs/2507.06567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06567">https://arxiv.org/pdf/2507.06567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06567]] SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference(https://arxiv.org/abs/2507.06567)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) models improve the scalability of large language models (LLMs) by activating only a small subset of relevant experts per input. However, the sheer number of expert networks in an MoE model introduces a significant storage burden for an edge device. To address this challenge, we consider a scenario where experts are dispersed within an edge network for distributed inference. Based on the popular Top-$K$ expert selection strategy, we formulate a latency minimization problem by optimizing expert caching on edge servers under storage constraints. When $K=1$, the problem reduces to a monotone submodular maximization problem with knapsack constraints, for which we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee. For the general case where $K\geq1$, expert co-activation within the same MoE layer introduces non-submodularity, causing greedy methods to be ineffective. To tackle this issue, we propose a successive greedy decomposition method to decompose the original problem into a series of subproblems, with each being solved by a dynamic programming approach. Furthermore, we design an accelerated algorithm based on the max-convolution technique to obtain the approximate solution with a provable guarantee in polynomial time. Simulation results on various MoE models demonstrate that our method significantly reduces inference latency compared to existing baselines.</li>
</ul>

<h3>Title: Edge-Boundary-Texture Loss: A Tri-Class Generalization of Weighted Binary Cross-Entropy for Enhanced Edge Detection</h3>
<ul>
<li><strong>Authors: </strong>Hao Shu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06569">https://arxiv.org/abs/2507.06569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06569">https://arxiv.org/pdf/2507.06569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06569]] Edge-Boundary-Texture Loss: A Tri-Class Generalization of Weighted Binary Cross-Entropy for Enhanced Edge Detection(https://arxiv.org/abs/2507.06569)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Edge detection (ED) remains a fundamental task in computer vision, yet its performance is often hindered by the ambiguous nature of non-edge pixels near object boundaries. The widely adopted Weighted Binary Cross-Entropy (WBCE) loss treats all non-edge pixels uniformly, overlooking the structural nuances around edges and often resulting in blurred predictions. In this paper, we propose the Edge-Boundary-Texture (EBT) loss, a novel objective that explicitly divides pixels into three categories, edge, boundary, and texture, and assigns each a distinct supervisory weight. This tri-class formulation enables more structured learning by guiding the model to focus on both edge precision and contextual boundary localization. We theoretically show that the EBT loss generalizes the WBCE loss, with the latter becoming a limit case. Extensive experiments across multiple benchmarks demonstrate the superiority of the EBT loss both quantitatively and perceptually. Furthermore, the consistent use of unified hyperparameters across all models and datasets, along with robustness to their moderate variations, indicates that the EBT loss requires minimal fine-tuning and is easily deployable in practice.</li>
</ul>

<h3>Title: Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis</h3>
<ul>
<li><strong>Authors: </strong>Srihari K B, Pushpak Bhattacharyya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06571">https://arxiv.org/abs/2507.06571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06571">https://arxiv.org/pdf/2507.06571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06571]] Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis(https://arxiv.org/abs/2507.06571)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We propose a unified food-domain QA framework that combines a large-scale multimodal knowledge graph (MMKG) with generative AI. Our MMKG links 13,000 recipes, 3,000 ingredients, 140,000 relations, and 14,000 images. We generate 40,000 QA pairs using 40 templates and LLaVA/DeepSeek augmentation. Joint fine-tuning of Meta LLaMA 3.1-8B and Stable Diffusion 3.5-Large improves BERTScore by 16.2\%, reduces FID by 37.8\%, and boosts CLIP alignment by 31.1\%. Diagnostic analyses-CLIP-based mismatch detection (35.2\% to 7.3\%) and LLaVA-driven hallucination checks-ensure factual and visual fidelity. A hybrid retrieval-generation strategy achieves 94.1\% accurate image reuse and 85\% adequacy in synthesis. Our results demonstrate that structured knowledge and multimodal generation together enhance reliability and diversity in food QA.</li>
</ul>

<h3>Title: From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization</h3>
<ul>
<li><strong>Authors: </strong>Xinjie Chen, Minpeng Liao, Guoxin Chen, Chengxi Li, Biao Fu, Kai Fan, Xinggao Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06573">https://arxiv.org/abs/2507.06573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06573">https://arxiv.org/pdf/2507.06573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06573]] From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization(https://arxiv.org/abs/2507.06573)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with verifiable rewards (RLVR) has recently advanced the reasoning capabilities of large language models (LLMs). While prior work has emphasized algorithmic design, data curation, and reward shaping, we investigate RLVR from a sample-centric perspective and introduce LPPO (Learning-Progress and Prefix-guided Optimization), a framework of progressive optimization techniques. Our work addresses a critical question: how to best leverage a small set of trusted, high-quality demonstrations, rather than simply scaling up data volume. First, motivated by how hints aid human problem-solving, we propose prefix-guided sampling, an online data augmentation method that incorporates partial solution prefixes from expert demonstrations to guide the policy, particularly for challenging instances. Second, inspired by how humans focus on important questions aligned with their current capabilities, we introduce learning-progress weighting, a dynamic strategy that adjusts each training sample's influence based on model progression. We estimate sample-level learning progress via an exponential moving average of per-sample pass rates, promoting samples that foster learning and de-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks demonstrate that our methods outperform strong baselines, yielding faster convergence and a higher performance ceiling.</li>
</ul>

<h3>Title: MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction</h3>
<ul>
<li><strong>Authors: </strong>Yin Wang, Mu li, Zhiying Leng, Frederick W. B. Li, Xiaohui Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06590">https://arxiv.org/abs/2507.06590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06590">https://arxiv.org/pdf/2507.06590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06590]] MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction(https://arxiv.org/abs/2507.06590)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf interaction, aimed at addressing the persistent challenge of generating human motion from rare language prompts. While previous approaches struggle with coarse-grained matching and overlook important semantic cues due to motion redundancy, our key insight lies in leveraging fine-grained clip relationships to mitigate these issues. MOST's retrieval stage presents the first formulation of its kind - temporal clip Banzhaf interaction - which precisely quantifies textual-motion coherence at the clip level. This facilitates direct, fine-grained text-to-motion clip matching and eliminates prevalent redundancy. In the generation stage, a motion prompt module effectively utilizes retrieved motion clips to produce semantically consistent movements. Extensive evaluations confirm that MOST achieves state-of-the-art text-to-motion retrieval and generation performance by comprehensively addressing previous challenges, as demonstrated through quantitative and qualitative results highlighting its effectiveness, especially for rare prompts.</li>
</ul>

<h3>Title: Ambiguity-aware Point Cloud Segmentation by Adaptive Margin Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Yang Chen, Yueqi Duan, Haowen Sun, Jiwen Lu, Yap-Peng Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06592">https://arxiv.org/abs/2507.06592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06592">https://arxiv.org/pdf/2507.06592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06592]] Ambiguity-aware Point Cloud Segmentation by Adaptive Margin Contrastive Learning(https://arxiv.org/abs/2507.06592)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>This paper proposes an adaptive margin contrastive learning method for 3D semantic segmentation on point clouds. Most existing methods use equally penalized objectives, which ignore the per-point ambiguities and less discriminated features stemming from transition regions. However, as highly ambiguous points may be indistinguishable even for humans, their manually annotated labels are less reliable, and hard constraints over these points would lead to sub-optimal models. To address this, we first design AMContrast3D, a method comprising contrastive learning into an ambiguity estimation framework, tailored to adaptive objectives for individual points based on ambiguity levels. As a result, our method promotes model training, which ensures the correctness of low-ambiguity points while allowing mistakes for high-ambiguity points. As ambiguities are formulated based on position discrepancies across labels, optimization during inference is constrained by the assumption that all unlabeled points are uniformly unambiguous, lacking ambiguity awareness. Inspired by the insight of joint training, we further propose AMContrast3D++ integrating with two branches trained in parallel, where a novel ambiguity prediction module concurrently learns point ambiguities from generated embeddings. To this end, we design a masked refinement mechanism that leverages predicted ambiguities to enable the ambiguous embeddings to be more reliable, thereby boosting segmentation performance and enhancing robustness. Experimental results on 3D indoor scene datasets, S3DIS and ScanNet, demonstrate the effectiveness of the proposed method. Code is available at this https URL.</li>
</ul>

<h3>Title: Capturing Stable HDR Videos Using a Dual-Camera System</h3>
<ul>
<li><strong>Authors: </strong>Qianyu Zhang, Bolun Zheng, Hangjia Pan, Lingyu Zhu, Zunjie Zhu, Zongpeng Li, Shiqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06593">https://arxiv.org/abs/2507.06593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06593">https://arxiv.org/pdf/2507.06593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06593]] Capturing Stable HDR Videos Using a Dual-Camera System(https://arxiv.org/abs/2507.06593)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In HDR video reconstruction, exposure fluctuations in reference images from alternating exposure methods often result in flickering. To address this issue, we propose a dual-camera system (DCS) for HDR video acquisition, where one camera is assigned to capture consistent reference sequences, while the other is assigned to capture non-reference sequences for information supplementation. To tackle the challenges posed by video data, we introduce an exposure-adaptive fusion network (EAFNet) to achieve more robust results. EAFNet introduced a pre-alignment subnetwork to explore the influence of exposure, selectively emphasizing the valuable features across different exposure levels. Then, the enhanced features are fused by the asymmetric cross-feature fusion subnetwork, which explores reference-dominated attention maps to improve image fusion by aligning cross-scale features and performing cross-feature fusion. Finally, the reconstruction subnetwork adopts a DWT-based multiscale architecture to reduce ghosting artifacts and refine features at different resolutions. Extensive experimental evaluations demonstrate that the proposed method achieves state-of-the-art performance on different datasets, validating the great potential of the DCS in HDR video reconstruction. The codes and data captured by DCS will be available at this https URL.</li>
</ul>

<h3>Title: Cross-Modal Dual-Causal Learning for Long-Term Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Xu Shaowu, Jia Xibin, Gao Junyu, Sun Qianmei, Chang Jing, Fan Chao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06603">https://arxiv.org/abs/2507.06603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06603">https://arxiv.org/pdf/2507.06603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06603]] Cross-Modal Dual-Causal Learning for Long-Term Action Recognition(https://arxiv.org/abs/2507.06603)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Long-term action recognition (LTAR) is challenging due to extended temporal spans with complex atomic action correlations and visual confounders. Although vision-language models (VLMs) have shown promise, they often rely on statistical correlations instead of causal mechanisms. Moreover, existing causality-based methods address modal-specific biases but lack cross-modal causal modeling, limiting their utility in VLM-based LTAR. This paper proposes \textbf{C}ross-\textbf{M}odal \textbf{D}ual-\textbf{C}ausal \textbf{L}earning (CMDCL), which introduces a structural causal model to uncover causal relationships between videos and label texts. CMDCL addresses cross-modal biases in text embeddings via textual causal intervention and removes confounders inherent in the visual modality through visual causal intervention guided by the debiased text. These dual-causal interventions enable robust action representations to address LTAR challenges. Experimental results on three benchmarks including Charades, Breakfast and COIN, demonstrate the effectiveness of the proposed model. Our code is available at this https URL.</li>
</ul>

<h3>Title: Omni-Fusion of Spatial and Spectral for Hyperspectral Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Qing Zhang, Guoquan Pei, Yan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06606">https://arxiv.org/abs/2507.06606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06606">https://arxiv.org/pdf/2507.06606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06606]] Omni-Fusion of Spatial and Spectral for Hyperspectral Image Segmentation(https://arxiv.org/abs/2507.06606)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Medical Hyperspectral Imaging (MHSI) has emerged as a promising tool for enhanced disease diagnosis, particularly in computational pathology, offering rich spectral information that aids in identifying subtle biochemical properties of tissues. Despite these advantages, effectively fusing both spatial-dimensional and spectral-dimensional information from MHSIs remains challenging due to its high dimensionality and spectral redundancy inherent characteristics. To solve the above challenges, we propose a novel spatial-spectral omni-fusion network for hyperspectral image segmentation, named as Omni-Fuse. Here, we introduce abundant cross-dimensional feature fusion operations, including a cross-dimensional enhancement module that refines both spatial and spectral features through bidirectional attention mechanisms, a spectral-guided spatial query selection to select the most spectral-related spatial feature as the query, and a two-stage cross-dimensional decoder which dynamically guide the model to focus on the selected spatial query. Despite of numerous attention blocks, Omni-Fuse remains efficient in execution. Experiments on two microscopic hyperspectral image datasets show that our approach can significantly improve the segmentation performance compared with the state-of-the-art methods, with over 5.73 percent improvement in DSC. Code available at: this https URL.</li>
</ul>

<h3>Title: Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation</h3>
<ul>
<li><strong>Authors: </strong>Liliang Ren, Congcong Chen, Haoran Xu, Young Jin Kim, Adam Atkinson, Zheng Zhan, Jiankai Sun, Baolin Peng, Liyuan Liu, Shuohang Wang, Hao Cheng, Jianfeng Gao, Weizhu Chen, Yelong Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06607">https://arxiv.org/abs/2507.06607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06607">https://arxiv.org/pdf/2507.06607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06607]] Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation(https://arxiv.org/abs/2507.06607)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in language modeling have demonstrated the effectiveness of State Space Models (SSMs) for efficient sequence modeling. While hybrid architectures such as Samba and the decoder-decoder architecture, YOCO, have shown promising performance gains over Transformers, prior works have not investigated the efficiency potential of representation sharing between SSM layers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet effective mechanism for efficient memory sharing across layers. We apply it to create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in the cross-decoder to share memory readout states from a Samba-based self-decoder. SambaY significantly enhances decoding efficiency, preserves linear pre-filling time complexity, and boosts long-context performance, all while eliminating the need for explicit positional encoding. Through extensive scaling experiments, we demonstrate that our model exhibits a significantly lower irreducible loss compared to a strong YOCO baseline, indicating superior performance scalability under large-scale compute regimes. Our largest model enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves significantly better performance than Phi4-mini-Reasoning on reasoning tasks such as Math500, AIME24/25, and GPQA Diamond without any reinforcement learning, while delivering up to 10x higher decoding throughput on 2K-length prompts with 32K generation length under the vLLM inference framework. We release our training codebase on open-source data at this https URL.</li>
</ul>

<h3>Title: Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation</h3>
<ul>
<li><strong>Authors: </strong>Anshuk Uppal, Yuhta Takida, Chieh-Hsin Lai, Yuki Mitsufuji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06613">https://arxiv.org/abs/2507.06613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06613">https://arxiv.org/pdf/2507.06613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06613]] Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation(https://arxiv.org/abs/2507.06613)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Disentangled and interpretable latent representations in generative models typically come at the cost of generation quality. The $\beta$-VAE framework introduces a hyperparameter $\beta$ to balance disentanglement and reconstruction quality, where setting $\beta > 1$ introduces an information bottleneck that favors disentanglement over sharp, accurate reconstructions. To address this trade-off, we propose a novel generative modeling framework that leverages a range of $\beta$ values to learn multiple corresponding latent representations. First, we obtain a slew of representations by training a single variational autoencoder (VAE), with a new loss function that controls the information retained in each latent representation such that the higher $\beta$ value prioritize disentanglement over reconstruction fidelity. We then, introduce a non-linear diffusion model that smoothly transitions latent representations corresponding to different $\beta$ values. This model denoises towards less disentangled and more informative representations, ultimately leading to (almost) lossless representations, enabling sharp reconstructions. Furthermore, our model supports sample generation without input images, functioning as a standalone generative model. We evaluate our framework in terms of both disentanglement and generation quality. Additionally, we observe smooth transitions in the latent spaces with respect to changes in $\beta$, facilitating consistent manipulation of generated outputs.</li>
</ul>

<h3>Title: PointVDP: Learning View-Dependent Projection by Fireworks Rays for 3D Point Cloud Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yang Chen, Yueqi Duan, Haowen Sun, Ziwei Wang, Jiwen Lu, Yap-Peng Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06618">https://arxiv.org/abs/2507.06618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06618">https://arxiv.org/pdf/2507.06618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06618]] PointVDP: Learning View-Dependent Projection by Fireworks Rays for 3D Point Cloud Segmentation(https://arxiv.org/abs/2507.06618)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we propose view-dependent projection (VDP) to facilitate point cloud segmentation, designing efficient 3D-to-2D mapping that dynamically adapts to the spatial geometry from view variations. Existing projection-based methods leverage view-independent projection in complex scenes, relying on straight lines to generate direct rays or upward curves to reduce occlusions. However, their view independence provides projection rays that are limited to pre-defined parameters by human settings, restricting point awareness and failing to capture sufficient projection diversity across different view planes. Although multiple projections per view plane are commonly used to enhance spatial variety, the projected redundancy leads to excessive computational overhead and inefficiency in image processing. To address these limitations, we design a framework of VDP to generate data-driven projections from 3D point distributions, producing highly informative single-image inputs by predicting rays inspired by the adaptive behavior of fireworks. In addition, we construct color regularization to optimize the framework, which emphasizes essential features within semantic pixels and suppresses the non-semantic features within black pixels, thereby maximizing 2D space utilization in a projected image. As a result, our approach, PointVDP, develops lightweight projections in marginal computation costs. Experiments on S3DIS and ScanNet benchmarks show that our approach achieves competitive results, offering a resource-efficient solution for semantic understanding.</li>
</ul>

<h3>Title: Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with Differential Privacy with HAM10000</h3>
<ul>
<li><strong>Authors: </strong>Xiaobo Huang, Fang Xie</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06619">https://arxiv.org/abs/2507.06619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06619">https://arxiv.org/pdf/2507.06619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06619]] Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with Differential Privacy with HAM10000(https://arxiv.org/abs/2507.06619)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>When applying machine learning to medical image classification, data leakage is a critical issue. Previous methods, such as adding noise to gradients for differential privacy, work well on large datasets like MNIST and CIFAR-100, but fail on small, imbalanced medical datasets like HAM10000. This is because the imbalanced distribution causes gradients from minority classes to be clipped and lose crucial information, while majority classes dominate. This leads the model to fall into suboptimal solutions early. To address this, we propose SAD-DPSGD, which uses a linear decaying mechanism for noise and clipping thresholds. By allocating more privacy budget and using higher clipping thresholds in the initial training phases, the model avoids suboptimal solutions and enhances performance. Experiments show that SAD-DPSGD outperforms Auto-DPSGD on HAM10000, improving accuracy by 2.15% under $\epsilon = 3.0$ , $\delta = 10^{-3}$.</li>
</ul>

<h3>Title: FuDoBa: Fusing Document and Knowledge Graph-based Representations with Bayesian Optimisation</h3>
<ul>
<li><strong>Authors: </strong>Boshko Koloski, Senja Pollak, Roberto Navigli, Blaž Škrlj</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06622">https://arxiv.org/abs/2507.06622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06622">https://arxiv.org/pdf/2507.06622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06622]] FuDoBa: Fusing Document and Knowledge Graph-based Representations with Bayesian Optimisation(https://arxiv.org/abs/2507.06622)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Building on the success of Large Language Models (LLMs), LLM-based representations have dominated the document representation landscape, achieving great performance on the document embedding benchmarks. However, the high-dimensional, computationally expensive embeddings from LLMs tend to be either too generic or inefficient for domain-specific applications. To address these limitations, we introduce FuDoBa a Bayesian optimisation-based method that integrates LLM-based embeddings with domain-specific structured knowledge, sourced both locally and from external repositories like WikiData. This fusion produces low-dimensional, task-relevant representations while reducing training complexity and yielding interpretable early-fusion weights for enhanced classification performance. We demonstrate the effectiveness of our approach on six datasets in two domains, showing that when paired with robust AutoML-based classifiers, our proposed representation learning approach performs on par with, or surpasses, those produced solely by the proprietary LLM-based embedding baselines.</li>
</ul>

<h3>Title: Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review</h3>
<ul>
<li><strong>Authors: </strong>James Stewart-Evans, Emma Wilson, Tessa Langley, Andrew Prayle, Angela Hands, Karen Exley, Jo Leonardi-Bee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06623">https://arxiv.org/abs/2507.06623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06623">https://arxiv.org/pdf/2507.06623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06623]] Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review(https://arxiv.org/abs/2507.06623)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>The data extraction stages of reviews are resource-intensive, and researchers may seek to expediate data extraction using online (large language models) LLMs and review protocols. Claude 3.5 Sonnet was used to trial two approaches that used a review protocol to prompt data extraction from 10 evidence sources included in a case study scoping review. A protocol-based approach was also used to review extracted data. Limited performance evaluation was undertaken which found high accuracy for the two extraction approaches (83.3% and 100%) when extracting simple, well-defined citation details; accuracy was lower (9.6% and 15.8%) when extracting more complex, subjective data items. Considering all data items, both approaches had precision >90% but low recall (<25%) and F1 scores (<40%). The context of a complex scoping review, open response types and methodological approach likely impacted performance due to missed and misattributed data. LLM feedback considered the baseline extraction accurate and suggested minor amendments: four of 15 (26.7%) to citation details and 8 of 38 (21.1%) to key findings data items were considered to potentially add value. However, when repeating the process with a dataset featuring deliberate errors, only 2 of 39 (5%) errors were detected. Review-protocol-based methods used for expediency require more robust performance evaluation across a range of LLMs and review contexts with comparison to conventional prompt engineering approaches. We recommend researchers evaluate and report LLM performance if using them similarly to conduct data extraction or review extracted data. LLM feedback contributed to protocol adaptation and may assist future review protocol drafting.</li>
</ul>

<h3>Title: Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jinmin He, Kai Li, Yifan Zang, Haobo Fu, Qiang Fu, Junliang Xing, Jian Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06628">https://arxiv.org/abs/2507.06628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06628">https://arxiv.org/pdf/2507.06628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06628]] Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning(https://arxiv.org/abs/2507.06628)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Offline multi-task reinforcement learning aims to learn a unified policy capable of solving multiple tasks using only pre-collected task-mixed datasets, without requiring any online interaction with the environment. However, it faces significant challenges in effectively sharing knowledge across tasks. Inspired by the efficient knowledge abstraction observed in human learning, we propose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed to extract and utilize reusable skills to enhance knowledge transfer and task performance. Our approach uncovers reusable skills through a goal-oriented skill extraction process and leverages vector quantization to construct a discrete skill library. To mitigate class imbalances between broadly applicable and task-specific skills, we introduce a skill enhancement phase to refine the extracted skills. Furthermore, we integrate these skills using hierarchical policy learning, enabling the construction of a high-level policy that dynamically orchestrates discrete skills to accomplish specific tasks. Extensive experiments on diverse robotic manipulation tasks within the MetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.</li>
</ul>

<h3>Title: Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator</h3>
<ul>
<li><strong>Authors: </strong>Enda D.V. Bigarella</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06631">https://arxiv.org/abs/2507.06631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06631">https://arxiv.org/pdf/2507.06631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06631]] Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator(https://arxiv.org/abs/2507.06631)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This document reports on a method for detecting and preventing overfitting on data regressions, herein applied to mesh-like data structures. The mesh structure allows for the straightforward computation of the Laplace-operator second-order derivatives in a finite-difference fashion for noiseless data. Derivatives of the training data are computed on the original training mesh to serve as a true label of the entropy of the training data. Derivatives of the trained data are computed on a staggered mesh to identify oscillations in the interior of the original training mesh cells. The loss of the Laplace-operator derivatives is used for hyperparameter optimisation, achieving a reduction of unwanted oscillation through the minimisation of the entropy of the trained model. In this setup, testing does not require the splitting of points from the training data, and training is thus directly performed on all available training points. The Laplace operator applied to the trained data on a staggered mesh serves as a surrogate testing metric based on diffusion properties.</li>
</ul>

<h3>Title: Deep Disentangled Representation Network for Treatment Effect Estimation</h3>
<ul>
<li><strong>Authors: </strong>Hui Meng, Keping Yang, Xuyu Peng, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06650">https://arxiv.org/abs/2507.06650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06650">https://arxiv.org/pdf/2507.06650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06650]] Deep Disentangled Representation Network for Treatment Effect Estimation(https://arxiv.org/abs/2507.06650)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Estimating individual-level treatment effect from observational data is a fundamental problem in causal inference and has attracted increasing attention in the fields of education, healthcare, and public this http URL this work, we concentrate on the study of disentangled representation methods that have shown promising outcomes by decomposing observed covariates into instrumental, confounding, and adjustment factors. However, most of the previous work has primarily revolved around generative models or hard decomposition methods for covariates, which often struggle to guarantee the attainment of precisely disentangled factors. In order to effectively model different causal relationships, we propose a novel treatment effect estimation algorithm that incorporates a mixture of experts with multi-head attention and a linear orthogonal regularizer to softly decompose the pre-treatment variables, and simultaneously eliminates selection bias via importance sampling re-weighting techniques. We conduct extensive experiments on both public semi-synthetic and real-world production datasets. The experimental results clearly demonstrate that our algorithm outperforms the state-of-the-art methods focused on individual treatment effects.</li>
</ul>

<h3>Title: Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior</h3>
<ul>
<li><strong>Authors: </strong>Juncheng Mu, Chengwei Ren, Weixiang Zhang, Liang Pan, Xiao-Ping Zhang, Yue Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06651">https://arxiv.org/abs/2507.06651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06651">https://arxiv.org/pdf/2507.06651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06651]] Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior(https://arxiv.org/abs/2507.06651)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Learning cross-modal correspondences is essential for image-to-point cloud (I2P) registration. Existing methods achieve this mostly by utilizing metric learning to enforce feature alignment across modalities, disregarding the inherent modality gap between image and point data. Consequently, this paradigm struggles to ensure accurate cross-modal correspondences. To this end, inspired by the cross-modal generation success of recent large diffusion models, we propose Diff$^2$I2P, a fully Differentiable I2P registration framework, leveraging a novel and effective Diffusion prior for bridging the modality gap. Specifically, we propose a Control-Side Score Distillation (CSD) technique to distill knowledge from a depth-conditioned diffusion model to directly optimize the predicted transformation. However, the gradients on the transformation fail to backpropagate onto the cross-modal features due to the non-differentiability of correspondence retrieval and PnP solver. To this end, we further propose a Deformable Correspondence Tuning (DCT) module to estimate the correspondences in a differentiable way, followed by the transformation estimation using a differentiable PnP solver. With these two designs, the Diffusion model serves as a strong prior to guide the cross-modal feature learning of image and point cloud for forming robust correspondences, which significantly improves the registration. Extensive experimental results demonstrate that Diff$^2$I2P consistently outperforms SoTA I2P registration methods, achieving over 7% improvement in registration recall on the 7-Scenes benchmark.</li>
</ul>

<h3>Title: Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating for Privacy and Scalable Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Arthur Alexander Lim (1), Zhen Bin It (2), Jovan Bowen Heng (2), Tee Hui Teo (2) ((1) The University of Newcastle, Callaghan, Australia (2) Singapore University of Technology and Design, Singapore)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06652">https://arxiv.org/abs/2507.06652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06652">https://arxiv.org/pdf/2507.06652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06652]] Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating for Privacy and Scalable Decision Making(https://arxiv.org/abs/2507.06652)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Fuzzy systems are a way to allow machines, systems and frameworks to deal with uncertainty, which is not possible in binary systems that most computers use. These systems have already been deployed for certain use cases, and fuzzy systems could be further improved as proposed in this paper. Such technologies to draw inspiration from include machine learning and federated learning. Machine learning is one of the recent breakthroughs of technology and could be applied to fuzzy systems to further improve the results it produces. Federated learning is also one of the recent technologies that have huge potential, which allows machine learning training to improve by reducing privacy risk, reducing burden on networking infrastructure, and reducing latency of the latest model. Aspects from federated learning could be used to improve federated learning, such as applying the idea of updating the fuzzy rules that make up a key part of fuzzy systems, to further improve it over time. This paper discusses how these improvements would be implemented in fuzzy systems, and how it would improve fuzzy systems. It also discusses certain limitations on the potential improvements. It concludes that these proposed ideas and improvements require further investigation to see how far the improvements are, but the potential is there to improve fuzzy systems.</li>
</ul>

<h3>Title: Enhancing Diffusion Model Stability for Image Restoration via Gradient Management</h3>
<ul>
<li><strong>Authors: </strong>Hongjie Wu, Mingqin Zhang, Linchao He, Ji-Zhe Zhou, Jiancheng Lv</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06656">https://arxiv.org/abs/2507.06656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06656">https://arxiv.org/pdf/2507.06656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06656]] Enhancing Diffusion Model Stability for Image Restoration via Gradient Management(https://arxiv.org/abs/2507.06656)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have shown remarkable promise for image restoration by leveraging powerful priors. Prominent methods typically frame the restoration problem within a Bayesian inference framework, which iteratively combines a denoising step with a likelihood guidance step. However, the interactions between these two components in the generation process remain underexplored. In this paper, we analyze the underlying gradient dynamics of these components and identify significant instabilities. Specifically, we demonstrate conflicts between the prior and likelihood gradient directions, alongside temporal fluctuations in the likelihood gradient itself. We show that these instabilities disrupt the generative process and compromise restoration performance. To address these issues, we propose Stabilized Progressive Gradient Diffusion (SPGD), a novel gradient management technique. SPGD integrates two synergistic components: (1) a progressive likelihood warm-up strategy to mitigate gradient conflicts; and (2) adaptive directional momentum (ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive experiments across diverse restoration tasks demonstrate that SPGD significantly enhances generation stability, leading to state-of-the-art performance in quantitative metrics and visually superior results. Code is available at \href{this https URL}{here}.</li>
</ul>

<h3>Title: Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gennadii Iakovlev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06658">https://arxiv.org/abs/2507.06658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06658">https://arxiv.org/pdf/2507.06658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06658]] Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models(https://arxiv.org/abs/2507.06658)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This project introduces a new measure of elite polarization via actor and subject detection using artificial intelligence. I identify when politicians mention one another in parliamentary speeches, note who is speaking and who is being addressed, and assess the emotional temperature behind these evaluations. This maps how elites evaluate their various out-parties, allowing us to create an index of mutual out-party hostility, that is, elite polarization. While I analyzed polarization data over the past four decades for the UK, and two decades for Hungary and Italy, my approach lays the groundwork for a twenty-year, EU-wide time-series dataset on elite polarization. I obtain the results that can be aggregated by party and quarter. The resulting index demonstrates a good face validity: it reacts to events such as electoral campaigns, country- and party-level crises, and to parties losing and assuming power.</li>
</ul>

<h3>Title: Text-promptable Object Counting via Quantity Awareness Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Miaojing Shi, Xiaowen Zhang, Zijie Yue, Yong Luo, Cairong Zhao, Li Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06679">https://arxiv.org/abs/2507.06679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06679">https://arxiv.org/pdf/2507.06679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06679]] Text-promptable Object Counting via Quantity Awareness Enhancement(https://arxiv.org/abs/2507.06679)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in large vision-language models (VLMs) have shown remarkable progress in solving the text-promptable object counting problem. Representative methods typically specify text prompts with object category information in images. This however is insufficient for training the model to accurately distinguish the number of objects in the counting task. To this end, we propose QUANet, which introduces novel quantity-oriented text prompts with a vision-text quantity alignment loss to enhance the model's quantity awareness. Moreover, we propose a dual-stream adaptive counting decoder consisting of a Transformer stream, a CNN stream, and a number of Transformer-to-CNN enhancement adapters (T2C-adapters) for density map prediction. The T2C-adapters facilitate the effective knowledge communication and aggregation between the Transformer and CNN streams. A cross-stream quantity ranking loss is proposed in the end to optimize the ranking orders of predictions from the two streams. Extensive experiments on standard benchmarks such as FSC-147, CARPK, PUCPR+, and ShanghaiTech demonstrate our model's strong generalizability for zero-shot class-agnostic counting. Code is available at this https URL</li>
</ul>

<h3>Title: StixelNExT++: Lightweight Monocular Scene Segmentation and Representation for Collective Perception</h3>
<ul>
<li><strong>Authors: </strong>Marcel Vosshans, Omar Ait-Aider, Youcef Mezouar, Markus Enzweiler</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06687">https://arxiv.org/abs/2507.06687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06687">https://arxiv.org/pdf/2507.06687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06687]] StixelNExT++: Lightweight Monocular Scene Segmentation and Representation for Collective Perception(https://arxiv.org/abs/2507.06687)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper presents StixelNExT++, a novel approach to scene representation for monocular perception systems. Building on the established Stixel representation, our method infers 3D Stixels and enhances object segmentation by clustering smaller 3D Stixel units. The approach achieves high compression of scene information while remaining adaptable to point cloud and bird's-eye-view representations. Our lightweight neural network, trained on automatically generated LiDAR-based ground truth, achieves real-time performance with computation times as low as 10 ms per frame. Experimental results on the Waymo dataset demonstrate competitive performance within a 30-meter range, highlighting the potential of StixelNExT++ for collective perception in autonomous systems.</li>
</ul>

<h3>Title: Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement</h3>
<ul>
<li><strong>Authors: </strong>Michael Bloesch, Markus Wulfmeier, Philemon Brakel, Todor Davchev, Martina Zambelli, Jost Tobias Springenberg, Abbas Abdolmaleki, William F Whitney, Nicolas Heess, Roland Hafner, Martin Riedmiller</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06701">https://arxiv.org/abs/2507.06701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06701">https://arxiv.org/pdf/2507.06701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06701]] Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement(https://arxiv.org/abs/2507.06701)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Imitation Learning from Observation (IfO) offers a powerful way to learn behaviors at large-scale: Unlike behavior cloning or offline reinforcement learning, IfO can leverage action-free demonstrations and thus circumvents the need for costly action-labeled demonstrations or reward functions. However, current IfO research focuses on idealized scenarios with mostly bimodal-quality data distributions, restricting the meaningfulness of the results. In contrast, this paper investigates more nuanced distributions and introduces a method to learn from such data, moving closer to a paradigm in which imitation learning can be performed iteratively via self-improvement. Our method adapts RL-based imitation learning to action-free demonstrations, using a value function to transfer information between expert and non-expert data. Through comprehensive evaluation, we delineate the relation between different data distributions and the applicability of algorithms and highlight the limitations of established methods. Our findings provide valuable insights for developing more robust and practical IfO techniques on a path to scalable behaviour learning.</li>
</ul>

<h3>Title: Approximating Euler Totient Function using Linear Regression on RSA moduli</h3>
<ul>
<li><strong>Authors: </strong>Gilda Rech Bansimba, Regis F. Babindamana, Beni Blaug N. Ibara</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06706">https://arxiv.org/abs/2507.06706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06706">https://arxiv.org/pdf/2507.06706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06706]] Approximating Euler Totient Function using Linear Regression on RSA moduli(https://arxiv.org/abs/2507.06706)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The security of the RSA cryptosystem is based on the intractability of computing Euler's totient function phi(n) for large integers n. Although deriving phi(n) deterministically remains computationally infeasible for cryptographically relevant bit lengths, and machine learning presents a promising alternative for constructing efficient approximations. In this work, we explore a machine learning approach to approximate Euler's totient function phi using linear regression models. We consider a dataset of RSA moduli of 64, 128, 256, 512 and 1024 bits along with their corresponding totient values. The regression model is trained to capture the relationship between the modulus and its totient, and tested on unseen samples to evaluate its prediction accuracy. Preliminary results suggest that phi can be approximated within a small relative error margin, which may be sufficient to aid in certain classes of RSA attacks. This research opens a direction for integrating statistical learning techniques into cryptanalysis, providing insights into the feasibility of attacking cryptosystems using approximation based strategies.</li>
</ul>

<h3>Title: PINN-Obs: Physics-Informed Neural Network-Based Observer for Nonlinear Dynamical Systems</h3>
<ul>
<li><strong>Authors: </strong>Ayoub Farkane, Mohamed Boutayeb, Mustapha Oudani, Mounir Ghogho</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS, nlin.CD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06712">https://arxiv.org/abs/2507.06712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06712">https://arxiv.org/pdf/2507.06712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06712]] PINN-Obs: Physics-Informed Neural Network-Based Observer for Nonlinear Dynamical Systems(https://arxiv.org/abs/2507.06712)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>State estimation for nonlinear dynamical systems is a critical challenge in control and engineering applications, particularly when only partial and noisy measurements are available. This paper introduces a novel Adaptive Physics-Informed Neural Network-based Observer (PINN-Obs) for accurate state estimation in nonlinear systems. Unlike traditional model-based observers, which require explicit system transformations or linearization, the proposed framework directly integrates system dynamics and sensor data into a physics-informed learning process. The observer adaptively learns an optimal gain matrix, ensuring convergence of the estimated states to the true system states. A rigorous theoretical analysis establishes formal convergence guarantees, demonstrating that the proposed approach achieves uniform error minimization under mild observability conditions. The effectiveness of PINN-Obs is validated through extensive numerical simulations on diverse nonlinear systems, including an induction motor model, a satellite motion system, and benchmark academic examples. Comparative experimental studies against existing observer designs highlight its superior accuracy, robustness, and adaptability.</li>
</ul>

<h3>Title: CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context Aware Text Generation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Garapati Keerthana, Manik Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06715">https://arxiv.org/abs/2507.06715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06715">https://arxiv.org/pdf/2507.06715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06715]] CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context Aware Text Generation with LLMs(https://arxiv.org/abs/2507.06715)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), including zero-shot and few-shot paradigms, have shown promising capabilities in clinical text generation. However, real-world applications face two key challenges: (1) patient data is highly unstructured, heterogeneous, and scattered across multiple note types and (2) clinical notes are often long and semantically dense, making naive prompting infeasible due to context length constraints and the risk of omitting clinically relevant information. We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a domain-specific framework for structured and clinically grounded text generation using LLMs. It incorporates a novel hierarchical chunking strategy that respects clinical document structure and introduces a task-specific dual-stage retrieval mechanism. The global stage identifies relevant note types using evidence-based queries, while the local stage extracts high-value content within those notes creating relevance at both document and section levels. We apply the system to generate structured progress notes for individual hospital visits using 15 clinical note types from the MIMIC-III dataset. Experiments show that it preserves temporal and semantic alignment across visits, achieving an average alignment score of 87.7%, surpassing the 80.7% baseline from real clinician-authored notes. The generated outputs also demonstrate high consistency across LLMs, reinforcing deterministic behavior essential for reproducibility, reliability, and clinical trust.</li>
</ul>

<h3>Title: A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding</h3>
<ul>
<li><strong>Authors: </strong>Zhenyang Liu, Sixiao Zheng, Siyu Chen, Cairong Zhao, Longfei Liang, Xiangyang Xue, Yanwei Fu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06719">https://arxiv.org/abs/2507.06719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06719">https://arxiv.org/pdf/2507.06719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06719]] A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding(https://arxiv.org/abs/2507.06719)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Open-vocabulary 3D visual grounding aims to localize target objects based on free-form language queries, which is crucial for embodied AI applications such as autonomous navigation, robotics, and augmented reality. Learning 3D language fields through neural representations enables accurate understanding of 3D scenes from limited viewpoints and facilitates the localization of target objects in complex environments. However, existing language field methods struggle to accurately localize instances using spatial relations in language queries, such as ``the book on the chair.'' This limitation mainly arises from inadequate reasoning about spatial relations in both language queries and 3D scenes. In this work, we propose SpatialReasoner, a novel neural representation-based framework with large language model (LLM)-driven spatial reasoning that constructs a visual properties-enhanced hierarchical feature field for open-vocabulary 3D visual grounding. To enable spatial reasoning in language queries, SpatialReasoner fine-tunes an LLM to capture spatial relations and explicitly infer instructions for the target, anchor, and spatial relation. To enable spatial reasoning in 3D scenes, SpatialReasoner incorporates visual properties (opacity and color) to construct a hierarchical feature field. This field represents language and instance features using distilled CLIP features and masks extracted via the Segment Anything Model (SAM). The field is then queried using the inferred instructions in a hierarchical manner to localize the target 3D instance based on the spatial relation in the language query. Extensive experiments show that our framework can be seamlessly integrated into different neural representations, outperforming baseline models in 3D visual grounding while empowering their spatial reasoning capability.</li>
</ul>

<h3>Title: On the Effect of Uncertainty on Layer-wise Inference Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Sunwoo Kim, Haneul Yoo, Alice Oh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06722">https://arxiv.org/abs/2507.06722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06722">https://arxiv.org/pdf/2507.06722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06722]] On the Effect of Uncertainty on Layer-wise Inference Dynamics(https://arxiv.org/abs/2507.06722)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Understanding how large language models (LLMs) internally represent and process their predictions is central to detecting uncertainty and preventing hallucinations. While several studies have shown that models encode uncertainty in their hidden states, it is underexplored how this affects the way they process such hidden states. In this work, we demonstrate that the dynamics of output token probabilities across layers for certain and uncertain outputs are largely aligned, revealing that uncertainty does not seem to affect inference dynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to analyze the layer-wise probability trajectories of final prediction tokens across 11 datasets and 5 models. Using incorrect predictions as those with higher epistemic uncertainty, our results show aligned trajectories for certain and uncertain predictions that both observe abrupt increases in confidence at similar layers. We balance this finding by showing evidence that more competent models may learn to process uncertainty differently. Our findings challenge the feasibility of leveraging simplistic methods for detecting uncertainty at inference. More broadly, our work demonstrates how interpretability methods may be used to investigate the way uncertainty affects inference.</li>
</ul>

<h3>Title: Hierarchical Feature Alignment for Gloss-Free Sign Language Translation</h3>
<ul>
<li><strong>Authors: </strong>Sobhan Asasi, Mohamed Ilyes Lakhal, Richard Bowden</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06732">https://arxiv.org/abs/2507.06732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06732">https://arxiv.org/pdf/2507.06732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06732]] Hierarchical Feature Alignment for Gloss-Free Sign Language Translation(https://arxiv.org/abs/2507.06732)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Sign Language Translation (SLT) attempts to convert sign language videos into spoken sentences. However, many existing methods struggle with the disparity between visual and textual representations during end-to-end learning. Gloss-based approaches help to bridge this gap by leveraging structured linguistic information. While, gloss-free methods offer greater flexibility and remove the burden of annotation, they require effective alignment strategies. Recent advances in Large Language Models (LLMs) have enabled gloss-free SLT by generating text-like representations from sign videos. In this work, we introduce a novel hierarchical pre-training strategy inspired by the structure of sign language, incorporating pseudo-glosses and contrastive video-language alignment. Our method hierarchically extracts features at frame, segment, and video levels, aligning them with pseudo-glosses and the spoken sentence to enhance translation quality. Experiments demonstrate that our approach improves BLEU-4 and ROUGE scores while maintaining efficiency.</li>
</ul>

<h3>Title: Residual Prior-driven Frequency-aware Network for Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Guan Zheng, Xue Wang, Wenhua Qian, Peng Liu, Runzhuo Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06735">https://arxiv.org/abs/2507.06735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06735">https://arxiv.org/pdf/2507.06735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06735]] Residual Prior-driven Frequency-aware Network for Image Fusion(https://arxiv.org/abs/2507.06735)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Image fusion aims to integrate complementary information across modalities to generate high-quality fused images, thereby enhancing the performance of high-level vision tasks. While global spatial modeling mechanisms show promising results, constructing long-range feature dependencies in the spatial domain incurs substantial computational costs. Additionally, the absence of ground-truth exacerbates the difficulty of capturing complementary features effectively. To tackle these challenges, we propose a Residual Prior-driven Frequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a dual-branch feature extraction framework: the Residual Prior Module (RPM) extracts modality-specific difference information from residual maps, thereby providing complementary priors for fusion; the Frequency Domain Fusion Module (FDFM) achieves efficient global feature modeling and integration through frequency-domain convolution. Additionally, the Cross Promotion Module (CPM) enhances the synergistic perception of local details and global structures through bidirectional feature interaction. During training, we incorporate an auxiliary decoder and saliency structure loss to strengthen the model's sensitivity to modality-specific differences. Furthermore, a combination of adaptive weight-based frequency contrastive loss and SSIM loss effectively constrains the solution space, facilitating the joint capture of local details and global features while ensuring the retention of complementary information. Extensive experiments validate the fusion performance of RPFNet, which effectively integrates discriminative features, enhances texture details and salient objects, and can effectively facilitate the deployment of the high-level vision task.</li>
</ul>

<h3>Title: DIFFUMA: High-Fidelity Spatio-Temporal Video Prediction via Dual-Path Mamba and Diffusion Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Xie, Weifeng Cao, Jun Shi, Yangyang Hu, Hui Liang, Wanyong Liang, Xiaoliang Qian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06738">https://arxiv.org/abs/2507.06738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06738">https://arxiv.org/pdf/2507.06738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06738]] DIFFUMA: High-Fidelity Spatio-Temporal Video Prediction via Dual-Path Mamba and Diffusion Enhancement(https://arxiv.org/abs/2507.06738)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Spatio-temporal video prediction plays a pivotal role in critical domains, ranging from weather forecasting to industrial automation. However, in high-precision industrial scenarios such as semiconductor manufacturing, the absence of specialized benchmark datasets severely hampers research on modeling and predicting complex processes. To address this challenge, we make a twofold this http URL, we construct and release the Chip Dicing Lane Dataset (CHDL), the first public temporal image dataset dedicated to the semiconductor wafer dicing process. Captured via an industrial-grade vision system, CHDL provides a much-needed and challenging benchmark for high-fidelity process modeling, defect detection, and digital twin this http URL, we propose DIFFUMA, an innovative dual-path prediction architecture specifically designed for such fine-grained dynamics. The model captures global long-range temporal context through a parallel Mamba module, while simultaneously leveraging a diffusion module, guided by temporal features, to restore and enhance fine-grained spatial details, effectively combating feature degradation. Experiments demonstrate that on our CHDL benchmark, DIFFUMA significantly outperforms existing methods, reducing the Mean Squared Error (MSE) by 39% and improving the Structural Similarity (SSIM) from 0.926 to a near-perfect 0.988. This superior performance also generalizes to natural phenomena datasets. Our work not only delivers a new state-of-the-art (SOTA) model but, more importantly, provides the community with an invaluable data resource to drive future research in industrial AI.</li>
</ul>

<h3>Title: PromptTea: Let Prompts Tell TeaCache the Optimal Threshold</h3>
<ul>
<li><strong>Authors: </strong>Zishen Huang, Chunyu Yang, Mengyuan Ren</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06739">https://arxiv.org/abs/2507.06739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06739">https://arxiv.org/pdf/2507.06739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06739]] PromptTea: Let Prompts Tell TeaCache the Optimal Threshold(https://arxiv.org/abs/2507.06739)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite recent progress in video generation, inference speed remains a major bottleneck. A common acceleration strategy involves reusing model outputs via caching mechanisms at fixed intervals. However, we find that such fixed-frequency reuse significantly degrades quality in complex scenes, while manually tuning reuse thresholds is inefficient and lacks robustness. To address this, we propose Prompt-Complexity-Aware (PCA) caching, a method that automatically adjusts reuse thresholds based on scene complexity estimated directly from the input prompt. By incorporating prompt-derived semantic cues, PCA enables more adaptive and informed reuse decisions than conventional caching methods. We also revisit the assumptions behind TeaCache and identify a key limitation: it suffers from poor input-output relationship modeling due to an oversimplified prior. To overcome this, we decouple the noisy input, enhance the contribution of meaningful textual information, and improve the model's predictive accuracy through multivariate polynomial feature expansion. To further reduce computational cost, we replace the static CFGCache with DynCFGCache, a dynamic mechanism that selectively reuses classifier-free guidance (CFG) outputs based on estimated output variations. This allows for more flexible reuse without compromising output quality. Extensive experiments demonstrate that our approach achieves significant acceleration-for example, 2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across a range of scenes.</li>
</ul>

<h3>Title: PenTest2.0: Towards Autonomous Privilege Escalation Using GenAI</h3>
<ul>
<li><strong>Authors: </strong>Haitham S. Al-Sinani, Chris J. Mitchell</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06742">https://arxiv.org/abs/2507.06742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06742">https://arxiv.org/pdf/2507.06742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06742]] PenTest2.0: Towards Autonomous Privilege Escalation Using GenAI(https://arxiv.org/abs/2507.06742)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Ethical hacking today relies on highly skilled practitioners executing complex sequences of commands, which is inherently time-consuming, difficult to scale, and prone to human error. To help mitigate these limitations, we previously introduced 'PenTest++', an AI-augmented system combining automation with generative AI supporting ethical hacking workflows. However, a key limitation of PenTest++ was its lack of support for privilege escalation, a crucial element of ethical hacking. In this paper we present 'PenTest2.0', a substantial evolution of PenTest++ supporting automated privilege escalation driven entirely by Large Language Model reasoning. It also incorporates several significant enhancements: 'Retrieval-Augmented Generation', including both one-line and offline modes; 'Chain-of-Thought' prompting for intermediate reasoning; persistent 'PenTest Task Trees' to track goal progression across turns; and the optional integration of human-authored hints. We describe how it operates, present a proof-of-concept prototype, and discuss its benefits and limitations. We also describe application of the system to a controlled Linux target, showing it can carry out multi-turn, adaptive privilege escalation. We explain the rationale behind its core design choices, and provide comprehensive testing results and cost analysis. Our findings indicate that 'PenTest2.0' represents a meaningful step toward practical, scalable, AI-automated penetration testing, whilst highlighting the shortcomings of generative AI systems, particularly their sensitivity to prompt structure, execution context, and semantic drift, reinforcing the need for further research and refinement in this emerging space. Keywords: AI, Ethical Hacking, Privilege Escalation, GenAI, ChatGPT, LLM (Large Language Model), HITL (Human-in-the-Loop)</li>
</ul>

<h3>Title: Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching</h3>
<ul>
<li><strong>Authors: </strong>Yafei Zhang, Yongle Shang, Huafeng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06744">https://arxiv.org/abs/2507.06744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06744">https://arxiv.org/pdf/2507.06744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06744]] Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching(https://arxiv.org/abs/2507.06744)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Weakly supervised text-to-person image matching, as a crucial approach to reducing models' reliance on large-scale manually labeled samples, holds significant research value. However, existing methods struggle to predict complex one-to-many identity relationships, severely limiting performance improvements. To address this challenge, we propose a local-and-global dual-granularity identity association mechanism. Specifically, at the local level, we explicitly establish cross-modal identity relationships within a batch, reinforcing identity constraints across different modalities and enabling the model to better capture subtle differences and correlations. At the global level, we construct a dynamic cross-modal identity association network with the visual modality as the anchor and introduce a confidence-based dynamic adjustment mechanism, effectively enhancing the model's ability to identify weakly associated samples while improving overall sensitivity. Additionally, we propose an information-asymmetric sample pair construction method combined with consistency learning to tackle hard sample mining and enhance model robustness. Experimental results demonstrate that the proposed method substantially boosts cross-modal matching accuracy, providing an efficient and practical solution for text-to-person image matching.</li>
</ul>

<h3>Title: KAConvText: Novel Approach to Burmese Sentence Classification using Kolmogorov-Arnold Convolution</h3>
<ul>
<li><strong>Authors: </strong>Ye Kyaw Thu, Thura Aung, Thazin Myint Oo, Thepchai Supnithi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06753">https://arxiv.org/abs/2507.06753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06753">https://arxiv.org/pdf/2507.06753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06753]] KAConvText: Novel Approach to Burmese Sentence Classification using Kolmogorov-Arnold Convolution(https://arxiv.org/abs/2507.06753)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This paper presents the first application of Kolmogorov-Arnold Convolution for Text (KAConvText) in sentence classification, addressing three tasks: imbalanced binary hate speech detection, balanced multiclass news classification, and imbalanced multiclass ethnic language identification. We investigate various embedding configurations, comparing random to fastText embeddings in both static and fine-tuned settings, with embedding dimensions of 100 and 300 using CBOW and Skip-gram models. Baselines include standard CNNs and CNNs augmented with a Kolmogorov-Arnold Network (CNN-KAN). In addition, we investigated KAConvText with different classification heads - MLP and KAN, where using KAN head supports enhanced interpretability. Results show that KAConvText-MLP with fine-tuned fastText embeddings achieves the best performance of 91.23% accuracy (F1-score = 0.9109) for hate speech detection, 92.66% accuracy (F1-score = 0.9267) for news classification, and 99.82% accuracy (F1-score = 0.9982) for language identification.</li>
</ul>

<h3>Title: FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views</h3>
<ul>
<li><strong>Authors: </strong>Saif Ur Rehman Khan, Muhammad Nabeel Asim, Sebastian Vollmer, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06763">https://arxiv.org/abs/2507.06763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06763">https://arxiv.org/pdf/2507.06763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06763]] FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views(https://arxiv.org/abs/2507.06763)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>The framework is designed to improve performance in the analysis of combined as well as single anatomical perspectives for MRI disease diagnosis. It specifically addresses the performance degradation observed in state-of-the-art (SOTA) models, particularly when processing axial, coronal, and sagittal anatomical planes. The paper introduces the FOLC-Net framework, which incorporates a novel federated-optimized lightweight architecture with approximately 1.217 million parameters and a storage requirement of only 0.9 MB. FOLC-Net integrates Manta-ray foraging optimization (MRFO) mechanisms for efficient model structure generation, global model cloning for scalable training, and ConvNeXt for enhanced client adaptability. The model was evaluated on combined multi-view data as well as individual views, such as axial, coronal, and sagittal, to assess its robustness in various medical imaging scenarios. Moreover, FOLC-Net tests a ShallowFed model on different data to evaluate its ability to generalize beyond the training dataset. The results show that FOLC-Net outperforms existing models, particularly in the challenging sagittal view. For instance, FOLC-Net achieved an accuracy of 92.44% on the sagittal view, significantly higher than the 88.37% accuracy of study method (DL + Residual Learning) and 88.95% of DL models. Additionally, FOLC-Net demonstrated improved accuracy across all individual views, providing a more reliable and robust solution for medical image analysis in decentralized environments. FOLC-Net addresses the limitations of existing SOTA models by providing a framework that ensures better adaptability to individual views while maintaining strong performance in multi-view settings. The incorporation of MRFO, global model cloning, and ConvNeXt ensures that FOLC-Net performs better in real-world medical applications.</li>
</ul>

<h3>Title: Robust Deep Network Learning of Nonlinear Regression Tasks by Parametric Leaky Exponential Linear Units (LELUs) and a Diffusion Metric</h3>
<ul>
<li><strong>Authors: </strong>Enda D.V. Bigarella</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06765">https://arxiv.org/abs/2507.06765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06765">https://arxiv.org/pdf/2507.06765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06765]] Robust Deep Network Learning of Nonlinear Regression Tasks by Parametric Leaky Exponential Linear Units (LELUs) and a Diffusion Metric(https://arxiv.org/abs/2507.06765)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>This document proposes a parametric activation function (ac.f.) aimed at improving multidimensional nonlinear data regression. It is a established knowledge that nonlinear ac.f.'s are required for learning nonlinear datasets. This work shows that smoothness and gradient properties of the ac.f. further impact the performance of large neural networks in terms of overfitting and sensitivity to model parameters. Smooth but vanishing-gradient ac.f.'s such as ELU or SiLU have limited performance and non-smooth ac.f.'s such as RELU and Leaky-RELU further impart discontinuity in the trained model. Improved performance is demonstrated with a smooth "Leaky Exponential Linear Unit", with non-zero gradient that can be trained. A novel diffusion-loss metric is also proposed to gauge the performance of the trained models in terms of overfitting.</li>
</ul>

<h3>Title: Checklist Engineering Empowers Multilingual LLM Judges</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Ghiasvand Mohammadkhani, Hamid Beigy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06774">https://arxiv.org/abs/2507.06774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06774">https://arxiv.org/pdf/2507.06774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06774]] Checklist Engineering Empowers Multilingual LLM Judges(https://arxiv.org/abs/2507.06774)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automated text evaluation has long been a central issue in Natural Language Processing (NLP). Recently, the field has shifted toward using Large Language Models (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While promising and easily adaptable across tasks, this approach has seen limited exploration in multilingual contexts. Existing multilingual studies often rely on proprietary models or require extensive training data for fine-tuning, raising concerns about cost, time, and efficiency. In this paper, we propose Checklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free framework that uses checklist intuition for multilingual evaluation with an open-source model. Experiments across multiple languages and three benchmark datasets, under both pointwise and pairwise settings, show that our method generally surpasses the baselines and performs on par with the GPT-4o model.</li>
</ul>

<h3>Title: Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications</h3>
<ul>
<li><strong>Authors: </strong>Seonwu Kim, Yohan Na, Kihun Kim, Hanhee Cho, Geun Lim, Mintae Kim, Seongik Park, Ki Hyun Kim, Youngsub Han, Byoung-Ki Jeon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06795">https://arxiv.org/abs/2507.06795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06795">https://arxiv.org/pdf/2507.06795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06795]] Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications(https://arxiv.org/abs/2507.06795)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The emergence of open-source large language models (LLMs) has expanded opportunities for enterprise applications; however, many organizations still lack the infrastructure to deploy and maintain large-scale models. As a result, small LLMs (sLLMs) have become a practical alternative, despite their inherent performance limitations. While Domain Adaptive Continual Pretraining (DACP) has been previously explored as a method for domain adaptation, its utility in commercial applications remains under-examined. In this study, we validate the effectiveness of applying a DACP-based recipe across diverse foundation models and service domains. Through extensive experiments and real-world evaluations, we demonstrate that DACP-applied sLLMs achieve substantial gains in target domain performance while preserving general capabilities, offering a cost-efficient and scalable solution for enterprise-level deployment.</li>
</ul>

<h3>Title: Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams</h3>
<ul>
<li><strong>Authors: </strong>Matthew Anderson Hendricks, Alice Cicirello</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06803">https://arxiv.org/abs/2507.06803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06803">https://arxiv.org/pdf/2507.06803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06803]] Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams(https://arxiv.org/abs/2507.06803)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper contributes to speeding up the design and deployment of engineering dynamical systems by proposing a strategy for exploiting domain and expert knowledge for the automated generation of dynamical system computational model starting from a corpus of document relevant to the dynamical system of interest and an input document describing the specific system. This strategy is implemented in five steps and, crucially, it uses system modeling language diagrams (SysML) to extract accurate information about the dependencies, attributes, and operations of components. Natural Language Processing (NLP) strategies and Large Language Models (LLMs) are employed in specific tasks to improve intermediate outputs of the SySML diagrams automated generation, such as: list of key nouns; list of extracted relationships; list of key phrases and key relationships; block attribute values; block relationships; and BDD diagram generation. The applicability of automated SysML diagram generation is illustrated with different case studies. The computational models of complex dynamical systems from SysML diagrams are then obtained via code generation and computational model generation steps. In the code generation step, NLP strategies are used for summarization, while LLMs are used for validation only. The proposed approach is not limited to a specific system, domain, or computational software. The applicability of the proposed approach is shown via an end-to-end example from text to model of a simple pendulum, showing improved performance compared to results yielded by LLMs only.</li>
</ul>

<h3>Title: GreenHyperSpectra: A multi-source hyperspectral dataset for global vegetation trait prediction</h3>
<ul>
<li><strong>Authors: </strong>Eya Cherif (1, 2 and 3), Arthur Ouaknine (3 and 4), Luke A. Brown (5), Phuong D. Dao (6, 7 and 8), Kyle R. Kovach (9), Bing Lu (10), Daniel Mederer (1), Hannes Feilhauer (1, 2, 12 and 13), Teja Kattenborn (11 and 12), David Rolnick (3 and 4) ((1) Institute for Earth System Science and Remote Sensing, Leipzig University, Germany, (2) Center for Scalable Data Analytics and Artificial Intelligence (<a href="http://ScaDS.AI" rel="external noopener nofollow" class="link-external link-http">this http URL</a>), Leipzig University, Germany, (3) Mila Quebec AI Institute, Canada, (4) McGill University, Canada, (5) School of Science, Engineering and Environment, University of Salford, UK, (6) Department of Agricultural Biology, Colorado State University, USA, (7) Graduate Degree Program in Ecology, Colorado State University, USA, (8) School of Global Environmental Sustainability, Colorado State University, USA, (9) Department of Forest and Wildlife Ecology, University of Wisconsin, USA, (10) Department of Geography, Simon Fraser University, Canada, (11) Chair of Sensor-based Geoinformatics (geosense), University of Freiburg, Germany, (12) German Centre for Integrative Biodiversity Research (iDiv), Halle-Jena-Leipzig, Germany, (13) Helmholtz-Centre for Environmental Research (UFZ), Leipzig, Germany)</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06806">https://arxiv.org/abs/2507.06806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06806">https://arxiv.org/pdf/2507.06806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06806]] GreenHyperSpectra: A multi-source hyperspectral dataset for global vegetation trait prediction(https://arxiv.org/abs/2507.06806)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Plant traits such as leaf carbon content and leaf mass are essential variables in the study of biodiversity and climate change. However, conventional field sampling cannot feasibly cover trait variation at ecologically meaningful spatial scales. Machine learning represents a valuable solution for plant trait prediction across ecosystems, leveraging hyperspectral data from remote sensing. Nevertheless, trait prediction from hyperspectral data is challenged by label scarcity and substantial domain shifts (\eg across sensors, ecological distributions), requiring robust cross-domain methods. Here, we present GreenHyperSpectra, a pretraining dataset encompassing real-world cross-sensor and cross-ecosystem samples designed to benchmark trait prediction with semi- and self-supervised methods. We adopt an evaluation framework encompassing in-distribution and out-of-distribution scenarios. We successfully leverage GreenHyperSpectra to pretrain label-efficient multi-output regression models that outperform the state-of-the-art supervised baseline. Our empirical analyses demonstrate substantial improvements in learning spectral representations for trait prediction, establishing a comprehensive methodological framework to catalyze research at the intersection of representation learning and plant functional traits assessment. All code and data are available at: this https URL.</li>
</ul>

<h3>Title: Democratizing High-Fidelity Co-Speech Gesture Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Xu Yang, Shaoli Huang, Shenbo Xie, Xuelin Chen, Yifei Liu, Changxing Ding</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06812">https://arxiv.org/abs/2507.06812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06812">https://arxiv.org/pdf/2507.06812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06812]] Democratizing High-Fidelity Co-Speech Gesture Video Generation(https://arxiv.org/abs/2507.06812)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Co-speech gesture video generation aims to synthesize realistic, audio-aligned videos of speakers, complete with synchronized facial expressions and body gestures. This task presents challenges due to the significant one-to-many mapping between audio and visual content, further complicated by the scarcity of large-scale public datasets and high computational demands. We propose a lightweight framework that utilizes 2D full-body skeletons as an efficient auxiliary condition to bridge audio signals with visual outputs. Our approach introduces a diffusion model conditioned on fine-grained audio segments and a skeleton extracted from the speaker's reference image, predicting skeletal motions through skeleton-audio feature fusion to ensure strict audio coordination and body shape consistency. The generated skeletons are then fed into an off-the-shelf human video generation model with the speaker's reference image to synthesize high-fidelity videos. To democratize research, we present CSG-405-the first public dataset with 405 hours of high-resolution videos across 71 speech types, annotated with 2D skeletons and diverse speaker demographics. Experiments show that our method exceeds state-of-the-art approaches in visual quality and synchronization while generalizing across speakers and contexts.</li>
</ul>

<h3>Title: Intrinsic Training Signals for Federated Learning Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Cosimo Fiorini, Matteo Mosconi, Pietro Buzzega, Riccardo Salami, Simone Calderara</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06813">https://arxiv.org/abs/2507.06813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06813">https://arxiv.org/pdf/2507.06813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06813]] Intrinsic Training Signals for Federated Learning Aggregation(https://arxiv.org/abs/2507.06813)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, explainability</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy. While existing approaches for aggregating client-specific classification heads and adapted backbone parameters require architectural modifications or loss function changes, our method uniquely leverages intrinsic training signals already available during standard optimization. We present LIVAR (Layer Importance and VARiance-based merging), which introduces: i) a variance-weighted classifier aggregation scheme using naturally emergent feature statistics, and ii) an explainability-driven LoRA merging technique based on SHAP analysis of existing update parameter patterns. Without any architectural overhead, LIVAR achieves state-of-the-art performance on multiple benchmarks while maintaining seamless integration with existing FL methods. This work demonstrates that effective model merging can be achieved solely through existing training signals, establishing a new paradigm for efficient federated model aggregation. The code will be made publicly available upon acceptance.</li>
</ul>

<h3>Title: Comprehensive Evaluation of Prototype Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Philipp Schlinge, Steffen Meinert, Martin Atzmueller</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06819">https://arxiv.org/abs/2507.06819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06819">https://arxiv.org/pdf/2507.06819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06819]] Comprehensive Evaluation of Prototype Neural Networks(https://arxiv.org/abs/2507.06819)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Prototype models are an important method for explainable artificial intelligence (XAI) and interpretable machine learning. In this paper, we perform an in-depth analysis of a set of prominent prototype models including ProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive set of metrics. In addition to applying standard metrics from literature, we propose several new metrics to further complement the analysis of model interpretability. In our experimentation, we apply the set of prototype models on a diverse set of datasets including fine-grained classification, Non-IID settings and multi-label classification to further contrast the performance. Furthermore, we also provide our code as an open-source library, which facilitates simple application of the metrics itself, as well as extensibility - providing the option for easily adding new metrics and models. this https URL</li>
</ul>

<h3>Title: Adaptive Termination for Multi-round Parallel Reasoning: An Universal Semantic Entropy-Guided Framework</h3>
<ul>
<li><strong>Authors: </strong>Zenan Xu, Zexuan Qiu, Guanhua Huang, Kun Li, Siheng Li, Chenchen Zhang, Kejiao Li, Qi Yi, Yuhao Jiang, Bo Zhou, Fengzong Lian, Zhanhui Kang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06829">https://arxiv.org/abs/2507.06829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06829">https://arxiv.org/pdf/2507.06829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06829]] Adaptive Termination for Multi-round Parallel Reasoning: An Universal Semantic Entropy-Guided Framework(https://arxiv.org/abs/2507.06829)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have accelerated progress toward artificial general intelligence, with inference-time scaling emerging as a key technique. Contemporary approaches leverage either sequential reasoning (iteratively extending chains of thought) or parallel reasoning (generating multiple solutions simultaneously) to scale inference. However, both paradigms face fundamental limitations: sequential scaling typically relies on arbitrary token budgets for termination, leading to inefficiency or premature cutoff; while parallel scaling often lacks coordination among parallel branches and requires intrusive fine-tuning to perform effectively. In light of these challenges, we aim to design a flexible test-time collaborative inference framework that exploits the complementary strengths of both sequential and parallel reasoning paradigms. Towards this goal, the core challenge lies in developing an efficient and accurate intrinsic quality metric to assess model responses during collaborative inference, enabling dynamic control and early termination of the reasoning trace. To address this challenge, we introduce semantic entropy (SE), which quantifies the semantic diversity of parallel model responses and serves as a robust indicator of reasoning quality due to its strong negative correlation with accuracy...</li>
</ul>

<h3>Title: Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Tao Feng, Xianbing Zhao, Zhenhua Chen, Tien Tsin Wong, Hamid Rezatofighi, Gholamreza Haffari, Lizhen Qu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06830">https://arxiv.org/abs/2507.06830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06830">https://arxiv.org/pdf/2507.06830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06830]] Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation(https://arxiv.org/abs/2507.06830)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion-based and autoregressive video generation models have achieved remarkable visual realism. However, these models typically lack accurate physical alignment, failing to replicate real-world dynamics in object motion. This limitation arises primarily from their reliance on learned statistical correlations rather than capturing mechanisms adhering to physical laws. To address this issue, we introduce a novel framework that integrates symbolic regression (SR) and trajectory-guided image-to-video (I2V) models for physics-grounded video forecasting. Our approach extracts motion trajectories from input videos, uses a retrieval-based pre-training mechanism to enhance symbolic regression, and discovers equations of motion to forecast physically accurate future trajectories. These trajectories then guide video generation without requiring fine-tuning of existing models. Evaluated on scenarios in Classical Mechanics, including spring-mass, pendulums, and projectile motions, our method successfully recovers ground-truth analytical equations and improves the physical alignment of generated videos over baseline methods.</li>
</ul>

<h3>Title: Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Joelle Hanna, Damian Borth</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06848">https://arxiv.org/abs/2507.06848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06848">https://arxiv.org/pdf/2507.06848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06848]] Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation(https://arxiv.org/abs/2507.06848)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that has been extensively studied in recent years. Traditional approaches often rely on external modules like Class Activation Maps to highlight regions of interest and generate pseudo segmentation masks. In this work, we propose an end-to-end method that directly utilizes the attention maps learned by a Vision Transformer (ViT) for WSSS. We propose training a sparse ViT with multiple [CLS] tokens (one for each class), using a random masking strategy to promote [CLS] token - class assignment. At inference time, we aggregate the different self-attention maps of each [CLS] token corresponding to the predicted labels to generate pseudo segmentation masks. Our proposed approach enhances the interpretability of self-attention maps and ensures accurate class assignments. Extensive experiments on two standard benchmarks and three specialized datasets demonstrate that our method generates accurate pseudo-masks, outperforming related works. Those pseudo-masks can be used to train a segmentation model which achieves results comparable to fully-supervised models, significantly reducing the need for fine-grained labeled data.</li>
</ul>

<h3>Title: The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover</h3>
<ul>
<li><strong>Authors: </strong>Matteo Lupinacci, Francesco Aurelio Pironti, Francesco Blefari, Francesco Romeo, Luigi Arena, Angelo Furfaro</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06850">https://arxiv.org/abs/2507.06850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06850">https://arxiv.org/pdf/2507.06850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06850]] The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover(https://arxiv.org/abs/2507.06850)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>The rapid adoption of Large Language Model (LLM) agents and multi-agent systems enables unprecedented capabilities in natural language processing and generation. However, these systems have introduced unprecedented security vulnerabilities that extend beyond traditional prompt injection attacks. This paper presents the first comprehensive evaluation of LLM agents as attack vectors capable of achieving complete computer takeover through the exploitation of trust boundaries within agentic AI systems where autonomous entities interact and influence each other. We demonstrate that adversaries can leverage three distinct attack surfaces - direct prompt injection, RAG backdoor attacks, and inter-agent trust exploitation - to coerce popular LLMs (including GPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing malware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals an alarming vulnerability hierarchy: while 41.2% of models succumb to direct prompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical 82.4% can be compromised through inter-agent trust exploitation. Notably, we discovered that LLMs which successfully resist direct malicious commands will execute identical payloads when requested by peer agents, revealing a fundamental flaw in current multi-agent security models. Our findings demonstrate that only 5.9% of tested models (1/17) proved resistant to all attack vectors, with the majority exhibiting context-dependent security behaviors that create exploitable blind spots. Our findings also highlight the need to increase awareness and research on the security risks of LLMs, showing a paradigm shift in cybersecurity threats, where AI tools themselves become sophisticated attack vectors.</li>
</ul>

<h3>Title: DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Liang Wang, Yu Rong, Tingyang Xu, Zhenyi Zhong, Zhiyuan Liu, Pengju Wang, Deli Zhao, Qiang Liu, Shu Wu, Liang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, physics.chem-ph, q-bio.MN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06853">https://arxiv.org/abs/2507.06853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06853">https://arxiv.org/pdf/2507.06853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06853]] DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models(https://arxiv.org/abs/2507.06853)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Molecular structure elucidation from spectra is a foundational problem in chemistry, with profound implications for compound identification, synthesis, and drug development. Traditional methods rely heavily on expert interpretation and lack scalability. Pioneering machine learning methods have introduced retrieval-based strategies, but their reliance on finite libraries limits generalization to novel molecules. Generative models offer a promising alternative, yet most adopt autoregressive SMILES-based architectures that overlook 3D geometry and struggle to integrate diverse spectral modalities. In this work, we present DiffSpectra, a generative framework that directly infers both 2D and 3D molecular structures from multi-modal spectral data using diffusion models. DiffSpectra formulates structure elucidation as a conditional generation process. Its denoising network is parameterized by Diffusion Molecule Transformer, an SE(3)-equivariant architecture that integrates topological and geometric information. Conditioning is provided by SpecFormer, a transformer-based spectral encoder that captures intra- and inter-spectral dependencies from multi-modal spectra. Extensive experiments demonstrate that DiffSpectra achieves high accuracy in structure elucidation, recovering exact structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through sampling. The model benefits significantly from 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. These results highlight the effectiveness of spectrum-conditioned diffusion modeling in addressing the challenge of molecular structure elucidation. To our knowledge, DiffSpectra is the first framework to unify multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation.</li>
</ul>

<h3>Title: IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization</h3>
<ul>
<li><strong>Authors: </strong>Subrat Kishore Dutta, Xiao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06856">https://arxiv.org/abs/2507.06856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06856">https://arxiv.org/pdf/2507.06856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06856]] IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization(https://arxiv.org/abs/2507.06856)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, steal</a></li>
<li><strong>Abstract: </strong>Despite modifying only a small localized input region, adversarial patches can drastically change the prediction of computer vision models. However, prior methods either cannot perform satisfactorily under targeted attack scenarios or fail to produce contextually coherent adversarial patches, causing them to be easily noticeable by human examiners and insufficiently stealthy against automatic patch defenses. In this paper, we introduce IAP, a novel attack framework that generates highly invisible adversarial patches based on perceptibility-aware localization and perturbation optimization schemes. Specifically, IAP first searches for a proper location to place the patch by leveraging classwise localization and sensitivity maps, balancing the susceptibility of patch location to both victim model prediction and human visual system, then employs a perceptibility-regularized adversarial loss and a gradient update rule that prioritizes color constancy for optimizing invisible perturbations. Comprehensive experiments across various image benchmarks and model architectures demonstrate that IAP consistently achieves competitive attack success rates in targeted settings with significantly improved patch invisibility compared to existing baselines. In addition to being highly imperceptible to humans, IAP is shown to be stealthy enough to render several state-of-the-art patch defenses ineffective.</li>
</ul>

<h3>Title: Longitudinal Study of Facial Biometrics at the BEZ: Temporal Variance Analysis</h3>
<ul>
<li><strong>Authors: </strong>Mathias Schulz, Alexander Spenke, Pia Funk, Florian Blümel, Markus Rohde, Ralph Breithaupt, Gerd Nolden, Norbert Jung, Robert Lange</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06858">https://arxiv.org/abs/2507.06858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06858">https://arxiv.org/pdf/2507.06858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06858]] Longitudinal Study of Facial Biometrics at the BEZ: Temporal Variance Analysis(https://arxiv.org/abs/2507.06858)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, biometric</a></li>
<li><strong>Abstract: </strong>This study presents findings from long-term biometric evaluations conducted at the Biometric Evaluation Center (bez). Over the course of two and a half years, our ongoing research with over 400 participants representing diverse ethnicities, genders, and age groups were regularly assessed using a variety of biometric tools and techniques at the controlled testing facilities. Our findings are based on the General Data Protection Regulation-compliant local bez database with more than 238.000 biometric data sets categorized into multiple biometric modalities such as face and finger. We used state-of-the-art face recognition algorithms to analyze long-term comparison scores. Our results show that these scores fluctuate more significantly between individual days than over the entire measurement period. These findings highlight the importance of testing biometric characteristics of the same individuals over a longer period of time in a controlled measurement environment and lays the groundwork for future advancements in biometric data analysis.</li>
</ul>

<h3>Title: Horizontal and Vertical Federated Causal Structure Learning via Higher-order Cumulants</h3>
<ul>
<li><strong>Authors: </strong>Wei Chen, Wanyang Gu, Linjun Peng, Ruichu Cai, Zhifeng Hao, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06888">https://arxiv.org/abs/2507.06888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06888">https://arxiv.org/pdf/2507.06888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06888]] Horizontal and Vertical Federated Causal Structure Learning via Higher-order Cumulants(https://arxiv.org/abs/2507.06888)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Federated causal discovery aims to uncover the causal relationships between entities while protecting data privacy, which has significant importance and numerous applications in real-world scenarios. Existing federated causal structure learning methods primarily focus on horizontal federated settings. However, in practical situations, different clients may not necessarily contain data on the same variables. In a single client, the incomplete set of variables can easily lead to spurious causal relationships, thereby affecting the information transmitted to other clients. To address this issue, we comprehensively consider causal structure learning methods under both horizontal and vertical federated settings. We provide the identification theories and methods for learning causal structure in the horizontal and vertical federal setting via higher-order cumulants. Specifically, we first aggregate higher-order cumulant information from all participating clients to construct global cumulant estimates. These global estimates are then used for recursive source identification, ultimately yielding a global causal strength matrix. Our approach not only enables the reconstruction of causal graphs but also facilitates the estimation of causal strength coefficients. Our algorithm demonstrates superior performance in experiments conducted on both synthetic data and real-world data.</li>
</ul>

<h3>Title: Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Jing Liang, Hongyao Tang, Yi Ma, Jinyi Liu, Yan Zheng, Shuyue Hu, Lei Bai, Jianye Hao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06892">https://arxiv.org/abs/2507.06892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06892">https://arxiv.org/pdf/2507.06892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06892]] Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model(https://arxiv.org/abs/2507.06892)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has demonstrated its potential to improve the reasoning ability of Large Language Models (LLMs). One major limitation of most existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL in nature, i.e., data generated during the past learning process is not fully utilized. This inevitably comes at a significant cost of compute and time, posing a stringent bottleneck on continuing economic and efficient scaling. To this end, we launch the renaissance of off-policy RL and propose Reincarnating Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix consists of three major components: (1) Mix-policy proximal policy gradient with an increased Update-To-Data (UTD) ratio for efficient training; (2) KL-Convex policy constraint to balance the trade-off between stability and flexibility; (3) Policy reincarnation to achieve a seamless transition from efficient early-stage learning to steady asymptotic improvement. In our experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with 0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level performance with an over 30x to 450x reduction in training cost in terms of rollout data volume. In addition, we reveal insightful findings via multifaceted analysis, including the implicit preference for shorter responses due to the Whipping Effect of off-policy discrepancy, the collapse mode of self-reflection behavior under the presence of severe off-policyness, etc.</li>
</ul>

<h3>Title: Developing and Maintaining an Open-Source Repository of AI Evaluations: Challenges and Insights</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Abbas, Celia Waggoner, Justin Olive</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06893">https://arxiv.org/abs/2507.06893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06893">https://arxiv.org/pdf/2507.06893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06893]] Developing and Maintaining an Open-Source Repository of AI Evaluations: Challenges and Insights(https://arxiv.org/abs/2507.06893)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>AI evaluations have become critical tools for assessing large language model capabilities and safety. This paper presents practical insights from eight months of maintaining $inspect\_evals$, an open-source repository of 70+ community-contributed AI evaluations. We identify key challenges in implementing and maintaining AI evaluations and develop solutions including: (1) a structured cohort management framework for scaling community contributions, (2) statistical methodologies for optimal resampling and cross-model comparison with uncertainty quantification, and (3) systematic quality control processes for reproducibility. Our analysis reveals that AI evaluation requires specialized infrastructure, statistical rigor, and community coordination beyond traditional software development practices.</li>
</ul>

<h3>Title: SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label Contrastive Learning and Bayesian kNN</h3>
<ul>
<li><strong>Authors: </strong>Luca Mariotti, Veronica Guidetti, Federica Mandreoli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06895">https://arxiv.org/abs/2507.06895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06895">https://arxiv.org/pdf/2507.06895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06895]] SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label Contrastive Learning and Bayesian kNN(https://arxiv.org/abs/2507.06895)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>The growing demand for efficient knowledge graph (KG) enrichment leveraging external corpora has intensified interest in relation extraction (RE), particularly under low-supervision settings. To address the need for adaptable and noise-resilient RE solutions that integrate seamlessly with pre-trained large language models (PLMs), we introduce SCoRE, a modular and cost-effective sentence-level RE system. SCoRE enables easy PLM switching, requires no finetuning, and adapts smoothly to diverse corpora and KGs. By combining supervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN) classifier for multi-label classification, it delivers robust performance despite the noisy annotations of distantly supervised corpora. To improve RE evaluation, we propose two novel metrics: Correlation Structure Distance (CSD), measuring the alignment between learned relational patterns and KG structures, and Precision at R (P@R), assessing utility as a recommender system. We also release Wiki20d, a benchmark dataset replicating real-world RE conditions where only KG-derived annotations are available. Experiments on five benchmarks show that SCoRE matches or surpasses state-of-the-art methods while significantly reducing energy consumption. Further analyses reveal that increasing model complexity, as seen in prior work, degrades performance, highlighting the advantages of SCoRE's minimal design. Combining efficiency, modularity, and scalability, SCoRE stands as an optimal choice for real-world RE applications.</li>
</ul>

<h3>Title: VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Ziang Ye, Yang Zhang, Wentao Shi, Xiaoyu You, Fuli Feng, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06899">https://arxiv.org/abs/2507.06899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06899">https://arxiv.org/pdf/2507.06899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06899]] VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation(https://arxiv.org/abs/2507.06899)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>Graphical User Interface (GUI) agents powered by Large Vision-Language Models (LVLMs) have emerged as a revolutionary approach to automating human-machine interactions, capable of autonomously operating personal devices (e.g., mobile phones) or applications within the device to perform complex real-world tasks in a human-like manner. However, their close integration with personal devices raises significant security concerns, with many threats, including backdoor attacks, remaining largely unexplored. This work reveals that the visual grounding of GUI agent-mapping textual plans to GUI elements-can introduce vulnerabilities, enabling new types of backdoor attacks. With backdoor attack targeting visual grounding, the agent's behavior can be compromised even when given correct task-solving plans. To validate this vulnerability, we propose VisualTrap, a method that can hijack the grounding by misleading the agent to locate textual plans to trigger locations instead of the intended targets. VisualTrap uses the common method of injecting poisoned data for attacks, and does so during the pre-training of visual grounding to ensure practical feasibility of attacking. Empirical results show that VisualTrap can effectively hijack visual grounding with as little as 5% poisoned data and highly stealthy visual triggers (invisible to the human eye); and the attack can be generalized to downstream tasks, even after clean fine-tuning. Moreover, the injected trigger can remain effective across different GUI environments, e.g., being trained on mobile/web and generalizing to desktop environments. These findings underscore the urgent need for further research on backdoor attack risks in GUI agents.</li>
</ul>

<h3>Title: Designing Adaptive Algorithms Based on Reinforcement Learning for Dynamic Optimization of Sliding Window Size in Multi-Dimensional Data Streams</h3>
<ul>
<li><strong>Authors: </strong>Abolfazl Zarghani, Sadegh Abedi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06901">https://arxiv.org/abs/2507.06901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06901">https://arxiv.org/pdf/2507.06901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06901]] Designing Adaptive Algorithms Based on Reinforcement Learning for Dynamic Optimization of Sliding Window Size in Multi-Dimensional Data Streams(https://arxiv.org/abs/2507.06901)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-dimensional data streams, prevalent in applications like IoT, financial markets, and real-time analytics, pose significant challenges due to their high velocity, unbounded nature, and complex inter-dimensional dependencies. Sliding window techniques are critical for processing such streams, but fixed-size windows struggle to adapt to dynamic changes like concept drift or bursty patterns. This paper proposes a novel reinforcement learning (RL)-based approach to dynamically optimize sliding window sizes for multi-dimensional data streams. By formulating window size selection as an RL problem, we enable an agent to learn an adaptive policy based on stream characteristics, such as variance, correlations, and temporal trends. Our method, RL-Window, leverages a Dueling Deep Q-Network (DQN) with prioritized experience replay to handle non-stationarity and high-dimensionality. Evaluations on benchmark datasets (UCI HAR, PAMAP2, Yahoo! Finance Stream) demonstrate that RL-Window outperforms state-of-the-art methods like ADWIN and CNN-Adaptive in classification accuracy, drift robustness, and computational efficiency. Additional qualitative analyses, extended metrics (e.g., energy efficiency, latency), and a comprehensive dataset characterization further highlight its adaptability and stability, making it suitable for real-time applications.</li>
</ul>

<h3>Title: SemRaFiner: Panoptic Segmentation in Sparse and Noisy Radar Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Matthias Zeller, Daniel Casado Herraez, Bengisu Ayan, Jens Behley, Michael Heidingsfeld, Cyrill Stachniss</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06906">https://arxiv.org/abs/2507.06906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06906">https://arxiv.org/pdf/2507.06906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06906]] SemRaFiner: Panoptic Segmentation in Sparse and Noisy Radar Point Clouds(https://arxiv.org/abs/2507.06906)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic scene understanding, including the perception and classification of moving agents, is essential to enabling safe and robust driving behaviours of autonomous vehicles. Cameras and LiDARs are commonly used for semantic scene understanding. However, both sensor modalities face limitations in adverse weather and usually do not provide motion information. Radar sensors overcome these limitations and directly offer information about moving agents by measuring the Doppler velocity, but the measurements are comparably sparse and noisy. In this paper, we address the problem of panoptic segmentation in sparse radar point clouds to enhance scene understanding. Our approach, called SemRaFiner, accounts for changing density in sparse radar point clouds and optimizes the feature extraction to improve accuracy. Furthermore, we propose an optimized training procedure to refine instance assignments by incorporating a dedicated data augmentation. Our experiments suggest that our approach outperforms state-of-the-art methods for radar-based panoptic segmentation.</li>
</ul>

<h3>Title: Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting</h3>
<ul>
<li><strong>Authors: </strong>Linyun Gao, Qiang Wen, Fumio Machida</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06907">https://arxiv.org/abs/2507.06907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06907">https://arxiv.org/pdf/2507.06907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06907]] Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting(https://arxiv.org/abs/2507.06907)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Autonomous driving is rapidly advancing as a key application of machine learning, yet ensuring the safety of these systems remains a critical challenge. Traffic sign recognition, an essential component of autonomous vehicles, is particularly vulnerable to adversarial attacks that can compromise driving safety. In this paper, we propose an N-version machine learning (NVML) framework that integrates a safety-aware weighted soft voting mechanism. Our approach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential safety risks and assign dynamic, safety-aware weights to the ensemble outputs. We evaluate the robustness of three-version NVML systems employing various voting mechanisms against adversarial samples generated using the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental results demonstrate that our NVML approach significantly enhances the robustness and safety of traffic sign recognition systems under adversarial conditions.</li>
</ul>

<h3>Title: MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection</h3>
<ul>
<li><strong>Authors: </strong>Ziyan Liu, Chunxiao Fan, Haoran Lou, Yuexin Wu, Kaiwei Deng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06908">https://arxiv.org/abs/2507.06908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06908">https://arxiv.org/pdf/2507.06908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06908]] MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection(https://arxiv.org/abs/2507.06908)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rapid expansion of memes on social media has highlighted the urgent need for effective approaches to detect harmful content. However, traditional data-driven approaches struggle to detect new memes due to their evolving nature and the lack of up-to-date annotated data. To address this issue, we propose MIND, a multi-agent framework for zero-shot harmful meme detection that does not rely on annotated data. MIND implements three key strategies: 1) We retrieve similar memes from an unannotated reference set to provide contextual information. 2) We propose a bi-directional insight derivation mechanism to extract a comprehensive understanding of similar memes. 3) We then employ a multi-agent debate mechanism to ensure robust decision-making through reasoned arbitration. Extensive experiments on three meme datasets demonstrate that our proposed framework not only outperforms existing zero-shot approaches but also shows strong generalization across different model architectures and parameter scales, providing a scalable solution for harmful meme detection. The code is available at this https URL.</li>
</ul>

<h3>Title: MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction</h3>
<ul>
<li><strong>Authors: </strong>Xiao Wang, Jiahuan Pei, Diancheng Shui, Zhiguang Han, Xin Sun, Dawei Zhu, Xiaoyu Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06909">https://arxiv.org/abs/2507.06909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06909">https://arxiv.org/pdf/2507.06909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06909]] MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction(https://arxiv.org/abs/2507.06909)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Legal judgment prediction offers a compelling method to aid legal practitioners and researchers. However, the research question remains relatively under-explored: Should multiple defendants and charges be treated separately in LJP? To address this, we introduce a new dataset namely multi-person multi-charge prediction (MPMCP), and seek the answer by evaluating the performance of several prevailing legal large language models (LLMs) on four practical legal judgment scenarios: (S1) single defendant with a single charge, (S2) single defendant with multiple charges, (S3) multiple defendants with a single charge, and (S4) multiple defendants with multiple charges. We evaluate the dataset across two LJP tasks, i.e., charge prediction and penalty term prediction. We have conducted extensive experiments and found that the scenario involving multiple defendants and multiple charges (S4) poses the greatest challenges, followed by S2, S3, and S1. The impact varies significantly depending on the model. For example, in S4 compared to S1, InternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD, while Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD. Our dataset and code are available at this https URL.</li>
</ul>

<h3>Title: Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Fareya Ikram, Alexander Scarlatos, Andrew Lan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06910">https://arxiv.org/abs/2507.06910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06910">https://arxiv.org/pdf/2507.06910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06910]] Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in Dialogues(https://arxiv.org/abs/2507.06910)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tutoring dialogues have gained significant attention in recent years, given the prominence of online learning and the emerging tutoring abilities of artificial intelligence (AI) agents powered by large language models (LLMs). Recent studies have shown that the strategies used by tutors can have significant effects on student outcomes, necessitating methods to predict how tutors will behave and how their actions impact students. However, few works have studied predicting tutor strategy in dialogues. Therefore, in this work we investigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to predict both future tutor moves and student outcomes in dialogues, using two math tutoring dialogue datasets. We find that even state-of-the-art LLMs struggle to predict future tutor strategy while tutor strategy is highly indicative of student outcomes, outlining a need for more powerful methods to approach this task.</li>
</ul>

<h3>Title: Rethinking Verification for LLM Code Generation: From Generation to Testing</h3>
<ul>
<li><strong>Authors: </strong>Zihan Ma, Taolin Zhang, Maosong Cao, Wenwei Zhang, Minnan Luo, Songyang Zhang, Kai Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06920">https://arxiv.org/abs/2507.06920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06920">https://arxiv.org/pdf/2507.06920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06920]] Rethinking Verification for LLM Code Generation: From Generation to Testing(https://arxiv.org/abs/2507.06920)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently achieved notable success in code-generation benchmarks such as HumanEval and LiveCodeBench. However, a detailed examination reveals that these evaluation suites often comprise only a limited number of homogeneous test cases, resulting in subtle faults going undetected. This not only artificially inflates measured performance but also compromises accurate reward estimation in reinforcement learning frameworks utilizing verifiable rewards (RLVR). To address these critical shortcomings, we systematically investigate the test-case generation (TCG) task by proposing multi-dimensional metrics designed to rigorously quantify test-suite thoroughness. Furthermore, we introduce a human-LLM collaborative method (SAGA), leveraging human programming expertise with LLM reasoning capability, aimed at significantly enhancing both the coverage and the quality of generated test cases. In addition, we develop a TCGBench to facilitate the study of the TCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a verifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc) of the code generation evaluation benchmark synthesized by SAGA is 10.78% higher than that of LiveCodeBench-v6. These results demonstrate the effectiveness of our proposed method. We hope this work contributes to building a scalable foundation for reliable LLM code evaluation, further advancing RLVR in code generation, and paving the way for automated adversarial test synthesis and adaptive benchmark integration.</li>
</ul>

<h3>Title: Are NFTs Ready to Keep Australian Artists Engaged?</h3>
<ul>
<li><strong>Authors: </strong>Ruiqiang Li, Brian Yecies, Qin Wang, Shiping Chen, Jun Shen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06926">https://arxiv.org/abs/2507.06926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06926">https://arxiv.org/pdf/2507.06926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06926]] Are NFTs Ready to Keep Australian Artists Engaged?(https://arxiv.org/abs/2507.06926)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect</a></li>
<li><strong>Abstract: </strong>Non-Fungible Tokens (NFTs) offer a promising mechanism to protect Australian and Indigenous artists' copyright. They represent and transfer the value of artwork in digital form. Before adopting NFTs to protect Australian artwork, we in this paper investigate them empericially. We focus on examining the details of NFT structure. We start from the underlying structure of NFTs to show how they represent copyright for both artists and production owners, as well as how they aim to safeguard or secure the value of digital artworks. We then involve data collection from various types of sources with different storage methods, including on-chain, centralized, and decentralized systems. Based on both metadata and artwork content, we present our analysis and discussion on the following key issues: copyright, security and artist identification. The final results of the evaluation, unfortnately, show that the NFT is NOT ready to protect Australian and Indigenous artists' copyright.</li>
</ul>

<h3>Title: Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Qiyuan Dai, Hanzhuo Huang, Yu Wu, Sibei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06928">https://arxiv.org/abs/2507.06928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06928">https://arxiv.org/pdf/2507.06928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06928]] Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement(https://arxiv.org/abs/2507.06928)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Generalized Category Discovery (GCD) aims to recognize unlabeled images from known and novel classes by distinguishing novel classes from known ones, while also transferring knowledge from another set of labeled images with known classes. Existing GCD methods rely on self-supervised vision transformers such as DINO for representation learning. However, focusing solely on the global representation of the DINO CLS token introduces an inherent trade-off between discriminability and generalization. In this paper, we introduce an adaptive part discovery and learning method, called APL, which generates consistent object parts and their correspondences across different similar images using a set of shared learnable part queries and DINO part priors, without requiring any additional annotations. More importantly, we propose a novel all-min contrastive loss to learn discriminative yet generalizable part representation, which adaptively highlights discriminative object parts to distinguish similar categories for enhanced discriminability while simultaneously sharing other parts to facilitate knowledge transfer for improved generalization. Our APL can easily be incorporated into different GCD frameworks by replacing their CLS token feature with our part representations, showing significant enhancements on fine-grained datasets.</li>
</ul>

<h3>Title: DICE: Data Influence Cascade in Decentralized Learning</h3>
<ul>
<li><strong>Authors: </strong>Tongtian Zhu, Wenhao Li, Can Wang, Fengxiang He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.MA, cs.SI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06931">https://arxiv.org/abs/2507.06931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06931">https://arxiv.org/pdf/2507.06931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06931]] DICE: Data Influence Cascade in Decentralized Learning(https://arxiv.org/abs/2507.06931)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Decentralized learning offers a promising approach to crowdsource data consumptions and computational workloads across geographically distributed compute interconnected through peer-to-peer networks, accommodating the exponentially increasing demands. However, proper incentives are still in absence, considerably discouraging participation. Our vision is that a fair incentive mechanism relies on fair attribution of contributions to participating nodes, which faces non-trivial challenges arising from the localized connections making influence ``cascade'' in a decentralized network. To overcome this, we design the first method to estimate \textbf{D}ata \textbf{I}nfluence \textbf{C}ascad\textbf{E} (DICE) in a decentralized environment. Theoretically, the framework derives tractable approximations of influence cascade over arbitrary neighbor hops, suggesting the influence cascade is determined by an interplay of data, communication topology, and the curvature of loss landscape. DICE also lays the foundations for applications including selecting suitable collaborators and identifying malicious behaviors. Project page is available at this https URL.</li>
</ul>

<h3>Title: Investigating the Robustness of Retrieval-Augmented Generation at the Query Level</h3>
<ul>
<li><strong>Authors: </strong>Sezen Perçin, Xin Su, Qutub Sha Syed, Phillip Howard, Aleksei Kuvshinov, Leo Schwinn, Kay-Ulrich Scholl</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06956">https://arxiv.org/abs/2507.06956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06956">https://arxiv.org/pdf/2507.06956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06956]] Investigating the Robustness of Retrieval-Augmented Generation at the Query Level(https://arxiv.org/abs/2507.06956)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are very costly and inefficient to update with new information. To address this limitation, retrieval-augmented generation (RAG) has been proposed as a solution that dynamically incorporates external knowledge during inference, improving factual consistency and reducing hallucinations. Despite its promise, RAG systems face practical challenges-most notably, a strong dependence on the quality of the input query for accurate retrieval. In this paper, we investigate the sensitivity of different components in the RAG pipeline to various types of query perturbations. Our analysis reveals that the performance of commonly used retrievers can degrade significantly even under minor query variations. We study each module in isolation as well as their combined effect in an end-to-end question answering setting, using both general-domain and domain-specific datasets. Additionally, we propose an evaluation framework to systematically assess the query-level robustness of RAG pipelines and offer actionable recommendations for practitioners based on the results of more than 1092 experiments we performed.</li>
</ul>

<h3>Title: Segmentation Regularized Training for Multi-Domain Deep Learning Registration applied to MR-Guided Prostate Cancer Radiotherapy</h3>
<ul>
<li><strong>Authors: </strong>Sudharsan Madhavan, Chengcheng Gui, Lando Bosma, Josiah Simeth, Jue Jiang, Nicolas Cote, Nima Hassan Rezaeian, Himanshu Nagar, Victoria Brennan, Neelam Tyagi, Harini Veeraraghavan</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06966">https://arxiv.org/abs/2507.06966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06966">https://arxiv.org/pdf/2507.06966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06966]] Segmentation Regularized Training for Multi-Domain Deep Learning Registration applied to MR-Guided Prostate Cancer Radiotherapy(https://arxiv.org/abs/2507.06966)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Background: Accurate deformable image registration (DIR) is required for contour propagation and dose accumulation in MR-guided adaptive radiotherapy (MRgART). This study trained and evaluated a deep learning DIR method for domain invariant MR-MR registration. Methods: A progressively refined registration and segmentation (ProRSeg) method was trained with 262 pairs of 3T MR simulation scans from prostate cancer patients using weighted segmentation consistency loss. ProRSeg was tested on same- (58 pairs), cross- (72 1.5T MR Linac pairs), and mixed-domain (42 MRSim-MRL pairs) datasets for contour propagation accuracy of clinical target volume (CTV), bladder, and rectum. Dose accumulation was performed for 42 patients undergoing 5-fraction MRgART. Results: ProRSeg demonstrated generalization for bladder with similar Dice Similarity Coefficients across domains (0.88, 0.87, 0.86). For rectum and CTV, performance was domain-dependent with higher accuracy on cross-domain MRL dataset (DSCs 0.89) versus same-domain data. The model's strong cross-domain performance prompted us to study the feasibility of using it for dose accumulation. Dose accumulation showed 83.3% of patients met CTV coverage (D95 >= 40.0 Gy) and bladder sparing (D50 <= 20.0 Gy) constraints. All patients achieved minimum mean target dose (>40.4 Gy), but only 9.5% remained under upper limit (<42.0 Gy). Conclusions: ProRSeg showed reasonable multi-domain MR-MR registration performance for prostate cancer patients with preliminary feasibility for evaluating treatment compliance to clinical constraints.</li>
</ul>

<h3>Title: Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Bogdan Kulynych, Juan Felipe Gomez, Georgios Kaissis, Jamie Hayes, Borja Balle, Flavio du Pin Calmon, Jean Louis Raisaro</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.CY, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06969">https://arxiv.org/abs/2507.06969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06969">https://arxiv.org/pdf/2507.06969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06969]] Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy(https://arxiv.org/abs/2507.06969)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>Differentially private (DP) mechanisms are difficult to interpret and calibrate because existing methods for mapping standard privacy parameters to concrete privacy risks -- re-identification, attribute inference, and data reconstruction -- are both overly pessimistic and inconsistent. In this work, we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that bounds on attack success can take the same unified form across re-identification, attribute inference, and data reconstruction risks. Our unified bounds are (1) consistent across a multitude of attack settings, and (2) tunable, enabling practitioners to evaluate risk with respect to arbitrary (including worst-case) levels of baseline risk. Empirically, our results are tighter than prior methods using $\varepsilon$-DP, Rényi DP, and concentrated DP. As a result, calibrating noise using our bounds can reduce the required noise by 20% at the same risk level, which yields, e.g., more than 15pp accuracy increase in a text classification task. Overall, this unifying perspective provides a principled framework for interpreting and calibrating the degree of protection in DP against specific levels of re-identification, attribute inference, or data reconstruction risk.</li>
</ul>

<h3>Title: Hallucinating 360°: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting</h3>
<ul>
<li><strong>Authors: </strong>Fei Teng, Kai Luo, Sheng Wu, Siyu Li, Pujun Guo, Jiale Wei, Kunyu Peng, Jiaming Zhang, Kailun Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06971">https://arxiv.org/abs/2507.06971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06971">https://arxiv.org/pdf/2507.06971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06971]] Hallucinating 360°: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting(https://arxiv.org/abs/2507.06971)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Panoramic perception holds significant potential for autonomous driving, enabling vehicles to acquire a comprehensive 360° surround view in a single shot. However, autonomous driving is a data-driven task. Complete panoramic data acquisition requires complex sampling systems and annotation pipelines, which are time-consuming and labor-intensive. Although existing street view generation models have demonstrated strong data regeneration capabilities, they can only learn from the fixed data distribution of existing datasets and cannot achieve high-quality, controllable panoramic generation. In this paper, we propose the first panoramic generation method Percep360 for autonomous driving. Percep360 enables coherent generation of panoramic data with control signals based on the stitched panoramic data. Percep360 focuses on two key aspects: coherence and controllability. Specifically, to overcome the inherent information loss caused by the pinhole sampling process, we propose the Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama generation as a spatially continuous diffusion process, bridging the gaps between different data distributions. Additionally, to achieve the controllable generation of panoramic images, we propose a Probabilistic Prompting Method (PPM). PPM dynamically selects the most relevant control cues, enabling controllable panoramic image generation. We evaluate the effectiveness of the generated images from three perspectives: image quality assessment (i.e., no-reference and with reference), controllability, and their utility in real-world Bird's Eye View (BEV) segmentation. Notably, the generated data consistently outperforms the original stitched images in no-reference quality metrics and enhances downstream perception models. The source code will be publicly available at this https URL.</li>
</ul>

<h3>Title: A multi-modal dataset for insect biodiversity with imagery and DNA at the trap and individual level</h3>
<ul>
<li><strong>Authors: </strong>Johanna Orsholm, John Quinto, Hannu Autto, Gaia Banelyte, Nicolas Chazot, Jeremy deWaard, Stephanie deWaard, Arielle Farrell, Brendan Furneaux, Bess Hardwick, Nao Ito, Amlan Kar, Oula Kalttopää, Deirdre Kerdraon, Erik Kristensen, Jaclyn McKeown, Tommi Mononen, Ellen Nein, Hanna Rogers, Tomas Roslin, Paula Schmitz, Jayme Sones, Maija Sujala, Amy Thompson, Evgeny V. Zakharov, Iuliia Zarubiieva, Akshita Gupta, Scott C. Lowe, Graham W. Taylor</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06972">https://arxiv.org/abs/2507.06972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06972">https://arxiv.org/pdf/2507.06972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06972]] A multi-modal dataset for insect biodiversity with imagery and DNA at the trap and individual level(https://arxiv.org/abs/2507.06972)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Insects comprise millions of species, many experiencing severe population declines under environmental and habitat changes. High-throughput approaches are crucial for accelerating our understanding of insect diversity, with DNA barcoding and high-resolution imaging showing strong potential for automatic taxonomic classification. However, most image-based approaches rely on individual specimen data, unlike the unsorted bulk samples collected in large-scale ecological surveys. We present the Mixed Arthropod Sample Segmentation and Identification (MassID45) dataset for training automatic classifiers of bulk insect samples. It uniquely combines molecular and imaging data at both the unsorted sample level and the full set of individual specimens. Human annotators, supported by an AI-assisted tool, performed two tasks on bulk images: creating segmentation masks around each individual arthropod and assigning taxonomic labels to over 17 000 specimens. Combining the taxonomic resolution of DNA barcodes with precise abundance estimates of bulk images holds great potential for rapid, large-scale characterization of insect communities. This dataset pushes the boundaries of tiny object detection and instance segmentation, fostering innovation in both ecological and machine learning research.</li>
</ul>

<h3>Title: BarkBeetle: Stealing Decision Tree Models with Fault Injection</h3>
<ul>
<li><strong>Authors: </strong>Qifan Wang, Jonas Sander, Minmin Jiang, Thomas Eisenbarth, David Oswald</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06986">https://arxiv.org/abs/2507.06986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06986">https://arxiv.org/pdf/2507.06986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06986]] BarkBeetle: Stealing Decision Tree Models with Fault Injection(https://arxiv.org/abs/2507.06986)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, steal, extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Machine learning models, particularly decision trees (DTs), are widely adopted across various domains due to their interpretability and efficiency. However, as ML models become increasingly integrated into privacy-sensitive applications, concerns about their confidentiality have grown, particularly in light of emerging threats such as model extraction and fault injection attacks. Assessing the vulnerability of DTs under such attacks is therefore important. In this work, we present BarkBeetle, a novel attack that leverages fault injection to extract internal structural information of DT models. BarkBeetle employs a bottom-up recovery strategy that uses targeted fault injection at specific nodes to efficiently infer feature splits and threshold values. Our proof-of-concept implementation demonstrates that BarkBeetle requires significantly fewer queries and recovers more structural information compared to prior approaches, when evaluated on DTs trained with public UCI datasets. To validate its practical feasibility, we implement BarkBeetle on a Raspberry Pi RP2350 board and perform fault injections using the Faultier voltage glitching tool. As BarkBeetle targets general DT models, we also provide an in-depth discussion on its applicability to a broader range of tree-based applications, including data stream classification, DT variants, and cryptography schemes.</li>
</ul>

<h3>Title: MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Qilong Xing, Zikai Song, Youjia Zhang, Na Feng, Junqing Yu, Wei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06992">https://arxiv.org/abs/2507.06992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06992">https://arxiv.org/pdf/2507.06992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06992]] MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation(https://arxiv.org/abs/2507.06992)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Despite significant advancements in adapting Large Language Models (LLMs) for radiology report generation (RRG), clinical adoption remains challenging due to difficulties in accurately mapping pathological and anatomical features to their corresponding text descriptions. Additionally, semantic agnostic feature extraction further hampers the generation of accurate diagnostic reports. To address these challenges, we introduce Medical Concept Aligned Radiology Report Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual features with distinct medical concepts to enhance the report generation process. MCA-RG utilizes two curated concept banks: a pathology bank containing lesion-related knowledge, and an anatomy bank with anatomical descriptions. The visual features are aligned with these medical concepts and undergo tailored enhancement. We further propose an anatomy-based contrastive learning procedure to improve the generalization of anatomical features, coupled with a matching loss for pathological features to prioritize clinically relevant regions. Additionally, a feature gating mechanism is employed to filter out low-quality concept features. Finally, the visual features are corresponding to individual medical concepts, and are leveraged to guide the report generation process. Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate that MCA-RG achieves superior performance, highlighting its effectiveness in radiology report generation.</li>
</ul>

<h3>Title: Cross-Modality Masked Learning for Survival Prediction in ICI Treated NSCLC Patients</h3>
<ul>
<li><strong>Authors: </strong>Qilong Xing, Zikai Song, Bingxin Gong, Lian Yang, Junqing Yu, Wei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06994">https://arxiv.org/abs/2507.06994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06994">https://arxiv.org/pdf/2507.06994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06994]] Cross-Modality Masked Learning for Survival Prediction in ICI Treated NSCLC Patients(https://arxiv.org/abs/2507.06994)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing immunotherapy is essential for personalized treatment planning, enabling informed patient decisions, and improving both treatment outcomes and quality of life. However, the lack of large, relevant datasets and effective multi-modal feature fusion strategies pose significant challenges in this domain. To address these challenges, we present a large-scale dataset and introduce a novel framework for multi-modal feature fusion aimed at enhancing the accuracy of survival prediction. The dataset comprises 3D CT images and corresponding clinical records from NSCLC patients treated with immune checkpoint inhibitors (ICI), along with progression-free survival (PFS) and overall survival (OS) data. We further propose a cross-modality masked learning approach for medical feature fusion, consisting of two distinct branches, each tailored to its respective modality: a Slice-Depth Transformer for extracting 3D features from CT images and a graph-based Transformer for learning node features and relationships among clinical variables in tabular data. The fusion process is guided by a masked modality learning strategy, wherein the model utilizes the intact modality to reconstruct missing components. This mechanism improves the integration of modality-specific features, fostering more effective inter-modality relationships and feature interactions. Our approach demonstrates superior performance in multi-modal integration for NSCLC survival prediction, surpassing existing methods and setting a new benchmark for prognostic models in this context.</li>
</ul>

<h3>Title: Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing</h3>
<ul>
<li><strong>Authors: </strong>Eunbyeol Cho, Jiyoun Kim, Minjae Lee, Sungjin Park, Edward Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06996">https://arxiv.org/abs/2507.06996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06996">https://arxiv.org/pdf/2507.06996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06996]] Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing(https://arxiv.org/abs/2507.06996)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Electronic Health Records (EHR) are time-series relational databases that record patient interactions and medical events over time, serving as a critical resource for healthcare research and applications. However, privacy concerns and regulatory restrictions limit the sharing and utilization of such sensitive data, necessitating the generation of synthetic EHR datasets. Unlike previous EHR synthesis methods, which typically generate medical records consisting of expert-chosen features (e.g. a few vital signs or structured codes only), we introduce RawMed, the first framework to synthesize multi-table, time-series EHR data that closely resembles raw EHRs. Using text-based representation and compression techniques, RawMed captures complex structures and temporal dynamics with minimal preprocessing. We also propose a new evaluation framework for multi-table time-series synthetic EHRs, assessing distributional similarity, inter-table relationships, temporal dynamics, and privacy. Validated on two open-source EHR datasets, RawMed outperforms baseline models in fidelity and utility. The code is available at this https URL.</li>
</ul>

<h3>Title: Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yahan Yu, Yuyang Dong, Masafumi Oyamada</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06999">https://arxiv.org/abs/2507.06999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06999">https://arxiv.org/pdf/2507.06999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06999]] Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs(https://arxiv.org/abs/2507.06999)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning is a key capability for large language models (LLMs), particularly when applied to complex tasks such as mathematical problem solving. However, multimodal reasoning research still requires further exploration of modality alignment and training costs. Many of these approaches rely on additional data annotation and relevant rule-based rewards to enhance the understanding and reasoning ability, which significantly increases training costs and limits scalability. To address these challenges, we propose the Deliberate-to-Intuitive reasoning framework (D2I) that improves the understanding and reasoning ability of multimodal LLMs (MLLMs) without extra annotations and complex rewards. Specifically, our method sets deliberate reasoning strategies to enhance modality alignment only through the rule-based format reward during training. While evaluating, the reasoning style shifts to intuitive, which removes deliberate reasoning strategies during training and implicitly reflects the model's acquired abilities in the response. D2I outperforms baselines across both in-domain and out-of-domain benchmarks. Our findings highlight the role of format reward in fostering transferable reasoning skills in MLLMs, and inspire directions for decoupling training-time reasoning depth from test-time response flexibility.</li>
</ul>

<h3>Title: GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning</h3>
<ul>
<li><strong>Authors: </strong>S M Taslim Uddin Raju, Md. Milon Islam, Md Rezwanul Haque, Hamdi Altaheri, Fakhri Karray</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07006">https://arxiv.org/abs/2507.07006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07006">https://arxiv.org/pdf/2507.07006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07006]] GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning(https://arxiv.org/abs/2507.07006)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Microscopic assessment of histopathology images is vital for accurate cancer diagnosis and treatment. Whole Slide Image (WSI) classification and captioning have become crucial tasks in computer-aided pathology. However, microscopic WSI face challenges such as redundant patches and unknown patch positions due to subjective pathologist captures. Moreover, generating automatic pathology captions remains a significant challenge. To address these issues, we introduce a novel GNN-ViTCap framework for classification and caption generation from histopathological microscopic images. First, a visual feature extractor generates patch embeddings. Redundant patches are then removed by dynamically clustering these embeddings using deep embedded clustering and selecting representative patches via a scalar dot attention mechanism. We build a graph by connecting each node to its nearest neighbors in the similarity matrix and apply a graph neural network to capture both local and global context. The aggregated image embeddings are projected into the language model's input space through a linear layer and combined with caption tokens to fine-tune a large language model. We validate our method on the BreakHis and PatchGastric datasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for classification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569 for captioning. Experimental results demonstrate that GNN-ViTCap outperforms state of the art approaches, offering a reliable and efficient solution for microscopy based patient diagnosis.</li>
</ul>

<h3>Title: Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions</h3>
<ul>
<li><strong>Authors: </strong>Emile Pierret, Bruno Galerne</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07008">https://arxiv.org/abs/2507.07008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07008">https://arxiv.org/pdf/2507.07008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07008]] Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions(https://arxiv.org/abs/2507.07008)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Used as priors for Bayesian inverse problems, diffusion models have recently attracted considerable attention in the literature. Their flexibility and high variance enable them to generate multiple solutions for a given task, such as inpainting, super-resolution, and deblurring. However, several unresolved questions remain about how well they perform. In this article, we investigate the accuracy of these models when applied to a Gaussian data distribution for deblurring. Within this constrained context, we are able to precisely analyze the discrepancy between the theoretical resolution of inverse problems and their resolution obtained using diffusion models by computing the exact Wasserstein distance between the distribution of the diffusion model sampler and the ideal distribution of solutions to the inverse problem. Our findings allow for the comparison of different algorithms from the literature.</li>
</ul>

<h3>Title: FlexOlmo: Open Language Models for Flexible Data Use</h3>
<ul>
<li><strong>Authors: </strong>Weijia Shi, Akshita Bhagia, Kevin Farhat, Niklas Muennighoff, Pete Walsh, Jacob Morrison, Dustin Schwenk, Shayne Longpre, Jake Poznanski, Allyson Ettinger, Daogao Liu, Margaret Li, Dirk Groeneveld, Mike Lewis, Wen-tau Yih, Luca Soldaini, Kyle Lo, Noah A. Smith, Luke Zettlemoyer, Pang Wei Koh, Hannaneh Hajishirzi, Ali Farhadi, Sewon Min</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07024">https://arxiv.org/abs/2507.07024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07024">https://arxiv.org/pdf/2507.07024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07024]] FlexOlmo: Open Language Models for Flexible Data Use(https://arxiv.org/abs/2507.07024)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>We introduce FlexOlmo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners, leading to an average 41% relative improvement while allowing users to opt out of certain data based on data licensing or permission requirements. Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, this research presents a solution for both data owners and researchers in regulated industries with sensitive or protected data. FlexOlmo enables benefiting from closed data while respecting data owners' preferences by keeping their data local and supporting fine-grained control of data access during inference.</li>
</ul>

<h3>Title: Design and Implementation of an OCR-Powered Pipeline for Table Extraction from Invoices</h3>
<ul>
<li><strong>Authors: </strong>Parshva Dhilankumar Patel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07029">https://arxiv.org/abs/2507.07029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07029">https://arxiv.org/pdf/2507.07029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07029]] Design and Implementation of an OCR-Powered Pipeline for Table Extraction from Invoices(https://arxiv.org/abs/2507.07029)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This paper presents the design and development of an OCR-powered pipeline for efficient table extraction from invoices. The system leverages Tesseract OCR for text recognition and custom post-processing logic to detect, align, and extract structured tabular data from scanned invoice documents. Our approach includes dynamic preprocessing, table boundary detection, and row-column mapping, optimized for noisy and non-standard invoice formats. The resulting pipeline significantly improves data extraction accuracy and consistency, supporting real-world use cases such as automated financial workflows and digital archiving.</li>
</ul>

<h3>Title: UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations</h3>
<ul>
<li><strong>Authors: </strong>Fengran Mo, Yifan Gao, Chuan Meng, Xin Liu, Zhuofeng Wu, Kelong Mao, Zhengyang Wang, Pei Chen, Zheng Li, Xian Li, Bing Yin, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07030">https://arxiv.org/abs/2507.07030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07030">https://arxiv.org/pdf/2507.07030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07030]] UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations(https://arxiv.org/abs/2507.07030)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of conversational search systems revolutionizes how information is accessed by enabling the multi-turn interaction between the user and the system. Existing conversational search systems are usually built with two different models. This separation restricts the system from leveraging the intrinsic knowledge of the models simultaneously, which cannot ensure the effectiveness of retrieval benefiting the generation. The existing studies for developing unified models cannot fully address the aspects of understanding conversational context, managing retrieval independently, and generating responses. In this paper, we explore how to unify dense retrieval and response generation for large language models in conversation. We conduct joint fine-tuning with different objectives and design two mechanisms to reduce the inconsistency risks while mitigating data discrepancy. The evaluations on five conversational search datasets demonstrate that our unified model can mutually improve both tasks and outperform the existing baselines.</li>
</ul>

<h3>Title: Self-Supervised Learning at the Edge: The Cost of Labeling</h3>
<ul>
<li><strong>Authors: </strong>Roberto Pereira, Fernanda Famá, Asal Rangrazi, Marco Miozzo, Charalampos Kalalas, Paolo Dini</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07033">https://arxiv.org/abs/2507.07033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07033">https://arxiv.org/pdf/2507.07033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07033]] Self-Supervised Learning at the Edge: The Cost of Labeling(https://arxiv.org/abs/2507.07033)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Contrastive learning (CL) has recently emerged as an alternative to traditional supervised machine learning solutions by enabling rich representations from unstructured and unlabeled data. However, CL and, more broadly, self-supervised learning (SSL) methods often demand a large amount of data and computational resources, posing challenges for deployment on resource-constrained edge devices. In this work, we explore the feasibility and efficiency of SSL techniques for edge-based learning, focusing on trade-offs between model performance and energy efficiency. In particular, we analyze how different SSL techniques adapt to limited computational, data, and energy budgets, evaluating their effectiveness in learning robust representations under resource-constrained settings. Moreover, we also consider the energy costs involved in labeling data and assess how semi-supervised learning may assist in reducing the overall energy consumed to train CL models. Through extensive experiments, we demonstrate that tailored SSL strategies can achieve competitive performance while reducing resource consumption by up to 4X, underscoring their potential for energy-efficient learning at the edge.</li>
</ul>

<h3>Title: Discrete Diffusion Models for Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Ashen Weligalle</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07050">https://arxiv.org/abs/2507.07050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07050">https://arxiv.org/pdf/2507.07050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07050]] Discrete Diffusion Models for Language Generation(https://arxiv.org/abs/2507.07050)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as a powerful class of generative models, achieving state-of-the-art results in continuous data domains such as image and video generation. Their core mechanism involves a forward diffusion process that gradually transforms structured data into a Gaussian-like distribution, followed by a learned reverse process to reconstruct the data. While successful in continuous modalities, applying this framework to discrete data-particularly natural language-remains challenging due to token dependency complexities and the lack of a defined generation this http URL thesis investigates the feasibility and performance of discrete diffusion models for natural language generation. Specifically, we evaluate the Discrete Denoising Diffusion Probabilistic Model (D3PM) and compare it with traditional autoregressive (AR) language models. To assess generative performance, we use Bits Per Token (BPT), Negative Log-Likelihood (NLL), Perplexity (PPL), and Batch Processing Speed. Results show the best-performing D3PM model achieves a BPT of 5.72, with a mean of 8.05. The AR model outperforms in compression with a lower mean BPT of 4.59, but D3PM achieves higher processing speed, reaching up to 3.97 batches per sec., indicating potential for parallel this http URL evaluations were conducted under consistent conditions-generating 100,000 tokens per model with a fixed batch size of four-for fair comparison. This research presents a detailed analysis of diffusion-based vs. autoregressive models, highlighting trade-offs in generative quality and efficiency. Findings emphasize both the promise and limitations of diffusion models for discrete data, supporting future work in non-autoregressive language generation.</li>
</ul>

<h3>Title: LoRAShield: Data-Free Editing Alignment for Secure Personalized LoRA Sharing</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Chen, junhao li, Yiming Wang, Zhe Ma, Yi Jiang, Chunyi Zhou, Qingming Li, Tianyu Du, Shouling Ji</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07056">https://arxiv.org/abs/2507.07056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07056">https://arxiv.org/pdf/2507.07056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07056]] LoRAShield: Data-Free Editing Alignment for Secure Personalized LoRA Sharing(https://arxiv.org/abs/2507.07056)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, defense, robust, diffusion, data-free, generative</a></li>
<li><strong>Abstract: </strong>The proliferation of Low-Rank Adaptation (LoRA) models has democratized personalized text-to-image generation, enabling users to share lightweight models (e.g., personal portraits) on platforms like Civitai and Liblib. However, this "share-and-play" ecosystem introduces critical risks: benign LoRAs can be weaponized by adversaries to generate harmful content (e.g., political, defamatory imagery), undermining creator rights and platform safety. Existing defenses like concept-erasure methods focus on full diffusion models (DMs), neglecting LoRA's unique role as a modular adapter and its vulnerability to adversarial prompt engineering. To bridge this gap, we propose LoRAShield, the first data-free editing framework for securing LoRA models against misuse. Our platform-driven approach dynamically edits and realigns LoRA's weight subspace via adversarial optimization and semantic augmentation. Experimental results demonstrate that LoRAShield achieves remarkable effectiveness, efficiency, and robustness in blocking malicious generations without sacrificing the functionality of the benign task. By shifting the defense to platforms, LoRAShield enables secure, scalable sharing of personalized models, a critical step toward trustworthy generative ecosystems.</li>
</ul>

<h3>Title: An Ensemble Embedding Approach for Improving Semantic Caching Performance in LLM-based Systems</h3>
<ul>
<li><strong>Authors: </strong>Shervin Ghaffari, Zohre Bahranifard, Mohammad Akbari</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07061">https://arxiv.org/abs/2507.07061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07061">https://arxiv.org/pdf/2507.07061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07061]] An Ensemble Embedding Approach for Improving Semantic Caching Performance in LLM-based Systems(https://arxiv.org/abs/2507.07061)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Semantic caching enhances the efficiency of large language model (LLM) systems by identifying semantically similar queries, storing responses once, and serving them for subsequent equivalent requests. However, existing semantic caching frameworks rely on single embedding models for query representation, which limits their ability to capture the diverse semantic relationships present in real-world query distributions. This paper presents an ensemble embedding approach that combines multiple embedding models through a trained meta-encoder to improve semantic similarity detection in LLM caching systems. We evaluate our method using the Quora Question Pairs (QQP) dataset, measuring cache hit ratios, cache miss ratios, token savings, and response times. Our ensemble approach achieves a 92\% cache hit ratio for semantically equivalent queries while maintaining an 85\% accuracy in correctly rejecting non-equivalent queries as cache misses. These results demonstrate that ensemble embedding methods significantly outperform single-model approaches in distinguishing between semantically similar and dissimilar queries, leading to more effective caching performance and reduced computational overhead in LLM-based systems.</li>
</ul>

<h3>Title: Reading a Ruler in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Yimu Pan, Manas Mehta, Gwen Sincerbeaux, Jeffery A. Goldstein, Alison D. Gernand, James Z. Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07077">https://arxiv.org/abs/2507.07077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07077">https://arxiv.org/pdf/2507.07077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07077]] Reading a Ruler in the Wild(https://arxiv.org/abs/2507.07077)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurately converting pixel measurements into absolute real-world dimensions remains a fundamental challenge in computer vision and limits progress in key applications such as biomedicine, forensics, nutritional analysis, and e-commerce. We introduce RulerNet, a deep learning framework that robustly infers scale "in the wild" by reformulating ruler reading as a unified keypoint-detection problem and by representing the ruler with geometric-progression parameters that are invariant to perspective transformations. Unlike traditional methods that rely on handcrafted thresholds or rigid, ruler-specific pipelines, RulerNet directly localizes centimeter marks using a distortion-invariant annotation and training strategy, enabling strong generalization across diverse ruler types and imaging conditions while mitigating data scarcity. We also present a scalable synthetic-data pipeline that combines graphics-based ruler generation with ControlNet to add photorealistic context, greatly increasing training diversity and improving performance. To further enhance robustness and efficiency, we propose DeepGP, a lightweight feed-forward network that regresses geometric-progression parameters from noisy marks and eliminates iterative optimization, enabling real-time scale estimation on mobile or edge devices. Experiments show that RulerNet delivers accurate, consistent, and efficient scale estimates under challenging real-world conditions. These results underscore its utility as a generalizable measurement tool and its potential for integration with other vision components for automated, scale-aware analysis in high-impact domains. A live demo is available at this https URL.</li>
</ul>

<h3>Title: Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful</h3>
<ul>
<li><strong>Authors: </strong>Martin Marek, Sanae Lotfi, Aditya Somasundaram, Andrew Gordon Wilson, Micah Goldblum</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07101">https://arxiv.org/abs/2507.07101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07101">https://arxiv.org/pdf/2507.07101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07101]] Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful(https://arxiv.org/abs/2507.07101)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Conventional wisdom dictates that small batch sizes make language model pretraining and fine-tuning unstable, motivating gradient accumulation, which trades off the number of optimizer steps for a proportional increase in batch size. While it is common to decrease the learning rate for smaller batch sizes, other hyperparameters are often held fixed. In this work, we revisit small batch sizes all the way down to batch size one, and we propose a rule for scaling Adam hyperparameters to small batch sizes. We find that small batch sizes (1) train stably, (2) are consistently more robust to hyperparameter choices, (3) achieve equal or better per-FLOP performance than larger batch sizes, and (4) notably enable stable language model training with vanilla SGD, even without momentum, despite storing no optimizer state. Building on these results, we provide practical recommendations for selecting a batch size and setting optimizer hyperparameters. We further recommend against gradient accumulation unless training on multiple devices with multiple model replicas, bottlenecked by inter-device bandwidth.</li>
</ul>

<h3>Title: Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tiezheng Zhang, Yitong Li, Yu-cheng Chou, Jieneng Chen, Alan Yuille, Chen Wei, Junfei Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07104">https://arxiv.org/abs/2507.07104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07104">https://arxiv.org/pdf/2507.07104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07104]] Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models(https://arxiv.org/abs/2507.07104)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billions of high-quality image-text pairs, requiring millions of GPU hours. This paper introduces the Vision-Language-Vision (VLV) auto-encoder framework, which strategically leverages key pretrained components: a vision encoder, the decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large Language Model (LLM). Specifically, we establish an information bottleneck by regularizing the language representation space, achieved through freezing the pretrained T2I diffusion decoder. Our VLV pipeline effectively distills knowledge from the text-conditioned diffusion model using continuous embeddings, demonstrating comprehensive semantic understanding via high-quality reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the intermediate language representations into detailed descriptions, we construct a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and significantly reduces data requirements; by primarily utilizing single-modal images for training and maximizing the utility of existing pretrained models (image encoder, T2I diffusion model, and LLM), it circumvents the need for massive paired image-text datasets, keeping the total training expenditure under $1,000 USD.</li>
</ul>

<h3>Title: Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor</h3>
<ul>
<li><strong>Authors: </strong>Vatsal Agarwal, Matthew Gwilliam, Gefen Kohavi, Eshan Verma, Daniel Ulbricht, Abhinav Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07106">https://arxiv.org/abs/2507.07106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07106">https://arxiv.org/pdf/2507.07106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07106]] Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor(https://arxiv.org/abs/2507.07106)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in multimodal large language models (MLLMs) have enabled image-based question-answering capabilities. However, a key limitation is the use of CLIP as the visual encoder; while it can capture coarse global information, it often can miss fine-grained details that are relevant to the input query. To address these shortcomings, this work studies whether pre-trained text-to-image diffusion models can serve as instruction-aware visual encoders. Through an analysis of their internal representations, we find diffusion features are both rich in semantics and can encode strong image-text alignment. Moreover, we find that we can leverage text conditioning to focus the model on regions relevant to the input question. We then investigate how to align these features with large language models and uncover a leakage phenomenon, where the LLM can inadvertently recover information from the original diffusion prompt. We analyze the causes of this leakage and propose a mitigation strategy. Based on these insights, we explore a simple fusion strategy that utilizes both CLIP and conditional diffusion features. We evaluate our approach on both general VQA and specialized MLLM benchmarks, demonstrating the promise of diffusion models for visual understanding, particularly in vision-centric tasks that require spatial and compositional reasoning. Our project page can be found this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
