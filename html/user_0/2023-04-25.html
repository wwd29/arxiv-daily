<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: Detecting Adversarial Faces Using Only Real Face Self-Perturbations. (arXiv:2304.11359v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11359">http://arxiv.org/abs/2304.11359</a></li>
<li>Code URL: <a href="https://github.com/cc13qq/sapd">https://github.com/cc13qq/sapd</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11359] Detecting Adversarial Faces Using Only Real Face Self-Perturbations](http://arxiv.org/abs/2304.11359) #security</code></li>
<li>Summary: <p>Adversarial attacks aim to disturb the functionality of a target system by
adding specific noise to the input samples, bringing potential threats to
security and robustness when applied to facial recognition systems. Although
existing defense techniques achieve high accuracy in detecting some specific
adversarial faces (adv-faces), new attack methods especially GAN-based attacks
with completely different noise patterns circumvent them and reach a higher
attack success rate. Even worse, existing techniques require attack data before
implementing the defense, making it impractical to defend newly emerging
attacks that are unseen to defenders. In this paper, we investigate the
intrinsic generality of adv-faces and propose to generate pseudo adv-faces by
perturbing real faces with three heuristically designed noise patterns. We are
the first to train an adv-face detector using only real faces and their
self-perturbations, agnostic to victim facial recognition systems, and agnostic
to unseen attacks. By regarding adv-faces as out-of-distribution data, we then
naturally introduce a novel cascaded system for adv-face detection, which
consists of training data self-perturbations, decision boundary regularization,
and a max-pooling-based binary classifier focusing on abnormal local color
aberrations. Experiments conducted on LFW and CelebA-HQ datasets with eight
gradient-based and two GAN-based attacks validate that our method generalizes
to a variety of unseen adversarial attacks.
</p></li>
</ul>

<h3>Title: FVCARE:Formal Verification of Security Primitives in Resilient Embedded SoCs. (arXiv:2304.11489v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11489">http://arxiv.org/abs/2304.11489</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11489] FVCARE:Formal Verification of Security Primitives in Resilient Embedded SoCs](http://arxiv.org/abs/2304.11489) #security</code></li>
<li>Summary: <p>With the increased utilization, the small embedded and IoT devices have
become an attractive target for sophisticated attacks that can exploit the
devices security critical information and data in malevolent activities. Secure
boot and Remote Attestation (RA) techniques verifies the integrity of the
devices software state at boot-time and runtime. Correct implementation and
formal verification of these security primitives provide strong security
guarantees and enhance user confidence. The formal verification of these
security primitives is considered challenging, as it involves complex hardware
software interactions, semantics gaps and requires bit-precise reasoning. To
address these challenges, this paper presents FVCARE an end to end system
co-verification framework. It also defines the security properties for
resilient small embedded systems. FVCARE divides the end to end system co
verification problem into two modules: 1) verifying the (bit precise) initial
system settings, registers, and access control policies by hardware
verification techniques, and 2) verifying the system specification, security
properties, and functional correctness using source-level software abstraction
of the hardware. The evaluation of proposed techniques on SRACARE based systems
demonstrates its efficacy in security co verification.
</p></li>
</ul>

<h3>Title: Constructing a meta-learner for unsupervised anomaly detection. (arXiv:2304.11438v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11438">http://arxiv.org/abs/2304.11438</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11438] Constructing a meta-learner for unsupervised anomaly detection](http://arxiv.org/abs/2304.11438) #security</code></li>
<li>Summary: <p>Unsupervised anomaly detection (AD) is critical for a wide range of practical
applications, from network security to health and medical tools. Due to the
diversity of problems, no single algorithm has been found to be superior for
all AD tasks. Choosing an algorithm, otherwise known as the Algorithm Selection
Problem (ASP), has been extensively examined in supervised classification
problems, through the use of meta-learning and AutoML, however, it has received
little attention in unsupervised AD tasks. This research proposes a new
meta-learning approach that identifies an appropriate unsupervised AD algorithm
given a set of meta-features generated from the unlabelled input dataset. The
performance of the proposed meta-learner is superior to the current state of
the art solution. In addition, a mixed model statistical analysis has been
conducted to examine the impact of the meta-learner components: the meta-model,
meta-features, and the base set of AD algorithms, on the overall performance of
the meta-learner. The analysis was conducted using more than 10,000 datasets,
which is significantly larger than previous studies. Results indicate that a
relatively small number of meta-features can be used to identify an appropriate
AD algorithm, but the choice of a meta-model in the meta-learner has a
considerable impact.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Power to the Data Defenders: Human-Centered Disclosure Risk Calibration of Open Data. (arXiv:2304.11278v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11278">http://arxiv.org/abs/2304.11278</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11278] Power to the Data Defenders: Human-Centered Disclosure Risk Calibration of Open Data](http://arxiv.org/abs/2304.11278) #privacy</code></li>
<li>Summary: <p>The open data ecosystem is susceptible to vulnerabilities due to disclosure
risks. Though the datasets are anonymized during release, the prevalence of the
release-and-forget model makes the data defenders blind to privacy issues
arising after the dataset release. One such issue can be the disclosure risks
in the presence of newly released datasets which may compromise the privacy of
the data subjects of the anonymous open datasets. In this paper, we first
examine some of these pitfalls through the examples we observed during a red
teaming exercise and then envision other possible vulnerabilities in this
context. We also discuss proactive risk monitoring, including developing a
collection of highly susceptible open datasets and a visual analytic workflow
that empowers data defenders towards undertaking dynamic risk calibration
strategies.
</p></li>
</ul>

<h3>Title: Privacy Computing Meets Metaverse: Necessity, Taxonomy and Challenges. (arXiv:2304.11643v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11643">http://arxiv.org/abs/2304.11643</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11643] Privacy Computing Meets Metaverse: Necessity, Taxonomy and Challenges](http://arxiv.org/abs/2304.11643) #privacy</code></li>
<li>Summary: <p>Metaverse, the core of the next-generation Internet, is a computer-generated
holographic digital environment that simultaneously combines spatio-temporal,
immersive, real-time, sustainable, interoperable, and data-sensitive
characteristics. It cleverly blends the virtual and real worlds, allowing users
to create, communicate, and transact in virtual form. With the rapid
development of emerging technologies including augmented reality, virtual
reality and blockchain, the metaverse system is becoming more and more
sophisticated and widely used in various fields such as social, tourism,
industry and economy. However, the high level of interaction with the real
world also means a huge risk of privacy leakage both for individuals and
enterprises, which has hindered the wide deployment of metaverse. Then, it is
inevitable to apply privacy computing techniques in the framework of metaverse,
which is a current research hotspot. In this paper, we conduct a comprehensive
research of the necessity, taxonomy and challenges when privacy computing meets
metaverse. Specifically, we first introduce the underlying technologies and
various applications of metaverse, on which we analyze the challenges of data
usage in metaverse, especially data privacy. Next, we review and summarize
state-of-the-art solutions based on federated learning, differential privacy,
homomorphic encryption, and zero-knowledge proofs for different privacy
problems in metaverse. Finally, we show the current security and privacy
challenges in the development of metaverse and provide open directions for
building a well-established privacy-preserving metaverse system.
</p></li>
</ul>

<h3>Title: Differentially Private Synthetic Data Generation via Lipschitz-Regularised Variational Autoencoders. (arXiv:2304.11336v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11336">http://arxiv.org/abs/2304.11336</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11336] Differentially Private Synthetic Data Generation via Lipschitz-Regularised Variational Autoencoders](http://arxiv.org/abs/2304.11336) #privacy</code></li>
<li>Summary: <p>Synthetic data has been hailed as the silver bullet for privacy preserving
data analysis. If a record is not real, then how could it violate a person's
privacy? In addition, deep-learning based generative models are employed
successfully to approximate complex high-dimensional distributions from data
and draw realistic samples from this learned distribution. It is often
overlooked though that generative models are prone to memorising many details
of individual training records and often generate synthetic data that too
closely resembles the underlying sensitive training data, hence violating
strong privacy regulations as, e.g., encountered in health care. Differential
privacy is the well-known state-of-the-art framework for guaranteeing
protection of sensitive individuals' data, allowing aggregate statistics and
even machine learning models to be released publicly without compromising
privacy. The training mechanisms however often add too much noise during the
training process, and thus severely compromise the utility of these private
models. Even worse, the tight privacy budgets do not allow for many training
epochs so that model quality cannot be properly controlled in practice. In this
paper we explore an alternative approach for privately generating data that
makes direct use of the inherent stochasticity in generative models, e.g.,
variational autoencoders. The main idea is to appropriately constrain the
continuity modulus of the deep models instead of adding another noise mechanism
on top. For this approach, we derive mathematically rigorous privacy guarantees
and illustrate its effectiveness with practical experiments.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: A Group-Specific Approach to NLP for Hate Speech Detection. (arXiv:2304.11223v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11223">http://arxiv.org/abs/2304.11223</a></li>
<li>Code URL: <a href="https://github.com/enscma2/knowledje">https://github.com/enscma2/knowledje</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11223] A Group-Specific Approach to NLP for Hate Speech Detection](http://arxiv.org/abs/2304.11223) #protect</code></li>
<li>Summary: <p>Automatic hate speech detection is an important yet complex task, requiring
knowledge of common sense, stereotypes of protected groups, and histories of
discrimination, each of which may constantly evolve. In this paper, we propose
a group-specific approach to NLP for online hate speech detection. The approach
consists of creating and infusing historical and linguistic knowledge about a
particular protected group into hate speech detection models, analyzing
historical data about discrimination against a protected group to better
predict spikes in hate speech against that group, and critically evaluating
hate speech detection models through lenses of intersectionality and ethics. We
demonstrate this approach through a case study on NLP for detection of
antisemitic hate speech. The case study synthesizes the current
English-language literature on NLP for antisemitism detection, introduces a
novel knowledge graph of antisemitic history and language from the 20th century
to the present, infuses information from the knowledge graph into a set of
tweets over Logistic Regression and uncased DistilBERT baselines, and suggests
that incorporating context from the knowledge graph can help models pick up
subtle stereotypes.
</p></li>
</ul>

<h3>Title: Identifying Appropriate Intellectual Property Protection Mechanisms for Machine Learning Models: A Systematization of Watermarking, Fingerprinting, Model Access, and Attacks. (arXiv:2304.11285v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11285">http://arxiv.org/abs/2304.11285</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11285] Identifying Appropriate Intellectual Property Protection Mechanisms for Machine Learning Models: A Systematization of Watermarking, Fingerprinting, Model Access, and Attacks](http://arxiv.org/abs/2304.11285) #protect</code></li>
<li>Summary: <p>The commercial use of Machine Learning (ML) is spreading; at the same time,
ML models are becoming more complex and more expensive to train, which makes
Intellectual Property Protection (IPP) of trained models a pressing issue.
Unlike other domains that can build on a solid understanding of the threats,
attacks and defenses available to protect their IP, the ML-related research in
this regard is still very fragmented. This is also due to a missing unified
view as well as a common taxonomy of these aspects.
</p></li>
</ul>

<p>In this paper, we systematize our findings on IPP in ML, while focusing on
threats and attacks identified and defenses proposed at the time of writing. We
develop a comprehensive threat model for IP in ML, categorizing attacks and
defenses within a unified and consolidated taxonomy, thus bridging research
from both the ML and security communities.
</p>

<h2>defense</h2>
<h3>Title: LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation. (arXiv:2304.11379v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11379">http://arxiv.org/abs/2304.11379</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11379] LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation](http://arxiv.org/abs/2304.11379) #defense</code></li>
<li>Summary: <p>Semantic map construction under bird's-eye view (BEV) plays an essential role
in autonomous driving. In contrast to camera image, LiDAR provides the accurate
3D observations to project the captured 3D features onto BEV space inherently.
However, the vanilla LiDAR-based BEV feature often contains many indefinite
noises, where the spatial features have little texture and semantic cues. In
this paper, we propose an effective LiDAR-based method to build semantic map.
Specifically, we introduce a BEV pyramid feature decoder that learns the robust
multi-scale BEV features for semantic map construction, which greatly boosts
the accuracy of the LiDAR-based method. To mitigate the defects caused by
lacking semantic cues in LiDAR data, we present an online Camera-to-LiDAR
distillation scheme to facilitate the semantic learning from image to point
cloud. Our distillation scheme consists of feature-level and logit-level
distillation to absorb the semantic information from camera in BEV. The
experimental results on challenging nuScenes dataset demonstrate the efficacy
of our proposed LiDAR2Map on semantic map construction, which significantly
outperforms the previous LiDAR-based methods over 27.9% mIoU and even performs
better than the state-of-the-art camera-based approaches. Source code is
available at: https://github.com/songw-zju/LiDAR2Map.
</p></li>
</ul>

<h3>Title: MAWSEO: Adversarial Wiki Search Poisoning for Illicit Online Promotion. (arXiv:2304.11300v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11300">http://arxiv.org/abs/2304.11300</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11300] MAWSEO: Adversarial Wiki Search Poisoning for Illicit Online Promotion](http://arxiv.org/abs/2304.11300) #defense</code></li>
<li>Summary: <p>As a prominent instance of vandalism edits, Wiki search poisoning for illicit
promotion is a cybercrime in which the adversary aims at editing Wiki articles
to promote illicit businesses through Wiki search results of relevant queries.
In this paper, we report a study that, for the first time, shows that such
stealthy blackhat SEO on Wiki can be automated. Our technique, called MAWSEO,
employs adversarial revisions to achieve real-world cybercriminal objectives,
including rank boosting, vandalism detection evasion, topic relevancy, semantic
consistency, user awareness (but not alarming) of promotional content, etc. Our
evaluation and user study demonstrate that MAWSEO is able to effectively and
efficiently generate adversarial vandalism edits, which can bypass
state-of-the-art built-in Wiki vandalism detectors, and also get promotional
content through to Wiki users without triggering their alarms. In addition, we
investigated potential defense, including coherence based detection and
adversarial training of vandalism detection, against our attack in the Wiki
ecosystem.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: StyLess: Boosting the Transferability of Adversarial Examples. (arXiv:2304.11579v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11579">http://arxiv.org/abs/2304.11579</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11579] StyLess: Boosting the Transferability of Adversarial Examples](http://arxiv.org/abs/2304.11579) #attack</code></li>
<li>Summary: <p>Adversarial attacks can mislead deep neural networks (DNNs) by adding
imperceptible perturbations to benign examples. The attack transferability
enables adversarial examples to attack black-box DNNs with unknown
architectures or parameters, which poses threats to many real-world
applications. We find that existing transferable attacks do not distinguish
between style and content features during optimization, limiting their attack
transferability. To improve attack transferability, we propose a novel attack
method called style-less perturbation (StyLess). Specifically, instead of using
a vanilla network as the surrogate model, we advocate using stylized networks,
which encode different style features by perturbing an adaptive instance
normalization. Our method can prevent adversarial examples from using
non-robust style features and help generate transferable perturbations.
Comprehensive experiments show that our method can significantly improve the
transferability of adversarial examples. Furthermore, our approach is generic
and can outperform state-of-the-art transferable attacks when combined with
other attack techniques.
</p></li>
</ul>

<h3>Title: Evading DeepFake Detectors via Adversarial Statistical Consistency. (arXiv:2304.11670v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11670">http://arxiv.org/abs/2304.11670</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11670] Evading DeepFake Detectors via Adversarial Statistical Consistency](http://arxiv.org/abs/2304.11670) #attack</code></li>
<li>Summary: <p>In recent years, as various realistic face forgery techniques known as
DeepFake improves by leaps and bounds,more and more DeepFake detection
techniques have been proposed. These methods typically rely on detecting
statistical differences between natural (i.e., real) and DeepFakegenerated
images in both spatial and frequency domains. In this work, we propose to
explicitly minimize the statistical differences to evade state-of-the-art
DeepFake detectors. To this end, we propose a statistical consistency attack
(StatAttack) against DeepFake detectors, which contains two main parts. First,
we select several statistical-sensitive natural degradations (i.e., exposure,
blur, and noise) and add them to the fake images in an adversarial way. Second,
we find that the statistical differences between natural and DeepFake images
are positively associated with the distribution shifting between the two kinds
of images, and we propose to use a distribution-aware loss to guide the
optimization of different degradations. As a result, the feature distributions
of generated adversarial examples is close to the natural images.Furthermore,
we extend the StatAttack to a more powerful version, MStatAttack, where we
extend the single-layer degradation to multi-layer degradations sequentially
and use the loss to tune the combination weights jointly. Comprehensive
experimental results on four spatial-based detectors and two frequency-based
detectors with four datasets demonstrate the effectiveness of our proposed
attack method in both white-box and black-box settings.
</p></li>
</ul>

<h3>Title: Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack. (arXiv:2304.11436v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11436">http://arxiv.org/abs/2304.11436</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11436] Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack](http://arxiv.org/abs/2304.11436) #attack</code></li>
<li>Summary: <p>Federated Learning with Model Distillation (FedMD) is a nascent collaborative
learning paradigm, where only output logits of public datasets are transmitted
as distilled knowledge, instead of passing on private model parameters that are
susceptible to gradient inversion attacks, a known privacy risk in federated
learning. In this paper, we found that even though sharing output logits of
public datasets is safer than directly sharing gradients, there still exists a
substantial risk of data exposure caused by carefully designed malicious
attacks. Our study shows that a malicious server can inject a PLI
(Paired-Logits Inversion) attack against FedMD and its variants by training an
inversion neural network that exploits the confidence gap between the server
and client models. Experiments on multiple facial recognition datasets validate
that under FedMD-like schemes, by using paired server-client logits of public
datasets only, the malicious server is able to reconstruct private images on
all tested benchmarks with a high success rate.
</p></li>
</ul>

<h3>Title: Money Over Morals: A Business Analysis of Conti Ransomware. (arXiv:2304.11681v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11681">http://arxiv.org/abs/2304.11681</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11681] Money Over Morals: A Business Analysis of Conti Ransomware](http://arxiv.org/abs/2304.11681) #attack</code></li>
<li>Summary: <p>Ransomware operations have evolved from relatively unsophisticated threat
actors into highly coordinated cybercrime syndicates that regularly extort
millions of dollars in a single attack. Despite dominating headlines and
crippling businesses across the globe, there is relatively little in-depth
research into the modern structure and economics of ransomware operations.
</p></li>
</ul>

<p>In this paper, we leverage leaked chat messages to provide an in-depth
empirical analysis of Conti, one of the largest ransomware groups. By analyzing
these chat messages, we construct a picture of Conti's operations as a
highly-profitable business, from profit structures to employee recruitment and
roles. We present novel methodologies to trace ransom payments, identifying
over $80 million in likely ransom payments to Conti and its predecessor -- over
five times as much as in previous public datasets. As part of our work, we
publish a dataset of 666 labeled Bitcoin addresses related to Conti and an
additional 75 Bitcoin addresses of likely ransom payments. Future work can
leverage this case study to more effectively trace -- and ultimately counteract
-- ransomware activity.
</p>

<h3>Title: Universal Adversarial Backdoor Attacks to Fool Vertical Federated Learning in Cloud-Edge Collaboration. (arXiv:2304.11432v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11432">http://arxiv.org/abs/2304.11432</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11432] Universal Adversarial Backdoor Attacks to Fool Vertical Federated Learning in Cloud-Edge Collaboration](http://arxiv.org/abs/2304.11432) #attack</code></li>
<li>Summary: <p>Vertical federated learning (VFL) is a cloud-edge collaboration paradigm that
enables edge nodes, comprising resource-constrained Internet of Things (IoT)
devices, to cooperatively train artificial intelligence (AI) models while
retaining their data locally. This paradigm facilitates improved privacy and
security for edges and IoT devices, making VFL an essential component of
Artificial Intelligence of Things (AIoT) systems. Nevertheless, the partitioned
structure of VFL can be exploited by adversaries to inject a backdoor, enabling
them to manipulate the VFL predictions. In this paper, we aim to investigate
the vulnerability of VFL in the context of binary classification tasks. To this
end, we define a threat model for backdoor attacks in VFL and introduce a
universal adversarial backdoor (UAB) attack to poison the predictions of VFL.
The UAB attack, consisting of universal trigger generation and clean-label
backdoor injection, is incorporated during the VFL training at specific
iterations. This is achieved by alternately optimizing the universal trigger
and model parameters of VFL sub-problems. Our work distinguishes itself from
existing studies on designing backdoor attacks for VFL, as those require the
knowledge of auxiliary information not accessible within the split VFL
architecture. In contrast, our approach does not necessitate any additional
data to execute the attack. On the LendingClub and Zhongyuan datasets, our
approach surpasses existing state-of-the-art methods, achieving up to 100\%
backdoor task performance while maintaining the main task performance. Our
results in this paper make a major advance to revealing the hidden backdoor
risks of VFL, hence paving the way for the future development of secure AIoT.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Benchmarking Low-Shot Robustness to Natural Distribution Shifts. (arXiv:2304.11263v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11263">http://arxiv.org/abs/2304.11263</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11263] Benchmarking Low-Shot Robustness to Natural Distribution Shifts](http://arxiv.org/abs/2304.11263) #robust</code></li>
<li>Summary: <p>Robustness to natural distribution shifts has seen remarkable progress thanks
to recent pre-training strategies combined with better fine-tuning methods.
However, such fine-tuning assumes access to large amounts of labelled data, and
the extent to which the observations hold when the amount of training data is
not as high remains unknown. We address this gap by performing the first
in-depth study of robustness to various natural distribution shifts in
different low-shot regimes: spanning datasets, architectures, pre-trained
initializations, and state-of-the-art robustness interventions. Most
importantly, we find that there is no single model of choice that is often more
robust than others, and existing interventions can fail to improve robustness
on some datasets even if they do so in the full-shot regime. We hope that our
work will motivate the community to focus on this problem of practical
importance.
</p></li>
</ul>

<h3>Title: SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models. (arXiv:2304.11619v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11619">http://arxiv.org/abs/2304.11619</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11619] SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models](http://arxiv.org/abs/2304.11619) #robust</code></li>
<li>Summary: <p>Interpreting remote sensing imagery enables numerous downstream applications
ranging from land-use planning to deforestation monitoring. Robustly
classifying this data is challenging due to the Earth's geographic diversity.
While many distinct satellite and aerial image classification datasets exist,
there is yet to be a benchmark curated that suitably covers this diversity. In
this work, we introduce SATellite ImageNet (SATIN), a metadataset curated from
27 existing remotely sensed datasets, and comprehensively evaluate the
zero-shot transfer classification capabilities of a broad range of
vision-language (VL) models on SATIN. We find SATIN to be a challenging
benchmark-the strongest method we evaluate achieves a classification accuracy
of 52.0%. We provide a $\href{https://satinbenchmark.github.io}{\text{public
leaderboard}}$ to guide and track the progress of VL models in this important
domain.
</p></li>
</ul>

<h3>Title: IslamicPCQA: A Dataset for Persian Multi-hop Complex Question Answering in Islamic Text Resources. (arXiv:2304.11664v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11664">http://arxiv.org/abs/2304.11664</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11664] IslamicPCQA: A Dataset for Persian Multi-hop Complex Question Answering in Islamic Text Resources](http://arxiv.org/abs/2304.11664) #robust</code></li>
<li>Summary: <p>Nowadays, one of the main challenges for Question Answering Systems is to
answer complex questions using various sources of information. Multi-hop
questions are a type of complex questions that require multi-step reasoning to
answer. In this article, the IslamicPCQA dataset is introduced. This is the
first Persian dataset for answering complex questions based on non-structured
information sources and consists of 12,282 question-answer pairs extracted from
9 Islamic encyclopedias. This dataset has been created inspired by the HotpotQA
English dataset approach, which was customized to suit the complexities of the
Persian language. Answering questions in this dataset requires more than one
paragraph and reasoning. The questions are not limited to any prior knowledge
base or ontology, and to provide robust reasoning ability, the dataset also
includes supporting facts and key sentences. The prepared dataset covers a wide
range of Islamic topics and aims to facilitate answering complex Persian
questions within this subject matter
</p></li>
</ul>

<h3>Title: Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11171">http://arxiv.org/abs/2304.11171</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11171] Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method](http://arxiv.org/abs/2304.11171) #robust</code></li>
<li>Summary: <p>Human cognition has a <code>large-scale first'' cognitive mechanism, therefore
possesses adaptive multi-granularity description capabilities. This results in
computational characteristics such as efficiency, robustness, and
interpretability. Although most existing artificial intelligence learning
methods have certain multi-granularity features, they do not fully align with
the</code>large-scale first'' cognitive mechanism. Multi-granularity granular-ball
computing is an important model method developed in recent years. This method
can use granular-balls of different sizes to adaptively represent and cover the
sample space, and perform learning based on granular-balls. Since the number of
coarse-grained "granular-ball" is smaller than the number of sample points,
granular-ball computing is more efficient; the coarse-grained characteristics
of granular-balls are less likely to be affected by fine-grained sample points,
making them more robust; the multi-granularity structure of granular-balls can
produce topological structures and coarse-grained descriptions, providing
natural interpretability. Granular-ball computing has now been effectively
extended to various fields of artificial intelligence, developing theoretical
methods such as granular-ball classifiers, granular-ball clustering methods,
granular-ball neural networks, granular-ball rough sets, and granular-ball
evolutionary computation, significantly improving the efficiency, noise
robustness, and interpretability of existing methods. It has good innovation,
practicality, and development potential. This article provides a systematic
introduction to these methods and analyzes the main problems currently faced by
granular-ball computing, discussing both the primary applicable scenarios for
granular-ball computing and offering references and suggestions for future
researchers to improve this theory.
</p></li>
</ul>

<h3>Title: Probabilistic selection and design of concrete using machine learning. (arXiv:2304.11226v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11226">http://arxiv.org/abs/2304.11226</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11226] Probabilistic selection and design of concrete using machine learning](http://arxiv.org/abs/2304.11226) #robust</code></li>
<li>Summary: <p>Development of robust concrete mixes with a lower environmental impact is
challenging due to natural variability in constituent materials and a multitude
of possible combinations of mix proportions. Making reliable property
predictions with machine learning can facilitate performance-based
specification of concrete, reducing material inefficiencies and improving the
sustainability of concrete construction. In this work, we develop a machine
learning algorithm that can utilize intermediate target variables and their
associated noise to predict the final target variable. We apply the methodology
to specify a concrete mix that has high resistance to carbonation, and another
concrete mix that has low environmental impact. Both mixes also fulfill targets
on the strength, density, and cost. The specified mixes are experimentally
validated against their predictions. Our generic methodology enables the
exploitation of noise in machine learning, which has a broad range of
applications in structural engineering and beyond.
</p></li>
</ul>

<h3>Title: Time Series Classification for Detecting Parkinson's Disease from Wrist Motions. (arXiv:2304.11265v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11265">http://arxiv.org/abs/2304.11265</a></li>
<li>Code URL: <a href="https://github.com/cedricdonie/tsc-for-wrist-motion-pd-detection">https://github.com/cedricdonie/tsc-for-wrist-motion-pd-detection</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11265] Time Series Classification for Detecting Parkinson's Disease from Wrist Motions](http://arxiv.org/abs/2304.11265) #robust</code></li>
<li>Summary: <p>Parkinson's disease (PD) is a neurodegenerative disease with frequently
changing motor symptoms where continuous symptom monitoring enables more
targeted treatment. Classical time series classification (TSC) and deep
learning techniques have limited performance for PD symptom monitoring using
wearable accelerometer data because PD movement patterns are complex, but
datasets are small. We investigate InceptionTime and RandOm Convolutional
KErnel Transform (ROCKET) because they are state-of-the-art for TSC and
promising for PD symptom monitoring: InceptionTime's high learning capacity is
suited to modeling complex movement patterns while ROCKET is suited to small
datasets. We used a random search to find the highest-scoring InceptionTime
architecture and compared it to ROCKET with a ridge classifier and a
multi-layer perceptron (MLP) on wrist motions of PD patients. We find that all
approaches are suitable for estimating tremor severity and bradykinesia
presence but struggle with detecting dyskinesia. ROCKET performs better for
dyskinesia, whereas InceptionTime is slightly better for tremor and
bradykinesia but has much higher variability in performance. Both outperform
the MLP. In conclusion, both InceptionTime and ROCKET are suitable for
continuous symptom monitoring, with the choice depending on the symptom of
interest and desired robustness.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: VisiTherS: Visible-thermal infrared stereo disparity estimation of human silhouette. (arXiv:2304.11291v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11291">http://arxiv.org/abs/2304.11291</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11291] VisiTherS: Visible-thermal infrared stereo disparity estimation of human silhouette](http://arxiv.org/abs/2304.11291) #extraction</code></li>
<li>Summary: <p>This paper presents a novel approach for visible-thermal infrared
stereoscopy, focusing on the estimation of disparities of human silhouettes.
Visible-thermal infrared stereo poses several challenges, including occlusions
and differently textured matching regions in both spectra. Finding matches
between two spectra with varying colors, textures, and shapes adds further
complexity to the task. To address the aforementioned challenges, this paper
proposes a novel approach where a high-resolution convolutional neural network
is used to better capture relationships between the two spectra. To do so, a
modified HRNet backbone is used for feature extraction. This HRNet backbone is
capable of capturing fine details and textures as it extracts features at
multiple scales, thereby enabling the utilization of both local and global
information. For matching visible and thermal infrared regions, our method
extracts features on each patch using two modified HRNet streams. Features from
the two streams are then combined for predicting the disparities by
concatenation and correlation. Results on public datasets demonstrate the
effectiveness of the proposed approach by improving the results by
approximately 18 percentage points on the $\leq$ 1 pixel error, highlighting
its potential for improving accuracy in this task. The code of VisiTherS is
available on GitHub at the following link
https://github.com/philippeDG/VisiTherS.
</p></li>
</ul>

<h3>Title: A Lightweight Recurrent Learning Network for Sustainable Compressed Sensing. (arXiv:2304.11674v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11674">http://arxiv.org/abs/2304.11674</a></li>
<li>Code URL: <a href="https://github.com/c66yu/csrn">https://github.com/c66yu/csrn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11674] A Lightweight Recurrent Learning Network for Sustainable Compressed Sensing](http://arxiv.org/abs/2304.11674) #extraction</code></li>
<li>Summary: <p>Recently, deep learning-based compressed sensing (CS) has achieved great
success in reducing the sampling and computational cost of sensing systems and
improving the reconstruction quality. These approaches, however, largely
overlook the issue of the computational cost; they rely on complex structures
and task-specific operator designs, resulting in extensive storage and high
energy consumption in CS imaging systems. In this paper, we propose a
lightweight but effective deep neural network based on recurrent learning to
achieve a sustainable CS system; it requires a smaller number of parameters but
obtains high-quality reconstructions. Specifically, our proposed network
consists of an initial reconstruction sub-network and a residual reconstruction
sub-network. While the initial reconstruction sub-network has a hierarchical
structure to progressively recover the image, reducing the number of
parameters, the residual reconstruction sub-network facilitates recurrent
residual feature extraction via recurrent learning to perform both feature
fusion and deep reconstructions across different scales. In addition, we also
demonstrate that, after the initial reconstruction, feature maps with reduced
sizes are sufficient to recover the residual information, and thus we achieved
a significant reduction in the amount of memory required. Extensive experiments
illustrate that our proposed model can achieve a better reconstruction quality
than existing state-of-the-art CS algorithms, and it also has a smaller number
of network parameters than these algorithms. Our source codes are available at:
https://github.com/C66YU/CSRN.
</p></li>
</ul>

<h3>Title: Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness. (arXiv:2304.11633v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11633">http://arxiv.org/abs/2304.11633</a></li>
<li>Code URL: <a href="https://github.com/pkuserc/chatgpt_for_ie">https://github.com/pkuserc/chatgpt_for_ie</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11633] Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness](http://arxiv.org/abs/2304.11633) #extraction</code></li>
<li>Summary: <p>The capability of Large Language Models (LLMs) like ChatGPT to comprehend
user intent and provide reasonable responses has made them extremely popular
lately. In this paper, we focus on assessing the overall ability of ChatGPT
using 7 fine-grained information extraction (IE) tasks. Specially, we present
the systematically analysis by measuring ChatGPT's performance, explainability,
calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT
or domain experts. Our findings reveal that ChatGPT's performance in
Standard-IE setting is poor, but it surprisingly exhibits excellent performance
in the OpenIE setting, as evidenced by human evaluation. In addition, our
research indicates that ChatGPT provides high-quality and trustworthy
explanations for its decisions. However, there is an issue of ChatGPT being
overconfident in its predictions, which resulting in low calibration.
Furthermore, ChatGPT demonstrates a high level of faithfulness to the original
text in the majority of cases. We manually annotate and release the test sets
of 7 fine-grained IE tasks contains 14 datasets to further promote the
research. The datasets and code are available at
https://github.com/pkuserc/ChatGPT_for_IE.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h2>fair</h2>
<h3>Title: Identifying Stochasticity in Time-Series with Autoencoder-Based Content-aware 2D Representation: Application to Black Hole Data. (arXiv:2304.11560v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11560">http://arxiv.org/abs/2304.11560</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11560] Identifying Stochasticity in Time-Series with Autoencoder-Based Content-aware 2D Representation: Application to Black Hole Data](http://arxiv.org/abs/2304.11560) #fair</code></li>
<li>Summary: <p>In this work, we report an autoencoder-based 2D representation to classify a
time-series as stochastic or non-stochastic, to understand the underlying
physical process. Content-aware conversion of 1D time-series to 2D
representation, that simultaneously utilizes time- and frequency-domain
characteristics, is proposed. An autoencoder is trained with a loss function to
learn latent space (using both time- and frequency domains) representation,
that is designed to be, time-invariant. Every element of the time-series is
represented as a tuple with two components, one each, from latent space
representation in time- and frequency-domains, forming a binary image. In this
binary image, those tuples that represent the points in the time-series,
together form the ``Latent Space Signature" (LSS) of the input time-series. The
obtained binary LSS images are fed to a classification network. The
EfficientNetv2-S classifier is trained using 421 synthetic time-series, with
fair representation from both categories. The proposed methodology is evaluated
on publicly available astronomical data which are 12 distinct temporal classes
of time-series pertaining to the black hole GRS 1915 + 105, obtained from RXTE
satellite. Results obtained using the proposed methodology are compared with
existing techniques. Concurrence in labels obtained across the classes,
illustrates the efficacy of the proposed 2D representation using the latent
space co-ordinates. The proposed methodology also outputs the confidence in the
classification label.
</p></li>
</ul>

<h3>Title: A Framework for Benchmarking Real-Time Embedded Object Detection. (arXiv:2304.11580v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11580">http://arxiv.org/abs/2304.11580</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11580] A Framework for Benchmarking Real-Time Embedded Object Detection](http://arxiv.org/abs/2304.11580) #fair</code></li>
<li>Summary: <p>Object detection is one of the key tasks in many applications of computer
vision. Deep Neural Networks (DNNs) are undoubtedly a well-suited approach for
object detection. However, such DNNs need highly adapted hardware together with
hardware-specific optimization to guarantee high efficiency during inference.
This is especially the case when aiming for efficient object detection in video
streaming applications on limited hardware such as edge devices. Comparing
vendor-specific hardware and related optimization software pipelines in a fair
experimental setup is a challenge. In this paper, we propose a framework that
uses a host computer with a host software application together with a
light-weight interface based on the Message Queuing Telemetry Transport (MQTT)
protocol. Various different target devices with target apps can be connected
via MQTT with this host computer. With well-defined and standardized MQTT
messages, object detection results can be reported to the host computer, where
the results are evaluated without harming or influencing the processing on the
device. With this quite generic framework, we can measure the object detection
performance, the runtime, and the energy efficiency at the same time. The
effectiveness of this framework is demonstrated in multiple experiments that
offer deep insights into the optimization of DNNs.
</p></li>
</ul>

<h3>Title: Child Face Recognition at Scale: Synthetic Data Generation and Performance Benchmark. (arXiv:2304.11685v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11685">http://arxiv.org/abs/2304.11685</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11685] Child Face Recognition at Scale: Synthetic Data Generation and Performance Benchmark](http://arxiv.org/abs/2304.11685) #fair</code></li>
<li>Summary: <p>We address the need for a large-scale database of children's faces by using
generative adversarial networks (GANs) and face age progression (FAP) models to
synthesize a realistic dataset referred to as HDA-SynChildFaces. To this end,
we proposed a processing pipeline that initially utilizes StyleGAN3 to sample
adult subjects, which are subsequently progressed to children of varying ages
using InterFaceGAN. Intra-subject variations, such as facial expression and
pose, are created by further manipulating the subjects in their latent space.
Additionally, the presented pipeline allows to evenly distribute the races of
subjects, allowing to generate a balanced and fair dataset with respect to race
distribution. The created HDA-SynChildFaces consists of 1,652 subjects and a
total of 188,832 images, each subject being present at various ages and with
many different intra-subject variations. Subsequently, we evaluates the
performance of various facial recognition systems on the generated database and
compare the results of adults and children at different ages. The study reveals
that children consistently perform worse than adults, on all tested systems,
and the degradation in performance is proportional to age. Additionally, our
study uncovers some biases in the recognition systems, with Asian and Black
subjects and females performing worse than White and Latino Hispanic subjects
and males.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: SSN: Stockwell Scattering Network for SAR Image Change Detection. (arXiv:2304.11404v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11404">http://arxiv.org/abs/2304.11404</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11404] SSN: Stockwell Scattering Network for SAR Image Change Detection](http://arxiv.org/abs/2304.11404) #interpretability</code></li>
<li>Summary: <p>Recently, synthetic aperture radar (SAR) image change detection has become an
interesting yet challenging direction due to the presence of speckle noise.
Although both traditional and modern learning-driven methods attempted to
overcome this challenge, deep convolutional neural networks (DCNNs)-based
methods are still hindered by the lack of interpretability and the requirement
of large computation power. To overcome this drawback, wavelet scattering
network (WSN) and Fourier scattering network (FSN) are proposed. Combining
respective merits of WSN and FSN, we propose Stockwell scattering network (SSN)
based on Stockwell transform which is widely applied against noisy signals and
shows advantageous characteristics in speckle reduction. The proposed SSN
provides noise-resilient feature representation and obtains state-of-art
performance in SAR image change detection as well as high computational
efficiency. Experimental results on three real SAR image datasets demonstrate
the effectiveness of the proposed method.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations. (arXiv:2304.11267v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11267">http://arxiv.org/abs/2304.11267</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11267] Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations](http://arxiv.org/abs/2304.11267) #diffusion</code></li>
<li>Summary: <p>The rapid development and application of foundation models have
revolutionized the field of artificial intelligence. Large diffusion models
have gained significant attention for their ability to generate photorealistic
images and support various tasks. On-device deployment of these models provides
benefits such as lower server costs, offline functionality, and improved user
privacy. However, common large diffusion models have over 1 billion parameters
and pose challenges due to restricted computational and memory resources on
devices. We present a series of implementation optimizations for large
diffusion models that achieve the fastest reported inference latency to-date
(under 12 seconds for Stable Diffusion 1.4 without int8 quantization on Samsung
S23 Ultra for a 512x512 image with 20 iterations) on GPU-equipped mobile
devices. These enhancements broaden the applicability of generative AI and
improve the overall user experience across a wide range of devices.
</p></li>
</ul>

<h3>Title: Fast Diffusion Probabilistic Model Sampling through the lens of Backward Error Analysis. (arXiv:2304.11446v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11446">http://arxiv.org/abs/2304.11446</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11446] Fast Diffusion Probabilistic Model Sampling through the lens of Backward Error Analysis](http://arxiv.org/abs/2304.11446) #diffusion</code></li>
<li>Summary: <p>Denoising diffusion probabilistic models (DDPMs) are a class of powerful
generative models. The past few years have witnessed the great success of DDPMs
in generating high-fidelity samples. A significant limitation of the DDPMs is
the slow sampling procedure. DDPMs generally need hundreds or thousands of
sequential function evaluations (steps) of neural networks to generate a
sample. This paper aims to develop a fast sampling method for DDPMs requiring
much fewer steps while retaining high sample quality. The inference process of
DDPMs approximates solving the corresponding diffusion ordinary differential
equations (diffusion ODEs) in the continuous limit. This work analyzes how the
backward error affects the diffusion ODEs and the sample quality in DDPMs. We
propose fast sampling through the \textbf{Restricting Backward Error schedule
(RBE schedule)} based on dynamically moderating the long-time backward error.
Our method accelerates DDPMs without any further training. Our experiments show
that sampling with an RBE schedule generates high-quality samples within only 8
to 20 function evaluations on various benchmark datasets. We achieved 12.01 FID
in 8 function evaluations on the ImageNet $128\times128$, and a $20\times$
speedup compared with previous baseline samplers.
</p></li>
</ul>

<h3>Title: LaMD: Latent Motion Diffusion for Video Generation. (arXiv:2304.11603v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11603">http://arxiv.org/abs/2304.11603</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11603] LaMD: Latent Motion Diffusion for Video Generation](http://arxiv.org/abs/2304.11603) #diffusion</code></li>
<li>Summary: <p>Generating coherent and natural movement is the key challenge in video
generation. This research proposes to condense video generation into a problem
of motion generation, to improve the expressiveness of motion and make video
generation more manageable. This can be achieved by breaking down the video
generation process into latent motion generation and video reconstruction. We
present a latent motion diffusion (LaMD) framework, which consists of a
motion-decomposed video autoencoder and a diffusion-based motion generator, to
implement this idea. Through careful design, the motion-decomposed video
autoencoder can compress patterns in movement into a concise latent motion
representation. Meanwhile, the diffusion-based motion generator is able to
efficiently generate realistic motion on a continuous latent space under
multi-modal conditions, at a cost that is similar to that of image diffusion
models. Results show that LaMD generates high-quality videos with a wide range
of motions, from stochastic dynamics to highly controllable movements. It
achieves new state-of-the-art performance on benchmark datasets, including
BAIR, Landscape and CATER-GENs, for Image-to-Video (I2V) and
Text-Image-to-Video (TI2V) generation. The source code of LaMD will be made
available soon.
</p></li>
</ul>

<h3>Title: On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation. (arXiv:2304.11328v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11328">http://arxiv.org/abs/2304.11328</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11328] On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation](http://arxiv.org/abs/2304.11328) #diffusion</code></li>
<li>Summary: <p>One popular diffusion-based sampling strategy attempts to solve the reverse
ordinary differential equations (ODEs) effectively. The coefficients of the
obtained ODE solvers are pre-determined by the ODE formulation, the reverse
discrete timesteps, and the employed ODE methods. In this paper, we consider
accelerating several popular ODE-based sampling processes by optimizing certain
coefficients via improved integration approximation (IIA). At each reverse
timestep, we propose to minimize a mean squared error (MSE) function with
respect to certain selected coefficients. The MSE is constructed by applying
the original ODE solver for a set of fine-grained timesteps which in principle
provides a more accurate integration approximation in predicting the next
diffusion hidden state. Given a pre-trained diffusion model, the procedure for
IIA for a particular number of neural functional evaluations (NFEs) only needs
to be conducted once over a batch of samples. The obtained optimal solutions
for those selected coefficients via minimum MSE (MMSE) can be restored and
reused later on to accelerate the sampling process. Extensive experiments on
EDM and DDIM show the IIA technique leads to significant performance gain when
the numbers of NFEs are small.
</p></li>
</ul>

<h3>Title: Conditional Denoising Diffusion for Sequential Recommendation. (arXiv:2304.11433v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11433">http://arxiv.org/abs/2304.11433</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11433] Conditional Denoising Diffusion for Sequential Recommendation](http://arxiv.org/abs/2304.11433) #diffusion</code></li>
<li>Summary: <p>Generative models have attracted significant interest due to their ability to
handle uncertainty by learning the inherent data distributions. However, two
prominent generative models, namely Generative Adversarial Networks (GANs) and
Variational AutoEncoders (VAEs), exhibit challenges that impede achieving
optimal performance in sequential recommendation tasks. Specifically, GANs
suffer from unstable optimization, while VAEs are prone to posterior collapse
and over-smoothed generations. The sparse and noisy nature of sequential
recommendation further exacerbates these issues. In response to these
limitations, we present a conditional denoising diffusion model, which includes
a sequence encoder, a cross-attentive denoising decoder, and a step-wise
diffuser. This approach streamlines the optimization and generation process by
dividing it into easier and tractable steps in a conditional autoregressive
manner. Furthermore, we introduce a novel optimization schema that incorporates
both cross-divergence loss and contrastive loss. This novel training schema
enables the model to generate high-quality sequence/item representations and
meanwhile precluding collapse. We conducted comprehensive experiments on four
benchmark datasets, and the superior performance achieved by our model attests
to its efficacy.
</p></li>
</ul>

<h3>Title: Diffusion Model for GPS Trajectory Generation. (arXiv:2304.11582v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11582">http://arxiv.org/abs/2304.11582</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11582] Diffusion Model for GPS Trajectory Generation](http://arxiv.org/abs/2304.11582) #diffusion</code></li>
<li>Summary: <p>With the deployment of GPS-enabled devices and data acquisition technology,
the massively generated GPS trajectory data provide a core support for
advancing spatial-temporal data mining research. Nonetheless, GPS trajectories
comprise personal geo-location information, rendering inevitable privacy
concerns on plain data. One promising solution to this problem is trajectory
generation, replacing the original data with the generated privacy-free ones.
However, owing to the complex and stochastic behavior of human activities,
generating high-quality trajectories is still in its infancy. To achieve the
objective, we propose a diffusion-based trajectory generation (Diff-Traj)
framework, effectively integrating the generation capability of the diffusion
model and learning from the spatial-temporal features of trajectories.
Specifically, we gradually convert real trajectories to noise through a forward
trajectory noising process. Then, Diff-Traj reconstructs forged trajectories
from the noise by a reverse trajectory denoising process. In addition, we
design a trajectory UNet (Traj-UNet) structure to extract trajectory features
for noise level prediction during the reverse process. Experiments on two
real-world datasets show that Diff-Traj can be intuitively applied to generate
high-quality trajectories while retaining the original distribution.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: eWaSR -- an embedded-compute-ready maritime obstacle detection network. (arXiv:2304.11249v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11249">http://arxiv.org/abs/2304.11249</a></li>
<li>Code URL: <a href="https://github.com/tersekmatija/ewasr">https://github.com/tersekmatija/ewasr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11249] eWaSR -- an embedded-compute-ready maritime obstacle detection network](http://arxiv.org/abs/2304.11249) #transformer</code></li>
<li>Summary: <p>Maritime obstacle detection is critical for safe navigation of autonomous
surface vehicles (ASVs). While the accuracy of image-based detection methods
has advanced substantially, their computational and memory requirements
prohibit deployment on embedded devices. In this paper we analyze the currently
best-performing maritime obstacle detection network WaSR. Based on the analysis
we then propose replacements for the most computationally intensive stages and
propose its embedded-compute-ready variant eWaSR. In particular, the new design
follows the most recent advancements of transformer-based lightweight networks.
eWaSR achieves comparable detection results to state-of-the-art WaSR with only
0.52% F1 score performance drop and outperforms other state-of-the-art
embedded-ready architectures by over 9.74% in F1 score. On a standard GPU,
eWaSR runs 10x faster than the original WaSR (115 FPS vs 11 FPS). Tests on a
real embedded device OAK-D show that, while WaSR cannot run due to memory
restrictions, eWaSR runs comfortably at 5.5 FPS. This makes eWaSR the first
practical embedded-compute-ready maritime obstacle detection network. The
source code and trained eWaSR models are publicly available here:
https://github.com/tersekmatija/eWaSR.
</p></li>
</ul>

<h3>Title: Self-supervised Learning by View Synthesis. (arXiv:2304.11330v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11330">http://arxiv.org/abs/2304.11330</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11330] Self-supervised Learning by View Synthesis](http://arxiv.org/abs/2304.11330) #transformer</code></li>
<li>Summary: <p>We present view-synthesis autoencoders (VSA) in this paper, which is a
self-supervised learning framework designed for vision transformers. Different
from traditional 2D pretraining methods, VSA can be pre-trained with multi-view
data. In each iteration, the input to VSA is one view (or multiple views) of a
3D object and the output is a synthesized image in another target pose. The
decoder of VSA has several cross-attention blocks, which use the source view as
value, source pose as key, and target pose as query. They achieve
cross-attention to synthesize the target view. This simple approach realizes
large-angle view synthesis and learns spatial invariant representation, where
the latter is decent initialization for transformers on downstream tasks, such
as 3D classification on ModelNet40, ShapeNet Core55, and ScanObjectNN. VSA
outperforms existing methods significantly for linear probing and is
competitive for fine-tuning. The code will be made publicly available.
</p></li>
</ul>

<h3>Title: Two Birds, One Stone: A Unified Framework for Joint Learning of Image and Video Style Transfers. (arXiv:2304.11335v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11335">http://arxiv.org/abs/2304.11335</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11335] Two Birds, One Stone: A Unified Framework for Joint Learning of Image and Video Style Transfers](http://arxiv.org/abs/2304.11335) #transformer</code></li>
<li>Summary: <p>Current arbitrary style transfer models are limited to either image or video
domains. In order to achieve satisfying image and video style transfers, two
different models are inevitably required with separate training processes on
image and video domains, respectively. In this paper, we show that this can be
precluded by introducing UniST, a Unified Style Transfer framework for both
images and videos. At the core of UniST is a domain interaction transformer
(DIT), which first explores context information within the specific domain and
then interacts contextualized domain information for joint learning. In
particular, DIT enables exploration of temporal information from videos for the
image style transfer task and meanwhile allows rich appearance texture from
images for video style transfer, thus leading to mutual benefits. Considering
heavy computation of traditional multi-head self-attention, we present a simple
yet effective axial multi-head self-attention (AMSA) for DIT, which improves
computational efficiency while maintains style transfer performance. To verify
the effectiveness of UniST, we conduct extensive experiments on both image and
video style transfer tasks and show that UniST performs favorably against
state-of-the-art approaches on both tasks. Our code and results will be
released.
</p></li>
</ul>

<h3>Title: Incomplete Multimodal Learning for Remote Sensing Data Fusion. (arXiv:2304.11381v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11381">http://arxiv.org/abs/2304.11381</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11381] Incomplete Multimodal Learning for Remote Sensing Data Fusion](http://arxiv.org/abs/2304.11381) #transformer</code></li>
<li>Summary: <p>The mechanism of connecting multimodal signals through self-attention
operation is a key factor in the success of multimodal Transformer networks in
remote sensing data fusion tasks. However, traditional approaches assume access
to all modalities during both training and inference, which can lead to severe
degradation when dealing with modal-incomplete inputs in downstream
applications. To address this limitation, our proposed approach introduces a
novel model for incomplete multimodal learning in the context of remote sensing
data fusion. This approach can be used in both supervised and self-supervised
pretraining paradigms and leverages the additional learned fusion tokens in
combination with Bi-LSTM attention and masked self-attention mechanisms to
collect multimodal signals. The proposed approach employs reconstruction and
contrastive loss to facilitate fusion in pre-training while allowing for random
modality combinations as inputs in network training. Our approach delivers
state-of-the-art performance on two multimodal datasets for tasks such as
building instance / semantic segmentation and land-cover mapping tasks when
dealing with incomplete inputs during inference.
</p></li>
</ul>

<h3>Title: Dilated-UNet: A Fast and Accurate Medical Image Segmentation Approach using a Dilated Transformer and U-Net Architecture. (arXiv:2304.11450v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11450">http://arxiv.org/abs/2304.11450</a></li>
<li>Code URL: <a href="https://github.com/Omid-Nejati/Dilated_Unet">https://github.com/Omid-Nejati/Dilated_Unet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11450] Dilated-UNet: A Fast and Accurate Medical Image Segmentation Approach using a Dilated Transformer and U-Net Architecture](http://arxiv.org/abs/2304.11450) #transformer</code></li>
<li>Summary: <p>Medical image segmentation is crucial for the development of computer-aided
diagnostic and therapeutic systems, but still faces numerous difficulties. In
recent years, the commonly used encoder-decoder architecture based on CNNs has
been applied effectively in medical image segmentation, but has limitations in
terms of learning global context and spatial relationships. Some researchers
have attempted to incorporate transformers into both the decoder and encoder
components, with promising results, but this approach still requires further
improvement due to its high computational complexity. This paper introduces
Dilated-UNet, which combines a Dilated Transformer block with the U-Net
architecture for accurate and fast medical image segmentation. Image patches
are transformed into tokens and fed into the U-shaped encoder-decoder
architecture, with skip-connections for local-global semantic feature learning.
The encoder uses a hierarchical Dilated Transformer with a combination of
Neighborhood Attention and Dilated Neighborhood Attention Transformer to
extract local and sparse global attention. The results of our experiments show
that Dilated-UNet outperforms other models on several challenging medical image
segmentation datasets, such as ISIC and Synapse.
</p></li>
</ul>

<h3>Title: Vision Transformers, a new approach for high-resolution and large-scale mapping of canopy heights. (arXiv:2304.11487v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11487">http://arxiv.org/abs/2304.11487</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11487] Vision Transformers, a new approach for high-resolution and large-scale mapping of canopy heights](http://arxiv.org/abs/2304.11487) #transformer</code></li>
<li>Summary: <p>Accurate and timely monitoring of forest canopy heights is critical for
assessing forest dynamics, biodiversity, carbon sequestration as well as forest
degradation and deforestation. Recent advances in deep learning techniques,
coupled with the vast amount of spaceborne remote sensing data offer an
unprecedented opportunity to map canopy height at high spatial and temporal
resolutions. Current techniques for wall-to-wall canopy height mapping
correlate remotely sensed 2D information from optical and radar sensors to the
vertical structure of trees using LiDAR measurements. While studies using deep
learning algorithms have shown promising performances for the accurate mapping
of canopy heights, they have limitations due to the type of architectures and
loss functions employed. Moreover, mapping canopy heights over tropical forests
remains poorly studied, and the accurate height estimation of tall canopies is
a challenge due to signal saturation from optical and radar sensors, persistent
cloud covers and sometimes the limited penetration capabilities of LiDARs.
Here, we map heights at 10 m resolution across the diverse landscape of Ghana
with a new vision transformer (ViT) model optimized concurrently with a
classification (discrete) and a regression (continuous) loss function. This
model achieves better accuracy than previously used convolutional based
approaches (ConvNets) optimized with only a continuous loss function. The ViT
model results show that our proposed discrete/continuous loss significantly
increases the sensitivity for very tall trees (i.e., > 35m), for which other
approaches show saturation effects. The height maps generated by the ViT also
have better ground sampling distance and better sensitivity to sparse
vegetation in comparison to a convolutional model. Our ViT model has a RMSE of
3.12m in comparison to a reference dataset while the ConvNet model has a RMSE
of 4.3m.
</p></li>
</ul>

<h3>Title: TransFlow: Transformer as Flow Learner. (arXiv:2304.11523v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11523">http://arxiv.org/abs/2304.11523</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11523] TransFlow: Transformer as Flow Learner](http://arxiv.org/abs/2304.11523) #transformer</code></li>
<li>Summary: <p>Optical flow is an indispensable building block for various important
computer vision tasks, including motion estimation, object tracking, and
disparity measurement. In this work, we propose TransFlow, a pure transformer
architecture for optical flow estimation. Compared to dominant CNN-based
methods, TransFlow demonstrates three advantages. First, it provides more
accurate correlation and trustworthy matching in flow estimation by utilizing
spatial self-attention and cross-attention mechanisms between adjacent frames
to effectively capture global dependencies; Second, it recovers more
compromised information (e.g., occlusion and motion blur) in flow estimation
through long-range temporal association in dynamic scenes; Third, it enables a
concise self-learning paradigm and effectively eliminate the complex and
laborious multi-stage pre-training procedures. We achieve the state-of-the-art
results on the Sintel, KITTI-15, as well as several downstream tasks, including
video object detection, interpolation and stabilization. For its efficacy, we
hope TransFlow could serve as a flexible baseline for optical flow estimation.
</p></li>
</ul>

<h3>Title: Transformer-Based LM Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens. (arXiv:2304.11389v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11389">http://arxiv.org/abs/2304.11389</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11389] Transformer-Based LM Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens](http://arxiv.org/abs/2304.11389) #transformer</code></li>
<li>Summary: <p>Recent psycholinguistic studies have drawn conflicting conclusions about the
relationship between the quality of a language model and the ability of its
surprisal estimates to predict human reading times, which has been speculated
to be due to the large gap in both the amount of training data and model
capacity across studies. The current work aims to consolidate these findings by
evaluating surprisal estimates from Transformer-based language model variants
that vary systematically in the amount of training data and model capacity on
their ability to predict human reading times. The results show that surprisal
estimates from most variants with contemporary model capacities provide the
best fit after seeing about two billion training tokens, after which they begin
to diverge from humanlike expectations. Additionally, newly-trained smaller
model variants reveal a 'tipping point' at convergence, after which the
decrease in language model perplexity begins to result in poorer fits to human
reading times. These results suggest that the massive amount of training data
is mainly responsible for the poorer fit achieved by surprisal from larger
pre-trained language models, and that a certain degree of model capacity is
necessary for Transformer-based language models to capture humanlike
expectations.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Fast GraspNeXt: A Fast Self-Attention Neural Network Architecture for Multi-task Learning in Computer Vision Tasks for Robotic Grasping on the Edge. (arXiv:2304.11196v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11196">http://arxiv.org/abs/2304.11196</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11196] Fast GraspNeXt: A Fast Self-Attention Neural Network Architecture for Multi-task Learning in Computer Vision Tasks for Robotic Grasping on the Edge](http://arxiv.org/abs/2304.11196) #generative</code></li>
<li>Summary: <p>Multi-task learning has shown considerable promise for improving the
performance of deep learning-driven vision systems for the purpose of robotic
grasping. However, high architectural and computational complexity can result
in poor suitability for deployment on embedded devices that are typically
leveraged in robotic arms for real-world manufacturing and warehouse
environments. As such, the design of highly efficient multi-task deep neural
network architectures tailored for computer vision tasks for robotic grasping
on the edge is highly desired for widespread adoption in manufacturing
environments. Motivated by this, we propose Fast GraspNeXt, a fast
self-attention neural network architecture tailored for embedded multi-task
learning in computer vision tasks for robotic grasping. To build Fast
GraspNeXt, we leverage a generative network architecture search strategy with a
set of architectural constraints customized to achieve a strong balance between
multi-task learning performance and embedded inference efficiency. Experimental
results on the MetaGraspNet benchmark dataset show that the Fast GraspNeXt
network design achieves the highest performance (average precision (AP),
accuracy, and mean squared error (MSE)) across multiple computer vision tasks
when compared to other efficient multi-task network architecture designs, while
having only 17.8M parameters (about >5x smaller), 259 GFLOPs (as much as >5x
lower) and as much as >3.15x faster on a NVIDIA Jetson TX2 embedded processor.
</p></li>
</ul>

<h3>Title: BiTrackGAN: Cascaded CycleGANs to Constraint Face Aging. (arXiv:2304.11313v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11313">http://arxiv.org/abs/2304.11313</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11313] BiTrackGAN: Cascaded CycleGANs to Constraint Face Aging](http://arxiv.org/abs/2304.11313) #generative</code></li>
<li>Summary: <p>With the increased accuracy of modern computer vision technology, many access
control systems are equipped with face recognition functions for faster
identification. In order to maintain high recognition accuracy, it is necessary
to keep the face database up-to-date. However, it is impractical to collect the
latest facial picture of the system's user through human effort. Thus, we
propose a bottom-up training method for our proposed network to address this
challenge. Essentially, our proposed network is a translation pipeline that
cascades two CycleGAN blocks (a widely used unpaired image-to-image translation
generative adversarial network) called BiTrackGAN. By bottom-up training, it
induces an ideal intermediate state between these two CycleGAN blocks, namely
the constraint mechanism. Experimental results show that BiTrackGAN achieves
more reasonable and diverse cross-age facial synthesis than other
CycleGAN-related methods. As far as we know, it is a novel and effective
constraint mechanism for more reason and accurate aging synthesis through the
CycleGAN approach.
</p></li>
</ul>

<h3>Title: Spectral normalized dual contrastive regularization for image-to-image translation. (arXiv:2304.11319v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11319">http://arxiv.org/abs/2304.11319</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11319] Spectral normalized dual contrastive regularization for image-to-image translation](http://arxiv.org/abs/2304.11319) #generative</code></li>
<li>Summary: <p>Existing image-to-image(I2I) translation methods achieve state-of-the-art
performance by incorporating the patch-wise contrastive learning into
Generative Adversarial Networks. However, patch-wise contrastive learning only
focuses on the local content similarity but neglects the global structure
constraint, which affects the quality of the generated images. In this paper,
we propose a new unpaired I2I translation framework based on dual contrastive
regularization and spectral normalization, namely SN-DCR. To maintain
consistency of the global structure and texture, we design the dual contrastive
regularization using different feature spaces respectively. In order to improve
the global structure information of the generated images, we formulate a
semantically contrastive loss to make the global semantic structure of the
generated images similar to the real images from the target domain in the
semantic feature space. We use Gram Matrices to extract the style of texture
from images. Similarly, we design style contrastive loss to improve the global
texture information of the generated images. Moreover, to enhance the stability
of model, we employ the spectral normalized convolutional network in the design
of our generator. We conduct the comprehensive experiments to evaluate the
effectiveness of SN-DCR, and the results prove that our method achieves SOTA in
multiple tasks.
</p></li>
</ul>

<h3>Title: NaviNeRF: NeRF-based 3D Representation Disentanglement by Latent Semantic Navigation. (arXiv:2304.11342v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11342">http://arxiv.org/abs/2304.11342</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11342] NaviNeRF: NeRF-based 3D Representation Disentanglement by Latent Semantic Navigation](http://arxiv.org/abs/2304.11342) #generative</code></li>
<li>Summary: <p>3D representation disentanglement aims to identify, decompose, and manipulate
the underlying explanatory factors of 3D data, which helps AI fundamentally
understand our 3D world. This task is currently under-explored and poses great
challenges: (i) the 3D representations are complex and in general contains much
more information than 2D image; (ii) many 3D representations are not well
suited for gradient-based optimization, let alone disentanglement. To address
these challenges, we use NeRF as a differentiable 3D representation, and
introduce a self-supervised Navigation to identify interpretable semantic
directions in the latent space. To our best knowledge, this novel method,
dubbed NaviNeRF, is the first work to achieve fine-grained 3D disentanglement
without any priors or supervisions. Specifically, NaviNeRF is built upon the
generative NeRF pipeline, and equipped with an Outer Navigation Branch and an
Inner Refinement Branch. They are complementary -- the outer navigation is to
identify global-view semantic directions, and the inner refinement dedicates to
fine-grained attributes. A synergistic loss is further devised to coordinate
two branches. Extensive experiments demonstrate that NaviNeRF has a superior
fine-grained 3D disentanglement ability than the previous 3D-aware models. Its
performance is also comparable to editing-oriented models relying on semantic
or geometry priors.
</p></li>
</ul>

<h3>Title: Medium. Permeation: SARS-COV-2 Painting Creation by Generative Model. (arXiv:2304.11354v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11354">http://arxiv.org/abs/2304.11354</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11354] Medium](http://arxiv.org/abs/2304.11354) #generative</code></li>
<li>Summary: <p>Airborne particles are the medium for SARS-CoV-2 to invade the human body.
Light also reflects through suspended particles in the air, allowing people to
see a colorful world. Impressionism is the most prominent art school that
explores the spectrum of color created through color reflection of light. We
find similarities of color structure and color stacking in the Impressionist
paintings and the illustrations of the novel coronavirus by artists around the
world. With computerized data analysis through the main tones, the way of color
layout, and the way of color stacking in the paintings of the Impressionists,
we train computers to draw the novel coronavirus in an Impressionist style
using a Generative Adversarial Network to create our artwork "Medium.
Permeation". This artwork is composed of 196 randomly generated viral pictures
arranged in a 14 by 14 matrix to form a large-scale painting. In addition, we
have developed an extended work: Gradual Change, which is presented as video
art. We use Graph Neural Network to present 196 paintings of the new
coronavirus to the audience one by one in a gradual manner. In front of LED TV
screen, audience will find 196 virus paintings whose colors will change
continuously. This large video painting symbolizes that worldwide 196 countries
have been invaded by the epidemic, and every nation continuously pops up mutant
viruses. The speed of vaccine development cannot keep up with the speed of
virus mutation. This is also the first generative art in the world based on the
common features and a metaphorical symbiosis between Impressionist art and the
novel coronavirus. This work warns us of the unprecedented challenges posed by
the SARS-CoV-2, implying that the world should not ignore the invisible enemy
who uses air as a medium.
</p></li>
</ul>

<h3>Title: Learn What NOT to Learn: Towards Generative Safety in Chatbots. (arXiv:2304.11220v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11220">http://arxiv.org/abs/2304.11220</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11220] Learn What NOT to Learn: Towards Generative Safety in Chatbots](http://arxiv.org/abs/2304.11220) #generative</code></li>
<li>Summary: <p>Conversational models that are generative and open-domain are particularly
susceptible to generating unsafe content since they are trained on web-based
social data. Prior approaches to mitigating this issue have drawbacks, such as
disrupting the flow of conversation, limited generalization to unseen toxic
input contexts, and sacrificing the quality of the dialogue for the sake of
safety. In this paper, we present a novel framework, named "LOT" (Learn NOT
to), that employs a contrastive loss to enhance generalization by learning from
both positive and negative training signals. Our approach differs from the
standard contrastive learning framework in that it automatically obtains
positive and negative signals from the safe and unsafe language distributions
that have been learned beforehand. The LOT framework utilizes divergence to
steer the generations away from the unsafe subspace and towards the safe
subspace while sustaining the flow of conversation. Our approach is memory and
time-efficient during decoding and effectively reduces toxicity while
preserving engagingness and fluency. Empirical results indicate that LOT
reduces toxicity by up to four-fold while achieving four to six-fold higher
rates of engagingness and fluency compared to baseline models. Our findings are
further corroborated by human evaluation.
</p></li>
</ul>

<h3>Title: Learning Symbolic Representations Through Joint GEnerative and DIscriminative Training. (arXiv:2304.11357v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11357">http://arxiv.org/abs/2304.11357</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11357] Learning Symbolic Representations Through Joint GEnerative and DIscriminative Training](http://arxiv.org/abs/2304.11357) #generative</code></li>
<li>Summary: <p>We introduce GEDI, a Bayesian framework that combines existing
self-supervised learning objectives with likelihood-based generative models.
This framework leverages the benefits of both GEnerative and DIscriminative
approaches, resulting in improved symbolic representations over standalone
solutions. Additionally, GEDI can be easily integrated and trained jointly with
existing neuro-symbolic frameworks without the need for additional supervision
or costly pre-training steps. We demonstrate through experiments on real-world
data, including SVHN, CIFAR10, and CIFAR100, that GEDI outperforms existing
self-supervised learning strategies in terms of clustering performance by a
significant margin. The symbolic component further allows it to leverage
knowledge in the form of logical constraints to improve performance in the
small data regime.
</p></li>
</ul>

<h3>Title: Physics-guided generative adversarial network to learn physical models. (arXiv:2304.11488v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11488">http://arxiv.org/abs/2304.11488</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11488] Physics-guided generative adversarial network to learn physical models](http://arxiv.org/abs/2304.11488) #generative</code></li>
<li>Summary: <p>This short note describes the concept of guided training of deep neural
networks (DNNs) to learn physically reasonable solutions. DNNs are being widely
used to predict phenomena in physics and mechanics. One of the issues of DNNs
is that their output does not always satisfy physical equations. One approach
to consider physical equations is adding a residual of equations into the loss
function; this is called physics-informed neural network (PINN). One feature of
PINNs is that the physical equations and corresponding residual must be
implemented as part of a neural network model. In addition, the residual does
not always converge to a small value. The proposed model is a physics-guided
generative adversarial network (PG-GAN) that uses a GAN architecture in which
physical equations are used to judge whether the neural network's output is
consistent with physics. The proposed method was applied to a simple problem to
assess its potential usability.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Who's the Best Detective? LLMs vs. MLs in Detecting Incoherent Fourth Grade Math Answers. (arXiv:2304.11257v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11257">http://arxiv.org/abs/2304.11257</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11257] Who's the Best Detective? LLMs vs](http://arxiv.org/abs/2304.11257) #large language model</code></li>
<li>Summary: <p>Written answers to open-ended questions can have a higher long-term effect on
learning than multiple-choice questions. However, it is critical that teachers
immediately review the answers, and ask to redo those that are incoherent. This
can be a difficult task and can be time-consuming for teachers. A possible
solution is to automate the detection of incoherent answers. One option is to
automate the review with Large Language Models (LLM). In this paper, we analyze
the responses of fourth graders in mathematics using three LLMs: GPT-3, BLOOM,
and YOU. We used them with zero, one, two, three and four shots. We compared
their performance with the results of various classifiers trained with Machine
Learning (ML). We found that LLMs perform worse than MLs in detecting
incoherent answers. The difficulty seems to reside in recursive questions that
contain both questions and answers, and in responses from students with typical
fourth-grader misspellings. Upon closer examination, we have found that the
ChatGPT model faces the same challenges.
</p></li>
</ul>

<h3>Title: LaMP: When Large Language Models Meet Personalization. (arXiv:2304.11406v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11406">http://arxiv.org/abs/2304.11406</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11406] LaMP: When Large Language Models Meet Personalization](http://arxiv.org/abs/2304.11406) #large language model</code></li>
<li>Summary: <p>This paper highlights the importance of personalization in the current state
of natural language understanding and generation and introduces the LaMP
benchmark -- a novel benchmark for training and evaluating language models for
producing personalized outputs. LaMP offers a comprehensive evaluation
framework with diverse language tasks and multiple entries for each user
profile. It consists of seven personalized tasks, spanning three classification
and four text generation tasks. We also propose a retrieval augmentation
approach that retrieves personalized items from user profiles to construct
personalized prompts for large language models. Our baseline zero-shot and
fine-tuned model results indicate that LMs utilizing profile augmentation
outperform their counterparts that do not factor in profile information.
</p></li>
</ul>

<h3>Title: Divide and Prompt: Chain of Thought Prompting for Text-to-SQL. (arXiv:2304.11556v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11556">http://arxiv.org/abs/2304.11556</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11556] Divide and Prompt: Chain of Thought Prompting for Text-to-SQL](http://arxiv.org/abs/2304.11556) #large language model</code></li>
<li>Summary: <p>Chain-of-thought (CoT) prompting combined with large language models (LLMs)
have achieved encouraging results on complex reasoning tasks. Text-to-SQL is a
critical semantic parsing task that converts natural language questions into
SQL statements, involving a complex reasoning process. However, there is little
work about using CoT prompting to activate LLM's reasoning capabilities on
Text-to-SQL tasks. In this work, we propose a new paradigm for prompting
Text-to-SQL tasks, called Divide-and-Prompt, which first divides the task into
subtasks, and then approach each subtask through CoT. We present 3
prompting-based methods to enhance the Text-to-SQL ability of LLMs. Experiments
show that these prompts guide LLMs to generate Text-to-SQL with higher
execution accuracy.
</p></li>
</ul>

<h3>Title: Differentiate ChatGPT-generated and Human-written Medical Texts. (arXiv:2304.11567v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11567">http://arxiv.org/abs/2304.11567</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11567] Differentiate ChatGPT-generated and Human-written Medical Texts](http://arxiv.org/abs/2304.11567) #large language model</code></li>
<li>Summary: <p>Background: Large language models such as ChatGPT are capable of generating
grammatically perfect and human-like text content, and a large number of
ChatGPT-generated texts have appeared on the Internet. However, medical texts
such as clinical notes and diagnoses require rigorous validation, and erroneous
medical content generated by ChatGPT could potentially lead to disinformation
that poses significant harm to healthcare and the general public.
</p></li>
</ul>

<p>Objective: This research is among the first studies on responsible and
ethical AIGC (Artificial Intelligence Generated Content) in medicine. We focus
on analyzing the differences between medical texts written by human experts and
generated by ChatGPT, and designing machine learning workflows to effectively
detect and differentiate medical texts generated by ChatGPT.
</p>
<p>Methods: We first construct a suite of datasets containing medical texts
written by human experts and generated by ChatGPT. In the next step, we analyze
the linguistic features of these two types of content and uncover differences
in vocabulary, part-of-speech, dependency, sentiment, perplexity, etc. Finally,
we design and implement machine learning methods to detect medical text
generated by ChatGPT.
</p>
<p>Results: Medical texts written by humans are more concrete, more diverse, and
typically contain more useful information, while medical texts generated by
ChatGPT pay more attention to fluency and logic, and usually express general
terminologies rather than effective information specific to the context of the
problem. A BERT-based model can effectively detect medical texts generated by
ChatGPT, and the F1 exceeds 95%.
</p>

<h3>Title: Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models. (arXiv:2304.11657v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11657">http://arxiv.org/abs/2304.11657</a></li>
<li>Code URL: <a href="https://github.com/gasolsun36/iter-cot">https://github.com/gasolsun36/iter-cot</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11657] Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models](http://arxiv.org/abs/2304.11657) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) can achieve highly effective performance on
various reasoning tasks by incorporating step-by-step chain-of-thought (CoT)
prompting as demonstrations. However, the reasoning chains of demonstrations
generated by LLMs are prone to errors, which can subsequently lead to incorrect
reasoning during inference. Furthermore, inappropriate exemplars (overly
simplistic or complex), can affect overall performance among varying levels of
difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts
Prompting), an iterative bootstrapping approach for selecting exemplars and
generating reasoning chains. By utilizing iterative bootstrapping, our approach
enables LLMs to autonomously rectify errors, resulting in more precise and
comprehensive reasoning chains. Simultaneously, our approach selects
challenging yet answerable questions accompanied by reasoning chains as
exemplars with a moderate level of difficulty, which enhances the LLMs'
generalizability across varying levels of difficulty. Experimental results
indicate that Iter-CoT exhibits superiority, achieving competitive performance
across three distinct reasoning tasks on eleven datasets.
</p></li>
</ul>

<h3>Title: Domain Mastery Benchmark: An Ever-Updating Benchmark for Evaluating Holistic Domain Knowledge of Large Language Model--A Preliminary Release. (arXiv:2304.11679v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11679">http://arxiv.org/abs/2304.11679</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11679] Domain Mastery Benchmark: An Ever-Updating Benchmark for Evaluating Holistic Domain Knowledge of Large Language Model--A Preliminary Release](http://arxiv.org/abs/2304.11679) #large language model</code></li>
<li>Summary: <p>Domain knowledge refers to the in-depth understanding, expertise, and
familiarity with a specific subject, industry, field, or area of special
interest. The existing benchmarks are all lack of an overall design for domain
knowledge evaluation. Holding the belief that the real ability of domain
language understanding can only be fairly evaluated by an comprehensive and
in-depth benchmark, we introduces the Domma, a Domain Mastery Benchmark. DomMa
targets at testing Large Language Models (LLMs) on their domain knowledge
understanding, it features extensive domain coverage, large data volume, and a
continually updated data set based on Chinese 112 first-level subject
classifications. DomMa consist of 100,000 questions in both Chinese and English
sourced from graduate entrance examinations and undergraduate exams in Chinese
college. We have also propose designs to make benchmark and evaluation process
more suitable to LLMs.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: SSS3D: Fast Neural Architecture Search For Efficient Three-Dimensional Semantic Segmentation. (arXiv:2304.11207v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11207">http://arxiv.org/abs/2304.11207</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11207] SSS3D: Fast Neural Architecture Search For Efficient Three-Dimensional Semantic Segmentation](http://arxiv.org/abs/2304.11207) #segmentation</code></li>
<li>Summary: <p>We present SSS3D, a fast multi-objective NAS framework designed to find
computationally efficient 3D semantic scene segmentation networks. It uses
RandLA-Net, an off-the-shelf point-based network, as a super-network to enable
weight sharing and reduce search time by 99.67% for single-stage searches.
SSS3D has a complex search space composed of sampling and architectural
parameters that can form 2.88 * 10^17 possible networks. To further reduce
search time, SSS3D splits the complete search space and introduces a two-stage
search that finds optimal subnetworks in 54% of the time required by
single-stage searches.
</p></li>
</ul>

<h3>Title: Advances in Deep Concealed Scene Understanding. (arXiv:2304.11234v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11234">http://arxiv.org/abs/2304.11234</a></li>
<li>Code URL: <a href="https://github.com/dengpingfan/csu">https://github.com/dengpingfan/csu</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11234] Advances in Deep Concealed Scene Understanding](http://arxiv.org/abs/2304.11234) #segmentation</code></li>
<li>Summary: <p>Concealed scene understanding (CSU) is a hot computer vision topic aiming to
perceive objects with camouflaged properties. The current boom in its advanced
techniques and novel applications makes it timely to provide an up-to-date
survey to enable researchers to understand the global picture of the CSU field,
including both current achievements and major challenges. This paper makes four
contributions: (1) For the first time, we present a comprehensive survey of the
deep learning techniques oriented at CSU, including a background with its
taxonomy, task-unique challenges, and a review of its developments in the deep
learning era via surveying existing datasets and deep techniques. (2) For a
quantitative comparison of the state-of-the-art, we contribute the largest and
latest benchmark for Concealed Object Segmentation (COS). (3) To evaluate the
transferability of deep CSU in practical scenarios, we re-organize the largest
concealed defect segmentation dataset termed CDS2K with the hard cases from
diversified industrial scenarios, on which we construct a comprehensive
benchmark. (4) We discuss open problems and potential research directions for
this community. Our code and datasets are available at
https://github.com/DengPingFan/CSU, which will be updated continuously to watch
and summarize the advancements in this rapidly evolving field.
</p></li>
</ul>

<h3>Title: Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model. (arXiv:2304.11332v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11332">http://arxiv.org/abs/2304.11332</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11332] Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model](http://arxiv.org/abs/2304.11332) #segmentation</code></li>
<li>Summary: <p>The Segment Anything Model (SAM) is a recently developed large model for
general-purpose segmentation for computer vision tasks. SAM was trained using
11 million images with over 1 billion masks and can produce segmentation
results for a wide range of objects in natural scene images. SAM can be viewed
as a general perception model for segmentation (partitioning images into
semantically meaningful regions). Thus, how to utilize such a large foundation
model for medical image segmentation is an emerging research target. This paper
shows that although SAM does not immediately give high-quality segmentation for
medical images, its generated masks, features, and stability scores are useful
for building and training better medical image segmentation models. In
particular, we demonstrate how to use SAM to augment image inputs for a
commonly-used medical image segmentation model (e.g., U-Net). Experiments on
two datasets show the effectiveness of our proposed method.
</p></li>
</ul>

<h3>Title: Single-stage Multi-human Parsing via Point Sets and Center-based Offsets. (arXiv:2304.11356v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11356">http://arxiv.org/abs/2304.11356</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11356] Single-stage Multi-human Parsing via Point Sets and Center-based Offsets](http://arxiv.org/abs/2304.11356) #segmentation</code></li>
<li>Summary: <p>This work studies the multi-human parsing problem. Existing methods, either
following top-down or bottom-up two-stage paradigms, usually involve expensive
computational costs. We instead present a high-performance Single-stage
Multi-human Parsing (SMP) deep architecture that decouples the multi-human
parsing problem into two fine-grained sub-problems, i.e., locating the human
body and parts. SMP leverages the point features in the barycenter positions to
obtain their segmentation and then generates a series of offsets from the
barycenter of the human body to the barycenters of parts, thus performing human
body and parts matching without the grouping process. Within the SMP
architecture, we propose a Refined Feature Retain module to extract the global
feature of instances through generated mask attention and a Mask of Interest
Reclassify module as a trainable plug-in module to refine the classification
results with the predicted segmentation. Extensive experiments on the MHPv2.0
dataset demonstrate the best effectiveness and efficiency of the proposed
method, surpassing the state-of-the-art method by 2.1% in AP50p, 1.0% in
APvolp, and 1.2% in PCP50. In particular, the proposed method requires fewer
training epochs and a less complex model architecture. We will release our
source codes, pretrained models, and online demos to facilitate further
studies.
</p></li>
</ul>

<h3>Title: Knowledge Distillation from 3D to Bird's-Eye-View for LiDAR Semantic Segmentation. (arXiv:2304.11393v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11393">http://arxiv.org/abs/2304.11393</a></li>
<li>Code URL: <a href="https://github.com/fengjiang5/knowledge-distillation-from-cylinder3d-to-polarnet">https://github.com/fengjiang5/knowledge-distillation-from-cylinder3d-to-polarnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11393] Knowledge Distillation from 3D to Bird's-Eye-View for LiDAR Semantic Segmentation](http://arxiv.org/abs/2304.11393) #segmentation</code></li>
<li>Summary: <p>LiDAR point cloud segmentation is one of the most fundamental tasks for
autonomous driving scene understanding. However, it is difficult for existing
models to achieve both high inference speed and accuracy simultaneously. For
example, voxel-based methods perform well in accuracy, while Bird's-Eye-View
(BEV)-based methods can achieve real-time inference. To overcome this issue, we
develop an effective 3D-to-BEV knowledge distillation method that transfers
rich knowledge from 3D voxel-based models to BEV-based models. Our framework
mainly consists of two modules: the voxel-to-pillar distillation module and the
label-weight distillation module. Voxel-to-pillar distillation distills sparse
3D features to BEV features for middle layers to make the BEV-based model aware
of more structural and geometric information. Label-weight distillation helps
the model pay more attention to regions with more height information. Finally,
we conduct experiments on the SemanticKITTI dataset and Paris-Lille-3D. The
results on SemanticKITTI show more than 5% improvement on the test set,
especially for classes such as motorcycle and person, with more than 15%
improvement. The code can be accessed at
https://github.com/fengjiang5/Knowledge-Distillation-from-Cylinder3D-to-PolarNet.
</p></li>
</ul>

<h3>Title: SACANet: scene-aware class attention network for semantic segmentation of remote sensing images. (arXiv:2304.11424v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11424">http://arxiv.org/abs/2304.11424</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11424] SACANet: scene-aware class attention network for semantic segmentation of remote sensing images](http://arxiv.org/abs/2304.11424) #segmentation</code></li>
<li>Summary: <p>Spatial attention mechanism has been widely used in semantic segmentation of
remote sensing images given its capability to model long-range dependencies.
Many methods adopting spatial attention mechanism aggregate contextual
information using direct relationships between pixels within an image, while
ignoring the scene awareness of pixels (i.e., being aware of the global context
of the scene where the pixels are located and perceiving their relative
positions). Given the observation that scene awareness benefits context
modeling with spatial correlations of ground objects, we design a scene-aware
attention module based on a refined spatial attention mechanism embedding scene
awareness. Besides, we present a local-global class attention mechanism to
address the problem that general attention mechanism introduces excessive
background noises while hardly considering the large intra-class variance in
remote sensing images. In this paper, we integrate both scene-aware and class
attentions to propose a scene-aware class attention network (SACANet) for
semantic segmentation of remote sensing images. Experimental results on three
datasets show that SACANet outperforms other state-of-the-art methods and
validate its effectiveness. Code is available at
https://github.com/xwmaxwma/rssegmentation.
</p></li>
</ul>

<h3>Title: Semi-Supervised Semantic Segmentation With Region Relevance. (arXiv:2304.11539v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11539">http://arxiv.org/abs/2304.11539</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11539] Semi-Supervised Semantic Segmentation With Region Relevance](http://arxiv.org/abs/2304.11539) #segmentation</code></li>
<li>Summary: <p>Semi-supervised semantic segmentation aims to learn from a small amount of
labeled data and plenty of unlabeled ones for the segmentation task. The most
common approach is to generate pseudo-labels for unlabeled images to augment
the training data. However, the noisy pseudo-labels will lead to cumulative
classification errors and aggravate the local inconsistency in prediction. This
paper proposes a Region Relevance Network (RRN) to alleviate the problem
mentioned above. Specifically, we first introduce a local pseudo-label
filtering module that leverages discriminator networks to assess the accuracy
of the pseudo-label at the region level. A local selection loss is proposed to
mitigate the negative impact of wrong pseudo-labels in consistency
regularization training. In addition, we propose a dynamic region-loss
correction module, which takes the merit of network diversity to further rate
the reliability of pseudo-labels and correct the convergence direction of the
segmentation network with a dynamic region loss. Extensive experiments are
conducted on PASCAL VOC 2012 and Cityscapes datasets with varying amounts of
labeled data, demonstrating that our proposed approach achieves
state-of-the-art performance compared to current counterparts.
</p></li>
</ul>

<h3>Title: Segment Anything in Non-Euclidean Domains: Challenges and Opportunities. (arXiv:2304.11595v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11595">http://arxiv.org/abs/2304.11595</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11595] Segment Anything in Non-Euclidean Domains: Challenges and Opportunities](http://arxiv.org/abs/2304.11595) #segmentation</code></li>
<li>Summary: <p>The recent work known as Segment Anything (SA) has made significant strides
in pushing the boundaries of semantic segmentation into the era of foundation
models. The impact of SA has sparked extremely active discussions and ushered
in an encouraging new wave of developing foundation models for the diverse
tasks in the Euclidean domain, such as object detection and image inpainting.
Despite the promising advances led by SA, the concept has yet to be extended to
the non-Euclidean graph domain. In this paper, we explore a novel Segment
Non-Euclidean Anything (SNA) paradigm that strives to develop foundation models
that can handle the diverse range of graph data within the non-Euclidean
domain, seeking to expand the scope of SA and lay the groundwork for future
research in this direction. To achieve this goal, we begin by discussing the
recent achievements in foundation models associated with SA. We then shed light
on the unique challenges that arise when applying the SA concept to graph
analysis, which involves understanding the differences between the Euclidean
and non-Euclidean domains from both the data and task perspectives. Motivated
by these observations, we present several preliminary solutions to tackle the
challenges of SNA and detail their corresponding limitations, along with
several potential directions to pave the way for future SNA research.
Experiments on five Open Graph Benchmark (OGB) datasets across various tasks,
including graph property classification and regression, as well as multi-label
prediction, demonstrate that the performance of the naive SNA solutions has
considerable room for improvement, pointing towards a promising avenue for
future exploration of Graph General Intelligence.
</p></li>
</ul>

<h3>Title: PiClick: Picking the desired mask in click-based interactive segmentation. (arXiv:2304.11609v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.11609">http://arxiv.org/abs/2304.11609</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.11609] PiClick: Picking the desired mask in click-based interactive segmentation](http://arxiv.org/abs/2304.11609) #segmentation</code></li>
<li>Summary: <p>Click-based interactive segmentation enables productive pixel-level
annotation and image editing with simple user clicks, whereas target ambiguity
remains a problem hindering precise segmentation. That is, in scenes with rich
context, one click may refer to multiple potential targets residing in
corresponding masks, while most interactive segmentors can only generate one
single mask and fail to capture the rich context. To resolve target ambiguity,
we here propose PiClick to produce semantically diversified masks. PiClick
leverages a transformer network design wherein mutually interactive mask
queries are integrated to infuse target priors. Moreover, a Target Reasoning
Module is designed in PiClick to automatically imply the best-matched mask from
all proposals, significantly relieving target ambiguity as well as extra human
intervention. Extensive experiments conducted on all 9 interactive segmentation
datasets not only demonstrate the state-of-the-art segmentation performance of
PiClick, but also reduces human interventions with multiple proposal generation
and target reasoning. To promote direct usage and future endeavors, we release
the source code of PiClick together with a plug-and-play annotation tool at
https://github.com/cilinyan/PiClick.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
