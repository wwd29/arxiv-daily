<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Is Semantic Communications Secure? A Tale of Multi-Domain Adversarial Attacks. (arXiv:2212.10438v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10438">http://arxiv.org/abs/2212.10438</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10438] Is Semantic Communications Secure? A Tale of Multi-Domain Adversarial Attacks](http://arxiv.org/abs/2212.10438) #secure</code></li>
<li>Summary: <p>Semantic communications seeks to transfer information from a source while
conveying a desired meaning to its destination. We model the
transmitter-receiver functionalities as an autoencoder followed by a task
classifier that evaluates the meaning of the information conveyed to the
receiver. The autoencoder consists of an encoder at the transmitter to jointly
model source coding, channel coding, and modulation, and a decoder at the
receiver to jointly model demodulation, channel decoding and source decoding.
By augmenting the reconstruction loss with a semantic loss, the two deep neural
networks (DNNs) of this encoder-decoder pair are interactively trained with the
DNN of the semantic task classifier. This approach effectively captures the
latent feature space and reliably transfers compressed feature vectors with a
small number of channel uses while keeping the semantic loss low. We identify
the multi-domain security vulnerabilities of using the DNNs for semantic
communications. Based on adversarial machine learning, we introduce test-time
(targeted and non-targeted) adversarial attacks on the DNNs by manipulating
their inputs at different stages of semantic communications. As a computer
vision attack, small perturbations are injected to the images at the input of
the transmitter's encoder. As a wireless attack, small perturbations signals
are transmitted to interfere with the input of the receiver's decoder. By
launching these stealth attacks individually or more effectively in a combined
form as a multi-domain attack, we show that it is possible to change the
semantics of the transferred information even when the reconstruction loss
remains low. These multi-domain adversarial attacks pose as a serious threat to
the semantics of information transfer (with larger impact than conventional
jamming) and raise the need of defense methods for the safe adoption of
semantic communications.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Benchmarking person re-identification datasets and approaches for practical real-world implementations. (arXiv:2212.09981v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09981">http://arxiv.org/abs/2212.09981</a></li>
<li>Code URL: <a href="https://github.com/josemiki/benchmarking_person_re_id">https://github.com/josemiki/benchmarking_person_re_id</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09981] Benchmarking person re-identification datasets and approaches for practical real-world implementations](http://arxiv.org/abs/2212.09981) #security</code></li>
<li>Summary: <p>Recently, Person Re-Identification (Re-ID) has received a lot of attention.
Large datasets containing labeled images of various individuals have been
released, allowing researchers to develop and test many successful approaches.
However, when such Re-ID models are deployed in new cities or environments, the
task of searching for people within a network of security cameras is likely to
face an important domain shift, thus resulting in decreased performance.
Indeed, while most public datasets were collected in a limited geographic area,
images from a new city present different features (e.g., people's ethnicity and
clothing style, weather, architecture, etc.). In addition, the whole frames of
the video streams must be converted into cropped images of people using
pedestrian detection models, which behave differently from the human annotators
who created the dataset used for training. To better understand the extent of
this issue, this paper introduces a complete methodology to evaluate Re-ID
approaches and training datasets with respect to their suitability for
unsupervised deployment for live operations. This method is used to benchmark
four Re-ID approaches on three datasets, providing insight and guidelines that
can help to design better Re-ID pipelines in the future.
</p></li>
</ul>

<h3>Title: Learned Systems Security. (arXiv:2212.10318v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10318">http://arxiv.org/abs/2212.10318</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10318] Learned Systems Security](http://arxiv.org/abs/2212.10318) #security</code></li>
<li>Summary: <p>A learned system uses machine learning (ML) internally to improve
performance. We can expect such systems to be vulnerable to some adversarial-ML
attacks. Often, the learned component is shared between mutually-distrusting
users or processes, much like microarchitectural resources such as caches,
potentially giving rise to highly-realistic attacker models. However, compared
to attacks on other ML-based systems, attackers face a level of indirection as
they cannot interact directly with the learned model. Additionally, the
difference between the attack surface of learned and non-learned versions of
the same system is often subtle. These factors obfuscate the de-facto risks
that the incorporation of ML carries. We analyze the root causes of
potentially-increased attack surface in learned systems and develop a framework
for identifying vulnerabilities that stem from the use of ML. We apply our
framework to a broad set of learned systems under active development. To
empirically validate the many vulnerabilities surfaced by our framework, we
choose 3 of them and implement and evaluate exploits against prominent
learned-system instances. We show that the use of ML caused leakage of past
queries in a database, enabled a poisoning attack that causes exponential
memory blowup in an index structure and crashes it in seconds, and enabled
index users to snoop on each others' key distributions by timing queries over
their own keys. We find that adversarial ML is a universal threat against
learned systems, point to open research gaps in our understanding of
learned-systems security, and conclude by discussing mitigations, while noting
that data leakage is inherent in systems whose learned component is shared
between multiple parties.
</p></li>
</ul>

<h3>Title: A World Full of Privacy and Security (Mis)conceptions? Findings of a Representative Survey in 12 Countries. (arXiv:2212.10382v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10382">http://arxiv.org/abs/2212.10382</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10382] A World Full of Privacy and Security (Mis)conceptions? Findings of a Representative Survey in 12 Countries](http://arxiv.org/abs/2212.10382) #security</code></li>
<li>Summary: <p>Misconceptions about digital security and privacy topics in the general
public frequently lead to insecure behavior. However, little is known about the
prevalence and extent of such misconceptions in a global context. In this work,
we present the results of the first large-scale survey of a global population
on misconceptions: We conducted an online survey with n = 12, 351 participants
in 12 countries on four continents. By investigating influencing factors of
misconceptions around eight common security and privacy topics (including E2EE,
Wi-Fi, VPN, and malware), we find the country of residence to be the strongest
estimate for holding misconceptions. We also identify differences between
non-Western and Western countries, demonstrating the need for region-specific
research on user security knowledge, perceptions, and behavior. While we did
not observe many outright misconceptions, we did identify a lack of
understanding and uncertainty about several fundamental privacy and security
topics.
</p></li>
</ul>

<h3>Title: ThreatKG: A Threat Knowledge Graph for Automated Open-Source Cyber Threat Intelligence Gathering and Management. (arXiv:2212.10388v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10388">http://arxiv.org/abs/2212.10388</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10388] ThreatKG: A Threat Knowledge Graph for Automated Open-Source Cyber Threat Intelligence Gathering and Management](http://arxiv.org/abs/2212.10388) #security</code></li>
<li>Summary: <p>Despite the increased adoption of open-source cyber threat intelligence
(OSCTI) for acquiring knowledge about cyber threats, little effort has been
made to harvest knowledge from a large number of unstructured OSCTI reports
available in the wild (e.g., security articles, threat reports). These reports
provide comprehensive threat knowledge in a variety of entities (e.g., IOCs,
threat actors, TTPs) and relations, which, however, are hard to gather due to
diverse report formats, large report quantities, and complex structures and
nuances in the natural language report text.
</p></li>
</ul>

<p>To bridge the gap, we propose ThreatKG, a system for automated open-source
cyber threat knowledge gathering and management. ThreatKG automatically
collects a large number of OSCTI reports from various sources, extracts
high-fidelity threat knowledge, constructs a threat knowledge graph, and
updates the knowledge graph by continuously ingesting new knowledge. To address
multiple challenges, ThreatKG provides: (1) a hierarchical ontology for
modeling a variety of threat knowledge entities and relations; (2) an accurate
deep learning-based pipeline for threat knowledge extraction; (3) a scalable
and extensible system architecture for threat knowledge graph construction,
persistence, updating, and exploration. Evaluations on a large number of
reports demonstrate the effectiveness of ThreatKG in threat knowledge gathering
and management
</p>

<h2>privacy</h2>
<h3>Title: Efficient aggregation of face embeddings for decentralized face recognition deployments (extended version). (arXiv:2212.10108v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10108">http://arxiv.org/abs/2212.10108</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10108] Efficient aggregation of face embeddings for decentralized face recognition deployments (extended version)](http://arxiv.org/abs/2212.10108) #privacy</code></li>
<li>Summary: <p>Biometrics are one of the most privacy-sensitive data. Ubiquitous
authentication systems with a focus on privacy favor decentralized approaches
as they reduce potential attack vectors, both on a technical and organizational
level. The gold standard is to let the user be in control of where their own
data is stored, which consequently leads to a high variety of devices used.
Moreover, in comparison with a centralized system, designs with higher end-user
freedom often incur additional network overhead. Therefore, when using face
recognition for biometric authentication, an efficient way to compare faces is
important in practical deployments, because it reduces both network and
hardware requirements that are essential to encourage device diversity. This
paper proposes an efficient way to aggregate embeddings used for face
recognition based on an extensive analysis on different datasets and the use of
different aggregation strategies. As part of this analysis, a new dataset has
been collected, which is available for research purposes. Our proposed method
supports the construction of massively scalable, decentralized face recognition
systems with a focus on both privacy and long-term usability.
</p></li>
</ul>

<h3>Title: Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09849">http://arxiv.org/abs/2212.09849</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09849] Dataless Knowledge Fusion by Merging Weights of Language Models](http://arxiv.org/abs/2212.09849) #privacy</code></li>
<li>Summary: <p>Fine-tuning pre-trained language models has become the prevalent paradigm for
building downstream NLP models. Oftentimes fine-tuned models are readily
available but their training data is not, due to data privacy or intellectual
property concerns. This creates a barrier to fusing knowledge across individual
models to yield a better single model. In this paper, we study the problem of
merging individual models built on different training data sets to obtain a
single model that performs well both across all data set domains and can
generalize on out-of-domain data. We propose a dataless knowledge fusion method
that merges models in their parameter space, guided by weights that minimize
prediction differences between the merged model and the individual models. Over
a battery of evaluation settings, we show that the proposed method
significantly outperforms baselines such as Fisher-weighted averaging or model
ensembling. Further, we find that our method is a promising alternative to
multi-task learning that can preserve or sometimes improve over the individual
models without access to the training data. Finally, model merging is more
efficient than training a multi-task model, thus making it applicable to a
wider set of scenarios.
</p></li>
</ul>

<h3>Title: Synthetic Pre-Training Tasks for Neural Machine Translation. (arXiv:2212.09864v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09864">http://arxiv.org/abs/2212.09864</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09864] Synthetic Pre-Training Tasks for Neural Machine Translation](http://arxiv.org/abs/2212.09864) #privacy</code></li>
<li>Summary: <p>Pre-training is an effective technique for ensuring robust performance on a
variety of machine learning tasks. It typically depends on large-scale crawled
corpora that can result in toxic or biased models. Such data can also be
problematic with respect to copyright, attribution, and privacy. Pre-training
with synthetic tasks and data is a promising way of alleviating such concerns
since no real-world information is ingested by the model. Our goal in this
paper is to understand what makes for a good pre-trained model when using
synthetic resources. We answer this question in the context of neural machine
translation by considering two novel approaches to translation model
pre-training. Our first approach studies the effect of pre-training on
obfuscated data derived from a parallel corpus by mapping words to a vocabulary
of 'nonsense' tokens. Our second approach explores the effect of pre-training
on procedurally generated synthetic parallel data that does not depend on any
real human language corpus. Our empirical evaluation on multiple language pairs
shows that, to a surprising degree, the benefits of pre-training can be
realized even with obfuscated or purely synthetic parallel data. In our
analysis, we consider the extent to which obfuscated and synthetic pre-training
techniques can be used to mitigate the issue of hallucinated model toxicity.
</p></li>
</ul>

<h3>Title: PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English. (arXiv:2212.10011v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10011">http://arxiv.org/abs/2212.10011</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10011] PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English](http://arxiv.org/abs/2212.10011) #privacy</code></li>
<li>Summary: <p>Privacy policies provide individuals with information about their rights and
how their personal information is handled. Natural language understanding (NLU)
technologies can support individuals and practitioners to understand better
privacy practices described in lengthy and complex documents. However, existing
efforts that use NLU technologies are limited by processing the language in a
way exclusive to a single task focusing on certain privacy practices. To this
end, we introduce the Privacy Policy Language Understanding Evaluation (PLUE)
benchmark, a multi-task benchmark for evaluating the privacy policy language
understanding across various tasks. We also collect a large corpus of privacy
policies to enable privacy policy domain-specific language model pre-training.
We demonstrate that domain-specific pre-training offers performance
improvements across all tasks. We release the benchmark to encourage future
research in this domain.
</p></li>
</ul>

<h3>Title: Privacy-Preserving Domain Adaptation of Semantic Parsers. (arXiv:2212.10520v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10520">http://arxiv.org/abs/2212.10520</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10520] Privacy-Preserving Domain Adaptation of Semantic Parsers](http://arxiv.org/abs/2212.10520) #privacy</code></li>
<li>Summary: <p>Task-oriented dialogue systems often assist users with personal or
confidential matters. For this reason, the developers of such a system are
generally prohibited from observing actual usage. So how can they know where
the system is failing and needs more training data or new functionality? In
this work, we study ways in which realistic user utterances can be generated
synthetically, to help increase the linguistic and functional coverage of the
system, without compromising the privacy of actual users. To this end, we
propose a two-stage Differentially Private (DP) generation method which first
generates latent semantic parses, and then generates utterances based on the
parses. Our proposed approach improves MAUVE by 3.8$\times$ and parse tree
node-type overlap by 1.4$\times$ relative to current approaches for private
synthetic data generation, improving both on fluency and semantic coverage. We
further validate our approach on a realistic domain adaptation task of adding
new functionality from private user data to a semantic parser, and show gains
of 1.3$\times$ on its accuracy with the new feature.
</p></li>
</ul>

<h3>Title: Continual Mean Estimation Under User-Level Privacy. (arXiv:2212.09980v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09980">http://arxiv.org/abs/2212.09980</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09980] Continual Mean Estimation Under User-Level Privacy](http://arxiv.org/abs/2212.09980) #privacy</code></li>
<li>Summary: <p>We consider the problem of continually releasing an estimate of the
population mean of a stream of samples that is user-level differentially
private (DP). At each time instant, a user contributes a sample, and the users
can arrive in arbitrary order. Until now these requirements of continual
release and user-level privacy were considered in isolation. But, in practice,
both these requirements come together as the users often contribute data
repeatedly and multiple queries are made. We provide an algorithm that outputs
a mean estimate at every time instant $t$ such that the overall release is
user-level $\varepsilon$-DP and has the following error guarantee: Denoting by
$M_t$ the maximum number of samples contributed by a user, as long as
$\tilde{\Omega}(1/\varepsilon)$ users have $M_t/2$ samples each, the error at
time $t$ is $\tilde{O}(1/\sqrt{t}+\sqrt{M}_t/t\varepsilon)$. This is a
universal error guarantee which is valid for all arrival patterns of the users.
Furthermore, it (almost) matches the existing lower bounds for the
single-release setting at all time instants when users have contributed equal
number of samples.
</p></li>
</ul>

<h3>Title: A Differential Approach for Data and Classification Service based Privacy-Preserving Machine Learning Model in Cloud Environment. (arXiv:2212.10177v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10177">http://arxiv.org/abs/2212.10177</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10177] A Differential Approach for Data and Classification Service based Privacy-Preserving Machine Learning Model in Cloud Environment](http://arxiv.org/abs/2212.10177) #privacy</code></li>
<li>Summary: <p>The massive upsurge in computational and storage has driven the local data
and machine learning applications to the cloud environment. The owners may not
fully trust the cloud environment as it is managed by third parties. However,
maintaining privacy while sharing data and the classifier with several
stakeholders is a critical challenge. This paper proposes a novel model based
on differential privacy and machine learning approaches that enable multiple
owners to share their data for utilization and the classifier to render
classification services for users in the cloud environment. To process owners
data and classifier, the model specifies a communication protocol among various
untrustworthy parties. The proposed model also provides a robust mechanism to
preserve the privacy of data and the classifier. The experiments are conducted
for a Naive Bayes classifier over numerous datasets to compute the proposed
model efficiency. The achieved results demonstrate that the proposed model has
high accuracy, precision, recall, and F1-score up to 94%, 95%, 94%, and 94%,
and improvement up to 16.95%, 20.16%, 16.95%, and 23.33%, respectively,
compared with state-of-the-art works.
</p></li>
</ul>

<h3>Title: Asynchronous Distributed Bilevel Optimization. (arXiv:2212.10048v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10048">http://arxiv.org/abs/2212.10048</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10048] Asynchronous Distributed Bilevel Optimization](http://arxiv.org/abs/2212.10048) #privacy</code></li>
<li>Summary: <p>Bilevel optimization plays an essential role in many machine learning tasks,
ranging from hyperparameter optimization to meta-learning. Existing studies on
bilevel optimization, however, focus on either centralized or synchronous
distributed setting. The centralized bilevel optimization approaches require
collecting massive amount of data to a single server, which inevitably incur
significant communication expenses and may give rise to data privacy risks.
Synchronous distributed bilevel optimization algorithms, on the other hand,
often face the straggler problem and will immediately stop working if a few
workers fail to respond. As a remedy, we propose Asynchronous Distributed
Bilevel Optimization (ADBO) algorithm. The proposed ADBO can tackle bilevel
optimization problems with both nonconvex upper-level and lower-level objective
functions, and its convergence is theoretically guaranteed. Furthermore, it is
revealed through theoretic analysis that the iteration complexity of ADBO to
obtain the $\epsilon$-stationary point is upper bounded by
$\mathcal{O}(\frac{1}{{{\epsilon ^2}}})$. Thorough empirical studies on public
datasets have been conducted to elucidate the effectiveness and efficiency of
the proposed ADBO.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Reinforced Clarification Question Generation with Defeasibility Rewards for Disambiguating Social and Moral Situations. (arXiv:2212.10409v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10409">http://arxiv.org/abs/2212.10409</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10409] Reinforced Clarification Question Generation with Defeasibility Rewards for Disambiguating Social and Moral Situations](http://arxiv.org/abs/2212.10409) #protect</code></li>
<li>Summary: <p>Context is vital for commonsense moral reasoning. "Lying to a friend" is
wrong if it is meant to deceive them, but may be morally okay if it is intended
to protect them. Such nuanced but salient contextual information can
potentially flip the moral judgment of an action. Thus, we present
ClarifyDelphi, an interactive system that elicits missing contexts of a moral
situation by generating clarification questions such as "Why did you lie to
your friend?". Our approach is inspired by the observation that questions whose
potential answers lead to diverging moral judgments are the most informative.
We learn to generate questions using Reinforcement Learning, by maximizing the
divergence between moral judgements of hypothetical answers to a question.
Human evaluation shows that our system generates more relevant, informative and
defeasible questions compared to other question generation baselines.
ClarifyDelphi assists informed moral reasoning processes by seeking additional
morally consequential context to disambiguate social and moral situations.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: SoK: Analysis of Root Causes and Defense Strategies for Attacks on Microarchitectural Optimizations. (arXiv:2212.10221v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10221">http://arxiv.org/abs/2212.10221</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10221] SoK: Analysis of Root Causes and Defense Strategies for Attacks on Microarchitectural Optimizations](http://arxiv.org/abs/2212.10221) #defense</code></li>
<li>Summary: <p>Microarchitectural optimizations are expected to play a crucial role in
ensuring performance scalability in future technology nodes. However, recent
attacks have demonstrated that microarchitectural optimizations, which were
assumed to be secure, can be exploited. Moreover, new attacks surface at a
rapid pace limiting the scope of existing defenses. These developments prompt
the need to review microarchitectural optimizations with an emphasis on
security, understand the attack landscape and the potential defense strategies.
</p></li>
</ul>

<p>We analyze timing-based side-channel attacks targeting a diverse set of
microarchitectural optimizations. We provide a framework for analysing
non-transient and transient attacks, which highlights the similarities. We
identify the four root causes of timing-based side-channel attacks:
determinism, sharing, access violation and information flow, through our
systematic analysis. Our key insight is that a subset (or all) of the root
causes are exploited by attacks and eliminating any of the exploited root
causes, in any attack step, is enough to provide protection. Leveraging our
framework, we systematize existing defenses and show that they target these
root causes in the different attack steps.
</p>

<h2>attack</h2>
<h3>Title: A Comprehensive Study and Comparison of the Robustness of 3D Object Detectors Against Adversarial Attacks. (arXiv:2212.10230v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10230">http://arxiv.org/abs/2212.10230</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10230] A Comprehensive Study and Comparison of the Robustness of 3D Object Detectors Against Adversarial Attacks](http://arxiv.org/abs/2212.10230) #attack</code></li>
<li>Summary: <p>Deep learning-based 3D object detectors have made significant progress in
recent years and have been deployed in a wide range of applications. It is
crucial to understand the robustness of detectors against adversarial attacks
when employing detectors in security-critical applications. In this paper, we
make the first attempt to conduct a thorough evaluation and analysis of the
robustness of 3D detectors under adversarial attacks. Specifically, we first
extend three kinds of adversarial attacks to the 3D object detection task to
benchmark the robustness of state-of-the-art 3D object detectors against
attacks on KITTI and Waymo datasets, subsequently followed by the analysis of
the relationship between robustness and properties of detectors. Then, we
explore the transferability of cross-model, cross-task, and cross-data attacks.
We finally conduct comprehensive experiments of defense for 3D detectors,
demonstrating that simple transformations like flipping are of little help in
improving robustness when the strategy of transformation imposed on input point
cloud data is exposed to attackers. Our findings will facilitate investigations
in understanding and defending the adversarial attacks against 3D object
detectors to advance this field.
</p></li>
</ul>

<h3>Title: Defending Against Poisoning Attacks in Open-Domain Question Answering. (arXiv:2212.10002v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10002">http://arxiv.org/abs/2212.10002</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10002] Defending Against Poisoning Attacks in Open-Domain Question Answering](http://arxiv.org/abs/2212.10002) #attack</code></li>
<li>Summary: <p>Recent work in open-domain question answering (ODQA) has shown that
adversarial poisoning of the input contexts can cause large drops in accuracy
for production systems. However, little to no work has proposed methods to
defend against these attacks. To do so, we introduce a new method that uses
query augmentation to search for a diverse set of retrieved passages that could
answer the original question. We integrate these new passages into the model
through the design of a novel confidence method, comparing the predicted answer
to its appearance in the retrieved contexts (what we call Confidence from
Answer Redundancy, e.g. CAR). Together these methods allow for a simple but
effective way to defend against poisoning attacks and provide gains of 5-20%
exact match across varying levels of data poisoning.
</p></li>
</ul>

<h3>Title: Multi-head Uncertainty Inference for Adversarial Attack Detection. (arXiv:2212.10006v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10006">http://arxiv.org/abs/2212.10006</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10006] Multi-head Uncertainty Inference for Adversarial Attack Detection](http://arxiv.org/abs/2212.10006) #attack</code></li>
<li>Summary: <p>Deep neural networks (DNNs) are sensitive and susceptible to tiny
perturbation by adversarial attacks which causes erroneous predictions. Various
methods, including adversarial defense and uncertainty inference (UI), have
been developed in recent years to overcome the adversarial attacks. In this
paper, we propose a multi-head uncertainty inference (MH-UI) framework for
detecting adversarial attack examples. We adopt a multi-head architecture with
multiple prediction heads (i.e., classifiers) to obtain predictions from
different depths in the DNNs and introduce shallow information for the UI.
Using independent heads at different depths, the normalized predictions are
assumed to follow the same Dirichlet distribution, and we estimate distribution
parameter of it by moment matching. Cognitive uncertainty brought by the
adversarial attacks will be reflected and amplified on the distribution.
Experimental results show that the proposed MH-UI framework can outperform all
the referred UI methods in the adversarial attack detection task with different
settings.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Panoptic Lifting for 3D Scene Understanding with Neural Fields. (arXiv:2212.09802v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09802">http://arxiv.org/abs/2212.09802</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09802] Panoptic Lifting for 3D Scene Understanding with Neural Fields](http://arxiv.org/abs/2212.09802) #robust</code></li>
<li>Summary: <p>We propose Panoptic Lifting, a novel approach for learning panoptic 3D
volumetric representations from images of in-the-wild scenes. Once trained, our
model can render color images together with 3D-consistent panoptic segmentation
from novel viewpoints.
</p></li>
</ul>

<p>Unlike existing approaches which use 3D input directly or indirectly, our
method requires only machine-generated 2D panoptic segmentation masks inferred
from a pre-trained network. Our core contribution is a panoptic lifting scheme
based on a neural field representation that generates a unified and multi-view
consistent, 3D panoptic representation of the scene. To account for
inconsistencies of 2D instance identifiers across views, we solve a linear
assignment with a cost based on the model's current predictions and the
machine-generated segmentation masks, thus enabling us to lift 2D instances to
3D in a consistent way. We further propose and ablate contributions that make
our method more robust to noisy, machine-generated labels, including test-time
augmentations for confidence estimates, segment consistency loss, bounded
segmentation fields, and gradient stopping.
</p>
<p>Experimental results validate our approach on the challenging Hypersim,
Replica, and ScanNet datasets, improving by 8.4, 13.8, and 10.6% in scene-level
PQ over state of the art.
</p>

<h3>Title: Robust and Resource-efficient Machine Learning Aided Viewport Prediction in Virtual Reality. (arXiv:2212.09945v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09945">http://arxiv.org/abs/2212.09945</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09945] Robust and Resource-efficient Machine Learning Aided Viewport Prediction in Virtual Reality](http://arxiv.org/abs/2212.09945) #robust</code></li>
<li>Summary: <p>360-degree panoramic videos have gained considerable attention in recent
years due to the rapid development of head-mounted displays (HMDs) and
panoramic cameras. One major problem in streaming panoramic videos is that
panoramic videos are much larger in size compared to traditional ones.
Moreover, the user devices are often in a wireless environment, with limited
battery, computation power, and bandwidth. To reduce resource consumption,
researchers have proposed ways to predict the users' viewports so that only
part of the entire video needs to be transmitted from the server. However, the
robustness of such prediction approaches has been overlooked in the literature:
it is usually assumed that only a few models, pre-trained on past users'
experiences, are applied for prediction to all users. We observe that those
pre-trained models can perform poorly for some users because they might have
drastically different behaviors from the majority, and the pre-trained models
cannot capture the features in unseen videos. In this work, we propose a novel
meta learning based viewport prediction paradigm to alleviate the worst
prediction performance and ensure the robustness of viewport prediction. This
paradigm uses two machine learning models, where the first model predicts the
viewing direction, and the second model predicts the minimum video prefetch
size that can include the actual viewport. We first train two meta models so
that they are sensitive to new training data, and then quickly adapt them to
users while they are watching the videos. Evaluation results reveal that the
meta models can adapt quickly to each user, and can significantly increase the
prediction accuracy, especially for the worst-performing predictions.
</p></li>
</ul>

<h3>Title: Domain Generalization with Correlated Style Uncertainty. (arXiv:2212.09950v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09950">http://arxiv.org/abs/2212.09950</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09950] Domain Generalization with Correlated Style Uncertainty](http://arxiv.org/abs/2212.09950) #robust</code></li>
<li>Summary: <p>Though impressive success has been witnessed in computer vision, deep
learning still suffers from the domain shift challenge when the target domain
for testing and the source domain for training do not share an identical
distribution. To address this, domain generalization approaches intend to
extract domain invariant features that can lead to a more robust model. Hence,
increasing the source domain diversity is a key component of domain
generalization. Style augmentation takes advantage of instance-specific feature
statistics containing informative style characteristics to synthetic novel
domains. However, all previous works ignored the correlation between different
feature channels or only limited the style augmentation through linear
interpolation. In this work, we propose a novel augmentation method, called
\textit{Correlated Style Uncertainty (CSU)}, to go beyond the linear
interpolation of style statistic space while preserving the essential
correlation information. We validate our method's effectiveness by extensive
experiments on multiple cross-domain classification tasks, including widely
used PACS, Office-Home, Camelyon17 datasets and the Duke-Market1501 instance
retrieval task and obtained significant margin improvements over the
state-of-the-art methods. The source code is available for public use.
</p></li>
</ul>

<h3>Title: VoronoiPatches: Evaluating A New Data Augmentation Method. (arXiv:2212.10054v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10054">http://arxiv.org/abs/2212.10054</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10054] VoronoiPatches: Evaluating A New Data Augmentation Method](http://arxiv.org/abs/2212.10054) #robust</code></li>
<li>Summary: <p>Overfitting is a problem in Convolutional Neural Networks (CNN) that causes
poor generalization of models on unseen data. To remediate this problem, many
new and diverse data augmentation methods (DA) have been proposed to supplement
or generate more training data, and thereby increase its quality. In this work,
we propose a new data augmentation algorithm: VoronoiPatches (VP). We primarily
utilize non-linear recombination of information within an image, fragmenting
and occluding small information patches. Unlike other DA methods, VP uses small
convex polygon-shaped patches in a random layout to transport information
around within an image. Sudden transitions created between patches and the
original image can, optionally, be smoothed. In our experiments, VP
outperformed current DA methods regarding model variance and overfitting
tendencies. We demonstrate data augmentation utilizing non-linear
re-combination of information within images, and non-orthogonal shapes and
structures improves CNN model robustness on unseen data.
</p></li>
</ul>

<h3>Title: Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection. (arXiv:2212.10147v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10147">http://arxiv.org/abs/2212.10147</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10147] Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection](http://arxiv.org/abs/2212.10147) #robust</code></li>
<li>Summary: <p>Scaling object taxonomies is one of the important steps toward a robust
real-world deployment of recognition systems. We have faced remarkable progress
in images since the introduction of the LVIS benchmark. To continue this
success in videos, a new video benchmark, TAO, was recently presented. Given
the recent encouraging results from both detection and tracking communities, we
are interested in marrying those two advances and building a strong large
vocabulary video tracker. However, supervisions in LVIS and TAO are inherently
sparse or even missing, posing two new challenges for training the large
vocabulary trackers. First, no tracking supervisions are in LVIS, which leads
to inconsistent learning of detection (with LVIS and TAO) and tracking (only
with TAO). Second, the detection supervisions in TAO are partial, which results
in catastrophic forgetting of absent LVIS categories during video fine-tuning.
To resolve these challenges, we present a simple but effective learning
framework that takes full advantage of all available training data to learn
detection and tracking while not losing any LVIS categories to recognize. With
this new learning scheme, we show that consistent improvements of various large
vocabulary trackers are capable, setting strong baseline results on the
challenging TAO benchmarks.
</p></li>
</ul>

<h3>Title: Tracking by Associating Clips. (arXiv:2212.10149v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10149">http://arxiv.org/abs/2212.10149</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10149] Tracking by Associating Clips](http://arxiv.org/abs/2212.10149) #robust</code></li>
<li>Summary: <p>The tracking-by-detection paradigm today has become the dominant method for
multi-object tracking and works by detecting objects in each frame and then
performing data association across frames. However, its sequential frame-wise
matching property fundamentally suffers from the intermediate interruptions in
a video, such as object occlusions, fast camera movements, and abrupt light
changes. Moreover, it typically overlooks temporal information beyond the two
frames for matching. In this paper, we investigate an alternative by treating
object association as clip-wise matching. Our new perspective views a single
long video sequence as multiple short clips, and then the tracking is performed
both within and between the clips. The benefits of this new approach are two
folds. First, our method is robust to tracking error accumulation or
propagation, as the video chunking allows bypassing the interrupted frames, and
the short clip tracking avoids the conventional error-prone long-term track
memory management. Second, the multiple frame information is aggregated during
the clip-wise matching, resulting in a more accurate long-range track
association than the current frame-wise matching. Given the state-of-the-art
tracking-by-detection tracker, QDTrack, we showcase how the tracking
performance improves with our new tracking formulation. We evaluate our
proposals on two tracking benchmarks, TAO and MOT17 that have complementary
characteristics and challenges each other.
</p></li>
</ul>

<h3>Title: Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training. (arXiv:2212.09897v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09897">http://arxiv.org/abs/2212.09897</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09897] Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training](http://arxiv.org/abs/2212.09897) #robust</code></li>
<li>Summary: <p>Language tasks involving character-level manipulations (e.g., spelling
correction, many word games) are challenging for models based in subword
tokenization. To address this, we adapt the interchange intervention training
method of Geiger et al. (2021) to operate on type-level variables over
characters. This allows us to encode robust, position-independent
character-level information in the internal representations of subword-based
models. We additionally introduce a suite of character-level tasks that
systematically vary in their dependence on meaning and sequence-level context.
While simple character-level tokenization approaches still perform best on
purely form-based tasks like string reversal, our method is superior for more
complex tasks that blend form, meaning, and context, such as spelling
correction in context and word search games. Our approach also leads to
subword-based models with human-intepretable internal representations of
characters.
</p></li>
</ul>

<h3>Title: Improving the Robustness of Summarization Models by Detecting and Removing Input Noise. (arXiv:2212.09928v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09928">http://arxiv.org/abs/2212.09928</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09928] Improving the Robustness of Summarization Models by Detecting and Removing Input Noise](http://arxiv.org/abs/2212.09928) #robust</code></li>
<li>Summary: <p>The evaluation of abstractive summarization models typically uses test data
that is identically distributed as training data. In real-world practice,
documents to be summarized may contain input noise caused by text extraction
artifacts or data pipeline bugs. The robustness of model performance under
distribution shift caused by such noise is relatively under-studied. We present
a large empirical study quantifying the sometimes severe loss in performance
(up to 12 ROUGE-1 points) from different types of input noise for a range of
datasets and model sizes. We then propose a light-weight method for detecting
and removing such noise in the input during model inference without requiring
any extra training, auxiliary models, or even prior knowledge of the type of
noise. Our proposed approach effectively mitigates the loss in performance,
recovering a large fraction of the performance drop, sometimes as large as 11
ROUGE-1 points.
</p></li>
</ul>

<h3>Title: Dialog2API: Task-Oriented Dialogue with API Description and Example Programs. (arXiv:2212.09946v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09946">http://arxiv.org/abs/2212.09946</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09946] Dialog2API: Task-Oriented Dialogue with API Description and Example Programs](http://arxiv.org/abs/2212.09946) #robust</code></li>
<li>Summary: <p>Functionality and dialogue experience are two important factors of
task-oriented dialogue systems. Conventional approaches with closed schema
(e.g., conversational semantic parsing) often fail as both the functionality
and dialogue experience are strongly constrained by the underlying schema. We
introduce a new paradigm for task-oriented dialogue - Dialog2API - to greatly
expand the functionality and provide seamless dialogue experience. The
conversational model interacts with the environment by generating and executing
programs triggering a set of pre-defined APIs. The model also manages the
dialogue policy and interact with the user through generating appropriate
natural language responses. By allowing generating free-form programs,
Dialog2API supports composite goals by combining different APIs, whereas
unrestricted program revision provides natural and robust dialogue experience.
To facilitate Dialog2API, the core model is provided with API documents, an
execution environment and optionally some example dialogues annotated with
programs. We propose an approach tailored for the Dialog2API, where the
dialogue states are represented by a stack of programs, with most recently
mentioned program on the top of the stack. Dialog2API can work with many
application scenarios such as software automation and customer service. In this
paper, we construct a dataset for AWS S3 APIs and present evaluation results of
in-context learning baselines.
</p></li>
</ul>

<h3>Title: Towards Robustness of Text-to-SQL Models Against Natural and Realistic Adversarial Table Perturbation. (arXiv:2212.09994v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09994">http://arxiv.org/abs/2212.09994</a></li>
<li>Code URL: <a href="https://github.com/microsoft/ContextualSP">https://github.com/microsoft/ContextualSP</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09994] Towards Robustness of Text-to-SQL Models Against Natural and Realistic Adversarial Table Perturbation](http://arxiv.org/abs/2212.09994) #robust</code></li>
<li>Summary: <p>The robustness of Text-to-SQL parsers against adversarial perturbations plays
a crucial role in delivering highly reliable applications. Previous studies
along this line primarily focused on perturbations in the natural language
question side, neglecting the variability of tables. Motivated by this, we
propose the Adversarial Table Perturbation (ATP) as a new attacking paradigm to
measure the robustness of Text-to-SQL models. Following this proposition, we
curate ADVETA, the first robustness evaluation benchmark featuring natural and
realistic ATPs. All tested state-of-the-art models experience dramatic
performance drops on ADVETA, revealing models' vulnerability in real-world
practices. To defend against ATP, we build a systematic adversarial training
example generation framework tailored for better contextualization of tabular
data. Experiments show that our approach not only brings the best robustness
improvement against table-side perturbations but also substantially empowers
models against NL-side perturbations. We release our benchmark and code at:
https://github.com/microsoft/ContextualSP.
</p></li>
</ul>

<h3>Title: On the Blind Spots of Model-Based Evaluation Metrics for Text Generation. (arXiv:2212.10020v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10020">http://arxiv.org/abs/2212.10020</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10020] On the Blind Spots of Model-Based Evaluation Metrics for Text Generation](http://arxiv.org/abs/2212.10020) #robust</code></li>
<li>Summary: <p>In this work, we explore a useful but often neglected methodology for
robustness analysis of text generation evaluation metrics: stress tests with
synthetic data. Basically, we design and synthesize a wide range of potential
errors and check whether they result in a commensurate drop in the metric
scores. We examine a range of recently proposed evaluation metrics based on
pretrained language models, for the tasks of open-ended generation,
translation, and summarization. Our experiments reveal interesting
insensitivities, biases, or even loopholes in existing metrics. For example, we
find that BERTScore ignores truncation errors in summarization, and MAUVE
(built on top of GPT-2) is insensitive to errors at the beginning of
generations. Further, we investigate the reasons behind these blind spots and
suggest practical workarounds for a more reliable evaluation of text
generation.
</p></li>
</ul>

<h3>Title: Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions. (arXiv:2212.10189v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10189">http://arxiv.org/abs/2212.10189</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10189] Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions](http://arxiv.org/abs/2212.10189) #robust</code></li>
<li>Summary: <p>When answering natural language questions over knowledge bases (KBs),
incompleteness in the KB can naturally lead to many questions being
unanswerable. While answerability has been explored in other QA settings, it
has not been studied for QA over knowledge bases (KBQA). We first identify
various forms of KB incompleteness that can result in a question being
unanswerable. We then propose GrailQAbility, a new benchmark dataset, which
systematically modifies GrailQA (a popular KBQA dataset) to represent all these
incompleteness issues. Testing two state-of-the-art KBQA models (trained on
original GrailQA as well as our GrailQAbility), we find that both models
struggle to detect unanswerable questions, or sometimes detect them for the
wrong reasons. Consequently, both models suffer significant loss in
performance, underscoring the need for further research in making KBQA systems
robust to unanswerability.
</p></li>
</ul>

<h3>Title: In and Out-of-Domain Text Adversarial Robustness via Label Smoothing. (arXiv:2212.10258v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10258">http://arxiv.org/abs/2212.10258</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10258] In and Out-of-Domain Text Adversarial Robustness via Label Smoothing](http://arxiv.org/abs/2212.10258) #robust</code></li>
<li>Summary: <p>Recently it has been shown that state-of-the-art NLP models are vulnerable to
adversarial attacks, where the predictions of a model can be drastically
altered by slight modifications to the input (such as synonym substitutions).
While several defense techniques have been proposed, and adapted, to the
discrete nature of text adversarial attacks, the benefits of general-purpose
regularization methods such as label smoothing for language models, have not
been studied. In this paper, we study the adversarial robustness provided by
various label smoothing strategies in foundational models for diverse NLP tasks
in both in-domain and out-of-domain settings. Our experiments show that label
smoothing significantly improves adversarial robustness in pre-trained models
like BERT, against various popular attacks. We also analyze the relationship
between prediction confidence and robustness, showing that label smoothing
reduces over-confident errors on adversarial examples.
</p></li>
</ul>

<h3>Title: ReCode: Robustness Evaluation of Code Generation Models. (arXiv:2212.10264v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10264">http://arxiv.org/abs/2212.10264</a></li>
<li>Code URL: <a href="https://github.com/amazon-science/recode">https://github.com/amazon-science/recode</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10264] ReCode: Robustness Evaluation of Code Generation Models](http://arxiv.org/abs/2212.10264) #robust</code></li>
<li>Summary: <p>Code generation models have achieved impressive performance. However, they
tend to be brittle as slight edits to a prompt could lead to very different
generations; these robustness properties, critical for user experience when
deployed in real-life applications, are not well understood. Most existing
works on robustness in text or code tasks have focused on classification, while
robustness in generation tasks is an uncharted area and to date there is no
comprehensive benchmark for robustness in code generation. In this paper, we
propose ReCode, a comprehensive robustness evaluation benchmark for code
generation models. We customize over 30 transformations specifically for code
on docstrings, function and variable names, code syntax, and code format. They
are carefully designed to be natural in real-life coding practice, preserve the
original semantic meaning, and thus provide multifaceted assessments of a
model's robustness performance. With human annotators, we verified that over
90% of the perturbed prompts do not alter the semantic meaning of the original
prompt. In addition, we define robustness metrics for code generation models
considering the worst-case behavior under each type of perturbation, taking
advantage of the fact that executing the generated code can serve as objective
evaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well
as function completion tasks derived from them. Interesting observations
include: better robustness for CodeGen over InCoder and GPT-J; models are most
sensitive to syntax perturbations; more challenging robustness evaluation on
MBPP over HumanEval.
</p></li>
</ul>

<h3>Title: To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering. (arXiv:2212.10381v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10381">http://arxiv.org/abs/2212.10381</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10381] To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering](http://arxiv.org/abs/2212.10381) #robust</code></li>
<li>Summary: <p>Recent advances in open-domain question answering (ODQA) have demonstrated
impressive accuracy on standard Wikipedia style benchmarks. However, it is less
clear how robust these models are and how well they perform when applied to
real-world applications in drastically different domains. While there has been
some work investigating how well ODQA models perform when tested for
out-of-domain (OOD) generalization, these studies have been conducted only
under conservative shifts in data distribution and typically focus on a single
component (ie. retrieval) rather than an end-to-end system. In response, we
propose a more realistic and challenging domain shift evaluation setting and,
through extensive experiments, study end-to-end model performance. We find that
not only do models fail to generalize, but high retrieval scores often still
yield poor answer prediction accuracy. We then categorize different types of
shifts and propose techniques that, when presented with a new dataset, predict
if intervention methods are likely to be successful. Finally, using insights
from this analysis, we propose and evaluate several intervention methods which
improve end-to-end answer F1 score by up to 24 points.
</p></li>
</ul>

<h3>Title: TeSS: Zero-Shot Classification via Textual Similarity Comparison with Prompting using Sentence Encoder. (arXiv:2212.10391v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10391">http://arxiv.org/abs/2212.10391</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10391] TeSS: Zero-Shot Classification via Textual Similarity Comparison with Prompting using Sentence Encoder](http://arxiv.org/abs/2212.10391) #robust</code></li>
<li>Summary: <p>We introduce TeSS (Text Similarity Comparison using Sentence Encoder), a
framework for zero-shot classification where the assigned label is determined
by the embedding similarity between the input text and each candidate label
prompt. We leverage representations from sentence encoders optimized to locate
semantically similar samples closer to each other in embedding space during
pre-training. The label prompt embeddings serve as prototypes of their
corresponding class clusters. Furthermore, to compensate for the potentially
poorly descriptive labels in their original format, we retrieve semantically
similar sentences from external corpora and additionally use them with the
original label prompt (TeSS-R). TeSS outperforms strong baselines on various
closed-set and open-set classification datasets under zero-shot setting, with
further gains when combined with label prompt diversification through
retrieval. These results are robustly attained to verbalizer variations, an
ancillary benefit of using a bi-encoder. Altogether, our method serves as a
reliable baseline for zero-shot classification and a simple interface to assess
the quality of sentence encoders.
</p></li>
</ul>

<h3>Title: HYRR: Hybrid Infused Reranking for Passage Retrieval. (arXiv:2212.10528v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10528">http://arxiv.org/abs/2212.10528</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10528] HYRR: Hybrid Infused Reranking for Passage Retrieval](http://arxiv.org/abs/2212.10528) #robust</code></li>
<li>Summary: <p>We present Hybrid Infused Reranking for Passages Retrieval (HYRR), a
framework for training rerankers based on a hybrid of BM25 and neural retrieval
models. Retrievers based on hybrid models have been shown to outperform both
BM25 and neural models alone. Our approach exploits this improved performance
when training a reranker, leading to a robust reranking model. The reranker, a
cross-attention neural model, is shown to be robust to different first-stage
retrieval systems, achieving better performance than rerankers simply trained
upon the first-stage retrievers in the multi-stage systems. We present
evaluations on a supervised passage retrieval task using MS MARCO and zero-shot
retrieval tasks using BEIR. The empirical results show strong performance on
both evaluations.
</p></li>
</ul>

<h3>Title: DISCO: Distilling Phrasal Counterfactuals with Large Language Models. (arXiv:2212.10534v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10534">http://arxiv.org/abs/2212.10534</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10534] DISCO: Distilling Phrasal Counterfactuals with Large Language Models](http://arxiv.org/abs/2212.10534) #robust</code></li>
<li>Summary: <p>Recent methods demonstrate that data augmentation using counterfactual
knowledge can teach models the causal structure of a task, leading to robust
and generalizable models. However, such counterfactual data often has a limited
scale and diversity if crowdsourced and is computationally expensive to extend
to new perturbation types if generated using supervised methods. To address
this, we introduce a new framework called DISCO for automatically generating
high-quality counterfactual data at scale. DISCO engineers prompts to generate
phrasal perturbations with a large general language model. Then, a
task-specific teacher model filters the generation to distill high-quality
counterfactual data. We show that learning with this counterfactual data yields
a comparatively small student model that is 6% (absolute) more robust and
generalizes 5% better across distributions than baselines on various
challenging evaluations. This model is also 15% more sensitive in
differentiating original and counterfactual examples, on three evaluation sets
written by human workers and via human-AI collaboration.
</p></li>
</ul>

<h3>Title: Distributional Robustness Bounds Generalization Errors. (arXiv:2212.09962v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09962">http://arxiv.org/abs/2212.09962</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09962] Distributional Robustness Bounds Generalization Errors](http://arxiv.org/abs/2212.09962) #robust</code></li>
<li>Summary: <p>Bayesian methods, distributionally robust optimization methods, and
regularization methods are three pillars of trustworthy machine learning
hedging against distributional uncertainty, e.g., the uncertainty of an
empirical distribution compared to the true underlying distribution. This paper
investigates the connections among the three frameworks and, in particular,
explores why these frameworks tend to have smaller generalization errors.
Specifically, first, we suggest a quantitative definition for "distributional
robustness", propose the concept of "robustness measure", and formalize several
philosophical concepts in distributionally robust optimization. Second, we show
that Bayesian methods are distributionally robust in the probably approximately
correct (PAC) sense; In addition, by constructing a Dirichlet-process-like
prior in Bayesian nonparametrics, it can be proven that any regularized
empirical risk minimization method is equivalent to a Bayesian method. Third,
we show that generalization errors of machine learning models can be
characterized using the distributional uncertainty of the nominal distribution
and the robustness measures of these machine learning models, which is a new
perspective to bound generalization errors, and therefore, explain the reason
why distributionally robust machine learning models, Bayesian models, and
regularization models tend to have smaller generalization errors.
</p></li>
</ul>

<h3>Title: Walking Noise: Understanding Implications of Noisy Computations on Classification Tasks. (arXiv:2212.10430v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10430">http://arxiv.org/abs/2212.10430</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10430] Walking Noise: Understanding Implications of Noisy Computations on Classification Tasks](http://arxiv.org/abs/2212.10430) #robust</code></li>
<li>Summary: <p>Machine learning methods like neural networks are extremely successful and
popular in a variety of applications, however, they come at substantial
computational costs, accompanied by high energy demands. In contrast, hardware
capabilities are limited and there is evidence that technology scaling is
stuttering, therefore, new approaches to meet the performance demands of
increasingly complex model architectures are required. As an unsafe
optimization, noisy computations are more energy efficient, and given a fixed
power budget also more time efficient. However, any kind of unsafe optimization
requires counter measures to ensure functionally correct results.
</p></li>
</ul>

<p>This work considers noisy computations in an abstract form, and gears to
understand the implications of such noise on the accuracy of
neural-network-based classifiers as an exemplary workload. We propose a
methodology called "Walking Noise" that allows to assess the robustness of
different layers of deep architectures by means of a so-called "midpoint noise
level" metric. We then investigate the implications of additive and
multiplicative noise for different classification tasks and model
architectures, with and without batch normalization. While noisy training
significantly increases robustness for both noise types, we observe a clear
trend to increase weights and thus increase the signal-to-noise ratio for
additive noise injection. For the multiplicative case, we find that some
networks, with suitably simple tasks, automatically learn an internal binary
representation, hence becoming extremely robust. Overall this work proposes a
method to measure the layer-specific robustness and shares first insights on
how networks learn to compensate injected noise, and thus, contributes to
understand robustness against noisy computations.
</p>

<h3>Title: On the Convergence of Policy Gradient in Robust MDPs. (arXiv:2212.10439v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10439">http://arxiv.org/abs/2212.10439</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10439] On the Convergence of Policy Gradient in Robust MDPs](http://arxiv.org/abs/2212.10439) #robust</code></li>
<li>Summary: <p>Robust Markov decision processes (RMDPs) are promising models that provide
reliable policies under ambiguities in model parameters. As opposed to nominal
Markov decision processes (MDPs), however, the state-of-the-art solution
methods for RMDPs are limited to value-based methods, such as value iteration
and policy iteration. This paper proposes Double-Loop Robust Policy Gradient
(DRPG), the first generic policy gradient method for RMDPs with a global
convergence guarantee in tabular problems. Unlike value-based methods, DRPG
does not rely on dynamic programming techniques. In particular, the inner-loop
robust policy evaluation problem is solved via projected gradient descent.
Finally, our experimental results demonstrate the performance of our algorithm
and verify our theoretical guarantees.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h3>Title: Flareon: Stealthy any2any Backdoor Injection via Poisoned Augmentation. (arXiv:2212.09979v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09979">http://arxiv.org/abs/2212.09979</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09979] Flareon: Stealthy any2any Backdoor Injection via Poisoned Augmentation](http://arxiv.org/abs/2212.09979) #steal</code></li>
<li>Summary: <p>Open software supply chain attacks, once successful, can exact heavy costs in
mission-critical applications. As open-source ecosystems for deep learning
flourish and become increasingly universal, they present attackers previously
unexplored avenues to code-inject malicious backdoors in deep neural network
models. This paper proposes Flareon, a small, stealthy, seemingly harmless code
modification that specifically targets the data augmentation pipeline with
motion-based triggers. Flareon neither alters ground-truth labels, nor modifies
the training loss objective, nor does it assume prior knowledge of the victim
model architecture, training data, and training hyperparameters. Yet, it has a
surprisingly large ramification on training -- models trained under Flareon
learn powerful target-conditional (or "any2any") backdoors. The resulting
models can exhibit high attack success rates for any target choices and better
clean accuracies than backdoor attacks that not only seize greater control, but
also assume more restrictive attack capabilities. We also demonstrate the
effectiveness of Flareon against recent defenses. Flareon is fully open-source
and available online to the deep learning community:
https://github.com/lafeat/flareon.
</p></li>
</ul>

<h2>extraction</h2>
<h3>Title: Eff-3DPSeg: 3D organ-level plant shoot segmentation using annotation-efficient point clouds. (arXiv:2212.10263v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10263">http://arxiv.org/abs/2212.10263</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10263] Eff-3DPSeg: 3D organ-level plant shoot segmentation using annotation-efficient point clouds](http://arxiv.org/abs/2212.10263) #extraction</code></li>
<li>Summary: <p>Reliable and automated 3D plant shoot segmentation is a core prerequisite for
the extraction of plant phenotypic traits at the organ level. Combining deep
learning and point clouds can provide effective ways to address the challenge.
However, fully supervised deep learning methods require datasets to be
point-wise annotated, which is extremely expensive and time-consuming. In our
work, we proposed a novel weakly supervised framework, Eff-3DPSeg, for 3D plant
shoot segmentation. First, high-resolution point clouds of soybean were
reconstructed using a low-cost photogrammetry system, and the Meshlab-based
Plant Annotator was developed for plant point cloud annotation. Second, a
weakly-supervised deep learning method was proposed for plant organ
segmentation. The method contained: (1) Pretraining a self-supervised network
using Viewpoint Bottleneck loss to learn meaningful intrinsic structure
representation from the raw point clouds; (2) Fine-tuning the pre-trained model
with about only 0.5% points being annotated to implement plant organ
segmentation. After, three phenotypic traits (stem diameter, leaf width, and
leaf length) were extracted. To test the generality of the proposed method, the
public dataset Pheno4D was included in this study. Experimental results showed
that the weakly-supervised network obtained similar segmentation performance
compared with the fully-supervised setting. Our method achieved 95.1%, 96.6%,
95.8% and 92.2% in the Precision, Recall, F1-score, and mIoU for stem leaf
segmentation and 53%, 62.8% and 70.3% in the AP, AP@25, and AP@50 for leaf
instance segmentation. This study provides an effective way for characterizing
3D plant architecture, which will become useful for plant breeders to enhance
selection processes.
</p></li>
</ul>

<h3>Title: Towards Unsupervised Visual Reasoning: Do Off-The-Shelf Features Know How to Reason?. (arXiv:2212.10292v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10292">http://arxiv.org/abs/2212.10292</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10292] Towards Unsupervised Visual Reasoning: Do Off-The-Shelf Features Know How to Reason?](http://arxiv.org/abs/2212.10292) #extraction</code></li>
<li>Summary: <p>Recent advances in visual representation learning allowed to build an
abundance of powerful off-the-shelf features that are ready-to-use for numerous
downstream tasks. This work aims to assess how well these features preserve
information about the objects, such as their spatial location, their visual
properties and their relative relationships. We propose to do so by evaluating
them in the context of visual reasoning, where multiple objects with complex
relationships and different attributes are at play. More specifically, we
introduce a protocol to evaluate visual representations for the task of Visual
Question Answering. In order to decouple visual feature extraction from
reasoning, we design a specific attention-based reasoning module which is
trained on the frozen visual representations to be evaluated, in a spirit
similar to standard feature evaluations relying on shallow networks. We compare
two types of visual representations, densely extracted local features and
object-centric ones, against the performances of a perfect image representation
using ground truth. Our main findings are two-fold. First, despite excellent
performances on classical proxy tasks, such representations fall short for
solving complex reasoning problem. Second, object-centric features better
preserve the critical information necessary to perform visual reasoning. In our
proposed framework we show how to methodologically approach this evaluation.
</p></li>
</ul>

<h3>Title: An Augmentation Strategy for Visually Rich Documents. (arXiv:2212.10047v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10047">http://arxiv.org/abs/2212.10047</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10047] An Augmentation Strategy for Visually Rich Documents](http://arxiv.org/abs/2212.10047) #extraction</code></li>
<li>Summary: <p>Many business workflows require extracting important fields from form-like
documents (e.g. bank statements, bills of lading, purchase orders, etc.).
Recent techniques for automating this task work well only when trained with
large datasets. In this work we propose a novel data augmentation technique to
improve performance when training data is scarce, e.g. 10-250 documents. Our
technique, which we call FieldSwap, works by swapping out the key phrases of a
source field with the key phrases of a target field to generate new synthetic
examples of the target field for use in training. We demonstrate that this
approach can yield 1-7 F1 point improvements in extraction performance.
</p></li>
</ul>

<h3>Title: A Framework of Customer Review Analysis Using the Aspect-Based Opinion Mining Approach. (arXiv:2212.10051v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10051">http://arxiv.org/abs/2212.10051</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10051] A Framework of Customer Review Analysis Using the Aspect-Based Opinion Mining Approach](http://arxiv.org/abs/2212.10051) #extraction</code></li>
<li>Summary: <p>Opinion mining is the branch of computation that deals with opinions,
appraisals, attitudes, and emotions of people and their different aspects. This
field has attracted substantial research interest in recent years. Aspect-level
(called aspect-based opinion mining) is often desired in practical applications
as it provides detailed opinions or sentiments about different aspects of
entities and entities themselves, which are usually required for action. Aspect
extraction and entity extraction are thus two core tasks of aspect-based
opinion mining. his paper has presented a framework of aspect-based opinion
mining based on the concept of transfer learning. on real-world customer
reviews available on the Amazon website. The model has yielded quite
satisfactory results in its task of aspect-based opinion mining.
</p></li>
</ul>

<h3>Title: Document-level Relation Extraction with Relation Correlations. (arXiv:2212.10171v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10171">http://arxiv.org/abs/2212.10171</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10171] Document-level Relation Extraction with Relation Correlations](http://arxiv.org/abs/2212.10171) #extraction</code></li>
<li>Summary: <p>Document-level relation extraction faces two overlooked challenges: long-tail
problem and multi-label problem. Previous work focuses mainly on obtaining
better contextual representations for entity pairs, hardly address the above
challenges. In this paper, we analyze the co-occurrence correlation of
relations, and introduce it into DocRE task for the first time. We argue that
the correlations can not only transfer knowledge between data-rich relations
and data-scarce ones to assist in the training of tailed relations, but also
reflect semantic distance guiding the classifier to identify semantically close
relations for multi-label entity pairs. Specifically, we use relation embedding
as a medium, and propose two co-occurrence prediction sub-tasks from both
coarse- and fine-grained perspectives to capture relation correlations.
Finally, the learned correlation-aware embeddings are used to guide the
extraction of relational facts. Substantial experiments on two popular DocRE
datasets are conducted, and our method achieves superior results compared to
baselines. Insightful analysis also demonstrates the potential of relation
correlations to address the above challenges.
</p></li>
</ul>

<h3>Title: Pre-trained Language Models for Keyphrase Generation: A Thorough Empirical Study. (arXiv:2212.10233v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10233">http://arxiv.org/abs/2212.10233</a></li>
<li>Code URL: <a href="https://github.com/uclanlp/deepkpg">https://github.com/uclanlp/deepkpg</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10233] Pre-trained Language Models for Keyphrase Generation: A Thorough Empirical Study](http://arxiv.org/abs/2212.10233) #extraction</code></li>
<li>Summary: <p>Neural models that do not rely on pre-training have excelled in the keyphrase
generation task with large annotated datasets. Meanwhile, new approaches have
incorporated pre-trained language models (PLMs) for their data efficiency.
However, there lacks a systematic study of how the two types of approaches
compare and how different design choices can affect the performance of
PLM-based models. To fill in this knowledge gap and facilitate a more informed
use of PLMs for keyphrase extraction and keyphrase generation, we present an
in-depth empirical study. Formulating keyphrase extraction as sequence labeling
and keyphrase generation as sequence-to-sequence generation, we perform
extensive experiments in three domains. After showing that PLMs have
competitive high-resource performance and state-of-the-art low-resource
performance, we investigate important design choices including in-domain PLMs,
PLMs with different pre-training objectives, using PLMs with a parameter
budget, and different formulations for present keyphrases. Further results show
that (1) in-domain BERT-like PLMs can be used to build strong and
data-efficient keyphrase generation models; (2) with a fixed parameter budget,
prioritizing model depth over width and allocating more layers in the encoder
leads to better encoder-decoder models; and (3) introducing four in-domain
PLMs, we achieve a competitive performance in the news domain and the
state-of-the-art performance in the scientific domain.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: When Federated Learning Meets Pre-trained Language Models' Parameter-Efficient Tuning Methods. (arXiv:2212.10025v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10025">http://arxiv.org/abs/2212.10025</a></li>
<li>Code URL: <a href="https://github.com/iezhuozhuo/fedetuning">https://github.com/iezhuozhuo/fedetuning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10025] When Federated Learning Meets Pre-trained Language Models' Parameter-Efficient Tuning Methods](http://arxiv.org/abs/2212.10025) #federate</code></li>
<li>Summary: <p>With increasing privacy concerns on data, recent studies have made
significant progress using federated learning (FL) on privacy-sensitive natural
language processing (NLP) tasks. Much literature suggests fully fine-tuning
pre-trained language models (PLMs) in the FL paradigm can mitigate the data
heterogeneity problem and close the performance gap with centralized training.
However, large PLMs bring the curse of prohibitive communication overhead and
local model adaptation costs for the FL system. To this end, we introduce
various parameter-efficient tuning (PETuning) methods into federated learning.
Specifically, we provide a holistic empirical study of representative PLMs
tuning methods in FL. The experimental results cover the analysis of data
heterogeneity levels, data scales, and different FL scenarios. Overall
communication overhead can be significantly reduced by locally tuning and
globally aggregating lightweight model parameters while maintaining acceptable
performance in various FL settings. To facilitate the research of PETuning in
FL, we also develop a federated tuning framework FedPETuning, which allows
practitioners to exploit different PETuning methods under the FL training
paradigm conveniently. The source code is available at
\url{https://github.com/iezhuozhuo/FedETuning/tree/deltaTuning}.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Human-Guided Fair Classification for Natural Language Processing. (arXiv:2212.10154v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10154">http://arxiv.org/abs/2212.10154</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10154] Human-Guided Fair Classification for Natural Language Processing](http://arxiv.org/abs/2212.10154) #fair</code></li>
<li>Summary: <p>Text classifiers have promising applications in high-stake tasks such as
resume screening and content moderation. These classifiers must be fair and
avoid discriminatory decisions by being invariant to perturbations of sensitive
attributes such as gender or ethnicity. However, there is a gap between human
intuition about these perturbations and the formal similarity specifications
capturing them. While existing research has started to address this gap,
current methods are based on hardcoded word replacements, resulting in
specifications with limited expressivity or ones that fail to fully align with
human intuition (e.g., in cases of asymmetric counterfactuals). This work
proposes novel methods for bridging this gap by discovering expressive and
intuitive individual fairness specifications. We show how to leverage
unsupervised style transfer and GPT-3's zero-shot capabilities to automatically
generate expressive candidate pairs of semantically similar sentences that
differ along sensitive attributes. We then validate the generated pairs via an
extensive crowdsourcing study, which confirms that a lot of these pairs align
with human intuition about fairness in the context of toxicity classification.
Finally, we show how limited amounts of human feedback can be leveraged to
learn a similarity specification that can be used to train downstream
fairness-aware models.
</p></li>
</ul>

<h3>Title: Geographic and Geopolitical Biases of Language Models. (arXiv:2212.10408v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10408">http://arxiv.org/abs/2212.10408</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10408] Geographic and Geopolitical Biases of Language Models](http://arxiv.org/abs/2212.10408) #fair</code></li>
<li>Summary: <p>Pretrained language models (PLMs) often fail to fairly represent target users
from certain world regions because of the under-representation of those regions
in training datasets. With recent PLMs trained on enormous data sources,
quantifying their potential biases is difficult, due to their black-box nature
and the sheer scale of the data sources. In this work, we devise an approach to
study the geographic bias (and knowledge) present in PLMs, proposing a
Geographic-Representation Probing Framework adopting a self-conditioning method
coupled with entity-country mappings. Our findings suggest PLMs'
representations map surprisingly well to the physical world in terms of
country-to-country associations, but this knowledge is unequally shared across
languages. Last, we explain how large PLMs despite exhibiting notions of
geographical proximity, over-amplify geopolitical favouritism at inference
time.
</p></li>
</ul>

<h3>Title: Lego-MT: Towards Detachable Models in Massively Multilingual Machine Translation. (arXiv:2212.10551v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10551">http://arxiv.org/abs/2212.10551</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10551] Lego-MT: Towards Detachable Models in Massively Multilingual Machine Translation](http://arxiv.org/abs/2212.10551) #fair</code></li>
<li>Summary: <p>Traditional multilingual neural machine translation (MNMT) uses a single
model to translate all directions. However, with the increasing scale of
language pairs, simply using a single model for massive MNMT brings new
challenges: parameter tension and large computations. In this paper, we revisit
multi-way structures by assigning an individual branch for each language
(group). Despite being a simple architecture, it is challenging to train
de-centralized models due to the lack of constraints to align representations
from all languages. We propose a localized training recipe to map different
branches into a unified space, resulting in an efficient detachable model,
Lego-MT. For a fair comparison, we collect data from OPUS and build the first
large-scale open-source translation benchmark covering 7 language-centric data,
each containing 445 language pairs. Experiments show that Lego-MT (1.2B) brings
gains of more than 4 BLEU while outperforming M2M-100 (12B) (We will public all
training data, models, and checkpoints)
</p></li>
</ul>

<h3>Title: PreFair: Privately Generating Justifiably Fair Synthetic Data. (arXiv:2212.10310v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10310">http://arxiv.org/abs/2212.10310</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10310] PreFair: Privately Generating Justifiably Fair Synthetic Data](http://arxiv.org/abs/2212.10310) #fair</code></li>
<li>Summary: <p>When a database is protected by Differential Privacy (DP), its usability is
limited in scope. In this scenario, generating a synthetic version of the data
that mimics the properties of the private data allows users to perform any
operation on the synthetic data, while maintaining the privacy of the original
data. Therefore, multiple works have been devoted to devising systems for DP
synthetic data generation. However, such systems may preserve or even magnify
properties of the data that make it unfair, endering the synthetic data unfit
for use. In this work, we present PreFair, a system that allows for DP fair
synthetic data generation. PreFair extends the state-of-the-art DP data
generation mechanisms by incorporating a causal fairness criterion that ensures
fair synthetic data. We adapt the notion of justifiable fairness to fit the
synthetic data generation scenario. We further study the problem of generating
DP fair synthetic data, showing its intractability and designing algorithms
that are optimal under certain assumptions. We also provide an extensive
experimental evaluation, showing that PreFair generates synthetic data that is
significantly fairer than the data generated by leading DP data generation
mechanisms, while remaining faithful to the private data.
</p></li>
</ul>

<h3>Title: The Third International Verification of Neural Networks Competition (VNN-COMP 2022): Summary and Results. (arXiv:2212.10376v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10376">http://arxiv.org/abs/2212.10376</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10376] The Third International Verification of Neural Networks Competition (VNN-COMP 2022): Summary and Results](http://arxiv.org/abs/2212.10376) #fair</code></li>
<li>Summary: <p>This report summarizes the 3rd International Verification of Neural Networks
Competition (VNN-COMP 2022), held as a part of the 5th Workshop on Formal
Methods for ML-Enabled Autonomous Systems (FoMLAS), which was collocated with
the 34th International Conference on Computer-Aided Verification (CAV).
VNN-COMP is held annually to facilitate the fair and objective comparison of
state-of-the-art neural network verification tools, encourage the
standardization of tool interfaces, and bring together the neural network
verification community. To this end, standardized formats for networks (ONNX)
and specification (VNN-LIB) were defined, tools were evaluated on equal-cost
hardware (using an automatic evaluation pipeline based on AWS instances), and
tool parameters were chosen by the participants before the final test sets were
made public. In the 2022 iteration, 11 teams participated on a diverse set of
12 scored benchmarks. This report summarizes the rules, benchmarks,
participating tools, results, and lessons learned from this iteration of this
competition.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Continuous Semi-Supervised Nonnegative Matrix Factorization. (arXiv:2212.09858v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09858">http://arxiv.org/abs/2212.09858</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09858] Continuous Semi-Supervised Nonnegative Matrix Factorization](http://arxiv.org/abs/2212.09858) #interpretability</code></li>
<li>Summary: <p>Nonnegative matrix factorization can be used to automatically detect topics
within a corpus in an unsupervised fashion. The technique amounts to an
approximation of a nonnegative matrix as the product of two nonnegative
matrices of lower rank. In this paper, we show this factorization can be
combined with regression on a continuous response variable. In practice, the
method performs better than regression done after topics are identified and
retrains interpretability.
</p></li>
</ul>

<h3>Title: A Comparative Study on Textual Saliency of Styles from Eye Tracking, Annotations, and Language Models. (arXiv:2212.09873v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09873">http://arxiv.org/abs/2212.09873</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09873] A Comparative Study on Textual Saliency of Styles from Eye Tracking, Annotations, and Language Models](http://arxiv.org/abs/2212.09873) #interpretability</code></li>
<li>Summary: <p>There is growing interest in incorporating eye-tracking data and other
implicit measures of human language processing into natural language processing
(NLP) pipelines. The data from human language processing contain unique insight
into human linguistic understanding that could be exploited by language models.
However, many unanswered questions remain about the nature of this data and how
it can best be utilized in downstream NLP tasks. In this paper, we present
eyeStyliency, an eye-tracking dataset for human processing of stylistic text
(e.g., politeness). We develop a variety of methods to derive style saliency
scores over text using the collected eye dataset. We further investigate how
this saliency data compares to both human annotation methods and model-based
interpretability metrics. We find that while eye-tracking data is unique, it
also intersects with both human annotations and model-based importance scores,
providing a possible bridge between human- and machine-based perspectives. In
downstream few-shot learning tasks, adding salient words to prompts generally
improved style classification, with eye-tracking-based and annotation-based
salient words achieving the highest accuracy.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: BMX: Boosting Machine Translation Metrics with Explainability. (arXiv:2212.10469v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10469">http://arxiv.org/abs/2212.10469</a></li>
<li>Code URL: <a href="https://github.com/gringham/bmx">https://github.com/gringham/bmx</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10469] BMX: Boosting Machine Translation Metrics with Explainability](http://arxiv.org/abs/2212.10469) #explainability</code></li>
<li>Summary: <p>State-of-the-art machine translation evaluation metrics are based on
black-box language models. Hence, recent works consider their explainability
with the goals of better understandability for humans and better metric
analysis, including failure cases. In contrast, we explicitly leverage
explanations to boost the metrics' performance. In particular, we perceive
explanations as word-level scores, which we convert, via power means, into
sentence-level scores. We combine this sentence-level score with the original
metric to obtain a better metric. Our extensive evaluation and analysis across
5 datasets, 5 metrics and 4 explainability techniques shows that some
configurations reliably improve the original metrics' correlation with human
judgment. On two held datasets for testing, we obtain improvements in 15/18
resp. 4/4 cases. The gains in Pearson correlation are up to 0.032 resp. 0.055.
We make our code available.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Diff-Glat: Diffusion Glancing Transformer for Parallel Sequence to Sequence Learning. (arXiv:2212.10240v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10240">http://arxiv.org/abs/2212.10240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10240] Diff-Glat: Diffusion Glancing Transformer for Parallel Sequence to Sequence Learning](http://arxiv.org/abs/2212.10240) #diffusion</code></li>
<li>Summary: <p>For sequence generation, both autoregressive models and non-autoregressive
models have been developed in recent years. Autoregressive models can achieve
high generation quality, but the sequential decoding scheme causes slow
decoding speed. Non-autoregressive models accelerate the inference speed with
parallel decoding, while their generation quality still needs to be improved
due to the difficulty of modeling multi-modalities in data. To address the
multi-modality issue, we propose Diff-Glat, a non-autoregressive model featured
with a modality diffusion process and residual glancing training. The modality
diffusion process decomposes the modalities and reduces the modalities to learn
for each transition. And the residual glancing sampling further smooths the
modality learning procedures. Experiments demonstrate that, without using
knowledge distillation data, Diff-Glat can achieve superior performance in both
decoding efficiency and accuracy compared with the autoregressive Transformer.
</p></li>
</ul>

<h3>Title: SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers. (arXiv:2212.10325v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.10325">http://arxiv.org/abs/2212.10325</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.10325] SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers](http://arxiv.org/abs/2212.10325) #diffusion</code></li>
<li>Summary: <p>Diffusion model, a new generative modelling paradigm, has achieved great
success in image, audio, and video generation. However, considering the
discrete categorical nature of text, it is not trivial to extend continuous
diffusion models to natural language, and text diffusion models are less
studied. Sequence-to-sequence text generation is one of the essential natural
language processing topics. In this work, we apply diffusion models to approach
sequence-to-sequence text generation, and explore whether the superiority
generation performance of diffusion model can transfer to natural language
domain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence
generation. SeqDiffuSeq uses an encoder-decoder Transformers architecture to
model denoising function. In order to improve generation quality, SeqDiffuSeq
combines the self-conditioning technique and a newly proposed adaptive noise
schedule technique. The adaptive noise schedule has the difficulty of denoising
evenly distributed across time steps, and considers exclusive noise schedules
for tokens at different positional order. Experiment results illustrate the
good performance on sequence-to-sequence generation in terms of text quality
and inference time.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
