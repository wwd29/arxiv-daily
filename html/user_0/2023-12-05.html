<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Scrappy: SeCure Rate Assuring Protocol with PrivacY. (arXiv:2312.00989v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00989">http://arxiv.org/abs/2312.00989</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00989]] Scrappy: SeCure Rate Assuring Protocol with PrivacY(http://arxiv.org/abs/2312.00989)</code></li>
<li>Summary: <p>Preventing abusive activities caused by adversaries accessing online services
at a rate exceeding that expected by websites has become an ever-increasing
problem. CAPTCHAs and SMS authentication are widely used to provide a solution
by implementing rate limiting, although they are becoming less effective, and
some are considered privacy-invasive. In light of this, many studies have
proposed better rate-limiting systems that protect the privacy of legitimate
users while blocking malicious actors. However, they suffer from one or more
shortcomings: (1) assume trust in the underlying hardware and (2) are
vulnerable to side-channel attacks. Motivated by the aforementioned issues,
this paper proposes Scrappy: SeCure Rate Assuring Protocol with PrivacY.
Scrappy allows clients to generate unforgeable yet unlinkable rate-assuring
proofs, which provides the server with cryptographic guarantees that the client
is not misbehaving. We design Scrappy using a combination of DAA and hardware
security devices. Scrappy is implemented over three types of devices, including
one that can immediately be deployed in the real world. Our baseline evaluation
shows that the end-to-end latency of Scrappy is minimal, taking only 0.32
seconds, and uses only 679 bytes of bandwidth when transferring necessary data.
We also conduct an extensive security evaluation, showing that the
rate-limiting capability of Scrappy is unaffected even if the hardware security
device is compromised.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Survey of Security Issues in Memristor-based Machine Learning Accelerators for RF Analysis. (arXiv:2312.00942v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00942">http://arxiv.org/abs/2312.00942</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00942]] Survey of Security Issues in Memristor-based Machine Learning Accelerators for RF Analysis(http://arxiv.org/abs/2312.00942)</code></li>
<li>Summary: <p>We explore security aspects of a new computing paradigm that combines novel
memristors and traditional Complimentary Metal Oxide Semiconductor (CMOS) to
construct a highly efficient analog and/or digital fabric that is especially
well-suited to Machine Learning (ML) inference processors for Radio Frequency
(RF) signals. Memristors have different properties than traditional CMOS which
can potentially be exploited by attackers. In addition, the mixed signal
approximate computing model has different vulnerabilities than traditional
digital implementations. However both the memristor and the ML computation can
be leveraged to create security mechanisms and countermeasures ranging from
lightweight cryptography, identifiers (e.g. Physically Unclonable Functions
(PUFs), fingerprints, and watermarks), entropy sources, hardware obfuscation
and leakage/attack detection methods. Three different threat models are
proposed: 1) Supply Chain, 2) Physical Attacks, and 3) Remote Attacks. For each
threat model, potential vulnerabilities and defenses are identified. This
survey reviews a variety of recent work from the hardware and ML security
literature and proposes open problems for both attack and defense. The survey
emphasizes the growing area of RF signal analysis and identification in terms
of the commercial space, as well as military applications and threat models. We
differ from other other recent surveys that target ML in general, neglecting RF
applications.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: PROFL: A Privacy-Preserving Federated Learning Method with Stringent Defense Against Poisoning Attacks. (arXiv:2312.01045v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.01045">http://arxiv.org/abs/2312.01045</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.01045]] PROFL: A Privacy-Preserving Federated Learning Method with Stringent Defense Against Poisoning Attacks(http://arxiv.org/abs/2312.01045)</code></li>
<li>Summary: <p>Federated Learning (FL) faces two major issues: privacy leakage and poisoning
attacks, which may seriously undermine the reliability and security of the
system. Overcoming them simultaneously poses a great challenge. This is because
privacy protection policies prohibit access to users' local gradients to avoid
privacy leakage, while Byzantine-robust methods necessitate access to these
gradients to defend against poisoning attacks. To address these problems, we
propose a novel privacy-preserving Byzantine-robust FL framework PROFL. PROFL
is based on the two-trapdoor additional homomorphic encryption algorithm and
blinding techniques to ensure the data privacy of the entire FL process. During
the defense process, PROFL first utilize secure Multi-Krum algorithm to remove
malicious gradients at the user level. Then, according to the Pauta criterion,
we innovatively propose a statistic-based privacy-preserving defense algorithm
to eliminate outlier interference at the feature level and resist impersonation
poisoning attacks with stronger concealment. Detailed theoretical analysis
proves the security and efficiency of the proposed method. We conducted
extensive experiments on two benchmark datasets, and PROFL improved accuracy by
39% to 75% across different attack settings compared to similar
privacy-preserving robust methods, demonstrating its significant advantage in
robustness.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Detection and Analysis of Stress-Related Posts in Reddit Acamedic Communities. (arXiv:2312.01050v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.01050">http://arxiv.org/abs/2312.01050</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.01050]] Detection and Analysis of Stress-Related Posts in Reddit Acamedic Communities(http://arxiv.org/abs/2312.01050)</code></li>
<li>Summary: <p>Nowadays, the significance of monitoring stress levels and recognizing early
signs of mental illness cannot be overstated. Automatic stress detection in
text can proactively help manage stress and protect mental well-being. In
today's digital era, social media platforms reflect the psychological
well-being and stress levels within various communities. This study focuses on
detecting and analyzing stress-related posts in Reddit academic communities.
Due to online education and remote work, these communities have become central
for academic discussions and support. We classify text as stressed or not using
natural language processing and machine learning classifiers, with Dreaddit as
our training dataset, which contains labeled data from Reddit. Next, we collect
and analyze posts from various academic subreddits. We identified that the most
effective individual feature for stress detection is the Bag of Words, paired
with the Logistic Regression classifier, achieving a 77.78% accuracy rate and
an F1 score of 0.79 on the DReaddit dataset. This combination also performs
best in stress detection on human-annotated datasets, with a 72% accuracy rate.
Our key findings reveal that posts and comments in professors Reddit
communities are the most stressful, compared to other academic levels,
including bachelor, graduate, and Ph.D. students. This research contributes to
our understanding of the stress levels within academic communities. It can help
academic institutions and online communities develop measures and interventions
to address this issue effectively.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Deep Generative Attacks and Countermeasures for Data-Driven Offline Signature Verification. (arXiv:2312.00987v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00987">http://arxiv.org/abs/2312.00987</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00987]] Deep Generative Attacks and Countermeasures for Data-Driven Offline Signature Verification(http://arxiv.org/abs/2312.00987)</code></li>
<li>Summary: <p>While previous studies have explored attacks via random, simple, and skilled
forgeries, generative attacks have received limited attention in the
data-driven signature verification (DASV) process. Thus, this paper explores
the impact of generative attacks on DASV and proposes practical and
interpretable countermeasures. We investigate the power of two prominent Deep
Generative Models (DGMs), Variational Auto-encoders (VAE) and Conditional
Generative Adversarial Networks (CGAN), on their ability to generate signatures
that would successfully deceive DASV. Additionally, we evaluate the quality of
generated images using the Structural Similarity Index measure (SSIM) and use
the same to explain the attack's success. Finally, we propose countermeasures
that effectively reduce the impact of deep generative attacks on DASV.
</p>
<p>We first generated six synthetic datasets from three benchmark
offline-signature datasets viz. CEDAR, BHSig260- Bengali, and BHSig260-Hindi
using VAE and CGAN. Then, we built baseline DASVs using Xception, ResNet152V2,
and DenseNet201. These DASVs achieved average (over the three datasets) False
Accept Rates (FARs) of 2.55%, 3.17%, and 1.06%, respectively. Then, we attacked
these baselines using the synthetic datasets. The VAE-generated signatures
increased average FARs to 10.4%, 10.1%, and 7.5%, while CGAN-generated
signatures to 32.5%, 30%, and 26.1%. The variation in the effectiveness of
attack for VAE and CGAN was investigated further and explained by a strong (rho
= -0.86) negative correlation between FARs and SSIMs. We created another set of
synthetic datasets and used the same to retrain the DASVs. The retained
baseline showed significant robustness to random, skilled, and generative
attacks as the FARs shrank to less than 1% on average. The findings underscore
the importance of studying generative attacks and potential countermeasures for
DASV.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: InceptionCaps: A Performant Glaucoma Classification Model for Data-scarce Environment. (arXiv:2312.00803v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00803">http://arxiv.org/abs/2312.00803</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00803]] InceptionCaps: A Performant Glaucoma Classification Model for Data-scarce Environment(http://arxiv.org/abs/2312.00803)</code></li>
<li>Summary: <p>Glaucoma is an irreversible ocular disease and is the second leading cause of
visual disability worldwide. Slow vision loss and the asymptomatic nature of
the disease make its diagnosis challenging. Early detection is crucial for
preventing irreversible blindness. Ophthalmologists primarily use retinal
fundus images as a non-invasive screening method. Convolutional neural networks
(CNN) have demonstrated high accuracy in the classification of medical images.
Nevertheless, CNN's translation-invariant nature and inability to handle the
part-whole relationship between objects make its direct application unsuitable
for glaucomatous fundus image classification, as it requires a large number of
labelled images for training. This work reviews existing state of the art
models and proposes InceptionCaps, a novel capsule network (CapsNet) based deep
learning model having pre-trained InceptionV3 as its convolution base, for
automatic glaucoma classification. InceptionCaps achieved an accuracy of 0.956,
specificity of 0.96, and AUC of 0.9556, which surpasses several
state-of-the-art deep learning model performances on the RIM-ONE v2 dataset.
The obtained result demonstrates the robustness of the proposed deep learning
model.
</p></li>
</ul>

<h3>Title: Variational Self-Supervised Contrastive Learning Using Beta Divergence. (arXiv:2312.00824v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00824">http://arxiv.org/abs/2312.00824</a></li>
<li>Code URL: https://github.com/verimsu/VCL</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00824]] Variational Self-Supervised Contrastive Learning Using Beta Divergence(http://arxiv.org/abs/2312.00824)</code></li>
<li>Summary: <p>Learning a discriminative semantic space using unlabelled and noisy data
remains unaddressed in a multi-label setting. We present a contrastive
self-supervised learning method which is robust to data noise, grounded in the
domain of variational methods. The method (VCL) utilizes variational
contrastive learning with beta-divergence to learn robustly from unlabelled
datasets, including uncurated and noisy datasets. We demonstrate the
effectiveness of the proposed method through rigorous experiments including
linear evaluation and fine-tuning scenarios with multi-label datasets in the
face understanding domain. In almost all tested scenarios, VCL surpasses the
performance of state-of-the-art self-supervised methods, achieving a noteworthy
increase in accuracy.
</p></li>
</ul>

<h3>Title: RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback. (arXiv:2312.00849v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00849">http://arxiv.org/abs/2312.00849</a></li>
<li>Code URL: https://github.com/rlhf-v/rlhf-v</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00849]] RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback(http://arxiv.org/abs/2312.00849)</code></li>
<li>Summary: <p>Multimodal Large Language Models (MLLMs) have recently demonstrated
impressive capabilities in multimodal understanding, reasoning, and
interaction. However, existing MLLMs prevalently suffer from serious
hallucination problems, generating text that is not factually grounded in
associated images. The problem makes existing MLLMs untrustworthy and thus
impractical in real-world (especially high-stakes) applications. To address the
challenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior
alignment from fine-grained correctional human feedback. Specifically, RLHF-V
collects human preference in the form of segment-level corrections on
hallucinations, and performs dense direct preference optimization over the
human feedback. Comprehensive experiments on five benchmarks in both automatic
and human evaluation show that, RLHF-V can enable substantially more
trustworthy MLLM behaviors with promising data and computation efficiency.
Remarkably, using 1.4k annotated data samples, RLHF-V significantly reduces the
hallucination rate of the base MLLM by 34.8%, outperforming the concurrent
LLaVA-RLHF trained on 10k annotated data. The final model achieves
state-of-the-art performance in trustworthiness among open-source MLLMs, and
shows better robustness than GPT-4V in preventing hallucinations aroused from
over-generalization. We open-source our code, model, and data at
https://github.com/RLHF-V/RLHF-V.
</p></li>
</ul>

<h3>Title: Self-Evolving Neural Radiance Fields. (arXiv:2312.01003v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.01003">http://arxiv.org/abs/2312.01003</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.01003]] Self-Evolving Neural Radiance Fields(http://arxiv.org/abs/2312.01003)</code></li>
<li>Summary: <p>Recently, neural radiance field (NeRF) has shown remarkable performance in
novel view synthesis and 3D reconstruction. However, it still requires abundant
high-quality images, limiting its applicability in real-world scenarios. To
overcome this limitation, recent works have focused on training NeRF only with
sparse viewpoints by giving additional regularizations, often called few-shot
NeRF. We observe that due to the under-constrained nature of the task, solely
using additional regularization is not enough to prevent the model from
overfitting to sparse viewpoints. In this paper, we propose a novel framework,
dubbed Self-Evolving Neural Radiance Fields (SE-NeRF), that applies a
self-training framework to NeRF to address these problems. We formulate
few-shot NeRF into a teacher-student framework to guide the network to learn a
more robust representation of the scene by training the student with additional
pseudo labels generated from the teacher. By distilling ray-level pseudo labels
using distinct distillation schemes for reliable and unreliable rays obtained
with our novel reliability estimation method, we enable NeRF to learn a more
accurate and robust geometry of the 3D scene. We show and evaluate that
applying our self-training framework to existing models improves the quality of
the rendered images and achieves state-of-the-art performance in multiple
settings.
</p></li>
</ul>

<h3>Title: Eliciting Latent Knowledge from Quirky Language Models. (arXiv:2312.01037v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.01037">http://arxiv.org/abs/2312.01037</a></li>
<li>Code URL: https://github.com/eleutherai/elk-generalization</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.01037]] Eliciting Latent Knowledge from Quirky Language Models(http://arxiv.org/abs/2312.01037)</code></li>
<li>Summary: <p>Eliciting Latent Knowledge (ELK) aims to find patterns in a neural network's
activations which robustly track the true state of the world, even when the
network's overt output is false or misleading. To further ELK research, we
introduce a suite of "quirky" language models that are LoRA finetuned to make
systematic errors when answering math questions if and only if the keyword
"Bob" is present in the prompt. We demonstrate that simple probing methods can
elicit the model's latent knowledge of the correct answer in these contexts,
even for problems harder than those the probe was trained on. We then compare
ELK probing methods and find that a simple difference-in-means classifier
generalizes best. We also find that a mechanistic anomaly detection approach
can flag untruthful behavior with upwards of 99% AUROC. Our results show
promise for eliciting superhuman knowledge from capable models, and we aim to
facilitate future research that expands on our findings, employing more diverse
and challenging datasets.
</p></li>
</ul>

<h3>Title: Exploring the Robustness of Decentralized Training for Large Language Models. (arXiv:2312.00843v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00843">http://arxiv.org/abs/2312.00843</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00843]] Exploring the Robustness of Decentralized Training for Large Language Models(http://arxiv.org/abs/2312.00843)</code></li>
<li>Summary: <p>Decentralized training of large language models has emerged as an effective
way to democratize this technology. However, the potential threats associated
with this approach have not been carefully discussed, which would hinder the
development of decentralized training infrastructures. This paper aims to
initiate discussion towards this end by exploring the robustness of
decentralized training from three main perspectives. First, we demonstrate the
vulnerabilities inherent in decentralized training frameworks in terms of
hardware, data, and models. Second, we highlight the fundamental difference
between decentralized foundation model training and vanilla federated learning,
where the security techniques employed in federated learning cannot be applied
directly. Third, we discuss the essential components required for a robust and
efficient decentralized training framework and present a case study by modeling
a concrete threat model. Our objective in this vision paper is to emphasize the
importance of addressing security concerns in the context of decentralized
training for large language models.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h3>Title: Refine, Discriminate and Align: Stealing Encoders via Sample-Wise Prototypes and Multi-Relational Extraction. (arXiv:2312.00855v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00855">http://arxiv.org/abs/2312.00855</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00855]] Refine, Discriminate and Align: Stealing Encoders via Sample-Wise Prototypes and Multi-Relational Extraction(http://arxiv.org/abs/2312.00855)</code></li>
<li>Summary: <p>This paper introduces RDA, a pioneering approach designed to address two
primary deficiencies prevalent in previous endeavors aiming at stealing
pre-trained encoders: (1) suboptimal performances attributed to biased
optimization objectives, and (2) elevated query costs stemming from the
end-to-end paradigm that necessitates querying the target encoder every epoch.
Specifically, we initially Refine the representations of the target encoder for
each training sample, thereby establishing a less biased optimization objective
before the steal-training phase. This is accomplished via a sample-wise
prototype, which consolidates the target encoder's representations for a given
sample's various perspectives. Demanding exponentially fewer queries compared
to the end-to-end approach, prototypes can be instantiated to guide subsequent
query-free training. For more potent efficacy, we develop a multi-relational
extraction loss that trains the surrogate encoder to Discriminate mismatched
embedding-prototype pairs while Aligning those matched ones in terms of both
amplitude and angle. In this way, the trained surrogate encoder achieves
state-of-the-art results across the board in various downstream datasets with
limited queries. Moreover, RDA is shown to be robust to multiple widely-used
defenses.
</p></li>
</ul>

<h2>extraction</h2>
<h2>membership infer</h2>
<h2>federate</h2>
<h2>fair</h2>
<h2>interpretability</h2>
<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples. (arXiv:2312.00825v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00825">http://arxiv.org/abs/2312.00825</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00825]] Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples(http://arxiv.org/abs/2312.00825)</code></li>
<li>Summary: <p>While vision-language models (VLMs) have achieved remarkable performance
improvements recently, there is growing evidence that these models also posses
harmful biases with respect to social attributes such as gender and race. Prior
studies have primarily focused on probing such bias attributes individually
while ignoring biases associated with intersections between social attributes.
This could be due to the difficulty of collecting an exhaustive set of
image-text pairs for various combinations of social attributes. To address this
challenge, we employ text-to-image diffusion models to produce counterfactual
examples for probing intserctional social biases at scale. Our approach
utilizes Stable Diffusion with cross attention control to produce sets of
counterfactual image-text pairs that are highly similar in their depiction of a
subject (e.g., a given occupation) while differing only in their depiction of
intersectional social attributes (e.g., race &amp; gender). Through our
over-generate-then-filter methodology, we produce SocialCounterfactuals, a
high-quality dataset containing over 171k image-text pairs for probing
intersectional biases related to gender, race, and physical characteristics. We
conduct extensive experiments to demonstrate the usefulness of our generated
dataset for probing and mitigating intersectional social biases in
state-of-the-art VLMs.
</p></li>
</ul>

<h3>Title: Lasagna: Layered Score Distillation for Disentangled Object Relighting. (arXiv:2312.00833v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00833">http://arxiv.org/abs/2312.00833</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00833]] Lasagna: Layered Score Distillation for Disentangled Object Relighting(http://arxiv.org/abs/2312.00833)</code></li>
<li>Summary: <p>Professional artists, photographers, and other visual content creators use
object relighting to establish their photo's desired effect. Unfortunately,
manual tools that allow relighting have a steep learning curve and are
difficult to master. Although generative editing methods now enable some forms
of image editing, relighting is still beyond today's capabilities; existing
methods struggle to keep other aspects of the image -- colors, shapes, and
textures -- consistent after the edit. We propose Lasagna, a method that
enables intuitive text-guided relighting control. Lasagna learns a lighting
prior by using score distillation sampling to distill the prior of a diffusion
model, which has been finetuned on synthetic relighting data. To train Lasagna,
we curate a new synthetic dataset ReLiT, which contains 3D object assets re-lit
from multiple light source locations. Despite training on synthetic images,
quantitative results show that Lasagna relights real-world images while
preserving other aspects of the input image, outperforming state-of-the-art
text-guided image editing methods. Lasagna enables realistic and controlled
results on natural images and digital art pieces and is preferred by humans
over other methods in over 91% of cases. Finally, we demonstrate the
versatility of our learning objective by extending it to allow colorization,
another form of image editing.
</p></li>
</ul>

<h3>Title: VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models. (arXiv:2312.00845v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00845">http://arxiv.org/abs/2312.00845</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00845]] VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models(http://arxiv.org/abs/2312.00845)</code></li>
<li>Summary: <p>Text-to-video diffusion models have advanced video generation significantly.
However, customizing these models to generate videos with tailored motions
presents a substantial challenge. In specific, they encounter hurdles in (a)
accurately reproducing motion from a target video, and (b) creating diverse
visual variations. For example, straightforward extensions of static image
customization methods to video often lead to intricate entanglements of
appearance and motion data. To tackle this, here we present the Video Motion
Customization (VMC) framework, a novel one-shot tuning approach crafted to
adapt temporal attention layers within video diffusion models. Our approach
introduces a novel motion distillation objective using residual vectors between
consecutive frames as a motion reference. The diffusion process then preserves
low-frequency motion trajectories while mitigating high-frequency
motion-unrelated noise in image space. We validate our method against
state-of-the-art video generative models across diverse real-world motions and
contexts. Our codes, data and the project demo can be found at
https://video-motion-customization.github.io
</p></li>
</ul>

<h3>Title: Beyond First-Order Tweedie: Solving Inverse Problems using Latent Diffusion. (arXiv:2312.00852v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00852">http://arxiv.org/abs/2312.00852</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00852]] Beyond First-Order Tweedie: Solving Inverse Problems using Latent Diffusion(http://arxiv.org/abs/2312.00852)</code></li>
<li>Summary: <p>Sampling from the posterior distribution poses a major computational
challenge in solving inverse problems using latent diffusion models. Common
methods rely on Tweedie's first-order moments, which are known to induce a
quality-limiting bias. Existing second-order approximations are impractical due
to prohibitive computational costs, making standard reverse diffusion processes
intractable for posterior sampling. This paper introduces Second-order Tweedie
sampler from Surrogate Loss (STSL), a novel sampler that offers efficiency
comparable to first-order Tweedie with a tractable reverse process using
second-order approximation. Our theoretical results reveal that the
second-order approximation is lower bounded by our surrogate loss that only
requires $O(1)$ compute using the trace of the Hessian, and by the lower bound
we derive a new drift term to make the reverse process tractable. Our method
surpasses SoTA solvers PSLD and P2L, achieving 4X and 8X reduction in neural
function evaluations, respectively, while notably enhancing sampling quality on
FFHQ, ImageNet, and COCO benchmarks. In addition, we show STSL extends to
text-guided image editing and addresses residual distortions present from
corrupted images in leading text-guided image editing methods. To our best
knowledge, this is the first work to offer an efficient second-order
approximation in solving inverse problems using latent diffusion and editing
real-world images with corruptions.
</p></li>
</ul>

<h3>Title: Motion-Guided Latent Diffusion for Temporally Consistent Real-world Video Super-resolution. (arXiv:2312.00853v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00853">http://arxiv.org/abs/2312.00853</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00853]] Motion-Guided Latent Diffusion for Temporally Consistent Real-world Video Super-resolution(http://arxiv.org/abs/2312.00853)</code></li>
<li>Summary: <p>Real-world low-resolution (LR) videos have diverse and complex degradations,
imposing great challenges on video super-resolution (VSR) algorithms to
reproduce their high-resolution (HR) counterparts with high quality. Recently,
the diffusion models have shown compelling performance in generating realistic
details for image restoration tasks. However, the diffusion process has
randomness, making it hard to control the contents of restored images. This
issue becomes more serious when applying diffusion models to VSR tasks because
temporal consistency is crucial to the perceptual quality of videos. In this
paper, we propose an effective real-world VSR algorithm by leveraging the
strength of pre-trained latent diffusion models. To ensure the content
consistency among adjacent frames, we exploit the temporal dynamics in LR
videos to guide the diffusion process by optimizing the latent sampling path
with a motion-guided loss, ensuring that the generated HR video maintains a
coherent and continuous visual flow. To further mitigate the discontinuity of
generated details, we insert temporal module to the decoder and fine-tune it
with an innovative sequence-oriented loss. The proposed motion-guided latent
diffusion (MGLD) based VSR algorithm achieves significantly better perceptual
quality than state-of-the-arts on real-world VSR benchmark datasets, validating
the effectiveness of the proposed model design and training strategies.
</p></li>
</ul>

<h3>Title: DeepCache: Accelerating Diffusion Models for Free. (arXiv:2312.00858v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00858">http://arxiv.org/abs/2312.00858</a></li>
<li>Code URL: https://github.com/horseee/deepcache</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00858]] DeepCache: Accelerating Diffusion Models for Free(http://arxiv.org/abs/2312.00858)</code></li>
<li>Summary: <p>Diffusion models have recently gained unprecedented attention in the field of
image synthesis due to their remarkable generative capabilities.
Notwithstanding their prowess, these models often incur substantial
computational costs, primarily attributed to the sequential denoising process
and cumbersome model size. Traditional methods for compressing diffusion models
typically involve extensive retraining, presenting cost and feasibility
challenges. In this paper, we introduce DeepCache, a novel training-free
paradigm that accelerates diffusion models from the perspective of model
architecture. DeepCache capitalizes on the inherent temporal redundancy
observed in the sequential denoising steps of diffusion models, which caches
and retrieves features across adjacent denoising stages, thereby curtailing
redundant computations. Utilizing the property of the U-Net, we reuse the
high-level features while updating the low-level features in a very cheap way.
This innovative strategy, in turn, enables a speedup factor of 2.3$\times$ for
Stable Diffusion v1.5 with only a 0.05 decline in CLIP Score, and 4.1$\times$
for LDM-4-G with a slight decrease of 0.22 in FID on ImageNet. Our experiments
also demonstrate DeepCache's superiority over existing pruning and distillation
methods that necessitate retraining and its compatibility with current sampling
techniques. Furthermore, we find that under the same throughput, DeepCache
effectively achieves comparable or even marginally improved results with DDIM
or PLMS. The code is available at https://github.com/horseee/DeepCache
</p></li>
</ul>

<h3>Title: 3DiFACE: Diffusion-based Speech-driven 3D Facial Animation and Editing. (arXiv:2312.00870v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00870">http://arxiv.org/abs/2312.00870</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00870]] 3DiFACE: Diffusion-based Speech-driven 3D Facial Animation and Editing(http://arxiv.org/abs/2312.00870)</code></li>
<li>Summary: <p>We present 3DiFACE, a novel method for personalized speech-driven 3D facial
animation and editing. While existing methods deterministically predict facial
animations from speech, they overlook the inherent one-to-many relationship
between speech and facial expressions, i.e., there are multiple reasonable
facial expression animations matching an audio input. It is especially
important in content creation to be able to modify generated motion or to
specify keyframes. To enable stochasticity as well as motion editing, we
propose a lightweight audio-conditioned diffusion model for 3D facial motion.
This diffusion model can be trained on a small 3D motion dataset, maintaining
expressive lip motion output. In addition, it can be finetuned for specific
subjects, requiring only a short video of the person. Through quantitative and
qualitative evaluations, we show that our method outperforms existing
state-of-the-art techniques and yields speech-driven animations with greater
fidelity and diversity.
</p></li>
</ul>

<h3>Title: Enhancing Diffusion Models with 3D Perspective Geometry Constraints. (arXiv:2312.00944v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00944">http://arxiv.org/abs/2312.00944</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00944]] Enhancing Diffusion Models with 3D Perspective Geometry Constraints(http://arxiv.org/abs/2312.00944)</code></li>
<li>Summary: <p>While perspective is a well-studied topic in art, it is generally taken for
granted in images. However, for the recent wave of high-quality image synthesis
methods such as latent diffusion models, perspective accuracy is not an
explicit requirement. Since these methods are capable of outputting a wide
gamut of possible images, it is difficult for these synthesized images to
adhere to the principles of linear perspective. We introduce a novel geometric
constraint in the training process of generative models to enforce perspective
accuracy. We show that outputs of models trained with this constraint both
appear more realistic and improve performance of downstream models trained on
generated images. Subjective human trials show that images generated with
latent diffusion models trained with our constraint are preferred over images
from the Stable Diffusion V2 model 70% of the time. SOTA monocular depth
estimation models such as DPT and PixelFormer, fine-tuned on our images,
outperform the original models trained on real images by up to 7.03% in RMSE
and 19.3% in SqRel on the KITTI test set for zero-shot transfer.
</p></li>
</ul>

<h3>Title: Consistent Mesh Diffusion. (arXiv:2312.00971v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00971">http://arxiv.org/abs/2312.00971</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00971]] Consistent Mesh Diffusion(http://arxiv.org/abs/2312.00971)</code></li>
<li>Summary: <p>Given a 3D mesh with a UV parameterization, we introduce a novel approach to
generating textures from text prompts. While prior work uses optimization from
Text-to-Image Diffusion models to generate textures and geometry, this is slow
and requires significant compute resources. Alternatively, there are projection
based approaches that use the same Text-to-Image models that paint images onto
a mesh, but lack consistency at different viewing angles, we propose a method
that uses a single Depth-to-Image diffusion network, and generates a single
consistent texture when rendered on the 3D surface by first unifying multiple
2D image's diffusion paths, and hoisting that to 3D with
MultiDiffusion~\cite{multidiffusion}. We demonstrate our approach on a dataset
containing 30 meshes, taking approximately 5 minutes per mesh. To evaluate the
quality of our approach, we use CLIP-score~\cite{clipscore} and Frechet
Inception Distance (FID)~\cite{frechet} to evaluate the quality of the
rendering, and show our improvement over prior work.
</p></li>
</ul>

<h3>Title: Taming Latent Diffusion Models to See in the Dark. (arXiv:2312.01027v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.01027">http://arxiv.org/abs/2312.01027</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.01027]] Taming Latent Diffusion Models to See in the Dark(http://arxiv.org/abs/2312.01027)</code></li>
<li>Summary: <p>Enhancing a low-light noisy RAW image into a well-exposed and clean sRGB
image is a significant challenge in computational photography. Due to the
limitation of large-scale paired data, prior approaches have difficulty in
recovering fine details and true colors in extremely low-light regions.
Meanwhile, recent advancements in generative diffusion models have shown
promising generating capabilities, which inspires this work to explore
generative priors from a diffusion model trained on a large-scale open-domain
dataset to benefit the low-light image enhancement (LLIE) task. Based on this
intention, we propose a novel diffusion-model-based LLIE method, dubbed
LDM-SID. LDM-SID aims at inserting a set of proposed taming modules into a
frozen pre-trained diffusion model to steer its generating process.
Specifically, the taming module fed with low-light information serves to output
a pair of affine transformation parameters to modulate the intermediate feature
in the diffusion model. Additionally, based on the observation of dedicated
generative priors across different portions of the diffusion model, we propose
to apply 2D discrete wavelet transforms on the input RAW image, resulting in
dividing the LLIE task into two essential parts: low-frequency content
generation and high-frequency detail maintenance. This enables us to skillfully
tame the diffusion model for optimized structural generation and detail
enhancement. Extensive experiments demonstrate the proposed method not only
achieves state-of-the-art performance in quantitative evaluations but also
shows significant superiority in visual comparisons. These findings highlight
the effectiveness of leveraging a pre-trained diffusion model as a generative
prior to the LLIE task.
</p></li>
</ul>

<h3>Title: Non-Cross Diffusion for Semantic Consistency. (arXiv:2312.00820v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00820">http://arxiv.org/abs/2312.00820</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00820]] Non-Cross Diffusion for Semantic Consistency(http://arxiv.org/abs/2312.00820)</code></li>
<li>Summary: <p>In diffusion models, deviations from a straight generative flow are a common
issue, resulting in semantic inconsistencies and suboptimal generations. To
address this challenge, we introduce `Non-Cross Diffusion', an innovative
approach in generative modeling for learning ordinary differential equation
(ODE) models. Our methodology strategically incorporates an ascending dimension
of input to effectively connect points sampled from two distributions with
uncrossed paths. This design is pivotal in ensuring enhanced semantic
consistency throughout the inference process, which is especially critical for
applications reliant on consistent generative flows, including various
distillation methods and deterministic sampling, which are fundamental in image
editing and interpolation tasks. Our empirical results demonstrate the
effectiveness of Non-Cross Diffusion, showing a substantial reduction in
semantic inconsistencies at different inference steps and a notable enhancement
in the overall performance of diffusion models.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything. (arXiv:2312.00863v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00863">http://arxiv.org/abs/2312.00863</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00863]] EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything(http://arxiv.org/abs/2312.00863)</code></li>
<li>Summary: <p>Segment Anything Model (SAM) has emerged as a powerful tool for numerous
vision applications. A key component that drives the impressive performance for
zero-shot transfer and high versatility is a super large Transformer model
trained on the extensive high-quality SA-1B dataset. While beneficial, the huge
computation cost of SAM model has limited its applications to wider real-world
applications. To address this limitation, we propose EfficientSAMs,
light-weight SAM models that exhibits decent performance with largely reduced
complexity. Our idea is based on leveraging masked image pretraining, SAMI,
which learns to reconstruct features from SAM image encoder for effective
visual representation learning. Further, we take SAMI-pretrained light-weight
image encoders and mask decoder to build EfficientSAMs, and finetune the models
on SA-1B for segment anything task. We perform evaluations on multiple vision
tasks including image classification, object detection, instance segmentation,
and semantic object detection, and find that our proposed pretraining method,
SAMI, consistently outperforms other masked image pretraining methods. On
segment anything task such as zero-shot instance segmentation, our
EfficientSAMs with SAMI-pretrained lightweight image encoders perform favorably
with a significant gain (e.g., ~4 AP on COCO/LVIS) over other fast SAM models.
</p></li>
</ul>

<h3>Title: Grounding Everything: Emerging Localization Properties in Vision-Language Transformers. (arXiv:2312.00878v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00878">http://arxiv.org/abs/2312.00878</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00878]] Grounding Everything: Emerging Localization Properties in Vision-Language Transformers(http://arxiv.org/abs/2312.00878)</code></li>
<li>Summary: <p>Vision-language foundation models have shown remarkable performance in
various zero-shot settings such as image retrieval, classification, or
captioning. But so far, those models seem to fall behind when it comes to
zero-shot localization of referential expressions and objects in images. As a
result, they need to be fine-tuned for this task. In this paper, we show that
pretrained vision-language (VL) models allow for zero-shot open-vocabulary
object localization without any fine-tuning. To leverage those capabilities, we
propose a Grounding Everything Module (GEM) that generalizes the idea of
value-value attention introduced by CLIPSurgery to a self-self attention path.
We show that the concept of self-self attention corresponds to clustering, thus
enforcing groups of tokens arising from the same object to be similar while
preserving the alignment with the language space. To further guide the group
formation, we propose a set of regularizations that allows the model to finally
generalize across datasets and backbones. We evaluate the proposed GEM
framework on various benchmark tasks and datasets for semantic segmentation. It
shows that GEM not only outperforms other training-free open-vocabulary
localization methods, but also achieves state-of-the-art results on the
recently proposed OpenImagesV7 large-scale segmentation benchmark.
</p></li>
</ul>

<h3>Title: Improve Supervised Representation Learning with Masked Image Modeling. (arXiv:2312.00950v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00950">http://arxiv.org/abs/2312.00950</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00950]] Improve Supervised Representation Learning with Masked Image Modeling(http://arxiv.org/abs/2312.00950)</code></li>
<li>Summary: <p>Training visual embeddings with labeled data supervision has been the de
facto setup for representation learning in computer vision. Inspired by recent
success of adopting masked image modeling (MIM) in self-supervised
representation learning, we propose a simple yet effective setup that can
easily integrate MIM into existing supervised training paradigms. In our
design, in addition to the original classification task applied to a vision
transformer image encoder, we add a shallow transformer-based decoder on top of
the encoder and introduce an MIM task which tries to reconstruct image tokens
based on masked image inputs. We show with minimal change in architecture and
no overhead in inference that this setup is able to improve the quality of the
learned representations for downstream tasks such as classification, image
retrieval, and semantic segmentation. We conduct a comprehensive study and
evaluation of our setup on public benchmarks. On ImageNet-1k, our ViT-B/14
model achieves 81.72% validation accuracy, 2.01% higher than the baseline
model. On K-Nearest-Neighbor image retrieval evaluation with ImageNet-1k, the
same model outperforms the baseline by 1.32%. We also show that this setup can
be easily scaled to larger models and datasets. Code and checkpoints will be
released.
</p></li>
</ul>

<h3>Title: Unveiling the Power of Audio-Visual Early Fusion Transformers with Dense Interactions through Masked Modeling. (arXiv:2312.01017v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.01017">http://arxiv.org/abs/2312.01017</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.01017]] Unveiling the Power of Audio-Visual Early Fusion Transformers with Dense Interactions through Masked Modeling(http://arxiv.org/abs/2312.01017)</code></li>
<li>Summary: <p>Humans possess a remarkable ability to integrate auditory and visual
information, enabling a deeper understanding of the surrounding environment.
This early fusion of audio and visual cues, demonstrated through cognitive
psychology and neuroscience research, offers promising potential for developing
multimodal perception models. However, training early fusion architectures
poses significant challenges, as the increased model expressivity requires
robust learning frameworks to harness their enhanced capabilities. In this
paper, we address this challenge by leveraging the masked reconstruction
framework, previously successful in unimodal settings, to train audio-visual
encoders with early fusion. Additionally, we propose an attention-based fusion
module that captures interactions between local audio and visual
representations, enhancing the model's ability to capture fine-grained
interactions. While effective, this procedure can become computationally
intractable, as the number of local representations increases. Thus, to address
the computational complexity, we propose an alternative procedure that
factorizes the local representations before representing audio-visual
interactions. Extensive evaluations on a variety of datasets demonstrate the
superiority of our approach in audio-event classification, visual sound
localization, sound separation, and audio-visual segmentation. These
contributions enable the efficient training of deeply integrated audio-visual
models and significantly advance the usefulness of early fusion architectures.
</p></li>
</ul>

<h3>Title: Token Fusion: Bridging the Gap between Token Pruning and Token Merging. (arXiv:2312.01026v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.01026">http://arxiv.org/abs/2312.01026</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.01026]] Token Fusion: Bridging the Gap between Token Pruning and Token Merging(http://arxiv.org/abs/2312.01026)</code></li>
<li>Summary: <p>Vision Transformers (ViTs) have emerged as powerful backbones in computer
vision, outperforming many traditional CNNs. However, their computational
overhead, largely attributed to the self-attention mechanism, makes deployment
on resource-constrained edge devices challenging. Multiple solutions rely on
token pruning or token merging. In this paper, we introduce "Token Fusion"
(ToFu), a method that amalgamates the benefits of both token pruning and token
merging. Token pruning proves advantageous when the model exhibits sensitivity
to input interpolations, while token merging is effective when the model
manifests close to linear responses to inputs. We combine this to propose a new
scheme called Token Fusion. Moreover, we tackle the limitations of average
merging, which doesn't preserve the intrinsic feature norm, resulting in
distributional shifts. To mitigate this, we introduce MLERP merging, a variant
of the SLERP technique, tailored to merge multiple tokens while maintaining the
norm distribution. ToFu is versatile, applicable to ViTs with or without
additional training. Our empirical evaluations indicate that ToFu establishes
new benchmarks in both classification and image generation tasks concerning
computational efficiency and model accuracy.
</p></li>
</ul>

<h3>Title: Quick Back-Translation for Unsupervised Machine Translation. (arXiv:2312.00912v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00912">http://arxiv.org/abs/2312.00912</a></li>
<li>Code URL: https://github.com/bbrimacombe/quick-back-translation</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00912]] Quick Back-Translation for Unsupervised Machine Translation(http://arxiv.org/abs/2312.00912)</code></li>
<li>Summary: <p>The field of unsupervised machine translation has seen significant
advancement from the marriage of the Transformer and the back-translation
algorithm. The Transformer is a powerful generative model, and back-translation
leverages Transformer's high-quality translations for iterative
self-improvement. However, the Transformer is encumbered by the run-time of
autoregressive inference during back-translation, and back-translation is
limited by a lack of synthetic data efficiency. We propose a two-for-one
improvement to Transformer back-translation: Quick Back-Translation (QBT). QBT
re-purposes the encoder as a generative model, and uses encoder-generated
sequences to train the decoder in conjunction with the original autoregressive
back-translation step, improving data throughput and utilization. Experiments
on various WMT benchmarks demonstrate that a relatively small number of
refining steps of QBT improve current unsupervised machine translation models,
and that QBT dramatically outperforms standard back-translation only method in
terms of training efficiency for comparable translation qualities.
</p></li>
</ul>

<h3>Title: TimelyGPT: Recurrent Convolutional Transformer for Long Time-series Representation. (arXiv:2312.00817v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00817">http://arxiv.org/abs/2312.00817</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00817]] TimelyGPT: Recurrent Convolutional Transformer for Long Time-series Representation(http://arxiv.org/abs/2312.00817)</code></li>
<li>Summary: <p>Pre-trained models (PTMs) have gained prominence in Natural Language
Processing and Computer Vision domains. When it comes to time-series PTMs,
their development has been limited. Previous research on time-series
transformers has mainly been devoted to small-scale tasks, yet these models
have not consistently outperformed traditional models. Additionally, the
performance of these transformers on large-scale data remains unexplored. These
findings raise doubts about Transformer's capabilities to scale up and capture
temporal dependencies. In this study, we re-examine time-series transformers
and identify the shortcomings of prior studies. Drawing from these insights, we
then introduce a pioneering architecture called Timely Generative Pre-trained
Transformer (\model). This architecture integrates recurrent attention and
temporal convolution modules to effectively capture global-local temporal
dependencies in long sequences. The relative position embedding with time decay
can effectively deal with trend and periodic patterns from time-series. Our
experiments show that \model~excels in modeling continuously monitored
biosignal as well as irregularly-sampled time-series data commonly observed in
longitudinal electronic health records. This breakthrough suggests a priority
shift in time-series deep learning research, moving from small-scale modeling
from scratch to large-scale pre-training.
</p></li>
</ul>

<h3>Title: Spatiotemporal Transformer for Imputing Sparse Data: A Deep Learning Approach. (arXiv:2312.00963v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00963">http://arxiv.org/abs/2312.00963</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00963]] Spatiotemporal Transformer for Imputing Sparse Data: A Deep Learning Approach(http://arxiv.org/abs/2312.00963)</code></li>
<li>Summary: <p>Effective management of environmental resources and agricultural
sustainability heavily depends on accurate soil moisture data. However,
datasets like the SMAP/Sentinel-1 soil moisture product often contain missing
values across their spatiotemporal grid, which poses a significant challenge.
This paper introduces a novel Spatiotemporal Transformer model (ST-Transformer)
specifically designed to address the issue of missing values in sparse
spatiotemporal datasets, particularly focusing on soil moisture data. The
ST-Transformer employs multiple spatiotemporal attention layers to capture the
complex spatiotemporal correlations in the data and can integrate additional
spatiotemporal covariates during the imputation process, thereby enhancing its
accuracy. The model is trained using a self-supervised approach, enabling it to
autonomously predict missing values from observed data points. Our model's
efficacy is demonstrated through its application to the SMAP 1km soil moisture
data over a 36 x 36 km grid in Texas. It showcases superior accuracy compared
to well-known imputation methods. Additionally, our simulation studies on other
datasets highlight the model's broader applicability in various spatiotemporal
imputation tasks.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts. (arXiv:2312.00968v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00968">http://arxiv.org/abs/2312.00968</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00968]] Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts(http://arxiv.org/abs/2312.00968)</code></li>
<li>Summary: <p>Large multi-modal models (LMMs) exhibit remarkable performance across
numerous tasks. However, generalist LMMs often suffer from performance
degradation when tuned over a large collection of tasks. Recent research
suggests that Mixture of Experts (MoE) architectures are useful for instruction
tuning, but for LMMs of parameter size around O(50-100B), the prohibitive cost
of replicating and storing the expert models severely limits the number of
experts we can use. We propose Omni-SMoLA, an architecture that uses the Soft
MoE approach to (softly) mix many multimodal low rank experts, and avoids
introducing a significant number of new parameters compared to conventional MoE
models. The core intuition here is that the large model provides a foundational
backbone, while different lightweight experts residually learn specialized
knowledge, either per-modality or multimodally. Extensive experiments
demonstrate that the SMoLA approach helps improve the generalist performance
across a broad range of generative vision-and-language tasks, achieving new
SoTA generalist performance that often matches or outperforms single
specialized LMM baselines, as well as new SoTA specialist performance.
</p></li>
</ul>

<h3>Title: Gender inference: can chatGPT outperform common commercial tools?. (arXiv:2312.00805v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00805">http://arxiv.org/abs/2312.00805</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00805]] Gender inference: can chatGPT outperform common commercial tools?(http://arxiv.org/abs/2312.00805)</code></li>
<li>Summary: <p>An increasing number of studies use gender information to understand
phenomena such as gender bias, inequity in access and participation, or the
impact of the Covid pandemic response. Unfortunately, most datasets do not
include self-reported gender information, making it necessary for researchers
to infer gender from other information, such as names or names and country
information. An important limitation of these tools is that they fail to
appropriately capture the fact that gender exists on a non-binary scale,
however, it remains important to evaluate and compare how well these tools
perform in a variety of contexts. In this paper, we compare the performance of
a generative Artificial Intelligence (AI) tool ChatGPT with three commercially
available list-based and machine learning-based gender inference tools (Namsor,
Gender-API, and genderize.io) on a unique dataset. Specifically, we use a large
Olympic athlete dataset and report how variations in the input (e.g., first
name and first and last name, with and without country information) impact the
accuracy of their predictions. We report results for the full set, as well as
for the subsets: medal versus non-medal winners, athletes from the largest
English-speaking countries, and athletes from East Asia. On these sets, we find
that Namsor is the best traditional commercially available tool. However,
ChatGPT performs at least as well as Namsor and often outperforms it,
especially for the female sample when country and/or last name information is
available. All tools perform better on medalists versus non-medalists and on
names from English-speaking countries. Although not designed for this purpose,
ChatGPT may be a cost-effective tool for gender prediction. In the future, it
might even be possible for ChatGPT or other large scale language models to
better identify self-reported gender rather than report gender on a binary
scale.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Zero-Shot Video Question Answering with Procedural Programs. (arXiv:2312.00937v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00937">http://arxiv.org/abs/2312.00937</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00937]] Zero-Shot Video Question Answering with Procedural Programs(http://arxiv.org/abs/2312.00937)</code></li>
<li>Summary: <p>We propose to answer zero-shot questions about videos by generating short
procedural programs that derive a final answer from solving a sequence of
visual subtasks. We present Procedural Video Querying (ProViQ), which uses a
large language model to generate such programs from an input question and an
API of visual modules in the prompt, then executes them to obtain the output.
Recent similar procedural approaches have proven successful for image question
answering, but videos remain challenging: we provide ProViQ with modules
intended for video understanding, allowing it to generalize to a wide variety
of videos. This code generation framework additionally enables ProViQ to
perform other video tasks in addition to question answering, such as
multi-object tracking or basic video editing. ProViQ achieves state-of-the-art
results on a diverse range of benchmarks, with improvements of up to 25% on
short, long, open-ended, and multimodal video question-answering datasets. Our
project page is at https://rccchoudhury.github.io/proviq2023.
</p></li>
</ul>

<h3>Title: Automatic detection of problem-gambling signs from online texts using large language models. (arXiv:2312.00804v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00804">http://arxiv.org/abs/2312.00804</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00804]] Automatic detection of problem-gambling signs from online texts using large language models(http://arxiv.org/abs/2312.00804)</code></li>
<li>Summary: <p>Problem gambling is a major public health concern and is associated with
profound psychological distress and economic problems. There are numerous
gambling communities on the internet where users exchange information about
games, gambling tactics, as well as gambling-related problems. Individuals
exhibiting higher levels of problem gambling engage more in such communities.
Online gambling communities may provide insights into problem-gambling
behaviour. Using data scraped from a major German gambling discussion board, we
fine-tuned a large language model, specifically a Bidirectional Encoder
Representations from Transformers (BERT) model, to predict signs of
problem-gambling from forum posts. Training data were generated by manual
annotation and by taking into account diagnostic criteria and gambling-related
cognitive distortions. Using k-fold cross-validation, our models achieved a
precision of 0.95 and F1 score of 0.71, demonstrating that satisfactory
classification performance can be achieved by generating high-quality training
material through manual annotation based on diagnostic criteria. The current
study confirms that a BERT-based model can be reliably used on small data sets
and to detect signatures of problem gambling in online communication data. Such
computational approaches may have potential for the detection of changes in
problem-gambling prevalence among online users.
</p></li>
</ul>

<h3>Title: Large Language Models for Travel Behavior Prediction. (arXiv:2312.00819v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00819">http://arxiv.org/abs/2312.00819</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00819]] Large Language Models for Travel Behavior Prediction(http://arxiv.org/abs/2312.00819)</code></li>
<li>Summary: <p>Travel behavior prediction is a fundamental task in transportation demand
management. The conventional methods for travel behavior prediction rely on
numerical data to construct mathematical models and calibrate model parameters
to represent human preferences. Recent advancement in large language models
(LLMs) has shown great reasoning abilities to solve complex problems. In this
study, we propose to use LLMs to predict travel behavior with prompt
engineering without data-based parameter learning. Specifically, we carefully
design our prompts that include 1) task description, 2) travel characteristics,
3) individual attributes, and 4) guides of thinking with domain knowledge, and
ask the LLMs to predict an individual's travel behavior and explain the
results. We select the travel mode choice task as a case study. Results show
that, though no training samples are provided, LLM-based predictions have
competitive accuracy and F1-score as canonical supervised learning methods such
as multinomial logit, random forest, and neural networks. LLMs can also output
reasons that support their prediction. However, though in most of the cases,
the output explanations are reasonable, we still observe cases that violate
logic or with hallucinations.
</p></li>
</ul>

<h3>Title: Hyperparameter Optimization for Large Language Model Instruction-Tuning. (arXiv:2312.00949v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00949">http://arxiv.org/abs/2312.00949</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00949]] Hyperparameter Optimization for Large Language Model Instruction-Tuning(http://arxiv.org/abs/2312.00949)</code></li>
<li>Summary: <p>The fine-tuning of Large Language Models (LLMs) has enabled them to recently
achieve milestones in natural language processing applications. The emergence
of ever larger LLMs has paved the way for more efficient fine-tuning methods.
Among these, the Low-Rank Adaptation (LoRA) method keeps most of the weights of
the pre-trained LLM frozen while introducing a low-rank decomposition of the
weight matrix, enabling the tuning of only a very small proportion of the
network. The performance on downstream tasks of models fine-tuned with LoRA
heavily relies on a set of hyperparameters including the rank of the
decomposition. In this work, we investigate the choice of these hyperparameters
through two main blackbox optimization (BBO) techniques. We examine the whole
pipeline of performing fine-tuning and validation on a pre-trained LLM as a
blackbox and efficiently explore the space of hyperparameters with the \nomad
algorithm, achieving a boost in performance and human alignment of the tuned
model.
</p></li>
</ul>

<h3>Title: The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models. (arXiv:2312.00960v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00960">http://arxiv.org/abs/2312.00960</a></li>
<li>Code URL: https://github.com/namburisrinath/llmcompression</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00960]] The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models(http://arxiv.org/abs/2312.00960)</code></li>
<li>Summary: <p>Compressing large language models (LLMs), often consisting of billions of
parameters, provides faster inference, smaller memory footprints, and enables
local deployment. Two standard compression techniques are pruning and
quantization, with the former eliminating redundant connections in model layers
and the latter representing model parameters with fewer bits. The key tradeoff
is between the degree of compression and the impact on the quality of the
compressed model. Existing research on LLM compression primarily focuses on
performance in terms of general metrics like perplexity or downstream task
accuracy. More fine-grained metrics, such as those measuring parametric
knowledge, remain significantly underexplored. To help bridge this gap, we
present a comprehensive analysis across multiple model families (ENCODER,
ENCODER-DECODER, and DECODER) using the LAMA and LM-HARNESS benchmarks in order
to systematically quantify the effect of commonly employed compression
techniques on model performance. A particular focus is on tradeoffs involving
parametric knowledge, with the goal of providing practitioners with practical
insights to help make informed decisions on compression. We release our
codebase1 to enable further research.
</p></li>
</ul>

<h3>Title: Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models. (arXiv:2312.01032v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.01032">http://arxiv.org/abs/2312.01032</a></li>
<li>Code URL: https://github.com/my625/promptqg</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.01032]] Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models(http://arxiv.org/abs/2312.01032)</code></li>
<li>Summary: <p>Designing high-quality educational questions is a challenging and
time-consuming task. In this work, we propose a novel approach that utilizes
prompt-based techniques to generate descriptive and reasoning-based questions.
However, current question-answering (QA) datasets are inadequate for conducting
our experiments on prompt-based question generation (QG) in an educational
setting. Therefore, we curate a new QG dataset called EduProbe for school-level
subjects, by leveraging the rich content of NCERT textbooks. We carefully
annotate this dataset as quadruples of 1) Context: a segment upon which the
question is formed; 2) Long Prompt: a long textual cue for the question (i.e.,
a longer sequence of words or phrases, covering the main theme of the context);
3) Short Prompt: a short textual cue for the question (i.e., a condensed
representation of the key information or focus of the context); 4) Question: a
deep question that aligns with the context and is coherent with the prompts. We
investigate several prompt-based QG methods by fine-tuning pre-trained
transformer-based large language models (LLMs), namely PEGASUS, T5, MBART, and
BART. Moreover, we explore the performance of two general-purpose pre-trained
LLMs such as Text-Davinci-003 and GPT-3.5-Turbo without any further training.
By performing automatic evaluation, we show that T5 (with long prompt)
outperforms all other models, but still falls short of the human baseline.
Under human evaluation criteria, TextDavinci-003 usually shows better results
than other models under various prompt settings. Even in the case of human
evaluation criteria, QG models mostly fall short of the human baseline. Our
code and dataset are available at: https://github.com/my625/PromptQG
</p></li>
</ul>

<h3>Title: From Beginner to Expert: Modeling Medical Knowledge into General LLMs. (arXiv:2312.01040v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.01040">http://arxiv.org/abs/2312.01040</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.01040]] From Beginner to Expert: Modeling Medical Knowledge into General LLMs(http://arxiv.org/abs/2312.01040)</code></li>
<li>Summary: <p>Recently, large language model (LLM) based artificial intelligence (AI)
systems have demonstrated remarkable capabilities in natural language
understanding and generation. However, these models face a significant
challenge when it comes to sensitive applications, such as reasoning over
medical knowledge and answering medical questions in a physician-like manner.
Prior studies attempted to overcome this challenge by increasing the model size
(&gt;100B) to learn more general medical knowledge, while there is still room for
improvement in LLMs with smaller-scale model sizes (&lt;100B). In this work, we
start from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from a
medical beginner towards a medical expert (called AntGLM-Med-10B), which
leverages a 3-stage optimization procedure, \textit{i.e.}, general medical
knowledge injection, medical domain instruction tuning, and specific medical
task adaptation. Our contributions are threefold: (1) We specifically
investigate how to adapt a pre-trained general LLM in medical domain,
especially for a specific medical task. (2) We collect and construct
large-scale medical datasets for each stage of the optimization process. These
datasets encompass various data types and tasks, such as question-answering,
medical reasoning, multi-choice questions, and medical conversations. (3)
Specifically for multi-choice questions in the medical domain, we propose a
novel Verification-of-Choice approach for prompting engineering, which
significantly enhances the reasoning ability of LLMs. Remarkably, by combining
the above approaches, our AntGLM-Med-10B model can outperform the most of LLMs
on PubMedQA, including both general and medical LLMs, even when these LLMs have
larger model size.
</p></li>
</ul>

<h3>Title: Large Language Models Are Zero-Shot Text Classifiers. (arXiv:2312.01044v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.01044">http://arxiv.org/abs/2312.01044</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.01044]] Large Language Models Are Zero-Shot Text Classifiers(http://arxiv.org/abs/2312.01044)</code></li>
<li>Summary: <p>Retrained large language models (LLMs) have become extensively used across
various sub-disciplines of natural language processing (NLP). In NLP, text
classification problems have garnered considerable focus, but still faced with
some limitations related to expensive computational cost, time consumption, and
robust performance to unseen classes. With the proposal of chain of thought
prompting (CoT), LLMs can be implemented using zero-shot learning (ZSL) with
the step by step reasoning prompts, instead of conventional question and answer
formats. The zero-shot LLMs in the text classification problems can alleviate
these limitations by directly utilizing pretrained models to predict both seen
and unseen classes. Our research primarily validates the capability of GPT
models in text classification. We focus on effectively utilizing prompt
strategies to various text classification scenarios. Besides, we compare the
performance of zero shot LLMs with other state of the art text classification
methods, including traditional machine learning methods, deep learning methods,
and ZSL methods. Experimental results demonstrate that the performance of LLMs
underscores their effectiveness as zero-shot text classifiers in three of the
four datasets analyzed. The proficiency is especially advantageous for small
businesses or teams that may not have extensive knowledge in text
classification.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Talent-Interview: Web-Client Cheating Detection for Online Exams. (arXiv:2312.00795v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00795">http://arxiv.org/abs/2312.00795</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00795]] Talent-Interview: Web-Client Cheating Detection for Online Exams(http://arxiv.org/abs/2312.00795)</code></li>
<li>Summary: <p>Online exams are more attractive after the Covid-19 pandemic. Furthermore,
during recruitment, online exams are used. However, there are more cheating
possibilities for online exams. Assigning a proctor for each exam increases
cost. At this point, automatic proctor systems detect possible cheating status.
This article proposes an end-to-end system and submodules to get better results
for online proctoring. Object detection, face recognition, human voice
detection, and segmentation are used in our system. Furthermore, our proposed
model works on the PCs of users, meaning a client-based system. So, server cost
is eliminated. As far as we know, it is the first time the client-based online
proctoring system has been used for recruitment. Online exams are more
attractive after the Covid-19 pandemic. Furthermore, during recruitment, online
exams are used. However, there are more cheating possibilities for online
exams. Assigning a proctor for each exam increases cost. At this point,
automatic proctor systems detect possible cheating status. This article
proposes an end-to-end system and submodules to get better results for online
proctoring. Object detection, face recognition, human voice detection, and
segmentation are used in our system. Furthermore, our proposed model works on
the PCs of users, meaning a client-based system. So, server cost is eliminated.
As far as we know, it is the first time the client-based online proctoring
system has been used for recruitment. Furthermore, this cheating system works
at https://www.talent-interview.com/tr/.
</p></li>
</ul>

<h3>Title: Segment Any 3D Gaussians. (arXiv:2312.00860v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00860">http://arxiv.org/abs/2312.00860</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00860]] Segment Any 3D Gaussians(http://arxiv.org/abs/2312.00860)</code></li>
<li>Summary: <p>Interactive 3D segmentation in radiance fields is an appealing task since its
importance in 3D scene understanding and manipulation. However, existing
methods face challenges in either achieving fine-grained, multi-granularity
segmentation or contending with substantial computational overhead, inhibiting
real-time interaction. In this paper, we introduce Segment Any 3D GAussians
(SAGA), a novel 3D interactive segmentation approach that seamlessly blends a
2D segmentation foundation model with 3D Gaussian Splatting (3DGS), a recent
breakthrough of radiance fields. SAGA efficiently embeds multi-granularity 2D
segmentation results generated by the segmentation foundation model into 3D
Gaussian point features through well-designed contrastive training. Evaluation
on existing benchmarks demonstrates that SAGA can achieve competitive
performance with state-of-the-art methods. Moreover, SAGA achieves
multi-granularity segmentation and accommodates various prompts, including
points, scribbles, and 2D masks. Notably, SAGA can finish the 3D segmentation
within milliseconds, achieving nearly 1000x acceleration compared to previous
SOTA. The project page is at https://jumpat.github.io/SAGA.
</p></li>
</ul>

<h3>Title: Segment and Caption Anything. (arXiv:2312.00869v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00869">http://arxiv.org/abs/2312.00869</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00869]] Segment and Caption Anything(http://arxiv.org/abs/2312.00869)</code></li>
<li>Summary: <p>We propose a method to efficiently equip the Segment Anything Model (SAM)
with the ability to generate regional captions. SAM presents strong
generalizability to segment anything while is short for semantic understanding.
By introducing a lightweight query-based feature mixer, we align the
region-specific features with the embedding space of language models for later
caption generation. As the number of trainable parameters is small (typically
in the order of tens of millions), it costs less computation, less memory
usage, and less communication bandwidth, resulting in both fast and scalable
training. To address the scarcity problem of regional caption data, we propose
to first pre-train our model on objection detection and segmentation tasks. We
call this step weak supervision pretraining since the pre-training data only
contains category names instead of full-sentence descriptions. The weak
supervision pretraining allows us to leverage many publicly available object
detection and segmentation datasets. We conduct extensive experiments to
demonstrate the superiority of our method and validate each design choice. This
work serves as a stepping stone towards scaling up regional captioning data and
sheds light on exploring efficient ways to augment SAM with regional semantics.
The project page, along with the associated code, can be accessed via the
following https://xk-huang.github.io/segment-caption-anything/.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
