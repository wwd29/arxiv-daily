<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-03-21</h1>
<h3>Title: Identifying Likely-Reputable Blockchain Projects on Ethereum</h3>
<ul>
<li><strong>Authors: </strong>Cyrus Malik, Josef Bajada, Joshua Ellul</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15542">https://arxiv.org/abs/2503.15542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15542">https://arxiv.org/pdf/2503.15542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15542]] Identifying Likely-Reputable Blockchain Projects on Ethereum(https://arxiv.org/abs/2503.15542)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust</a></li>
<li><strong>Abstract: </strong>Identifying reputable Ethereum projects remains a critical challenge within the expanding blockchain ecosystem. The ability to distinguish between legitimate initiatives and potentially fraudulent schemes is non-trivial. This work presents a systematic approach that integrates multiple data sources with advanced analytics to evaluate credibility, transparency, and overall trustworthiness. The methodology applies machine learning techniques to analyse transaction histories on the Ethereum blockchain. The study classifies accounts based on a dataset comprising 2,179 entities linked to illicit activities and 3,977 associated with reputable projects. Using the LightGBM algorithm, the approach achieves an average accuracy of 0.984 and an average AUC of 0.999, validated through 10-fold cross-validation. Key influential factors include time differences between transactions and received_tnx. The proposed methodology provides a robust mechanism for identifying reputable Ethereum projects, fostering a more secure and transparent investment environment. By equipping stakeholders with data-driven insights, this research enables more informed decision-making, risk mitigation, and the promotion of legitimate blockchain initiatives. Furthermore, it lays the foundation for future advancements in trust assessment methodologies, contributing to the continued development and maturity of the Ethereum ecosystem.</li>
</ul>

<h3>Title: Data-Driven Approximation of Binary-State Network Reliability Function: Algorithm Selection and Reliability Thresholds for Large-Scale Systems</h3>
<ul>
<li><strong>Authors: </strong>Wei-Chang Yeh</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15545">https://arxiv.org/abs/2503.15545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15545">https://arxiv.org/pdf/2503.15545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15545]] Data-Driven Approximation of Binary-State Network Reliability Function: Algorithm Selection and Reliability Thresholds for Large-Scale Systems(https://arxiv.org/abs/2503.15545)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Network reliability assessment is pivotal for ensuring the robustness of modern infrastructure systems, from power grids to communication networks. While exact reliability computation for binary-state networks is NP-hard, existing approximation methods face critical tradeoffs between accuracy, scalability, and data efficiency. This study evaluates 20 machine learning methods across three reliability regimes full range (0.0-1.0), high reliability (0.9-1.0), and ultra high reliability (0.99-1.0) to address these gaps. We demonstrate that large-scale networks with arc reliability larger than or equal to 0.9 exhibit near-unity system reliability, enabling computational simplifications. Further, we establish a dataset-scale-driven paradigm for algorithm selection: Artificial Neural Networks (ANN) excel with limited data, while Polynomial Regression (PR) achieves superior accuracy in data-rich environments. Our findings reveal ANN's Test-MSE of 7.24E-05 at 30,000 samples and PR's optimal performance (5.61E-05) at 40,000 samples, outperforming traditional Monte Carlo simulations. These insights provide actionable guidelines for balancing accuracy, interpretability, and computational efficiency in reliability engineering, with implications for infrastructure resilience and system optimization.</li>
</ul>

<h3>Title: Enforcing Cybersecurity Constraints for LLM-driven Robot Agents for Online Transactions</h3>
<ul>
<li><strong>Authors: </strong>Shraddha Pradipbhai Shah, Aditya Vilas Deshpande</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15546">https://arxiv.org/abs/2503.15546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15546">https://arxiv.org/pdf/2503.15546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15546]] Enforcing Cybersecurity Constraints for LLM-driven Robot Agents for Online Transactions(https://arxiv.org/abs/2503.15546)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust, large language model</a></li>
<li><strong>Abstract: </strong>The integration of Large Language Models (LLMs) into autonomous robotic agents for conducting online transactions poses significant cybersecurity challenges. This study aims to enforce robust cybersecurity constraints to mitigate the risks associated with data breaches, transaction fraud, and system manipulation. The background focuses on the rise of LLM-driven robotic systems in e-commerce, finance, and service industries, alongside the vulnerabilities they introduce. A novel security architecture combining blockchain technology with multi-factor authentication (MFA) and real-time anomaly detection was implemented to safeguard transactions. Key performance metrics such as transaction integrity, response time, and breach detection accuracy were evaluated, showing improved security and system performance. The results highlight that the proposed architecture reduced fraudulent transactions by 90%, improved breach detection accuracy to 98%, and ensured secure transaction validation within a latency of 0.05 seconds. These findings emphasize the importance of cybersecurity in the deployment of LLM-driven robotic systems and suggest a framework adaptable to various online platforms.</li>
</ul>

<h3>Title: Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Juhee Kim, Woohyuk Choi, Byoungyoung Lee</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15547">https://arxiv.org/abs/2503.15547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15547">https://arxiv.org/pdf/2503.15547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15547]] Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents(https://arxiv.org/abs/2503.15547)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are combined with plugins to create powerful LLM agents that provide a wide range of services. Unlike traditional software, LLM agent's behavior is determined at runtime by natural language prompts from either user or plugin's data. This flexibility enables a new computing paradigm with unlimited capabilities and programmability, but also introduces new security risks, vulnerable to privilege escalation attacks. Moreover, user prompt is prone to be interpreted in an insecure way by LLM agents, creating non-deterministic behaviors that can be exploited by attackers. To address these security risks, we propose Prompt Flow Integrity (PFI), a system security-oriented solution to prevent privilege escalation in LLM agents. Analyzing the architectural characteristics of LLM agents, PFI features three mitigation techniques -- i.e., untrusted data identification, enforcing least privilege on LLM agents, and validating unsafe data flows. Our evaluation result shows that PFI effectively mitigates privilege escalation attacks while successfully preserving the utility of LLM agents.</li>
</ul>

<h3>Title: Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Pengcheng Zhou, Yinglun Feng, Zhongliang Yang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15548">https://arxiv.org/abs/2503.15548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15548">https://arxiv.org/pdf/2503.15548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15548]] Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval(https://arxiv.org/abs/2503.15548)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>The widespread adoption of Retrieval-Augmented Generation (RAG) systems in real-world applications has heightened concerns about the confidentiality and integrity of their proprietary knowledge bases. These knowledge bases, which play a critical role in enhancing the generative capabilities of Large Language Models (LLMs), are increasingly vulnerable to breaches that could compromise sensitive information. To address these challenges, this paper proposes an advanced encryption methodology designed to protect RAG systems from unauthorized access and data leakage. Our approach encrypts both textual content and its corresponding embeddings prior to storage, ensuring that all data remains securely encrypted. This mechanism restricts access to authorized entities with the appropriate decryption keys, thereby significantly reducing the risk of unintended data exposure. Furthermore, we demonstrate that our encryption strategy preserves the performance and functionality of RAG pipelines, ensuring compatibility across diverse domains and applications. To validate the robustness of our method, we provide comprehensive security proofs that highlight its resilience against potential threats and vulnerabilities. These proofs also reveal limitations in existing approaches, which often lack robustness, adaptability, or reliance on open-source models. Our findings suggest that integrating advanced encryption techniques into the design and deployment of RAG systems can effectively enhance privacy safeguards. This research contributes to the ongoing discourse on improving security measures for AI-driven services and advocates for stricter data protection standards within RAG architectures.</li>
</ul>

<h3>Title: Zero-Knowledge Federated Learning: A New Trustworthy and Privacy-Preserving Distributed Learning Paradigm</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Jin, Taotao Wang, Qing Yang, Long Shi, Shengli Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15550">https://arxiv.org/abs/2503.15550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15550">https://arxiv.org/pdf/2503.15550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15550]] Zero-Knowledge Federated Learning: A New Trustworthy and Privacy-Preserving Distributed Learning Paradigm(https://arxiv.org/abs/2503.15550)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has emerged as a promising paradigm in distributed machine learning, enabling collaborative model training while preserving data privacy. However, despite its many advantages, FL still contends with significant challenges -- most notably regarding security and trust. Zero-Knowledge Proofs (ZKPs) offer a potential solution by establishing trust and enhancing system integrity throughout the FL process. Although several studies have explored ZKP-based FL (ZK-FL), a systematic framework and comprehensive analysis are still lacking. This article makes two key contributions. First, we propose a structured ZK-FL framework that categorizes and analyzes the technical roles of ZKPs across various FL stages and tasks. Second, we introduce a novel algorithm, Verifiable Client Selection FL (Veri-CS-FL), which employs ZKPs to refine the client selection process. In Veri-CS-FL, participating clients generate verifiable proofs for the performance metrics of their local models and submit these concise proofs to the server for efficient verification. The server then selects clients with high-quality local models for uploading, subsequently aggregating the contributions from these selected clients. By integrating ZKPs, Veri-CS-FL not only ensures the accuracy of performance metrics but also fortifies trust among participants while enhancing the overall efficiency and security of FL systems.</li>
</ul>

<h3>Title: Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack</h3>
<ul>
<li><strong>Authors: </strong>Murong Yue, Ziyu Yao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15551">https://arxiv.org/abs/2503.15551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15551">https://arxiv.org/pdf/2503.15551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15551]] Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack(https://arxiv.org/abs/2503.15551)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Batch prompting, which combines a batch of multiple queries sharing the same context in one inference, has emerged as a promising solution to reduce inference costs. However, our study reveals a significant security vulnerability in batch prompting: malicious users can inject attack instructions into a batch, leading to unwanted interference across all queries, which can result in the inclusion of harmful content, such as phishing links, or the disruption of logical reasoning. In this paper, we construct BATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of two types and 8k batch instances, to study the batch prompting vulnerability systematically. Our evaluation of both closed-source and open-weight LLMs demonstrates that all LLMs are susceptible to batch-prompting attacks. We then explore multiple defending approaches. While the prompting-based defense shows limited effectiveness for smaller LLMs, the probing-based approach achieves about 95% accuracy in detecting attacks. Additionally, we perform a mechanistic analysis to understand the attack and identify attention heads that are responsible for it.</li>
</ul>

<h3>Title: Personalized Attacks of Social Engineering in Multi-turn Conversations -- LLM Agents for Simulation and Detection</h3>
<ul>
<li><strong>Authors: </strong>Tharindu Kumarage, Cameron Johnson, Jadie Adams, Lin Ai, Matthias Kirchner, Anthony Hoogs, Joshua Garland, Julia Hirschberg, Arslan Basharat, Huan Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15552">https://arxiv.org/abs/2503.15552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15552">https://arxiv.org/pdf/2503.15552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15552]] Personalized Attacks of Social Engineering in Multi-turn Conversations -- LLM Agents for Simulation and Detection(https://arxiv.org/abs/2503.15552)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of conversational agents, particularly chatbots powered by Large Language Models (LLMs), poses a significant risk of social engineering (SE) attacks on social media platforms. SE detection in multi-turn, chat-based interactions is considerably more complex than single-instance detection due to the dynamic nature of these conversations. A critical factor in mitigating this threat is understanding the mechanisms through which SE attacks operate, specifically how attackers exploit vulnerabilities and how victims' personality traits contribute to their susceptibility. In this work, we propose an LLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by generating multi-turn conversations. We model victim agents with varying personality traits to assess how psychological profiles influence susceptibility to manipulation. Using a dataset of over 1000 simulated conversations, we examine attack scenarios in which adversaries, posing as recruiters, funding agencies, and journalists, attempt to extract sensitive information. Based on this analysis, we present a proof of concept, SE-OmniGuard, to offer personalized protection to users by leveraging prior knowledge of the victims personality, evaluating attack strategies, and monitoring information exchanges in conversations to identify potential SE attempts.</li>
</ul>

<h3>Title: A Comprehensive Study of LLM Secure Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Shih-Chieh Dai, Jun Xu, Guanhong Tao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15554">https://arxiv.org/abs/2503.15554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15554">https://arxiv.org/pdf/2503.15554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15554]] A Comprehensive Study of LLM Secure Code Generation(https://arxiv.org/abs/2503.15554)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>LLMs are widely used in software development. However, the code generated by LLMs often contains vulnerabilities. Several secure code generation methods have been proposed to address this issue, but their current evaluation schemes leave several concerns unaddressed. Specifically, most existing studies evaluate security and functional correctness separately, using different datasets. That is, they assess vulnerabilities using security-related code datasets while validating functionality with general code datasets. In addition, prior research primarily relies on a single static analyzer, CodeQL, to detect vulnerabilities in generated code, which limits the scope of security evaluation. In this work, we conduct a comprehensive study to systematically assess the improvements introduced by four state-of-the-art secure code generation techniques. Specifically, we apply both security inspection and functionality validation to the same generated code and evaluate these two aspects together. We also employ three popular static analyzers and two LLMs to identify potential vulnerabilities in the generated code. Our study reveals that existing techniques often compromise the functionality of generated code to enhance security. Their overall performance remains limited when evaluating security and functionality together. In fact, many techniques even degrade the performance of the base LLM. Our further inspection reveals that these techniques often either remove vulnerable lines of code entirely or generate ``garbage code'' that is unrelated to the intended task. Moreover, the commonly used static analyzer CodeQL fails to detect several vulnerabilities, further obscuring the actual security improvements achieved by existing techniques. Our study serves as a guideline for a more rigorous and comprehensive evaluation of secure code generation performance in future work.</li>
</ul>

<h3>Title: Advanced Relay-Based Collaborative Framework for Optimizing Synchronization in Split Federated Learning over Wireless Networks</h3>
<ul>
<li><strong>Authors: </strong>Haoran Gao, Samuel D. Okegbile, Jun Cai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15559">https://arxiv.org/abs/2503.15559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15559">https://arxiv.org/pdf/2503.15559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15559]] Advanced Relay-Based Collaborative Framework for Optimizing Synchronization in Split Federated Learning over Wireless Networks(https://arxiv.org/abs/2503.15559)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Split Federated Learning (SFL) offers a promising approach for distributed model training in edge computing, combining the strengths of split learning in reducing computational demands on edge devices and enhancing data privacy, with the role of federated aggregation to ensure model convergence and synchronization across users. However, synchronization issues caused by user heterogeneity have hindered the development of the framework. To optimize synchronization efficiency among users and improve overall system performance, we propose a collaborative SFL framework (CSFL). Based on the model's partitioning capabilities, we design a mechanism called the collaborative relay optimization mechanism (CROM), where the assistance provided by high-efficiency users is seen as a relay process, with the portion of the model they compute acting as the relay point. Wireless communication between users facilitates real-time collaboration, allowing high-efficiency users to assist bottleneck users in handling part of the model's computation, thereby alleviating the computational load on bottleneck users. Simulation results show that our proposed CSFL framework reduces synchronization delays and improves overall system throughput while maintaining similar performance and convergence rate to the SFL framework. This demonstrates that the collaboration not only reduces synchronization waiting time but also accelerates model convergence.</li>
</ul>

<h3>Title: Temporal Context Awareness: A Defense Framework Against Multi-turn Manipulation Attacks on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Prashant Kulkarni, Assaf Namer</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15560">https://arxiv.org/abs/2503.15560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15560">https://arxiv.org/pdf/2503.15560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15560]] Temporal Context Awareness: A Defense Framework Against Multi-turn Manipulation Attacks on Large Language Models(https://arxiv.org/abs/2503.15560)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly vulnerable to sophisticated multi-turn manipulation attacks, where adversaries strategically build context through seemingly benign conversational turns to circumvent safety measures and elicit harmful or unauthorized responses. These attacks exploit the temporal nature of dialogue to evade single-turn detection methods, representing a critical security vulnerability with significant implications for real-world deployments. This paper introduces the Temporal Context Awareness (TCA) framework, a novel defense mechanism designed to address this challenge by continuously analyzing semantic drift, cross-turn intention consistency and evolving conversational patterns. The TCA framework integrates dynamic context embedding analysis, cross-turn consistency verification, and progressive risk scoring to detect and mitigate manipulation attempts effectively. Preliminary evaluations on simulated adversarial scenarios demonstrate the framework's potential to identify subtle manipulation patterns often missed by traditional detection techniques, offering a much-needed layer of security for conversational AI systems. In addition to outlining the design of TCA , we analyze diverse attack vectors and their progression across multi-turn conversation, providing valuable insights into adversarial tactics and their impact on LLM vulnerabilities. Our findings underscore the pressing need for robust, context-aware defenses in conversational AI systems and highlight TCA framework as a promising direction for securing LLMs while preserving their utility in legitimate applications. We make our implementation available to support further research in this emerging area of AI security.</li>
</ul>

<h3>Title: Dynamic Power Flow Analysis and Fault Characteristics: A Graph Attention Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Tan Le, Van Le</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15563">https://arxiv.org/abs/2503.15563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15563">https://arxiv.org/pdf/2503.15563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15563]] Dynamic Power Flow Analysis and Fault Characteristics: A Graph Attention Neural Network(https://arxiv.org/abs/2503.15563)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We propose the joint graph attention neural network (GAT), clustering with adaptive neighbors (CAN) and probabilistic graphical model for dynamic power flow analysis and fault characteristics. In fact, computational efficiency is the main focus to enhance, whilst we ensure the performance accuracy at the accepted level. Note that Machine Learning (ML) based schemes have a requirement of sufficient labeled data during training, which is not easily satisfied in practical applications. Also, there are unknown data due to new arrived measurements or incompatible smart devices in complex smart grid systems. These problems would be resolved by our proposed GAT based framework, which models the label dependency between the network data and learns object representations such that it could achieve the semi-supervised fault diagnosis. To create the joint label dependency, we develop the graph construction from the raw acquired signals by using CAN. Next, we develop the probabilistic graphical model of Markov random field for graph representation, which supports for the GAT based framework. We then evaluate the proposed framework in the use-case application in smart grid and make a fair comparison to the existing methods.</li>
</ul>

<h3>Title: GReaTER: Generate Realistic Tabular data after data Enhancement and Reduction</h3>
<ul>
<li><strong>Authors: </strong>Tung Sum Thomas Kwok, Chi-Hua Wang, Guang Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15564">https://arxiv.org/abs/2503.15564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15564">https://arxiv.org/pdf/2503.15564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15564]] GReaTER: Generate Realistic Tabular data after data Enhancement and Reduction(https://arxiv.org/abs/2503.15564)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tabular data synthesis involves not only multi-table synthesis but also generating multi-modal data (e.g., strings and categories), which enables diverse knowledge synthesis. However, separating numerical and categorical data has limited the effectiveness of tabular data generation. The GReaT (Generate Realistic Tabular Data) framework uses Large Language Models (LLMs) to encode entire rows, eliminating the need to partition data types. Despite this, the framework's performance is constrained by two issues: (1) tabular data entries lack sufficient semantic meaning, limiting LLM's ability to leverage pre-trained knowledge for in-context learning, and (2) complex multi-table datasets struggle to establish effective relationships for collaboration. To address these, we propose GReaTER (Generate Realistic Tabular Data after data Enhancement and Reduction), which includes: (1) a data semantic enhancement system that improves LLM's understanding of tabular data through mapping, enabling better in-context learning, and (2) a cross-table connecting method to establish efficient relationships across complex tables. Experimental results show that GReaTER outperforms the GReaT framework.</li>
</ul>

<h3>Title: Enforcing Consistency and Fairness in Multi-level Hierarchical Classification with a Mask-based Output Layer</h3>
<ul>
<li><strong>Authors: </strong>Shijing Chen, Shoaib Jameel, Mohamed Reda Bouadjenek, Feilong Tang, Usman Naseem, Basem Suleiman, Hakim Hacid, Flora D. Salim, Imran Razzak</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15566">https://arxiv.org/abs/2503.15566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15566">https://arxiv.org/pdf/2503.15566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15566]] Enforcing Consistency and Fairness in Multi-level Hierarchical Classification with a Mask-based Output Layer(https://arxiv.org/abs/2503.15566)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, fair, large language model</a></li>
<li><strong>Abstract: </strong>Traditional Multi-level Hierarchical Classification (MLHC) classifiers often rely on backbone models with $n$ independent output layers. This structure tends to overlook the hierarchical relationships between classes, leading to inconsistent predictions that violate the underlying taxonomy. Additionally, once a backbone architecture for an MLHC classifier is selected, adapting the model to accommodate new tasks can be challenging. For example, incorporating fairness to protect sensitive attributes within a hierarchical classifier necessitates complex adjustments to maintain the class hierarchy while enforcing fairness constraints. In this paper, we extend this concept to hierarchical classification by introducing a fair, model-agnostic layer designed to enforce taxonomy and optimize specific objectives, including consistency, fairness, and exact match. Our evaluations demonstrate that the proposed layer not only improves the fairness of predictions but also enforces the taxonomy, resulting in consistent predictions and superior performance. Compared to Large Language Models (LLMs) employing in-processing de-biasing techniques and models without any bias correction, our approach achieves better outcomes in both fairness and accuracy, making it particularly valuable in sectors like e-commerce, healthcare, and education, where predictive reliability is crucial.</li>
</ul>

<h3>Title: Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yanchen Luo, Zhiyuan Liu, Yi Zhao, Sihang Li, Kenji Kawaguchi, Tat-Seng Chua, Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15567">https://arxiv.org/abs/2503.15567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15567">https://arxiv.org/pdf/2503.15567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15567]] Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling(https://arxiv.org/abs/2503.15567)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>3D molecule generation is crucial for drug discovery and material science, requiring models to process complex multi-modalities, including atom types, chemical bonds, and 3D coordinates. A key challenge is integrating these modalities of different shapes while maintaining SE(3) equivariance for 3D coordinates. To achieve this, existing approaches typically maintain separate latent spaces for invariant and equivariant modalities, reducing efficiency in both training and sampling. In this work, we propose \textbf{U}nified Variational \textbf{A}uto-\textbf{E}ncoder for \textbf{3D} Molecular Latent Diffusion Modeling (\textbf{UAE-3D}), a multi-modal VAE that compresses 3D molecules into latent sequences from a unified latent space, while maintaining near-zero reconstruction error. This unified latent space eliminates the complexities of handling multi-modality and equivariance when performing latent diffusion modeling. We demonstrate this by employing the Diffusion Transformer--a general-purpose diffusion model without any molecular inductive bias--for latent generation. Extensive experiments on GEOM-Drugs and QM9 datasets demonstrate that our method significantly establishes new benchmarks in both \textit{de novo} and conditional 3D molecule generation, achieving leading efficiency and quality.</li>
</ul>

<h3>Title: RAG-based User Profiling for Precision Planning in Mixed-precision Over-the-Air Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Jinsheng Yuan, Yun Tang, Weisi Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15569">https://arxiv.org/abs/2503.15569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15569">https://arxiv.org/pdf/2503.15569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15569]] RAG-based User Profiling for Precision Planning in Mixed-precision Over-the-Air Federated Learning(https://arxiv.org/abs/2503.15569)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Mixed-precision computing, a widely applied technique in AI, offers a larger trade-off space between accuracy and efficiency. The recent purposed Mixed-Precision Over-the-Air Federated Learning (MP-OTA-FL) enables clients to operate at appropriate precision levels based on their heterogeneous hardware, taking advantages of the larger trade-off space while covering the quantization overheads in the mixed-precision modulation scheme for the OTA aggregation process. A key to further exploring the potential of the MP-OTA-FL framework is the optimization of client precision levels. The choice of precision level hinges on multifaceted factors including hardware capability, potential client contribution, and user satisfaction, among which factors can be difficult to define or quantify. In this paper, we propose a RAG-based User Profiling for precision planning framework that integrates retrieval-augmented LLMs and dynamic client profiling to optimize satisfaction and contributions. This includes a hybrid interface for gathering device/user insights and an RAG database storing historical quantization decisions with feedback. Experiments show that our method boosts satisfaction, energy savings, and global model accuracy in MP-OTA-FL systems.</li>
</ul>

<h3>Title: Neuronal Activation States as Sample Embeddings for Data Selection in Task-Specific Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Da Ma, Gonghu Shang, Zhi Chen, Libo Qin, Yijie Luo, Lei Pan, Shuai Fan, Lu Chen, Kai Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15573">https://arxiv.org/abs/2503.15573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15573">https://arxiv.org/pdf/2503.15573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15573]] Neuronal Activation States as Sample Embeddings for Data Selection in Task-Specific Instruction Tuning(https://arxiv.org/abs/2503.15573)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Task-specific instruction tuning enhances the performance of large language models (LLMs) on specialized tasks, yet efficiently selecting relevant data for this purpose remains a challenge. Inspired by neural coactivation in the human brain, we propose a novel data selection method called NAS, which leverages neuronal activation states as embeddings for samples in the feature space. Extensive experiments show that NAS outperforms classical data selection methods in terms of both effectiveness and robustness across different models, datasets, and selection ratios.</li>
</ul>

<h3>Title: Sparseformer: a Transferable Transformer with Multi-granularity Token Sparsification for Medical Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Jiexia Ye, Weiqi Zhang, Ziyue Li, Jia Li, Fugee Tsung</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15578">https://arxiv.org/abs/2503.15578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15578">https://arxiv.org/pdf/2503.15578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15578]] Sparseformer: a Transferable Transformer with Multi-granularity Token Sparsification for Medical Time Series Classification(https://arxiv.org/abs/2503.15578)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Medical time series (MedTS) classification is crucial for improved diagnosis in healthcare, and yet it is challenging due to the varying granularity of patterns, intricate inter-channel correlation, information redundancy, and label scarcity. While existing transformer-based models have shown promise in time series analysis, they mainly focus on forecasting and fail to fully exploit the distinctive characteristics of MedTS data. In this paper, we introduce Sparseformer, a transformer specifically designed for MedTS classification. We propose a sparse token-based dual-attention mechanism that enables global modeling and token compression, allowing dynamic focus on the most informative tokens while distilling redundant features. This mechanism is then applied to the multi-granularity, cross-channel encoding of medical signals, capturing intra- and inter-granularity correlations and inter-channel connections. The sparsification design allows our model to handle heterogeneous inputs of varying lengths and channels directly. Further, we introduce an adaptive label encoder to address label space misalignment across datasets, equipping our model with cross-dataset transferability to alleviate the medical label scarcity issue. Our model outperforms 12 baselines across seven medical datasets under supervised learning. In the few-shot learning experiments, our model also achieves superior average results. In addition, the in-domain and cross-domain experiments among three diagnostic scenarios demonstrate our model's zero-shot learning capability. Collectively, these findings underscore the robustness and transferability of our model in various medical applications.</li>
</ul>

<h3>Title: Understanding the Generalization of In-Context Learning in Transformers: An Empirical Study</h3>
<ul>
<li><strong>Authors: </strong>Xingxuan Zhang, Haoran Wang, Jiansheng Li, Yuan Xue, Shikai Guan, Renzhe Xu, Hao Zou, Han Yu, Peng Cui</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15579">https://arxiv.org/abs/2503.15579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15579">https://arxiv.org/pdf/2503.15579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15579]] Understanding the Generalization of In-Context Learning in Transformers: An Empirical Study(https://arxiv.org/abs/2503.15579)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) like GPT-4 and LLaMA-3 utilize the powerful in-context learning (ICL) capability of Transformer architecture to learn on the fly from limited examples. While ICL underpins many LLM applications, its full potential remains hindered by a limited understanding of its generalization boundaries and vulnerabilities. We present a systematic investigation of transformers' generalization capability with ICL relative to training data coverage by defining a task-centric framework along three dimensions: inter-problem, intra-problem, and intra-task generalization. Through extensive simulation and real-world experiments, encompassing tasks such as function fitting, API calling, and translation, we find that transformers lack inter-problem generalization with ICL, but excel in intra-task and intra-problem generalization. When the training data includes a greater variety of mixed tasks, it significantly enhances the generalization ability of ICL on unseen tasks and even on known simple tasks. This guides us in designing training data to maximize the diversity of tasks covered and to combine different tasks whenever possible, rather than solely focusing on the target task for testing.</li>
</ul>

<h3>Title: PEnGUiN: Partially Equivariant Graph NeUral Networks for Sample Efficient MARL</h3>
<ul>
<li><strong>Authors: </strong>Joshua McClellan, Greyson Brothers, Furong Huang, Pratap Tokekar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15615">https://arxiv.org/abs/2503.15615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15615">https://arxiv.org/pdf/2503.15615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15615]] PEnGUiN: Partially Equivariant Graph NeUral Networks for Sample Efficient MARL(https://arxiv.org/abs/2503.15615)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Equivariant Graph Neural Networks (EGNNs) have emerged as a promising approach in Multi-Agent Reinforcement Learning (MARL), leveraging symmetry guarantees to greatly improve sample efficiency and generalization. However, real-world environments often exhibit inherent asymmetries arising from factors such as external forces, measurement inaccuracies, or intrinsic system biases. This paper introduces \textit{Partially Equivariant Graph NeUral Networks (PEnGUiN)}, a novel architecture specifically designed to address these challenges. We formally identify and categorize various types of partial equivariance relevant to MARL, including subgroup equivariance, feature-wise equivariance, regional equivariance, and approximate equivariance. We theoretically demonstrate that PEnGUiN is capable of learning both fully equivariant (EGNN) and non-equivariant (GNN) representations within a unified framework. Through extensive experiments on a range of MARL problems incorporating various asymmetries, we empirically validate the efficacy of PEnGUiN. Our results consistently demonstrate that PEnGUiN outperforms both EGNNs and standard GNNs in asymmetric environments, highlighting their potential to improve the robustness and applicability of graph-based MARL algorithms in real-world scenarios.</li>
</ul>

<h3>Title: CAM-Seg: A Continuous-valued Embedding Approach for Semantic Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Masud Ahmed, Zahid Hasan, Syed Arefinul Haque, Abu Zaher Md Faridee, Sanjay Purushotham, Suya You, Nirmalya Roy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15617">https://arxiv.org/abs/2503.15617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15617">https://arxiv.org/pdf/2503.15617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15617]] CAM-Seg: A Continuous-valued Embedding Approach for Semantic Image Generation(https://arxiv.org/abs/2503.15617)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, diffusion, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Traditional transformer-based semantic segmentation relies on quantized embeddings. However, our analysis reveals that autoencoder accuracy on segmentation mask using quantized embeddings (e.g. VQ-VAE) is 8% lower than continuous-valued embeddings (e.g. KL-VAE). Motivated by this, we propose a continuous-valued embedding framework for semantic segmentation. By reformulating semantic mask generation as a continuous image-to-embedding diffusion process, our approach eliminates the need for discrete latent representations while preserving fine-grained spatial and semantic details. Our key contribution includes a diffusion-guided autoregressive transformer that learns a continuous semantic embedding space by modeling long-range dependencies in image features. Our framework contains a unified architecture combining a VAE encoder for continuous feature extraction, a diffusion-guided transformer for conditioned embedding generation, and a VAE decoder for semantic mask reconstruction. Our setting facilitates zero-shot domain adaptation capabilities enabled by the continuity of the embedding space. Experiments across diverse datasets (e.g., Cityscapes and domain-shifted variants) demonstrate state-of-the-art robustness to distribution shifts, including adverse weather (e.g., fog, snow) and viewpoint variations. Our model also exhibits strong noise resilience, achieving robust performance ($\approx$ 95% AP compared to baseline) under gaussian noise, moderate motion blur, and moderate brightness/contrast variations, while experiencing only a moderate impact ($\approx$ 90% AP compared to baseline) from 50% salt and pepper noise, saturation and hue shifts. Code available: this https URL</li>
</ul>

<h3>Title: Does Context Matter? ContextualJudgeBench for Evaluating LLM-based Judges in Contextual Settings</h3>
<ul>
<li><strong>Authors: </strong>Austin Xu, Srijan Bansal, Yifei Ming, Semih Yavuz, Shafiq Joty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15620">https://arxiv.org/abs/2503.15620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15620">https://arxiv.org/pdf/2503.15620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15620]] Does Context Matter? ContextualJudgeBench for Evaluating LLM-based Judges in Contextual Settings(https://arxiv.org/abs/2503.15620)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The large language model (LLM)-as-judge paradigm has been used to meet the demand for a cheap, reliable, and fast evaluation of model outputs during AI system development and post-deployment monitoring. While judge models -- LLMs finetuned to specialize in assessing and critiquing model outputs -- have been touted as general purpose evaluators, they are typically evaluated only on non-contextual scenarios, such as instruction following. The omission of contextual settings -- those where external information is used as context to generate an output -- is surprising given the increasing prevalence of retrieval-augmented generation (RAG) and summarization use cases. Contextual assessment is uniquely challenging, as evaluation often depends on practitioner priorities, leading to conditional evaluation criteria (e.g., comparing responses based on factuality and then considering completeness if they are equally factual). To address the gap, we propose ContextualJudgeBench, a judge benchmark with 2,000 challenging response pairs across eight splits inspired by real-world contextual evaluation scenarios. We build our benchmark with a multi-pronged data construction pipeline that leverages both existing human annotations and model-based perturbations. Our comprehensive study across 11 judge models and 9 general purpose models, reveals that the contextual information and its assessment criteria present a significant challenge to even state-of-the-art models. For example, OpenAI's o1, the best-performing model, barely reaches 55% consistent accuracy.</li>
</ul>

<h3>Title: LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Federico Cocchi, Nicholas Moratelli, Davide Caffagni, Sara Sarto, Lorenzo Baraldi, Marcella Cornia, Rita Cucchiara</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15621">https://arxiv.org/abs/2503.15621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15621">https://arxiv.org/pdf/2503.15621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15621]] LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning(https://arxiv.org/abs/2503.15621)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Recent progress in Multimodal Large Language Models (MLLMs) has highlighted the critical roles of both the visual backbone and the underlying language model. While prior work has primarily focused on scaling these components to billions of parameters, the trade-offs between model size, architecture, and performance remain underexplored. Additionally, inconsistencies in training data and evaluation protocols have hindered direct comparisons, making it difficult to derive optimal design choices. In this paper, we introduce LLaVA-MORE, a new family of MLLMs that integrates recent language models with diverse visual backbones. To ensure fair comparisons, we employ a unified training protocol applied consistently across all architectures. Our analysis systematically explores both small- and medium-scale LLMs -- including Phi-4, LLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and instruction following, while examining the relationship between model size and performance. Beyond evaluating the LLM impact on final results, we conduct a comprehensive study of various visual encoders, ranging from CLIP-based architectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional experiments investigate the effects of increased image resolution and variations in pre-training datasets. Overall, our results provide insights into the design of more effective MLLMs, offering a reproducible evaluation framework that facilitates direct comparisons and can guide future model development. Our source code and trained models are publicly available at: this https URL.</li>
</ul>

<h3>Title: EarthScape: A Multimodal Dataset for Surficial Geologic Mapping and Earth Surface Analysis</h3>
<ul>
<li><strong>Authors: </strong>Matthew Massey, Abdullah-Al-Zubaer Imran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15625">https://arxiv.org/abs/2503.15625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15625">https://arxiv.org/pdf/2503.15625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15625]] EarthScape: A Multimodal Dataset for Surficial Geologic Mapping and Earth Surface Analysis(https://arxiv.org/abs/2503.15625)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Surficial geologic mapping is essential for understanding Earth surface processes, addressing modern challenges such as climate change and national security, and supporting common applications in engineering and resource management. However, traditional mapping methods are labor-intensive, limiting spatial coverage and introducing potential biases. To address these limitations, we introduce EarthScape, a novel, AI-ready multimodal dataset specifically designed for surficial geologic mapping and Earth surface analysis. EarthScape integrates high-resolution aerial RGB and near-infrared (NIR) imagery, digital elevation models (DEM), multi-scale DEM-derived terrain features, and hydrologic and infrastructure vector data. The dataset provides detailed annotations for seven distinct surficial geologic classes encompassing various geological processes. We present a comprehensive data processing pipeline using open-sourced raw data and establish baseline benchmarks using different spatial modalities to demonstrate the utility of EarthScape. As a living dataset with a vision for expansion, EarthScape bridges the gap between computer vision and Earth sciences, offering a valuable resource for advancing research in multimodal learning, geospatial analysis, and geological mapping. Our code is available at this https URL.</li>
</ul>

<h3>Title: A Context-Driven Training-Free Network for Lightweight Scene Text Segmentation and Recognition</h3>
<ul>
<li><strong>Authors: </strong>Ritabrata Chakraborty, Shivakumara Palaiahnakote, Umapada Pal, Cheng-Lin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15639">https://arxiv.org/abs/2503.15639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15639">https://arxiv.org/pdf/2503.15639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15639]] A Context-Driven Training-Free Network for Lightweight Scene Text Segmentation and Recognition(https://arxiv.org/abs/2503.15639)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Modern scene text recognition systems often depend on large end-to-end architectures that require extensive training and are prohibitively expensive for real-time scenarios. In such cases, the deployment of heavy models becomes impractical due to constraints on memory, computational resources, and latency. To address these challenges, we propose a novel, training-free plug-and-play framework that leverages the strengths of pre-trained text recognizers while minimizing redundant computations. Our approach uses context-based understanding and introduces an attention-based segmentation stage, which refines candidate text regions at the pixel level, improving downstream recognition. Instead of performing traditional text detection that follows a block-level comparison between feature map and source image and harnesses contextual information using pretrained captioners, allowing the framework to generate word predictions directly from scene this http URL texts are semantically and lexically evaluated to get a final score. Predictions that meet or exceed a pre-defined confidence threshold bypass the heavier process of end-to-end text STR profiling, ensuring faster inference and cutting down on unnecessary computations. Experiments on public benchmarks demonstrate that our paradigm achieves performance on par with state-of-the-art systems, yet requires substantially fewer resources.</li>
</ul>

<h3>Title: Cancelable Biometric Template Generation Using Random Feature Vector Transformations</h3>
<ul>
<li><strong>Authors: </strong>Ragendhu Sp, Tony Thomas, Sabu Emmanuel</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15648">https://arxiv.org/abs/2503.15648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15648">https://arxiv.org/pdf/2503.15648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15648]] Cancelable Biometric Template Generation Using Random Feature Vector Transformations(https://arxiv.org/abs/2503.15648)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, biometric, steal</a></li>
<li><strong>Abstract: </strong>Cancelable biometric schemes are designed to extract an identity-preserving, non-invertible as well as revocable pseudo-identifier from biometric data. Recognition systems need to store only this pseudo-identifier, to avoid tampering and/or stealing of original biometric data during the recognition process. State-of-the-art cancelable schemes generate pseudo-identifiers by transforming the original template using either user-specific salting or many-to-one transformations. In addition to the performance concerns, most of such schemes are modality-specific and prone to reconstruction attacks as there are chances for unauthorized access to security-critical transformation keys. A novel, modality-independent cancelable biometric scheme is proposed to overcome these limitations. In this scheme, a cancelable template (pseudo identifier) is generated as a distance vector between multiple random transformations of the biometric feature vector. These transformations were done by grouping feature vector components based on a set of user-specific random vectors. The proposed scheme nullifies the possibility of template reconstruction as the generated cancelable template contains only the distance values between the different random transformations of the feature vector and it does not store any details of the biometric template. The recognition performance of the proposed scheme is evaluated for face and fingerprint modalities. Equal Error Rate (EER) of 1.5 is obtained for face and 1.7 is obtained for the fingerprint in the worst case.</li>
</ul>

<h3>Title: Transport-Related Surface Detection with Machine Learning: Analyzing Temporal Trends in Madrid and Vienna</h3>
<ul>
<li><strong>Authors: </strong>Miguel Ureña Pliego, Rubén Martínez Marín, Nianfang Shi, Takeru Shibayama, Ulrich Leth, Miguel Marchamalo Sacristán</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15653">https://arxiv.org/abs/2503.15653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15653">https://arxiv.org/pdf/2503.15653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15653]] Transport-Related Surface Detection with Machine Learning: Analyzing Temporal Trends in Madrid and Vienna(https://arxiv.org/abs/2503.15653)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>This study explores the integration of machine learning into urban aerial image analysis, with a focus on identifying infrastructure surfaces for cars and pedestrians and analyzing historical trends. It emphasizes the transition from convolutional architectures to transformer-based pre-trained models, underscoring their potential in global geospatial analysis. A workflow is presented for automatically generating geospatial datasets, enabling the creation of semantic segmentation datasets from various sources, including WMS/WMTS links, vectorial cartography, and OpenStreetMap (OSM) overpass-turbo requests. The developed code allows a fast dataset generation process for training machine learning models using openly available data without manual labelling. Using aerial imagery and vectorial data from the respective geographical offices of Madrid and Vienna, two datasets were generated for car and pedestrian surface detection. A transformer-based model was trained and evaluated for each city, demonstrating good accuracy values. The historical trend analysis involved applying the trained model to earlier images predating the availability of vectorial data 10 to 20 years, successfully identifying temporal trends in infrastructure for pedestrians and cars across different city areas. This technique is applicable for municipal governments to gather valuable data at a minimal cost.</li>
</ul>

<h3>Title: Acurast: Decentralized Serverless Cloud</h3>
<ul>
<li><strong>Authors: </strong>Christian Killer, Alessandro De Carli, Pascal Brun, Amadeo Victor Charlé, Mike Godenzi, Simon Wehrli</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15654">https://arxiv.org/abs/2503.15654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15654">https://arxiv.org/pdf/2503.15654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15654]] Acurast: Decentralized Serverless Cloud(https://arxiv.org/abs/2503.15654)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>Centralized trust is ubiquitous in today's interconnected world, from computational resources to data storage and its underlying infrastructure. The monopolization of cloud computing resembles a feudalistic system, causing a loss of privacy and data ownership. Cloud Computing and the Internet in general face widely recognized challenges, such as (1) the centralization of trust in auxiliary systems (e.g., centralized cloud providers), (2) the seamless and permissionless interoperability of fragmented ecosystems and (2) the effectiveness, verifiability, and confidentiality of the computation. Acurast is a decentralized serverless cloud that addresses all these shortcomings, following the call for a global-scale cloud founded on the principles of the open-source movement. In Acurast, a purpose-built orchestrator, a reputation engine, and an attestation service are enshrined in the consensus layer. Developers can off-load their computations and verify executions cryptographically. Furthermore, Acurast offers a modular execution layer, taking advantage of secure hardware and trusted execution environments, removing the trust required in third parties, and reducing them to cryptographic hardness assumptions. With this modular architecture, Acurast serves as a decentralized and serverless cloud, allowing confidential and verifiable compute backed by the hardware of security and performance mobile devices.</li>
</ul>

<h3>Title: Enhancing Pancreatic Cancer Staging with Large Language Models: The Role of Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Hisashi Johno, Yuki Johno, Akitomo Amakawa, Junichi Sato, Ryota Tozuka, Atsushi Komaba, Hiroaki Watanabe, Hiroki Watanabe, Chihiro Goto, Hiroyuki Morisaka, Hiroshi Onishi, Kazunori Nakamoto</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15664">https://arxiv.org/abs/2503.15664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15664">https://arxiv.org/pdf/2503.15664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15664]] Enhancing Pancreatic Cancer Staging with Large Language Models: The Role of Retrieval-Augmented Generation(https://arxiv.org/abs/2503.15664)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Purpose: Retrieval-augmented generation (RAG) is a technology to enhance the functionality and reliability of large language models (LLMs) by retrieving relevant information from reliable external knowledge (REK). RAG has gained interest in radiology, and we previously reported the utility of NotebookLM, an LLM with RAG (RAG-LLM), for lung cancer staging. However, since the comparator LLM differed from NotebookLM's internal model, it remained unclear whether its advantage stemmed from RAG or inherent model differences. To better isolate RAG's impact and assess its utility across different cancers, we compared NotebookLM with its internal LLM, Gemini 2.0 Flash, in a pancreatic cancer staging experiment. Materials and Methods: A summary of Japan's pancreatic cancer staging guidelines was used as REK. We compared three groups - REK+/RAG+ (NotebookLM with REK), REK+/RAG- (Gemini 2.0 Flash with REK), and REK-/RAG- (Gemini 2.0 Flash without REK) - in staging 100 fictional pancreatic cancer cases based on CT findings. Staging criteria included TNM classification, local invasion factors, and resectability classification. In REK+/RAG+, retrieval accuracy was quantified based on the sufficiency of retrieved REK excerpts. Results: REK+/RAG+ achieved a staging accuracy of 70%, outperforming REK+/RAG- (38%) and REK-/RAG- (35%). For TNM classification, REK+/RAG+ attained 80% accuracy, exceeding REK+/RAG- (55%) and REK-/RAG- (50%). Additionally, REK+/RAG+ explicitly presented retrieved REK excerpts, achieving a retrieval accuracy of 92%. Conclusion: NotebookLM, a RAG-LLM, outperformed its internal LLM, Gemini 2.0 Flash, in a pancreatic cancer staging experiment, suggesting that RAG may improve LLM's staging accuracy. Furthermore, its ability to retrieve and present REK excerpts provides transparency for physicians, highlighting its applicability for clinical diagnosis and classification.</li>
</ul>

<h3>Title: DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yuming Gu, Phong Tran, Yujian Zheng, Hongyi Xu, Heyuan Li, Adilbek Karmanov, Hao Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15667">https://arxiv.org/abs/2503.15667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15667">https://arxiv.org/pdf/2503.15667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15667]] DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis(https://arxiv.org/abs/2503.15667)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Generating high-quality 360-degree views of human heads from single-view images is essential for enabling accessible immersive telepresence applications and scalable personalized content creation. While cutting-edge methods for full head generation are limited to modeling realistic human heads, the latest diffusion-based approaches for style-omniscient head synthesis can produce only frontal views and struggle with view consistency, preventing their conversion into true 3D models for rendering from arbitrary angles. We introduce a novel approach that generates fully consistent 360-degree head views, accommodating human, stylized, and anthropomorphic forms, including accessories like glasses and hats. Our method builds on the DiffPortrait3D framework, incorporating a custom ControlNet for back-of-head detail generation and a dual appearance module to ensure global front-back consistency. By training on continuous view sequences and integrating a back reference image, our approach achieves robust, locally continuous view synthesis. Our model can be used to produce high-quality neural radiance fields (NeRFs) for real-time, free-viewpoint rendering, outperforming state-of-the-art methods in object synthesis and 360-degree head generation for very challenging input portraits.</li>
</ul>

<h3>Title: CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Arindam Dutta, Meng Zheng, Zhongpai Gao, Benjamin Planche, Anwesha Choudhuri, Terrence Chen, Amit K. Roy-Chowdhury, Ziyan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15671">https://arxiv.org/abs/2503.15671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15671">https://arxiv.org/pdf/2503.15671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15671]] CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image(https://arxiv.org/abs/2503.15671)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reconstructing clothed humans from a single image is a fundamental task in computer vision with wide-ranging applications. Although existing monocular clothed human reconstruction solutions have shown promising results, they often rely on the assumption that the human subject is in an occlusion-free environment. Thus, when encountering in-the-wild occluded images, these algorithms produce multiview inconsistent and fragmented reconstructions. Additionally, most algorithms for monocular 3D human reconstruction leverage geometric priors such as SMPL annotations for training and inference, which are extremely challenging to acquire in real-world applications. To address these limitations, we propose CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-ConsistEncy from a Single Image, a novel pipeline designed to reconstruct occlusion-resilient 3D humans with multiview consistency from a single occluded image, without requiring either ground-truth geometric prior annotations or 3D supervision. Specifically, CHROME leverages a multiview diffusion model to first synthesize occlusion-free human images from the occluded input, compatible with off-the-shelf pose control to explicitly enforce cross-view consistency during synthesis. A 3D reconstruction model is then trained to predict a set of 3D Gaussians conditioned on both the occluded input and synthesized views, aligning cross-view details to produce a cohesive and accurate 3D representation. CHROME achieves significant improvements in terms of both novel view synthesis (upto 3 db PSNR) and geometric reconstruction under challenging conditions.</li>
</ul>

<h3>Title: GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>William Ljungbergh, Adam Lilja, Adam Tonderski. Arvid Laveno Ling, Carl Lindström, Willem Verbeke, Junsheng Fu, Christoffer Petersson, Lars Hammarstrand, Michael Felsberg</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15672">https://arxiv.org/abs/2503.15672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15672">https://arxiv.org/pdf/2503.15672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15672]] GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for Autonomous Driving(https://arxiv.org/abs/2503.15672)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Self-supervised pre-training based on next-token prediction has enabled large language models to capture the underlying structure of text, and has led to unprecedented performance on a large array of tasks when applied at scale. Similarly, autonomous driving generates vast amounts of spatiotemporal data, alluding to the possibility of harnessing scale to learn the underlying geometric and semantic structure of the environment and its evolution over time. In this direction, we propose a geometric and semantic self-supervised pre-training method, GASP, that learns a unified representation by predicting, at any queried future point in spacetime, (1) general occupancy, capturing the evolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle path through the environment; and (3) distilled high-level features from a vision foundation model. By modeling geometric and semantic 4D occupancy fields instead of raw sensor measurements, the model learns a structured, generalizable representation of the environment and its evolution through time. We validate GASP on multiple autonomous driving benchmarks, demonstrating significant improvements in semantic occupancy forecasting, online mapping, and ego trajectory prediction. Our results demonstrate that continuous 4D geometric and semantic occupancy prediction provides a scalable and effective pre-training paradigm for autonomous driving. For code and additional visualizations, see \href{this https URL.</li>
</ul>

<h3>Title: High Temporal Consistency through Semantic Similarity Propagation in Semi-Supervised Video Semantic Segmentation for Autonomous Flight</h3>
<ul>
<li><strong>Authors: </strong>Cédric Vincent, Taehyoung Kim, Henri Meeß</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15676">https://arxiv.org/abs/2503.15676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15676">https://arxiv.org/pdf/2503.15676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15676]] High Temporal Consistency through Semantic Similarity Propagation in Semi-Supervised Video Semantic Segmentation for Autonomous Flight(https://arxiv.org/abs/2503.15676)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation from RGB cameras is essential to the perception of autonomous flying vehicles. The stability of predictions through the captured videos is paramount to their reliability and, by extension, to the trustworthiness of the agents. In this paper, we propose a lightweight video semantic segmentation approach-suited to onboard real-time inference-achieving high temporal consistency on aerial data through Semantic Similarity Propagation across frames. SSP temporally propagates the predictions of an efficient image segmentation model with global registration alignment to compensate for camera movements. It combines the current estimation and the prior prediction with linear interpolation using weights computed from the features similarities of the two frames. Because data availability is a challenge in this domain, we propose a consistency-aware Knowledge Distillation training procedure for sparsely labeled datasets with few annotations. Using a large image segmentation model as a teacher to train the efficient SSP, we leverage the strong correlations between labeled and unlabeled frames in the same training videos to obtain high-quality supervision on all frames. KD-SSP obtains a significant temporal consistency increase over the base image segmentation model of 12.5% and 6.7% TC on UAVid and RuralScapes respectively, with higher accuracy and comparable inference speed. On these aerial datasets, KD-SSP provides a superior segmentation quality and inference speed trade-off than other video methods proposed for general applications and shows considerably higher consistency. The code will be made publicly available upon acceptance.</li>
</ul>

<h3>Title: Cyber Threats in Financial Transactions -- Addressing the Dual Challenge of AI and Quantum Computing</h3>
<ul>
<li><strong>Authors: </strong>Ahmed M. Elmisery, Mirela Sertovic, Andrew Zayin, Paul Watson</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15678">https://arxiv.org/abs/2503.15678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15678">https://arxiv.org/pdf/2503.15678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15678]] Cyber Threats in Financial Transactions -- Addressing the Dual Challenge of AI and Quantum Computing(https://arxiv.org/abs/2503.15678)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The financial sector faces escalating cyber threats amplified by artificial intelligence (AI) and the advent of quantum computing. AI is being weaponized for sophisticated attacks like deepfakes and AI-driven malware, while quantum computing threatens to render current encryption methods obsolete. This report analyzes these threats, relevant frameworks, and possible countermeasures like quantum cryptography. AI enhances social engineering and phishing attacks via personalized content, lowers entry barriers for cybercriminals, and introduces risks like data poisoning and adversarial AI. Quantum computing, particularly Shor's algorithm, poses a fundamental threat to current encryption standards (RSA and ECC), with estimates suggesting cryptographically relevant quantum computers could emerge within the next 5-30 years. The "harvest now, decrypt later" scenario highlights the urgency of transitioning to quantum-resistant cryptography. This is key. Existing legal frameworks are evolving to address AI in cybercrime, but quantum threats require new initiatives. International cooperation and harmonized regulations are crucial. Quantum Key Distribution (QKD) offers theoretical security but faces practical limitations. Post-quantum cryptography (PQC) is a promising alternative, with ongoing standardization efforts. Recommendations for international regulators include fostering collaboration and information sharing, establishing global standards, supporting research and development in quantum security, harmonizing legal frameworks, promoting cryptographic agility, and raising awareness and education. The financial industry must adopt a proactive and adaptive approach to cybersecurity, investing in research, developing migration plans for quantum-resistant cryptography, and embracing a multi-faceted, collaborative strategy to build a resilient, quantum-safe, and AI-resilient financial ecosystem</li>
</ul>

<h3>Title: The Change You Want To Detect: Semantic Change Detection In Earth Observation With Hybrid Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Benidir Yanis, Gonthier Nicolas, Mallet Clement</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15683">https://arxiv.org/abs/2503.15683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15683">https://arxiv.org/pdf/2503.15683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15683]] The Change You Want To Detect: Semantic Change Detection In Earth Observation With Hybrid Data Generation(https://arxiv.org/abs/2503.15683)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Bi-temporal change detection at scale based on Very High Resolution (VHR) images is crucial for Earth monitoring. This remains poorly addressed so far: methods either require large volumes of annotated data (semantic case), or are limited to restricted datasets (binary set-ups). Most approaches do not exhibit the versatility required for temporal and spatial adaptation: simplicity in architecture design and pretraining on realistic and comprehensive datasets. Synthetic datasets are the key solution but still fail to handle complex and diverse scenes. In this paper, we present HySCDG a generative pipeline for creating a large hybrid semantic change detection dataset that contains both real VHR images and inpainted ones, along with land cover semantic map at both dates and the change map. Being semantically and spatially guided, HySCDG generates realistic images, leading to a comprehensive and hybrid transfer-proof dataset FSC-180k. We evaluate FSC-180k on five change detection cases (binary and semantic), from zero-shot to mixed and sequential training, and also under low data regime training. Experiments demonstrate that pretraining on our hybrid dataset leads to a significant performance boost, outperforming SyntheWorld, a fully synthetic dataset, in every configuration. All codes, models, and data are available here: $\href{this https URL}{this https URL}$.</li>
</ul>

<h3>Title: Multi-focal Conditioned Latent Diffusion for Person Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Liu, Jichao Zahng, Paolo Rota, Nicu Sebe</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15686">https://arxiv.org/abs/2503.15686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15686">https://arxiv.org/pdf/2503.15686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15686]] Multi-focal Conditioned Latent Diffusion for Person Image Synthesis(https://arxiv.org/abs/2503.15686)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The Latent Diffusion Model (LDM) has demonstrated strong capabilities in high-resolution image generation and has been widely employed for Pose-Guided Person Image Synthesis (PGPIS), yielding promising results. However, the compression process of LDM often results in the deterioration of details, particularly in sensitive areas such as facial features and clothing textures. In this paper, we propose a Multi-focal Conditioned Latent Diffusion (MCLD) method to address these limitations by conditioning the model on disentangled, pose-invariant features from these sensitive regions. Our approach utilizes a multi-focal condition aggregation module, which effectively integrates facial identity and texture-specific information, enhancing the model's ability to produce appearance realistic and identity-consistent images. Our method demonstrates consistent identity and appearance generation on the DeepFashion dataset and enables flexible person image editing due to its generation consistency. The code is available at this https URL.</li>
</ul>

<h3>Title: Sustainable Deep Learning-Based Breast Lesion Segmentation: Impact of Breast Region Segmentation on Performance</h3>
<ul>
<li><strong>Authors: </strong>Sam Narimani, Solveig Roth Hoff, Kathinka Dahli Kurz, Kjell-Inge Gjesdal, Jurgen Geisler, Endre Grovik</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15708">https://arxiv.org/abs/2503.15708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15708">https://arxiv.org/pdf/2503.15708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15708]] Sustainable Deep Learning-Based Breast Lesion Segmentation: Impact of Breast Region Segmentation on Performance(https://arxiv.org/abs/2503.15708)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Purpose: Segmentation of the breast lesion in dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is an essential step to accurately diagnose and plan treatment and monitor progress. This study aims to highlight the impact of breast region segmentation (BRS) on deep learning-based breast lesion segmentation (BLS) in breast DCE-MRI. Methods Using the Stavanger Dataset containing primarily 59 DCE-MRI scans and UNet++ as deep learning models, four different process were conducted to compare effect of BRS on BLS. These four approaches included the whole volume without BRS and with BRS, BRS with the selected lesion slices and lastly optimal volume with BRS. Preprocessing methods like augmentation and oversampling were used to enhance the small dataset, data shape uniformity and improve model performance. Optimal volume size were investigated by a precise process to ensure that all lesions existed in slices. To evaluate the model, a hybrid loss function including dice, focal and cross entropy along with 5-fold cross validation method were used and lastly a test dataset which was randomly split used to evaluate the model performance on unseen data for each of four mentioned approaches. Results Results demonstrate that using BRS considerably improved model performance and validation. Significant improvement in last approach -- optimal volume with BRS -- compared to the approach without BRS counting around 50 percent demonstrating how effective BRS has been in BLS. Moreover, huge improvement in energy consumption, decreasing up to 450 percent, introduces a green solution toward a more environmentally sustainable approach for future work on large dataset.</li>
</ul>

<h3>Title: SPNeRF: Open Vocabulary 3D Neural Scene Segmentation with Superpoints</h3>
<ul>
<li><strong>Authors: </strong>Weiwen Hu, Niccolò Parodi, Marcus Zepp, Ingo Feldmann, Oliver Schreer, Peter Eisert</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15712">https://arxiv.org/abs/2503.15712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15712">https://arxiv.org/pdf/2503.15712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15712]] SPNeRF: Open Vocabulary 3D Neural Scene Segmentation with Superpoints(https://arxiv.org/abs/2503.15712)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Open-vocabulary segmentation, powered by large visual-language models like CLIP, has expanded 2D segmentation capabilities beyond fixed classes predefined by the dataset, enabling zero-shot understanding across diverse scenes. Extending these capabilities to 3D segmentation introduces challenges, as CLIP's image-based embeddings often lack the geometric detail necessary for 3D scene segmentation. Recent methods tend to address this by introducing additional segmentation models or replacing CLIP with variations trained on segmentation data, which lead to redundancy or loss on CLIP's general language capabilities. To overcome this limitation, we introduce SPNeRF, a NeRF based zero-shot 3D segmentation approach that leverages geometric priors. We integrate geometric primitives derived from the 3D scene into NeRF training to produce primitive-wise CLIP features, avoiding the ambiguity of point-wise features. Additionally, we propose a primitive-based merging mechanism enhanced with affinity scores. Without relying on additional segmentation models, our method further explores CLIP's capability for 3D segmentation and achieves notable improvements over original LERF.</li>
</ul>

<h3>Title: Am I eligible? Natural Language Inference for Clinical Trial Patient Recruitment: the Patient's Point of View</h3>
<ul>
<li><strong>Authors: </strong>Mathilde Aguiar, Pierre Zweigenbaum, Nona Naderi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15718">https://arxiv.org/abs/2503.15718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15718">https://arxiv.org/pdf/2503.15718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15718]] Am I eligible? Natural Language Inference for Clinical Trial Patient Recruitment: the Patient's Point of View(https://arxiv.org/abs/2503.15718)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recruiting patients to participate in clinical trials can be challenging and time-consuming. Usually, participation in a clinical trial is initiated by a healthcare professional and proposed to the patient. Promoting clinical trials directly to patients via online recruitment might help to reach them more efficiently. In this study, we address the case where a patient is initiating their own recruitment process and wants to determine whether they are eligible for a given clinical trial, using their own language to describe their medical profile. To study whether this creates difficulties in the patient trial matching process, we design a new dataset and task, Natural Language Inference for Patient Recruitment (NLI4PR), in which patient language profiles must be matched to clinical trials. We create it by adapting the TREC 2022 Clinical Trial Track dataset, which provides patients' medical profiles, and rephrasing them manually using patient language. We also use the associated clinical trial reports where the patients are either eligible or excluded. We prompt several open-source Large Language Models on our task and achieve from 56.5 to 71.8 of F1 score using patient language, against 64.7 to 73.1 for the same task using medical language. When using patient language, we observe only a small loss in performance for the best model, suggesting that having the patient as a starting point could be adopted to help recruit patients for clinical trials. The corpus and code bases are all freely available on our Github and HuggingFace repositories.</li>
</ul>

<h3>Title: Cybersecurity in Vehicle-to-Grid (V2G) Systems: A Systematic Review</h3>
<ul>
<li><strong>Authors: </strong>Mohammad A Razzaque, Shafiuzzaman K Khadem, Sandipan Patra, Glory Okwata, Md. Noor-A-Rahim</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15730">https://arxiv.org/abs/2503.15730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15730">https://arxiv.org/pdf/2503.15730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15730]] Cybersecurity in Vehicle-to-Grid (V2G) Systems: A Systematic Review(https://arxiv.org/abs/2503.15730)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, robust, explainability</a></li>
<li><strong>Abstract: </strong>This paper presents a systematic review of recent advancements in V2G cybersecurity, employing the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) framework for detailed searches across three journal databases and included only peer-reviewed studies published between 2020 and 2024 (June). We identified and reviewed 133 V2G cybersecurity studies and found five important insights on existing V2G cybersecurity research. First, most studies (103 of 133) focused on protecting V2G systems against cyber threats, while only seven studies addressed the recovery aspect of the CRML (Cybersecurity Risk Management Lifecycle) function. Second, existing studies have adequately addressed the security of EVs and EVCS (EV charging stations) in V2G systems (112 and 81 of 133 studies, respectively). However, none have focused on the linkage between the behaviour of EV users and the cybersecurity of V2G systems. Third, physical access, control-related vulnerabilities, and user behaviour-related attacks in V2G systems are not addressed significantly. Furthermore, existing studies overlook vulnerabilities and attacks specific to AI and blockchain technologies. Fourth, blockchain, artificial intelligence (AI), encryption, control theory, and optimisation are the main technologies used, and finally, the inclusion of quantum safety within encryption and AI models and AI assurance (AIA) is in a very early stage; only two and one of 133 studies explicitly addressed quantum safety and AIA through explainability. By providing a holistic perspective, this study identifies critical research gaps and outlines future directions for developing robust end-to-end cybersecurity solutions to safeguard V2G systems and support global sustainability goals.</li>
</ul>

<h3>Title: KoGNER: A Novel Framework for Knowledge Graph Distillation on Biomedical Named Entity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Heming Zhang, Wenyu Li, Di Huang, Yinjie Tang, Yixin Chen, Philip Payne, Fuhai Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15737">https://arxiv.org/abs/2503.15737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15737">https://arxiv.org/pdf/2503.15737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15737]] KoGNER: A Novel Framework for Knowledge Graph Distillation on Biomedical Named Entity Recognition(https://arxiv.org/abs/2503.15737)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) that plays a crucial role in information extraction, question answering, and knowledge-based systems. Traditional deep learning-based NER models often struggle with domain-specific generalization and suffer from data sparsity issues. In this work, we introduce Knowledge Graph distilled for Named Entity Recognition (KoGNER), a novel approach that integrates Knowledge Graph (KG) distillation into NER models to enhance entity recognition performance. Our framework leverages structured knowledge representations from KGs to enrich contextual embeddings, thereby improving entity classification and reducing ambiguity in entity detection. KoGNER employs a two-step process: (1) Knowledge Distillation, where external knowledge sources are distilled into a lightweight representation for seamless integration with NER models, and (2) Entity-Aware Augmentation, which integrates contextual embeddings that have been enriched with knowledge graph information directly into GNN, thereby improving the model's ability to understand and represent entity relationships. Experimental results on benchmark datasets demonstrate that KoGNER achieves state-of-the-art performance, outperforming finetuned NER models and LLMs by a significant margin. These findings suggest that leveraging knowledge graphs as auxiliary information can significantly improve NER accuracy, making KoGNER a promising direction for future research in knowledge-aware NLP.</li>
</ul>

<h3>Title: Uncertainty-Aware Diffusion Guided Refinement of 3D Scenes</h3>
<ul>
<li><strong>Authors: </strong>Sarosij Bose, Arindam Dutta, Sayak Nag, Junge Zhang, Jiachen Li, Konstantinos Karydis, Amit K. Roy Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15742">https://arxiv.org/abs/2503.15742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15742">https://arxiv.org/pdf/2503.15742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15742]] Uncertainty-Aware Diffusion Guided Refinement of 3D Scenes(https://arxiv.org/abs/2503.15742)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Reconstructing 3D scenes from a single image is a fundamentally ill-posed task due to the severely under-constrained nature of the problem. Consequently, when the scene is rendered from novel camera views, existing single image to 3D reconstruction methods render incoherent and blurry views. This problem is exacerbated when the unseen regions are far away from the input camera. In this work, we address these inherent limitations in existing single image-to-3D scene feedforward networks. To alleviate the poor performance due to insufficient information beyond the input image's view, we leverage a strong generative prior in the form of a pre-trained latent video diffusion model, for iterative refinement of a coarse scene represented by optimizable Gaussian parameters. To ensure that the style and texture of the generated images align with that of the input image, we incorporate on-the-fly Fourier-style transfer between the generated images and the input image. Additionally, we design a semantic uncertainty quantification module that calculates the per-pixel entropy and yields uncertainty maps used to guide the refinement process from the most confident pixels while discarding the remaining highly uncertain ones. We conduct extensive experiments on real-world scene datasets, including in-domain RealEstate-10K and out-of-domain KITTI-v2, showing that our approach can provide more realistic and high-fidelity novel view synthesis results compared to existing state-of-the-art methods.</li>
</ul>

<h3>Title: PARQ: Piecewise-Affine Regularized Quantization</h3>
<ul>
<li><strong>Authors: </strong>Lisa Jin, Jianhao Ma, Zechun Liu, Andrey Gromov, Aaron Defazio, Lin Xiao</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15748">https://arxiv.org/abs/2503.15748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15748">https://arxiv.org/pdf/2503.15748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15748]] PARQ: Piecewise-Affine Regularized Quantization(https://arxiv.org/abs/2503.15748)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We develop a principled method for quantization-aware training (QAT) of large-scale machine learning models. Specifically, we show that convex, piecewise-affine regularization (PAR) can effectively induce the model parameters to cluster towards discrete values. We minimize PAR-regularized loss functions using an aggregate proximal stochastic gradient method (AProx) and prove that it has last-iterate convergence. Our approach provides an interpretation of the straight-through estimator (STE), a widely used heuristic for QAT, as the asymptotic form of PARQ. We conduct experiments to demonstrate that PARQ obtains competitive performance on convolution- and transformer-based vision tasks.</li>
</ul>

<h3>Title: AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration</h3>
<ul>
<li><strong>Authors: </strong>Andy Zhou, Kevin Wu, Francesco Pinto, Zhaorun Chen, Yi Zeng, Yu Yang, Shuang Yang, Sanmi Koyejo, James Zou, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15754">https://arxiv.org/abs/2503.15754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15754">https://arxiv.org/pdf/2503.15754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15754]] AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration(https://arxiv.org/abs/2503.15754)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly capable, security and safety evaluation are crucial. While current red teaming approaches have made strides in assessing LLM vulnerabilities, they often rely heavily on human input and lack comprehensive coverage of emerging attack vectors. This paper introduces AutoRedTeamer, a novel framework for fully automated, end-to-end red teaming against LLMs. AutoRedTeamer combines a multi-agent architecture with a memory-guided attack selection mechanism to enable continuous discovery and integration of new attack vectors. The dual-agent framework consists of a red teaming agent that can operate from high-level risk categories alone to generate and execute test cases and a strategy proposer agent that autonomously discovers and implements new attacks by analyzing recent research. This modular design allows AutoRedTeamer to adapt to emerging threats while maintaining strong performance on existing attack vectors. We demonstrate AutoRedTeamer's effectiveness across diverse evaluation settings, achieving 20% higher attack success rates on HarmBench against Llama-3.1-70B while reducing computational costs by 46% compared to existing approaches. AutoRedTeamer also matches the diversity of human-curated benchmarks in generating test cases, providing a comprehensive, scalable, and continuously evolving framework for evaluating the security of AI systems.</li>
</ul>

<h3>Title: ATTENTION2D: Communication Efficient Distributed Self-Attention Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Venmugil Elango</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15758">https://arxiv.org/abs/2503.15758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15758">https://arxiv.org/pdf/2503.15758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15758]] ATTENTION2D: Communication Efficient Distributed Self-Attention Mechanism(https://arxiv.org/abs/2503.15758)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based models have emerged as a leading architecture for natural language processing, natural language generation, and image generation tasks. A fundamental element of the transformer architecture is self-attention, which allows the model to capture intricate dependencies within the data. However, the self-attention mechanism also incurs significant computational and memory costs, particularly for long sequences. In this paper, we introduce ATTENTION2D, a novel approach that exploits parallelism along two dimensions - query and key/value - of the self-attention operation. This method enables efficient distribution and parallelization of computations across multiple devices. Our approach facilitates asymptotically faster training and inference phases compared to previous methods, without relying on approximations or incurring additional computational or memory overheads. Furthermore, unlike existing techniques that struggle to scale with an increasing number of processing units, our approach effectively scales with additional processing units. Our experimental results confirm the effectiveness of our method in improving communication efficiency and scalability. Compared to Ring Attention, our approach demonstrated up to a 5x performance boost on a GPT-3-like model using 64 NVIDIA A100 GPUs across 16 nodes, and up to a 9.4x performance boost on 64 NVIDIA H100 GPUs across 64 nodes.</li>
</ul>

<h3>Title: GraPLUS: Graph-based Placement Using Semantics for Image Composition</h3>
<ul>
<li><strong>Authors: </strong>Mir Mohammad Khaleghi, Mehran Safayani, Abdolreza Mirzaei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15761">https://arxiv.org/abs/2503.15761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15761">https://arxiv.org/pdf/2503.15761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15761]] GraPLUS: Graph-based Placement Using Semantics for Image Composition(https://arxiv.org/abs/2503.15761)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present GraPLUS (Graph-based Placement Using Semantics), a novel framework for plausible object placement in images that leverages scene graphs and large language models. Our approach uniquely combines graph-structured scene representation with semantic understanding to determine contextually appropriate object positions. The framework employs GPT-2 to transform categorical node and edge labels into rich semantic embeddings that capture both definitional characteristics and typical spatial contexts, enabling nuanced understanding of object relationships and placement patterns. GraPLUS achieves placement accuracy of 92.1% and an FID score of 28.83 on the OPA dataset, outperforming state-of-the-art methods by 8.1% while maintaining competitive visual quality. In human evaluation studies involving 964 samples assessed by 19 participants, our method was preferred in 52.1% of cases, significantly outperforming previous approaches. The framework's key innovations include: (i) leveraging pre-trained scene graph models that transfer knowledge from other domains, (ii) edge-aware graph neural networks that process scene semantics through structured relationships, (iii) a cross-modal attention mechanism that aligns categorical embeddings with enhanced scene features, and (iv) a multiobjective training strategy incorporating semantic consistency constraints.</li>
</ul>

<h3>Title: OffsetOPT: Explicit Surface Reconstruction without Normals</h3>
<ul>
<li><strong>Authors: </strong>Huan Lei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15763">https://arxiv.org/abs/2503.15763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15763">https://arxiv.org/pdf/2503.15763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15763]] OffsetOPT: Explicit Surface Reconstruction without Normals(https://arxiv.org/abs/2503.15763)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Neural surface reconstruction has been dominated by implicit representations with marching cubes for explicit surface extraction. However, those methods typically require high-quality normals for accurate reconstruction. We propose OffsetOPT, a method that reconstructs explicit surfaces directly from 3D point clouds and eliminates the need for point normals. The approach comprises two stages: first, we train a neural network to predict surface triangles based on local point geometry, given uniformly distributed training point clouds. Next, we apply the frozen network to reconstruct surfaces from unseen point clouds by optimizing a per-point offset to maximize the accuracy of triangle predictions. Compared to state-of-the-art methods, OffsetOPT not only excels at reconstructing overall surfaces but also significantly preserves sharp surface features. We demonstrate its accuracy on popular benchmarks, including small-scale shapes and large-scale open surfaces.</li>
</ul>

<h3>Title: Accelerating Transient CFD through Machine Learning-Based Flow Initialization</h3>
<ul>
<li><strong>Authors: </strong>Peter Sharpe, Rishikesh Ranade, Sanjay Choudhry</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15766">https://arxiv.org/abs/2503.15766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15766">https://arxiv.org/pdf/2503.15766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15766]] Accelerating Transient CFD through Machine Learning-Based Flow Initialization(https://arxiv.org/abs/2503.15766)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Transient computational fluid dynamics (CFD) simulations are essential for many industrial applications, but a significant portion of their computational cost stems from the time needed to reach statistical steadiness from initial conditions. We present a novel machine learning-based initialization method that reduces the cost of this subsequent transient solve substantially, achieving a 50% reduction in time-to-convergence compared to traditional uniform and potential flow-based initializations. Through a case study in automotive aerodynamics using a 16.7M-cell unsteady RANS simulation, we evaluate three ML-based initialization strategies. Two of these strategies are recommended for general use: (1) a physics-informed hybrid method combining ML predictions with potential flow solutions, and (2) a more versatile approach integrating ML predictions with uniform flow. Both strategies enable CFD solvers to achieve convergence times comparable to computationally expensive steady RANS initializations, while requiring only seconds of computation. We develop a robust statistical convergence metric based on windowed time-averaging for performance comparison between initialization strategies. Notably, these improvements are achieved using an ML model trained on a different dataset of automotive geometries, demonstrating strong generalization capabilities. The proposed methods integrate seamlessly with existing CFD workflows without requiring modifications to the underlying flow solver, providing a practical approach to accelerating industrial CFD simulations through improved ML-based initialization strategies.</li>
</ul>

<h3>Title: AutoDrive-QA- Automated Generation of Multiple-Choice Questions for Autonomous Driving Datasets Using Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Boshra Khalili, Andrew W.Smyth</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15778">https://arxiv.org/abs/2503.15778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15778">https://arxiv.org/pdf/2503.15778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15778]] AutoDrive-QA- Automated Generation of Multiple-Choice Questions for Autonomous Driving Datasets Using Large Vision-Language Models(https://arxiv.org/abs/2503.15778)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In autonomous driving, open-ended question answering often suffers from unreliable evaluations because freeform responses require either complex metrics or subjective human judgment. To address this challenge, we introduce AutoDrive-QA, an automatic pipeline that converts existing driving QA datasets (including DriveLM, NuScenes-QA, and LingoQA) into a structured multiple-choice question (MCQ) format. This benchmark systematically assesses perception, prediction, and planning tasks, providing a standardized and objective evaluation framework. AutoDrive-QA employs an automated pipeline that leverages large language models (LLMs) to generate high-quality, contextually relevant distractors based on domain-specific error patterns commonly found in autonomous driving scenarios. To evaluate both general capabilities and generalization performance, we test the benchmark on three public datasets and conduct zero-shot experiments on an unseen dataset. The zero-shot evaluations reveal that GPT-4V leads with 69.57% accuracy -- achieving 74.94% in Perception, 65.33% in Prediction, and 68.45% in Planning -- demonstrating that while all models excel in Perception, they struggle in Prediction. Consequently, AutoDrive-QA establishes a rigorous, unbiased standard for integrating and evaluating different vision-language models across various autonomous driving datasets, thereby improving generalization in this field. We release all the codes in the AutoDrive-QA GitHub Repository.</li>
</ul>

<h3>Title: MobiFuse: Learning Universal Human Mobility Patterns through Cross-domain Data Fusion</h3>
<ul>
<li><strong>Authors: </strong>Haoxuan Ma, Xishun Liao, Yifan Liu, Qinhua Jiang, Chris Stanford, Shangqing Cao, Jiaqi Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15779">https://arxiv.org/abs/2503.15779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15779">https://arxiv.org/pdf/2503.15779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15779]] MobiFuse: Learning Universal Human Mobility Patterns through Cross-domain Data Fusion(https://arxiv.org/abs/2503.15779)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Human mobility modeling is critical for urban planning and transportation management, yet existing datasets often lack the resolution and semantic richness required for comprehensive analysis. To address this, we proposed a cross-domain data fusion framework that integrates multi-modal data of distinct nature and spatio-temporal resolution, including geographical, mobility, socio-demographic, and traffic information, to construct a privacy-preserving and semantically enriched human travel trajectory dataset. This framework is demonstrated through two case studies in Los Angeles (LA) and Egypt, where a domain adaptation algorithm ensures its transferability across diverse urban contexts. Quantitative evaluation shows that the generated synthetic dataset accurately reproduces mobility patterns observed in empirical data. Moreover, large-scale traffic simulations for LA County based on the generated synthetic demand align well with observed traffic. On California's I-405 corridor, the simulation yields a Mean Absolute Percentage Error of 5.85% for traffic volume and 4.36% for speed compared to Caltrans PeMS observations.</li>
</ul>

<h3>Title: Grammar and Gameplay-aligned RL for Game Description Generation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tsunehiko Tanaka, Edgar Simo-Serra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15783">https://arxiv.org/abs/2503.15783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15783">https://arxiv.org/pdf/2503.15783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15783]] Grammar and Gameplay-aligned RL for Game Description Generation with LLMs(https://arxiv.org/abs/2503.15783)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Game Description Generation (GDG) is the task of generating a game description written in a Game Description Language (GDL) from natural language text. Previous studies have explored generation methods leveraging the contextual understanding capabilities of Large Language Models (LLMs); however, accurately reproducing the game features of the game descriptions remains a challenge. In this paper, we propose reinforcement learning-based fine-tuning of LLMs for GDG (RLGDG). Our training method simultaneously improves grammatical correctness and fidelity to game concepts by introducing both grammar rewards and concept rewards. Furthermore, we adopt a two-stage training strategy where Reinforcement Learning (RL) is applied following Supervised Fine-Tuning (SFT). Experimental results demonstrate that our proposed method significantly outperforms baseline methods using SFT alone.</li>
</ul>

<h3>Title: RL4Med-DDPO: Reinforcement Learning for Controlled Guidance Towards Diverse Medical Image Generation using Vision-Language Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Parham Saremi, Amar Kumar, Mohammed Mohammed, Zahra TehraniNasab, Tal Arbel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15784">https://arxiv.org/abs/2503.15784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15784">https://arxiv.org/pdf/2503.15784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15784]] RL4Med-DDPO: Reinforcement Learning for Controlled Guidance Towards Diverse Medical Image Generation using Vision-Language Foundation Models(https://arxiv.org/abs/2503.15784)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Vision-Language Foundation Models (VLFM) have shown a tremendous increase in performance in terms of generating high-resolution, photorealistic natural images. While VLFMs show a rich understanding of semantic content across modalities, they often struggle with fine-grained alignment tasks that require precise correspondence between image regions and textual descriptions a limitation in medical imaging, where accurate localization and detection of clinical features are essential for diagnosis and analysis. To address this issue, we propose a multi-stage architecture where a pre-trained VLFM provides a cursory semantic understanding, while a reinforcement learning (RL) algorithm refines the alignment through an iterative process that optimizes for understanding semantic context. The reward signal is designed to align the semantic information of the text with synthesized images. We demonstrate the effectiveness of our method on a medical imaging skin dataset where the generated images exhibit improved generation quality and alignment with prompt over the fine-tuned Stable Diffusion. We also show that the synthesized samples could be used to improve disease classifier performance for underrepresented subgroups through augmentation.</li>
</ul>

<h3>Title: DNA Bench: When Silence is Smarter -- Benchmarking Over-Reasoning in Reasoning LLMs</h3>
<ul>
<li><strong>Authors: </strong>Masoud Hashemi, Oluwanifemi Bamgbose, Sathwik Tejaswi Madhusudhan, Jishnu Sethumadhavan Nair, Aman Tiwari, Vikas Yadav</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15793">https://arxiv.org/abs/2503.15793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15793">https://arxiv.org/pdf/2503.15793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15793]] DNA Bench: When Silence is Smarter -- Benchmarking Over-Reasoning in Reasoning LLMs(https://arxiv.org/abs/2503.15793)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Test-time scaling has significantly improved large language model performance, enabling deeper reasoning to solve complex problems. However, this increased reasoning capability also leads to excessive token generation and unnecessary problem-solving attempts. We introduce Dont Answer Bench (DNA Bench), a new benchmark designed to evaluate LLMs ability to robustly understand the tricky reasoning triggers and avoiding unnecessary generation. DNA Bench consists of 150 adversarially designed prompts that are easy for humans to understand and respond to, but surprisingly not for many of the recent prominent LLMs. DNA Bench tests models abilities across different capabilities, such as instruction adherence, hallucination avoidance, redundancy filtering, and unanswerable question recognition. We evaluate reasoning LLMs (RLMs), including DeepSeek-R1, OpenAI O3-mini, Claude-3.7-sonnet and compare them against a powerful non-reasoning model, e.g., GPT-4o. Our experiments reveal that RLMs generate up to 70x more tokens than necessary, often failing at tasks that simpler non-reasoning models handle efficiently with higher accuracy. Our findings underscore the need for more effective training and inference strategies in RLMs.</li>
</ul>

<h3>Title: Disentangling Uncertainties by Learning Compressed Data Representation</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu An, Zhibo Hou, Wan Du</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15801">https://arxiv.org/abs/2503.15801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15801">https://arxiv.org/pdf/2503.15801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15801]] Disentangling Uncertainties by Learning Compressed Data Representation(https://arxiv.org/abs/2503.15801)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We study aleatoric and epistemic uncertainty estimation in a learned regressive system dynamics model. Disentangling aleatoric uncertainty (the inherent randomness of the system) from epistemic uncertainty (the lack of data) is crucial for downstream tasks such as risk-aware control and reinforcement learning, efficient exploration, and robust policy transfer. While existing approaches like Gaussian Processes, Bayesian networks, and model ensembles are widely adopted, they suffer from either high computational complexity or inaccurate uncertainty estimation. To address these limitations, we propose the Compressed Data Representation Model (CDRM), a framework that learns a neural network encoding of the data distribution and enables direct sampling from the output distribution. Our approach incorporates a novel inference procedure based on Langevin dynamics sampling, allowing CDRM to predict arbitrary output distributions rather than being constrained to a Gaussian prior. Theoretical analysis provides the conditions where CDRM achieves better memory and computational complexity compared to bin-based compression methods. Empirical evaluations show that CDRM demonstrates a superior capability to identify aleatoric and epistemic uncertainties separately, achieving AUROCs of 0.8876 and 0.9981 on a single test set containing a mixture of both uncertainties. Qualitative results further show that CDRM's capability extends to datasets with multimodal output distributions, a challenging scenario where existing methods consistently fail. Code and supplementary materials are available at this https URL.</li>
</ul>

<h3>Title: Communication Efficient Federated Learning with Linear Convergence on Heterogeneous Data</h3>
<ul>
<li><strong>Authors: </strong>Jie Liu, Yongqiang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15804">https://arxiv.org/abs/2503.15804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15804">https://arxiv.org/pdf/2503.15804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15804]] Communication Efficient Federated Learning with Linear Convergence on Heterogeneous Data(https://arxiv.org/abs/2503.15804)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>By letting local clients perform multiple local updates before communicating with a parameter server, modern federated learning algorithms such as FedAvg tackle the communication bottleneck problem in distributed learning and have found many successful applications. However, this asynchrony between local updates and communication also leads to a ''client-drift'' problem when the data is heterogeneous (not independent and identically distributed), resulting in errors in the final learning result. In this paper, we propose a federated learning algorithm, which is called FedCET, to ensure accurate convergence even under heterogeneous distributions of data across clients. Inspired by the distributed optimization algorithm NIDS, we use learning rates to weight information received from local clients to eliminate the ''client-drift''. We prove that under appropriate learning rates, FedCET can ensure linear convergence to the exact solution. Different from existing algorithms which have to share both gradients and a drift-correction term to ensure accurate convergence under heterogeneous data distributions, FedCET only shares one variable, which significantly reduces communication overhead. Numerical comparison with existing counterpart algorithms confirms the effectiveness of FedCET.</li>
</ul>

<h3>Title: A Vision Centric Remote Sensing Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Abduljaleel Adejumo, Faegheh Yeganli, Clifford Broni-bediako, Aoran Xiao, Naoto Yokoya, Mennatullah Siam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15816">https://arxiv.org/abs/2503.15816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15816">https://arxiv.org/pdf/2503.15816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15816]] A Vision Centric Remote Sensing Benchmark(https://arxiv.org/abs/2503.15816)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have achieved remarkable success in vision-language tasks but their remote sensing (RS) counterpart are relatively under explored. Unlike natural images, RS imagery presents unique challenges that current MLLMs struggle to handle, particularly in visual grounding and spatial reasoning. This study investigates the limitations of CLIP-based MLLMs in RS, highlighting their failure to differentiate visually distinct yet semantically similar RS images. To address this, we introduce a remote sensing multimodal visual patterns (RSMMVP) benchmark. It is designed to evaluate MLLMs in RS tasks by identifying the CLIP-blind pairs, where CLIP-based models incorrectly assign high similarity scores to visually distinct RS images. Through a visual question answering (VQA) evaluation, we analyze the performance of state-of-the-art MLLMs, revealing significant limitations in RS specific representation learning. The results provide valuable insights into the weaknesses of CLIP-based visual encoding and offer a foundation for future research to develop more effective MLLMs tailored for remote sensing applications.</li>
</ul>

<h3>Title: Computation-Efficient and Recognition-Friendly 3D Point Cloud Privacy Protection</h3>
<ul>
<li><strong>Authors: </strong>Haotian Ma, Lin Gu, Siyi Wu, Yingying Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15818">https://arxiv.org/abs/2503.15818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15818">https://arxiv.org/pdf/2503.15818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15818]] Computation-Efficient and Recognition-Friendly 3D Point Cloud Privacy Protection(https://arxiv.org/abs/2503.15818)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, generative, segmentation</a></li>
<li><strong>Abstract: </strong>3D point cloud has been widely used in applications such as self-driving cars, robotics, CAD models, etc. To the best of our knowledge, these applications raised the issue of privacy leakage in 3D point clouds, which has not been studied well. Different from the 2D image privacy, which is related to texture and 2D geometric structure, the 3D point cloud is texture-less and only relevant to 3D geometric structure. In this work, we defined the 3D point cloud privacy problem and proposed an efficient privacy-preserving framework named PointFlowGMM that can support downstream classification and segmentation tasks without seeing the original data. Using a flow-based generative model, the point cloud is projected into a latent Gaussian mixture distributed subspace. We further designed a novel angular similarity loss to obfuscate the original geometric structure and reduce the model size from 767MB to 120MB without a decrease in recognition performance. The projected point cloud in the latent space is orthogonally rotated randomly to further protect the original geometric structure, the class-to-class relationship is preserved after rotation, thus, the protected point cloud can support the recognition task. We evaluated our model on multiple datasets and achieved comparable recognition results on encrypted point clouds compared to the original point clouds.</li>
</ul>

<h3>Title: EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation</h3>
<ul>
<li><strong>Authors: </strong>Zihao Zhang, Haoran Chen, Haoyu Zhao, Guansong Lu, Yanwei Fu, Hang Xu, Zuxuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15831">https://arxiv.org/abs/2503.15831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15831">https://arxiv.org/pdf/2503.15831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15831]] EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation(https://arxiv.org/abs/2503.15831)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Handling complex or nonlinear motion patterns has long posed challenges for video frame interpolation. Although recent advances in diffusion-based methods offer improvements over traditional optical flow-based approaches, they still struggle to generate sharp, temporally consistent frames in scenarios with large motion. To address this limitation, we introduce EDEN, an Enhanced Diffusion for high-quality large-motion vidEo frame iNterpolation. Our approach first utilizes a transformer-based tokenizer to produce refined latent representations of the intermediate frames for diffusion models. We then enhance the diffusion transformer with temporal attention across the process and incorporate a start-end frame difference embedding to guide the generation of dynamic motion. Extensive experiments demonstrate that EDEN achieves state-of-the-art results across popular benchmarks, including nearly a 10% LPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD.</li>
</ul>

<h3>Title: BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Yiren Lu, Yunlai Zhou, Disheng Liu, Tuo Liang, Yu Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15835">https://arxiv.org/abs/2503.15835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15835">https://arxiv.org/pdf/2503.15835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15835]] BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting(https://arxiv.org/abs/2503.15835)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (3DGS) has shown remarkable potential for static scene reconstruction, and recent advancements have extended its application to dynamic scenes. However, the quality of reconstructions depends heavily on high-quality input images and precise camera poses, which are not that trivial to fulfill in real-world scenarios. Capturing dynamic scenes with handheld monocular cameras, for instance, typically involves simultaneous movement of both the camera and objects within a single exposure. This combined motion frequently results in image blur that existing methods cannot adequately handle. To address these challenges, we introduce BARD-GS, a novel approach for robust dynamic scene reconstruction that effectively handles blurry inputs and imprecise camera poses. Our method comprises two main components: 1) camera motion deblurring and 2) object motion deblurring. By explicitly decomposing motion blur into camera motion blur and object motion blur and modeling them separately, we achieve significantly improved rendering results in dynamic regions. In addition, we collect a real-world motion blur dataset of dynamic scenes to evaluate our approach. Extensive experiments demonstrate that BARD-GS effectively reconstructs high-quality dynamic scenes under realistic conditions, significantly outperforming existing methods.</li>
</ul>

<h3>Title: Fùxì: A Benchmark for Evaluating Language Models on Ancient Chinese Text Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Shangqing Zhao, Yuhao Zhou, Yupei Ren, Zhe Chen, Chenghao Jia, Fang Zhe, Zhaogaung Long, Shu Liu, Man Lan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15837">https://arxiv.org/abs/2503.15837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15837">https://arxiv.org/pdf/2503.15837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15837]] Fùxì: A Benchmark for Evaluating Language Models on Ancient Chinese Text Understanding and Generation(https://arxiv.org/abs/2503.15837)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Ancient Chinese text processing presents unique challenges for large language models (LLMs) due to its distinct linguistic features, complex structural constraints, and rich cultural context. While existing benchmarks have primarily focused on evaluating comprehension through multiple-choice questions, there remains a critical gap in assessing models' generative capabilities in classical Chinese. We introduce Fùxì, a comprehensive benchmark that evaluates both understanding and generation capabilities across 21 diverse tasks. Our benchmark distinguishes itself through three key contributions: (1) balanced coverage of both comprehension and generation tasks, including novel tasks like poetry composition and couplet completion, (2) specialized evaluation metrics designed specifically for classical Chinese text generation, combining rule-based verification with fine-tuned LLM evaluators, and (3) a systematic assessment framework that considers both linguistic accuracy and cultural authenticity. Through extensive evaluation of state-of-the-art LLMs, we reveal significant performance gaps between understanding and generation tasks, with models achieving promising results in comprehension but struggling considerably in generation tasks, particularly those requiring deep cultural knowledge and adherence to classical formats. Our findings highlight the current limitations in ancient Chinese text processing and provide insights for future model development. The benchmark, evaluation toolkit, and baseline results are publicly available to facilitate research in this domain.</li>
</ul>

<h3>Title: FedAWA: Adaptive Optimization of Aggregation Weights in Federated Learning Using Client Vectors</h3>
<ul>
<li><strong>Authors: </strong>Changlong Shi, He Zhao, Bingjie Zhang, Mingyuan Zhou, Dandan Guo, Yi Chang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15842">https://arxiv.org/abs/2503.15842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15842">https://arxiv.org/pdf/2503.15842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15842]] FedAWA: Adaptive Optimization of Aggregation Weights in Federated Learning Using Client Vectors(https://arxiv.org/abs/2503.15842)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has emerged as a promising framework for distributed machine learning, enabling collaborative model training without sharing local data, thereby preserving privacy and enhancing security. However, data heterogeneity resulting from differences across user behaviors, preferences, and device characteristics poses a significant challenge for federated learning. Most previous works overlook the adjustment of aggregation weights, relying solely on dataset size for weight assignment, which often leads to unstable convergence and reduced model performance. Recently, several studies have sought to refine aggregation strategies by incorporating dataset characteristics and model alignment. However, adaptively adjusting aggregation weights while ensuring data security-without requiring additional proxy data-remains a significant challenge. In this work, we propose Federated learning with Adaptive Weight Aggregation (FedAWA), a novel method that adaptively adjusts aggregation weights based on client vectors during the learning process. The client vector captures the direction of model updates, reflecting local data variations, and is used to optimize the aggregation weight without requiring additional datasets or violating privacy. By assigning higher aggregation weights to local models whose updates align closely with the global optimization direction, FedAWA enhances the stability and generalization of the global model. Extensive experiments under diverse scenarios demonstrate the superiority of our method, providing a promising solution to the challenges of data heterogeneity in federated learning.</li>
</ul>

<h3>Title: Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Xiaoou Liu, Tiejin Chen, Longchao Da, Chacha Chen, Zhen Lin, Hua Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15850">https://arxiv.org/abs/2503.15850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15850">https://arxiv.org/pdf/2503.15850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15850]] Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey(https://arxiv.org/abs/2503.15850)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel in text generation, reasoning, and decision-making, enabling their adoption in high-stakes domains such as healthcare, law, and transportation. However, their reliability is a major concern, as they often produce plausible but incorrect responses. Uncertainty quantification (UQ) enhances trustworthiness by estimating confidence in outputs, enabling risk mitigation and selective prediction. However, traditional UQ methods struggle with LLMs due to computational constraints and decoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources, such as input ambiguity, reasoning path divergence, and decoding stochasticity, that extend beyond classical aleatoric and epistemic uncertainty. To address this, we introduce a new taxonomy that categorizes UQ methods based on computational efficiency and uncertainty dimensions (input, reasoning, parameter, and prediction uncertainty). We evaluate existing techniques, assess their real-world applicability, and identify open challenges, emphasizing the need for scalable, interpretable, and robust UQ approaches to enhance LLM reliability.</li>
</ul>

<h3>Title: Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Zhou Zhenglin, Ma Fan, Fan Hehe, Chua Tat-Seng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15851">https://arxiv.org/abs/2503.15851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15851">https://arxiv.org/pdf/2503.15851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15851]] Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video Diffusion(https://arxiv.org/abs/2503.15851)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, data-free</a></li>
<li><strong>Abstract: </strong>Animatable head avatar generation typically requires extensive data for training. To reduce the data requirements, a natural solution is to leverage existing data-free static avatar generation methods, such as pre-trained diffusion models with score distillation sampling (SDS), which align avatars with pseudo ground-truth outputs from the diffusion model. However, directly distilling 4D avatars from video diffusion often leads to over-smooth results due to spatial and temporal inconsistencies in the generated video. To address this issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial and temporal consistency dataset for 4D avatar reconstruction using the video diffusion model. Specifically, Zero-1-to-A iteratively constructs video datasets and optimizes animatable avatars in a progressive manner, ensuring that avatar quality increases smoothly and consistently throughout the learning process. This progressive learning involves two stages: (1) Spatial Consistency Learning fixes expressions and learns from front-to-side views, and (2) Temporal Consistency Learning fixes views and learns from relaxed to exaggerated expressions, generating 4D avatars in a simple-to-complex manner. Extensive experiments demonstrate that Zero-1-to-A improves fidelity, animation quality, and rendering speed compared to existing diffusion-based methods, providing a solution for lifelike avatar creation. Code is publicly available at: this https URL.</li>
</ul>

<h3>Title: Network Embedding Exploration Tool (NEExT)</h3>
<ul>
<li><strong>Authors: </strong>Ashkan Dehghan, Paweł Prałat, François Théberge</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15853">https://arxiv.org/abs/2503.15853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15853">https://arxiv.org/pdf/2503.15853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15853]] Network Embedding Exploration Tool (NEExT)(https://arxiv.org/abs/2503.15853)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Many real-world and artificial systems and processes can be represented as graphs. Some examples of such systems include social networks, financial transactions, supply chains, and molecular structures. In many of these cases, one needs to consider a collection of graphs, rather than a single network. This could be a collection of distinct but related graphs, such as different protein structures or graphs resulting from dynamic processes on the same network. Examples of the latter include the evolution of social networks, community-induced graphs, or ego-nets around various nodes. A significant challenge commonly encountered is the absence of ground-truth labels for graphs or nodes, necessitating the use of unsupervised techniques to analyze such systems. Moreover, even when ground-truth labels are available, many existing graph machine learning methods depend on complex deep learning models, complicating model explainability and interpretability. To address some of these challenges, we have introduced NEExT (Network Embedding Exploration Tool) for embedding collections of graphs via user-defined node features. The advantages of the framework are twofold: (i) the ability to easily define your own interpretable node-based features in view of the task at hand, and (ii) fast embedding of graphs provided by the Vectorizers library. In this paper, we demonstrate the usefulness of NEExT on collections of synthetic and real-world graphs. For supervised tasks, we demonstrate that performance in graph classification tasks could be achieved similarly to other state-of-the-art techniques while maintaining model interpretability. Furthermore, our framework can also be used to generate high-quality embeddings in an unsupervised way, where target variables are not available.</li>
</ul>

<h3>Title: VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting Generation with Flexible Pose and Multi-View Joint Modeling</h3>
<ul>
<li><strong>Authors: </strong>Hyojun Go, Byeongjun Park, Hyelin Nam, Byung-Hoon Kim, Hyungjin Chung, Changick Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15855">https://arxiv.org/abs/2503.15855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15855">https://arxiv.org/pdf/2503.15855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15855]] VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting Generation with Flexible Pose and Multi-View Joint Modeling(https://arxiv.org/abs/2503.15855)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose VideoRFSplat, a direct text-to-3D model leveraging a video generation model to generate realistic 3D Gaussian Splatting (3DGS) for unbounded real-world scenes. To generate diverse camera poses and unbounded spatial extent of real-world scenes, while ensuring generalization to arbitrary text prompts, previous methods fine-tune 2D generative models to jointly model camera poses and multi-view images. However, these methods suffer from instability when extending 2D generative models to joint modeling due to the modality gap, which necessitates additional models to stabilize training and inference. In this work, we propose an architecture and a sampling strategy to jointly model multi-view images and camera poses when fine-tuning a video generation model. Our core idea is a dual-stream architecture that attaches a dedicated pose generation model alongside a pre-trained video generation model via communication blocks, generating multi-view images and camera poses through separate streams. This design reduces interference between the pose and image modalities. Additionally, we propose an asynchronous sampling strategy that denoises camera poses faster than multi-view images, allowing rapidly denoised poses to condition multi-view generation, reducing mutual ambiguity and enhancing cross-modal consistency. Trained on multiple large-scale real-world datasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms existing text-to-3D direct generation methods that heavily depend on post-hoc refinement via score distillation sampling, achieving superior results without such refinement.</li>
</ul>

<h3>Title: DroidTTP: Mapping Android Applications with TTP for Cyber Threat Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Dincy R Arikkat, Vinod P., Rafidha Rehiman K. A., Serena Nicolazzo, Marco Arazzi, Antonino Nocera, Mauro Conti</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15866">https://arxiv.org/abs/2503.15866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15866">https://arxiv.org/pdf/2503.15866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15866]] DroidTTP: Mapping Android Applications with TTP for Cyber Threat Intelligence(https://arxiv.org/abs/2503.15866)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The widespread adoption of Android devices for sensitive operations like banking and communication has made them prime targets for cyber threats, particularly Advanced Persistent Threats (APT) and sophisticated malware attacks. Traditional malware detection methods rely on binary classification, failing to provide insights into adversarial Tactics, Techniques, and Procedures (TTPs). Understanding malware behavior is crucial for enhancing cybersecurity defenses. To address this gap, we introduce DroidTTP, a framework mapping Android malware behaviors to TTPs based on the MITRE ATT&CK framework. Our curated dataset explicitly links MITRE TTPs to Android applications. We developed an automated solution leveraging the Problem Transformation Approach (PTA) and Large Language Models (LLMs) to map applications to both Tactics and Techniques. Additionally, we employed Retrieval-Augmented Generation (RAG) with prompt engineering and LLM fine-tuning for TTP predictions. Our structured pipeline includes dataset creation, hyperparameter tuning, data augmentation, feature selection, model development, and SHAP-based model interpretability. Among LLMs, Llama achieved the highest performance in Tactic classification with a Jaccard Similarity of 0.9583 and Hamming Loss of 0.0182, and in Technique classification with a Jaccard Similarity of 0.9348 and Hamming Loss of 0.0127. However, the Label Powerset XGBoost model outperformed LLMs, achieving a Jaccard Similarity of 0.9893 for Tactic classification and 0.9753 for Technique classification, with a Hamming Loss of 0.0054 and 0.0050, respectively. While XGBoost showed superior performance, the narrow margin highlights the potential of LLM-based approaches in TTP classification.</li>
</ul>

<h3>Title: TruthLens: Explainable DeepFake Detection for Face Manipulated and Fully Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Rohit Kundu, Athula Balachandran, Amit K. Roy-Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15867">https://arxiv.org/abs/2503.15867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15867">https://arxiv.org/pdf/2503.15867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15867]] TruthLens: Explainable DeepFake Detection for Face Manipulated and Fully Synthetic Data(https://arxiv.org/abs/2503.15867)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability, explainability, large language model</a></li>
<li><strong>Abstract: </strong>Detecting DeepFakes has become a crucial research area as the widespread use of AI image generators enables the effortless creation of face-manipulated and fully synthetic content, yet existing methods are often limited to binary classification (real vs. fake) and lack interpretability. To address these challenges, we propose TruthLens, a novel and highly generalizable framework for DeepFake detection that not only determines whether an image is real or fake but also provides detailed textual reasoning for its predictions. Unlike traditional methods, TruthLens effectively handles both face-manipulated DeepFakes and fully AI-generated content while addressing fine-grained queries such as "Does the eyes/nose/mouth look real or fake?" The architecture of TruthLens combines the global contextual understanding of multimodal large language models like PaliGemma2 with the localized feature extraction capabilities of vision-only models like DINOv2. This hybrid design leverages the complementary strengths of both models, enabling robust detection of subtle manipulations while maintaining interpretability. Extensive experiments on diverse datasets demonstrate that TruthLens outperforms state-of-the-art methods in detection accuracy (by 2-14%) and explainability, in both in-domain and cross-data settings, generalizing effectively across traditional and emerging manipulation techniques.</li>
</ul>

<h3>Title: UniCoRN: Latent Diffusion-based Unified Controllable Image Restoration Network across Multiple Degradations</h3>
<ul>
<li><strong>Authors: </strong>Debabrata Mandal, Soumitri Chattopadhyay, Guansen Tong, Praneeth Chakravarthula</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15868">https://arxiv.org/abs/2503.15868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15868">https://arxiv.org/pdf/2503.15868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15868]] UniCoRN: Latent Diffusion-based Unified Controllable Image Restoration Network across Multiple Degradations(https://arxiv.org/abs/2503.15868)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Image restoration is essential for enhancing degraded images across computer vision tasks. However, most existing methods address only a single type of degradation (e.g., blur, noise, or haze) at a time, limiting their real-world applicability where multiple degradations often occur simultaneously. In this paper, we propose UniCoRN, a unified image restoration approach capable of handling multiple degradation types simultaneously using a multi-head diffusion model. Specifically, we uncover the potential of low-level visual cues extracted from images in guiding a controllable diffusion model for real-world image restoration and we design a multi-head control network adaptable via a mixture-of-experts strategy. We train our model without any prior assumption of specific degradations, through a smartly designed curriculum learning recipe. Additionally, we also introduce MetaRestore, a metalens imaging benchmark containing images with multiple degradations and artifacts. Extensive evaluations on several challenging datasets, including our benchmark, demonstrate that our method achieves significant performance gains and can robustly restore images with severe degradations. Project page: this https URL</li>
</ul>

<h3>Title: FedSAF: A Federated Learning Framework for Enhanced Gastric Cancer Detection and Privacy Preservation</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Miao, Xinyuan Yang, Hongda Fan, Yichun Li, Yishu Hong, Xiechen Guo, Ali Braytee, Weidong Huang, Ali Anaissi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15870">https://arxiv.org/abs/2503.15870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15870">https://arxiv.org/pdf/2503.15870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15870]] FedSAF: A Federated Learning Framework for Enhanced Gastric Cancer Detection and Privacy Preservation(https://arxiv.org/abs/2503.15870)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Gastric cancer is one of the most commonly diagnosed cancers and has a high mortality rate. Due to limited medical resources, developing machine learning models for gastric cancer recognition provides an efficient solution for medical institutions. However, such models typically require large sample sizes for training and testing, which can challenge patient privacy. Federated learning offers an effective alternative by enabling model training across multiple institutions without sharing sensitive patient data. This paper addresses the limited sample size of publicly available gastric cancer data with a modified data processing method. This paper introduces FedSAF, a novel federated learning algorithm designed to improve the performance of existing methods, particularly in non-independent and identically distributed (non-IID) data scenarios. FedSAF incorporates attention-based message passing and the Fisher Information Matrix to enhance model accuracy, while a model splitting function reduces computation and transmission costs. Hyperparameter tuning and ablation studies demonstrate the effectiveness of this new algorithm, showing improvements in test accuracy on gastric cancer datasets, with FedSAF outperforming existing federated learning methods like FedAMP, FedAvg, and FedProx. The framework's robustness and generalization ability were further validated across additional datasets (SEED, BOT, FashionMNIST, and CIFAR-10), achieving high performance in diverse environments.</li>
</ul>

<h3>Title: MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations</h3>
<ul>
<li><strong>Authors: </strong>Kyungho Bae, Jinhyung Kim, Sihaeng Lee, Soonyoung Lee, Gunhee Lee, Jinwoo Choi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15871">https://arxiv.org/abs/2503.15871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15871">https://arxiv.org/pdf/2503.15871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15871]] MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations(https://arxiv.org/abs/2503.15871)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this work, we tackle action-scene hallucination in Video Large Language Models (Video-LLMs), where models incorrectly predict actions based on the scene context or scenes based on observed actions. We observe that existing Video-LLMs often suffer from action-scene hallucination due to two main factors. First, existing Video-LLMs intermingle spatial and temporal features by applying an attention operation across all tokens. Second, they use the standard Rotary Position Embedding (RoPE), which causes the text tokens to overemphasize certain types of tokens depending on their sequential orders. To address these issues, we introduce MASH-VLM, Mitigating Action-Scene Hallucination in Video-LLMs through disentangled spatial-temporal representations. Our approach includes two key innovations: (1) DST-attention, a novel attention mechanism that disentangles the spatial and temporal tokens within the LLM by using masked attention to restrict direct interactions between the spatial and temporal tokens; (2) Harmonic-RoPE, which extends the dimensionality of the positional IDs, allowing the spatial and temporal tokens to maintain balanced positions relative to the text tokens. To evaluate the action-scene hallucination in Video-LLMs, we introduce the UNSCENE benchmark with 1,320 videos and 4,078 QA pairs. Extensive experiments demonstrate that MASH-VLM achieves state-of-the-art results on the UNSCENE benchmark, as well as on existing video understanding benchmarks.</li>
</ul>

<h3>Title: Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Tiange Xiang, Kai Li, Chengjiang Long, Christian Häne, Peihong Guo, Scott Delp, Ehsan Adeli, Li Fei-Fei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15877">https://arxiv.org/abs/2503.15877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15877">https://arxiv.org/pdf/2503.15877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15877]] Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation(https://arxiv.org/abs/2503.15877)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-image diffusion models have been driven by the increasing availability of paired 2D data. However, the development of 3D diffusion models has been hindered by the scarcity of high-quality 3D data, resulting in less competitive performance compared to their 2D counterparts. To address this challenge, we propose repurposing pre-trained 2D diffusion models for 3D object generation. We introduce Gaussian Atlas, a novel representation that utilizes dense 2D grids, enabling the fine-tuning of 2D diffusion models to generate 3D Gaussians. Our approach demonstrates successful transfer learning from a pre-trained 2D diffusion model to a 2D manifold flattened from 3D structures. To support model training, we compile GaussianVerse, a large-scale dataset comprising 205K high-quality 3D Gaussian fittings of various 3D objects. Our experimental results show that text-to-image diffusion models can be effectively adapted for 3D content generation, bridging the gap between 2D and 3D modeling.</li>
</ul>

<h3>Title: Using Data Redundancy Techniques to Detect and Correct Errors in Logical Data</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Sharuvan, Ahmed Naufal Abdul Hadee</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15881">https://arxiv.org/abs/2503.15881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15881">https://arxiv.org/pdf/2503.15881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15881]] Using Data Redundancy Techniques to Detect and Correct Errors in Logical Data(https://arxiv.org/abs/2503.15881)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Data redundancy techniques have been tested in several different applications to provide fault tolerance and performance gains. The use of these techniques is mostly seen at the hardware, device driver, or file system level. In practice, the use of data integrity techniques with logical data has largely been limited to verifying the integrity of transferred files using cryptographic hashes. In this paper, we study the RAID scheme used with disk arrays and adapt it for use with logical data. An implementation for such a system is devised in theory and implemented in software, providing the specifications for the procedures and file formats used. Rigorous experimentation is conducted to test the effectiveness of the developed system for multiple use cases. With computer-generated benchmarks and simulated experiments, the system demonstrates robust performance in recovering arbitrary faults in large archive files only using a small fraction of redundant data. This was achieved by leveraging computing power for the process of data recovery.</li>
</ul>

<h3>Title: Enhancing Zero-Shot Image Recognition in Vision-Language Models through Human-like Concept Guidance</h3>
<ul>
<li><strong>Authors: </strong>Hui Liu, Wenya Wang, Kecheng Chen, Jie Liu, Yibing Liu, Tiexin Qin, Peisong He, Xinghao Jiang, Haoliang Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15886">https://arxiv.org/abs/2503.15886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15886">https://arxiv.org/pdf/2503.15886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15886]] Enhancing Zero-Shot Image Recognition in Vision-Language Models through Human-like Concept Guidance(https://arxiv.org/abs/2503.15886)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In zero-shot image recognition tasks, humans demonstrate remarkable flexibility in classifying unseen categories by composing known simpler concepts. However, existing vision-language models (VLMs), despite achieving significant progress through large-scale natural language supervision, often underperform in real-world applications because of sub-optimal prompt engineering and the inability to adapt effectively to target classes. To address these issues, we propose a Concept-guided Human-like Bayesian Reasoning (CHBR) framework. Grounded in Bayes' theorem, CHBR models the concept used in human image recognition as latent variables and formulates this task by summing across potential concepts, weighted by a prior distribution and a likelihood function. To tackle the intractable computation over an infinite concept space, we introduce an importance sampling algorithm that iteratively prompts large language models (LLMs) to generate discriminative concepts, emphasizing inter-class differences. We further propose three heuristic approaches involving Average Likelihood, Confidence Likelihood, and Test Time Augmentation (TTA) Likelihood, which dynamically refine the combination of concepts based on the test image. Extensive evaluations across fifteen datasets demonstrate that CHBR consistently outperforms existing state-of-the-art zero-shot generalization methods.</li>
</ul>

<h3>Title: DocVideoQA: Towards Comprehensive Understanding of Document-Centric Videos through Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Haochen Wang, Kai Hu, Liangcai Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15887">https://arxiv.org/abs/2503.15887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15887">https://arxiv.org/pdf/2503.15887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15887]] DocVideoQA: Towards Comprehensive Understanding of Document-Centric Videos through Question Answering(https://arxiv.org/abs/2503.15887)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Remote work and online courses have become important methods of knowledge dissemination, leading to a large number of document-based instructional videos. Unlike traditional video datasets, these videos mainly feature rich-text images and audio that are densely packed with information closely tied to the visual content, requiring advanced multimodal understanding capabilities. However, this domain remains underexplored due to dataset availability and its inherent complexity. In this paper, we introduce the DocVideoQA task and dataset for the first time, comprising 1454 videos across 23 categories with a total duration of about 828 hours. The dataset is annotated with 154k question-answer pairs generated manually and via GPT, assessing models' comprehension, temporal awareness, and modality integration capabilities. Initially, we establish a baseline using open-source MLLMs. Recognizing the challenges in modality comprehension for document-centric videos, we present DV-LLaMA, a robust video MLLM baseline. Our method enhances unimodal feature extraction with diverse instruction-tuning data and employs contrastive learning to strengthen modality integration. Through fine-tuning, the LLM is equipped with audio-visual capabilities, leading to significant improvements in document-centric video understanding. Extensive testing on the DocVideoQA dataset shows that DV-LLaMA significantly outperforms existing models. We'll release the code and dataset to facilitate future research.</li>
</ul>

<h3>Title: Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Baolong Bi, Shenghua Liu, Yiwei Wang, Yilong Xu, Junfeng Fang, Lingrui Mei, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15888">https://arxiv.org/abs/2503.15888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15888">https://arxiv.org/pdf/2503.15888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15888]] Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in Language Models(https://arxiv.org/abs/2503.15888)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) mitigates hallucinations in Large Language Models (LLMs) by integrating external knowledge. However, conflicts between parametric knowledge and retrieved context pose challenges, particularly when retrieved information is unreliable or the model's internal knowledge is outdated. In such cases, LLMs struggle to determine whether to rely more on their own parameters or the conflicted context. To address this, we propose **CK-PLUG**, a plug-and-play method for controlling LLMs' reliance on parametric and contextual knowledge. We introduce a novel knowledge consistency metric, Confidence Gain, which detects knowledge conflicts by measuring entropy shifts in token probability distributions after context insertion. CK-PLUG then enables fine-grained control over knowledge preference by adjusting the probability distribution of tokens with negative confidence gain through a single tuning parameter. Experiments demonstrate CK-PLUG's ability to significantly regulate knowledge reliance in counterfactual RAG scenarios while maintaining generation fluency and knowledge accuracy. For instance, on Llama3-8B, memory recall (MR) of RAG response can be adjusted within a broad range (9.9%-71.9%), compared to the baseline of 42.1%. Moreover, CK-PLUG supports adaptive control based on the model's confidence in both internal and external knowledge, achieving consistent performance improvements across various general RAG tasks. Our code is available at: $\href{this https URL}{\text{this https URL}}$.</li>
</ul>

<h3>Title: LeanTTA: A Backpropagation-Free and Stateless Approach to Quantized Test-Time Adaptation on Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Cynthia Dong, Hong Jia, Young D. Kwon, Georgios Rizos, Cecilia Mascolo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15889">https://arxiv.org/abs/2503.15889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15889">https://arxiv.org/pdf/2503.15889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15889]] LeanTTA: A Backpropagation-Free and Stateless Approach to Quantized Test-Time Adaptation on Edge Devices(https://arxiv.org/abs/2503.15889)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While there are many advantages to deploying machine learning models on edge devices, the resource constraints of mobile platforms, the dynamic nature of the environment, and differences between the distribution of training versus in-the-wild data make such deployments challenging. Current test-time adaptation methods are often memory-intensive and not designed to be quantization-compatible or deployed on low-resource devices. To address these challenges, we present LeanTTA, a novel backpropagation-free and stateless framework for quantized test-time adaptation tailored to edge devices. Our approach minimizes computational costs by dynamically updating normalization statistics without backpropagation, which frees LeanTTA from the common pitfall of relying on large batches and historical data, making our method robust to realistic deployment scenarios. Our approach is the first to enable further computational gains by combining partial adaptation with quantized module fusion. We validate our framework across sensor modalities, demonstrating significant improvements over state-of-the-art TTA methods, including a 15.7% error reduction, peak memory usage of only 11.2MB for ResNet18, and fast adaptation within an order-of-magnitude of normal inference speeds on-device. LeanTTA provides a robust solution for achieving the right trade offs between accuracy and system efficiency in edge deployments, addressing the unique challenges posed by limited data and varied operational conditions.</li>
</ul>

<h3>Title: Time After Time: Deep-Q Effect Estimation for Interventions on When and What to do</h3>
<ul>
<li><strong>Authors: </strong>Yoav Wald, Mark Goldstein, Yonathan Efroni, Wouter A.C. van Amsterdam, Rajesh Ranganath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15890">https://arxiv.org/abs/2503.15890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15890">https://arxiv.org/pdf/2503.15890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15890]] Time After Time: Deep-Q Effect Estimation for Interventions on When and What to do(https://arxiv.org/abs/2503.15890)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Problems in fields such as healthcare, robotics, and finance requires reasoning about the value both of what decision or action to take and when to take it. The prevailing hope is that artificial intelligence will support such decisions by estimating the causal effect of policies such as how to treat patients or how to allocate resources over time. However, existing methods for estimating the effect of a policy struggle with \emph{irregular time}. They either discretize time, or disregard the effect of timing policies. We present a new deep-Q algorithm that estimates the effect of both when and what to do called Earliest Disagreement Q-Evaluation (EDQ). EDQ makes use of recursion for the Q-function that is compatible with flexible sequence models, such as transformers. EDQ provides accurate estimates under standard assumptions. We validate the approach through experiments on survival time and tumor growth tasks.</li>
</ul>

<h3>Title: UniHDSA: A Unified Relation Prediction Approach for Hierarchical Document Structure Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Wang, Kai Hu, Qiang Huo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15893">https://arxiv.org/abs/2503.15893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15893">https://arxiv.org/pdf/2503.15893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15893]] UniHDSA: A Unified Relation Prediction Approach for Hierarchical Document Structure Analysis(https://arxiv.org/abs/2503.15893)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Document structure analysis, aka document layout analysis, is crucial for understanding both the physical layout and logical structure of documents, serving information retrieval, document summarization, knowledge extraction, etc. Hierarchical Document Structure Analysis (HDSA) specifically aims to restore the hierarchical structure of documents created using authoring software with hierarchical schemas. Previous research has primarily followed two approaches: one focuses on tackling specific subtasks of HDSA in isolation, such as table detection or reading order prediction, while the other adopts a unified framework that uses multiple branches or modules, each designed to address a distinct task. In this work, we propose a unified relation prediction approach for HDSA, called UniHDSA, which treats various HDSA sub-tasks as relation prediction problems and consolidates relation prediction labels into a unified label space. This allows a single relation prediction module to handle multiple tasks simultaneously, whether at a page-level or document-level structure analysis. To validate the effectiveness of UniHDSA, we develop a multimodal end-to-end system based on Transformer architectures. Extensive experimental results demonstrate that our approach achieves state-of-the-art performance on a hierarchical document structure analysis benchmark, Comp-HRDoc, and competitive results on a large-scale document layout analysis dataset, DocLayNet, effectively illustrating the superiority of our method across all sub-tasks.</li>
</ul>

<h3>Title: Learning 3D Scene Analogies with Neural Contextual Scene Maps</h3>
<ul>
<li><strong>Authors: </strong>Junho Kim, Gwangtak Bae, Eun Sun Lee, Young Min Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15897">https://arxiv.org/abs/2503.15897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15897">https://arxiv.org/pdf/2503.15897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15897]] Learning 3D Scene Analogies with Neural Contextual Scene Maps(https://arxiv.org/abs/2503.15897)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Understanding scene contexts is crucial for machines to perform tasks and adapt prior knowledge in unseen or noisy 3D environments. As data-driven learning is intractable to comprehensively encapsulate diverse ranges of layouts and open spaces, we propose teaching machines to identify relational commonalities in 3D spaces. Instead of focusing on point-wise or object-wise representations, we introduce 3D scene analogies, which are smooth maps between 3D scene regions that align spatial relationships. Unlike well-studied single instance-level maps, these scene-level maps smoothly link large scene regions, potentially enabling unique applications in trajectory transfer in AR/VR, long demonstration transfer for imitation learning, and context-aware object rearrangement. To find 3D scene analogies, we propose neural contextual scene maps, which extract descriptor fields summarizing semantic and geometric contexts, and holistically align them in a coarse-to-fine manner for map estimation. This approach reduces reliance on individual feature points, making it robust to input noise or shape variations. Experiments demonstrate the effectiveness of our approach in identifying scene analogies and transferring trajectories or object placements in diverse indoor scenes, indicating its potential for robotics and AR/VR applications.</li>
</ul>

<h3>Title: On the Limits of Applying Graph Transformers for Brain Connectome Classification</h3>
<ul>
<li><strong>Authors: </strong>Jose Lara-Rangel, Clare Heinbaugh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15902">https://arxiv.org/abs/2503.15902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15902">https://arxiv.org/pdf/2503.15902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15902]] On the Limits of Applying Graph Transformers for Brain Connectome Classification(https://arxiv.org/abs/2503.15902)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Brain connectomes offer detailed maps of neural connections within the brain. Recent studies have proposed novel connectome graph datasets and attempted to improve connectome classification by using graph deep learning. With recent advances demonstrating transformers' ability to model intricate relationships and outperform in various domains, this work explores their performance on the novel NeuroGraph benchmark datasets and synthetic variants derived from probabilistically removing edges to simulate noisy data. Our findings suggest that graph transformers offer no major advantage over traditional GNNs on this dataset. Furthermore, both traditional and transformer GNN models maintain accuracy even with all edges removed, suggesting that the dataset's graph structures may not significantly impact predictions. We propose further assessing NeuroGraph as a brain connectome benchmark, emphasizing the need for well-curated datasets and improved preprocessing strategies to obtain meaningful edge connections.</li>
</ul>

<h3>Title: From Structured Prompts to Open Narratives: Measuring Gender Bias in LLMs Through Open-Ended Storytelling</h3>
<ul>
<li><strong>Authors: </strong>Evan Chen, Run-Jun Zhan, Yan-Bai Lin, Hung-Hsuan Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15904">https://arxiv.org/abs/2503.15904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15904">https://arxiv.org/pdf/2503.15904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15904]] From Structured Prompts to Open Narratives: Measuring Gender Bias in LLMs Through Open-Ended Storytelling(https://arxiv.org/abs/2503.15904)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized natural language processing, yet concerns persist regarding their tendency to reflect or amplify social biases present in their training data. This study introduces a novel evaluation framework to uncover gender biases in LLMs, focusing on their occupational narratives. Unlike previous methods relying on structured scenarios or carefully crafted prompts, our approach leverages free-form storytelling to reveal biases embedded in the models. Systematic analyses show an overrepresentation of female characters across occupations in six widely used LLMs. Additionally, our findings reveal that LLM-generated occupational gender rankings align more closely with human stereotypes than actual labor statistics. These insights underscore the need for balanced mitigation strategies to ensure fairness while avoiding the reinforcement of new stereotypes.</li>
</ul>

<h3>Title: Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Jiyuan Wang, Chunyu Lin, Cheng Guan, Lang Nie, Jing He, Haodong Li, Kang Liao, Yao Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15905">https://arxiv.org/abs/2503.15905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15905">https://arxiv.org/pdf/2503.15905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15905]] Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation(https://arxiv.org/abs/2503.15905)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we propose Jasmine, the first Stable Diffusion (SD)-based self-supervised framework for monocular depth estimation, which effectively harnesses SD's visual priors to enhance the sharpness and generalization of unsupervised prediction. Previous SD-based methods are all supervised since adapting diffusion models for dense prediction requires high-precision supervision. In contrast, self-supervised reprojection suffers from inherent challenges (e.g., occlusions, texture-less regions, illumination variance), and the predictions exhibit blurs and artifacts that severely compromise SD's latent priors. To resolve this, we construct a novel surrogate task of hybrid image reconstruction. Without any additional supervision, it preserves the detail priors of SD models by reconstructing the images themselves while preventing depth estimation from degradation. Furthermore, to address the inherent misalignment between SD's scale and shift invariant estimation and self-supervised scale-invariant depth estimation, we build the Scale-Shift GRU. It not only bridges this distribution gap but also isolates the fine-grained texture of SD output against the interference of reprojection loss. Extensive experiments demonstrate that Jasmine achieves SoTA performance on the KITTI benchmark and exhibits superior zero-shot generalization across multiple datasets.</li>
</ul>

<h3>Title: No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather</h3>
<ul>
<li><strong>Authors: </strong>Junsung Park, Hwijeong Lee, Inha Kang, Hyunjung Shim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15910">https://arxiv.org/abs/2503.15910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15910">https://arxiv.org/pdf/2503.15910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15910]] No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather(https://arxiv.org/abs/2503.15910)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Existing domain generalization methods for LiDAR semantic segmentation under adverse weather struggle to accurately predict "things" categories compared to "stuff" categories. In typical driving scenes, "things" categories can be dynamic and associated with higher collision risks, making them crucial for safe navigation and planning. Recognizing the importance of "things" categories, we identify their performance drop as a serious bottleneck in existing approaches. We observed that adverse weather induces degradation of semantic-level features and both corruption of local features, leading to a misprediction of "things" as "stuff". To mitigate these corruptions, we suggest our method, NTN - segmeNt Things for No-accident. To address semantic-level feature corruption, we bind each point feature to its superclass, preventing the misprediction of things classes into visually dissimilar categories. Additionally, to enhance robustness against local corruption caused by adverse weather, we define each LiDAR beam as a local region and propose a regularization term that aligns the clean data with its corrupted counterpart in feature space. NTN achieves state-of-the-art performance with a +2.6 mIoU gain on the SemanticKITTI-to-SemanticSTF benchmark and +7.9 mIoU on the SemanticPOSS-to-SemanticSTF benchmark. Notably, NTN achieves a +4.8 and +7.9 mIoU improvement on "things" classes, respectively, highlighting its effectiveness.</li>
</ul>

<h3>Title: Text-Driven Diffusion Model for Sign Language Production</h3>
<ul>
<li><strong>Authors: </strong>Jiayi He, Xu Wang, Ruobei Zhang, Shengeng Tang, Yaxiong Wang, Lechao Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15914">https://arxiv.org/abs/2503.15914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15914">https://arxiv.org/pdf/2503.15914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15914]] Text-Driven Diffusion Model for Sign Language Production(https://arxiv.org/abs/2503.15914)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce the hfut-lmc team's solution to the SLRTP Sign Production Challenge. The challenge aims to generate semantically aligned sign language pose sequences from text inputs. To this end, we propose a Text-driven Diffusion Model (TDM) framework. During the training phase, TDM utilizes an encoder to encode text sequences and incorporates them into the diffusion model as conditional input to generate sign pose sequences. To guarantee the high quality and accuracy of the generated pose sequences, we utilize two key loss functions. The joint loss function L_{joint} is used to precisely measure and minimize the differences between the joint positions of the generated pose sequences and those of the ground truth. Similarly, the bone orientation loss function L_{bone} is instrumental in ensuring that the orientation of the bones in the generated poses aligns with the actual, correct orientations. In the inference stage, the TDM framework takes on a different yet equally important task. It starts with noisy sequences and, under the strict constraints of the text conditions, gradually refines and generates semantically consistent sign language pose sequences. Our carefully designed framework performs well on the sign language production task, and our solution achieves a BLEU-1 score of 20.17, placing second in the challenge.</li>
</ul>

<h3>Title: ALLMod: Exploring $\underline{\mathbf{A}}$rea-Efficiency of $\underline{\mathbf{L}}$UT-based $\underline{\mathbf{L}}$arge Number $\underline{\mathbf{Mod}}$ular Reduction via Hybrid Workloads</h3>
<ul>
<li><strong>Authors: </strong>Fangxin Liu, Haomin Li, Zongwu Wang, Bo Zhang, Mingzhe Zhang, Shoumeng Yan, Li Jiang, Haibing Guan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15916">https://arxiv.org/abs/2503.15916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15916">https://arxiv.org/pdf/2503.15916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15916]] ALLMod: Exploring $\underline{\mathbf{A}}$rea-Efficiency of $\underline{\mathbf{L}}$UT-based $\underline{\mathbf{L}}$arge Number $\underline{\mathbf{Mod}}$ular Reduction via Hybrid Workloads(https://arxiv.org/abs/2503.15916)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Modular arithmetic, particularly modular reduction, is widely used in cryptographic applications such as homomorphic encryption (HE) and zero-knowledge proofs (ZKP). High-bit-width operations are crucial for enhancing security; however, they are computationally intensive due to the large number of modular operations required. The lookup-table-based (LUT-based) approach, a ``space-for-time'' technique, reduces computational load by segmenting the input number into smaller bit groups, pre-computing modular reduction results for each segment, and storing these results in LUTs. While effective, this method incurs significant hardware overhead due to extensive LUT usage. In this paper, we introduce ALLMod, a novel approach that improves the area efficiency of LUT-based large-number modular reduction by employing hybrid workloads. Inspired by the iterative method, ALLMod splits the bit groups into two distinct workloads, achieving lower area costs without compromising throughput. We first develop a template to facilitate workload splitting and ensure balanced distribution. Then, we conduct design space exploration to evaluate the optimal timing for fusing workload results, enabling us to identify the most efficient design under specific constraints. Extensive evaluations show that ALLMod achieves up to $1.65\times$ and $3\times$ improvements in area efficiency over conventional LUT-based methods for bit-widths of $128$ and $8,192$, respectively.</li>
</ul>

<h3>Title: Towards Automatic Continual Learning: A Self-Adaptive Framework for Continual Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Peiyi Lin, Fukai Zhang, Kai Niu, Hao Fu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15924">https://arxiv.org/abs/2503.15924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15924">https://arxiv.org/pdf/2503.15924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15924]] Towards Automatic Continual Learning: A Self-Adaptive Framework for Continual Instruction Tuning(https://arxiv.org/abs/2503.15924)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Continual instruction tuning enables large language models (LLMs) to learn incrementally while retaining past knowledge, whereas existing methods primarily focus on how to retain old knowledge rather than on selecting which new knowledge to learn. In domain-specific contexts, maintaining data quality and managing system constraints remain key challenges. To address these issues, we propose an automated continual instruction tuning framework that dynamically filters incoming data, which identify and reduce redundant data across successive updates. Our approach utilizes a small proxy model for efficient perplexity-based filtering, and updates the proxy to ensure that the filtering criteria remain aligned with the evolving state of the deployed model. Compared to existing static data selection methods, our framework can effectively handle incrementally acquired data and shifting distributions. Additionally, it addresses practical deployment challenges by enabling seamless model updates, supporting version rollback and incorporating automatic checkpoint evaluation. We evaluated the system in real-world medical scenarios. It reduced computational costs by 66.7% and improved model performance, and achieved autonomous updates, thus demonstrating its effectiveness for automatic continual instruction tuning.</li>
</ul>

<h3>Title: BlockDance: Reuse Structurally Similar Spatio-Temporal Features to Accelerate Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Hui Zhang, Tingwei Gao, Jie Shao, Zuxuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15927">https://arxiv.org/abs/2503.15927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15927">https://arxiv.org/pdf/2503.15927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15927]] BlockDance: Reuse Structurally Similar Spatio-Temporal Features to Accelerate Diffusion Transformers(https://arxiv.org/abs/2503.15927)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated impressive generation capabilities, particularly with recent advancements leveraging transformer architectures to improve both visual and artistic quality. However, Diffusion Transformers (DiTs) continue to encounter challenges related to low inference speed, primarily due to the iterative denoising process. To address this issue, we propose BlockDance, a training-free approach that explores feature similarities at adjacent time steps to accelerate DiTs. Unlike previous feature-reuse methods that lack tailored reuse strategies for features at different scales, BlockDance prioritizes the identification of the most structurally similar features, referred to as Structurally Similar Spatio-Temporal (STSS) features. These features are primarily located within the structure-focused blocks of the transformer during the later stages of denoising. BlockDance caches and reuses these highly similar features to mitigate redundant computation, thereby accelerating DiTs while maximizing consistency with the generated results of the original model. Furthermore, considering the diversity of generated content and the varying distributions of redundant features, we introduce BlockDance-Ada, a lightweight decision-making network tailored for instance-specific acceleration. BlockDance-Ada dynamically allocates resources and provides superior content quality. Both BlockDance and BlockDance-Ada have proven effective across various generation tasks and models, achieving accelerations between 25% and 50% while maintaining generation quality.</li>
</ul>

<h3>Title: SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer</h3>
<ul>
<li><strong>Authors: </strong>Hongda Liu, Longguang Wang, Ye Zhang, Ziru Yu, Yulan Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15934">https://arxiv.org/abs/2503.15934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15934">https://arxiv.org/pdf/2503.15934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15934]] SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer(https://arxiv.org/abs/2503.15934)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Global effective receptive field plays a crucial role for image style transfer (ST) to obtain high-quality stylized results. However, existing ST backbones (e.g., CNNs and Transformers) suffer huge computational complexity to achieve global receptive fields. Recently, the State Space Model (SSM), especially the improved variant Mamba, has shown great potential for long-range dependency modeling with linear complexity, which offers a approach to resolve the above dilemma. In this paper, we develop a Mamba-based style transfer framework, termed SaMam. Specifically, a mamba encoder is designed to efficiently extract content and style information. In addition, a style-aware mamba decoder is developed to flexibly adapt to various styles. Moreover, to address the problems of local pixel forgetting, channel redundancy and spatial discontinuity of existing SSMs, we introduce both local enhancement and zigzag scan. Qualitative and quantitative results demonstrate that our SaMam outperforms state-of-the-art methods in terms of both accuracy and efficiency.</li>
</ul>

<h3>Title: From Chaos to Order: The Atomic Reasoner Framework for Fine-grained Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jinyi Liu, Yan Zheng, Rong Cheng, Qiyu Wu, Wei Guo, Fei Ni, Hebin Liang, Yifu Yuan, Hangyu Mao, Fuzheng Zhang, Jianye Hao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15944">https://arxiv.org/abs/2503.15944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15944">https://arxiv.org/pdf/2503.15944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15944]] From Chaos to Order: The Atomic Reasoner Framework for Fine-grained Reasoning in Large Language Models(https://arxiv.org/abs/2503.15944)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have shown remarkable progress, yet their capacity for logical ``slow-thinking'' reasoning persists as a critical research frontier. Current inference scaling paradigms suffer from two fundamental constraints: fragmented thought flows compromising logical coherence, and intensively computational complexity that escalates with search space dimensions. To overcome these limitations, we present \textbf{Atomic Reasoner} (\textbf{AR}), a cognitive inference strategy that enables fine-grained reasoning through systematic atomic-level operations. AR decomposes the reasoning process into atomic cognitive units, employing a cognitive routing mechanism to dynamically construct reasoning representations and orchestrate inference pathways. This systematic methodology implements stepwise, structured cognition, which ensures logical coherence while significantly reducing cognitive load, effectively simulating the cognitive patterns observed in human deep thinking processes. Extensive experimental results demonstrate AR's superior reasoning capabilities without the computational burden of exhaustive solution searches, particularly excelling in linguistic logic puzzles. These findings substantiate AR's effectiveness in enhancing LLMs' capacity for robust, long-sequence logical reasoning and deliberation.</li>
</ul>

<h3>Title: CausalCLIPSeg: Unlocking CLIP's Potential in Referring Medical Image Segmentation with Causal Intervention</h3>
<ul>
<li><strong>Authors: </strong>Yaxiong Chen, Minghong Wei, Zixuan Zheng, Jingliang Hu, Yilei Shi, Shengwu Xiong, Xiao Xiang Zhu, Lichao Mou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15949">https://arxiv.org/abs/2503.15949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15949">https://arxiv.org/pdf/2503.15949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15949]] CausalCLIPSeg: Unlocking CLIP's Potential in Referring Medical Image Segmentation with Causal Intervention(https://arxiv.org/abs/2503.15949)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Referring medical image segmentation targets delineating lesions indicated by textual descriptions. Aligning visual and textual cues is challenging due to their distinct data properties. Inspired by large-scale pre-trained vision-language models, we propose CausalCLIPSeg, an end-to-end framework for referring medical image segmentation that leverages CLIP. Despite not being trained on medical data, we enforce CLIP's rich semantic space onto the medical domain by a tailored cross-modal decoding method to achieve text-to-pixel alignment. Furthermore, to mitigate confounding bias that may cause the model to learn spurious correlations instead of meaningful causal relationships, CausalCLIPSeg introduces a causal intervention module which self-annotates confounders and excavates causal features from inputs for segmentation judgments. We also devise an adversarial min-max game to optimize causal features while penalizing confounding ones. Extensive experiments demonstrate the state-of-the-art performance of our proposed method. Code is available at this https URL.</li>
</ul>

<h3>Title: Information maximization for a broad variety of multi-armed bandit games</h3>
<ul>
<li><strong>Authors: </strong>Alex Barbier-Chebbah (EPIMETHEE), Christian L. Vestergaard (EPIMETHEE), Jean-Baptiste Masson (EPIMETHEE)</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.stat-mech, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15962">https://arxiv.org/abs/2503.15962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15962">https://arxiv.org/pdf/2503.15962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15962]] Information maximization for a broad variety of multi-armed bandit games(https://arxiv.org/abs/2503.15962)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Information and free-energy maximization are physics principles that provide general rules for an agent to optimize actions in line with specific goals and policies. These principles are the building blocks for designing decision-making policies capable of efficient performance with only partial information. Notably, the information maximization principle has shown remarkable success in the classical bandit problem and has recently been shown to yield optimal algorithms for Gaussian and sub-Gaussian reward distributions. This article explores a broad extension of physics-based approaches to more complex and structured bandit problems. To this end, we cover three distinct types of bandit problems, where information maximization is adapted and leads to strong performance. Since the main challenge of information maximization lies in avoiding over-exploration, we highlight how information is tailored at various levels to mitigate this issue, paving the way for more efficient and robust decision-making strategies.</li>
</ul>

<h3>Title: Digital Asset Data Lakehouse. The concept based on a blockchain research center</h3>
<ul>
<li><strong>Authors: </strong>Raul Cristian Bag</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DB, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15968">https://arxiv.org/abs/2503.15968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15968">https://arxiv.org/pdf/2503.15968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15968]] Digital Asset Data Lakehouse. The concept based on a blockchain research center(https://arxiv.org/abs/2503.15968)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving landscape of digital assets and blockchain technologies, the necessity for robust, scalable, and secure data management platforms has never been more critical. This paper introduces a novel software architecture designed to meet these demands by leveraging the inherent strengths of cloud-native technologies and modular micro-service based architectures, to facilitate efficient data management, storage and access, across different stakeholders. We detail the architectural design, including its components and interactions, and discuss how it addresses common challenges in managing blockchain data and digital assets, such as scalability, data siloing, and security vulnerabilities. We demonstrate the capabilities of the platform by employing it into multiple real-life scenarios, namely providing data in near real-time to scientists in help with their research. Our results indicate that the proposed architecture not only enhances the efficiency and scalability of distributed data management but also opens new avenues for innovation in the research reproducibility area. This work lays the groundwork for future research and development in machine learning operations systems, offering a scalable and secure framework for the burgeoning digital economy.</li>
</ul>

<h3>Title: TVineSynth: A Truncated C-Vine Copula Generator of Synthetic Tabular Data to Balance Privacy and Utility</h3>
<ul>
<li><strong>Authors: </strong>Elisabeth Griesbauer, Claudia Czado, Arnoldo Frigessi, Ingrid Hobæk Haff</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15972">https://arxiv.org/abs/2503.15972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15972">https://arxiv.org/pdf/2503.15972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15972]] TVineSynth: A Truncated C-Vine Copula Generator of Synthetic Tabular Data to Balance Privacy and Utility(https://arxiv.org/abs/2503.15972)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack</a></li>
<li><strong>Abstract: </strong>We propose TVineSynth, a vine copula based synthetic tabular data generator, which is designed to balance privacy and utility, using the vine tree structure and its truncation to do the trade-off. Contrary to synthetic data generators that achieve DP by globally adding noise, TVineSynth performs a controlled approximation of the estimated data generating distribution, so that it does not suffer from poor utility of the resulting synthetic data for downstream prediction tasks. TVineSynth introduces a targeted bias into the vine copula model that, combined with the specific tree structure of the vine, causes the model to zero out privacy-leaking dependencies while relying on those that are beneficial for utility. Privacy is here measured with membership (MIA) and attribute inference attacks (AIA). Further, we theoretically justify how the construction of TVineSynth ensures AIA privacy under a natural privacy measure for continuous sensitive attributes. When compared to competitor models, with and without DP, on simulated and on real-world data, TVineSynth achieves a superior privacy-utility balance.</li>
</ul>

<h3>Title: Acc3D: Accelerating Single Image to 3D Diffusion Models via Edge Consistency Guided Score Distillation</h3>
<ul>
<li><strong>Authors: </strong>Kendong Liu, Zhiyu Zhu, Hui Liu, Junhui Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15975">https://arxiv.org/abs/2503.15975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15975">https://arxiv.org/pdf/2503.15975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15975]] Acc3D: Accelerating Single Image to 3D Diffusion Models via Edge Consistency Guided Score Distillation(https://arxiv.org/abs/2503.15975)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present Acc3D to tackle the challenge of accelerating the diffusion process to generate 3D models from single images. To derive high-quality reconstructions through few-step inferences, we emphasize the critical issue of regularizing the learning of score function in states of random noise. To this end, we propose edge consistency, i.e., consistent predictions across the high signal-to-noise ratio region, to enhance a pre-trained diffusion model, enabling a distillation-based refinement of the endpoint score function. Building on those distilled diffusion models, we propose an adversarial augmentation strategy to further enrich the generation detail and boost overall generation quality. The two modules complement each other, mutually reinforcing to elevate generative performance. Extensive experiments demonstrate that our Acc3D not only achieves over a $20\times$ increase in computational efficiency but also yields notable quality improvements, compared to the state-of-the-arts.</li>
</ul>

<h3>Title: A Survey on fMRI-based Brain Decoding for Reconstructing Multimodal Stimuli</h3>
<ul>
<li><strong>Authors: </strong>Pengyu Liu, Guohua Dong, Dan Guo, Kun Li, Fengling Li, Xun Yang, Meng Wang, Xiaomin Ying</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15978">https://arxiv.org/abs/2503.15978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15978">https://arxiv.org/pdf/2503.15978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15978]] A Survey on fMRI-based Brain Decoding for Reconstructing Multimodal Stimuli(https://arxiv.org/abs/2503.15978)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In daily life, we encounter diverse external stimuli, such as images, sounds, and videos. As research in multimodal stimuli and neuroscience advances, fMRI-based brain decoding has become a key tool for understanding brain perception and its complex cognitive processes. Decoding brain signals to reconstruct stimuli not only reveals intricate neural mechanisms but also drives progress in AI, disease treatment, and brain-computer interfaces. Recent advancements in neuroimaging and image generation models have significantly improved fMRI-based decoding. While fMRI offers high spatial resolution for precise brain activity mapping, its low temporal resolution and signal noise pose challenges. Meanwhile, techniques like GANs, VAEs, and Diffusion Models have enhanced reconstructed image quality, and multimodal pre-trained models have boosted cross-modal decoding tasks. This survey systematically reviews recent progress in fMRI-based brain decoding, focusing on stimulus reconstruction from passive brain signals. It summarizes datasets, relevant brain regions, and categorizes existing methods by model structure. Additionally, it evaluates model performance and discusses their effectiveness. Finally, it identifies key challenges and proposes future research directions, offering valuable insights for the field. For more information and resources related to this survey, visit this https URL.</li>
</ul>

<h3>Title: InhibiDistilbert: Knowledge Distillation for a ReLU and Addition-based Transformer</h3>
<ul>
<li><strong>Authors: </strong>Tony Zhang, Rickard Brännvall</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15983">https://arxiv.org/abs/2503.15983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15983">https://arxiv.org/pdf/2503.15983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15983]] InhibiDistilbert: Knowledge Distillation for a ReLU and Addition-based Transformer(https://arxiv.org/abs/2503.15983)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This work explores optimizing transformer-based language models by integrating model compression techniques with inhibitor attention, a novel alternative attention mechanism. Inhibitor attention employs Manhattan distances and ReLU activations instead of the matrix multiplications and softmax activation of the conventional scaled dot-product attention. This shift offers potential computational and energy savings while maintaining model effectiveness. We propose further adjustments to improve the inhibitor mechanism's training efficiency and evaluate its performance on the DistilBERT architecture. Our knowledge distillation experiments indicate that the modified inhibitor transformer model can achieve competitive performance on standard NLP benchmarks, including General Language Understanding Evaluation (GLUE) and sentiment analysis tasks.</li>
</ul>

<h3>Title: DIPLI: Deep Image Prior Lucky Imaging for Blind Astronomical Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Suraj Singh, Anastasia Batsheva, Oleg Y. Rogov, Ahmed Bouridane</a></li>
<li><strong>Subjects: </strong>cs.CV, astro-ph.IM, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15984">https://arxiv.org/abs/2503.15984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15984">https://arxiv.org/pdf/2503.15984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15984]] DIPLI: Deep Image Prior Lucky Imaging for Blind Astronomical Image Restoration(https://arxiv.org/abs/2503.15984)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, noise learning, transformer</a></li>
<li><strong>Abstract: </strong>Contemporary image restoration and super-resolution techniques effectively harness deep neural networks, markedly outperforming traditional methods. However, astrophotography presents unique challenges for deep learning due to limited training data. This work explores hybrid strategies, such as the Deep Image Prior (DIP) model, which facilitates blind training but is susceptible to overfitting, artifact generation, and instability when handling noisy images. We propose enhancements to the DIP model's baseline performance through several advanced techniques. First, we refine the model to process multiple frames concurrently, employing the Back Projection method and the TVNet model. Next, we adopt a Markov approach incorporating Monte Carlo estimation, Langevin dynamics, and a variational input technique to achieve unbiased estimates with minimal variance and counteract overfitting effectively. Collectively, these modifications reduce the likelihood of noise learning and mitigate loss function fluctuations during training, enhancing result stability. We validated our algorithm across multiple image sets of astronomical and celestial objects, achieving performance that not only mitigates limitations of Lucky Imaging, a classical computer vision technique that remains a standard in astronomical image reconstruction but surpasses the original DIP model, state of the art transformer- and diffusion-based models, underscoring the significance of our improvements.</li>
</ul>

<h3>Title: ECKGBench: Benchmarking Large Language Models in E-commerce Leveraging Knowledge Graph</h3>
<ul>
<li><strong>Authors: </strong>Langming Liu, Haibin Chen, Yuhao Wang, Yujin Yuan, Shilei Liu, Wenbo Su, Xiangyu Zhao, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.15990">https://arxiv.org/abs/2503.15990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.15990">https://arxiv.org/pdf/2503.15990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.15990]] ECKGBench: Benchmarking Large Language Models in E-commerce Leveraging Knowledge Graph(https://arxiv.org/abs/2503.15990)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated their capabilities across various NLP tasks. Their potential in e-commerce is also substantial, evidenced by practical implementations such as platform search, personalized recommendations, and customer service. One primary concern associated with LLMs is their factuality (e.g., hallucination), which is urgent in e-commerce due to its significant impact on user experience and revenue. Despite some methods proposed to evaluate LLMs' factuality, issues such as lack of reliability, high consumption, and lack of domain expertise leave a gap between effective assessment in e-commerce. To bridge the evaluation gap, we propose ECKGBench, a dataset specifically designed to evaluate the capacities of LLMs in e-commerce knowledge. Specifically, we adopt a standardized workflow to automatically generate questions based on a large-scale knowledge graph, guaranteeing sufficient reliability. We employ the simple question-answering paradigm, substantially improving the evaluation efficiency by the least input and output tokens. Furthermore, we inject abundant e-commerce expertise in each evaluation stage, including human annotation, prompt design, negative sampling, and verification. Besides, we explore the LLMs' knowledge boundaries in e-commerce from a novel perspective. Through comprehensive evaluations of several advanced LLMs on ECKGBench, we provide meticulous analysis and insights into leveraging LLMs for e-commerce.</li>
</ul>

<h3>Title: SenseExpo: Efficient Autonomous Exploration with Prediction Information from Lightweight Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Haojia Gao, Haohua Que, Hoiian Au, Weihao Shan, Mingkai Liu, Yusen Qin, Lei Mu, Rong Zhao, Xinghua Yang, Qi Wei, Fei Qiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16000">https://arxiv.org/abs/2503.16000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16000">https://arxiv.org/pdf/2503.16000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16000]] SenseExpo: Efficient Autonomous Exploration with Prediction Information from Lightweight Neural Networks(https://arxiv.org/abs/2503.16000)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>This paper proposes SenseExpo, an efficient autonomous exploration framework based on a lightweight prediction network, which addresses the limitations of traditional methods in computational overhead and environmental generalization. By integrating Generative Adversarial Networks (GANs), Transformer, and Fast Fourier Convolution (FFC), we designed a lightweight prediction model with merely 709k parameters. Our smallest model achieves better performance on the KTH dataset than U-net (24.5M) and LaMa (51M), delivering PSNR 9.026 and SSIM 0.718, particularly representing a 38.7% PSNR improvement over the 51M-parameter LaMa model. Cross-domain testing demonstrates its strong generalization capability, with an FID score of 161.55 on the HouseExpo dataset, significantly outperforming comparable methods. Regarding exploration efficiency, on the KTH dataset,SenseExpo demonstrates approximately a 67.9% time reduction in exploration time compared to MapEx. On the MRPB 1.0 dataset, SenseExpo achieves 77.1% time reduction roughly compared to MapEx. Deployed as a plug-and-play ROS node, the framework seamlessly integrates with existing navigation systems, providing an efficient solution for resource-constrained devices.</li>
</ul>

<h3>Title: Corrective In-Context Learning: Evaluating Self-Correction in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mario Sanz-Guerrero, Katharina von der Wense</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16022">https://arxiv.org/abs/2503.16022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16022">https://arxiv.org/pdf/2503.16022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16022]] Corrective In-Context Learning: Evaluating Self-Correction in Large Language Models(https://arxiv.org/abs/2503.16022)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) has transformed the use of large language models (LLMs) for NLP tasks, enabling few-shot learning by conditioning on labeled examples without finetuning. Despite its effectiveness, ICL is prone to errors, especially for challenging examples. With the goal of improving the performance of ICL, we propose corrective in-context learning (CICL), an approach that incorporates a model's incorrect predictions alongside ground truth corrections into the prompt, aiming to enhance classification accuracy through self-correction. However, contrary to our hypothesis, extensive experiments on text classification tasks demonstrate that CICL consistently underperforms standard ICL, with performance degrading as the proportion of corrections in the prompt increases. Our findings indicate that CICL introduces confusion by disrupting the model's task understanding, rather than refining its predictions. Additionally, we observe that presenting harder examples in standard ICL does not improve performance, suggesting that example difficulty alone may not be a reliable criterion for effective selection. By presenting these negative results, we provide important insights into the limitations of self-corrective mechanisms in LLMs and offer directions for future research.</li>
</ul>

<h3>Title: BadToken: Token-level Backdoor Attacks to Multi-modal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zenghui Yuan, Jiawen Shi, Pan Zhou, Neil Zhenqiang Gong, Lichao Sun</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16023">https://arxiv.org/abs/2503.16023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16023">https://arxiv.org/pdf/2503.16023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16023]] BadToken: Token-level Backdoor Attacks to Multi-modal Large Language Models(https://arxiv.org/abs/2503.16023)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Multi-modal large language models (MLLMs) extend large language models (LLMs) to process multi-modal information, enabling them to generate responses to image-text inputs. MLLMs have been incorporated into diverse multi-modal applications, such as autonomous driving and medical diagnosis, via plug-and-play without fine-tuning. This deployment paradigm increases the vulnerability of MLLMs to backdoor attacks. However, existing backdoor attacks against MLLMs achieve limited effectiveness and stealthiness. In this work, we propose BadToken, the first token-level backdoor attack to MLLMs. BadToken introduces two novel backdoor behaviors: Token-substitution and Token-addition, which enable flexible and stealthy attacks by making token-level modifications to the original output for backdoored inputs. We formulate a general optimization problem that considers the two backdoor behaviors to maximize the attack effectiveness. We evaluate BadToken on two open-source MLLMs and various tasks. Our results show that our attack maintains the model's utility while achieving high attack success rates and stealthiness. We also show the real-world threats of BadToken in two scenarios, i.e., autonomous driving and medical diagnosis. Furthermore, we consider defenses including fine-tuning and input purification. Our results highlight the threat of our attack.</li>
</ul>

<h3>Title: The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement</h3>
<ul>
<li><strong>Authors: </strong>Ruihan Yang, Fanghua Ye, Jian Li, Siyu Yuan, Yikai Zhang, Zhaopeng Tu, Xiaolong Li, Deqing Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16024">https://arxiv.org/abs/2503.16024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16024">https://arxiv.org/pdf/2503.16024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16024]] The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement(https://arxiv.org/abs/2503.16024)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently transformed from text-based assistants to autonomous agents capable of planning, reasoning, and iteratively improving their actions. While numerical reward signals and verifiers can effectively rank candidate actions, they often provide limited contextual guidance. In contrast, natural language feedback better aligns with the generative capabilities of LLMs, providing richer and more actionable suggestions. However, parsing and implementing this feedback effectively can be challenging for LLM-based agents. In this work, we introduce Critique-Guided Improvement (CGI), a novel two-player framework, comprising an actor model that explores an environment and a critic model that generates detailed nature language feedback. By training the critic to produce fine-grained assessments and actionable revisions, and the actor to utilize these critiques, our approach promotes more robust exploration of alternative strategies while avoiding local optima. Experiments in three interactive environments show that CGI outperforms existing baselines by a substantial margin. Notably, even a small critic model surpasses GPT-4 in feedback quality. The resulting actor achieves state-of-the-art performance, demonstrating the power of explicit iterative guidance to enhance decision-making in LLM-based agents.</li>
</ul>

<h3>Title: Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhihang Liu, Chen-Wei Xie, Pandeng Li, Liming Zhao, Longxiang Tang, Yun Zheng, Chuanbin Liu, Hongtao Xie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16036">https://arxiv.org/abs/2503.16036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16036">https://arxiv.org/pdf/2503.16036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16036]] Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models(https://arxiv.org/abs/2503.16036)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent Multi-modal Large Language Models (MLLMs) have been challenged by the computational overhead resulting from massive video frames, often alleviated through compression strategies. However, the visual content is not equally contributed to user instructions, existing strategies (\eg, average pool) inevitably lead to the loss of potentially useful information. To tackle this, we propose the Hybrid-level Instruction Injection Strategy for Conditional Token Compression in MLLMs (HICom), utilizing the instruction as a condition to guide the compression from both local and global levels. This encourages the compression to retain the maximum amount of user-focused information while reducing visual tokens to minimize computational burden. Specifically, the instruction condition is injected into the grouped visual tokens at the local level and the learnable tokens at the global level, and we conduct the attention mechanism to complete the conditional compression. From the hybrid-level compression, the instruction-relevant visual parts are highlighted while the temporal-spatial structure is also preserved for easier understanding of LLMs. To further unleash the potential of HICom, we introduce a new conditional pre-training stage with our proposed dataset HICom-248K. Experiments show that our HICom can obtain distinguished video understanding ability with fewer tokens, increasing the performance by 2.43\% average on three multiple-choice QA benchmarks and saving 78.8\% tokens compared with the SOTA method. The code is available at this https URL.</li>
</ul>

<h3>Title: Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1, DeepSeek-R1, and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Yaoyao Yu, Leilei Gan, Yinghao Hu, Bin Wei, Kun Kuang, Fei Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16040">https://arxiv.org/abs/2503.16040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16040">https://arxiv.org/pdf/2503.16040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16040]] Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1, DeepSeek-R1, and Beyond(https://arxiv.org/abs/2503.16040)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, Test-Time Scaling Large Language Models (LLMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated exceptional capabilities across various domains and tasks, particularly in reasoning. While these models have shown impressive performance on general language tasks, their effectiveness in specialized fields like legal remains unclear. To address this, we present a preliminary evaluation of LLMs in various legal scenarios, covering both Chinese and English legal tasks. Our analysis includes 9 LLMs and 17 legal tasks, with a focus on newly published and more complex challenges such as multi-defendant legal judgments and legal argument reasoning. Our findings indicate that, despite DeepSeek-R1 and OpenAI o1 being among the most powerful models, their legal reasoning capabilities are still lacking. Specifically, these models score below 80\% on seven Chinese legal reasoning tasks and below 80\% on two English legal reasoning tasks. This suggests that, even among the most advanced reasoning models, legal reasoning abilities remain underdeveloped.</li>
</ul>

<h3>Title: Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in Network Traffic</h3>
<ul>
<li><strong>Authors: </strong>Bisola Faith Kayode, Akinyemi Sadeeq Akintola, Oluwole Fagbohun, Egonna Anaesiuba-Bristol, Onyekachukwu Ojumah, Oluwagbade Odimayo, Toyese Oloyede, Aniema Inyang, Teslim Kazeem, Habeeb Alli, Udodirim Ibem Offia, Prisca Chinazor Amajuoyi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16047">https://arxiv.org/abs/2503.16047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16047">https://arxiv.org/pdf/2503.16047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16047]] Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in Network Traffic(https://arxiv.org/abs/2503.16047)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Denial-of-Service (DoS) attacks remain a critical threat to network security, disrupting services and causing significant economic losses. Traditional detection methods, including statistical and rule-based models, struggle to adapt to evolving attack patterns. To address this challenge, we propose a novel Temporal-Spatial Attention Network (TSAN) architecture for detecting Denial of Service (DoS) attacks in network traffic. By leveraging both temporal and spatial features of network traffic, our approach captures complex traffic patterns and anomalies that traditional methods might miss. The TSAN model incorporates transformer-based temporal encoding, convolutional spatial encoding, and a cross-attention mechanism to fuse these complementary feature spaces. Additionally, we employ multi-task learning with auxiliary tasks to enhance the model's robustness. Experimental results on the NSL-KDD dataset demonstrate that TSAN outperforms state-of-the-art models, achieving superior accuracy, precision, recall, and F1-score while maintaining computational efficiency for real-time deployment. The proposed architecture offers an optimal balance between detection accuracy and computational overhead, making it highly suitable for real-world network security applications.</li>
</ul>

<h3>Title: Meta-Learning Neural Mechanisms rather than Bayesian Priors</h3>
<ul>
<li><strong>Authors: </strong>Michael Goodale, Salvador Mascarenhas, Yair Lakretz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16048">https://arxiv.org/abs/2503.16048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16048">https://arxiv.org/pdf/2503.16048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16048]] Meta-Learning Neural Mechanisms rather than Bayesian Priors(https://arxiv.org/abs/2503.16048)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Children acquire language despite being exposed to several orders of magnitude less data than large language models require. Meta-learning has been proposed as a way to integrate human-like learning biases into neural-network architectures, combining both the structured generalizations of symbolic models with the scalability of neural-network models. But what does meta-learning exactly imbue the model with? We investigate the meta-learning of formal languages and find that, contrary to previous claims, meta-trained models are not learning simplicity-based priors when meta-trained on datasets organised around simplicity. Rather, we find evidence that meta-training imprints neural mechanisms (such as counters) into the model, which function like cognitive primitives for the network on downstream tasks. Most surprisingly, we find that meta-training on a single formal language can provide as much improvement to a model as meta-training on 5000 different formal languages, provided that the formal language incentivizes the learning of useful neural mechanisms. Taken together, our findings provide practical implications for efficient meta-learning paradigms and new theoretical insights into linking symbolic theories and neural mechanisms.</li>
</ul>

<h3>Title: Closer to Ground Truth: Realistic Shape and Appearance Labeled Data Generation for Unsupervised Underwater Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Andrei Jelea, Ahmed Nabil Belbachir, Marius Leordeanu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16051">https://arxiv.org/abs/2503.16051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16051">https://arxiv.org/pdf/2503.16051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16051]] Closer to Ground Truth: Realistic Shape and Appearance Labeled Data Generation for Unsupervised Underwater Image Segmentation(https://arxiv.org/abs/2503.16051)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Solving fish segmentation in underwater videos, a real-world problem of great practical value in marine and aquaculture industry, is a challenging task due to the difficulty of the filming environment, poor visibility and limited existing annotated underwater fish data. In order to overcome these obstacles, we introduce a novel two stage unsupervised segmentation approach that requires no human annotations and combines artificially created and real images. Our method generates challenging synthetic training data, by placing virtual fish in real-world underwater habitats, after performing fish transformations such as Thin Plate Spline shape warping and color Histogram Matching, which realistically integrate synthetic fish into the backgrounds, making the generated images increasingly closer to the real world data with every stage of our approach. While we validate our unsupervised method on the popular DeepFish dataset, obtaining a performance close to a fully-supervised SoTA model, we further show its effectiveness on the specific case of salmon segmentation in underwater videos, for which we introduce DeepSalmon, the largest dataset of its kind in the literature (30 GB). Moreover, on both datasets we prove the capability of our approach to boost the performance of the fully-supervised SoTA model.</li>
</ul>

<h3>Title: Expert Race: A Flexible Routing Strategy for Scaling Diffusion Transformer with Mixture of Experts</h3>
<ul>
<li><strong>Authors: </strong>Yike Yuan, Ziyu Wang, Zihao Huang, Defa Zhu, Xun Zhou, Jingyi Yu, Qiyang Min</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16057">https://arxiv.org/abs/2503.16057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16057">https://arxiv.org/pdf/2503.16057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16057]] Expert Race: A Flexible Routing Strategy for Scaling Diffusion Transformer with Mixture of Experts(https://arxiv.org/abs/2503.16057)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as mainstream framework in visual generation. Building upon this success, the integration of Mixture of Experts (MoE) methods has shown promise in enhancing model scalability and performance. In this paper, we introduce Race-DiT, a novel MoE model for diffusion transformers with a flexible routing strategy, Expert Race. By allowing tokens and experts to compete together and select the top candidates, the model learns to dynamically assign experts to critical tokens. Additionally, we propose per-layer regularization to address challenges in shallow layer learning, and router similarity loss to prevent mode collapse, ensuring better expert utilization. Extensive experiments on ImageNet validate the effectiveness of our approach, showcasing significant performance gains while promising scaling properties.</li>
</ul>

<h3>Title: PromptHash: Affinity-Prompted Collaborative Cross-Modal Learning for Adaptive Hashing Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Qiang Zou, Shuli Cheng, Jiayi Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16064">https://arxiv.org/abs/2503.16064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16064">https://arxiv.org/pdf/2503.16064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16064]] PromptHash: Affinity-Prompted Collaborative Cross-Modal Learning for Adaptive Hashing Retrieval(https://arxiv.org/abs/2503.16064)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Cross-modal hashing is a promising approach for efficient data retrieval and storage optimization. However, contemporary methods exhibit significant limitations in semantic preservation, contextual integrity, and information redundancy, which constrains retrieval efficacy. We present PromptHash, an innovative framework leveraging affinity prompt-aware collaborative learning for adaptive cross-modal hashing. We propose an end-to-end framework for affinity-prompted collaborative hashing, with the following fundamental technical contributions: (i) a text affinity prompt learning mechanism that preserves contextual information while maintaining parameter efficiency, (ii) an adaptive gated selection fusion architecture that synthesizes State Space Model with Transformer network for precise cross-modal feature integration, and (iii) a prompt affinity alignment strategy that bridges modal heterogeneity through hierarchical contrastive learning. To the best of our knowledge, this study presents the first investigation into affinity prompt awareness within collaborative cross-modal adaptive hash learning, establishing a paradigm for enhanced semantic consistency across modalities. Through comprehensive evaluation on three benchmark multi-label datasets, PromptHash demonstrates substantial performance improvements over existing approaches. Notably, on the NUS-WIDE dataset, our method achieves significant gains of 18.22% and 18.65% in image-to-text and text-to-image retrieval tasks, respectively. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: Shining Yourself: High-Fidelity Ornaments Virtual Try-on with Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yingmao Miao, Zhanpeng Huang, Rui Han, Zibin Wang, Chenhao Lin, Chao Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16065">https://arxiv.org/abs/2503.16065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16065">https://arxiv.org/pdf/2503.16065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16065]] Shining Yourself: High-Fidelity Ornaments Virtual Try-on with Diffusion Model(https://arxiv.org/abs/2503.16065)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While virtual try-on for clothes and shoes with diffusion models has gained attraction, virtual try-on for ornaments, such as bracelets, rings, earrings, and necklaces, remains largely unexplored. Due to the intricate tiny patterns and repeated geometric sub-structures in most ornaments, it is much more difficult to guarantee identity and appearance consistency under large pose and scale variances between ornaments and models. This paper proposes the task of virtual try-on for ornaments and presents a method to improve the geometric and appearance preservation of ornament virtual try-ons. Specifically, we estimate an accurate wearing mask to improve the alignments between ornaments and models in an iterative scheme alongside the denoising process. To preserve structure details, we further regularize attention layers to map the reference ornament mask to the wearing mask in an implicit way. Experimental results demonstrate that our method successfully wears ornaments from reference images onto target models, handling substantial differences in scale and pose while preserving identity and achieving realistic visual effects.</li>
</ul>

<h3>Title: PoseTraj: Pose-Aware Trajectory Control in Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Longbin Ji, Lei Zhong, Pengfei Wei, Changjian Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16068">https://arxiv.org/abs/2503.16068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16068">https://arxiv.org/pdf/2503.16068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16068]] PoseTraj: Pose-Aware Trajectory Control in Video Diffusion(https://arxiv.org/abs/2503.16068)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in trajectory-guided video generation have achieved notable progress. However, existing models still face challenges in generating object motions with potentially changing 6D poses under wide-range rotations, due to limited 3D understanding. To address this problem, we introduce PoseTraj, a pose-aware video dragging model for generating 3D-aligned motion from 2D trajectories. Our method adopts a novel two-stage pose-aware pretraining framework, improving 3D understanding across diverse trajectories. Specifically, we propose a large-scale synthetic dataset PoseTraj-10K, containing 10k videos of objects following rotational trajectories, and enhance the model perception of object pose changes by incorporating 3D bounding boxes as intermediate supervision signals. Following this, we fine-tune the trajectory-controlling module on real-world videos, applying an additional camera-disentanglement module to further refine motion accuracy. Experiments on various benchmark datasets demonstrate that our method not only excels in 3D pose-aligned dragging for rotational trajectories but also outperforms existing baselines in trajectory accuracy and video quality.</li>
</ul>

<h3>Title: Disentangled and Interpretable Multimodal Attention Fusion for Cancer Survival Prediction</h3>
<ul>
<li><strong>Authors: </strong>Aniek Eijpe, Soufyan Lakbir, Melis Erdal Cesur, Sara P. Oliveira, Sanne Abeln, Wilson Silva</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16069">https://arxiv.org/abs/2503.16069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16069">https://arxiv.org/pdf/2503.16069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16069]] Disentangled and Interpretable Multimodal Attention Fusion for Cancer Survival Prediction(https://arxiv.org/abs/2503.16069)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>To improve the prediction of cancer survival using whole-slide images and transcriptomics data, it is crucial to capture both modality-shared and modality-specific information. However, multimodal frameworks often entangle these representations, limiting interpretability and potentially suppressing discriminative features. To address this, we propose Disentangled and Interpretable Multimodal Attention Fusion (DIMAF), a multimodal framework that separates the intra- and inter-modal interactions within an attention-based fusion mechanism to learn distinct modality-specific and modality-shared representations. We introduce a loss based on Distance Correlation to promote disentanglement between these representations and integrate Shapley additive explanations to assess their relative contributions to survival prediction. We evaluate DIMAF on four public cancer survival datasets, achieving a relative average improvement of 1.85% in performance and 23.7% in disentanglement compared to current state-of-the-art multimodal models. Beyond improved performance, our interpretable framework enables a deeper exploration of the underlying interactions between and within modalities in cancer biology.</li>
</ul>

<h3>Title: Tuning LLMs by RAG Principles: Towards LLM-native Memory</h3>
<ul>
<li><strong>Authors: </strong>Jiale Wei, Shuchi Wu, Ruochen Liu, Xiang Ying, Jingbo Shang, Fangbo Tao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16071">https://arxiv.org/abs/2503.16071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16071">https://arxiv.org/pdf/2503.16071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16071]] Tuning LLMs by RAG Principles: Towards LLM-native Memory(https://arxiv.org/abs/2503.16071)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Memory, additional information beyond the training of large language models (LLMs), is crucial to various real-world applications, such as personal assistant. The two mainstream solutions to incorporate memory into the generation process are long-context LLMs and retrieval-augmented generation (RAG). In this paper, we first systematically compare these two types of solutions on three renovated/new datasets and show that (1) long-context solutions, although more expensive, shall be easier to capture the big picture and better answer queries which require considering the memory as a whole; and (2) when the queries concern specific information, RAG solutions shall be more competitive especially when the keywords can be explicitly matched. Therefore, we propose a novel method RAG-Tuned-LLM which fine-tunes a relative small (e.g., 7B) LLM using the data generated following the RAG principles, so it can combine the advantages of both solutions. Extensive experiments on three datasets demonstrate that RAG-Tuned-LLM can beat long-context LLMs and RAG methods across a wide range of query types.</li>
</ul>

<h3>Title: Redefining Toxicity: An Objective and Context-Aware Approach for Stress-Level-Based Detection</h3>
<ul>
<li><strong>Authors: </strong>Sergey Berezin, Reza Farahbakhsh, Noel Crespi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16072">https://arxiv.org/abs/2503.16072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16072">https://arxiv.org/pdf/2503.16072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16072]] Redefining Toxicity: An Objective and Context-Aware Approach for Stress-Level-Based Detection(https://arxiv.org/abs/2503.16072)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The fundamental problem of toxicity detection lies in the fact that the term "toxicity" is ill-defined. Such uncertainty causes researchers to rely on subjective and vague data during model training, which leads to non-robust and inaccurate results, following the 'garbage in - garbage out' paradigm. This study introduces a novel, objective, and context-aware framework for toxicity detection, leveraging stress levels as a key determinant of toxicity. We propose new definition, metric and training approach as a parts of our framework and demonstrate it's effectiveness using a dataset we collected.</li>
</ul>

<h3>Title: Fast Homomorphic Linear Algebra with BLAS</h3>
<ul>
<li><strong>Authors: </strong>Youngjin Bae, Jung Hee Cheon, Guillaume Hanrot, Jai Hyun Park, Damien Stehlé</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16080">https://arxiv.org/abs/2503.16080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16080">https://arxiv.org/pdf/2503.16080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16080]] Fast Homomorphic Linear Algebra with BLAS(https://arxiv.org/abs/2503.16080)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Homomorphic encryption is a cryptographic paradigm allowing to compute on encrypted data, opening a wide range of applications in privacy-preserving data manipulation, notably in AI. Many of those applications require significant linear algebra computations (matrix x vector products, and matrix x matrix products). This central role of linear algebra computations goes far beyond homomorphic algebra and applies to most areas of scientific computing. This high versatility led, over time, to the development of a set of highly optimized routines, specified in 1979 under the name BLAS (basic linear algebra subroutines). Motivated both by the applicative importance of homomorphic linear algebra and the access to highly efficient implementations of cleartext linear algebra able to draw the most out of available hardware, we explore the connections between CKKS-based homomorphic linear algebra and floating-point plaintext linear algebra. The CKKS homomorphic encryption system is the most natural choice in this setting, as it natively handles real numbers and offers a large SIMD parallelism. We provide reductions for matrix-vector products, vector-vector products for moderate-sized to large matrices to their plaintext equivalents. Combined with BLAS, we demonstrate that the efficiency loss between CKKS-based encrypted square matrix multiplication and double-precision floating-point square matrix multiplication is a mere 4-12 factor, depending on the precise situation.</li>
</ul>

<h3>Title: Hyperspectral Imaging for Identifying Foreign Objects on Pork Belly</h3>
<ul>
<li><strong>Authors: </strong>Gabriela Ghimpeteanu, Hayat Rajani, Josep Quintana, Rafael Garcia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16086">https://arxiv.org/abs/2503.16086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16086">https://arxiv.org/pdf/2503.16086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16086]] Hyperspectral Imaging for Identifying Foreign Objects on Pork Belly(https://arxiv.org/abs/2503.16086)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Ensuring food safety and quality is critical in the food processing industry, where the detection of contaminants remains a persistent challenge. This study presents an automated solution for detecting foreign objects on pork belly meat using hyperspectral imaging (HSI). A hyperspectral camera was used to capture data across various bands in the near-infrared (NIR) spectrum (900-1700 nm), enabling accurate identification of contaminants that are often undetectable through traditional visual inspection methods. The proposed solution combines pre-processing techniques with a segmentation approach based on a lightweight Vision Transformer (ViT) to distinguish contaminants from meat, fat, and conveyor belt materials. The adopted strategy demonstrates high detection accuracy and training efficiency, while also addressing key industrial challenges such as inherent noise, temperature variations, and spectral similarity between contaminants and pork belly. Experimental results validate the effectiveness of hyperspectral imaging in enhancing food safety, highlighting its potential for broad real-time applications in automated quality control processes.</li>
</ul>

<h3>Title: Cultural Alignment in Large Language Models Using Soft Prompt Tuning</h3>
<ul>
<li><strong>Authors: </strong>Reem I. Masoud, Martin Ferianc, Philip Treleaven, Miguel Rodrigues</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16094">https://arxiv.org/abs/2503.16094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16094">https://arxiv.org/pdf/2503.16094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16094]] Cultural Alignment in Large Language Models Using Soft Prompt Tuning(https://arxiv.org/abs/2503.16094)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) alignment conventionally relies on supervised fine-tuning or reinforcement learning based alignment frameworks. These methods typically require labeled or preference datasets and involve updating model weights to align the LLM with the training objective or reward model. Meanwhile, in social sciences such as cross-cultural studies, factor analysis is widely used to uncover underlying dimensions or latent variables that explain observed patterns in survey data. The non-differentiable nature of these measurements deriving from survey data renders the former alignment methods infeasible for alignment with cultural dimensions. To overcome this, we propose a parameter efficient strategy that combines soft prompt tuning, which freezes the model parameters while modifying the input prompt embeddings, with Differential Evolution (DE), a black-box optimization method for cases where a differentiable objective is unattainable. This strategy ensures alignment consistency without the need for preference data or model parameter updates, significantly enhancing efficiency and mitigating overfitting. Our method demonstrates significant improvements in LLama-3-8B-Instruct's cultural dimensions across multiple regions, outperforming both the Naive LLM and the In-context Learning (ICL) baseline, and effectively bridges computational models with human cultural nuances.</li>
</ul>

<h3>Title: MarkushGrapher: Joint Visual and Textual Recognition of Markush Structures</h3>
<ul>
<li><strong>Authors: </strong>Lucas Morin, Valéry Weber, Ahmed Nassar, Gerhard Ingmar Meijer, Luc Van Gool, Yawei Li, Peter Staar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16096">https://arxiv.org/abs/2503.16096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16096">https://arxiv.org/pdf/2503.16096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16096]] MarkushGrapher: Joint Visual and Textual Recognition of Markush Structures(https://arxiv.org/abs/2503.16096)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The automated analysis of chemical literature holds promise to accelerate discovery in fields such as material science and drug development. In particular, search capabilities for chemical structures and Markush structures (chemical structure templates) within patent documents are valuable, e.g., for prior-art search. Advancements have been made in the automatic extraction of chemical structures from text and images, yet the Markush structures remain largely unexplored due to their complex multi-modal nature. In this work, we present MarkushGrapher, a multi-modal approach for recognizing Markush structures in documents. Our method jointly encodes text, image, and layout information through a Vision-Text-Layout encoder and an Optical Chemical Structure Recognition vision encoder. These representations are merged and used to auto-regressively generate a sequential graph representation of the Markush structure along with a table defining its variable groups. To overcome the lack of real-world training data, we propose a synthetic data generation pipeline that produces a wide range of realistic Markush structures. Additionally, we present M2S, the first annotated benchmark of real-world Markush structures, to advance research on this challenging task. Extensive experiments demonstrate that our approach outperforms state-of-the-art chemistry-specific and general-purpose vision-language models in most evaluation settings. Code, models, and datasets will be available.</li>
</ul>

<h3>Title: Improving Discriminator Guidance in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Verine, Mehdi Inane, Florian Le Bronnec, Benjamin Negrevergne, Yann Chevaleyre</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16117">https://arxiv.org/abs/2503.16117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16117">https://arxiv.org/pdf/2503.16117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16117]] Improving Discriminator Guidance in Diffusion Models(https://arxiv.org/abs/2503.16117)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discriminator Guidance has become a popular method for efficiently refining pre-trained Score-Matching Diffusion models. However, in this paper, we demonstrate that the standard implementation of this technique does not necessarily lead to a distribution closer to the real data distribution. Specifically, we show that training the discriminator using Cross-Entropy loss, as commonly done, can in fact increase the Kullback-Leibler divergence between the model and target distributions, particularly when the discriminator overfits. To address this, we propose a theoretically sound training objective for discriminator guidance that properly minimizes the KL divergence. We analyze its properties and demonstrate empirically across multiple datasets that our proposed method consistently improves over the conventional method by producing samples of higher quality.</li>
</ul>

<h3>Title: MKG-Rank: Enhancing Large Language Models with Knowledge Graph for Multilingual Medical Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Feiyang Li, Yingjian Chen, Haoran Liu, Rui Yang, Han Yuan, Yuang Jiang, Tianxiao Li, Edison Marrese Taylor, Hossein Rouhizadeh, Yusuke Iwasawa, Douglas Teodoro, Yutaka Matsuo, Irene Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16131">https://arxiv.org/abs/2503.16131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16131">https://arxiv.org/pdf/2503.16131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16131]] MKG-Rank: Enhancing Large Language Models with Knowledge Graph for Multilingual Medical Question Answering(https://arxiv.org/abs/2503.16131)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable progress in medical question answering (QA), yet their effectiveness remains predominantly limited to English due to imbalanced multilingual training data and scarce medical resources for low-resource languages. To address this critical language gap in medical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking (MKG-Rank), a knowledge graph-enhanced framework that enables English-centric LLMs to perform multilingual medical QA. Through a word-level translation mechanism, our framework efficiently integrates comprehensive English-centric medical knowledge graphs into LLM reasoning at a low cost, mitigating cross-lingual semantic distortion and achieving precise medical QA across language barriers. To enhance efficiency, we introduce caching and multi-angle ranking strategies to optimize the retrieval process, significantly reducing response times and prioritizing relevant medical knowledge. Extensive evaluations on multilingual medical QA benchmarks across Chinese, Japanese, Korean, and Swahili demonstrate that MKG-Rank consistently outperforms zero-shot LLMs, achieving maximum 33.89% increase in accuracy, while maintaining an average retrieval time of only 0.0009 seconds.</li>
</ul>

<h3>Title: Binarized Mamba-Transformer for Lightweight Quad Bayer HybridEVS Demosaicing</h3>
<ul>
<li><strong>Authors: </strong>Shiyang Zhou, Haijin Zeng, Yunfan Lu, Tong Shao, Ke Tang, Yongyong Chen, Jie Liu, Jingyong Su</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16134">https://arxiv.org/abs/2503.16134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16134">https://arxiv.org/pdf/2503.16134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16134]] Binarized Mamba-Transformer for Lightweight Quad Bayer HybridEVS Demosaicing(https://arxiv.org/abs/2503.16134)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Quad Bayer demosaicing is the central challenge for enabling the widespread application of Hybrid Event-based Vision Sensors (HybridEVS). Although existing learning-based methods that leverage long-range dependency modeling have achieved promising results, their complexity severely limits deployment on mobile devices for real-world applications. To address these limitations, we propose a lightweight Mamba-based binary neural network designed for efficient and high-performing demosaicing of HybridEVS RAW images. First, to effectively capture both global and local dependencies, we introduce a hybrid Binarized Mamba-Transformer architecture that combines the strengths of the Mamba and Swin Transformer architectures. Next, to significantly reduce computational complexity, we propose a binarized Mamba (Bi-Mamba), which binarizes all projections while retaining the core Selective Scan in full precision. Bi-Mamba also incorporates additional global visual information to enhance global context and mitigate precision loss. We conduct quantitative and qualitative experiments to demonstrate the effectiveness of BMTNet in both performance and computational efficiency, providing a lightweight demosaicing solution suited for real-world edge devices. Our codes and models are available at this https URL.</li>
</ul>

<h3>Title: FreeFlux: Understanding and Exploiting Layer-Specific Roles in RoPE-Based MMDiT for Versatile Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Wei, Yifan Zhou, Dongdong Chen, Xingang Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16153">https://arxiv.org/abs/2503.16153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16153">https://arxiv.org/pdf/2503.16153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16153]] FreeFlux: Understanding and Exploiting Layer-Specific Roles in RoPE-Based MMDiT for Versatile Image Editing(https://arxiv.org/abs/2503.16153)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>The integration of Rotary Position Embedding (RoPE) in Multimodal Diffusion Transformer (MMDiT) has significantly enhanced text-to-image generation quality. However, the fundamental reliance of self-attention layers on positional embedding versus query-key similarity during generation remains an intriguing question. We present the first mechanistic analysis of RoPE-based MMDiT models (e.g., FLUX), introducing an automated probing strategy that disentangles positional information versus content dependencies by strategically manipulating RoPE during generation. Our analysis reveals distinct dependency patterns that do not straightforwardly correlate with depth, offering new insights into the layer-specific roles in RoPE-based MMDiT. Based on these findings, we propose a training-free, task-specific image editing framework that categorizes editing tasks into three types: position-dependent editing (e.g., object addition), content similarity-dependent editing (e.g., non-rigid editing), and region-preserved editing (e.g., background replacement). For each type, we design tailored key-value injection strategies based on the characteristics of the editing task. Extensive qualitative and quantitative evaluations demonstrate that our method outperforms state-of-the-art approaches, particularly in preserving original semantic content and achieving seamless modifications.</li>
</ul>

<h3>Title: Automatically Generating Chinese Homophone Words to Probe Machine Translation Estimation Systems</h3>
<ul>
<li><strong>Authors: </strong>Shenbin Qian, Constantin Orăsan, Diptesh Kanojia, Félix do Carmo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16158">https://arxiv.org/abs/2503.16158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16158">https://arxiv.org/pdf/2503.16158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16158]] Automatically Generating Chinese Homophone Words to Probe Machine Translation Estimation Systems(https://arxiv.org/abs/2503.16158)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Evaluating machine translation (MT) of user-generated content (UGC) involves unique challenges such as checking whether the nuance of emotions from the source are preserved in the target text. Recent studies have proposed emotion-related datasets, frameworks and models to automatically evaluate MT quality of Chinese UGC, without relying on reference translations. However, whether these models are robust to the challenge of preserving emotional nuances has been left largely unexplored. To address this gap, we introduce a novel method inspired by information theory which generates challenging Chinese homophone words related to emotions, by leveraging the concept of self-information. Our approach generates homophones that were observed to cause translation errors in emotion preservation, and exposes vulnerabilities in MT systems and their evaluation methods when tackling emotional UGC. We evaluate the efficacy of our method using human evaluation for the quality of these generated homophones, and compare it with an existing one, showing that our method achieves higher correlation with human judgments. The generated Chinese homophones, along with their manual translations, are utilized to generate perturbations and to probe the robustness of existing quality evaluation models, including models trained using multi-task learning, fine-tuned variants of multilingual language models, as well as large language models (LLMs). Our results indicate that LLMs with larger size exhibit higher stability and robustness to such perturbations. We release our data and code for reproducibility and further research.</li>
</ul>

<h3>Title: Towards Lighter and Robust Evaluation for Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Alex-Razvan Ispas, Charles-Elie Simon, Fabien Caspani, Vincent Guigue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16161">https://arxiv.org/abs/2503.16161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16161">https://arxiv.org/pdf/2503.16161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16161]] Towards Lighter and Robust Evaluation for Retrieval Augmented Generation(https://arxiv.org/abs/2503.16161)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models are prompting us to view more NLP tasks from a generative perspective. At the same time, they offer a new way of accessing information, mainly through the RAG framework. While there have been notable improvements for the autoregressive models, overcoming hallucination in the generated answers remains a continuous problem. A standard solution is to use commercial LLMs, such as GPT4, to evaluate these algorithms. However, such frameworks are expensive and not very transparent. Therefore, we propose a study which demonstrates the interest of open-weight models for evaluating RAG hallucination. We develop a lightweight approach using smaller, quantized LLMs to provide an accessible and interpretable metric that gives continuous scores for the generated answer with respect to their correctness and faithfulness. This score allows us to question decisions' reliability and explore thresholds to develop a new AUC metric as an alternative to correlation with human judgment.</li>
</ul>

<h3>Title: SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shibo Jie, Yehui Tang, Kai Han, Zhi-Hong Deng, Jing Han</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16163">https://arxiv.org/abs/2503.16163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16163">https://arxiv.org/pdf/2503.16163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16163]] SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs(https://arxiv.org/abs/2503.16163)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based large language models (LLMs) have already achieved remarkable results on long-text tasks, but the limited GPU memory (VRAM) resources struggle to accommodate the linearly growing demand for key-value (KV) cache as the sequence length increases, which has become a bottleneck for the application of LLMs on long sequences. Existing KV cache compression methods include eviction, merging, or quantization of the KV cache to reduce its size. However, compression results in irreversible information forgetting, potentially affecting the accuracy of subsequent decoding. In this paper, we propose SpeCache, which takes full advantage of the large and easily expandable CPU memory to offload the complete KV cache, and dynamically fetches KV pairs back in each decoding step based on their importance measured by low-bit KV cache copy in VRAM. To avoid inference latency caused by CPU-GPU communication, SpeCache speculatively predicts the KV pairs that the next token might attend to, allowing us to prefetch them before the next decoding step which enables parallelization of prefetching and computation. Experiments on LongBench and Needle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM usage while avoiding information forgetting for long sequences without re-training, even with a 10x high KV cache compression ratio.</li>
</ul>

<h3>Title: Iterative Optimal Attention and Local Model for Single Image Rain Streak Removal</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Li, Wanshu Fan, Yue Shen, Cong Wang, Wei Wang, Xin Yang, Qiang Zhang, Dongsheng Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16165">https://arxiv.org/abs/2503.16165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16165">https://arxiv.org/pdf/2503.16165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16165]] Iterative Optimal Attention and Local Model for Single Image Rain Streak Removal(https://arxiv.org/abs/2503.16165)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>High-fidelity imaging is crucial for the successful safety supervision and intelligent deployment of vision-based measurement systems (VBMS). It ensures high-quality imaging in VBMS, which is fundamental for reliable visual measurement and analysis. However, imaging quality can be significantly impaired by adverse weather conditions, particularly rain, leading to blurred images and reduced contrast. Such impairments increase the risk of inaccurate evaluations and misinterpretations in VBMS. To address these limitations, we propose an Expectation Maximization Reconstruction Transformer (EMResformer) for single image rain streak removal. The EMResformer retains the key self-attention values for feature aggregation, enhancing local features to produce superior image reconstruction. Specifically, we propose an Expectation Maximization Block seamlessly integrated into the single image rain streak removal network, enhancing its ability to eliminate superfluous information and restore a cleaner background image. Additionally, to further enhance local information for improved detail rendition, we introduce a Local Model Residual Block, which integrates two local model blocks along with a sequence of convolutions and activation functions. This integration synergistically facilitates the extraction of more pertinent features for enhanced single image rain streak removal. Extensive experiments validate that our proposed EMResformer surpasses current state-of-the-art single image rain streak removal methods on both synthetic and real-world datasets, achieving an improved balance between model complexity and single image deraining performance. Furthermore, we evaluate the effectiveness of our method in VBMS scenarios, demonstrating that high-quality imaging significantly improves the accuracy and reliability of VBMS tasks.</li>
</ul>

<h3>Title: Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Soham Roy, Abhishek Mishra, Shirish Karande, Murari Mandal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16171">https://arxiv.org/abs/2503.16171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16171">https://arxiv.org/pdf/2503.16171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16171]] Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation(https://arxiv.org/abs/2503.16171)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Modern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement. We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory. Together, these components form a robust shield that enables a tunable balance between preserving creative fidelity and ensuring copyright compliance. We validate our method on a variety of generative models such as Stable Diffusion, SDXL, and Flux, demonstrating substantial reductions in copyrighted content generation with negligible impact on output fidelity or alignment with user intent. This work provides a practical, plug-and-play safeguard for generative image models, enabling more responsible deployment under real-world copyright constraints. Source code is available at: this https URL</li>
</ul>

<h3>Title: Narrowing Class-Wise Robustness Gaps in Adversarial Training</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Amerehi, Patrick Healy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16179">https://arxiv.org/abs/2503.16179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16179">https://arxiv.org/pdf/2503.16179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16179]] Narrowing Class-Wise Robustness Gaps in Adversarial Training(https://arxiv.org/abs/2503.16179)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Efforts to address declining accuracy as a result of data shifts often involve various data-augmentation strategies. Adversarial training is one such method, designed to improve robustness to worst-case distribution shifts caused by adversarial examples. While this method can improve robustness, it may also hinder generalization to clean examples and exacerbate performance imbalances across different classes. This paper explores the impact of adversarial training on both overall and class-specific performance, as well as its spill-over effects. We observe that enhanced labeling during training boosts adversarial robustness by 53.50% and mitigates class imbalances by 5.73%, leading to improved accuracy in both clean and adversarial settings compared to standard adversarial training.</li>
</ul>

<h3>Title: Variance-Aware Noisy Training: Hardening DNNs against Unstable Analog Computations</h3>
<ul>
<li><strong>Authors: </strong>Xiao Wang, Hendrik Borras, Bernhard Klein, Holger Fröning</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16183">https://arxiv.org/abs/2503.16183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16183">https://arxiv.org/pdf/2503.16183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16183]] Variance-Aware Noisy Training: Hardening DNNs against Unstable Analog Computations(https://arxiv.org/abs/2503.16183)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The disparity between the computational demands of deep learning and the capabilities of compute hardware is expanding drastically. Although deep learning achieves remarkable performance in countless tasks, its escalating requirements for computational power and energy consumption surpass the sustainable limits of even specialized neural processing units, including the Apple Neural Engine and NVIDIA TensorCores. This challenge is intensified by the slowdown in CMOS scaling. Analog computing presents a promising alternative, offering substantial improvements in energy efficiency by directly manipulating physical quantities such as current, voltage, charge, or photons. However, it is inherently vulnerable to manufacturing variations, nonlinearities, and noise, leading to degraded prediction accuracy. One of the most effective techniques for enhancing robustness, Noisy Training, introduces noise during the training phase to reinforce the model against disturbances encountered during inference. Although highly effective, its performance degrades in real-world environments where noise characteristics fluctuate due to external factors such as temperature variations and temporal drift. This study underscores the necessity of Noisy Training while revealing its fundamental limitations in the presence of dynamic noise. To address these challenges, we propose Variance-Aware Noisy Training, a novel approach that mitigates performance degradation by incorporating noise schedules which emulate the evolving noise conditions encountered during inference. Our method substantially improves model robustness, without training overhead. We demonstrate a significant increase in robustness, from 72.3\% with conventional Noisy Training to 97.3\% with Variance-Aware Noisy Training on CIFAR-10 and from 38.5\% to 89.9\% on Tiny ImageNet.</li>
</ul>

<h3>Title: MapGlue: Multimodal Remote Sensing Image Matching</h3>
<ul>
<li><strong>Authors: </strong>Peihao Wu, Yongxiang Yao, Wenfei Zhang, Dong Wei, Yi Wan, Yansheng Li, Yongjun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16185">https://arxiv.org/abs/2503.16185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16185">https://arxiv.org/pdf/2503.16185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16185]] MapGlue: Multimodal Remote Sensing Image Matching(https://arxiv.org/abs/2503.16185)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal remote sensing image (MRSI) matching is pivotal for cross-modal fusion, localization, and object detection, but it faces severe challenges due to geometric, radiometric, and viewpoint discrepancies across imaging modalities. Existing unimodal datasets lack scale and diversity, limiting deep learning solutions. This paper proposes MapGlue, a universal MRSI matching framework, and MapData, a large-scale multimodal dataset addressing these gaps. Our contributions are twofold. MapData, a globally diverse dataset spanning 233 sampling points, offers original images (7,000x5,000 to 20,000x15,000 pixels). After rigorous cleaning, it provides 121,781 aligned electronic map-visible image pairs (512x512 pixels) with hybrid manual-automated ground truth, addressing the scarcity of scalable multimodal benchmarks. MapGlue integrates semantic context with a dual graph-guided mechanism to extract cross-modal invariant features. This structure enables global-to-local interaction, enhancing descriptor robustness against modality-specific distortions. Extensive evaluations on MapData and five public datasets demonstrate MapGlue's superiority in matching accuracy under complex conditions, outperforming state-of-the-art methods. Notably, MapGlue generalizes effectively to unseen modalities without retraining, highlighting its adaptability. This work addresses longstanding challenges in MRSI matching by combining scalable dataset construction with a robust, semantics-driven framework. Furthermore, MapGlue shows strong generalization capabilities on other modality matching tasks for which it was not specifically trained. The dataset and code are available at this https URL.</li>
</ul>

<h3>Title: CLS-RL: Image Classification with Rule-Based Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ming Li, Shitian Zhao, Jike Zhong, Yuxiang Lai, Kaipeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16188">https://arxiv.org/abs/2503.16188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16188">https://arxiv.org/pdf/2503.16188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16188]] CLS-RL: Image Classification with Rule-Based Reinforcement Learning(https://arxiv.org/abs/2503.16188)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Classification is a core task in machine learning. Recent research has shown that although Multimodal Large Language Models (MLLMs) are initially poor at image classification, fine-tuning them with an adequate amount of data can significantly enhance their performance, making them comparable to SOTA classification models. However, acquiring large-scale labeled data is expensive. In this paper, we explore few-shot MLLM classification fine-tuning. We found that SFT can cause severe overfitting issues and may even degrade performance over the zero-shot approach. To address this challenge, inspired by the recent successes in rule-based reinforcement learning, we propose CLS-RL, which uses verifiable signals as reward to fine-tune MLLMs. We discovered that CLS-RL outperforms SFT in most datasets and has a much higher average accuracy on both base-to-new and few-shot learning setting. Moreover, we observed a free-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular dataset, their performance on other distinct datasets may also improve over zero-shot models, even if those datasets differ in distribution and class names. This suggests that RL-based methods effectively teach models the fundamentals of classification. Lastly, inspired by recent works in inference time thinking, we re-examine the `thinking process' during fine-tuning, a critical aspect of RL-based methods, in the context of visual classification. We question whether such tasks require extensive thinking process during fine-tuning, proposing that this may actually detract from performance. Based on this premise, we introduce the No-Thinking-CLS-RL method, which minimizes thinking processes during training by setting an equality accuracy reward. Our findings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL method achieves superior in-domain performance and generalization capabilities than CLS-RL.</li>
</ul>

<h3>Title: VP-NTK: Exploring the Benefits of Visual Prompting in Differentially Private Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Chia-Yi Hsu, Jia-You Chen, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen, Chia-Mu Yu, Chun-Ying Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16195">https://arxiv.org/abs/2503.16195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16195">https://arxiv.org/pdf/2503.16195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16195]] VP-NTK: Exploring the Benefits of Visual Prompting in Differentially Private Data Synthesis(https://arxiv.org/abs/2503.16195)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Differentially private (DP) synthetic data has become the de facto standard for releasing sensitive data. However, many DP generative models suffer from the low utility of synthetic data, especially for high-resolution images. On the other hand, one of the emerging techniques in parameter efficient fine-tuning (PEFT) is visual prompting (VP), which allows well-trained existing models to be reused for the purpose of adapting to subsequent downstream tasks. In this work, we explore such a phenomenon in constructing captivating generative models with DP constraints. We show that VP in conjunction with DP-NTK, a DP generator that exploits the power of the neural tangent kernel (NTK) in training DP generative models, achieves a significant performance boost, particularly for high-resolution image datasets, with accuracy improving from 0.644$\pm$0.044 to 0.769. Lastly, we perform ablation studies on the effect of different parameters that influence the overall performance of VP-NTK. Our work demonstrates a promising step forward in improving the utility of DP synthetic data, particularly for high-resolution images.</li>
</ul>

<h3>Title: Deferring Concept Bottleneck Models: Learning to Defer Interventions to Inaccurate Experts</h3>
<ul>
<li><strong>Authors: </strong>Andrea Pugnana, Riccardo Massidda, Francesco Giannini, Pietro Barbiero, Mateo Espinosa Zarlenga, Roberto Pellungrini, Gabriele Dominici, Fosca Giannotti, Davide Bacciu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16199">https://arxiv.org/abs/2503.16199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16199">https://arxiv.org/pdf/2503.16199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16199]] Deferring Concept Bottleneck Models: Learning to Defer Interventions to Inaccurate Experts(https://arxiv.org/abs/2503.16199)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Concept Bottleneck Models (CBMs) are machine learning models that improve interpretability by grounding their predictions on human-understandable concepts, allowing for targeted interventions in their decision-making process. However, when intervened on, CBMs assume the availability of humans that can identify the need to intervene and always provide correct interventions. Both assumptions are unrealistic and impractical, considering labor costs and human error-proneness. In contrast, Learning to Defer (L2D) extends supervised learning by allowing machine learning models to identify cases where a human is more likely to be correct than the model, thus leading to deferring systems with improved performance. In this work, we gain inspiration from L2D and propose Deferring CBMs (DCBMs), a novel framework that allows CBMs to learn when an intervention is needed. To this end, we model DCBMs as a composition of deferring systems and derive a consistent L2D loss to train them. Moreover, by relying on a CBM architecture, DCBMs can explain why defer occurs on the final task. Our results show that DCBMs achieve high predictive performance and interpretability at the cost of deferring more to humans.</li>
</ul>

<h3>Title: MathFusion: Enhancing Mathematic Problem-solving of LLM through Instruction Fusion</h3>
<ul>
<li><strong>Authors: </strong>Qizhi Pei, Lijun Wu, Zhuoshi Pan, Yu Li, Honglin Lin, Chenlin Ming, Xin Gao, Conghui He, Rui Yan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16212">https://arxiv.org/abs/2503.16212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16212">https://arxiv.org/pdf/2503.16212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16212]] MathFusion: Enhancing Mathematic Problem-solving of LLM through Instruction Fusion(https://arxiv.org/abs/2503.16212)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive progress in mathematical reasoning. While data augmentation is promising to enhance mathematical problem-solving ability, current approaches are predominantly limited to instance-level modifications-such as rephrasing or generating syntactic variations-which fail to capture and leverage the intrinsic relational structures inherent in mathematical knowledge. Inspired by human learning processes, where mathematical proficiency develops through systematic exposure to interconnected concepts, we introduce MathFusion, a novel framework that enhances mathematical reasoning through cross-problem instruction synthesis. MathFusion implements this through three fusion strategies: (1) sequential fusion, which chains related problems to model solution dependencies; (2) parallel fusion, which combines analogous problems to reinforce conceptual understanding; and (3) conditional fusion, which creates context-aware selective problems to enhance reasoning flexibility. By applying these strategies, we generate a new dataset, \textbf{MathFusionQA}, followed by fine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental results demonstrate that MathFusion achieves substantial improvements in mathematical reasoning while maintaining high data efficiency, boosting performance by 18.0 points in accuracy across diverse benchmarks while requiring only 45K additional synthetic instructions, representing a substantial improvement over traditional single-instruction approaches. Our datasets, models, and code are publicly available at this https URL.</li>
</ul>

<h3>Title: Temporal Score Analysis for Understanding and Correcting Diffusion Artifacts</h3>
<ul>
<li><strong>Authors: </strong>Yu Cao, Zengqun Zhao, Ioannis Patras, Shaogang Gong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16218">https://arxiv.org/abs/2503.16218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16218">https://arxiv.org/pdf/2503.16218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16218]] Temporal Score Analysis for Understanding and Correcting Diffusion Artifacts(https://arxiv.org/abs/2503.16218)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Visual artifacts remain a persistent challenge in diffusion models, even with training on massive datasets. Current solutions primarily rely on supervised detectors, yet lack understanding of why these artifacts occur in the first place. In our analysis, we identify three distinct phases in the diffusion generative process: Profiling, Mutation, and Refinement. Artifacts typically emerge during the Mutation phase, where certain regions exhibit anomalous score dynamics over time, causing abrupt disruptions in the normal evolution pattern. This temporal nature explains why existing methods focusing only on spatial uncertainty of the final output fail at effective artifact localization. Based on these insights, we propose ASCED (Abnormal Score Correction for Enhancing Diffusion), that detects artifacts by monitoring abnormal score dynamics during the diffusion process, with a trajectory-aware on-the-fly mitigation strategy that appropriate generation of noise in the detected areas. Unlike most existing methods that apply post hoc corrections, \eg, by applying a noising-denoising scheme after generation, our mitigation strategy operates seamlessly within the existing diffusion process. Extensive experiments demonstrate that our proposed approach effectively reduces artifacts across diverse domains, matching or surpassing existing supervised methods without additional training.</li>
</ul>

<h3>Title: Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't</h3>
<ul>
<li><strong>Authors: </strong>Quy-Anh Dang, Chris Ngo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16219">https://arxiv.org/abs/2503.16219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16219">https://arxiv.org/pdf/2503.16219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16219]] Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't(https://arxiv.org/abs/2503.16219)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Enhancing the reasoning capabilities of large language models (LLMs) typically relies on massive computational resources and extensive datasets, limiting accessibility for resource-constrained settings. Our study investigates the potential of reinforcement learning (RL) to improve reasoning in small LLMs, focusing on a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA A40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy Optimization (GRPO) algorithm and curating a compact, high-quality mathematical reasoning dataset, we conducted three experiments to explore model behavior and performance. Our results demonstrate rapid reasoning gains - e.g., AMC23 accuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing o1-preview - using only 7,000 samples and a $42 training cost, compared to thousands of dollars for baseline models. However, challenges such as optimization instability and length constraints emerged with prolonged training. These findings highlight the efficacy of RL-based fine-tuning for small LLMs, offering a cost-effective alternative to large-scale approaches. We release our code and datasets as open-source resources, providing insights into trade-offs and laying a foundation for scalable, reasoning-capable LLMs in resource-limited environments. All are available at this https URL.</li>
</ul>

<h3>Title: Empirical Analysis of Privacy-Fairness-Accuracy Trade-offs in Federated Learning: A Step Towards Responsible AI</h3>
<ul>
<li><strong>Authors: </strong>Dawood Wasif, Dian Chen, Sindhuja Madabushi, Nithin Alluru, Terrence J. Moore, Jin-Hee Cho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.DC, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16233">https://arxiv.org/abs/2503.16233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16233">https://arxiv.org/pdf/2503.16233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16233]] Empirical Analysis of Privacy-Fairness-Accuracy Trade-offs in Federated Learning: A Step Towards Responsible AI(https://arxiv.org/abs/2503.16233)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, federate, fair</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative machine learning while preserving data privacy but struggles to balance privacy preservation (PP) and fairness. Techniques like Differential Privacy (DP), Homomorphic Encryption (HE), and Secure Multi-Party Computation (SMC) protect sensitive data but introduce trade-offs. DP enhances privacy but can disproportionately impact underrepresented groups, while HE and SMC mitigate fairness concerns at the cost of computational overhead. This work explores the privacy-fairness trade-offs in FL under IID (Independent and Identically Distributed) and non-IID data distributions, benchmarking q-FedAvg, q-MAML, and Ditto on diverse datasets. Our findings highlight context-dependent trade-offs and offer guidelines for designing FL systems that uphold responsible AI principles, ensuring fairness, privacy, and equitable real-world applications.</li>
</ul>

<h3>Title: OpenMIBOOD: Open Medical Imaging Benchmarks for Out-Of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Max Gutbrod, David Rauber, Danilo Weber Nunes, Christoph Palm</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16247">https://arxiv.org/abs/2503.16247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16247">https://arxiv.org/pdf/2503.16247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16247]] OpenMIBOOD: Open Medical Imaging Benchmarks for Out-Of-Distribution Detection(https://arxiv.org/abs/2503.16247)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>The growing reliance on Artificial Intelligence (AI) in critical domains such as healthcare demands robust mechanisms to ensure the trustworthiness of these systems, especially when faced with unexpected or anomalous inputs. This paper introduces the Open Medical Imaging Benchmarks for Out-Of-Distribution Detection (OpenMIBOOD), a comprehensive framework for evaluating out-of-distribution (OOD) detection methods specifically in medical imaging contexts. OpenMIBOOD includes three benchmarks from diverse medical domains, encompassing 14 datasets divided into covariate-shifted in-distribution, near-OOD, and far-OOD categories. We evaluate 24 post-hoc methods across these benchmarks, providing a standardized reference to advance the development and fair comparison of OOD detection methods. Results reveal that findings from broad-scale OOD benchmarks in natural image domains do not translate to medical applications, underscoring the critical need for such benchmarks in the medical field. By mitigating the risk of exposing AI models to inputs outside their training distribution, OpenMIBOOD aims to support the advancement of reliable and trustworthy AI systems in healthcare. The repository is available at this https URL.</li>
</ul>

<h3>Title: AI Agents in Cryptoland: Practical Attacks and No Silver Bullet</h3>
<ul>
<li><strong>Authors: </strong>Atharv Singh Patlan, Peiyao Sheng, S. Ashwin Hebbar, Prateek Mittal, Pramod Viswanath</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16248">https://arxiv.org/abs/2503.16248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16248">https://arxiv.org/pdf/2503.16248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16248]] AI Agents in Cryptoland: Practical Attacks and No Silver Bullet(https://arxiv.org/abs/2503.16248)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, defense, attack</a></li>
<li><strong>Abstract: </strong>The integration of AI agents with Web3 ecosystems harnesses their complementary potential for autonomy and openness, yet also introduces underexplored security risks, as these agents dynamically interact with financial protocols and immutable smart contracts. This paper investigates the vulnerabilities of AI agents within blockchain-based financial ecosystems when exposed to adversarial threats in real-world scenarios. We introduce the concept of context manipulation -- a comprehensive attack vector that exploits unprotected context surfaces, including input channels, memory modules, and external data feeds. Through empirical analysis of ElizaOS, a decentralized AI agent framework for automated Web3 operations, we demonstrate how adversaries can manipulate context by injecting malicious instructions into prompts or historical interaction records, leading to unintended asset transfers and protocol violations which could be financially devastating. Our findings indicate that prompt-based defenses are insufficient, as malicious inputs can corrupt an agent's stored context, creating cascading vulnerabilities across interactions and platforms. This research highlights the urgent need to develop AI agents that are both secure and fiduciarily responsible.</li>
</ul>

<h3>Title: RESFL: An Uncertainty-Aware Framework for Responsible Federated Learning by Balancing Privacy, Fairness and Utility in Autonomous Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Dawood Wasif, Terrence J. Moore, Jin-Hee Cho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.DC, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16251">https://arxiv.org/abs/2503.16251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16251">https://arxiv.org/pdf/2503.16251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16251]] RESFL: An Uncertainty-Aware Framework for Responsible Federated Learning by Balancing Privacy, Fairness and Utility in Autonomous Vehicles(https://arxiv.org/abs/2503.16251)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, federate, fair</a></li>
<li><strong>Abstract: </strong>Autonomous vehicles (AVs) increasingly rely on Federated Learning (FL) to enhance perception models while preserving privacy. However, existing FL frameworks struggle to balance privacy, fairness, and robustness, leading to performance disparities across demographic groups. Privacy-preserving techniques like differential privacy mitigate data leakage risks but worsen fairness by restricting access to sensitive attributes needed for bias correction. This work explores the trade-off between privacy and fairness in FL-based object detection for AVs and introduces RESFL, an integrated solution optimizing both. RESFL incorporates adversarial privacy disentanglement and uncertainty-guided fairness-aware aggregation. The adversarial component uses a gradient reversal layer to remove sensitive attributes, reducing privacy risks while maintaining fairness. The uncertainty-aware aggregation employs an evidential neural network to weight client updates adaptively, prioritizing contributions with lower fairness disparities and higher confidence. This ensures robust and equitable FL model updates. We evaluate RESFL on the FACET dataset and CARLA simulator, assessing accuracy, fairness, privacy resilience, and robustness under varying conditions. RESFL improves detection accuracy, reduces fairness disparities, and lowers privacy attack success rates while demonstrating superior robustness to adversarial conditions compared to other approaches.</li>
</ul>

<h3>Title: Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, Chao Li, Sheng Xu, Dezhi Chen, Yun Chen, Zuo Bai, Liwen Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16252">https://arxiv.org/abs/2503.16252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16252">https://arxiv.org/pdf/2503.16252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16252]] Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning(https://arxiv.org/abs/2503.16252)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning large language models are rapidly evolving across various domains. However, their capabilities in handling complex financial tasks still require in-depth exploration. In this paper, we introduce Fin-R1, a reasoning large language model specifically designed for the financial sector. Fin-R1 is built using a two-stage architecture, leveraging a financial reasoning dataset distilled and processed based on DeepSeek-R1. Through supervised fine-tuning (SFT) and reinforcement learning (RL) training, it demonstrates performance close to DeepSeek-R1 with a parameter size of 7 billion across a range of financial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA and ConvFinQA tasks between those LLMs in our evaluation, surpassing larger models in other tasks as well. Fin-R1 showcases strong reasoning and decision-making capabilities, providing solutions to various problems encountered in the financial domain. Our code is available at this https URL.</li>
</ul>

<h3>Title: M2N2V2: Multi-Modal Unsupervised and Training-free Interactive Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Markus Karmann, Peng-Tao Jiang, Bo Li, Onay Urfalioglu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16254">https://arxiv.org/abs/2503.16254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16254">https://arxiv.org/pdf/2503.16254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16254]] M2N2V2: Multi-Modal Unsupervised and Training-free Interactive Segmentation(https://arxiv.org/abs/2503.16254)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>We present Markov Map Nearest Neighbor V2 (M2N2V2), a novel and simple, yet effective approach which leverages depth guidance and attention maps for unsupervised and training-free point-prompt-based interactive segmentation. Following recent trends in supervised multimodal approaches, we carefully integrate depth as an additional modality to create novel depth-guided Markov-maps. Furthermore, we observe occasional segment size fluctuations in M2N2 during the interactive process, which can decrease the overall mIoU's. To mitigate this problem, we model the prompting as a sequential process and propose a novel adaptive score function which considers the previous segmentation and the current prompt point in order to prevent unreasonable segment size changes. Using Stable Diffusion 2 and Depth Anything V2 as backbones, we empirically show that our proposed M2N2V2 significantly improves the Number of Clicks (NoC) and mIoU compared to M2N2 in all datasets except those from the medical domain. Interestingly, our unsupervised approach achieves competitive results compared to supervised methods like SAM and SimpleClick in the more challenging DAVIS and HQSeg44K datasets in the NoC metric, reducing the gap between supervised and unsupervised methods.</li>
</ul>

<h3>Title: Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Keda Tao, Haoxuan You, Yang Sui, Can Qin, Huan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16257">https://arxiv.org/abs/2503.16257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16257">https://arxiv.org/pdf/2503.16257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16257]] Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language Models(https://arxiv.org/abs/2503.16257)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video large language models (VideoLLMs) have demonstrated the capability to process longer video inputs and enable complex reasoning and analysis. However, due to the thousands of visual tokens from the video frames, key-value (KV) cache can significantly increase memory requirements, becoming a bottleneck for inference speed and memory usage. KV cache quantization is a widely used approach to address this problem. In this paper, we find that 2-bit KV quantization of VideoLLMs can hardly hurt the model performance, while the limit of KV cache quantization in even lower bits has not been investigated. To bridge this gap, we introduce VidKV, a plug-and-play KV cache quantization method to compress the KV cache to lower than 2 bits. Specifically, (1) for key, we propose a mixed-precision quantization strategy in the channel dimension, where we perform 2-bit quantization for anomalous channels and 1-bit quantization combined with FFT for normal channels; (2) for value, we implement 1.58-bit quantization while selectively filtering semantically salient visual tokens for targeted preservation, for a better trade-off between precision and model performance. Importantly, our findings suggest that the value cache of VideoLLMs should be quantized in a per-channel fashion instead of the per-token fashion proposed by prior KV cache quantization works for LLMs. Empirically, extensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show that VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit precision with almost no performance drop compared to the FP16 counterparts.</li>
</ul>

<h3>Title: Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart Reasoning Data</h3>
<ul>
<li><strong>Authors: </strong>Zijian Li, Jingjing Fu, Lei Song, Jiang Bian, Jun Zhang, Rui Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16260">https://arxiv.org/abs/2503.16260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16260">https://arxiv.org/pdf/2503.16260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16260]] Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart Reasoning Data(https://arxiv.org/abs/2503.16260)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Visual reasoning is crucial for multimodal large language models (MLLMs) to address complex chart queries, yet high-quality rationale data remains scarce. Existing methods leveraged (M)LLMs for data generation, but direct prompting often yields limited precision and diversity. In this paper, we propose \textit{Chain of Functions (CoF)}, a novel programmatic reasoning data generation pipeline that utilizes freely-explored reasoning paths as supervision to ensure data precision and diversity. Specifically, it starts with human-free exploration among the atomic functions (e.g., maximum data and arithmetic operations) to generate diverse function chains, which are then translated into linguistic rationales and questions with only a moderate open-sourced LLM. \textit{CoF} provides multiple benefits: 1) Precision: function-governed generation reduces hallucinations compared to freeform generation; 2) Diversity: enumerating function chains enables varied question taxonomies; 3) Explainability: function chains serve as built-in rationales, allowing fine-grained evaluation beyond overall accuracy; 4) Practicality: eliminating reliance on extremely large models. Employing \textit{CoF}, we construct the \textit{ChartCoF} dataset, with 1.4k complex reasoning Q\&A for fine-grained analysis and 50k Q\&A for reasoning enhancement. The fine-grained evaluation on \textit{ChartCoF} reveals varying performance across question taxonomies for each MLLM, and the experiments also show that finetuning with \textit{ChartCoF} achieves state-of-the-art performance among same-scale MLLMs on widely used benchmarks. Furthermore, the novel paradigm of function-governed rationale generation in \textit{CoF} could inspire broader applications beyond charts.</li>
</ul>

<h3>Title: From Head to Tail: Efficient Black-box Model Inversion Attack via Long-tailed Learning</h3>
<ul>
<li><strong>Authors: </strong>Ziang Li, Hongguang Zhang, Juan Wang, Meihui Chen, Hongxin Hu, Wenzhe Yi, Xiaoyang Xu, Mengda Yang, Chenjun Ma</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16266">https://arxiv.org/abs/2503.16266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16266">https://arxiv.org/pdf/2503.16266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16266]] From Head to Tail: Efficient Black-box Model Inversion Attack via Long-tailed Learning(https://arxiv.org/abs/2503.16266)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack</a></li>
<li><strong>Abstract: </strong>Model Inversion Attacks (MIAs) aim to reconstruct private training data from models, leading to privacy leakage, particularly in facial recognition systems. Although many studies have enhanced the effectiveness of white-box MIAs, less attention has been paid to improving efficiency and utility under limited attacker capabilities. Existing black-box MIAs necessitate an impractical number of queries, incurring significant overhead. Therefore, we analyze the limitations of existing MIAs and introduce Surrogate Model-based Inversion with Long-tailed Enhancement (SMILE), a high-resolution oriented and query-efficient MIA for the black-box setting. We begin by analyzing the initialization of MIAs from a data distribution perspective and propose a long-tailed surrogate training method to obtain high-quality initial points. We then enhance the attack's effectiveness by employing the gradient-free black-box optimization algorithm selected by NGOpt. Our experiments show that SMILE outperforms existing state-of-the-art black-box MIAs while requiring only about 5% of the query overhead.</li>
</ul>

<h3>Title: Rethinking Robustness in Machine Learning: A Posterior Agreement Approach</h3>
<ul>
<li><strong>Authors: </strong>João Borges S. Carvalho, Alessandro Torcinovich, Victor Jimenez Rodriguez, Antonio E. Cinà, Carlos Cotrini, Lea Schönherr, Joachim M. Buhmann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16271">https://arxiv.org/abs/2503.16271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16271">https://arxiv.org/pdf/2503.16271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16271]] Rethinking Robustness in Machine Learning: A Posterior Agreement Approach(https://arxiv.org/abs/2503.16271)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The robustness of algorithms against covariate shifts is a fundamental problem with critical implications for the deployment of machine learning algorithms in the real world. Current evaluation methods predominantly match the robustness definition to that of standard generalization, relying on standard metrics like accuracy-based scores, which, while designed for performance assessment, lack a theoretical foundation encompassing their application in estimating robustness to distribution shifts. In this work, we set the desiderata for a robustness metric, and we propose a novel principled framework for the robustness assessment problem that directly follows the Posterior Agreement (PA) theory of model validation. Specifically, we extend the PA framework to the covariate shift setting by proposing a PA metric for robustness evaluation in supervised classification tasks. We assess the soundness of our metric in controlled environments and through an empirical robustness analysis in two different covariate shift scenarios: adversarial learning and domain generalization. We illustrate the suitability of PA by evaluating several models under different nature and magnitudes of shift, and proportion of affected observations. The results show that the PA metric provides a sensible and consistent analysis of the vulnerabilities in learning algorithms, even in the presence of few perturbed observations.</li>
</ul>

<h3>Title: Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on Compressed Spatial Tokens</h3>
<ul>
<li><strong>Authors: </strong>Shuqi Lu, Haowei Lin, Lin Yao, Zhifeng Gao, Xiaohong Ji, Weinan E, Linfeng Zhang, Guolin Ke</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16278">https://arxiv.org/abs/2503.16278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16278">https://arxiv.org/pdf/2503.16278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16278]] Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on Compressed Spatial Tokens(https://arxiv.org/abs/2503.16278)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models and their multi-modal extensions have demonstrated the effectiveness of unifying generation and understanding through autoregressive next-token prediction. However, despite the critical role of 3D structural generation and understanding ({3D GU}) in AI for science, these tasks have largely evolved independently, with autoregressive methods remaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified framework that seamlessly integrates {3D GU} tasks via autoregressive prediction. At its core, Uni-3DAR employs a novel hierarchical tokenization that compresses 3D space using an octree, leveraging the inherent sparsity of 3D structures. It then applies an additional tokenization for fine-grained structural details, capturing key attributes such as atom types and precise spatial coordinates in microscopic 3D structures. We further propose two optimizations to enhance efficiency and effectiveness. The first is a two-level subtree compression strategy, which reduces the octree token sequence by up to 8x. The second is a masked next-token prediction mechanism tailored for dynamically varying token positions, significantly boosting model performance. By combining these strategies, Uni-3DAR successfully unifies diverse {3D GU} tasks within a single autoregressive framework. Extensive experiments across multiple microscopic {3D GU} tasks, including molecules, proteins, polymers, and crystals, validate its effectiveness and versatility. Notably, Uni-3DAR surpasses previous state-of-the-art diffusion models by a substantial margin, achieving up to 256\% relative improvement while delivering inference speeds up to 21.8x faster. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model</h3>
<ul>
<li><strong>Authors: </strong>Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Junlin Han, Ender Konukoglu, Serge Belongie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16282">https://arxiv.org/abs/2503.16282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16282">https://arxiv.org/pdf/2503.16282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16282]] Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model(https://arxiv.org/abs/2503.16282)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to new classes with few support samples while retaining base class segmentation. Existing GFS-PCS methods enhance prototypes via interacting with support or query features but remain limited by sparse knowledge from few-shot samples. Meanwhile, 3D vision-language models (3D VLMs), generalizing across open-world novel classes, contain rich but noisy novel class knowledge. In this work, we introduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels from 3D VLMs with precise yet sparse few-shot samples to maximize the strengths of both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label selection to filter low-quality regions, followed by an adaptive infilling strategy that combines knowledge from pseudo-label contexts and few-shot samples to adaptively label the filtered, unlabeled areas. Additionally, we design a novel-base mix strategy to embed few-shot samples into training scenes, preserving essential context for improved novel class learning. Moreover, recognizing the limited diversity in current GFS-PCS benchmarks, we introduce two challenging benchmarks with diverse novel classes for comprehensive generalization evaluation. Experiments validate the effectiveness of our framework across models and datasets. Our approach and benchmarks provide a solid foundation for advancing GFS-PCS in the real world. The code is at this https URL</li>
</ul>

<h3>Title: Investigating The Implications of Cyberattacks Against Precision Agricultural Equipment</h3>
<ul>
<li><strong>Authors: </strong>Mark Freyhof, George Grispos, Santosh K. Pitla, William Mahoney</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16283">https://arxiv.org/abs/2503.16283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16283">https://arxiv.org/pdf/2503.16283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16283]] Investigating The Implications of Cyberattacks Against Precision Agricultural Equipment(https://arxiv.org/abs/2503.16283)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>As various technologies are integrated and implemented into the food and agricultural industry, it is increasingly important for stakeholders throughout the sector to identify and reduce cybersecurity vulnerabilities and risks associated with these technologies. However, numerous industry and government reports suggest that many farmers and agricultural equipment manufacturers do not fully understand the cyber threats posed by modern agricultural technologies, including CAN bus-driven farming equipment. This paper addresses this knowledge gap by attempting to quantify the cybersecurity risks associated with cyberattacks on farming equipment that utilize CAN bus technology. The contribution of this paper is twofold. First, it presents a hypothetical case study, using real-world data, to illustrate the specific and wider impacts of a cyberattack on a CAN bus-driven fertilizer applicator employed in row-crop farming. Second, it establishes a foundation for future research on quantifying cybersecurity risks related to agricultural machinery.</li>
</ul>

<h3>Title: Explainable Graph-theoretical Machine Learning: with Application to Alzheimer's Disease Prediction</h3>
<ul>
<li><strong>Authors: </strong>Narmina Baghirova, Duy-Thanh Vũ, Duy-Cat Can, Christelle Schneuwly Diaz, Julien Bodlet, Guillaume Blanc, Georgi Hrusanov, Bernard Ries, Oliver Y. Chén</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16286">https://arxiv.org/abs/2503.16286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16286">https://arxiv.org/pdf/2503.16286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16286]] Explainable Graph-theoretical Machine Learning: with Application to Alzheimer's Disease Prediction(https://arxiv.org/abs/2503.16286)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Alzheimer's disease (AD) affects 50 million people worldwide and is projected to overwhelm 152 million by 2050. AD is characterized by cognitive decline due partly to disruptions in metabolic brain connectivity. Thus, early and accurate detection of metabolic brain network impairments is crucial for AD management. Chief to identifying such impairments is FDG-PET data. Despite advancements, most graph-based studies using FDG-PET data rely on group-level analysis or thresholding. Yet, group-level analysis can veil individual differences and thresholding may overlook weaker but biologically critical brain connections. Additionally, machine learning-based AD prediction largely focuses on univariate outcomes, such as disease status. Here, we introduce explainable graph-theoretical machine learning (XGML), a framework employing kernel density estimation and dynamic time warping to construct individual metabolic brain graphs that capture the distance between pair-wise brain regions and identify subgraphs most predictive of multivariate AD-related outcomes. Using FDG-PET data from the Alzheimer's Disease Neuroimaging Initiative, XGML builds metabolic brain graphs and uncovers subgraphs predictive of eight AD-related cognitive scores in new subjects. XGML shows robust performance, particularly for predicting scores measuring learning, memory, language, praxis, and orientation, such as CDRSB ($r = 0.74$), ADAS11 ($r = 0.73$), and ADAS13 ($r = 0.71$). Moreover, XGML unveils key edges jointly but differentially predictive of several AD-related outcomes; they may serve as potential network biomarkers for assessing overall cognitive decline. Together, we show the promise of graph-theoretical machine learning in biomarker discovery and disease prediction and its potential to improve our understanding of network neural mechanisms underlying AD.</li>
</ul>

<h3>Title: Securing Satellite Communications: Real-Time Video Encryption Scheme on Satellite Payloads</h3>
<ul>
<li><strong>Authors: </strong>Hanshuo Qiu, Jing Lian, Xiaoyuan Wang, Jizhao Liu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16287">https://arxiv.org/abs/2503.16287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16287">https://arxiv.org/pdf/2503.16287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16287]] Securing Satellite Communications: Real-Time Video Encryption Scheme on Satellite Payloads(https://arxiv.org/abs/2503.16287)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack</a></li>
<li><strong>Abstract: </strong>The rapid development of low-Earth orbit (LEO) satellite constellations and satellite communication systems has elevated the importance of secure video transmission, which is the key to applications such as remote sensing, disaster relief, and secure information exchange. In this context, three serious issues arise concerning real-time encryption of videos on satellite embedded devices: (a) the challenge of achieving real-time performance; (b) the limitations posed by the constrained computing performance of satellite payloads; and (c) the potential for excessive power consumption leading to overheating, thereby escalating safety risks. To overcome these challenges, this study introduced a novel approach for encrypting videos by employing two 1D chaotic maps, which was deployed on a satellite for the first time. The experiment on the satellite confirms that our scheme is suitable for complex satellite environments. In addition, the proposed chaotic maps were implemented on a Field Programmable Gate Array (FPGA) platform, and simulation results showed consistency with those obtained on a Raspberry Pi. Experiments on the Raspberry Pi 4B demonstrate exceptional real-time performance and low power consumption, validating both the hardware feasibility and the stability of our design. Rigorous statistical testing also confirms the scheme's resilience against a variety of attacks, underscoring its potential for secure, real-time data transmission in satellite communication systems.</li>
</ul>

<h3>Title: SceneMI: Motion In-betweening for Modeling Human-Scene Interactions</h3>
<ul>
<li><strong>Authors: </strong>Inwoo Hwang, Bing Zhou, Young Min Kim, Jian Wang, Chuan Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16289">https://arxiv.org/abs/2503.16289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16289">https://arxiv.org/pdf/2503.16289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16289]] SceneMI: Motion In-betweening for Modeling Human-Scene Interactions(https://arxiv.org/abs/2503.16289)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Modeling human-scene interactions (HSI) is essential for understanding and simulating everyday human behaviors. Recent approaches utilizing generative modeling have made progress in this domain; however, they are limited in controllability and flexibility for real-world applications. To address these challenges, we propose reformulating the HSI modeling problem as Scene-aware Motion In-betweening -- a more tractable and practical task. We introduce SceneMI, a framework that supports several practical applications, including keyframe-guided character animation in 3D scenes and enhancing the motion quality of imperfect HSI data. SceneMI employs dual scene descriptors to comprehensively encode global and local scene context. Furthermore, our framework leverages the inherent denoising nature of diffusion models to generalize on noisy keyframes. Experimental results demonstrate SceneMI's effectiveness in scene-aware keyframe in-betweening and generalization to the real-world GIMO dataset, where motions and scenes are acquired by noisy IMU sensors and smartphones. We further showcase SceneMI's applicability in HSI reconstruction from monocular videos.</li>
</ul>

<h3>Title: Cultivating Cybersecurity: Designing a Cybersecurity Curriculum for the Food and Agriculture Sector</h3>
<ul>
<li><strong>Authors: </strong>George Grispos, Logan Mears, Larry Loucks, William Mahoney</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16292">https://arxiv.org/abs/2503.16292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16292">https://arxiv.org/pdf/2503.16292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16292]] Cultivating Cybersecurity: Designing a Cybersecurity Curriculum for the Food and Agriculture Sector(https://arxiv.org/abs/2503.16292)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>As technology increasingly integrates into farm settings, the food and agriculture sector has become vulnerable to cyberattacks. However, previous research has indicated that many farmers and food producers lack the cybersecurity education they require to identify and mitigate the growing number of threats and risks impacting the industry. This paper presents an ongoing research effort describing a cybersecurity initiative to educate various populations in the farming and agriculture community. The initiative proposes the development and delivery of a ten-module cybersecurity course, to create a more secure workforce, focusing on individuals who, in the past, have received minimal exposure to cybersecurity education initiatives.</li>
</ul>

<h3>Title: Unleashing Vecset Diffusion Model for Fast Shape Generation</h3>
<ul>
<li><strong>Authors: </strong>Zeqiang Lai, Yunfei Zhao, Zibo Zhao, Haolin Liu, Fuyun Wang, Huiwen Shi, Xianghui Yang, Qinxiang Lin, Jinwei Huang, Yuhong Liu, Jie Jiang, Chunchao Guo, Xiangyu Yue</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16302">https://arxiv.org/abs/2503.16302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16302">https://arxiv.org/pdf/2503.16302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16302]] Unleashing Vecset Diffusion Model for Fast Shape Generation(https://arxiv.org/abs/2503.16302)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>3D shape generation has greatly flourished through the development of so-called "native" 3D diffusion, particularly through the Vecset Diffusion Model (VDM). While recent advancements have shown promising results in generating high-resolution 3D shapes, VDM still struggles with high-speed generation. Challenges exist because of difficulties not only in accelerating diffusion sampling but also VAE decoding in VDM, areas under-explored in previous works. To address these challenges, we present FlashVDM, a systematic framework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables flexible diffusion sampling with as few as 5 inference steps and comparable quality, which is made possible by stabilizing consistency distillation with our newly introduced Progressive Flow Distillation. For VAE, we introduce a lightning vecset decoder equipped with Adaptive KV Selection, Hierarchical Volume Decoding, and Efficient Network Design. By exploiting the locality of the vecset and the sparsity of shape surface in the volume, our decoder drastically lowers FLOPs, minimizing the overall decoding overhead. We apply FlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic evaluation, we show that our model significantly outperforms existing fast 3D generation methods, achieving comparable performance to the state-of-the-art while reducing inference time by over 45x for reconstruction and 32x for generation. Code and models are available at this https URL.</li>
</ul>

<h3>Title: Dynamic Point Maps: A Versatile Representation for Dynamic 3D Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Edgar Sucar, Zihang Lai, Eldar Insafutdinov, Andrea Vedaldi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16318">https://arxiv.org/abs/2503.16318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16318">https://arxiv.org/pdf/2503.16318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16318]] Dynamic Point Maps: A Versatile Representation for Dynamic 3D Reconstruction(https://arxiv.org/abs/2503.16318)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>DUSt3R has recently shown that one can reduce many tasks in multi-view geometry, including estimating camera intrinsics and extrinsics, reconstructing the scene in 3D, and establishing image correspondences, to the prediction of a pair of viewpoint-invariant point maps, i.e., pixel-aligned point clouds defined in a common reference frame. This formulation is elegant and powerful, but unable to tackle dynamic scenes. To address this challenge, we introduce the concept of Dynamic Point Maps (DPM), extending standard point maps to support 4D tasks such as motion segmentation, scene flow estimation, 3D object tracking, and 2D correspondence. Our key intuition is that, when time is introduced, there are several possible spatial and time references that can be used to define the point maps. We identify a minimal subset of such combinations that can be regressed by a network to solve the sub tasks mentioned above. We train a DPM predictor on a mixture of synthetic and real data and evaluate it across diverse benchmarks for video depth prediction, dynamic point cloud reconstruction, 3D scene flow and object pose tracking, achieving state-of-the-art performance. Code, models and additional results are available at this https URL.</li>
</ul>

<h3>Title: Ultra-Resolution Adaptation with Ease</h3>
<ul>
<li><strong>Authors: </strong>Ruonan Yu, Songhua Liu, Zhenxiong Tan, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16322">https://arxiv.org/abs/2503.16322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16322">https://arxiv.org/pdf/2503.16322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16322]] Ultra-Resolution Adaptation with Ease(https://arxiv.org/abs/2503.16322)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have achieved remarkable progress in recent years. However, training models for high-resolution image generation remains challenging, particularly when training data and computational resources are limited. In this paper, we explore this practical problem from two key perspectives: data and parameter efficiency, and propose a set of key guidelines for ultra-resolution adaptation termed \emph{URAE}. For data efficiency, we theoretically and empirically demonstrate that synthetic data generated by some teacher models can significantly promote training convergence. For parameter efficiency, we find that tuning minor components of the weight matrices outperforms widely-used low-rank adapters when synthetic data are unavailable, offering substantial performance gains while maintaining efficiency. Additionally, for models leveraging guidance distillation, such as FLUX, we show that disabling classifier-free guidance, \textit{i.e.}, setting the guidance scale to 1 during adaptation, is crucial for satisfactory performance. Extensive experiments validate that URAE achieves comparable 2K-generation performance to state-of-the-art closed-source models like FLUX1.1 [Pro] Ultra with only 3K samples and 2K iterations, while setting new benchmarks for 4K-resolution generation. Codes are available \href{this https URL}{here}.</li>
</ul>

<h3>Title: Knowledge-guided machine learning model with soil moisture for corn yield prediction under drought conditions</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Wang, Yijia Xu, Jingyi Huang, Zhengwei Yang, Zhou Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16328">https://arxiv.org/abs/2503.16328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16328">https://arxiv.org/pdf/2503.16328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16328]] Knowledge-guided machine learning model with soil moisture for corn yield prediction under drought conditions(https://arxiv.org/abs/2503.16328)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Remote sensing (RS) techniques, by enabling non-contact acquisition of extensive ground observations, have become a valuable tool for corn yield prediction. Traditional process-based (PB) models are limited by fixed input features and struggle to incorporate large volumes of RS data. In contrast, machine learning (ML) models are often criticized for being ``black boxes'' with limited interpretability. To address these limitations, we used Knowledge-Guided Machine Learning (KGML), which combined the strengths of both approaches and fully used RS data. However, previous KGML methods overlooked the crucial role of soil moisture in plant growth. To bridge this gap, we proposed the Knowledge-Guided Machine Learning with Soil Moisture (KGML-SM) framework, using soil moisture as an intermediate variable to emphasize its key role in plant development. Additionally, based on the prior knowledge that the model may overestimate under drought conditions, we designed a drought-aware loss function that penalizes predicted yield in drought-affected areas. Our experiments showed that the KGML-SM model outperformed other ML models. Finally, we explored the relationships between drought, soil moisture, and corn yield prediction, assessing the importance of various features and analyzing how soil moisture impacts corn yield predictions across different regions and time periods.</li>
</ul>

<h3>Title: LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates</h3>
<ul>
<li><strong>Authors: </strong>Ying Shen, Lifu Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16334">https://arxiv.org/abs/2503.16334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16334">https://arxiv.org/pdf/2503.16334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16334]] LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates(https://arxiv.org/abs/2503.16334)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recent findings reveal that much of the knowledge in a Transformer-based Large Language Model (LLM) is encoded in its feed-forward (FFN) layers, where each FNN layer can be interpreted as the summation of sub-updates, each corresponding to a weighted column vector from the FFN's value parameter matrix that often encodes human-interpretable concepts. In light of this, we hypothesize that model performance and behaviors can be further enhanced and controlled by modulating the contributions of these sub-updates based on their relevance to the input or target output style, and propose LLMBRACES, a novel and efficient method that computes relevance scores associated with value vectors in FFN layers and leverages these scores to dynamically adjust the contribution of sub-updates. By optimizing sub-update contributions, LLMBRACES refines the prediction process, leading to more accurate and reliable outputs, much like a 'brace' providing support and stability. Moreover, LLMBRACES can be extended to support conditional control over generation characteristics, such as sentiment, thereby offering fine-grained steering of LLM outputs. Extensive experiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and Llama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both fine-tuning and zero-shot settings while requiring significantly fewer tunable parameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in sentiment-controlled generation and toxicity reduction, highlighting its potential for flexible, controlled text generation across applications.</li>
</ul>

<h3>Title: Nonlinear action prediction models reveal multi-timescale locomotor control</h3>
<ul>
<li><strong>Authors: </strong>Wei-Chen Wang, Antoine De Comite, Monica Daley, Alexandra Voloshina, Nidhi Seethapathi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16340">https://arxiv.org/abs/2503.16340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16340">https://arxiv.org/pdf/2503.16340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16340]] Nonlinear action prediction models reveal multi-timescale locomotor control(https://arxiv.org/abs/2503.16340)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Modeling movement in real-world tasks is a fundamental scientific goal. However, it is unclear whether existing models and their assumptions, overwhelmingly tested in laboratory-constrained settings, generalize to the real world. For example, data-driven models of foot placement control -- a crucial action for stable locomotion -- assume linear and single timescale mappings. We develop nonlinear foot placement prediction models, finding that neural network architectures with flexible input history-dependence like GRU and Transformer perform best across multiple contexts (walking and running, treadmill and overground, varying terrains) and input modalities (multiple body states, gaze), outperforming traditional models. These models reveal context- and modality-dependent timescales: there is more reliance on fast-timescale predictions in complex terrain, gaze predictions precede body state predictions, and full-body state predictions precede center-of-mass-relevant predictions. Thus, nonlinear action prediction models provide quantifiable insights into real-world motor control and can be extended to other actions, contexts, and populations.</li>
</ul>

<h3>Title: HiQ-Lip: The First Quantum-Classical Hierarchical Method for Global Lipschitz Constant Estimation of ReLU Networks</h3>
<ul>
<li><strong>Authors: </strong>Haoqi He, Yan Xiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16342">https://arxiv.org/abs/2503.16342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16342">https://arxiv.org/pdf/2503.16342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16342]] HiQ-Lip: The First Quantum-Classical Hierarchical Method for Global Lipschitz Constant Estimation of ReLU Networks(https://arxiv.org/abs/2503.16342)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Estimating the global Lipschitz constant of neural networks is crucial for understanding and improving their robustness and generalization capabilities. However, precise calculations are NP-hard, and current semidefinite programming (SDP) methods face challenges such as high memory usage and slow processing speeds. In this paper, we propose \textbf{HiQ-Lip}, a hybrid quantum-classical hierarchical method that leverages Coherent Ising Machines (CIMs) to estimate the global Lipschitz constant. We tackle the estimation by converting it into a Quadratic Unconstrained Binary Optimization (QUBO) problem and implement a multilevel graph coarsening and refinement strategy to adapt to the constraints of contemporary quantum hardware. Our experimental evaluations on fully connected neural networks demonstrate that HiQ-Lip not only provides estimates comparable to state-of-the-art methods but also significantly accelerates the computation process. In specific tests involving two-layer neural networks with 256 hidden neurons, HiQ-Lip doubles the solving speed and offers more accurate upper bounds than the existing best method, LiPopt. These findings highlight the promising utility of small-scale quantum devices in advancing the estimation of neural network robustness.</li>
</ul>

<h3>Title: Lyra: An Efficient and Expressive Subquadratic Architecture for Modeling Biological Sequences</h3>
<ul>
<li><strong>Authors: </strong>Krithik Ramesh (1 and 2), Sameed M. Siddiqui (1 and 3), Albert Gu (4), Michael D. Mitzenmacher (1 and 5), Pardis C. Sabeti (1 and 6 and 7 and 8) ((1) Broad Institute of MIT and Harvard, (2) Massachusetts Institute of Technology, (3) Computational and Systems Biology Program, Massachusetts Institute of Technology, (4) Machine Learning Department, Carnegie Mellon University, (5) School of Engineering and Applied Sciences, Harvard University, (6) Department of Organismic and Evolutionary Biology, Harvard University, (7) Department of Immunology and Infectious Diseases, Harvard T.H. Chan School of Public Health, Harvard University, (8) Howard Hughes Medical Institute)</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16351">https://arxiv.org/abs/2503.16351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16351">https://arxiv.org/pdf/2503.16351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16351]] Lyra: An Efficient and Expressive Subquadratic Architecture for Modeling Biological Sequences(https://arxiv.org/abs/2503.16351)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Deep learning architectures such as convolutional neural networks and Transformers have revolutionized biological sequence modeling, with recent advances driven by scaling up foundation and task-specific models. The computational resources and large datasets required, however, limit their applicability in biological contexts. We introduce Lyra, a subquadratic architecture for sequence modeling, grounded in the biological framework of epistasis for understanding sequence-to-function relationships. Mathematically, we demonstrate that state space models efficiently capture global epistatic interactions and combine them with projected gated convolutions for modeling local relationships. We demonstrate that Lyra is performant across over 100 wide-ranging biological tasks, achieving state-of-the-art (SOTA) performance in many key areas, including protein fitness landscape prediction, biophysical property prediction (e.g. disordered protein region functions) peptide engineering applications (e.g. antibody binding, cell-penetrating peptide prediction), RNA structure analysis, RNA function prediction, and CRISPR guide design. It achieves this with orders-of-magnitude improvements in inference speed and reduction in parameters (up to 120,000-fold in our tests) compared to recent biology foundation models. Using Lyra, we were able to train and run every task in this study on two or fewer GPUs in under two hours, democratizing access to biological sequence modeling at SOTA performance, with potential applications to many fields.</li>
</ul>

<h3>Title: CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners</h3>
<ul>
<li><strong>Authors: </strong>Yunzhi Yao, Jizhan Fang, Jia-Chen Gu, Ningyu Zhang, Shumin Deng, Huajun Chen, Nanyun Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16356">https://arxiv.org/abs/2503.16356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16356">https://arxiv.org/pdf/2503.16356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16356]] CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners(https://arxiv.org/abs/2503.16356)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge Editing (KE) enables the modification of outdated or incorrect information in large language models (LLMs). While existing KE methods can update isolated facts, they struggle to generalize these updates to multi-hop reasoning tasks that depend on the modified knowledge. Through an analysis of reasoning circuits -- the neural pathways LLMs use for knowledge-based inference, we observe that current layer-localized KE approaches, such as MEMIT and WISE, which edit only single or a few model layers, struggle to effectively incorporate updated information into these reasoning pathways. To address this limitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method that enables more effective integration of updated knowledge in LLMs. CaKE leverages strategically curated data, guided by our circuits-based analysis, that enforces the model to utilize the modified knowledge, stimulating the model to develop appropriate reasoning circuits for newly integrated knowledge. Experimental results show that CaKE enables more accurate and consistent use of updated knowledge across related reasoning tasks, leading to an average of 20% improvement in multi-hop reasoning accuracy on MQuAKE dataset compared to existing KE methods. We release the code and data in this https URL.</li>
</ul>

<h3>Title: Probabilistic Quantum SVM Training on Ising Machine</h3>
<ul>
<li><strong>Authors: </strong>Haoqi He, Yan Xiao</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16363">https://arxiv.org/abs/2503.16363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16363">https://arxiv.org/pdf/2503.16363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16363]] Probabilistic Quantum SVM Training on Ising Machine(https://arxiv.org/abs/2503.16363)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Quantum computing holds significant potential to accelerate machine learning algorithms, especially in solving optimization problems like those encountered in Support Vector Machine (SVM) training. However, current QUBO-based Quantum SVM (QSVM) methods rely solely on binary optimal solutions, limiting their ability to identify fuzzy boundaries in data. Additionally, the limited qubit count in contemporary quantum devices constrains training on larger datasets. In this paper, we propose a probabilistic quantum SVM training framework suitable for Coherent Ising Machines (CIMs). By formulating the SVM training problem as a QUBO model, we leverage CIMs' energy minimization capabilities and introduce a Boltzmann distribution-based probabilistic approach to better approximate optimal SVM solutions, enhancing robustness. To address qubit limitations, we employ batch processing and multi-batch ensemble strategies, enabling small-scale quantum devices to train SVMs on larger datasets and support multi-class classification tasks via a one-vs-one approach. Our method is validated through simulations and real-machine experiments on binary and multi-class datasets. On the banknote binary classification dataset, our CIM-based QSVM, utilizing an energy-based probabilistic approach, achieved up to 20% higher accuracy compared to the original QSVM, while training up to $10^4$ times faster than simulated annealing methods. Compared with classical SVM, our approach either matched or reduced training time. On the IRIS three-class dataset, our improved QSVM outperformed existing QSVM models in all key metrics. As quantum technology advances, increased qubit counts are expected to further enhance QSVM performance relative to classical SVM.</li>
</ul>

<h3>Title: NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes</h3>
<ul>
<li><strong>Authors: </strong>Han-Hung Lee, Qinghong Han, Angel X. Chang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16375">https://arxiv.org/abs/2503.16375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16375">https://arxiv.org/pdf/2503.16375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16375]] NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes(https://arxiv.org/abs/2503.16375)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we explore the task of generating expansive outdoor scenes, ranging from castles to high-rises. Unlike indoor scene generation, which has been a primary focus of prior work, outdoor scene generation presents unique challenges, including wide variations in scene heights and the need for a method capable of rapidly producing large landscapes. To address this, we propose an efficient approach that encodes scene chunks as uniform vector sets, offering better compression and performance than the spatially structured latents used in prior methods. Furthermore, we train an explicit outpainting model for unbounded generation, which improves coherence compared to prior resampling-based inpainting schemes while also speeding up generation by eliminating extra diffusion steps. To facilitate this task, we curate NuiScene43, a small but high-quality set of scenes, preprocessed for joint training. Notably, when trained on scenes of varying styles, our model can blend different environments, such as rural houses and city skyscrapers, within the same scene, highlighting the potential of our curation process to leverage heterogeneous scenes for joint training.</li>
</ul>

<h3>Title: LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial Images</h3>
<ul>
<li><strong>Authors: </strong>Leyang Wang, Joice Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16376">https://arxiv.org/abs/2503.16376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16376">https://arxiv.org/pdf/2503.16376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16376]] LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial Images(https://arxiv.org/abs/2503.16376)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>The success of modern machine learning, particularly in facial translation networks, is highly dependent on the availability of high-quality, paired, large-scale datasets. However, acquiring sufficient data is often challenging and costly. Inspired by the recent success of diffusion models in high-quality image synthesis and advancements in Large Language Models (LLMs), we propose a novel framework called LLM-assisted Paired Image Generation (LaPIG). This framework enables the construction of comprehensive, high-quality paired visible and thermal images using captions generated by LLMs. Our method encompasses three parts: visible image synthesis with ArcFace embedding, thermal image translation using Latent Diffusion Models (LDMs), and caption generation with LLMs. Our approach not only generates multi-view paired visible and thermal images to increase data diversity but also produces high-quality paired data while maintaining their identity information. We evaluate our method on public datasets by comparing it with existing methods, demonstrating the superiority of LaPIG.</li>
</ul>

<h3>Title: Panoptic-CUDAL Technical Report: Rural Australia Point Cloud Dataset in Rainy Conditions</h3>
<ul>
<li><strong>Authors: </strong>Tzu-Yun Tseng, Alexey Nekrasov, Malcolm Burdorf, Bastian Leibe, Julie Stephany Berrio, Mao Shan, Stewart Worrall</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16378">https://arxiv.org/abs/2503.16378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16378">https://arxiv.org/pdf/2503.16378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16378]] Panoptic-CUDAL Technical Report: Rural Australia Point Cloud Dataset in Rainy Conditions(https://arxiv.org/abs/2503.16378)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Existing autonomous driving datasets are predominantly oriented towards well-structured urban settings and favorable weather conditions, leaving the complexities of rural environments and adverse weather conditions largely unaddressed. Although some datasets encompass variations in weather and lighting, bad weather scenarios do not appear often. Rainfall can significantly impair sensor functionality, introducing noise and reflections in LiDAR and camera data and reducing the system's capabilities for reliable environmental perception and safe navigation. We introduce the Panoptic-CUDAL dataset, a novel dataset purpose-built for panoptic segmentation in rural areas subject to rain. By recording high-resolution LiDAR, camera, and pose data, Panoptic-CUDAL offers a diverse, information-rich dataset in a challenging scenario. We present analysis of the recorded data and provide baseline results for panoptic and semantic segmentation methods on LiDAR point clouds. The dataset can be found here: this https URL</li>
</ul>

<h3>Title: Graph of Effort: Quantifying Risk of AI Usage for Vulnerability Assessment</h3>
<ul>
<li><strong>Authors: </strong>Anket Mehra, Andreas Aßmuth, Malte Prieß</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16392">https://arxiv.org/abs/2503.16392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16392">https://arxiv.org/pdf/2503.16392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16392]] Graph of Effort: Quantifying Risk of AI Usage for Vulnerability Assessment(https://arxiv.org/abs/2503.16392)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>With AI-based software becoming widely available, the risk of exploiting its capabilities, such as high automation and complex pattern recognition, could significantly increase. An AI used offensively to attack non-AI assets is referred to as offensive AI. Current research explores how offensive AI can be utilized and how its usage can be classified. Additionally, methods for threat modeling are being developed for AI-based assets within organizations. However, there are gaps that need to be addressed. Firstly, there is a need to quantify the factors contributing to the AI threat. Secondly, there is a requirement to create threat models that analyze the risk of being attacked by AI for vulnerability assessment across all assets of an organization. This is particularly crucial and challenging in cloud environments, where sophisticated infrastructure and access control landscapes are prevalent. The ability to quantify and further analyze the threat posed by offensive AI enables analysts to rank vulnerabilities and prioritize the implementation of proactive countermeasures. To address these gaps, this paper introduces the Graph of Effort, an intuitive, flexible, and effective threat modeling method for analyzing the effort required to use offensive AI for vulnerability exploitation by an adversary. While the threat model is functional and provides valuable support, its design choices need further empirical validation in future work.</li>
</ul>

<h3>Title: Do Visual Imaginations Improve Vision-and-Language Navigation Agents?</h3>
<ul>
<li><strong>Authors: </strong>Akhil Perincherry, Jacob Krantz, Stefan Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16394">https://arxiv.org/abs/2503.16394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16394">https://arxiv.org/pdf/2503.16394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16394]] Do Visual Imaginations Improve Vision-and-Language Navigation Agents?(https://arxiv.org/abs/2503.16394)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Vision-and-Language Navigation (VLN) agents are tasked with navigating an unseen environment using natural language instructions. In this work, we study if visual representations of sub-goals implied by the instructions can serve as navigational cues and lead to increased navigation performance. To synthesize these visual representations or imaginations, we leverage a text-to-image diffusion model on landmark references contained in segmented instructions. These imaginations are provided to VLN agents as an added modality to act as landmark cues and an auxiliary loss is added to explicitly encourage relating these with their corresponding referring expressions. Our findings reveal an increase in success rate (SR) of around 1 point and up to 0.5 points in success scaled by inverse path length (SPL) across agents. These results suggest that the proposed approach reinforces visual understanding compared to relying on language instructions alone. Code and data for our work can be found at this https URL.</li>
</ul>

<h3>Title: SV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video Diffusion for High-Quality 4D Generation</h3>
<ul>
<li><strong>Authors: </strong>Chun-Han Yao, Yiming Xie, Vikram Voleti, Huaizu Jiang, Varun Jampani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16396">https://arxiv.org/abs/2503.16396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16396">https://arxiv.org/pdf/2503.16396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16396]] SV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video Diffusion for High-Quality 4D Generation(https://arxiv.org/abs/2503.16396)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>We present Stable Video 4D 2.0 (SV4D 2.0), a multi-view video diffusion model for dynamic 3D asset generation. Compared to its predecessor SV4D, SV4D 2.0 is more robust to occlusions and large motion, generalizes better to real-world videos, and produces higher-quality outputs in terms of detail sharpness and spatio-temporal consistency. We achieve this by introducing key improvements in multiple aspects: 1) network architecture: eliminating the dependency of reference multi-views and designing blending mechanism for 3D and frame attention, 2) data: enhancing quality and quantity of training data, 3) training strategy: adopting progressive 3D-4D training for better generalization, and 4) 4D optimization: handling 3D inconsistency and large motion via 2-stage refinement and progressive frame sampling. Extensive experiments demonstrate significant performance gain by SV4D 2.0 both visually and quantitatively, achieving better detail (-14\% LPIPS) and 4D consistency (-44\% FV4D) in novel-view video synthesis and 4D optimization (-12\% LPIPS and -24\% FV4D) compared to SV4D. Project page: this https URL.</li>
</ul>

<h3>Title: Scale-wise Distillation of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Nikita Starodubcev, Denis Kuznedelev, Artem Babenko, Dmitry Baranchuk</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16397">https://arxiv.org/abs/2503.16397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16397">https://arxiv.org/pdf/2503.16397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16397]] Scale-wise Distillation of Diffusion Models(https://arxiv.org/abs/2503.16397)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present SwD, a scale-wise distillation framework for diffusion models (DMs), which effectively employs next-scale prediction ideas for diffusion-based few-step generators. In more detail, SwD is inspired by the recent insights relating diffusion processes to the implicit spectral autoregression. We suppose that DMs can initiate generation at lower data resolutions and gradually upscale the samples at each denoising step without loss in performance while significantly reducing computational costs. SwD naturally integrates this idea into existing diffusion distillation methods based on distribution matching. Also, we enrich the family of distribution matching approaches by introducing a novel patch loss enforcing finer-grained similarity to the target distribution. When applied to state-of-the-art text-to-image diffusion models, SwD approaches the inference times of two full resolution steps and significantly outperforms the counterparts under the same computation budget, as evidenced by automated metrics and human preference studies.</li>
</ul>

<h3>Title: SA-Occ: Satellite-Assisted 3D Occupancy Prediction in Real World</h3>
<ul>
<li><strong>Authors: </strong>Chen Chen, Zhirui Wang, Taowei Sheng, Yi Jiang, Yundu Li, Peirui Cheng, Luning Zhang, Kaiqiang Chen, Yanfeng Hu, Xue Yang, Xian Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16399">https://arxiv.org/abs/2503.16399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16399">https://arxiv.org/pdf/2503.16399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16399]] SA-Occ: Satellite-Assisted 3D Occupancy Prediction in Real World(https://arxiv.org/abs/2503.16399)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Existing vision-based 3D occupancy prediction methods are inherently limited in accuracy due to their exclusive reliance on street-view imagery, neglecting the potential benefits of incorporating satellite views. We propose SA-Occ, the first Satellite-Assisted 3D occupancy prediction model, which leverages GPS & IMU to integrate historical yet readily available satellite imagery into real-time applications, effectively mitigating limitations of ego-vehicle perceptions, involving occlusions and degraded performance in distant regions. To address the core challenges of cross-view perception, we propose: 1) Dynamic-Decoupling Fusion, which resolves inconsistencies in dynamic regions caused by the temporal asynchrony between satellite and street views; 2) 3D-Proj Guidance, a module that enhances 3D feature extraction from inherently 2D satellite imagery; and 3) Uniform Sampling Alignment, which aligns the sampling density between street and satellite views. Evaluated on Occ3D-nuScenes, SA-Occ achieves state-of-the-art performance, especially among single-frame methods, with a 39.05% mIoU (a 6.97% improvement), while incurring only 6.93 ms of additional latency per frame. Our code and newly curated dataset are available at this https URL.</li>
</ul>

<h3>Title: ScalingNoise: Scaling Inference-Time Search for Generating Infinite Videos</h3>
<ul>
<li><strong>Authors: </strong>Haolin Yang, Feilong Tang, Ming Hu, Yulong Li, Junjie Guo, Yexin Liu, Zelin Peng, Junjun He, Zongyuan Ge, Imran Razzak,</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16400">https://arxiv.org/abs/2503.16400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16400">https://arxiv.org/pdf/2503.16400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16400]] ScalingNoise: Scaling Inference-Time Search for Generating Infinite Videos(https://arxiv.org/abs/2503.16400)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video diffusion models (VDMs) facilitate the generation of high-quality videos, with current research predominantly concentrated on scaling efforts during training through improvements in data quality, computational resources, and model complexity. However, inference-time scaling has received less attention, with most approaches restricting models to a single generation attempt. Recent studies have uncovered the existence of "golden noises" that can enhance video quality during generation. Building on this, we find that guiding the scaling inference-time search of VDMs to identify better noise candidates not only evaluates the quality of the frames generated in the current step but also preserves the high-level object features by referencing the anchor frame from previous multi-chunks, thereby delivering long-term value. Our analysis reveals that diffusion models inherently possess flexible adjustments of computation by varying denoising steps, and even a one-step denoising approach, when guided by a reward signal, yields significant long-term benefits. Based on the observation, we proposeScalingNoise, a plug-and-play inference-time search strategy that identifies golden initial noises for the diffusion sampling process to improve global content consistency and visual diversity. Specifically, we perform one-step denoising to convert initial noises into a clip and subsequently evaluate its long-term value, leveraging a reward model anchored by previously generated content. Moreover, to preserve diversity, we sample candidates from a tilted noise distribution that up-weights promising noises. In this way, ScalingNoise significantly reduces noise-induced errors, ensuring more coherent and spatiotemporally consistent video generation. Extensive experiments on benchmark datasets demonstrate that the proposed ScalingNoise effectively improves long video generation.</li>
</ul>

<h3>Title: Exploring the Hidden Reasoning Process of Large Language Models by Misleading Them</h3>
<ul>
<li><strong>Authors: </strong>Guanyu Chen, Peiyang Wang, Tianren Zhang, Feng Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16401">https://arxiv.org/abs/2503.16401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16401">https://arxiv.org/pdf/2503.16401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16401]] Exploring the Hidden Reasoning Process of Large Language Models by Misleading Them(https://arxiv.org/abs/2503.16401)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) and Vision language models (VLMs) have been able to perform various forms of reasoning tasks in a wide range of scenarios, but are they truly engaging in task abstraction and rule-based reasoning beyond mere memorization and pattern matching? To answer this question, we propose a novel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether LLMs/VLMs perform abstract reasoning by altering their original understanding of fundamental rules. In particular, by constructing a dataset with math expressions that contradict correct operation principles, we fine-tune the model to learn those contradictory rules and assess its generalization ability on different test domains. Through a series of experiments, we find that current LLMs/VLMs are capable of effectively applying contradictory rules to solve practical math word problems and math expressions represented by images, implying the presence of an internal mechanism that abstracts before reasoning.</li>
</ul>

<h3>Title: DreamTexture: Shape from Virtual Texture with Analysis by Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Ananta R. Bhattarai, Xingzhe He, Alla Sheffer, Helge Rhodin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16412">https://arxiv.org/abs/2503.16412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16412">https://arxiv.org/pdf/2503.16412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16412]] DreamTexture: Shape from Virtual Texture with Analysis by Augmentation(https://arxiv.org/abs/2503.16412)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>DreamFusion established a new paradigm for unsupervised 3D reconstruction from virtual views by combining advances in generative models and differentiable rendering. However, the underlying multi-view rendering, along with supervision from large-scale generative models, is computationally expensive and under-constrained. We propose DreamTexture, a novel Shape-from-Virtual-Texture approach that leverages monocular depth cues to reconstruct 3D objects. Our method textures an input image by aligning a virtual texture with the real depth cues in the input, exploiting the inherent understanding of monocular geometry encoded in modern diffusion models. We then reconstruct depth from the virtual texture deformation with a new conformal map optimization, which alleviates memory-intensive volumetric representations. Our experiments reveal that generative models possess an understanding of monocular shape cues, which can be extracted by augmenting and aligning texture cues -- a novel monocular reconstruction paradigm that we call Analysis by Augmentation.</li>
</ul>

<h3>Title: InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity</h3>
<ul>
<li><strong>Authors: </strong>Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Hao Kang, Xin Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16418">https://arxiv.org/abs/2503.16418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16418">https://arxiv.org/pdf/2503.16418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16418]] InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity(https://arxiv.org/abs/2503.16418)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Achieving flexible and high-fidelity identity-preserved image generation remains formidable, particularly with advanced Diffusion Transformers (DiTs) like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust frameworks leveraging DiTs for this task. InfU addresses significant issues of existing methods, such as insufficient identity similarity, poor text-image alignment, and low generation quality and aesthetics. Central to InfU is InfuseNet, a component that injects identity features into the DiT base model via residual connections, enhancing identity similarity while maintaining generation capabilities. A multi-stage training strategy, including pretraining and supervised fine-tuning (SFT) with synthetic single-person-multiple-sample (SPMS) data, further improves text-image alignment, ameliorates image quality, and alleviates face copy-pasting. Extensive experiments demonstrate that InfU achieves state-of-the-art performance, surpassing existing baselines. In addition, the plug-and-play design of InfU ensures compatibility with various existing methods, offering a valuable contribution to the broader community.</li>
</ul>

<h3>Title: Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen (Henry)Zhong, Hanjie Chen, Xia Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16419">https://arxiv.org/abs/2503.16419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16419">https://arxiv.org/pdf/2503.16419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16419]] Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models(https://arxiv.org/abs/2503.16419)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the "overthinking phenomenon". In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: (1) model-based efficient reasoning, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; (2) reasoning output-based efficient reasoning, which aims to dynamically reduce reasoning steps and length during inference; (3) input prompts-based efficient reasoning, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking.</li>
</ul>

<h3>Title: SynCity: Training-Free Generation of 3D Worlds</h3>
<ul>
<li><strong>Authors: </strong>Paul Engstler, Aleksandar Shtedritski, Iro Laina, Christian Rupprecht, Andrea Vedaldi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16420">https://arxiv.org/abs/2503.16420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16420">https://arxiv.org/pdf/2503.16420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16420]] SynCity: Training-Free Generation of 3D Worlds(https://arxiv.org/abs/2503.16420)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We address the challenge of generating 3D worlds from textual descriptions. We propose SynCity, a training- and optimization-free approach, which leverages the geometric precision of pre-trained 3D generative models and the artistic versatility of 2D image generators to create large, high-quality 3D spaces. While most 3D generative models are object-centric and cannot generate large-scale worlds, we show how 3D and 2D generators can be combined to generate ever-expanding scenes. Through a tile-based approach, we allow fine-grained control over the layout and the appearance of scenes. The world is generated tile-by-tile, and each new tile is generated within its world-context and then fused with the scene. SynCity generates compelling and immersive scenes that are rich in detail and diversity.</li>
</ul>

<h3>Title: MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance</h3>
<ul>
<li><strong>Authors: </strong>Quanhao Li, Zhen Xing, Rui Wang, Hui Zhang, Qi Dai, Zuxuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16421">https://arxiv.org/abs/2503.16421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16421">https://arxiv.org/pdf/2503.16421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16421]] MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance(https://arxiv.org/abs/2503.16421)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in video generation have led to remarkable improvements in visual quality and temporal coherence. Upon this, trajectory-controllable video generation has emerged to enable precise object motion control through explicitly defined spatial paths. However, existing methods struggle with complex object movements and multi-object motion control, resulting in imprecise trajectory adherence, poor object consistency, and compromised visual quality. Furthermore, these methods only support trajectory control in a single format, limiting their applicability in diverse scenarios. Additionally, there is no publicly available dataset or benchmark specifically tailored for trajectory-controllable video generation, hindering robust training and systematic evaluation. To address these challenges, we introduce MagicMotion, a novel image-to-video generation framework that enables trajectory control through three levels of conditions from dense to sparse: masks, bounding boxes, and sparse boxes. Given an input image and trajectories, MagicMotion seamlessly animates objects along defined trajectories while maintaining object consistency and visual quality. Furthermore, we present MagicData, a large-scale trajectory-controlled video dataset, along with an automated pipeline for annotation and filtering. We also introduce MagicBench, a comprehensive benchmark that assesses both video quality and trajectory control accuracy across different numbers of objects. Extensive experiments demonstrate that MagicMotion outperforms previous methods across various metrics. Our project page are publicly available at this https URL.</li>
</ul>

<h3>Title: Tokenize Image as a Set</h3>
<ul>
<li><strong>Authors: </strong>Zigang Geng, Mengde Xu, Han Hu, Shuyang Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16425">https://arxiv.org/abs/2503.16425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16425">https://arxiv.org/pdf/2503.16425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16425]] Tokenize Image as a Set(https://arxiv.org/abs/2503.16425)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>This paper proposes a fundamentally new paradigm for image generation through set-based tokenization and distribution modeling. Unlike conventional methods that serialize images into fixed-position latent codes with a uniform compression ratio, we introduce an unordered token set representation to dynamically allocate coding capacity based on regional semantic complexity. This TokenSet enhances global context aggregation and improves robustness against local perturbations. To address the critical challenge of modeling discrete sets, we devise a dual transformation mechanism that bijectively converts sets into fixed-length integer sequences with summation constraints. Further, we propose Fixed-Sum Discrete Diffusion--the first framework to simultaneously handle discrete values, fixed sequence length, and summation invariance--enabling effective set distribution modeling. Experiments demonstrate our method's superiority in semantic-aware representation and generation quality. Our innovations, spanning novel representation and modeling strategies, advance visual generation beyond traditional sequential token paradigms. Our code and models are publicly available at this https URL.</li>
</ul>

<h3>Title: DynamicVis: An Efficient and General Visual Foundation Model for Remote Sensing Image Understanding</h3>
<ul>
<li><strong>Authors: </strong>Keyan Chen, Chenyang Liu, Bowen Chen, Wenyuan Li, Zhengxia Zou, Zhenwei Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16426">https://arxiv.org/abs/2503.16426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16426">https://arxiv.org/pdf/2503.16426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16426]] DynamicVis: An Efficient and General Visual Foundation Model for Remote Sensing Image Understanding(https://arxiv.org/abs/2503.16426)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The advancement of remote sensing technology has improved the spatial resolution of satellite imagery, facilitating more detailed visual representations for diverse interpretations. However, existing methods exhibit limited generalization capabilities across varied applications. While some contemporary foundation models demonstrate potential, they are hindered by insufficient cross-task adaptability and primarily process low-resolution imagery of restricted sizes, thus failing to fully exploit high-resolution data or leverage comprehensive large-scene semantics. Crucially, remote sensing imagery differs fundamentally from natural images, as key foreground targets (eg., maritime objects, artificial structures) often occupy minimal spatial proportions (~1%) and exhibit sparse distributions. Efficiently modeling cross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a significant challenge yet remains critical for remote sensing image understanding. Motivated by the selective attention mechanisms inherent to the human visual system, we propose DynamicVis, a dynamic visual perception foundation model for remote sensing imagery. The framework integrates a novel dynamic region perception backbone based on the selective state space model, which strategically balances localized detail extraction with global contextual integration, enabling computationally efficient encoding of large-scale data while maintaining architectural scalability. To enhance cross-task knowledge transferring, we introduce a multi-instance learning paradigm utilizing meta-embedding representations, trained on million-scale region-level annotations. Evaluations across nine downstream tasks demonstrate the model's versatility. DynamicVis achieves multi-level feature modeling with exceptional efficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and 833 MB GPU memory (3% of ViT's).</li>
</ul>

<h3>Title: XAttention: Block Sparse Attention with Antidiagonal Scoring</h3>
<ul>
<li><strong>Authors: </strong>Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, Song Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16428">https://arxiv.org/abs/2503.16428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16428">https://arxiv.org/pdf/2503.16428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16428]] XAttention: Block Sparse Attention with Antidiagonal Scoring(https://arxiv.org/abs/2503.16428)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements. In this paper, we introduce XAttention, a plug-and-play framework that dramatically accelerates long-context inference in Transformers models using sparse attention. XAttention's key innovation is the insight that the sum of antidiagonal values (i.e., from the lower-left to upper-right) in the attention matrix provides a powerful proxy for block importance. This allows for precise identification and pruning of non-essential blocks, resulting in high sparsity and dramatically accelerated inference. Across comprehensive evaluations on demanding long-context benchmarks-including RULER and LongBench for language, VideoMME for video understanding, and VBench for video generation. XAttention achieves accuracy comparable to full attention while delivering substantial computational gains. We demonstrate up to 13.5x acceleration in attention computation. These results underscore XAttention's ability to unlock the practical potential of block sparse attention, paving the way for scalable and efficient deployment of LCTMs in real-world applications. Code is available at this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
