<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Face Anti-Spoofing from the Perspective of Data Sampling. (arXiv:2208.13164v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13164">http://arxiv.org/abs/2208.13164</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13164] Face Anti-Spoofing from the Perspective of Data Sampling](http://arxiv.org/abs/2208.13164)</code></li>
<li>Summary: <p>Without deploying face anti-spoofing countermeasures, face recognition
systems can be spoofed by presenting a printed photo, a video, or a silicon
mask of a genuine user. Thus, face presentation attack detection (PAD) plays a
vital role in providing secure facial access to digital devices. Most existing
video-based PAD countermeasures lack the ability to cope with long-range
temporal variations in videos. Moreover, the key-frame sampling prior to the
feature extraction step has not been widely studied in the face anti-spoofing
domain. To mitigate these issues, this paper provides a data sampling approach
by proposing a video processing scheme that models the long-range temporal
variations based on Gaussian Weighting Function. Specifically, the proposed
scheme encodes the consecutive t frames of video sequences into a single RGB
image based on a Gaussian-weighted summation of the t frames. Using simply the
data sampling scheme alone, we demonstrate that state-of-the-art performance
can be achieved without any bells and whistles in both intra-database and
inter-database testing scenarios for the three public benchmark datasets;
namely, Replay-Attack, MSU-MFSD, and CASIA-FASD. In particular, the proposed
scheme provides a much lower error (from 15.2% to 6.7% on CASIA-FASD and 5.9%
to 4.9% on Replay-Attack) compared to baselines in cross-database scenarios.
</p></li>
</ul>

<h3>Title: Categorical composable cryptography: extended version. (arXiv:2208.13232v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13232">http://arxiv.org/abs/2208.13232</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13232] Categorical composable cryptography: extended version](http://arxiv.org/abs/2208.13232)</code></li>
<li>Summary: <p>We formalize the simulation paradigm of cryptography in terms of category
theory and show that protocols secure against abstract attacks form a symmetric
monoidal category, thus giving an abstract model of composable security
definitions in cryptography. Our model is able to incorporate computational
security, set-up assumptions and various attack models such as colluding or
independently acting subsets of adversaries in a modular, flexible fashion. W
We conclude by using string diagrams to rederive the security of the one-time
pad and no-go results concerning the limits of bipartite and tripartite
cryptography, ruling out e.g., composable commitments and broadcasting. On the
way, we exhibit two categorical constructions of resource theories that might
be of independent interest: one capturing resources shared among multiple
parties and one capturing resource conversions that succeed asymptotically.
</p></li>
</ul>

<h3>Title: DP-PSI: Private and Secure Set Intersection. (arXiv:2208.13249v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13249">http://arxiv.org/abs/2208.13249</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13249] DP-PSI: Private and Secure Set Intersection](http://arxiv.org/abs/2208.13249)</code></li>
<li>Summary: <p>One way to classify private set intersection (PSI) for secure 2-party
computation is whether the intersection is (a) revealed to both parties or (b)
hidden from both parties while only the computing function of the matched
payload is exposed. Both aim to provide cryptographic security while avoiding
exposing the unmatched elements of the other. They may, however, be
insufficient to achieve security and privacy in one practical scenario: when
the intersection is required and the information leaked through the function's
output must be considered for legal, ethical, and competitive reasons. Two
parties, such as the advertiser and the ads supplier, hold sets of users for
PSI computation, for example, to reveal common users to the ads supplier in
joint marketing applications. In addition to the security guarantees required
by standard PSIs to secure unmatched elements, neither party is allowed to
"single out" whether an element/user belongs to the other party or not, even
though common users are required for joint advertising. This is a fascinating
problem for which none of the PSI techniques have provided a solution. In light
of this shortcoming, we compose differential privacy (DP) and S2PC to provide
the best of both worlds and propose differentially-private PSI (DP-PSI), a new
privacy model that shares PSI's strong security protection while adhering to
the GDPR's recent formalization of the notion of excluding "signaling out"
attacks by each party except with very low probability.
</p></li>
</ul>

<h3>Title: Incrementality Bidding and Attribution. (arXiv:2208.12809v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12809">http://arxiv.org/abs/2208.12809</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12809] Incrementality Bidding and Attribution](http://arxiv.org/abs/2208.12809)</code></li>
<li>Summary: <p>The causal effect of showing an ad to a potential customer versus not,
commonly referred to as "incrementality", is the fundamental question of
advertising effectiveness. In digital advertising three major puzzle pieces are
central to rigorously quantifying advertising incrementality: ad
buying/bidding/pricing, attribution, and experimentation. Building on the
foundations of machine learning and causal econometrics, we propose a
methodology that unifies these three concepts into a computationally viable
model of both bidding and attribution which spans the randomization, training,
cross validation, scoring, and conversion attribution of advertising's causal
effects. Implementation of this approach is likely to secure a significant
improvement in the return on investment of advertising.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Factors Influencing the Organizational Decision to Outsource IT Security: A Review and Research Agenda. (arXiv:2208.12875v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12875">http://arxiv.org/abs/2208.12875</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12875] Factors Influencing the Organizational Decision to Outsource IT Security: A Review and Research Agenda](http://arxiv.org/abs/2208.12875)</code></li>
<li>Summary: <p>IT security outsourcing is the process of contracting a third-party security
service provider to perform, the full or partial IT security functions of an
organization. Little is known about the factors influencing organizational
decisions in outsourcing such a critical function. Our review of the research
and practice literature identified several managerial factors and legal
factors. We found research in IT security outsourcing to be immature and the
focus areas not addressing the critical issues facing industry practice. We
therefore present a research agenda consisting of fifteen questions to address
five key gaps relating to knowledge of IT security outsourcing, specifically
effectiveness of the outcome, lived experience of the practice, the temporal
dimension, multi-stakeholder perspectives, and the impact on IT security
practices, particularly agility in incident response.
</p></li>
</ul>

<h3>Title: Fat Pointers for Temporal Memory Safety of C. (arXiv:2208.12900v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12900">http://arxiv.org/abs/2208.12900</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12900] Fat Pointers for Temporal Memory Safety of C](http://arxiv.org/abs/2208.12900)</code></li>
<li>Summary: <p>Temporal memory safety bugs, especially use-after-free and double free bugs,
pose a major security threat to C programs. Real-world exploits utilizing these
bugs enable attackers to read and write arbitrary memory locations, causing
disastrous violations of confidentiality, integrity, and availability. Many
previous solutions retrofit temporal memory safety to C, but they all either
incur high performance overhead and/or miss detecting certain types of temporal
memory safety bugs.
</p></li>
</ul>

<p>In this paper, we propose a temporal memory safety solution that is both
efficient and comprehensive. Specifically, we extend Checked C, a
spatially-safe extension to C, with temporally-safe pointers. These are
implemented by combining two techniques: fat pointers and dynamic key-lock
checks. We show that the fat-pointer solution significantly improves running
time and memory overhead compared to the disjoint-metadata approach that
provides the same level of protection. With empirical program data and hands-on
experience porting real-world applications, we also show that our solution is
practical in terms of backward compatibility -- one of the major complaints
about fat pointers.
</p>

<h3>Title: An Automated Analyzer for Financial Security of Ethereum Smart Contracts. (arXiv:2208.12960v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12960">http://arxiv.org/abs/2208.12960</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12960] An Automated Analyzer for Financial Security of Ethereum Smart Contracts](http://arxiv.org/abs/2208.12960)</code></li>
<li>Summary: <p>At present, millions of Ethereum smart contracts are created per year and
become attractive targets for financially motivated attackers. However,
existing analyzers are not sufficient to analyze the financial security of
massive contracts precisely. In this paper, we propose and implement FASVERIF,
an automated analyzer for fine-grained analysis of smart contracts' financial
security. On the one hand, FASVERIF automatically generates models to be
verified against security properties of smart contracts. On the other hand, our
analyzer automatically generates the security properties, which is different
from existing approaches of formal verifications. Specifically, we propose two
types of security properties, invariant properties and equivalence properties,
which can be used to detect various types of finance-related vulnerabilities
and can be automatically generated based on our statistical analysis. As a
result, FASVERIF can automatically process source code of smart contracts, and
uses formal methods whenever possible to simultaneously maximize its accuracy.
We also prove the soundness of verifying our properties using our translated
model based on a custom semantics of Solidity.
</p></li>
</ul>

<p>We evaluate FASVERIF on a vulnerabilities dataset of 548 contracts by
comparing it with other automatic tools. Our evaluation shows that FASVERIF
greatly outperforms the representative tools using different technologies, with
respect to accuracy and coverage of types of vulnerabilities.
</p>

<h3>Title: SoK: Decentralized Finance (DeFi) Incidents. (arXiv:2208.13035v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13035">http://arxiv.org/abs/2208.13035</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13035] SoK: Decentralized Finance (DeFi) Incidents](http://arxiv.org/abs/2208.13035)</code></li>
<li>Summary: <p>Within just four years, the blockchain-based Decentralized Finance (DeFi)
ecosystem has accumulated a peak total value locked (TVL) of more than 253
billion USD. This surge in DeFi's popularity has, unfortunately, been
accompanied by many impactful incidents. According to our data, users,
liquidity providers, speculators, and protocol operators suffered a total loss
of at least 3.24 USD from Apr 30, 2018 to Apr 30, 2022. Given the blockchain's
transparency and increasing incident frequency, two questions arise: How can we
systematically measure, evaluate, and compare DeFi incidents? How can we learn
from past attacks to strengthen DeFi security?
</p></li>
</ul>

<p>In this paper, we introduce a common reference frame to systematically
evaluate and compare DeFi incidents. We investigate 77 academic papers, 30
audit reports, and 181 real-world incidents. Our open data reveals several gaps
between academia and the practitioners' community. For example, few academic
papers address "price oracle attacks" and "permissonless interactions", while
our data suggests that they are the two most frequent incident types (15% and
10.5% correspondingly). We also investigate potential defenses, and find that:
(i) 103 (56%) of the attacks are not executed atomically, granting a rescue
time frame for defenders; (ii) SoTA bytecode similarity analysis can at least
detect 31 vulnerable/23 adversarial contracts; and (iii) 33 (15.3%) of the
adversaries leak potentially identifiable information by interacting with
centralized exchanges.
</p>

<h3>Title: Information Security Management in High Quality IS Journals: A Review and Research Agenda. (arXiv:2208.13087v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13087">http://arxiv.org/abs/2208.13087</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13087] Information Security Management in High Quality IS Journals: A Review and Research Agenda](http://arxiv.org/abs/2208.13087)</code></li>
<li>Summary: <p>In the digital age, the protection of information resources is critical to
the viability of organizations. Information Security Management (ISM) is a
protective function that preserves the confidentiality, integrity and
availability of information resources in organizations operating in a complex
and evolving security threat landscape. This paper analyses ISM research
themes, methods, and theories in high quality IS journals over a period of 30
years (up to the end of 2017). Although our review found that less than 1
percent of papers to be in the area of ISM, there has been a dramatic increase
in the number of ISM publications as well as new emerging themes in the past
decade. Further, past trends towards subjective-argumentative papers have
reversed in favour of empirically validated research. Our analysis of research
methods and approaches found ISM studies to be dominated by one-time surveys
rather than case studies and action research. The findings suggest that
although ISM research has improved its empirical backing over the years, it
remains relatively disengaged from organisational practice.
</p></li>
</ul>

<h3>Title: IoT Droplocks: Wireless Fingerprint Theft Using Hacked Smart Locks. (arXiv:2208.13343v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13343">http://arxiv.org/abs/2208.13343</a></li>
<li>Code URL: <a href="https://gitlab.com/iot-droplocks-public/nrf-sdk">https://gitlab.com/iot-droplocks-public/nrf-sdk</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13343] IoT Droplocks: Wireless Fingerprint Theft Using Hacked Smart Locks](http://arxiv.org/abs/2208.13343)</code></li>
<li>Summary: <p>Electronic locks can provide security- and convenience-enhancing features,
with fingerprint readers an increasingly common feature in these products. When
equipped with a wireless radio, they become a smart lock and join the billions
of IoT devices proliferating our world. However, such capabilities can also be
used to transform smart locks into fingerprint harvesters that compromise an
individual's security without their knowledge. We have named this the droplock
attack. This paper demonstrates how the harvesting technique works, shows that
off-the-shelf smart locks can be invisibly modified to perform such attacks,
discusses the implications for smart device design and usage, and calls for
better manufacturer and public treatment of this issue.
</p></li>
</ul>

<h3>Title: Lateral Movement Detection Using User Behavioral Analysis. (arXiv:2208.13524v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13524">http://arxiv.org/abs/2208.13524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13524] Lateral Movement Detection Using User Behavioral Analysis](http://arxiv.org/abs/2208.13524)</code></li>
<li>Summary: <p>Lateral Movement refers to methods by which threat actors gain initial access
to a network and then progressively move through said network collecting key
data about assets until they reach the ultimate target of their attack. Lateral
Movement intrusions have become more intricate with the increasing complexity
and interconnected nature of enterprise networks, and require equally
sophisticated detection mechanisms to proactively detect such threats in near
real-time at enterprise scale. In this paper, the authors propose a novel,
lightweight method for Lateral Movement detection using user behavioral
analysis and machine learning. Specifically, this paper introduces a novel
methodology for cyber domain-specific feature engineering that identifies
Lateral Movement behavior on a per-user basis. Furthermore, the engineered
features have also been used to develop two supervised machine learning models
for Lateral Movement identification that have demonstrably outperformed models
previously seen in literature while maintaining robust performance on datasets
with high class imbalance. The models and methodology introduced in this paper
have also been designed in collaboration with security operators to be relevant
and interpretable in order to maximize impact and minimize time to value as a
cyber threat detection toolkit. The underlying goal of the paper is to provide
a computationally efficient, domain-specific approach to near real-time Lateral
Movement detection that is interpretable and robust to enterprise-scale data
volumes and class imbalance.
</p></li>
</ul>

<h3>Title: MSWasm: Soundly Enforcing Memory-Safe Execution of Unsafe Code. (arXiv:2208.13583v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13583">http://arxiv.org/abs/2208.13583</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13583] MSWasm: Soundly Enforcing Memory-Safe Execution of Unsafe Code](http://arxiv.org/abs/2208.13583)</code></li>
<li>Summary: <p>Most programs compiled to WebAssembly (Wasm) today are written in unsafe
languages like C and C++. Unfortunately, memory-unsafe C code remains unsafe
when compiled to Wasm -- and attackers can exploit buffer overflows and
use-after-frees in Wasm almost as easily as they can on native platforms.
Memory-Safe WebAssembly (MSWasm) proposes to extend Wasm with language-level
memory-safety abstractions to precisely address this problem. In this paper, we
build on the original MSWasm position paper to realize this vision. We give a
precise and formal semantics of MSWasm, and prove that well-typed MSWasm
programs are, by construction, robustly memory safe. To this end, we develop a
novel, language-independent memory-safety property based on colored memory
locations and pointers. This property also lets us reason about the security
guarantees of a formal C-to-MSWasm compiler -- and prove that it always
produces memory-safe programs (and preserves the semantics of safe programs).
We use these formal results to then guide several implementations: Two
compilers of MSWasm to native code, and a C-to-MSWasm compiler (that extends
Clang). Our MSWasm compilers support different enforcement mechanisms, allowing
developers to make security-performance trade-offs according to their needs.
Our evaluation shows that the overhead of enforcing memory safety in software
ranges from 22% (enforcing spatial safety alone) to 198% (enforcing full memory
safety) on the PolyBenchC suite. More importantly, MSWasm's design makes it
easy to swap between enforcement mechanisms; as fast (especially
hardware-based) enforcement techniques become available, MSWasm will be able to
take advantage of these advances almost for free.
</p></li>
</ul>

<h3>Title: Spatio-Temporal Wind Speed Forecasting using Graph Networks and Novel Transformer Architectures. (arXiv:2208.13585v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13585">http://arxiv.org/abs/2208.13585</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13585] Spatio-Temporal Wind Speed Forecasting using Graph Networks and Novel Transformer Architectures](http://arxiv.org/abs/2208.13585)</code></li>
<li>Summary: <p>To improve the security and reliability of wind energy production, short-term
forecasting has become of utmost importance. This study focuses on multi-step
spatio-temporal wind speed forecasting for the Norwegian continental shelf. A
graph neural network (GNN) architecture was used to extract spatial
dependencies, with different update functions to learn temporal correlations.
These update functions were implemented using different neural network
architectures. One such architecture, the Transformer, has become increasingly
popular for sequence modelling in recent years. Various alterations of the
original architecture have been proposed to better facilitate time-series
forecasting, of which this study focused on the Informer, LogSparse Transformer
and Autoformer. This is the first time the LogSparse Transformer and Autoformer
have been applied to wind forecasting and the first time any of these or the
Informer have been formulated in a spatio-temporal setting for wind
forecasting. By comparing against spatio-temporal Long Short-Term Memory (LSTM)
and Multi-Layer Perceptron (MLP) models, the study showed that the models using
the altered Transformer architectures as update functions in GNNs were able to
outperform these. Furthermore, we propose the Fast Fourier Transformer
(FFTransformer), which is a novel Transformer architecture based on signal
decomposition and consists of two separate streams that analyse trend and
periodic components separately. The FFTransformer and Autoformer were found to
achieve superior results for the 10-minute and 1-hour ahead forecasts, with the
FFTransformer significantly outperforming all other models for the 4-hour ahead
forecasts. Finally, by varying the degree of connectivity for the graph
representations, the study explicitly demonstrates how all models were able to
leverage spatial dependencies to improve local short-term wind speed
forecasting.
</p></li>
</ul>

<h3>Title: Virtual Control Group: Measuring Hidden Performance Metrics. (arXiv:2208.12941v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12941">http://arxiv.org/abs/2208.12941</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12941] Virtual Control Group: Measuring Hidden Performance Metrics](http://arxiv.org/abs/2208.12941)</code></li>
<li>Summary: <p>Performance metrics measuring in Financial Integrity systems are crucial for
maintaining an efficient and cost effective operation. An important performance
metric is False Positive Rate. This metric cannot be directly monitored since
we don't know for sure if a user is bad once blocked. We present a statistical
method based on survey theory and causal inference methods to estimate the
false positive rate of the system or a single blocking policy. We also suggest
a new approach of outcome matching that in some cases including empirical data
outperformed other commonly used methods. The approaches described in this
paper can be applied in other Integrity domains such as Cyber Security.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: On GANs perpetuating biases for face verification. (arXiv:2208.13061v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13061">http://arxiv.org/abs/2208.13061</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13061] On GANs perpetuating biases for face verification](http://arxiv.org/abs/2208.13061)</code></li>
<li>Summary: <p>DeepLearningsystemsneedlargedatafortraining.Datasets for training face
verification systems are difficult to obtain and prone to privacy issues.
Synthetic data generated by generative models such as GANs can be a good
alternative. However, we show that data generated from GANs are prone to bias
and fairness issues. Specifically GANs trained on FFHQ dataset show bias
towards generating white faces in the age group of 20-29. We also demonstrate
that synthetic faces cause disparate impact, specifically for race attribute,
when used for fine tuning face verification systems. This is measured using
$DoB_{fv}$ metric, which is defined as standard deviation of GAR@FAR for face
verification.
</p></li>
</ul>

<h3>Title: NL2GDPR: Automatically Develop GDPR Compliant Android Application Features from Natural Language. (arXiv:2208.13361v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13361">http://arxiv.org/abs/2208.13361</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13361] NL2GDPR: Automatically Develop GDPR Compliant Android Application Features from Natural Language](http://arxiv.org/abs/2208.13361)</code></li>
<li>Summary: <p>The recent privacy leakage incidences and the more strict policy regulations
demand a much higher standard of compliance for companies and mobile apps.
However, such obligations also impose significant challenges on app developers
for complying with these regulations that contain various perspectives,
activities, and roles, especially for small companies and developers who are
less experienced in this matter or with limited resources. To address these
hurdles, we develop an automatic tool, NL2GDPR, which can generate policies
from natural language descriptions from the developer while also ensuring the
app's functionalities are compliant with General Data Protection Regulation
(GDPR). NL2GDPR is developed by leveraging an information extraction tool, OIA
(Open Information Annotation), developed by Baidu Cognitive Computing Lab.
</p></li>
</ul>

<p>At the core, NL2GDPR is a privacy-centric information extraction model,
appended with a GDPR policy finder and a policy generator. We perform a
comprehensive study to grasp the challenges in extracting privacy-centric
information and generating privacy policies, while exploiting optimizations for
this specific task. With NL2GDPR, we can achieve 92.9%, 95.2%, and 98.4%
accuracy in correctly identifying GDPR policies related to personal data
storage, process, and share types, respectively. To the best of our knowledge,
NL2GDPR is the first tool that allows a developer to automatically generate
GDPR compliant policies, with only the need of entering the natural language
for describing the app features. Note that other non-GDPR-related features
might be integrated with the generated features to build a complex app.
</p>

<h3>Title: RL-DistPrivacy: Privacy-Aware Distributed Deep Inference for low latency IoT systems. (arXiv:2208.13032v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13032">http://arxiv.org/abs/2208.13032</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13032] RL-DistPrivacy: Privacy-Aware Distributed Deep Inference for low latency IoT systems](http://arxiv.org/abs/2208.13032)</code></li>
<li>Summary: <p>Although Deep Neural Networks (DNN) have become the backbone technology of
several ubiquitous applications, their deployment in resource-constrained
machines, e.g., Internet of Things (IoT) devices, is still challenging. To
satisfy the resource requirements of such a paradigm, collaborative deep
inference with IoT synergy was introduced. However, the distribution of DNN
networks suffers from severe data leakage. Various threats have been presented,
including black-box attacks, where malicious participants can recover arbitrary
inputs fed into their devices. Although many countermeasures were designed to
achieve privacy-preserving DNN, most of them result in additional computation
and lower accuracy. In this paper, we present an approach that targets the
security of collaborative deep inference via re-thinking the distribution
strategy, without sacrificing the model performance. Particularly, we examine
different DNN partitions that make the model susceptible to black-box threats
and we derive the amount of data that should be allocated per device to hide
proprieties of the original input. We formulate this methodology, as an
optimization, where we establish a trade-off between the latency of
co-inference and the privacy-level of data. Next, to relax the optimal
solution, we shape our approach as a Reinforcement Learning (RL) design that
supports heterogeneous devices as well as multiple DNNs/datasets.
</p></li>
</ul>

<h3>Title: FedEgo: Privacy-preserving Personalized Federated Graph Learning with Ego-graphs. (arXiv:2208.13685v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13685">http://arxiv.org/abs/2208.13685</a></li>
<li>Code URL: <a href="https://github.com/fedego/fedego">https://github.com/fedego/fedego</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13685] FedEgo: Privacy-preserving Personalized Federated Graph Learning with Ego-graphs](http://arxiv.org/abs/2208.13685)</code></li>
<li>Summary: <p>As special information carriers containing both structure and feature
information, graphs are widely used in graph mining, e.g., Graph Neural
Networks (GNNs). However, in some practical scenarios, graph data are stored
separately in multiple distributed parties, which may not be directly shared
due to conflicts of interest. Hence, federated graph neural networks are
proposed to address such data silo problems while preserving the privacy of
each party (or client). Nevertheless, different graph data distributions among
various parties, which is known as the statistical heterogeneity, may degrade
the performance of naive federated learning algorithms like FedAvg. In this
paper, we propose FedEgo, a federated graph learning framework based on
ego-graphs to tackle the challenges above, where each client will train their
local models while also contributing to the training of a global model. FedEgo
applies GraphSAGE over ego-graphs to make full use of the structure information
and utilizes Mixup for privacy concerns. To deal with the statistical
heterogeneity, we integrate personalization into learning and propose an
adaptive mixing coefficient strategy that enables clients to achieve their
optimal personalization. Extensive experimental results and in-depth analysis
demonstrate the effectiveness of FedEgo.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: An Access Control Method with Secret Key for Semantic Segmentation Models. (arXiv:2208.13135v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13135">http://arxiv.org/abs/2208.13135</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13135] An Access Control Method with Secret Key for Semantic Segmentation Models](http://arxiv.org/abs/2208.13135)</code></li>
<li>Summary: <p>A novel method for access control with a secret key is proposed to protect
models from unauthorized access in this paper. We focus on semantic
segmentation models with the vision transformer (ViT), called segmentation
transformer (SETR). Most existing access control methods focus on image
classification tasks, or they are limited to CNNs. By using a patch embedding
structure that ViT has, trained models and test images can be efficiently
encrypted with a secret key, and then semantic segmentation tasks are carried
out in the encrypted domain. In an experiment, the method is confirmed to
provide the same accuracy as that of using plain images without any encryption
to authorized users with a correct key and also to provide an extremely
degraded accuracy to unauthorized users.
</p></li>
</ul>

<h3>Title: A Note on Copy-Protection from Random Oracles. (arXiv:2208.12884v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12884">http://arxiv.org/abs/2208.12884</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12884] A Note on Copy-Protection from Random Oracles](http://arxiv.org/abs/2208.12884)</code></li>
<li>Summary: <p>Quantum copy-protection, introduced by Aaronson (CCC'09), uses the no-cloning
principle of quantum mechanics to protect software from being illegally
distributed. Constructing copy-protection has been an important problem in
quantum cryptography. Since copy-protection is shown to be impossible to
achieve in the plain model, we investigate the question of constructing
copy-protection for arbitrary classes of unlearnable functions in the random
oracle model. We present an impossibility result that rules out a class of
copy-protection schemes in the random oracle model assuming the existence of
quantum fully homomorphic encryption and quantum hardness of learning with
errors. En route, we prove the impossibility of approximately correct
copy-protection in the plain model.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Anti-Retroactive Interference for Lifelong Learning. (arXiv:2208.12967v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12967">http://arxiv.org/abs/2208.12967</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12967] Anti-Retroactive Interference for Lifelong Learning](http://arxiv.org/abs/2208.12967)</code></li>
<li>Summary: <p>Humans can continuously learn new knowledge. However, machine learning models
suffer from drastic dropping in performance on previous tasks after learning
new tasks. Cognitive science points out that the competition of similar
knowledge is an important cause of forgetting. In this paper, we design a
paradigm for lifelong learning based on meta-learning and associative mechanism
of the brain. It tackles the problem from two aspects: extracting knowledge and
memorizing knowledge. First, we disrupt the sample's background distribution
through a background attack, which strengthens the model to extract the key
features of each task. Second, according to the similarity between incremental
knowledge and base knowledge, we design an adaptive fusion of incremental
knowledge, which helps the model allocate capacity to the knowledge of
different difficulties. It is theoretically analyzed that the proposed learning
paradigm can make the models of different tasks converge to the same optimum.
The proposed method is validated on the MNIST, CIFAR100, CUB200 and ImageNet100
datasets.
</p></li>
</ul>

<h3>Title: Self-Supervised Face Presentation Attack Detection with Dynamic Grayscale Snippets. (arXiv:2208.13070v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13070">http://arxiv.org/abs/2208.13070</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13070] Self-Supervised Face Presentation Attack Detection with Dynamic Grayscale Snippets](http://arxiv.org/abs/2208.13070)</code></li>
<li>Summary: <p>Face presentation attack detection (PAD) plays an important role in defending
face recognition systems against presentation attacks. The success of PAD
largely relies on supervised learning that requires a huge number of labeled
data, which is especially challenging for videos and often requires expert
knowledge. To avoid the costly collection of labeled data, this paper presents
a novel method for self-supervised video representation learning via motion
prediction. To achieve this, we exploit the temporal consistency based on three
RGB frames which are acquired at three different times in the video sequence.
The obtained frames are then transformed into grayscale images where each image
is specified to three different channels such as R(red), G(green), and B(blue)
to form a dynamic grayscale snippet (DGS). Motivated by this, the labels are
automatically generated to increase the temporal diversity based on DGS by
using the different temporal lengths of the videos, which prove to be very
helpful for the downstream task. Benefiting from the self-supervised nature of
our method, we report the results that outperform existing methods on four
public benchmark datasets, namely Replay-Attack, MSU-MFSD, CASIA-FASD, and
OULU-NPU. Explainability analysis has been carried out through LIME and
Grad-CAM techniques to visualize the most important features used in the DGS.
</p></li>
</ul>

<h3>Title: Living-off-the-Land Abuse Detection Using Natural Language Processing and Supervised Learning. (arXiv:2208.12836v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12836">http://arxiv.org/abs/2208.12836</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12836] Living-off-the-Land Abuse Detection Using Natural Language Processing and Supervised Learning](http://arxiv.org/abs/2208.12836)</code></li>
<li>Summary: <p>Living-off-the-Land is an evasion technique used by attackers where native
binaries are abused to achieve malicious intent. Since these binaries are often
legitimate system files, detecting such abuse is difficult and often missed by
modern anti-virus software. This paper proposes a novel abuse detection
algorithm using raw command strings. First, natural language processing
techniques such as regular expressions and one-hot encoding are utilized for
encoding the command strings as numerical token vectors. Next, supervised
learning techniques are employed to learn the malicious patterns in the token
vectors and ultimately predict the command's label. Finally, the model is
evaluated using statistics from the training phase and in a virtual environment
to compare its effectiveness at detecting new commands to existing anti-virus
products such as Windows Defender.
</p></li>
</ul>

<h3>Title: ATTRITION: Attacking Static Hardware Trojan Detection Techniques Using Reinforcement Learning. (arXiv:2208.12897v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12897">http://arxiv.org/abs/2208.12897</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12897] ATTRITION: Attacking Static Hardware Trojan Detection Techniques Using Reinforcement Learning](http://arxiv.org/abs/2208.12897)</code></li>
<li>Summary: <p>Stealthy hardware Trojans (HTs) inserted during the fabrication of integrated
circuits can bypass the security of critical infrastructures. Although
researchers have proposed many techniques to detect HTs, several limitations
exist, including: (i) a low success rate, (ii) high algorithmic complexity, and
(iii) a large number of test patterns. Furthermore, the most pertinent drawback
of prior detection techniques stems from an incorrect evaluation methodology,
i.e., they assume that an adversary inserts HTs randomly. Such inappropriate
adversarial assumptions enable detection techniques to claim high HT detection
accuracy, leading to a "false sense of security." Unfortunately, to the best of
our knowledge, despite more than a decade of research on detecting HTs inserted
during fabrication, there have been no concerted efforts to perform a
systematic evaluation of HT detection techniques.
</p></li>
</ul>

<p>In this paper, we play the role of a realistic adversary and question the
efficacy of HT detection techniques by developing an automated, scalable, and
practical attack framework, ATTRITION, using reinforcement learning (RL).
ATTRITION evades eight detection techniques across two HT detection categories,
showcasing its agnostic behavior. ATTRITION achieves average attack success
rates of $47\times$ and $211\times$ compared to randomly inserted HTs against
state-of-the-art HT detection techniques. We demonstrate ATTRITION's ability to
evade detection techniques by evaluating designs ranging from the widely-used
academic suites to larger designs such as the open-source MIPS and mor1kx
processors to AES and a GPS module. Additionally, we showcase the impact of
ATTRITION-generated HTs through two case studies (privilege escalation and kill
switch) on the mor1kx processor. We envision that our work, along with our
released HT benchmarks and models, fosters the development of better HT
detection techniques.
</p>

<h3>Title: TrojViT: Trojan Insertion in Vision Transformers. (arXiv:2208.13049v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13049">http://arxiv.org/abs/2208.13049</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13049] TrojViT: Trojan Insertion in Vision Transformers](http://arxiv.org/abs/2208.13049)</code></li>
<li>Summary: <p>Vision Transformers (ViTs) have demonstrated the state-of-the-art performance
in various vision-related tasks. The success of ViTs motivates adversaries to
perform backdoor attacks on ViTs. Although the vulnerability of traditional
CNNs to backdoor attacks is well-known, backdoor attacks on ViTs are
seldom-studied. Compared to CNNs capturing pixel-wise local features by
convolutions, ViTs extract global context information through patches and
attentions. Na\"ively transplanting CNN-specific backdoor attacks to ViTs
yields only a low clean data accuracy and a low attack success rate. In this
paper, we propose a stealth and practical ViT-specific backdoor attack
$TrojViT$. Rather than an area-wise trigger used by CNN-specific backdoor
attacks, TrojViT generates a patch-wise trigger designed to build a Trojan
composed of some vulnerable bits on the parameters of a ViT stored in DRAM
memory through patch salience ranking and attention-target loss. TrojViT
further uses minimum-tuned parameter update to reduce the bit number of the
Trojan. Once the attacker inserts the Trojan into the ViT model by flipping the
vulnerable bits, the ViT model still produces normal inference accuracy with
benign inputs. But when the attacker embeds a trigger into an input, the ViT
model is forced to classify the input to a predefined target class. We show
that flipping only few vulnerable bits identified by TrojViT on a ViT model
using the well-known RowHammer can transform the model into a backdoored one.
We perform extensive experiments of multiple datasets on various ViT models.
TrojViT can classify $99.64\%$ of test images to a target class by flipping
$345$ bits on a ViT for ImageNet.
</p></li>
</ul>

<h3>Title: Cross-domain Cross-architecture Black-box Attacks on Fine-tuned Models with Transferred Evolutionary Strategies. (arXiv:2208.13182v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13182">http://arxiv.org/abs/2208.13182</a></li>
<li>Code URL: <a href="https://github.com/hkust-knowcomp/tes">https://github.com/hkust-knowcomp/tes</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13182] Cross-domain Cross-architecture Black-box Attacks on Fine-tuned Models with Transferred Evolutionary Strategies](http://arxiv.org/abs/2208.13182)</code></li>
<li>Summary: <p>Fine-tuning can be vulnerable to adversarial attacks. Existing works about
black-box attacks on fine-tuned models (BAFT) are limited by strong
assumptions. To fill the gap, we propose two novel BAFT settings, cross-domain
and cross-domain cross-architecture BAFT, which only assume that (1) the target
model for attacking is a fine-tuned model, and (2) the source domain data is
known and accessible. To successfully attack fine-tuned models under both
settings, we propose to first train an adversarial generator against the source
model, which adopts an encoder-decoder architecture and maps a clean input to
an adversarial example. Then we search in the low-dimensional latent space
produced by the encoder of the adversarial generator. The search is conducted
under the guidance of the surrogate gradient obtained from the source model.
Experimental results on different domains and different network architectures
demonstrate that the proposed attack method can effectively and efficiently
attack the fine-tuned models.
</p></li>
</ul>

<h3>Title: Understanding the Limits of Poisoning Attacks in Episodic Reinforcement Learning. (arXiv:2208.13663v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13663">http://arxiv.org/abs/2208.13663</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13663] Understanding the Limits of Poisoning Attacks in Episodic Reinforcement Learning](http://arxiv.org/abs/2208.13663)</code></li>
<li>Summary: <p>To understand the security threats to reinforcement learning (RL) algorithms,
this paper studies poisoning attacks to manipulate \emph{any} order-optimal
learning algorithm towards a targeted policy in episodic RL and examines the
potential damage of two natural types of poisoning attacks, i.e., the
manipulation of \emph{reward} and \emph{action}. We discover that the effect of
attacks crucially depend on whether the rewards are bounded or unbounded. In
bounded reward settings, we show that only reward manipulation or only action
manipulation cannot guarantee a successful attack. However, by combining reward
and action manipulation, the adversary can manipulate any order-optimal
learning algorithm to follow any targeted policy with
$\tilde{\Theta}(\sqrt{T})$ total attack cost, which is order-optimal, without
any knowledge of the underlying MDP. In contrast, in unbounded reward settings,
we show that reward manipulation attacks are sufficient for an adversary to
successfully manipulate any order-optimal learning algorithm to follow any
targeted policy using $\tilde{O}(\sqrt{T})$ amount of contamination. Our
results reveal useful insights about what can or cannot be achieved by
poisoning attacks, and are set to spur more works on the design of robust RL
algorithms.
</p></li>
</ul>

<h3>Title: Demystifying Arch-hints for Model Extraction: An Attack in Unified Memory System. (arXiv:2208.13720v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13720">http://arxiv.org/abs/2208.13720</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13720] Demystifying Arch-hints for Model Extraction: An Attack in Unified Memory System](http://arxiv.org/abs/2208.13720)</code></li>
<li>Summary: <p>The deep neural network (DNN) models are deemed confidential due to their
unique value in expensive training efforts, privacy-sensitive training data,
and proprietary network characteristics. Consequently, the model value raises
incentive for adversary to steal the model for profits, such as the
representative model extraction attack. Emerging attack can leverage
timing-sensitive architecture-level events (i.e., Arch-hints) disclosed in
hardware platforms to extract DNN model layer information accurately. In this
paper, we take the first step to uncover the root cause of such Arch-hints and
summarize the principles to identify them. We then apply these principles to
emerging Unified Memory (UM) management system and identify three new
Arch-hints caused by UM's unique data movement patterns. We then develop a new
extraction attack, UMProbe. We also create the first DNN benchmark suite in UM
and utilize the benchmark suite to evaluate UMProbe. Our evaluation shows that
UMProbe can extract the layer sequence with an accuracy of 95% for almost all
victim test models, which thus calls for more attention to the DNN security in
UM system.
</p></li>
</ul>

<h3>Title: What Does the Gradient Tell When Attacking the Graph Structure. (arXiv:2208.12815v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12815">http://arxiv.org/abs/2208.12815</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12815] What Does the Gradient Tell When Attacking the Graph Structure](http://arxiv.org/abs/2208.12815)</code></li>
<li>Summary: <p>Recent studies have proven that graph neural networks are vulnerable to
adversarial attacks. Attackers can rely solely on the training labels to
disrupt the performance of the agnostic victim model by edge perturbations.
Researchers observe that the saliency-based attackers tend to add edges rather
than delete them, which is previously explained by the fact that adding edges
pollutes the nodes' features by aggregation while removing edges only leads to
some loss of information. In this paper, we further prove that the attackers
perturb graphs by adding inter-class edges, which also manifests as a reduction
in the homophily of the perturbed graph. From this point of view,
saliency-based attackers still have room for improvement in capability and
imperceptibility. The message passing of the GNN-based surrogate model leads to
the oversmoothing of nodes connected by inter-class edges, preventing attackers
from obtaining the distinctiveness of node features. To solve this issue, we
introduce a multi-hop aggregated message passing to preserve attribute
differences between nodes. In addition, we propose a regularization term to
restrict the homophily variance to enhance the attack imperceptibility.
Experiments verify that our proposed surrogate model improves the attacker's
versatility and the regularization term helps to limit the homophily of the
perturbed graph.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Semantic Clustering of a Sequence of Satellite Images. (arXiv:2208.13504v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13504">http://arxiv.org/abs/2208.13504</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13504] Semantic Clustering of a Sequence of Satellite Images](http://arxiv.org/abs/2208.13504)</code></li>
<li>Summary: <p>Satellite images constitute a highly valuable and abundant resource for many
real world applications. However, the labeled data needed to train most machine
learning models are scarce and difficult to obtain. In this context, the
current work investigates a fully unsupervised methodology that, given a
temporal sequence of satellite images, creates a partition of the ground
according to its semantic properties and their evolution over time. The
sequences of images are translated into a grid of multivariate time series of
embedded tiles. The embedding and the partitional clustering of these sequences
of tiles are constructed in two iterative steps: In the first step, the
embedding is able to extract the information of the sequences of tiles based on
a geographical neighborhood, and the tiles are grouped into clusters. In the
second step, the embedding is refined by using the neighborhood defined by the
clusters, and the final clustering of the sequences of tiles is obtained. We
illustrate the methodology by conducting the semantic clustering of a sequence
of 20 satellite images of the region of Navarra (Spain). The results show that
the clustering of multivariate time series is robust and contains trustful
spatio-temporal semantic information about the region under study. We unveil
the close connection that exists between the geographic and embedded spaces,
and find out that the semantic properties attributed to these kinds of
embeddings are fully exploited and even enhanced by the proposed clustering of
time series.
</p></li>
</ul>

<h3>Title: Towards Robust Face Recognition with Comprehensive Search. (arXiv:2208.13600v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13600">http://arxiv.org/abs/2208.13600</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13600] Towards Robust Face Recognition with Comprehensive Search](http://arxiv.org/abs/2208.13600)</code></li>
<li>Summary: <p>Data cleaning, architecture, and loss function design are important factors
contributing to high-performance face recognition. Previously, the research
community tries to improve the performance of each single aspect but failed to
present a unified solution on the joint search of the optimal designs for all
three aspects. In this paper, we for the first time identify that these aspects
are tightly coupled to each other. Optimizing the design of each aspect
actually greatly limits the performance and biases the algorithmic design.
Specifically, we find that the optimal model architecture or loss function is
closely coupled with the data cleaning. To eliminate the bias of single-aspect
research and provide an overall understanding of the face recognition model
design, we first carefully design the search space for each aspect, then a
comprehensive search method is introduced to jointly search optimal data
cleaning, architecture, and loss function design. In our framework, we make the
proposed comprehensive search as flexible as possible, by using an innovative
reinforcement learning based approach. Extensive experiments on million-level
face recognition benchmarks demonstrate the effectiveness of our newly-designed
search space for each aspect and the comprehensive search. We outperform expert
algorithms developed for each single research track by large margins. More
importantly, we analyze the difference between our searched optimal design and
the independent design of the single factors. We point out that strong models
tend to optimize with more difficult training datasets and loss functions. Our
empirical study can provide guidance in future research towards more robust
face recognition systems.
</p></li>
</ul>

<h3>Title: Open-Set Semi-Supervised Object Detection. (arXiv:2208.13722v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13722">http://arxiv.org/abs/2208.13722</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13722] Open-Set Semi-Supervised Object Detection](http://arxiv.org/abs/2208.13722)</code></li>
<li>Summary: <p>Recent developments for Semi-Supervised Object Detection (SSOD) have shown
the promise of leveraging unlabeled data to improve an object detector.
However, thus far these methods have assumed that the unlabeled data does not
contain out-of-distribution (OOD) classes, which is unrealistic with
larger-scale unlabeled datasets. In this paper, we consider a more practical
yet challenging problem, Open-Set Semi-Supervised Object Detection (OSSOD). We
first find the existing SSOD method obtains a lower performance gain in
open-set conditions, and this is caused by the semantic expansion, where the
distracting OOD objects are mispredicted as in-distribution pseudo-labels for
the semi-supervised training. To address this problem, we consider online and
offline OOD detection modules, which are integrated with SSOD methods. With the
extensive studies, we found that leveraging an offline OOD detector based on a
self-supervised vision transformer performs favorably against online OOD
detectors due to its robustness to the interference of pseudo-labeling. In the
experiment, our proposed framework effectively addresses the semantic expansion
issue and shows consistent improvements on many OSSOD benchmarks, including
large-scale COCO-OpenImages. We also verify the effectiveness of our framework
under different OSSOD conditions, including varying numbers of in-distribution
classes, different degrees of supervision, and different combinations of
unlabeled sets.
</p></li>
</ul>

<h3>Title: Adversarial Robustness for Tabular Data through Cost and Utility Awareness. (arXiv:2208.13058v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13058">http://arxiv.org/abs/2208.13058</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13058] Adversarial Robustness for Tabular Data through Cost and Utility Awareness](http://arxiv.org/abs/2208.13058)</code></li>
<li>Summary: <p>Many machine learning problems use data in the tabular domains. Adversarial
examples can be especially damaging for these applications. Yet, existing works
on adversarial robustness mainly focus on machine-learning models in the image
and text domains. We argue that due to the differences between tabular data and
images or text, existing threat models are inappropriate for tabular domains.
These models do not capture that cost can be more important than
imperceptibility, nor that the adversary could ascribe different value to the
utility obtained from deploying different adversarial examples. We show that
due to these differences the attack and defence methods used for images and
text cannot be directly applied to the tabular setup. We address these issues
by proposing new cost and utility-aware threat models tailored to the
adversarial capabilities and constraints of attackers targeting tabular
domains. We introduce a framework that enables us to design attack and defence
mechanisms which result in models protected against cost or utility-aware
adversaries, e.g., adversaries constrained by a certain dollar budget. We show
that our approach is effective on three tabular datasets corresponding to
applications for which adversarial examples can have economic and social
implications.
</p></li>
</ul>

<h3>Title: Generalization In Multi-Objective Machine Learning. (arXiv:2208.13499v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13499">http://arxiv.org/abs/2208.13499</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13499] Generalization In Multi-Objective Machine Learning](http://arxiv.org/abs/2208.13499)</code></li>
<li>Summary: <p>Modern machine learning tasks often require considering not just one but
multiple objectives. For example, besides the prediction quality, this could be
the efficiency, robustness or fairness of the learned models, or any of their
combinations. Multi-objective learning offers a natural framework for handling
such problems without having to commit to early trade-offs. Surprisingly,
statistical learning theory so far offers almost no insight into the
generalization properties of multi-objective learning. In this work, we make
first steps to fill this gap: we establish foundational generalization bounds
for the multi-objective setting as well as generalization and excess bounds for
learning with scalarizations. We also provide the first theoretical analysis of
the relation between the Pareto-optimal sets of the true objectives and the
Pareto-optimal sets of their empirical approximations from training data. In
particular, we show a surprising asymmetry: all Pareto-optimal solutions can be
approximated by empirically Pareto-optimal ones, but not vice versa.
</p></li>
</ul>

<h3>Title: Categorical semantics of compositional reinforcement learning. (arXiv:2208.13687v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13687">http://arxiv.org/abs/2208.13687</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13687] Categorical semantics of compositional reinforcement learning](http://arxiv.org/abs/2208.13687)</code></li>
<li>Summary: <p>Reinforcement learning (RL) often requires decomposing a problem into
subtasks and composing learned behaviors on these tasks. Compositionality in RL
has the potential to create modular subtask units that interface with other
system capabilities. However, generating compositional models requires the
characterization of minimal assumptions for the robustness of the compositional
feature. We develop a framework for a \emph{compositional theory} of RL using a
categorical point of view. Given the categorical representation of
compositionality, we investigate sufficient conditions under which
learning-by-parts results in the same optimal policy as learning on the whole.
In particular, our approach introduces a category $\mathsf{MDP}$, whose objects
are Markov decision processes (MDPs) acting as models of tasks. We show that
$\mathsf{MDP}$ admits natural compositional operations, such as certain fiber
products and pushouts. These operations make explicit compositional phenomena
in RL and unify existing constructions, such as puncturing hazardous states in
composite MDPs and incorporating state-action symmetry. We also model
sequential task completion by introducing the language of zig-zag diagrams that
is an immediate application of the pushout operation in $\mathsf{MDP}$.
</p></li>
</ul>

<h3>Title: Overparameterized (robust) models from computational constraints. (arXiv:2208.12926v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12926">http://arxiv.org/abs/2208.12926</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12926] Overparameterized (robust) models from computational constraints](http://arxiv.org/abs/2208.12926)</code></li>
<li>Summary: <p>Overparameterized models with millions of parameters have been hugely
successful. In this work, we ask: can the need for large models be, at least in
part, due to the \emph{computational} limitations of the learner? Additionally,
we ask, is this situation exacerbated for \emph{robust} learning? We show that
this indeed could be the case. We show learning tasks for which computationally
bounded learners need \emph{significantly more} model parameters than what
information-theoretic learners need. Furthermore, we show that even more model
parameters could be necessary for robust learning. In particular, for
computationally bounded learners, we extend the recent result of Bubeck and
Sellke [NeurIPS'2021] which shows that robust models might need more
parameters, to the computational regime and show that bounded learners could
provably need an even larger number of parameters. Then, we address the
following related question: can we hope to remedy the situation for robust
computationally bounded learning by restricting \emph{adversaries} to also be
computationally bounded for sake of obtaining models with fewer parameters?
Here again, we show that this could be possible. Specifically, building on the
work of Garg, Jha, Mahloujifar, and Mahmoody [ALT'2020], we demonstrate a
learning task that can be learned efficiently and robustly against a
computationally bounded attacker, while to be robust against an
information-theoretic attacker requires the learner to utilize significantly
more parameters.
</p></li>
</ul>

<h3>Title: BOBA: Byzantine-Robust Federated Learning with Label Skewness. (arXiv:2208.12932v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12932">http://arxiv.org/abs/2208.12932</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12932] BOBA: Byzantine-Robust Federated Learning with Label Skewness](http://arxiv.org/abs/2208.12932)</code></li>
<li>Summary: <p>In federated learning, most existing techniques for robust aggregation
against Byzantine attacks are designed for the IID setting, i.e., the data
distributions for clients are independent and identically distributed. In this
paper, we address label skewness, a more realistic and challenging non-IID
setting, where each client only has access to a few classes of data. In this
setting, state-of-the-art techniques suffer from selection bias, leading to
significant performance drop for particular classes; they are also more
vulnerable to Byzantine attacks due to the increased deviation among gradients
of honest clients. To address these limitations, we propose an efficient
two-stage method named BOBA. Theoretically, we prove the convergence of BOBA
with an error of optimal order. Empirically, we verify the superior
unbiasedness and robustness of BOBA across a wide range of models and data sets
against various baselines.
</p></li>
</ul>

<h3>Title: Shaken, and Stirred: Long-Range Dependencies Enable Robust Outlier Detection with PixelCNN++. (arXiv:2208.13579v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13579">http://arxiv.org/abs/2208.13579</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13579] Shaken, and Stirred: Long-Range Dependencies Enable Robust Outlier Detection with PixelCNN++](http://arxiv.org/abs/2208.13579)</code></li>
<li>Summary: <p>Reliable outlier detection is critical for real-world applications of deep
learning models. Likelihoods produced by deep generative models, although
extensively studied, have been largely dismissed as being impractical for
outlier detection. For one, deep generative model likelihoods are readily
biased by low-level input statistics. Second, many recent solutions for
correcting these biases are computationally expensive or do not generalize well
to complex, natural datasets. Here, we explore outlier detection with a
state-of-the-art deep autoregressive model: PixelCNN++. We show that biases in
PixelCNN++ likelihoods arise primarily from predictions based on local
dependencies. We propose two families of bijective transformations that we term
"shaking" and "stirring", which ameliorate low-level biases and isolate the
contribution of long-range dependencies to the PixelCNN++ likelihood. These
transformations are computationally inexpensive and readily applied at
evaluation time. We evaluate our approaches extensively with five grayscale and
six natural image datasets and show that they achieve or exceed
state-of-the-art outlier detection performance. In sum, lightweight remedies
suffice to achieve robust outlier detection on images with deep generative
models.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Artificial Neural Networks for Finger Vein Recognition: A Survey. (arXiv:2208.13341v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13341">http://arxiv.org/abs/2208.13341</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13341] Artificial Neural Networks for Finger Vein Recognition: A Survey](http://arxiv.org/abs/2208.13341)</code></li>
<li>Summary: <p>Finger vein recognition is an emerging biometric recognition technology.
Different from the other biometric features on the body surface, the venous
vascular tissue of the fingers is buried deep inside the skin. Due to this
advantage, finger vein recognition is highly stable and private. They are
almost impossible to be stolen and difficult to interfere with by external
conditions. Unlike the finger vein recognition methods based on traditional
machine learning, the artificial neural network technique, especially deep
learning, it without relying on feature engineering and have superior
performance. To summarize the development of finger vein recognition based on
artificial neural networks, this paper collects 149 related papers. First, we
introduce the background of finger vein recognition and the motivation of this
survey. Then, the development history of artificial neural networks and the
representative networks on finger vein recognition tasks are introduced. The
public datasets that are widely used in finger vein recognition are then
described. After that, we summarize the related finger vein recognition tasks
based on classical neural networks and deep neural networks, respectively.
Finally, the challenges and potential development directions in finger vein
recognition are discussed. To our best knowledge, this paper is the first
comprehensive survey focusing on finger vein recognition based on artificial
neural networks.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Minimal Feature Analysis for Isolated Digit Recognition for varying encoding rates in noisy environments. (arXiv:2208.13100v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13100">http://arxiv.org/abs/2208.13100</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13100] Minimal Feature Analysis for Isolated Digit Recognition for varying encoding rates in noisy environments](http://arxiv.org/abs/2208.13100)</code></li>
<li>Summary: <p>This research work is about recent development made in speech recognition. In
this research work, analysis of isolated digit recognition in the presence of
different bit rates and at different noise levels has been performed. This
research work has been carried using audacity and HTK toolkit. Hidden Markov
Model (HMM) is the recognition model which was used to perform this experiment.
The feature extraction techniques used are Mel Frequency Cepstrum coefficient
(MFCC), Linear Predictive Coding (LPC), perceptual linear predictive (PLP), mel
spectrum (MELSPEC), filter bank (FBANK). There were three types of different
noise levels which have been considered for testing of data. These include
random noise, fan noise and random noise in real time environment. This was
done to analyse the best environment which can used for real time applications.
Further, five different types of commonly used bit rates at different sampling
rates were considered to find out the most optimum bit rate.
</p></li>
</ul>

<h3>Title: Light-YOLOv5: A Lightweight Algorithm for Improved YOLOv5 in Complex Fire Scenarios. (arXiv:2208.13422v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13422">http://arxiv.org/abs/2208.13422</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13422] Light-YOLOv5: A Lightweight Algorithm for Improved YOLOv5 in Complex Fire Scenarios](http://arxiv.org/abs/2208.13422)</code></li>
<li>Summary: <p>In response to the existing object detection algorithms are applied to
complex fire scenarios with poor detection accuracy, slow speed and difficult
deployment., this paper proposes a lightweight fire detection algorithm of
Light-YOLOv5 that achieves a balance of speed and accuracy. First, the last
layer of backbone network is replaced with SepViT Block to enhance the contact
of backbone network to global information; second, a Light-BiFPN neck network
is designed to lighten the model while improving the feature extraction; third,
Global Attention Mechanism (GAM) is fused into the network to make the model
more focused on global dimensional features; finally, we use the Mish
activation function and SIoU loss to increase the convergence speed and improve
the accuracy at the same time. The experimental results show that Light-YOLOv5
improves mAP by 3.3% compared to the original algorithm, reduces the number of
parameters by 27.1%, decreases the computation by 19.1%, achieves FPS of 91.1.
Even compared to the latest YOLOv7-tiny, the mAP of Light-YOLOv5 is 6.8%
higher, which shows the effectiveness of the algorithm.
</p></li>
</ul>

<h3>Title: A Multi-Format Transfer Learning Model for Event Argument Extraction via Variational Information Bottleneck. (arXiv:2208.13017v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13017">http://arxiv.org/abs/2208.13017</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13017] A Multi-Format Transfer Learning Model for Event Argument Extraction via Variational Information Bottleneck](http://arxiv.org/abs/2208.13017)</code></li>
<li>Summary: <p>Event argument extraction (EAE) aims to extract arguments with given roles
from texts, which have been widely studied in natural language processing. Most
previous works have achieved good performance in specific EAE datasets with
dedicated neural architectures. Whereas, these architectures are usually
difficult to adapt to new datasets/scenarios with various annotation schemas or
formats. Furthermore, they rely on large-scale labeled data for training, which
is unavailable due to the high labelling cost in most cases. In this paper, we
propose a multi-format transfer learning model with variational information
bottleneck, which makes use of the information especially the common knowledge
in existing datasets for EAE in new datasets. Specifically, we introduce a
shared-specific prompt framework to learn both format-shared and
format-specific knowledge from datasets with different formats. In order to
further absorb the common knowledge for EAE and eliminate the irrelevant noise,
we integrate variational information bottleneck into our architecture to refine
the shared representation. We conduct extensive experiments on three benchmark
datasets, and obtain new state-of-the-art performance on EAE.
</p></li>
</ul>

<h3>Title: Supporting Medical Relation Extraction via Causality-Pruned Semantic Dependency Forest. (arXiv:2208.13472v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13472">http://arxiv.org/abs/2208.13472</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13472] Supporting Medical Relation Extraction via Causality-Pruned Semantic Dependency Forest](http://arxiv.org/abs/2208.13472)</code></li>
<li>Summary: <p>Medical Relation Extraction (MRE) task aims to extract relations between
entities in medical texts. Traditional relation extraction methods achieve
impressive success by exploring the syntactic information, e.g., dependency
tree. However, the quality of the 1-best dependency tree for medical texts
produced by an out-of-domain parser is relatively limited so that the
performance of medical relation extraction method may degenerate. To this end,
we propose a method to jointly model semantic and syntactic information from
medical texts based on causal explanation theory. We generate dependency
forests consisting of the semantic-embedded 1-best dependency tree. Then, a
task-specific causal explainer is adopted to prune the dependency forests,
which are further fed into a designed graph convolutional network to learn the
corresponding representation for downstream task. Empirically, the various
comparisons on benchmark medical datasets demonstrate the effectiveness of our
model.
</p></li>
</ul>

<h3>Title: Learning a General Clause-to-Clause Relationships for Enhancing Emotion-Cause Pair Extraction. (arXiv:2208.13549v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13549">http://arxiv.org/abs/2208.13549</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13549] Learning a General Clause-to-Clause Relationships for Enhancing Emotion-Cause Pair Extraction](http://arxiv.org/abs/2208.13549)</code></li>
<li>Summary: <p>Emotion-cause pair extraction (ECPE) is an emerging task aiming to extract
potential pairs of emotions and corresponding causes from documents. Previous
approaches have focused on modeling the pair-to-pair relationship and achieved
promising results. However, the clause-to-clause relationship, which
fundamentally symbolizes the underlying structure of a document, has still been
in its research infancy. In this paper, we define a novel clause-to-clause
relationship. To learn it applicably, we propose a general clause-level
encoding model named EA-GAT comprising E-GAT and Activation Sort. E-GAT is
designed to aggregate information from different types of clauses; Activation
Sort leverages the individual emotion/cause prediction and the sort-based
mapping to propel the clause to a more favorable representation. Since EA-GAT
is a clause-level encoding model, it can be broadly integrated with any
previous approach. Experimental results show that our approach has a
significant advantage over all current approaches on the Chinese and English
benchmark corpus, with an average of $2.1\%$ and $1.03\%$.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: A Federated Learning-enabled Smart Street Light Monitoring Application: Benefits and Future Challenges. (arXiv:2208.12996v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12996">http://arxiv.org/abs/2208.12996</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12996] A Federated Learning-enabled Smart Street Light Monitoring Application: Benefits and Future Challenges](http://arxiv.org/abs/2208.12996)</code></li>
<li>Summary: <p>Data-enabled cities are recently accelerated and enhanced with automated
learning for improved Smart Cities applications. In the context of an Internet
of Things (IoT) ecosystem, the data communication is frequently costly,
inefficient, not scalable and lacks security. Federated Learning (FL) plays a
pivotal role in providing privacy-preserving and communication efficient
Machine Learning (ML) frameworks. In this paper we evaluate the feasibility of
FL in the context of a Smart Cities Street Light Monitoring application. FL is
evaluated against benchmarks of centralised and (fully) personalised machine
learning techniques for the classification task of the lampposts operation.
Incorporating FL in such a scenario shows minimal performance reduction in
terms of the classification task, but huge improvements in the communication
cost and the privacy preserving. These outcomes strengthen FL's viability and
potential for IoT applications.
</p></li>
</ul>

<h3>Title: Federated Zero-Shot Learning with Mid-Level Semantic Knowledge Transfer. (arXiv:2208.13465v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13465">http://arxiv.org/abs/2208.13465</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13465] Federated Zero-Shot Learning with Mid-Level Semantic Knowledge Transfer](http://arxiv.org/abs/2208.13465)</code></li>
<li>Summary: <p>Conventional centralised deep learning paradigms are not feasible when data
from different sources cannot be shared due to data privacy or transmission
limitation. To resolve this problem, federated learning has been introduced to
transfer knowledge across multiple sources (clients) with non-shared data while
optimising a globally generalised central model (server). Existing federated
learning paradigms mostly focus on transferring holistic high-level knowledge
(such as class) across models, which are closely related to specific objects of
interest so may suffer from inverse attack. In contrast, in this work, we
consider transferring mid-level semantic knowledge (such as attribute) which is
not sensitive to specific objects of interest and therefore is more
privacy-preserving and scalable. To this end, we formulate a new Federated
Zero-Shot Learning (FZSL) paradigm to learn mid-level semantic knowledge at
multiple local clients with non-shared local data and cumulatively aggregate a
globally generalised central model for deployment. To improve model
discriminative ability, we propose to explore semantic knowledge augmentation
from external knowledge for enriching the mid-level semantic space in FZSL.
Extensive experiments on five zeroshot learning benchmark datasets validate the
effectiveness of our approach for optimising a generalisable federated learning
model with mid-level semantic knowledge transfer.
</p></li>
</ul>

<h3>Title: Network-Level Adversaries in Federated Learning. (arXiv:2208.12911v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12911">http://arxiv.org/abs/2208.12911</a></li>
<li>Code URL: <a href="https://github.com/clonedone/network-level-adversaries-in-federated-learning">https://github.com/clonedone/network-level-adversaries-in-federated-learning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12911] Network-Level Adversaries in Federated Learning](http://arxiv.org/abs/2208.12911)</code></li>
<li>Summary: <p>Federated learning is a popular strategy for training models on distributed,
sensitive data, while preserving data privacy. Prior work identified a range of
security threats on federated learning protocols that poison the data or the
model. However, federated learning is a networked system where the
communication between clients and server plays a critical role for the learning
task performance. We highlight how communication introduces another
vulnerability surface in federated learning and study the impact of
network-level adversaries on training federated learning models. We show that
attackers dropping the network traffic from carefully selected clients can
significantly decrease model accuracy on a target population. Moreover, we show
that a coordinated poisoning campaign from a few clients can amplify the
dropping attacks. Finally, we develop a server-side defense which mitigates the
impact of our attacks by identifying and up-sampling clients likely to
positively contribute towards target accuracy. We comprehensively evaluate our
attacks and defenses on three datasets, assuming encrypted communication
channels and attackers with partial visibility of the network.
</p></li>
</ul>

<h3>Title: Towards Federated Learning against Noisy Labels via Local Self-Regularization. (arXiv:2208.12807v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12807">http://arxiv.org/abs/2208.12807</a></li>
<li>Code URL: <a href="https://github.com/sprinter1999/fedlsr">https://github.com/sprinter1999/fedlsr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12807] Towards Federated Learning against Noisy Labels via Local Self-Regularization](http://arxiv.org/abs/2208.12807)</code></li>
<li>Summary: <p>Federated learning (FL) aims to learn joint knowledge from a large scale of
decentralized devices with labeled data in a privacy-preserving manner.
However, since high-quality labeled data require expensive human intelligence
and efforts, data with incorrect labels (called noisy labels) are ubiquitous in
reality, which inevitably cause performance degradation. Although a lot of
methods are proposed to directly deal with noisy labels, these methods either
require excessive computation overhead or violate the privacy protection
principle of FL. To this end, we focus on this issue in FL with the purpose of
alleviating performance degradation yielded by noisy labels meanwhile
guaranteeing data privacy. Specifically, we propose a Local Self-Regularization
method, which effectively regularizes the local training process via implicitly
hindering the model from memorizing noisy labels and explicitly narrowing the
model output discrepancy between original and augmented instances using self
distillation. Experimental results demonstrate that our proposed method can
achieve notable resistance against noisy labels in various noise levels on
three benchmark datasets. In addition, we integrate our method with existing
state-of-the-arts and achieve superior performance on the real-world dataset
Clothing1M. The code is available at https://github.com/Sprinter1999/FedLSR.
</p></li>
</ul>

<h3>Title: Abnormal Local Clustering in Federated Learning. (arXiv:2208.12813v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12813">http://arxiv.org/abs/2208.12813</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12813] Abnormal Local Clustering in Federated Learning](http://arxiv.org/abs/2208.12813)</code></li>
<li>Summary: <p>Federated learning is a model for privacy without revealing private data by
transfer models instead of personal and private data from local client devices.
While, in the global model, it's crucial to recognize each local data is
normal. This paper suggests one method to separate normal locals and abnormal
locals by Euclidean similarity clustering of vectors extracted by inputting
dummy data in local models. In a federated classification model, this method
divided locals into normal and abnormal.
</p></li>
</ul>

<h3>Title: Federated Learning of Large Models at the Edge via Principal Sub-Model Training. (arXiv:2208.13141v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13141">http://arxiv.org/abs/2208.13141</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13141] Federated Learning of Large Models at the Edge via Principal Sub-Model Training](http://arxiv.org/abs/2208.13141)</code></li>
<li>Summary: <p>Limited compute and communication capabilities of edge users create a
significant bottleneck for federated learning (FL) of large models. We consider
a realistic, but much less explored, cross-device FL setting in which no client
has the capacity to train a full large model nor is willing to share any
intermediate activations with the server. To this end, we present Principal
Sub-Model (PriSM) training methodology, which leverages models low-rank
structure and kernel orthogonality to train sub-models in the orthogonal kernel
space. More specifically, by applying singular value decomposition (SVD) to
original kernels in the server model, PriSM first obtains a set of principal
orthogonal kernels in which each one is weighed by its singular value.
Thereafter, PriSM utilizes our novel sampling strategy that selects different
subsets of the principal kernels independently to create sub-models for
clients. Importantly, a kernel with a large singular value is assigned with a
high sampling probability. Thus, each sub-model is a low-rank approximation of
the full large model, and all clients together achieve the near full-model
training. Our extensive evaluations on multiple datasets in various
resource-constrained settings show that PriSM can yield an improved performance
of up to 10% compared to existing alternatives, with only around 20% sub-model
training.
</p></li>
</ul>

<h3>Title: Tensor Decomposition based Personalized Federated Learning. (arXiv:2208.12959v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12959">http://arxiv.org/abs/2208.12959</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12959] Tensor Decomposition based Personalized Federated Learning](http://arxiv.org/abs/2208.12959)</code></li>
<li>Summary: <p>Federated learning (FL) is a new distributed machine learning framework that
can achieve reliably collaborative training without collecting users' private
data. However, due to FL's frequent communication and average aggregation
strategy, they experience challenges scaling to statistical diversity data and
large-scale models. In this paper, we propose a personalized FL framework,
named Tensor Decomposition based Personalized Federated learning (TDPFed), in
which we design a novel tensorized local model with tensorized linear layers
and convolutional layers to reduce the communication cost. TDPFed uses a
bi-level loss function to decouple personalized model optimization from the
global model learning by controlling the gap between the personalized model and
the tensorized local model. Moreover, an effective distributed learning
strategy and two different model aggregation strategies are well designed for
the proposed TDPFed framework. Theoretical convergence analysis and thorough
experiments demonstrate that our proposed TDPFed framework achieves
state-of-the-art performance while reducing the communication cost.
</p></li>
</ul>

<h3>Title: Federated Sparse Training: Lottery Aware Model Compression for Resource Constrained Edge. (arXiv:2208.13092v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13092">http://arxiv.org/abs/2208.13092</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13092] Federated Sparse Training: Lottery Aware Model Compression for Resource Constrained Edge](http://arxiv.org/abs/2208.13092)</code></li>
<li>Summary: <p>Limited computation and communication capabilities of clients pose
significant challenges in federated learning (FL) over resource-limited edge
nodes. A potential solution to this problem is to deploy off-the-shelf sparse
learning algorithms that train a binary sparse mask on each client with the
expectation of training a consistent sparse server mask. However, as we
investigate in this paper, such naive deployments result in a significant
accuracy drop compared to FL with dense models, especially under low client's
resource budget. In particular, our investigations reveal a serious lack of
consensus among the trained masks on clients, which prevents convergence on the
server mask and potentially leads to a substantial drop in model performance.
Based on such key observations, we propose federated lottery aware sparsity
hunting (FLASH), a unified sparse learning framework to make the server win a
lottery in terms of a sparse sub-model, which can greatly improve performance
under highly resource-limited client settings. Moreover, to address the issue
of device heterogeneity, we leverage our findings to propose hetero-FLASH,
where clients can have different target sparsity budgets based on their device
resource limits. Extensive experimental evaluations with multiple models on
various datasets (both IID and non-IID) show superiority of our models in
yielding up to $\mathord{\sim}10.1\%$ improved accuracy with
$\mathord{\sim}10.26\times$ fewer communication costs, compared to existing
alternatives, at similar hyperparameter settings.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Towards Explaining Demographic Bias through the Eyes of Face Recognition Models. (arXiv:2208.13400v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13400">http://arxiv.org/abs/2208.13400</a></li>
<li>Code URL: <a href="https://github.com/fbiying87/demographic-bias-visualization">https://github.com/fbiying87/demographic-bias-visualization</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13400] Towards Explaining Demographic Bias through the Eyes of Face Recognition Models](http://arxiv.org/abs/2208.13400)</code></li>
<li>Summary: <p>Biases inherent in both data and algorithms make the fairness of widespread
machine learning (ML)-based decision-making systems less than optimal. To
improve the trustfulness of such ML decision systems, it is crucial to be aware
of the inherent biases in these solutions and to make them more transparent to
the public and developers. In this work, we aim at providing a set of
explainability tool that analyse the difference in the face recognition models'
behaviors when processing different demographic groups. We do that by
leveraging higher-order statistical information based on activation maps to
build explainability tools that link the FR models' behavior differences to
certain facial regions. The experimental results on two datasets and two face
recognition models pointed out certain areas of the face where the FR models
react differently for certain demographic groups compared to reference groups.
The outcome of these analyses interestingly aligns well with the results of
studies that analyzed the anthropometric differences and the human judgment
differences on the faces of different demographic groups. This is thus the
first study that specifically tries to explain the biased behavior of FR models
on different demographic groups and link it directly to the spatial facial
features. The code is publicly available here.
</p></li>
</ul>

<h3>Title: CIRCLe: Color Invariant Representation Learning for Unbiased Classification of Skin Lesions. (arXiv:2208.13528v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13528">http://arxiv.org/abs/2208.13528</a></li>
<li>Code URL: <a href="https://github.com/arezou-pakzad/circle">https://github.com/arezou-pakzad/circle</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13528] CIRCLe: Color Invariant Representation Learning for Unbiased Classification of Skin Lesions](http://arxiv.org/abs/2208.13528)</code></li>
<li>Summary: <p>While deep learning based approaches have demonstrated expert-level
performance in dermatological diagnosis tasks, they have also been shown to
exhibit biases toward certain demographic attributes, particularly skin types
(e.g., light versus dark), a fairness concern that must be addressed. We
propose CIRCLe, a skin color invariant deep representation learning method for
improving fairness in skin lesion classification. CIRCLe is trained to classify
images by utilizing a regularization loss that encourages images with the same
diagnosis but different skin types to have similar latent representations.
Through extensive evaluation and ablation studies, we demonstrate CIRCLe's
superior performance over the state-of-the-art when evaluated on 16k+ images
spanning 6 Fitzpatrick skin types and 114 diseases, using classification
accuracy, equal opportunity difference (for light versus dark groups), and
normalized accuracy range, a new measure we propose to assess fairness on
multiple skin type groups.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Interpreting Black-box Machine Learning Models for High Dimensional Datasets. (arXiv:2208.13405v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.13405">http://arxiv.org/abs/2208.13405</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.13405] Interpreting Black-box Machine Learning Models for High Dimensional Datasets](http://arxiv.org/abs/2208.13405)</code></li>
<li>Summary: <p>Deep neural networks (DNNs) have been shown to outperform traditional machine
learning algorithms in a broad variety of application domains due to their
effectiveness in modeling intricate problems and handling high-dimensional
datasets. Many real-life datasets, however, are of increasingly high
dimensionality, where a large number of features may be irrelevant to the task
at hand. The inclusion of such features would not only introduce unwanted noise
but also increase computational complexity. Furthermore, due to high
non-linearity and dependency among a large number of features, DNN models tend
to be unavoidably opaque and perceived as black-box methods because of their
not well-understood internal functioning. A well-interpretable model can
identify statistically significant features and explain the way they affect the
model's outcome. In this paper, we propose an efficient method to improve the
interpretability of black-box models for classification tasks in the case of
high-dimensional datasets. To this end, we first train a black-box model on a
high-dimensional dataset to learn the embeddings on which the classification is
performed. To decompose the inner working principles of the black-box model and
to identify top-k important features, we employ different probing and
perturbing techniques. We then approximate the behavior of the black-box model
by means of an interpretable surrogate model on the top-k feature space.
Finally, we derive decision rules and local explanations from the surrogate
model to explain individual decisions. Our approach outperforms and competes
with state-of-the-art methods such as TabNet, XGboost, and SHAP-based
interpretability techniques when tested on different datasets with varying
dimensionality between 50 and 20,000.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
