<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: A Cyber-Physical Architecture for Microgrids based on Deep learning and LORA Technology. (arXiv:2312.08818v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08818">http://arxiv.org/abs/2312.08818</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08818]] A Cyber-Physical Architecture for Microgrids based on Deep learning and LORA Technology(http://arxiv.org/abs/2312.08818)</code></li>
<li>Summary: <p>This paper proposes a cyber-physical architecture for the secured social
operation of isolated hybrid microgrids (HMGs). On the physical side of the
proposed architecture, an optimal scheduling scheme considering various
renewable energy sources (RESs) and fossil fuel-based distributed generation
units (DGs) is proposed. Regarding the cyber layer of MGs, a wireless
architecture based on low range wide area (LORA) technology is introduced for
advanced metering infrastructure (AMI) in smart electricity grids. In the
proposed architecture, the LORA data frame is described in detail and designed
for the application of smart meters considering DGs and ac-dc converters.
Additionally, since the cyber layer of smart grids is highly vulnerable to
cyber-attacks, t1his paper proposes a deep-learning-based cyber-attack
detection model (CADM) based on bidirectional long short-term memory (BLSTM)
and sequential hypothesis testing (SHT) to detect false data injection attacks
(FDIA) on the smart meters within AMI. The performance of the proposed energy
management architecture is evaluated using the IEEE 33-bus test system. In
order to investigate the effect of FDIA on the isolated HMGs and highlight the
interactions between the cyber layer and physical layer, an FDIA is launched
against the test system. The results showed that a successful attack can highly
damage the system and cause widespread load shedding. Also, the performance of
the proposed CADM is examined using a real-world dataset. Results prove the
effectiveness of the proposed CADM in detecting the attacks using only two
samples.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: PhyOT: Physics-informed object tracking in surveillance cameras. (arXiv:2312.08650v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08650">http://arxiv.org/abs/2312.08650</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08650]] PhyOT: Physics-informed object tracking in surveillance cameras(http://arxiv.org/abs/2312.08650)</code></li>
<li>Summary: <p>While deep learning has been very successful in computer vision, real world
operating conditions such as lighting variation, background clutter, or
occlusion hinder its accuracy across several tasks. Prior work has shown that
hybrid models -- combining neural networks and heuristics/algorithms -- can
outperform vanilla deep learning for several computer vision tasks, such as
classification or tracking. We consider the case of object tracking, and
evaluate a hybrid model (PhyOT) that conceptualizes deep neural networks as
``sensors'' in a Kalman filter setup, where prior knowledge, in the form of
Newtonian laws of motion, is used to fuse sensor observations and to perform
improved estimations. Our experiments combine three neural networks, performing
position, indirect velocity and acceleration estimation, respectively, and
evaluate such a formulation on two benchmark datasets: a warehouse security
camera dataset that we collected and annotated and a traffic camera open
dataset. Results suggest that our PhyOT can track objects in extreme conditions
that the state-of-the-art deep neural networks fail while its performance in
general cases does not degrade significantly from that of existing deep
learning approaches. Results also suggest that our PhyOT components are
generalizable and transferable.
</p></li>
</ul>

<h3>Title: Attestation with Constrained Relying Party. (arXiv:2312.08903v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08903">http://arxiv.org/abs/2312.08903</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08903]] Attestation with Constrained Relying Party(http://arxiv.org/abs/2312.08903)</code></li>
<li>Summary: <p>Allowing a compromised device to receive privacy-sensitive sensor readings,
or to operate a safety-critical actuator, carries significant risk. Usually,
such risks are mitigated by validating the device's security state with remote
attestation, but current remote attestation protocols are not suitable when the
beneficiary of attestation, the relying party, is a constrained device such as
a small sensor or actuator. These devices typically lack the power and memory
to operate public-key cryptography needed by such protocols, and may only be
able to communicate with devices in their physical proximity, such as with the
controller whose security state they wish to evaluate. In this paper, we
present a remote platform attestation protocol suitable for relying parties
that are limited to symmetric-key cryptography and a single communication
channel. We show that our protocol, including the needed cryptography and
message processing, can be implemented with a code size of 6 KB and validate
its security via model checking with the ProVerif tool.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Privacy Constrained Fairness Estimation for Decision Trees. (arXiv:2312.08413v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08413">http://arxiv.org/abs/2312.08413</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08413]] Privacy Constrained Fairness Estimation for Decision Trees(http://arxiv.org/abs/2312.08413)</code></li>
<li>Summary: <p>The protection of sensitive data becomes more vital, as data increases in
value and potency. Furthermore, the pressure increases from regulators and
society on model developers to make their Artificial Intelligence (AI) models
non-discriminatory. To boot, there is a need for interpretable, transparent AI
models for high-stakes tasks. In general, measuring the fairness of any AI
model requires the sensitive attributes of the individuals in the dataset, thus
raising privacy concerns. In this work, the trade-offs between fairness,
privacy and interpretability are further explored. We specifically examine the
Statistical Parity (SP) of Decision Trees (DTs) with Differential Privacy (DP),
that are each popular methods in their respective subfield. We propose a novel
method, dubbed Privacy-Aware Fairness Estimation of Rules (PAFER), that can
estimate SP in a DP-aware manner for DTs. DP, making use of a third-party legal
entity that securely holds this sensitive data, guarantees privacy by adding
noise to the sensitive data. We experimentally compare several DP mechanisms.
We show that using the Laplacian mechanism, the method is able to estimate SP
with low error while guaranteeing the privacy of the individuals in the dataset
with high certainty. We further show experimentally and theoretically that the
method performs better for DTs that humans generally find easier to interpret.
</p></li>
</ul>

<h3>Title: Privacy Amplification by Iteration for ADMM with (Strongly) Convex Objective Functions. (arXiv:2312.08685v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08685">http://arxiv.org/abs/2312.08685</a></li>
<li>Code URL: https://github.com/kawaiimengshi/privacy-amplification-by-iteration-for-admm</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08685]] Privacy Amplification by Iteration for ADMM with (Strongly) Convex Objective Functions(http://arxiv.org/abs/2312.08685)</code></li>
<li>Summary: <p>We examine a private ADMM variant for (strongly) convex objectives which is a
primal-dual iterative method. Each iteration has a user with a private function
used to update the primal variable, masked by Gaussian noise for local privacy,
without directly adding noise to the dual variable. Privacy amplification by
iteration explores if noises from later iterations can enhance the privacy
guarantee when releasing final variables after the last iteration. Cyffers et
al. [ICML 2023] explored privacy amplification by iteration for the proximal
ADMM variant, where a user's entire private function is accessed and noise is
added to the primal variable. In contrast, we examine a private ADMM variant
requiring just one gradient access to a user's function, but both primal and
dual variables must be passed between successive iterations. To apply Balle et
al.'s [NeurIPS 2019] coupling framework to the gradient ADMM variant, we tackle
technical challenges with novel ideas. First, we address the non-expansive
mapping issue in ADMM iterations by using a customized norm. Second, because
the dual variables are not masked with any noise directly, their privacy
guarantees are achieved by treating two consecutive noisy ADMM iterations as a
Markov operator. Our main result is that the privacy guarantee for the gradient
ADMM variant can be amplified proportionally to the number of iterations. For
strongly convex objective functions, this amplification exponentially increases
with the number of iterations. These amplification results align with the
previously studied special case of stochastic gradient descent.
</p></li>
</ul>

<h3>Title: Conformalised data synthesis with statistical quality guarantees. (arXiv:2312.08999v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08999">http://arxiv.org/abs/2312.08999</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08999]] Conformalised data synthesis with statistical quality guarantees(http://arxiv.org/abs/2312.08999)</code></li>
<li>Summary: <p>With the proliferation of ever more complicated Deep Learning architectures,
data synthesis is a highly promising technique to address the demand of
data-hungry models. However, reliably assessing the quality of a 'synthesiser'
model's output is an open research question with significant associated risks
for high-stake domains. To address this challenge, we have designed a unique
confident data synthesis algorithm that introduces statistical confidence
guarantees through a novel extension of the Conformal Prediction framework. We
support our proposed algorithm with theoretical proofs and an extensive
empirical evaluation of five benchmark datasets. To show our approach's
versatility on ubiquitous real-world challenges, the datasets were carefully
selected for their variety of difficult characteristics: low sample count,
class imbalance and non-separability, and privacy-sensitive data. In all
trials, training sets extended with our confident synthesised data performed at
least as well as the original, and frequently significantly improved Deep
Learning performance by up to +65% F1-score.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: EditGuard: Versatile Image Watermarking for Tamper Localization and Copyright Protection. (arXiv:2312.08883v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08883">http://arxiv.org/abs/2312.08883</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08883]] EditGuard: Versatile Image Watermarking for Tamper Localization and Copyright Protection(http://arxiv.org/abs/2312.08883)</code></li>
<li>Summary: <p>In the era where AI-generated content (AIGC) models can produce stunning and
lifelike images, the lingering shadow of unauthorized reproductions and
malicious tampering poses imminent threats to copyright integrity and
information security. Current image watermarking methods, while widely accepted
for safeguarding visual content, can only protect copyright and ensure
traceability. They fall short in localizing increasingly realistic image
tampering, potentially leading to trust crises, privacy violations, and legal
disputes. To solve this challenge, we propose an innovative proactive forensics
framework EditGuard, to unify copyright protection and tamper-agnostic
localization, especially for AIGC-based editing methods. It can offer a
meticulous embedding of imperceptible watermarks and precise decoding of
tampered areas and copyright information. Leveraging our observed fragility and
locality of image-into-image steganography, the realization of EditGuard can be
converted into a united image-bit steganography issue, thus completely
decoupling the training process from the tampering types. Extensive experiments
demonstrate that our EditGuard balances the tamper localization accuracy,
copyright recovery precision, and generalizability to various AIGC-based
tampering methods, especially for image forgery that is difficult for the naked
eye to detect. The project page is available at
https://xuanyuzhang21.github.io/project/editguard/.
</p></li>
</ul>

<h3>Title: On Mask-based Image Set Desensitization with Recognition Support. (arXiv:2312.08975v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08975">http://arxiv.org/abs/2312.08975</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08975]] On Mask-based Image Set Desensitization with Recognition Support(http://arxiv.org/abs/2312.08975)</code></li>
<li>Summary: <p>In recent years, Deep Neural Networks (DNN) have emerged as a practical
method for image recognition. The raw data, which contain sensitive
information, are generally exploited within the training process. However, when
the training process is outsourced to a third-party organization, the raw data
should be desensitized before being transferred to protect sensitive
information. Although masks are widely applied to hide important sensitive
information, preventing inpainting masked images is critical, which may restore
the sensitive information. The corresponding models should be adjusted for the
masked images to reduce the degradation of the performance for recognition or
classification tasks due to the desensitization of images. In this paper, we
propose a mask-based image desensitization approach while supporting
recognition. This approach consists of a mask generation algorithm and a model
adjustment method. We propose exploiting an interpretation algorithm to
maintain critical information for the recognition task in the mask generation
algorithm. In addition, we propose a feature selection masknet as the model
adjustment method to improve the performance based on the masked images.
Extensive experimentation results based on multiple image datasets reveal
significant advantages (up to 9.34% in terms of accuracy) of our approach for
image desensitization while supporting recognition.
</p></li>
</ul>

<h3>Title: Google Tag Manager: Hidden Data Leaks and its Potential Violations under EU Data Protection Law. (arXiv:2312.08806v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08806">http://arxiv.org/abs/2312.08806</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08806]] Google Tag Manager: Hidden Data Leaks and its Potential Violations under EU Data Protection Law(http://arxiv.org/abs/2312.08806)</code></li>
<li>Summary: <p>Tag Management Systems were developed in order to support website publishers
in installing multiple third-party JavaScript scripts (Tags) on their websites.
In 2012, Google developed its own TMS called "Google Tag Manager" (GTM) that is
currently present on 28 million live websites. In 2020, a new "Server-side" GTM
was introduced, allowing publishers to include Tags directly on the server.
However, neither version of GTM has yet been thoroughly evaluated by the
academic research community. In this work, we study, for the first time, the
two versions of the Google Tag Management (GTM) architectures: Client- and
Server-side GTM. By analyzing these systems with 78 Client-side Tags, 8
Server-side Tags and two Consent Management Platforms (CMPs) from the inside,
we discover multiple hidden data leaks, Tags bypassing GTM permission system to
inject scripts, and consent enabled by default. With a legal expert, we perform
an in-depth legal analysis of GTM and its actors to identify potential legal
violations and their liabilities. We provide recommendations and propose
numerous improvements for GTM to facilitate legal compliance.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Defenses in Adversarial Machine Learning: A Survey. (arXiv:2312.08890v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08890">http://arxiv.org/abs/2312.08890</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08890]] Defenses in Adversarial Machine Learning: A Survey(http://arxiv.org/abs/2312.08890)</code></li>
<li>Summary: <p>Adversarial phenomenon has been widely observed in machine learning (ML)
systems, especially in those using deep neural networks, describing that ML
systems may produce inconsistent and incomprehensible predictions with humans
at some particular cases. This phenomenon poses a serious security threat to
the practical application of ML systems, and several advanced attack paradigms
have been developed to explore it, mainly including backdoor attacks, weight
attacks, and adversarial examples. For each individual attack paradigm, various
defense paradigms have been developed to improve the model robustness against
the corresponding attack paradigm. However, due to the independence and
diversity of these defense paradigms, it is difficult to examine the overall
robustness of an ML system against different kinds of attacks.This survey aims
to build a systematic review of all existing defense paradigms from a unified
perspective. Specifically, from the life-cycle perspective, we factorize a
complete machine learning system into five stages, including pre-training,
training, post-training, deployment, and inference stages, respectively. Then,
we present a clear taxonomy to categorize and review representative defense
methods at each individual stage. The unified perspective and presented
taxonomies not only facilitate the analysis of the mechanism of each defense
paradigm but also help us to understand connections and differences among
different defense paradigms, which may inspire future research to develop more
advanced, comprehensive defenses.
</p></li>
</ul>

<h3>Title: Data and Model Poisoning Backdoor Attacks on Wireless Federated Learning, and the Defense Mechanisms: A Comprehensive Survey. (arXiv:2312.08667v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08667">http://arxiv.org/abs/2312.08667</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08667]] Data and Model Poisoning Backdoor Attacks on Wireless Federated Learning, and the Defense Mechanisms: A Comprehensive Survey(http://arxiv.org/abs/2312.08667)</code></li>
<li>Summary: <p>Due to the greatly improved capabilities of devices, massive data, and
increasing concern about data privacy, Federated Learning (FL) has been
increasingly considered for applications to wireless communication networks
(WCNs). Wireless FL (WFL) is a distributed method of training a global deep
learning model in which a large number of participants each train a local model
on their training datasets and then upload the local model updates to a central
server. However, in general, non-independent and identically distributed
(non-IID) data of WCNs raises concerns about robustness, as a malicious
participant could potentially inject a "backdoor" into the global model by
uploading poisoned data or models over WCN. This could cause the model to
misclassify malicious inputs as a specific target class while behaving normally
with benign inputs. This survey provides a comprehensive review of the latest
backdoor attacks and defense mechanisms. It classifies them according to their
targets (data poisoning or model poisoning), the attack phase (local data
collection, training, or aggregation), and defense stage (local training,
before aggregation, during aggregation, or after aggregation). The strengths
and limitations of existing attack strategies and defense mechanisms are
analyzed in detail. Comparisons of existing attack methods and defense designs
are carried out, pointing to noteworthy findings, open challenges, and
potential future research directions related to security and privacy of WFL.
</p></li>
</ul>

<h3>Title: Detection and Defense of Unlearnable Examples. (arXiv:2312.08898v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08898">http://arxiv.org/abs/2312.08898</a></li>
<li>Code URL: https://github.com/hala64/udp</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08898]] Detection and Defense of Unlearnable Examples(http://arxiv.org/abs/2312.08898)</code></li>
<li>Summary: <p>Privacy preserving has become increasingly critical with the emergence of
social media. Unlearnable examples have been proposed to avoid leaking personal
information on the Internet by degrading generalization abilities of deep
learning models. However, our study reveals that unlearnable examples are
easily detectable. We provide theoretical results on linear separability of
certain unlearnable poisoned dataset and simple network based detection methods
that can identify all existing unlearnable examples, as demonstrated by
extensive experiments. Detectability of unlearnable examples with simple
networks motivates us to design a novel defense method. We propose using
stronger data augmentations coupled with adversarial noises generated by simple
networks, to degrade the detectability and thus provide effective defense
against unlearnable examples with a lower cost. Adversarial training with large
budgets is a widely-used defense method on unlearnable examples. We establish
quantitative criteria between the poison and adversarial budgets which
determine the existence of robust unlearnable examples or the failure of the
adversarial defense.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: AVA: Inconspicuous Attribute Variation-based Adversarial Attack bypassing DeepFake Detection. (arXiv:2312.08675v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08675">http://arxiv.org/abs/2312.08675</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08675]] AVA: Inconspicuous Attribute Variation-based Adversarial Attack bypassing DeepFake Detection(http://arxiv.org/abs/2312.08675)</code></li>
<li>Summary: <p>While DeepFake applications are becoming popular in recent years, their
abuses pose a serious privacy threat. Unfortunately, most related detection
algorithms to mitigate the abuse issues are inherently vulnerable to
adversarial attacks because they are built atop DNN-based classification
models, and the literature has demonstrated that they could be bypassed by
introducing pixel-level perturbations. Though corresponding mitigation has been
proposed, we have identified a new attribute-variation-based adversarial attack
(AVA) that perturbs the latent space via a combination of Gaussian prior and
semantic discriminator to bypass such mitigation. It perturbs the semantics in
the attribute space of DeepFake images, which are inconspicuous to human beings
(e.g., mouth open) but can result in substantial differences in DeepFake
detection. We evaluate our proposed AVA attack on nine state-of-the-art
DeepFake detection algorithms and applications. The empirical results
demonstrate that AVA attack defeats the state-of-the-art black box attacks
against DeepFake detectors and achieves more than a 95% success rate on two
commercial DeepFake detectors. Moreover, our human study indicates that
AVA-generated DeepFake images are often imperceptible to humans, which presents
huge security and privacy concerns.
</p></li>
</ul>

<h3>Title: Forbidden Facts: An Investigation of Competing Objectives in Llama-2. (arXiv:2312.08793v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08793">http://arxiv.org/abs/2312.08793</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08793]] Forbidden Facts: An Investigation of Competing Objectives in Llama-2(http://arxiv.org/abs/2312.08793)</code></li>
<li>Summary: <p>LLMs often face competing pressures (for example helpfulness vs.
harmlessness). To understand how models resolve such conflicts, we study
Llama-2-chat models on the forbidden fact task. Specifically, we instruct
Llama-2 to truthfully complete a factual recall statement while forbidding it
from saying the correct answer. This often makes the model give incorrect
answers. We decompose Llama-2 into 1000+ components, and rank each one with
respect to how useful it is for forbidding the correct answer. We find that in
aggregate, around 35 components are enough to reliably implement the full
suppression behavior. However, these components are fairly heterogeneous and
many operate using faulty heuristics. We discover that one of these heuristics
can be exploited via a manually designed adversarial attack which we call The
California Attack. Our results highlight some roadblocks standing in the way of
being able to successfully interpret advanced ML systems. Project website
available at https://forbiddenfacts.github.io .
</p></li>
</ul>

<h3>Title: EmbAu: A Novel Technique to Embed Audio Data Using Shuffled Frog Leaping Algorithm. (arXiv:2312.08417v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08417">http://arxiv.org/abs/2312.08417</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08417]] EmbAu: A Novel Technique to Embed Audio Data Using Shuffled Frog Leaping Algorithm(http://arxiv.org/abs/2312.08417)</code></li>
<li>Summary: <p>The aim of steganographic algorithms is to identify the appropriate pixel
positions in the host or cover image, where bits of sensitive information can
be concealed for data encryption. Work is being done to improve the capacity to
integrate sensitive information and to maintain the visual appearance of the
steganographic image. Consequently, steganography is a challenging research
area. In our currently proposed image steganographic technique, we used the
Shuffled Frog Leaping Algorithm (SFLA) to determine the order of pixels by
which sensitive information can be placed in the cover image. To achieve
greater embedding capacity, pixels from the spatial domain of the cover image
are carefully chosen and used for placing the sensitive data. Bolstered via
image steganography, the final image after embedding is resistant to
steganalytic attacks. The SFLA algorithm serves in the optimal pixels selection
of any colored (RGB) cover image for secret bit embedding. Using the fitness
function, the SFLA benefits by reaching a minimum cost value in an acceptable
amount of time. The pixels for embedding are meticulously chosen to minimize
the host image's distortion upon embedding. Moreover, an effort has been taken
to make the detection of embedded data in the steganographic image a formidable
challenge. Due to the enormous need for audio data encryption in the current
world, we feel that our suggested method has significant potential in
real-world applications. In this paper, we propose and compare our strategy to
existing steganographic methods.
</p></li>
</ul>

<h3>Title: Deep Learning-Based Cyber-Attack Detection Model for Smart Grids. (arXiv:2312.08810v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08810">http://arxiv.org/abs/2312.08810</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08810]] Deep Learning-Based Cyber-Attack Detection Model for Smart Grids(http://arxiv.org/abs/2312.08810)</code></li>
<li>Summary: <p>In this paper, a novel artificial intelligence-based cyber-attack detection
model for smart grids is developed to stop data integrity cyber-attacks (DIAs)
on the received load data by supervisory control and data acquisition (SCADA).
In the proposed model, first the load data is forecasted using a regression
model and after processing stage, the processed data is clustered using the
unsupervised learning method. In this work, in order to achieve the best
performance, three load forecasting methods (i.e. extra tree regression (ETR),
long short-term memory (LSTM) and bidirectional long short-term memory
(BiLSTM)) are utilized as regression models and their performance is compared.
For clustering and outlying detection, the covariance elliptic envelope (EE) is
employed as an unsupervised learning method. To examine the proposed model, the
hourly load data of the power company of the city of Johor in Malaysia is
employed and Two common DIAs, which are DIAs targeting economic loss and DIAs
targeting blackouts, are used to evaluate the accuracy of detection methods in
several scenarios. The simulation results show that the proposed EE-BiLSTM
method can perform more robust and accurate compared to the other two methods.
</p></li>
</ul>

<h3>Title: Towards Inductive Robustness: Distilling and Fostering Wave-induced Resonance in Transductive GCNs Against Graph Adversarial Attacks. (arXiv:2312.08651v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08651">http://arxiv.org/abs/2312.08651</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08651]] Towards Inductive Robustness: Distilling and Fostering Wave-induced Resonance in Transductive GCNs Against Graph Adversarial Attacks(http://arxiv.org/abs/2312.08651)</code></li>
<li>Summary: <p>Graph neural networks (GNNs) have recently been shown to be vulnerable to
adversarial attacks, where slight perturbations in the graph structure can lead
to erroneous predictions. However, current robust models for defending against
such attacks inherit the transductive limitations of graph convolutional
networks (GCNs). As a result, they are constrained by fixed structures and do
not naturally generalize to unseen nodes. Here, we discover that transductive
GCNs inherently possess a distillable robustness, achieved through a
wave-induced resonance process. Based on this, we foster this resonance to
facilitate inductive and robust learning. Specifically, we first prove that the
signal formed by GCN-driven message passing (MP) is equivalent to the
edge-based Laplacian wave, where, within a wave system, resonance can naturally
emerge between the signal and its transmitting medium. This resonance provides
inherent resistance to malicious perturbations inflicted on the signal system.
We then prove that merely three MP iterations within GCNs can induce signal
resonance between nodes and edges, manifesting as a coupling between nodes and
their distillable surrounding local subgraph. Consequently, we present Graph
Resonance-fostering Network (GRN) to foster this resonance via learning node
representations from their distilled resonating subgraphs. By capturing the
edge-transmitted signals within this subgraph and integrating them with the
node signal, GRN embeds these combined signals into the central node's
representation. This node-wise embedding approach allows for generalization to
unseen nodes. We validate our theoretical findings with experiments, and
demonstrate that GRN generalizes robustness to unseen nodes, whilst maintaining
state-of-the-art classification accuracy on perturbed graphs.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: MmAP : Multi-modal Alignment Prompt for Cross-domain Multi-task Learning. (arXiv:2312.08636v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08636">http://arxiv.org/abs/2312.08636</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08636]] MmAP : Multi-modal Alignment Prompt for Cross-domain Multi-task Learning(http://arxiv.org/abs/2312.08636)</code></li>
<li>Summary: <p>Multi-Task Learning (MTL) is designed to train multiple correlated tasks
simultaneously, thereby enhancing the performance of individual tasks.
Typically, a multi-task network structure consists of a shared backbone and
task-specific decoders. However, the complexity of the decoders increases with
the number of tasks. To tackle this challenge, we integrate the decoder-free
vision-language model CLIP, which exhibits robust zero-shot generalization
capability. Recently, parameter-efficient transfer learning methods have been
extensively explored with CLIP for adapting to downstream tasks, where prompt
tuning showcases strong potential. Nevertheless, these methods solely fine-tune
a single modality (text or visual), disrupting the modality structure of CLIP.
In this paper, we first propose Multi-modal Alignment Prompt (MmAP) for CLIP,
which aligns text and visual modalities during fine-tuning process. Building
upon MmAP, we develop an innovative multi-task prompt learning framework. On
the one hand, to maximize the complementarity of tasks with high similarity, we
utilize a gradient-driven task grouping method that partitions tasks into
several disjoint groups and assign a group-shared MmAP to each group. On the
other hand, to preserve the unique characteristics of each task, we assign an
task-specific MmAP to each task. Comprehensive experiments on two large
multi-task learning datasets demonstrate that our method achieves significant
performance improvements compared to full fine-tuning while only utilizing
approximately 0.09% of trainable parameters.
</p></li>
</ul>

<h3>Title: SPEAL: Skeletal Prior Embedded Attention Learning for Cross-Source Point Cloud Registration. (arXiv:2312.08664v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08664">http://arxiv.org/abs/2312.08664</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08664]] SPEAL: Skeletal Prior Embedded Attention Learning for Cross-Source Point Cloud Registration(http://arxiv.org/abs/2312.08664)</code></li>
<li>Summary: <p>Point cloud registration, a fundamental task in 3D computer vision, has
remained largely unexplored in cross-source point clouds and unstructured
scenes. The primary challenges arise from noise, outliers, and variations in
scale and density. However, neglected geometric natures of point clouds
restricts the performance of current methods. In this paper, we propose a novel
method termed SPEAL to leverage skeletal representations for effective learning
of intrinsic topologies of point clouds, facilitating robust capture of
geometric intricacy. Specifically, we design the Skeleton Extraction Module to
extract skeleton points and skeletal features in an unsupervised manner, which
is inherently robust to noise and density variances. Then, we propose the
Skeleton-Aware GeoTransformer to encode high-level skeleton-aware features. It
explicitly captures the topological natures and inter-point-cloud skeletal
correlations with the noise-robust and density-invariant skeletal
representations. Next, we introduce the Correspondence Dual-Sampler to
facilitate correspondences by augmenting the correspondence set with skeletal
correspondences. Furthermore, we construct a challenging novel large-scale
cross-source point cloud dataset named KITTI CrossSource for benchmarking
cross-source point cloud registration methods. Extensive quantitative and
qualitative experiments are conducted to demonstrate our approach's superiority
and robustness on both cross-source and same-source datasets. To the best of
our knowledge, our approach is the first to facilitate point cloud registration
with skeletal geometric priors.
</p></li>
</ul>

<h3>Title: Towards Robust and Expressive Whole-body Human Pose and Shape Estimation. (arXiv:2312.08730v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08730">http://arxiv.org/abs/2312.08730</a></li>
<li>Code URL: https://github.com/robosmplx/robosmplx</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08730]] Towards Robust and Expressive Whole-body Human Pose and Shape Estimation(http://arxiv.org/abs/2312.08730)</code></li>
<li>Summary: <p>Whole-body pose and shape estimation aims to jointly predict different
behaviors (e.g., pose, hand gesture, facial expression) of the entire human
body from a monocular image. Existing methods often exhibit degraded
performance under the complexity of in-the-wild scenarios. We argue that the
accuracy and reliability of these models are significantly affected by the
quality of the predicted \textit{bounding box}, e.g., the scale and alignment
of body parts. The natural discrepancy between the ideal bounding box
annotations and model detection results is particularly detrimental to the
performance of whole-body pose and shape estimation. In this paper, we propose
a novel framework to enhance the robustness of whole-body pose and shape
estimation. Our framework incorporates three new modules to address the above
challenges from three perspectives: \textbf{1) Localization Module} enhances
the model's awareness of the subject's location and semantics within the image
space. \textbf{2) Contrastive Feature Extraction Module} encourages the model
to be invariant to robust augmentations by incorporating contrastive loss with
dedicated positive samples. \textbf{3) Pixel Alignment Module} ensures the
reprojected mesh from the predicted camera and body model parameters are
accurate and pixel-aligned. We perform comprehensive experiments to demonstrate
the effectiveness of our proposed framework on body, hands, face and whole-body
benchmarks. Codebase is available at
\url{https://github.com/robosmplx/robosmplx}.
</p></li>
</ul>

<h3>Title: CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental Learning. (arXiv:2312.08760v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08760">http://arxiv.org/abs/2312.08760</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08760]] CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental Learning(http://arxiv.org/abs/2312.08760)</code></li>
<li>Summary: <p>Neural Radiance Fields (NeRF) have demonstrated impressive performance in
novel view synthesis. However, NeRF and most of its variants still rely on
traditional complex pipelines to provide extrinsic and intrinsic camera
parameters, such as COLMAP. Recent works, like NeRFmm, BARF, and L2G-NeRF,
directly treat camera parameters as learnable and estimate them through
differential volume rendering. However, these methods work for forward-looking
scenes with slight motions and fail to tackle the rotation scenario in
practice. To overcome this limitation, we propose a novel \underline{c}amera
parameter \underline{f}ree neural radiance field (CF-NeRF), which incrementally
reconstructs 3D representations and recovers the camera parameters inspired by
incremental structure from motion (SfM). Given a sequence of images, CF-NeRF
estimates the camera parameters of images one by one and reconstructs the scene
through initialization, implicit localization, and implicit optimization. To
evaluate our method, we use a challenging real-world dataset NeRFBuster which
provides 12 scenes under complex trajectories. Results demonstrate that CF-NeRF
is robust to camera rotation and achieves state-of-the-art results without
providing prior information and constraints.
</p></li>
</ul>

<h3>Title: Managing the unknown: a survey on Open Set Recognition and tangential areas. (arXiv:2312.08785v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08785">http://arxiv.org/abs/2312.08785</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08785]] Managing the unknown: a survey on Open Set Recognition and tangential areas(http://arxiv.org/abs/2312.08785)</code></li>
<li>Summary: <p>In real-world scenarios classification models are often required to perform
robustly when predicting samples belonging to classes that have not appeared
during its training stage. Open Set Recognition addresses this issue by
devising models capable of detecting unknown classes from samples arriving
during the testing phase, while maintaining a good level of performance in the
classification of samples belonging to known classes. This review
comprehensively overviews the recent literature related to Open Set
Recognition, identifying common practices, limitations, and connections of this
field with other machine learning research areas, such as continual learning,
out-of-distribution detection, novelty detection, and uncertainty estimation.
Our work also uncovers open problems and suggests several research directions
that may motivate and articulate future efforts towards more safe Artificial
Intelligence methods.
</p></li>
</ul>

<h3>Title: Achelous++: Power-Oriented Water-Surface Panoptic Perception Framework on Edge Devices based on Vision-Radar Fusion and Pruning of Heterogeneous Modalities. (arXiv:2312.08851v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08851">http://arxiv.org/abs/2312.08851</a></li>
<li>Code URL: https://github.com/GuanRunwei/Achelous</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08851]] Achelous++: Power-Oriented Water-Surface Panoptic Perception Framework on Edge Devices based on Vision-Radar Fusion and Pruning of Heterogeneous Modalities(http://arxiv.org/abs/2312.08851)</code></li>
<li>Summary: <p>Urban water-surface robust perception serves as the foundation for
intelligent monitoring of aquatic environments and the autonomous navigation
and operation of unmanned vessels, especially in the context of waterway
safety. It is worth noting that current multi-sensor fusion and multi-task
learning models consume substantial power and heavily rely on high-power GPUs
for inference. This contributes to increased carbon emissions, a concern that
runs counter to the prevailing emphasis on environmental preservation and the
pursuit of sustainable, low-carbon urban environments. In light of these
concerns, this paper concentrates on low-power, lightweight, multi-task
panoptic perception through the fusion of visual and 4D radar data, which is
seen as a promising low-cost perception method. We propose a framework named
Achelous++ that facilitates the development and comprehensive evaluation of
multi-task water-surface panoptic perception models. Achelous++ can
simultaneously execute five perception tasks with high speed and low power
consumption, including object detection, object semantic segmentation,
drivable-area segmentation, waterline segmentation, and radar point cloud
semantic segmentation. Furthermore, to meet the demand for developers to
customize models for real-time inference on low-performance devices, a novel
multi-modal pruning strategy known as Heterogeneous-Aware SynFlow (HA-SynFlow)
is proposed. Besides, Achelous++ also supports random pruning at initialization
with different layer-wise sparsity, such as Uniform and Erdos-Renyi-Kernel
(ERK). Overall, our Achelous++ framework achieves state-of-the-art performance
on the WaterScenes benchmark, excelling in both accuracy and power efficiency
compared to other single-task and multi-task models. We release and maintain
the code at https://github.com/GuanRunwei/Achelous.
</p></li>
</ul>

<h3>Title: HeadRecon: High-Fidelity 3D Head Reconstruction from Monocular Video. (arXiv:2312.08863v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08863">http://arxiv.org/abs/2312.08863</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08863]] HeadRecon: High-Fidelity 3D Head Reconstruction from Monocular Video(http://arxiv.org/abs/2312.08863)</code></li>
<li>Summary: <p>Recently, the reconstruction of high-fidelity 3D head models from static
portrait image has made great progress. However, most methods require
multi-view or multi-illumination information, which therefore put forward high
requirements for data acquisition. In this paper, we study the reconstruction
of high-fidelity 3D head models from arbitrary monocular videos. Non-rigid
structure from motion (NRSFM) methods have been widely used to solve such
problems according to the two-dimensional correspondence between different
frames. However, the inaccurate correspondence caused by high-complex hair
structures and various facial expression changes would heavily influence the
reconstruction accuracy. To tackle these problems, we propose a prior-guided
dynamic implicit neural network. Specifically, we design a two-part dynamic
deformation field to transform the current frame space to the canonical one. We
further model the head geometry in the canonical space with a learnable signed
distance field (SDF) and optimize it using the volumetric rendering with the
guidance of two-main head priors to improve the reconstruction accuracy and
robustness. Extensive ablation studies and comparisons with state-of-the-art
methods demonstrate the effectiveness and robustness of our proposed method.
</p></li>
</ul>

<h3>Title: May the Noise be with you: Adversarial Training without Adversarial Examples. (arXiv:2312.08877v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08877">http://arxiv.org/abs/2312.08877</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08877]] May the Noise be with you: Adversarial Training without Adversarial Examples(http://arxiv.org/abs/2312.08877)</code></li>
<li>Summary: <p>In this paper, we investigate the following question: Can we obtain
adversarially-trained models without training on adversarial examples? Our
intuition is that training a model with inherent stochasticity, i.e.,
optimizing the parameters by minimizing a stochastic loss function, yields a
robust expectation function that is non-stochastic. In contrast to related
methods that introduce noise at the input level, our proposed approach
incorporates inherent stochasticity by embedding Gaussian noise within the
layers of the NN model at training time. We model the propagation of noise
through the layers, introducing a closed-form stochastic loss function that
encapsulates a noise variance parameter. Additionally, we contribute a
formalized noise-aware gradient, enabling the optimization of model parameters
while accounting for stochasticity. Our experimental results confirm that the
expectation model of a stochastic architecture trained on benign distribution
is adversarially robust. Interestingly, we find that the impact of the applied
Gaussian noise's standard deviation on both robustness and baseline accuracy
closely mirrors the impact of the noise magnitude employed in adversarial
training. Our work contributes adversarially trained networks using a
completely different approach, with empirically similar robustness to
adversarial training.
</p></li>
</ul>

<h3>Title: Domain Prompt Learning with Quaternion Networks. (arXiv:2312.08878v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08878">http://arxiv.org/abs/2312.08878</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08878]] Domain Prompt Learning with Quaternion Networks(http://arxiv.org/abs/2312.08878)</code></li>
<li>Summary: <p>Prompt learning has emerged as an effective and data-efficient technique in
large Vision-Language Models (VLMs). However, when adapting VLMs to specialized
domains such as remote sensing and medical imaging, domain prompt learning
remains underexplored. While large-scale domain-specific foundation models can
help tackle this challenge, their concentration on a single vision level makes
it challenging to prompt both vision and language modalities. To overcome this,
we propose to leverage domain-specific knowledge from domain-specific
foundation models to transfer the robust recognition ability of VLMs from
generalized to specialized domains, using quaternion networks. Specifically,
the proposed method involves using domain-specific vision features from
domain-specific foundation models to guide the transformation of generalized
contextual embeddings from the language branch into a specialized space within
the quaternion networks. Moreover, we present a hierarchical approach that
generates vision prompt features by analyzing intermodal relationships between
hierarchical language prompt features and domain-specific vision features. In
this way, quaternion networks can effectively mine the intermodal relationships
in the specific domain, facilitating domain-specific vision-language
contrastive learning. Extensive experiments on domain-specific datasets show
that our proposed method achieves new state-of-the-art results in prompt
learning.
</p></li>
</ul>

<h3>Title: UCMCTrack: Multi-Object Tracking with Uniform Camera Motion Compensation. (arXiv:2312.08952v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08952">http://arxiv.org/abs/2312.08952</a></li>
<li>Code URL: https://github.com/corfyi/ucmctrack</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08952]] UCMCTrack: Multi-Object Tracking with Uniform Camera Motion Compensation(http://arxiv.org/abs/2312.08952)</code></li>
<li>Summary: <p>Multi-object tracking (MOT) in video sequences remains a challenging task,
especially in scenarios with significant camera movements. This is because
targets can drift considerably on the image plane, leading to erroneous
tracking outcomes. Addressing such challenges typically requires supplementary
appearance cues or Camera Motion Compensation (CMC). While these strategies are
effective, they also introduce a considerable computational burden, posing
challenges for real-time MOT. In response to this, we introduce UCMCTrack, a
novel motion model-based tracker robust to camera movements. Unlike
conventional CMC that computes compensation parameters frame-by-frame,
UCMCTrack consistently applies the same compensation parameters throughout a
video sequence. It employs a Kalman filter on the ground plane and introduces
the Mapped Mahalanobis Distance (MMD) as an alternative to the traditional
Intersection over Union (IoU) distance measure. By leveraging projected
probability distributions on the ground plane, our approach efficiently
captures motion patterns and adeptly manages uncertainties introduced by
homography projections. Remarkably, UCMCTrack, relying solely on motion cues,
achieves state-of-the-art performance across a variety of challenging datasets,
including MOT17, MOT20, DanceTrack and KITTI, with an exceptional speed of over
1000 FPS on a single CPU. More details and code are available at
https://github.com/corfyi/UCMCTrack
</p></li>
</ul>

<h3>Title: Exploring Transferability for Randomized Smoothing. (arXiv:2312.09020v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09020">http://arxiv.org/abs/2312.09020</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09020]] Exploring Transferability for Randomized Smoothing(http://arxiv.org/abs/2312.09020)</code></li>
<li>Summary: <p>Training foundation models on extensive datasets and then finetuning them on
specific tasks has emerged as the mainstream approach in artificial
intelligence. However, the model robustness, which is a critical aspect for
safety, is often optimized for each specific task rather than at the
pretraining stage. In this paper, we propose a method for pretraining
certifiably robust models that can be readily finetuned for adaptation to a
particular task. A key challenge is dealing with the compromise between
semantic learning and robustness. We address this with a simple yet highly
effective strategy based on significantly broadening the pretraining data
distribution, which is shown to greatly benefit finetuning for downstream
tasks. Through pretraining on a mixture of clean and various noisy images, we
find that surprisingly strong certified accuracy can be achieved even when
finetuning on only clean images. Furthermore, this strategy requires just a
single model to deal with various noise levels, thus substantially reducing
computational costs in relation to previous works that employ multiple models.
Despite using just one model, our method can still yield results that are on
par with, or even superior to, existing multi-model methods.
</p></li>
</ul>

<h3>Title: iComMa: Inverting 3D Gaussians Splatting for Camera Pose Estimation via Comparing and Matching. (arXiv:2312.09031v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09031">http://arxiv.org/abs/2312.09031</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09031]] iComMa: Inverting 3D Gaussians Splatting for Camera Pose Estimation via Comparing and Matching(http://arxiv.org/abs/2312.09031)</code></li>
<li>Summary: <p>We present a method named iComMa to address the 6D pose estimation problem in
computer vision. The conventional pose estimation methods typically rely on the
target's CAD model or necessitate specific network training tailored to
particular object classes. Some existing methods address mesh-free 6D pose
estimation by employing the inversion of a Neural Radiance Field (NeRF), aiming
to overcome the aforementioned constraints. However, it still suffers from
adverse initializations. By contrast, we model the pose estimation as the
problem of inverting the 3D Gaussian Splatting (3DGS) with both the comparing
and matching loss. In detail, a render-and-compare strategy is adopted for the
precise estimation of poses. Additionally, a matching module is designed to
enhance the model's robustness against adverse initializations by minimizing
the distances between 2D keypoints. This framework systematically incorporates
the distinctive characteristics and inherent rationale of render-and-compare
and matching-based approaches. This comprehensive consideration equips the
framework to effectively address a broader range of intricate and challenging
scenarios, including instances with substantial angular deviations, all while
maintaining a high level of prediction accuracy. Experimental results
demonstrate the superior precision and robustness of our proposed jointly
optimized framework when evaluated on synthetic and complex real-world data in
challenging scenarios.
</p></li>
</ul>

<h3>Title: Object Recognition from Scientific Document based on Compartment Refinement Framework. (arXiv:2312.09038v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09038">http://arxiv.org/abs/2312.09038</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09038]] Object Recognition from Scientific Document based on Compartment Refinement Framework(http://arxiv.org/abs/2312.09038)</code></li>
<li>Summary: <p>With the rapid development of the internet in the past decade, it has become
increasingly important to extract valuable information from vast resources
efficiently, which is crucial for establishing a comprehensive digital
ecosystem, particularly in the context of research surveys and comprehension.
The foundation of these tasks focuses on accurate extraction and deep mining of
data from scientific documents, which are essential for building a robust data
infrastructure. However, parsing raw data or extracting data from complex
scientific documents have been ongoing challenges. Current data extraction
methods for scientific documents typically use rule-based (RB) or machine
learning (ML) approaches. However, using rule-based methods can incur high
coding costs for articles with intricate typesetting. Conversely, relying
solely on machine learning methods necessitates annotation work for complex
content types within the scientific document, which can be costly.
Additionally, few studies have thoroughly defined and explored the hierarchical
layout within scientific documents. The lack of a comprehensive definition of
the internal structure and elements of the documents indirectly impacts the
accuracy of text classification and object recognition tasks. From the
perspective of analyzing the standard layout and typesetting used in the
specified publication, we propose a new document layout analysis framework
called CTBR(Compartment &amp; Text Blocks Refinement). Firstly, we define
scientific documents into hierarchical divisions: base domain, compartment, and
text blocks. Next, we conduct an in-depth exploration and classification of the
meanings of text blocks. Finally, we utilize the results of text block
classification to implement object recognition within scientific documents
based on rule-based compartment segmentation.
</p></li>
</ul>

<h3>Title: Improving age prediction: Utilizing LSTM-based dynamic forecasting for data augmentation in multivariate time series analysis. (arXiv:2312.08383v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08383">http://arxiv.org/abs/2312.08383</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08383]] Improving age prediction: Utilizing LSTM-based dynamic forecasting for data augmentation in multivariate time series analysis(http://arxiv.org/abs/2312.08383)</code></li>
<li>Summary: <p>The high dimensionality and complexity of neuroimaging data necessitate large
datasets to develop robust and high-performing deep learning models. However,
the neuroimaging field is notably hampered by the scarcity of such datasets. In
this work, we proposed a data augmentation and validation framework that
utilizes dynamic forecasting with Long Short-Term Memory (LSTM) networks to
enrich datasets. We extended multivariate time series data by predicting the
time courses of independent component networks (ICNs) in both one-step and
recursive configurations. The effectiveness of these augmented datasets was
then compared with the original data using various deep learning models
designed for chronological age prediction tasks. The results suggest that our
approach improves model performance, providing a robust solution to overcome
the challenges presented by the limited size of neuroimaging datasets.
</p></li>
</ul>

<h3>Title: Markov Decision Processes with Noisy State Observation. (arXiv:2312.08536v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08536">http://arxiv.org/abs/2312.08536</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08536]] Markov Decision Processes with Noisy State Observation(http://arxiv.org/abs/2312.08536)</code></li>
<li>Summary: <p>This paper addresses the challenge of a particular class of noisy state
observations in Markov Decision Processes (MDPs), a common issue in various
real-world applications. We focus on modeling this uncertainty through a
confusion matrix that captures the probabilities of misidentifying the true
state. Our primary goal is to estimate the inherent measurement noise, and to
this end, we propose two novel algorithmic approaches. The first, the method of
second-order repetitive actions, is designed for efficient noise estimation
within a finite time window, providing identifiable conditions for system
analysis. The second approach comprises a family of Bayesian algorithms, which
we thoroughly analyze and compare in terms of performance and limitations. We
substantiate our theoretical findings with simulations, demonstrating the
effectiveness of our methods in different scenarios, particularly highlighting
their behavior in environments with varying stationary distributions. Our work
advances the understanding of reinforcement learning in noisy environments,
offering robust techniques for more accurate state estimation in MDPs.
</p></li>
</ul>

<h3>Title: MotherNet: A Foundational Hypernetwork for Tabular Classification. (arXiv:2312.08598v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08598">http://arxiv.org/abs/2312.08598</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08598]] MotherNet: A Foundational Hypernetwork for Tabular Classification(http://arxiv.org/abs/2312.08598)</code></li>
<li>Summary: <p>The advent of Foundation Models is transforming machine learning across many
modalities (e.g., language, images, videos) with prompt engineering replacing
training in many settings. Recent work on tabular data (e.g., TabPFN) hints at
a similar opportunity to build Foundation Models for classification for
numerical data. In this paper, we go one step further and propose a
hypernetwork architecture that we call MotherNet, trained on millions of
classification tasks, that, once prompted with a never-seen-before training set
generates the weights of a trained ``child'' neural-network. Like other
Foundation Models, MotherNet replaces training on specific datasets with
in-context learning through a single forward pass. In contrast to existing
hypernetworks that were either task-specific or trained for relatively
constraint multi-task settings, MotherNet is trained to generate networks to
perform multiclass classification on arbitrary tabular datasets without any
dataset specific gradient descent.
</p>
<p>The child network generated by MotherNet using in-context learning
outperforms neural networks trained using gradient descent on small datasets,
and is competitive with predictions by TabPFN and standard ML methods like
Gradient Boosting. Unlike a direct application of transformer models like
TabPFN, MotherNet generated networks are highly efficient at inference time.
This methodology opens up a new approach to building predictive models on
tabular data that is both efficient and robust, without any dataset-specific
training.
</p></li>
</ul>

<h3>Title: Improve Robustness of Reinforcement Learning against Observation Perturbations via $l_\infty$ Lipschitz Policy Networks. (arXiv:2312.08751v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08751">http://arxiv.org/abs/2312.08751</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08751]] Improve Robustness of Reinforcement Learning against Observation Perturbations via $l_\infty$ Lipschitz Policy Networks(http://arxiv.org/abs/2312.08751)</code></li>
<li>Summary: <p>Deep Reinforcement Learning (DRL) has achieved remarkable advances in
sequential decision tasks. However, recent works have revealed that DRL agents
are susceptible to slight perturbations in observations. This vulnerability
raises concerns regarding the effectiveness and robustness of deploying such
agents in real-world applications. In this work, we propose a novel robust
reinforcement learning method called SortRL, which improves the robustness of
DRL policies against observation perturbations from the perspective of the
network architecture. We employ a novel architecture for the policy network
that incorporates global $l_\infty$ Lipschitz continuity and provide a
convenient method to enhance policy robustness based on the output margin.
Besides, a training framework is designed for SortRL, which solves given tasks
while maintaining robustness against $l_\infty$ bounded perturbations on the
observations. Several experiments are conducted to evaluate the effectiveness
of our method, including classic control tasks and video games. The results
demonstrate that SortRL achieves state-of-the-art robustness performance
against different perturbation strength.
</p></li>
</ul>

<h3>Title: Learning from Polar Representation: An Extreme-Adaptive Model for Long-Term Time Series Forecasting. (arXiv:2312.08763v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08763">http://arxiv.org/abs/2312.08763</a></li>
<li>Code URL: https://github.com/davidanastasiu/dan</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08763]] Learning from Polar Representation: An Extreme-Adaptive Model for Long-Term Time Series Forecasting(http://arxiv.org/abs/2312.08763)</code></li>
<li>Summary: <p>In the hydrology field, time series forecasting is crucial for efficient
water resource management, improving flood and drought control and increasing
the safety and quality of life for the general population. However, predicting
long-term streamflow is a complex task due to the presence of extreme events.
It requires the capture of long-range dependencies and the modeling of rare but
important extreme values. Existing approaches often struggle to tackle these
dual challenges simultaneously. In this paper, we specifically delve into these
issues and propose Distance-weighted Auto-regularized Neural network (DAN), a
novel extreme-adaptive model for long-range forecasting of stremflow enhanced
by polar representation learning. DAN utilizes a distance-weighted multi-loss
mechanism and stackable blocks to dynamically refine indicator sequences from
exogenous data, while also being able to handle uni-variate time-series by
employing Gaussian Mixture probability modeling to improve robustness to severe
events. We also introduce Kruskal-Wallis sampling and gate control vectors to
handle imbalanced extreme data. On four real-life hydrologic streamflow
datasets, we demonstrate that DAN significantly outperforms both
state-of-the-art hydrologic time series prediction methods and general methods
designed for long-term time series prediction.
</p></li>
</ul>

<h3>Title: ERASE: Error-Resilient Representation Learning on Graphs for Label Noise Tolerance. (arXiv:2312.08852v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08852">http://arxiv.org/abs/2312.08852</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08852]] ERASE: Error-Resilient Representation Learning on Graphs for Label Noise Tolerance(http://arxiv.org/abs/2312.08852)</code></li>
<li>Summary: <p>Deep learning has achieved remarkable success in graph-related tasks, yet
this accomplishment heavily relies on large-scale high-quality annotated
datasets. However, acquiring such datasets can be cost-prohibitive, leading to
the practical use of labels obtained from economically efficient sources such
as web searches and user tags. Unfortunately, these labels often come with
noise, compromising the generalization performance of deep networks. To tackle
this challenge and enhance the robustness of deep learning models against label
noise in graph-based tasks, we propose a method called ERASE (Error-Resilient
representation learning on graphs for lAbel noiSe tolerancE). The core idea of
ERASE is to learn representations with error tolerance by maximizing coding
rate reduction. Particularly, we introduce a decoupled label propagation method
for learning representations. Before training, noisy labels are pre-corrected
through structural denoising. During training, ERASE combines prototype
pseudo-labels with propagated denoised labels and updates representations with
error resilience, which significantly improves the generalization performance
in node classification. The proposed method allows us to more effectively
withstand errors caused by mislabeled nodes, thereby strengthening the
robustness of deep networks in handling noisy graph data. Extensive
experimental results show that our method can outperform multiple baselines
with clear margins in broad noise levels and enjoy great scalability. Codes are
released at https://github.com/eraseai/erase.
</p></li>
</ul>

<h3>Title: Multi-Modal Learning-based Reconstruction of High-Resolution Spatial Wind Speed Fields. (arXiv:2312.08933v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08933">http://arxiv.org/abs/2312.08933</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08933]] Multi-Modal Learning-based Reconstruction of High-Resolution Spatial Wind Speed Fields(http://arxiv.org/abs/2312.08933)</code></li>
<li>Summary: <p>Wind speed at sea surface is a key quantity for a variety of scientific
applications and human activities. Due to the non-linearity of the phenomenon,
a complete description of such variable is made infeasible on both the small
scale and large spatial extents. Methods relying on Data Assimilation
techniques, despite being the state-of-the-art for Numerical Weather
Prediction, can not provide the reconstructions with a spatial resolution that
can compete with satellite imagery. In this work we propose a framework based
on Variational Data Assimilation and Deep Learning concepts. This framework is
applied to recover rich-in-time, high-resolution information on sea surface
wind speed. We design our experiments using synthetic wind data and different
sampling schemes for high-resolution and low-resolution versions of original
data to emulate the real-world scenario of spatio-temporally heterogeneous
observations. Extensive numerical experiments are performed to assess
systematically the impact of low and high-resolution wind fields and in-situ
observations on the model reconstruction performance. We show that in-situ
observations with richer temporal resolution represent an added value in terms
of the model reconstruction performance. We show how a multi-modal approach,
that explicitly informs the model about the heterogeneity of the available
observations, can improve the reconstruction task by exploiting the
complementary information in spatial and local point-wise data. To conclude, we
propose an analysis to test the robustness of the chosen framework against
phase delay and amplitude biases in low-resolution data and against
interruptions of in-situ observations supply at evaluation time
</p></li>
</ul>

<h3>Title: LSTM Network Analysis of Vehicle-Type Fatalities on Great Britain's Roads. (arXiv:2312.08948v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08948">http://arxiv.org/abs/2312.08948</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08948]] LSTM Network Analysis of Vehicle-Type Fatalities on Great Britain's Roads(http://arxiv.org/abs/2312.08948)</code></li>
<li>Summary: <p>This study harnesses the predictive capabilities of Long Short-Term Memory
(LSTM) networks to analyse and predict road traffic accidents in Great Britain.
It addresses the challenge of traffic accident forecasting, which is paramount
for devising effective preventive measures. We utilised an extensive dataset
encompassing reported collisions, casualties, and vehicles involvements from
1926 to 2022, provided by the Department for Transport (DfT). The data
underwent stringent processing to rectify missing values and normalise
features, ensuring robust LSTM network input.
</p></li>
</ul>

<h3>Title: Uncertainty in GNN Learning Evaluations: A Comparison Between Measures for Quantifying Randomness in GNN Community Detection. (arXiv:2312.09015v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09015">http://arxiv.org/abs/2312.09015</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09015]] Uncertainty in GNN Learning Evaluations: A Comparison Between Measures for Quantifying Randomness in GNN Community Detection(http://arxiv.org/abs/2312.09015)</code></li>
<li>Summary: <p>(1) The enhanced capability of Graph Neural Networks (GNNs) in unsupervised
community detection of clustered nodes is attributed to their capacity to
encode both the connectivity and feature information spaces of graphs. The
identification of latent communities holds practical significance in various
domains, from social networks to genomics. Current real-world performance
benchmarks are perplexing due to the multitude of decisions influencing GNN
evaluations for this task. (2) Three metrics are compared to assess the
consistency of algorithm rankings in the presence of randomness. The
consistency and quality of performance between the results under a
hyperparameter optimisation with the default hyperparameters is evaluated. (3)
The results compare hyperparameter optimisation with default hyperparameters,
revealing a significant performance loss when neglecting hyperparameter
investigation. A comparison of metrics indicates that ties in ranks can
substantially alter the quantification of randomness. (4) Ensuring adherence to
the same evaluation criteria may result in notable differences in the reported
performance of methods for this task. The $W$ Randomness coefficient, based on
the Wasserstein distance, is identified as providing the most robust assessment
of randomness.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: EVP: Enhanced Visual Perception using Inverse Multi-Attentive Feature Refinement and Regularized Image-Text Alignment. (arXiv:2312.08548v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08548">http://arxiv.org/abs/2312.08548</a></li>
<li>Code URL: https://github.com/lavreniuk/evp</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08548]] EVP: Enhanced Visual Perception using Inverse Multi-Attentive Feature Refinement and Regularized Image-Text Alignment(http://arxiv.org/abs/2312.08548)</code></li>
<li>Summary: <p>This work presents the network architecture EVP (Enhanced Visual Perception).
EVP builds on the previous work VPD which paved the way to use the Stable
Diffusion network for computer vision tasks. We propose two major enhancements.
First, we develop the Inverse Multi-Attentive Feature Refinement (IMAFR) module
which enhances feature learning capabilities by aggregating spatial information
from higher pyramid levels. Second, we propose a novel image-text alignment
module for improved feature extraction of the Stable Diffusion backbone. The
resulting architecture is suitable for a wide variety of tasks and we
demonstrate its performance in the context of single-image depth estimation
with a specialized decoder using classification-based bins and referring
segmentation with an off-the-shelf decoder. Comprehensive experiments conducted
on established datasets show that EVP achieves state-of-the-art results in
single-image depth estimation for indoor (NYU Depth v2, 11.8% RMSE improvement
over VPD) and outdoor (KITTI) environments, as well as referring segmentation
(RefCOCO, 2.53 IoU improvement over ReLA). The code and pre-trained models are
publicly available at https://github.com/Lavreniuk/EVP.
</p></li>
</ul>

<h3>Title: VMT-Adapter: Parameter-Efficient Transfer Learning for Multi-Task Dense. (arXiv:2312.08733v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08733">http://arxiv.org/abs/2312.08733</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08733]] VMT-Adapter: Parameter-Efficient Transfer Learning for Multi-Task Dense(http://arxiv.org/abs/2312.08733)</code></li>
<li>Summary: <p>Large-scale pre-trained models have achieved remarkable success in various
computer vision tasks. A standard approach to leverage these models is to
fine-tune all model parameters for downstream tasks, which poses challenges in
terms of computational and storage costs. Recently, inspired by Natural
Language Processing (NLP), parameter-efficient transfer learning has been
successfully applied to vision tasks. However, most existing techniques
primarily focus on single-task adaptation, and despite limited research on
multi-task adaptation, these methods often exhibit suboptimal training and
inference efficiency. In this paper, we first propose an once-for-all Vision
Multi-Task Adapter (VMT-Adapter), which strikes approximately O(1) training and
inference efficiency w.r.t task number. Concretely, VMT-Adapter shares the
knowledge from multiple tasks to enhance cross-task interaction while preserves
task-specific knowledge via independent knowledge extraction modules. Notably,
since task-specific modules require few parameters, VMT-Adapter can handle an
arbitrary number of tasks with a negligible increase of trainable parameters.
We also propose VMT-Adapter-Lite, which further reduces the trainable
parameters by learning shared parameters between down- and up-projections.
Extensive experiments on four dense scene understanding tasks demonstrate the
superiority of VMT-Adapter(-Lite), achieving a 3.96%(1.34%) relative
improvement compared to single-task full fine-tuning, while utilizing merely
~1% (0.36%) trainable parameters of the pre-trained model.
</p></li>
</ul>

<h3>Title: LEMON: Learning 3D Human-Object Interaction Relation from 2D Images. (arXiv:2312.08963v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08963">http://arxiv.org/abs/2312.08963</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08963]] LEMON: Learning 3D Human-Object Interaction Relation from 2D Images(http://arxiv.org/abs/2312.08963)</code></li>
<li>Summary: <p>Learning 3D human-object interaction relation is pivotal to embodied AI and
interaction modeling. Most existing methods approach the goal by learning to
predict isolated interaction elements, e.g., human contact, object affordance,
and human-object spatial relation, primarily from the perspective of either the
human or the object. Which underexploit certain correlations between the
interaction counterparts (human and object), and struggle to address the
uncertainty in interactions. Actually, objects' functionalities potentially
affect humans' interaction intentions, which reveals what the interaction is.
Meanwhile, the interacting humans and objects exhibit matching geometric
structures, which presents how to interact. In light of this, we propose
harnessing these inherent correlations between interaction counterparts to
mitigate the uncertainty and jointly anticipate the above interaction elements
in 3D space. To achieve this, we present LEMON (LEarning 3D huMan-Object
iNteraction relation), a unified model that mines interaction intentions of the
counterparts and employs curvatures to guide the extraction of geometric
correlations, combining them to anticipate the interaction elements. Besides,
the 3D Interaction Relation dataset (3DIR) is collected to serve as the test
bed for training and evaluation. Extensive experiments demonstrate the
superiority of LEMON over methods estimating each element in isolation.
</p></li>
</ul>

<h3>Title: ComOM at VLSP 2023: A Dual-Stage Framework with BERTology and Unified Multi-Task Instruction Tuning Model for Vietnamese Comparative Opinion Mining. (arXiv:2312.09000v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09000">http://arxiv.org/abs/2312.09000</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09000]] ComOM at VLSP 2023: A Dual-Stage Framework with BERTology and Unified Multi-Task Instruction Tuning Model for Vietnamese Comparative Opinion Mining(http://arxiv.org/abs/2312.09000)</code></li>
<li>Summary: <p>The ComOM shared task aims to extract comparative opinions from product
reviews in Vietnamese language. There are two sub-tasks, including (1)
Comparative Sentence Identification (CSI) and (2) Comparative Element
Extraction (CEE). The first task is to identify whether the input is a
comparative review, and the purpose of the second task is to extract the
quintuplets mentioned in the comparative review. To address this task, our team
proposes a two-stage system based on fine-tuning a BERTology model for the CSI
task and unified multi-task instruction tuning for the CEE task. Besides, we
apply the simple data augmentation technique to increase the size of the
dataset for training our model in the second stage. Experimental results show
that our approach outperforms the other competitors and has achieved the top
score on the official private test.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: CLIP-guided Federated Learning on Heterogeneous and Long-Tailed Data. (arXiv:2312.08648v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08648">http://arxiv.org/abs/2312.08648</a></li>
<li>Code URL: https://github.com/shijiangming1/clip2fl</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08648]] CLIP-guided Federated Learning on Heterogeneous and Long-Tailed Data(http://arxiv.org/abs/2312.08648)</code></li>
<li>Summary: <p>Federated learning (FL) provides a decentralized machine learning paradigm
where a server collaborates with a group of clients to learn a global model
without accessing the clients' data. User heterogeneity is a significant
challenge for FL, which together with the class-distribution imbalance further
enhances the difficulty of FL. Great progress has been made in large
vision-language models, such as Contrastive Language-Image Pre-training (CLIP),
which paves a new way for image classification and object recognition. Inspired
by the success of CLIP on few-shot and zero-shot learning, we use CLIP to
optimize the federated learning between server and client models under its
vision-language supervision. It is promising to mitigate the user heterogeneity
and class-distribution balance due to the powerful cross-modality
representation and rich open-vocabulary prior knowledge. In this paper, we
propose the CLIP-guided FL (CLIP2FL) method on heterogeneous and long-tailed
data. In CLIP2FL, the knowledge of the off-the-shelf CLIP model is transferred
to the client-server models, and a bridge is built between the client and
server. Specifically, for client-side learning, knowledge distillation is
conducted between client models and CLIP to improve the ability of client-side
feature representation. For server-side learning, in order to mitigate the
heterogeneity and class-distribution imbalance, we generate federated features
to retrain the server model. A prototype contrastive learning with the
supervision of the text encoder of CLIP is introduced to generate federated
features depending on the client-side gradients, and they are used to retrain a
balanced server classifier.
</p></li>
</ul>

<h3>Title: Contractive error feedback for gradient compression. (arXiv:2312.08538v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08538">http://arxiv.org/abs/2312.08538</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08538]] Contractive error feedback for gradient compression(http://arxiv.org/abs/2312.08538)</code></li>
<li>Summary: <p>On-device memory concerns in distributed deep learning have become severe due
to (i) the growth of model size in multi-GPU training, and (ii) the wide
adoption of deep neural networks for federated learning on IoT devices which
have limited storage. In such settings, communication efficient optimization
methods are attractive alternatives, however they still struggle with memory
issues. To tackle these challenges, we propose an communication efficient
method called contractive error feedback (ConEF). As opposed to SGD with
error-feedback (EFSGD) that inefficiently manages memory, ConEF obtains the
sweet spot of convergence and memory usage, and achieves communication
efficiency by leveraging biased and all-reducable gradient compression. We
empirically validate ConEF on various learning tasks that include image
classification, language modeling, and machine translation and observe that
ConEF saves 80\% - 90\% of the extra memory in EFSGD with almost no loss on
test performance, while also achieving 1.3x - 5x speedup of SGD. Through our
work, we also demonstrate the feasibility and convergence of ConEF to clear up
the theoretical barrier of integrating ConEF to popular memory efficient
frameworks such as ZeRO-3.
</p></li>
</ul>

<h3>Title: FedSSA: Semantic Similarity-based Aggregation for Efficient Model-Heterogeneous Personalized Federated Learning. (arXiv:2312.09006v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09006">http://arxiv.org/abs/2312.09006</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09006]] FedSSA: Semantic Similarity-based Aggregation for Efficient Model-Heterogeneous Personalized Federated Learning(http://arxiv.org/abs/2312.09006)</code></li>
<li>Summary: <p>Federated learning (FL) is a privacy-preserving collaboratively machine
learning paradigm. Traditional FL requires all data owners (a.k.a. FL clients)
to train the same local model. This design is not well-suited for scenarios
involving data and/or system heterogeneity. Model-Heterogeneous Personalized FL
(MHPFL) has emerged to address this challenge. Existing MHPFL approaches often
rely on having a public dataset with the same nature of the learning task, or
incur high computation and communication costs. To address these limitations,
we propose the Federated Semantic Similarity Aggregation (FedSSA) approach,
which splits each client's model into a heterogeneous (structure-different)
feature extractor and a homogeneous (structure-same) classification header. It
performs local-to-global knowledge transfer via semantic similarity-based
header parameter aggregation. In addition, global-to-local knowledge transfer
is achieved via an adaptive parameter stabilization strategy which fuses the
seen-class parameters of historical local headers with that of the latest
global header for each client. In this way, FedSSA does not rely on public
datasets, while only requiring partial header parameter transmission (thereby
saving costs). Theoretical analysis proves the convergence of FedSSA. Extensive
experiments demonstrate that FedSSA achieves up to $3.62 \times\%$ higher
accuracy, $15.54$ times higher communication efficiency, and $15.52 \times$
higher computational efficiency compared to 7 state-of-the-art MHPFL baselines.
</p></li>
</ul>

<h3>Title: A Framework for Exploring Federated Community Detection. (arXiv:2312.09023v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09023">http://arxiv.org/abs/2312.09023</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09023]] A Framework for Exploring Federated Community Detection(http://arxiv.org/abs/2312.09023)</code></li>
<li>Summary: <p>Federated Learning is machine learning in the context of a network of clients
whilst maintaining data residency and/or privacy constraints. Community
detection is the unsupervised discovery of clusters of nodes within
graph-structured data. The intersection of these two fields uncovers much
opportunity, but also challenge. For example, it adds complexity due to missing
connectivity information between privately held graphs. In this work, we
explore the potential of federated community detection by conducting initial
experiments across a range of existing datasets that showcase the gap in
performance introduced by the distributed data. We demonstrate that isolated
models would benefit from collaboration establishing a framework for
investigating challenges within this domain. The intricacies of these research
frontiers are discussed alongside proposed solutions to these issues.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Fair Active Learning in Low-Data Regimes. (arXiv:2312.08559v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08559">http://arxiv.org/abs/2312.08559</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08559]] Fair Active Learning in Low-Data Regimes(http://arxiv.org/abs/2312.08559)</code></li>
<li>Summary: <p>In critical machine learning applications, ensuring fairness is essential to
avoid perpetuating social inequities. In this work, we address the challenges
of reducing bias and improving accuracy in data-scarce environments, where the
cost of collecting labeled data prohibits the use of large, labeled datasets.
In such settings, active learning promises to maximize marginal accuracy gains
of small amounts of labeled data. However, existing applications of active
learning for fairness fail to deliver on this, typically requiring large
labeled datasets, or failing to ensure the desired fairness tolerance is met on
the population distribution.
</p>
<p>To address such limitations, we introduce an innovative active learning
framework that combines an exploration procedure inspired by posterior sampling
with a fair classification subroutine. We demonstrate that this framework
performs effectively in very data-scarce regimes, maximizing accuracy while
satisfying fairness constraints with high probability. We evaluate our proposed
approach using well-established real-world benchmark datasets and compare it
against state-of-the-art methods, demonstrating its effectiveness in producing
fair models, and improvement over existing methods.
</p></li>
</ul>

<h3>Title: Mitigating Label Bias in Machine Learning: Fairness through Confident Learning. (arXiv:2312.08749v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08749">http://arxiv.org/abs/2312.08749</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08749]] Mitigating Label Bias in Machine Learning: Fairness through Confident Learning(http://arxiv.org/abs/2312.08749)</code></li>
<li>Summary: <p>Discrimination can occur when the underlying unbiased labels are overwritten
by an agent with potential bias, resulting in biased datasets that unfairly
harm specific groups and cause classifiers to inherit these biases. In this
paper, we demonstrate that despite only having access to the biased labels, it
is possible to eliminate bias by filtering the fairest instances within the
framework of confident learning. In the context of confident learning, low
self-confidence usually indicates potential label errors; however, this is not
always the case. Instances, particularly those from underrepresented groups,
might exhibit low confidence scores for reasons other than labeling errors. To
address this limitation, our approach employs truncation of the confidence
score and extends the confidence interval of the probabilistic threshold.
Additionally, we incorporate with co-teaching paradigm for providing a more
robust and reliable selection of fair instances and effectively mitigating the
adverse effects of biased labels. Through extensive experimentation and
evaluation of various datasets, we demonstrate the efficacy of our approach in
promoting fairness and reducing the impact of label bias in machine learning
models.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Simplicial Representation Learning with Neural $k$-forms. (arXiv:2312.08515v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08515">http://arxiv.org/abs/2312.08515</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08515]] Simplicial Representation Learning with Neural $k$-forms(http://arxiv.org/abs/2312.08515)</code></li>
<li>Summary: <p>Geometric deep learning extends deep learning to incorporate information
about the geometry and topology data, especially in complex domains like
graphs. Despite the popularity of message passing in this field, it has
limitations such as the need for graph rewiring, ambiguity in interpreting
data, and over-smoothing. In this paper, we take a different approach, focusing
on leveraging geometric information from simplicial complexes embedded in
$\mathbb{R}^n$ using node coordinates. We use differential k-forms in
\mathbb{R}^n to create representations of simplices, offering interpretability
and geometric consistency without message passing. This approach also enables
us to apply differential geometry tools and achieve universal approximation.
Our method is efficient, versatile, and applicable to various input complexes,
including graphs, simplicial complexes, and cell complexes. It outperforms
existing message passing neural networks in harnessing information from
geometrical graphs with node features serving as coordinates.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: An Explainable Machine Learning Framework for the Accurate Diagnosis of Ovarian Cancer. (arXiv:2312.08381v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08381">http://arxiv.org/abs/2312.08381</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08381]] An Explainable Machine Learning Framework for the Accurate Diagnosis of Ovarian Cancer(http://arxiv.org/abs/2312.08381)</code></li>
<li>Summary: <p>Ovarian cancer (OC) is one of the most prevalent types of cancer in women.
Early and accurate diagnosis is crucial for the survival of the patients.
However, the majority of women are diagnosed in advanced stages due to the lack
of effective biomarkers and accurate screening tools. While previous studies
sought a common biomarker, our study suggests different biomarkers for the
premenopausal and postmenopausal populations. This can provide a new
perspective in the search for novel predictors for the effective diagnosis of
OC. Lack of explainability is one major limitation of current AI systems. The
stochastic nature of the ML algorithms raises concerns about the reliability of
the system as it is difficult to interpret the reasons behind the decisions. To
increase the trustworthiness and accountability of the diagnostic system as
well as to provide transparency and explanations behind the predictions,
explainable AI has been incorporated into the ML framework. SHAP is employed to
quantify the contributions of the selected biomarkers and determine the most
discriminative features. A hybrid decision support system has been established
that can eliminate the bottlenecks caused by the black-box nature of the ML
algorithms providing a safe and trustworthy AI tool. The diagnostic accuracy
obtained from the proposed system outperforms the existing methods as well as
the state-of-the-art ROMA algorithm by a substantial margin which signifies its
potential to be an effective tool in the differential diagnosis of OC.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models. (arXiv:2312.08459v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08459">http://arxiv.org/abs/2312.08459</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08459]] FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models(http://arxiv.org/abs/2312.08459)</code></li>
<li>Summary: <p>We introduce FaceTalk, a novel generative approach designed for synthesizing
high-fidelity 3D motion sequences of talking human heads from input audio
signal. To capture the expressive, detailed nature of human heads, including
hair, ears, and finer-scale eye movements, we propose to couple speech signal
with the latent space of neural parametric head models to create high-fidelity,
temporally coherent motion sequences. We propose a new latent diffusion model
for this task, operating in the expression space of neural parametric head
models, to synthesize audio-driven realistic head sequences. In the absence of
a dataset with corresponding NPHM expressions to audio, we optimize for these
correspondences to produce a dataset of temporally-optimized NPHM expressions
fit to audio-video recordings of people talking. To the best of our knowledge,
this is the first work to propose a generative approach for realistic and
high-quality motion synthesis of volumetric human heads, representing a
significant advancement in the field of audio-driven 3D animation. Notably, our
approach stands out in its ability to generate plausible motion sequences that
can produce high-fidelity head animation coupled with the NPHM shape space. Our
experimental results substantiate the effectiveness of FaceTalk, consistently
achieving superior and visually natural motion, encompassing diverse facial
expressions and styles, outperforming existing methods by 75% in perceptual
user study evaluation.
</p></li>
</ul>

<h3>Title: Efficient-NeRF2NeRF: Streamlining Text-Driven 3D Editing with Multiview Correspondence-Enhanced Diffusion Models. (arXiv:2312.08563v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08563">http://arxiv.org/abs/2312.08563</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08563]] Efficient-NeRF2NeRF: Streamlining Text-Driven 3D Editing with Multiview Correspondence-Enhanced Diffusion Models(http://arxiv.org/abs/2312.08563)</code></li>
<li>Summary: <p>The advancement of text-driven 3D content editing has been blessed by the
progress from 2D generative diffusion models. However, a major obstacle
hindering the widespread adoption of 3D content editing is its time-intensive
processing. This challenge arises from the iterative and refining steps
required to achieve consistent 3D outputs from 2D image-based generative
models. Recent state-of-the-art methods typically require optimization time
ranging from tens of minutes to several hours to edit a 3D scene using a single
GPU. In this work, we propose that by incorporating correspondence
regularization into diffusion models, the process of 3D editing can be
significantly accelerated. This approach is inspired by the notion that the
estimated samples during diffusion should be multiview-consistent during the
diffusion generation process. By leveraging this multiview consistency, we can
edit 3D content at a much faster speed. In most scenarios, our proposed
technique brings a 10$\times$ speed-up compared to the baseline method and
completes the editing of a 3D scene in 2 minutes with comparable quality.
</p></li>
</ul>

<h3>Title: Joint2Human: High-quality 3D Human Generation via Compact Spherical Embedding of 3D Joints. (arXiv:2312.08591v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08591">http://arxiv.org/abs/2312.08591</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08591]] Joint2Human: High-quality 3D Human Generation via Compact Spherical Embedding of 3D Joints(http://arxiv.org/abs/2312.08591)</code></li>
<li>Summary: <p>3D human generation is increasingly significant in various applications.
However, the direct use of 2D generative methods in 3D generation often results
in significant loss of local details, while methods that reconstruct geometry
from generated images struggle with global view consistency. In this work, we
introduce Joint2Human, a novel method that leverages 2D diffusion models to
generate detailed 3D human geometry directly, ensuring both global structure
and local details. To achieve this, we employ the Fourier occupancy field (FOF)
representation, enabling the direct production of 3D shapes as preliminary
results using 2D generative models. With the proposed high-frequency enhancer
and the multi-view recarving strategy, our method can seamlessly integrate the
details from different views into a uniform global shape.To better utilize the
3D human prior and enhance control over the generated geometry, we introduce a
compact spherical embedding of 3D joints. This allows for effective application
of pose guidance during the generation process. Additionally, our method is
capable of generating 3D humans guided by textual inputs. Our experimental
results demonstrate the capability of our method to ensure global structure,
local details, high resolution, and low computational cost, simultaneously.
More results and code can be found on our project page at
<a href="http://cic.tju.edu.cn/faculty/likun/projects/Joint2Human.">this http URL</a>
</p></li>
</ul>

<h3>Title: GOEnFusion: Gradient Origin Encodings for 3D Forward Diffusion Models. (arXiv:2312.08744v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08744">http://arxiv.org/abs/2312.08744</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08744]] GOEnFusion: Gradient Origin Encodings for 3D Forward Diffusion Models(http://arxiv.org/abs/2312.08744)</code></li>
<li>Summary: <p>The recently introduced Forward-Diffusion method allows to train a 3D
diffusion model using only 2D images for supervision. However, it does not
easily generalise to different 3D representations and requires a
computationally expensive auto-regressive sampling process to generate the
underlying 3D scenes. In this paper, we propose GOEn: Gradient Origin Encoding
(pronounced "gone"). GOEn can encode input images into any type of 3D
representation without the need to use a pre-trained image feature extractor.
It can also handle single, multiple or no source view(s) alike, by design, and
tries to maximise the information transfer from the views to the encodings. Our
proposed GOEnFusion model pairs GOEn encodings with a realisation of the
Forward-Diffusion model which addresses the limitations of the vanilla
Forward-Diffusion realisation. We evaluate how much information the GOEn
mechanism transfers to the encoded representations, and how well it captures
the prior distribution over the underlying 3D scenes, through the lens of a
partial AutoEncoder. Lastly, the efficacy of the GOEnFusion model is evaluated
on the recently proposed OmniObject3D dataset while comparing to the
state-of-the-art Forward and non-Forward-Diffusion models and other 3D
generative models.
</p></li>
</ul>

<h3>Title: DreamDrone. (arXiv:2312.08746v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08746">http://arxiv.org/abs/2312.08746</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08746]] DreamDrone(http://arxiv.org/abs/2312.08746)</code></li>
<li>Summary: <p>We introduce DreamDrone, an innovative method for generating unbounded
flythrough scenes from textual prompts. Central to our method is a novel
feature-correspondence-guidance diffusion process, which utilizes the strong
correspondence of intermediate features in the diffusion model. Leveraging this
guidance strategy, we further propose an advanced technique for editing the
intermediate latent code, enabling the generation of subsequent novel views
with geometric consistency. Extensive experiments reveal that DreamDrone
significantly surpasses existing methods, delivering highly authentic scene
generation with exceptional visual quality. This approach marks a significant
step in zero-shot perpetual view generation from textual prompts, enabling the
creation of diverse scenes, including natural landscapes like oases and caves,
as well as complex urban settings such as Lego-style street views. Our code is
publicly available.
</p></li>
</ul>

<h3>Title: UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation. (arXiv:2312.08754v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08754">http://arxiv.org/abs/2312.08754</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08754]] UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation(http://arxiv.org/abs/2312.08754)</code></li>
<li>Summary: <p>Recent advancements in text-to-3D generation technology have significantly
advanced the conversion of textual descriptions into imaginative
well-geometrical and finely textured 3D objects. Despite these developments, a
prevalent limitation arises from the use of RGB data in diffusion or
reconstruction models, which often results in models with inherent lighting and
shadows effects that detract from their realism, thereby limiting their
usability in applications that demand accurate relighting capabilities. To
bridge this gap, we present UniDream, a text-to-3D generation framework by
incorporating unified diffusion priors. Our approach consists of three main
components: (1) a dual-phase training process to get albedo-normal aligned
multi-view diffusion and reconstruction models, (2) a progressive generation
procedure for geometry and albedo-textures based on Score Distillation Sample
(SDS) using the trained reconstruction and diffusion models, and (3) an
innovative application of SDS for finalizing PBR generation while keeping a
fixed albedo based on Stable Diffusion model. Extensive evaluations demonstrate
that UniDream surpasses existing methods in generating 3D objects with clearer
albedo textures, smoother surfaces, enhanced realism, and superior relighting
capabilities.
</p></li>
</ul>

<h3>Title: Local Conditional Controlling for Text-to-Image Diffusion Models. (arXiv:2312.08768v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08768">http://arxiv.org/abs/2312.08768</a></li>
<li>Code URL: https://github.com/yiboozhao/local-control</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08768]] Local Conditional Controlling for Text-to-Image Diffusion Models(http://arxiv.org/abs/2312.08768)</code></li>
<li>Summary: <p>Diffusion models have exhibited impressive prowess in the text-to-image task.
Recent methods add image-level controls, e.g., edge and depth maps, to
manipulate the generation process together with text prompts to obtain desired
images. This controlling process is globally operated on the entire image,
which limits the flexibility of control regions. In this paper, we introduce a
new simple yet practical task setting: local control. It focuses on controlling
specific local areas according to user-defined image conditions, where the rest
areas are only conditioned by the original text prompt. This manner allows the
users to flexibly control the image generation in a fine-grained way. However,
it is non-trivial to achieve this goal. The naive manner of directly adding
local conditions may lead to the local control dominance problem. To mitigate
this problem, we propose a training-free method that leverages the updates of
noised latents and parameters in the cross-attention map during the denosing
process to promote concept generation in non-control areas. Moreover, we use
feature mask constraints to mitigate the degradation of synthesized image
quality caused by information differences inside and outside the local control
area. Extensive experiments demonstrate that our method can synthesize
high-quality images to the prompt under local control conditions. Code is
available at https://github.com/YibooZhao/Local-Control.
</p></li>
</ul>

<h3>Title: Guided Diffusion from Self-Supervised Diffusion Features. (arXiv:2312.08825v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08825">http://arxiv.org/abs/2312.08825</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08825]] Guided Diffusion from Self-Supervised Diffusion Features(http://arxiv.org/abs/2312.08825)</code></li>
<li>Summary: <p>Guidance serves as a key concept in diffusion models, yet its effectiveness
is often limited by the need for extra data annotation or classifier
pretraining. That is why guidance was harnessed from self-supervised learning
backbones, like DINO. However, recent studies have revealed that the feature
representation derived from diffusion model itself is discriminative for
numerous downstream tasks as well, which prompts us to propose a framework to
extract guidance from, and specifically for, diffusion models. Our research has
yielded several significant contributions. Firstly, the guidance signals from
diffusion models are on par with those from class-conditioned diffusion models.
Secondly, feature regularization, when based on the Sinkhorn-Knopp algorithm,
can further enhance feature discriminability in comparison to unconditional
diffusion models. Thirdly, we have constructed an online training approach that
can concurrently derive guidance from diffusion models for diffusion models.
Lastly, we have extended the application of diffusion models along the constant
velocity path of ODE to achieve a more favorable balance between sampling steps
and fidelity. The performance of our methods has been outstanding,
outperforming related baseline comparisons in large-resolution datasets, such
as ImageNet256, ImageNet256-100 and LSUN-Churches. Our code will be released.
</p></li>
</ul>

<h3>Title: Diffusion-C: Unveiling the Generative Challenges of Diffusion Models through Corrupted Data. (arXiv:2312.08843v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08843">http://arxiv.org/abs/2312.08843</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08843]] Diffusion-C: Unveiling the Generative Challenges of Diffusion Models through Corrupted Data(http://arxiv.org/abs/2312.08843)</code></li>
<li>Summary: <p>In our contemporary academic inquiry, we present "Diffusion-C," a
foundational methodology to analyze the generative restrictions of Diffusion
Models, particularly those akin to GANs, DDPM, and DDIM. By employing input
visual data that has been subjected to a myriad of corruption modalities and
intensities, we elucidate the performance characteristics of those Diffusion
Models. The noise component takes center stage in our analysis, hypothesized to
be a pivotal element influencing the mechanics of deep learning systems. In our
rigorous expedition utilizing Diffusion-C, we have discerned the following
critical observations: (I) Within the milieu of generative models under the
Diffusion taxonomy, DDPM emerges as a paragon, consistently exhibiting superior
performance metrics. (II) Within the vast spectrum of corruption frameworks,
the fog and fractal corruptions notably undermine the functional robustness of
both DDPM and DDIM. (III) The vulnerability of Diffusion Models to these
particular corruptions is significantly influenced by topological and
statistical similarities, particularly concerning the alignment between mean
and variance. This scholarly work highlights Diffusion-C's core understandings
regarding the impacts of various corruptions, setting the stage for future
research endeavors in the realm of generative models.
</p></li>
</ul>

<h3>Title: I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions. (arXiv:2312.08869v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08869">http://arxiv.org/abs/2312.08869</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08869]] I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions(http://arxiv.org/abs/2312.08869)</code></li>
<li>Summary: <p>We are living in a world surrounded by diverse and "smart" devices with rich
modalities of sensing ability. Conveniently capturing the interactions between
us humans and these objects remains far-reaching. In this paper, we present
I'm-HOI, a monocular scheme to faithfully capture the 3D motions of both the
human and object in a novel setting: using a minimal amount of RGB camera and
object-mounted Inertial Measurement Unit (IMU). It combines general motion
inference and category-aware refinement. For the former, we introduce a
holistic human-object tracking method to fuse the IMU signals and the RGB
stream and progressively recover the human motions and subsequently the
companion object motions. For the latter, we tailor a category-aware motion
diffusion model, which is conditioned on both the raw IMU observations and the
results from the previous stage under over-parameterization representation. It
significantly refines the initial results and generates vivid body, hand, and
object motions. Moreover, we contribute a large dataset with ground truth human
and object motions, dense RGB inputs, and rich object-mounted IMU measurements.
Extensive experiments demonstrate the effectiveness of I'm-HOI under a hybrid
capture setting. Our dataset and code will be released to the community.
</p></li>
</ul>

<h3>Title: Semantic-Driven Initial Image Construction for Guided Image Synthesis in Diffusion Model. (arXiv:2312.08872v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08872">http://arxiv.org/abs/2312.08872</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08872]] Semantic-Driven Initial Image Construction for Guided Image Synthesis in Diffusion Model(http://arxiv.org/abs/2312.08872)</code></li>
<li>Summary: <p>The initial noise image has demonstrated a significant influence on image
generation, and manipulating the initial noise image can effectively increase
control over the generation. All of the current generation is based only on a
single initial noise drawn from a normal distribution, which may not be suited
to the desired content specified by the prompt. In this research, we propose a
novel approach using pre-collected, semantically-informed pixel blocks from
multiple initial noises for the initial image construction to enhance control
over the image generation. The inherent tendencies of these pixel blocks can
easily generate specific content, thus effectively guiding the generation
process towards the desired content. The pursuit of tailored initial image
construction inevitably leads to deviations from the normal distribution, and
our experimental results show that the diffusion model exhibits a certain
degree of tolerance towards the distribution of initial images. Our approach
achieves state-of-the-art performance in the training-free layout-to-image
synthesis task, demonstrating the adaptability of the initial image
construction in guiding the content of the generated image. Our code will be
made publicly available.
</p></li>
</ul>

<h3>Title: Diffusion Cocktail: Fused Generation from Diffusion Models. (arXiv:2312.08873v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08873">http://arxiv.org/abs/2312.08873</a></li>
<li>Code URL: https://github.com/MAPS-research/Ditail</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08873]] Diffusion Cocktail: Fused Generation from Diffusion Models(http://arxiv.org/abs/2312.08873)</code></li>
<li>Summary: <p>Diffusion models excel at generating high-quality images and are easy to
extend, making them extremely popular among active users who have created an
extensive collection of diffusion models with various styles by fine-tuning
base models such as Stable Diffusion. Recent work has focused on uncovering
semantic and visual information encoded in various components of a diffusion
model, enabling better generation quality and more fine-grained control.
However, those methods target improving a single model and overlook the vastly
available collection of fine-tuned diffusion models. In this work, we study the
combinations of diffusion models. We propose Diffusion Cocktail (Ditail), a
training-free method that can accurately transfer content information between
two diffusion models. This allows us to perform diverse generations using a set
of diffusion models, resulting in novel images that are unlikely to be obtained
by a single model alone. We also explore utilizing Ditail for style transfer,
with the target style set by a diffusion model instead of an image. Ditail
offers a more detailed manipulation of the diffusion generation, thereby
enabling the vast community to integrate various styles and contents seamlessly
and generate any content of any style.
</p></li>
</ul>

<h3>Title: Agent Attention: On the Integration of Softmax and Linear Attention. (arXiv:2312.08874v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08874">http://arxiv.org/abs/2312.08874</a></li>
<li>Code URL: https://github.com/leaplabthu/agent-attention</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08874]] Agent Attention: On the Integration of Softmax and Linear Attention(http://arxiv.org/abs/2312.08874)</code></li>
<li>Summary: <p>The attention module is the key component in Transformers. While the global
attention mechanism offers high expressiveness, its excessive computational
cost restricts its applicability in various scenarios. In this paper, we
propose a novel attention paradigm, Agent Attention, to strike a favorable
balance between computational efficiency and representation power.
Specifically, the Agent Attention, denoted as a quadruple $(Q, A, K, V)$,
introduces an additional set of agent tokens $A$ into the conventional
attention module. The agent tokens first act as the agent for the query tokens
$Q$ to aggregate information from $K$ and $V$, and then broadcast the
information back to $Q$. Given the number of agent tokens can be designed to be
much smaller than the number of query tokens, the agent attention is
significantly more efficient than the widely adopted Softmax attention, while
preserving global context modelling capability. Interestingly, we show that the
proposed agent attention is equivalent to a generalized form of linear
attention. Therefore, agent attention seamlessly integrates the powerful
Softmax attention and the highly efficient linear attention. Extensive
experiments demonstrate the effectiveness of agent attention with various
vision Transformers and across diverse vision tasks, including image
classification, object detection, semantic segmentation and image generation.
Notably, agent attention has shown remarkable performance in high-resolution
scenarios, owning to its linear attention nature. For instance, when applied to
Stable Diffusion, our agent attention accelerates generation and substantially
enhances image generation quality without any additional training. Code is
available at https://github.com/LeapLabTHU/Agent-Attention.
</p></li>
</ul>

<h3>Title: Neural Video Fields Editing. (arXiv:2312.08882v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08882">http://arxiv.org/abs/2312.08882</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08882]] Neural Video Fields Editing(http://arxiv.org/abs/2312.08882)</code></li>
<li>Summary: <p>Diffusion models have revolutionized text-driven video editing. However,
applying these methods to real-world editing encounters two significant
challenges: (1) the rapid increase in graphics memory demand as the number of
frames grows, and (2) the inter-frame inconsistency in edited videos. To this
end, we propose NVEdit, a novel text-driven video editing framework designed to
mitigate memory overhead and improve consistent editing for real-world long
videos. Specifically, we construct a neural video field, powered by tri-plane
and sparse grid, to enable encoding long videos with hundreds of frames in a
memory-efficient manner. Next, we update the video field through off-the-shelf
Text-to-Image (T2I) models to impart text-driven editing effects. A progressive
optimization strategy is developed to preserve original temporal priors.
Importantly, both the neural video field and T2I model are adaptable and
replaceable, thus inspiring future research. Experiments demonstrate that our
approach successfully edits hundreds of frames with impressive inter-frame
consistency.
</p></li>
</ul>

<h3>Title: SceneWiz3D: Towards Text-guided 3D Scene Composition. (arXiv:2312.08885v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08885">http://arxiv.org/abs/2312.08885</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08885]] SceneWiz3D: Towards Text-guided 3D Scene Composition(http://arxiv.org/abs/2312.08885)</code></li>
<li>Summary: <p>We are witnessing significant breakthroughs in the technology for generating
3D objects from text. Existing approaches either leverage large text-to-image
models to optimize a 3D representation or train 3D generators on object-centric
datasets. Generating entire scenes, however, remains very challenging as a
scene contains multiple 3D objects, diverse and scattered. In this work, we
introduce SceneWiz3D, a novel approach to synthesize high-fidelity 3D scenes
from text. We marry the locality of objects with globality of scenes by
introducing a hybrid 3D representation: explicit for objects and implicit for
scenes. Remarkably, an object, being represented explicitly, can be either
generated from text using conventional text-to-3D approaches, or provided by
users. To configure the layout of the scene and automatically place objects, we
apply the Particle Swarm Optimization technique during the optimization
process. Furthermore, it is difficult for certain parts of the scene (e.g.,
corners, occlusion) to receive multi-view supervision, leading to inferior
geometry. We incorporate an RGBD panorama diffusion model to mitigate it,
resulting in high-quality geometry. Extensive evaluation supports that our
approach achieves superior quality over previous approaches, enabling the
generation of detailed and view-consistent 3D scenes.
</p></li>
</ul>

<h3>Title: Diffusion-based Blind Text Image Super-Resolution. (arXiv:2312.08886v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08886">http://arxiv.org/abs/2312.08886</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08886]] Diffusion-based Blind Text Image Super-Resolution(http://arxiv.org/abs/2312.08886)</code></li>
<li>Summary: <p>Recovering degraded low-resolution text images is challenging, especially for
Chinese text images with complex strokes and severe degradation in real-world
scenarios. Ensuring both text fidelity and style realness is crucial for
high-quality text image super-resolution. Recently, diffusion models have
achieved great success in natural image synthesis and restoration due to their
powerful data distribution modeling abilities and data generation capabilities.
In this work, we propose an Image Diffusion Model (IDM) to restore text images
with realistic styles. For diffusion models, they are not only suitable for
modeling realistic image distribution but also appropriate for learning text
distribution. Since text prior is important to guarantee the correctness of the
restored text structure according to existing arts, we also propose a Text
Diffusion Model (TDM) for text recognition which can guide IDM to generate text
images with correct structures. We further propose a Mixture of Multi-modality
module (MoM) to make these two diffusion models cooperate with each other in
all the diffusion steps. Extensive experiments on synthetic and real-world
datasets demonstrate that our Diffusion-based Blind Text Image Super-Resolution
(DiffTSR) can restore text images with more accurate text structures as well as
more realistic appearances simultaneously.
</p></li>
</ul>

<h3>Title: SpeedUpNet: A Plug-and-Play Hyper-Network for Accelerating Text-to-Image Diffusion Models. (arXiv:2312.08887v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08887">http://arxiv.org/abs/2312.08887</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08887]] SpeedUpNet: A Plug-and-Play Hyper-Network for Accelerating Text-to-Image Diffusion Models(http://arxiv.org/abs/2312.08887)</code></li>
<li>Summary: <p>Text-to-image diffusion models (SD) exhibit significant advancements while
requiring extensive computational resources. Though many acceleration methods
have been proposed, they suffer from generation quality degradation or extra
training cost generalizing to new fine-tuned models. To address these
limitations, we propose a novel and universal Stable-Diffusion (SD)
acceleration module called SpeedUpNet(SUN). SUN can be directly plugged into
various fine-tuned SD models without extra training. This technique utilizes
cross-attention layers to learn the relative offsets in the generated image
results between negative and positive prompts achieving classifier-free
guidance distillation with negative prompts controllable, and introduces a
Multi-Step Consistency (MSC) loss to ensure a harmonious balance between
reducing inference steps and maintaining consistency in the generated output.
Consequently, SUN significantly reduces the number of inference steps to just 4
steps and eliminates the need for classifier-free guidance. It leads to an
overall speedup of more than 10 times for SD models compared to the
state-of-the-art 25-step DPM-solver++, and offers two extra advantages: (1)
classifier-free guidance distillation with controllable negative prompts and
(2) seamless integration into various fine-tuned Stable-Diffusion models
without training. The effectiveness of the SUN has been verified through
extensive experimentation. Project Page:
https://williechai.github.io/speedup-plugin-for-stable-diffusions.github.io
</p></li>
</ul>

<h3>Title: SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained Geometry and Appearance. (arXiv:2312.08889v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08889">http://arxiv.org/abs/2312.08889</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08889]] SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained Geometry and Appearance(http://arxiv.org/abs/2312.08889)</code></li>
<li>Summary: <p>Powered by large-scale text-to-image generation models, text-to-3D avatar
generation has made promising progress. However, most methods fail to produce
photorealistic results, limited by imprecise geometry and low-quality
appearance. Towards more practical avatar generation, we present SEEAvatar, a
method for generating photorealistic 3D avatars from text with SElf-Evolving
constraints for decoupled geometry and appearance. For geometry, we propose to
constrain the optimized avatar in a decent global shape with a template avatar.
The template avatar is initialized with human prior and can be updated by the
optimized avatar periodically as an evolving template, which enables more
flexible shape generation. Besides, the geometry is also constrained by the
static human prior in local parts like face and hands to maintain the delicate
structures. For appearance generation, we use diffusion model enhanced by
prompt engineering to guide a physically based rendering pipeline to generate
realistic textures. The lightness constraint is applied on the albedo texture
to suppress incorrect lighting effect. Experiments show that our method
outperforms previous methods on both global and local geometry and appearance
quality by a large margin. Since our method can produce high-quality meshes and
textures, such assets can be directly applied in classic graphics pipeline for
realistic rendering under any lighting condition. Project page at:
https://seeavatar3d.github.io.
</p></li>
</ul>

<h3>Title: VaLID: Variable-Length Input Diffusion for Novel View Synthesis. (arXiv:2312.08892v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08892">http://arxiv.org/abs/2312.08892</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08892]] VaLID: Variable-Length Input Diffusion for Novel View Synthesis(http://arxiv.org/abs/2312.08892)</code></li>
<li>Summary: <p>Novel View Synthesis (NVS), which tries to produce a realistic image at the
target view given source view images and their corresponding poses, is a
fundamental problem in 3D Vision. As this task is heavily under-constrained,
some recent work, like Zero123, tries to solve this problem with generative
modeling, specifically using pre-trained diffusion models. Although this
strategy generalizes well to new scenes, compared to neural radiance
field-based methods, it offers low levels of flexibility. For example, it can
only accept a single-view image as input, despite realistic applications often
offering multiple input images. This is because the source-view images and
corresponding poses are processed separately and injected into the model at
different stages. Thus it is not trivial to generalize the model into
multi-view source images, once they are available. To solve this issue, we try
to process each pose image pair separately and then fuse them as a unified
visual representation which will be injected into the model to guide image
synthesis at the target-views. However, inconsistency and computation costs
increase as the number of input source-view images increases. To solve these
issues, the Multi-view Cross Former module is proposed which maps
variable-length input data to fix-size output data. A two-stage training
strategy is introduced to further improve the efficiency during training time.
Qualitative and quantitative evaluation over multiple datasets demonstrates the
effectiveness of the proposed method against previous approaches. The code will
be released according to the acceptance.
</p></li>
</ul>

<h3>Title: Motion Flow Matching for Human Motion Synthesis and Editing. (arXiv:2312.08895v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08895">http://arxiv.org/abs/2312.08895</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08895]] Motion Flow Matching for Human Motion Synthesis and Editing(http://arxiv.org/abs/2312.08895)</code></li>
<li>Summary: <p>Human motion synthesis is a fundamental task in computer animation. Recent
methods based on diffusion models or GPT structure demonstrate commendable
performance but exhibit drawbacks in terms of slow sampling speeds and error
accumulation. In this paper, we propose \emph{Motion Flow Matching}, a novel
generative model designed for human motion generation featuring efficient
sampling and effectiveness in motion editing applications. Our method reduces
the sampling complexity from thousand steps in previous diffusion models to
just ten steps, while achieving comparable performance in text-to-motion and
action-to-motion generation benchmarks. Noticeably, our approach establishes a
new state-of-the-art Fr\'echet Inception Distance on the KIT-ML dataset. What
is more, we tailor a straightforward motion editing paradigm named
\emph{sampling trajectory rewriting} leveraging the ODE-style generative models
and apply it to various editing scenarios including motion prediction, motion
in-between prediction, motion interpolation, and upper-body editing. Our code
will be released.
</p></li>
</ul>

<h3>Title: OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers. (arXiv:2312.08985v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08985">http://arxiv.org/abs/2312.08985</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08985]] OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers(http://arxiv.org/abs/2312.08985)</code></li>
<li>Summary: <p>We have recently seen tremendous progress in realistic text-to-motion
generation. Yet, the existing methods often fail or produce implausible motions
with unseen text inputs, which limits the applications. In this paper, we
present OMG, a novel framework, which enables compelling motion generation from
zero-shot open-vocabulary text prompts. Our key idea is to carefully tailor the
pretrain-then-finetune paradigm into the text-to-motion generation. At the
pre-training stage, our model improves the generation ability by learning the
rich out-of-domain inherent motion traits. To this end, we scale up a large
unconditional diffusion model up to 1B parameters, so as to utilize the massive
unlabeled motion data up to over 20M motion instances. At the subsequent
fine-tuning stage, we introduce motion ControlNet, which incorporates text
prompts as conditioning information, through a trainable copy of the
pre-trained model and the proposed novel Mixture-of-Controllers (MoC) block.
MoC block adaptively recognizes various ranges of the sub-motions with a
cross-attention mechanism and processes them separately with the
text-token-specific experts. Such a design effectively aligns the CLIP token
embeddings of text prompts to various ranges of compact and expressive motion
features. Extensive experiments demonstrate that our OMG achieves significant
improvements over the state-of-the-art methods on zero-shot text-to-motion
generation. Project page: https://tr3e.github.io/omg-page.
</p></li>
</ul>

<h3>Title: Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer. (arXiv:2312.09008v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09008">http://arxiv.org/abs/2312.09008</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09008]] Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer(http://arxiv.org/abs/2312.09008)</code></li>
<li>Summary: <p>Despite the impressive generative capabilities of diffusion models, existing
diffusion model-based style transfer methods require inference-stage
optimization (e.g. fine-tuning or textual inversion of style) which is
time-consuming, or fails to leverage the generative ability of large-scale
diffusion models. To address these issues, we introduce a novel artistic style
transfer method based on a pre-trained large-scale diffusion model without any
optimization. Specifically, we manipulate the features of self-attention layers
as the way the cross-attention mechanism works; in the generation process,
substituting the key and value of content with those of style image. This
approach provides several desirable characteristics for style transfer
including 1) preservation of content by transferring similar styles into
similar image patches and 2) transfer of style based on similarity of local
texture (e.g. edge) between content and style images. Furthermore, we introduce
query preservation and attention temperature scaling to mitigate the issue of
disruption of original content, and initial latent Adaptive Instance
Normalization (AdaIN) to deal with the disharmonious color (failure to transfer
the colors of style). Our experimental results demonstrate that our proposed
method surpasses state-of-the-art methods in both conventional and
diffusion-based style transfer baselines.
</p></li>
</ul>

<h3>Title: World Models via Policy-Guided Trajectory Diffusion. (arXiv:2312.08533v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08533">http://arxiv.org/abs/2312.08533</a></li>
<li>Code URL: https://github.com/marc-rigter/polygrad-world-models</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08533]] World Models via Policy-Guided Trajectory Diffusion(http://arxiv.org/abs/2312.08533)</code></li>
<li>Summary: <p>World models are a powerful tool for developing intelligent agents. By
predicting the outcome of a sequence of actions, world models enable policies
to be optimised via on-policy reinforcement learning (RL) using synthetic data,
i.e. in ``in imagination''. Existing world models are autoregressive, and
interleave predicting the next state with sampling the next action from the
policy. Thus, the prediction error inevitably compounds as the trajectory
length grows. In this work, we propose a novel world modelling approach that is
not autoregressive and generates entire on-policy trajectories via a single
pass through a diffusion model. Our approach, Policy-Guided Trajectory
Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient
of the action distribution of the policy to diffuse a trajectory of initially
random states and actions into an on-policy synthetic trajectory. We analyse
the capabilities of our approach and demonstrate that it obtains competitive
prediction errors to state-of-the-art autoregressive baselines. PolyGRAD also
enables performant policies to be trained via on-policy RL in imagination. We
believe that PolyGRAD introduces a promising paradigm for world modelling with
many possible extensions to explore in future work.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Vision Transformer-Based Deep Learning for Histologic Classification of Endometrial Cancer. (arXiv:2312.08479v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08479">http://arxiv.org/abs/2312.08479</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08479]] Vision Transformer-Based Deep Learning for Histologic Classification of Endometrial Cancer(http://arxiv.org/abs/2312.08479)</code></li>
<li>Summary: <p>Endometrial cancer, the sixth most common cancer in females worldwide,
presents as a heterogeneous group with certain types prone to recurrence.
Precise histologic evaluation of endometrial cancer is essential for effective
patient management and determining the best treatment modalities. This study
introduces EndoNet, a transformer-based deep learning approach for histologic
classification of endometrial cancer. EndoNet uses convolutional neural
networks for extracting histologic features and a vision transformer for
aggregating these features and classifying slides based on their visual
characteristics. The model was trained on 929 digitized hematoxylin and
eosin-stained whole slide images of endometrial cancer from hysterectomy cases
at Dartmouth Health. It classifies these slides into low grade (Endometroid
Grades 1 and 2) and high-grade (endometroid carcinoma FIGO grade 3, uterine
serous carcinoma, carcinosarcoma) categories. EndoNet was evaluated on an
internal test set of 218 slides and an external test set of 100 random slides
from the public TCGA database. The model achieved a weighted average F1-score
of 0.92 (95% CI: 0.87-0.95) and an AUC of 0.93 (95% CI: 0.88-0.96) on the
internal test, and 0.86 (95% CI: 0.80-0.94) for F1-score and 0.86 (95% CI:
0.75-0.93) for AUC on the external test. Pending further validation, EndoNet
has the potential to assist pathologists in classifying challenging gynecologic
pathology tumors and enhancing patient care.
</p></li>
</ul>

<h3>Title: NViST: In the Wild New View Synthesis from a Single Image with Transformers. (arXiv:2312.08568v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08568">http://arxiv.org/abs/2312.08568</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08568]] NViST: In the Wild New View Synthesis from a Single Image with Transformers(http://arxiv.org/abs/2312.08568)</code></li>
<li>Summary: <p>We propose NViST, a transformer-based model for novel-view synthesis from a
single image, trained on a large-scale dataset of in-the-wild images with
complex backgrounds. NViST transforms image inputs directly into a radiance
field, adopting a scalable transformer-based architecture. In practice, NViST
exploits the self-supervised features learnt by a masked autoencoder (MAE), and
learns a novel decoder that translates features to 3D tokens via
cross-attention and adaptive layer normalization. Our model is efficient at
inference since only a single forward-pass is needed to predict a 3D
representation, unlike methods that require test-time optimization or sampling
such as 3D-aware diffusion models. We tackle further limitations of current
new-view synthesis models. First, unlike most generative models that are
trained in a category-specific manner, often on synthetic datasets or on masked
inputs, our model is trained on MVImgNet, a large-scale dataset of real-world,
casually-captured videos containing hundreds of object categories with diverse
backgrounds. Secondly, our model does not require canonicalization of the
training data - i.e. aligning all objects with a frontal view - only needing
relative pose at training time which removes a substantial barrier to it being
used on casually captured datasets. We show results on unseen objects and
categories on MVImgNet and even casual phone captures. We conduct qualitative
and quantitative evaluations on MVImgNet and ShapeNet to show that our model
represents a step forward towards enabling true in-the-wild novel-view
synthesis from a single image.
</p></li>
</ul>

<h3>Title: CT-MVSNet: Efficient Multi-View Stereo with Cross-scale Transformer. (arXiv:2312.08594v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08594">http://arxiv.org/abs/2312.08594</a></li>
<li>Code URL: https://github.com/wscstrive/ct-mvsnet</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08594]] CT-MVSNet: Efficient Multi-View Stereo with Cross-scale Transformer(http://arxiv.org/abs/2312.08594)</code></li>
<li>Summary: <p>Recent deep multi-view stereo (MVS) methods have widely incorporated
transformers into cascade network for high-resolution depth estimation,
achieving impressive results. However, existing transformer-based methods are
constrained by their computational costs, preventing their extension to finer
stages. In this paper, we propose a novel cross-scale transformer (CT) that
processes feature representations at different stages without additional
computation. Specifically, we introduce an adaptive matching-aware transformer
(AMT) that employs different interactive attention combinations at multiple
scales. This combined strategy enables our network to capture intra-image
context information and enhance inter-image feature relationships. Besides, we
present a dual-feature guided aggregation (DFGA) that embeds the coarse global
semantic information into the finer cost volume construction to further
strengthen global and local feature awareness. Meanwhile, we design a feature
metric loss (FM Loss) that evaluates the feature bias before and after
transformation to reduce the impact of feature mismatch on depth estimation.
Extensive experiments on DTU dataset and Tanks and Temples (T\&amp;T) benchmark
demonstrate that our method achieves state-of-the-art results. Code is
available at https://github.com/wscstrive/CT-MVSNet.
</p></li>
</ul>

<h3>Title: Factorization Vision Transformer: Modeling Long Range Dependency with Local Window Cost. (arXiv:2312.08614v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08614">http://arxiv.org/abs/2312.08614</a></li>
<li>Code URL: https://github.com/q2479036243/favit</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08614]] Factorization Vision Transformer: Modeling Long Range Dependency with Local Window Cost(http://arxiv.org/abs/2312.08614)</code></li>
<li>Summary: <p>Transformers have astounding representational power but typically consume
considerable computation which is quadratic with image resolution. The
prevailing Swin transformer reduces computational costs through a local window
strategy. However, this strategy inevitably causes two drawbacks: (1) the local
window-based self-attention hinders global dependency modeling capability; (2)
recent studies point out that local windows impair robustness. To overcome
these challenges, we pursue a preferable trade-off between computational cost
and performance. Accordingly, we propose a novel factorization self-attention
mechanism (FaSA) that enjoys both the advantages of local window cost and
long-range dependency modeling capability. By factorizing the conventional
attention matrix into sparse sub-attention matrices, FaSA captures long-range
dependencies while aggregating mixed-grained information at a computational
cost equivalent to the local window-based self-attention. Leveraging FaSA, we
present the factorization vision transformer (FaViT) with a hierarchical
structure. FaViT achieves high performance and robustness, with linear
computational complexity concerning input image spatial resolution. Extensive
experiments have shown FaViT's advanced performance in classification and
downstream tasks. Furthermore, it also exhibits strong model robustness to
corrupted and biased data and hence demonstrates benefits in favor of practical
applications. In comparison to the baseline model Swin-T, our FaViT-B2
significantly improves classification accuracy by 1% and robustness by 7%,
while reducing model parameters by 14%. Our code will soon be publicly
available at https://github.com/q2479036243/FaViT.
</p></li>
</ul>

<h3>Title: PairingNet: A Learning-based Pair-searching and -matching Network for Image Fragments. (arXiv:2312.08704v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08704">http://arxiv.org/abs/2312.08704</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08704]] PairingNet: A Learning-based Pair-searching and -matching Network for Image Fragments(http://arxiv.org/abs/2312.08704)</code></li>
<li>Summary: <p>In this paper, we propose a learning-based image fragment pair-searching and
-matching approach to solve the challenging restoration problem. Existing works
use rule-based methods to match similar contour shapes or textures, which are
always difficult to tune hyperparameters for extensive data and computationally
time-consuming. Therefore, we propose a neural network that can effectively
utilize neighbor textures with contour shape information to fundamentally
improve performance. First, we employ a graph-based network to extract the
local contour and texture features of fragments. Then, for the pair-searching
task, we adopt a linear transformer-based module to integrate these local
features and use contrastive loss to encode the global features of each
fragment. For the pair-matching task, we design a weighted fusion module to
dynamically fuse extracted local contour and texture features, and formulate a
similarity matrix for each pair of fragments to calculate the matching score
and infer the adjacent segment of contours. To faithfully evaluate our proposed
network, we created a new image fragment dataset through an algorithm we
designed that tears complete images into irregular fragments. The experimental
results show that our proposed network achieves excellent pair-searching
accuracy, reduces matching errors, and significantly reduces computational
time. Details, sourcecode, and data are available in our supplementary
material.
</p></li>
</ul>

<h3>Title: VSFormer: Visual-Spatial Fusion Transformer for Correspondence Pruning. (arXiv:2312.08774v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08774">http://arxiv.org/abs/2312.08774</a></li>
<li>Code URL: https://github.com/sugar-fly/vsformer</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08774]] VSFormer: Visual-Spatial Fusion Transformer for Correspondence Pruning(http://arxiv.org/abs/2312.08774)</code></li>
<li>Summary: <p>Correspondence pruning aims to find correct matches (inliers) from an initial
set of putative correspondences, which is a fundamental task for many
applications. The process of finding is challenging, given the varying inlier
ratios between scenes/image pairs due to significant visual differences.
However, the performance of the existing methods is usually limited by the
problem of lacking visual cues (\eg texture, illumination, structure) of
scenes. In this paper, we propose a Visual-Spatial Fusion Transformer
(VSFormer) to identify inliers and recover camera poses accurately. Firstly, we
obtain highly abstract visual cues of a scene with the cross attention between
local features of two-view images. Then, we model these visual cues and
correspondences by a joint visual-spatial fusion module, simultaneously
embedding visual cues into correspondences for pruning. Additionally, to mine
the consistency of correspondences, we also design a novel module that combines
the KNN-based graph and the transformer, effectively capturing both local and
global contexts. Extensive experiments have demonstrated that the proposed
VSFormer outperforms state-of-the-art methods on outdoor and indoor benchmarks.
</p></li>
</ul>

<h3>Title: What, How, and When Should Object Detectors Update in Continually Changing Test Domains?. (arXiv:2312.08875v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08875">http://arxiv.org/abs/2312.08875</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08875]] What, How, and When Should Object Detectors Update in Continually Changing Test Domains?(http://arxiv.org/abs/2312.08875)</code></li>
<li>Summary: <p>It is a well-known fact that the performance of deep learning models
deteriorates when they encounter a distribution shift at test time. Test-time
adaptation (TTA) algorithms have been proposed to adapt the model online while
inferring test data. However, existing research predominantly focuses on
classification tasks through the optimization of batch normalization layers or
classification heads, but this approach limits its applicability to various
model architectures like Transformers and makes it challenging to apply to
other tasks, such as object detection. In this paper, we propose a novel online
adaption approach for object detection in continually changing test domains,
considering which part of the model to update, how to update it, and when to
perform the update. By introducing architecture-agnostic and lightweight
adaptor modules and only updating these while leaving the pre-trained backbone
unchanged, we can rapidly adapt to new test domains in an efficient way and
prevent catastrophic forgetting. Furthermore, we present a practical and
straightforward class-wise feature aligning method for object detection to
resolve domain shifts. Additionally, we enhance efficiency by determining when
the model is sufficiently adapted or when additional adaptation is needed due
to changes in the test distribution. Our approach surpasses baselines on widely
used benchmarks, achieving improvements of up to 4.9\%p and 7.9\%p in mAP for
COCO $\rightarrow$ COCO-corrupted and SHIFT, respectively, while maintaining
about 20 FPS or higher.
</p></li>
</ul>

<h3>Title: An Incremental Unified Framework for Small Defect Inspection. (arXiv:2312.08917v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08917">http://arxiv.org/abs/2312.08917</a></li>
<li>Code URL: https://github.com/jqtangust/IUF</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08917]] An Incremental Unified Framework for Small Defect Inspection(http://arxiv.org/abs/2312.08917)</code></li>
<li>Summary: <p>Artificial Intelligence (AI)-driven defect inspection is pivotal in
industrial manufacturing. Yet, many methods, tailored to specific pipelines,
grapple with diverse product portfolios and evolving processes. Addressing
this, we present the Incremental Unified Framework (IUF) that can reduce the
feature conflict problem when continuously integrating new objects in the
pipeline, making it advantageous in object-incremental learning scenarios.
Employing a state-of-the-art transformer, we introduce Object-Aware
Self-Attention (OASA) to delineate distinct semantic boundaries. Semantic
Compression Loss (SCL) is integrated to optimize non-primary semantic space,
enhancing network adaptability for novel objects. Additionally, we prioritize
retaining the features of established objects during weight updates.
Demonstrating prowess in both image and pixel-level defect inspection, our
approach achieves state-of-the-art performance, proving indispensable for
dynamic and scalable industrial inspections. Our code will be released at
https://github.com/jqtangust/IUF.
</p></li>
</ul>

<h3>Title: Dual Branch Network Towards Accurate Printed Mathematical Expression Recognition. (arXiv:2312.09030v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09030">http://arxiv.org/abs/2312.09030</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09030]] Dual Branch Network Towards Accurate Printed Mathematical Expression Recognition(http://arxiv.org/abs/2312.09030)</code></li>
<li>Summary: <p>Over the past years, Printed Mathematical Expression Recognition (PMER) has
progressed rapidly. However, due to the insufficient context information
captured by Convolutional Neural Networks, some mathematical symbols might be
incorrectly recognized or missed. To tackle this problem, in this paper, a Dual
Branch transformer-based Network (DBN) is proposed to learn both local and
global context information for accurate PMER. In our DBN, local and global
features are extracted simultaneously, and a Context Coupling Module (CCM) is
developed to complement the features between the global and local contexts. CCM
adopts an interactive manner so that the coupled context clues are highly
correlated to each expression symbol. Additionally, we design a Dynamic Soft
Target (DST) strategy to utilize the similarities among symbol categories for
reasonable label generation. Our experimental results have demonstrated that
DBN can accurately recognize mathematical expressions and has achieved
state-of-the-art performance.
</p></li>
</ul>

<h3>Title: Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention. (arXiv:2312.08618v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08618">http://arxiv.org/abs/2312.08618</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08618]] Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention(http://arxiv.org/abs/2312.08618)</code></li>
<li>Summary: <p>This paper introduces a novel approach to enhance the capabilities of Large
Language Models (LLMs) in processing and understanding extensive text
sequences, a critical aspect in applications requiring deep comprehension and
synthesis of large volumes of information. Recognizing the inherent challenges
in extending the context window for LLMs, primarily built on Transformer
architecture, we propose a new model architecture, referred to as Zebra. This
architecture efficiently manages the quadratic time and memory complexity
issues associated with full attention in the Transformer by employing grouped
local-global attention layers. Our model, akin to a zebra's alternating
stripes, balances local and global attention layers, significantly reducing
computational requirements and memory consumption. Comprehensive experiments,
including pretraining from scratch, continuation of long context adaptation
training, and long instruction tuning, are conducted to evaluate the Zebra's
performance. The results show that Zebra achieves comparable or superior
performance on both short and long sequence benchmarks, while also enhancing
training and inference efficiency.
</p></li>
</ul>

<h3>Title: BiPFT: Binary Pre-trained Foundation Transformer with Low-rank Estimation of Binarization Residual Polynomials. (arXiv:2312.08937v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08937">http://arxiv.org/abs/2312.08937</a></li>
<li>Code URL: https://github.com/xingrun-xing/bipft</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08937]] BiPFT: Binary Pre-trained Foundation Transformer with Low-rank Estimation of Binarization Residual Polynomials(http://arxiv.org/abs/2312.08937)</code></li>
<li>Summary: <p>Pretrained foundation models offer substantial benefits for a wide range of
downstream tasks, which can be one of the most potential techniques to access
artificial general intelligence. However, scaling up foundation transformers
for maximal task-agnostic knowledge has brought about computational challenges,
especially on resource-limited devices such as mobiles. This work proposes the
first Binary Pretrained Foundation Transformer (BiPFT) for natural language
understanding (NLU) tasks, which remarkably saves 56 times operations and 28
times memory. In contrast to previous task-specific binary transformers, BiPFT
exhibits a substantial enhancement in the learning capabilities of binary
neural networks (BNNs), promoting BNNs into the era of pre-training. Benefiting
from extensive pretraining data, we further propose a data-driven binarization
method. Specifically, we first analyze the binarization error in self-attention
operations and derive the polynomials of binarization error. To simulate
full-precision self-attention, we define binarization error as binarization
residual polynomials, and then introduce low-rank estimators to model these
polynomials. Extensive experiments validate the effectiveness of BiPFTs,
surpassing task-specific baseline by 15.4% average performance on the GLUE
benchmark. BiPFT also demonstrates improved robustness to hyperparameter
changes, improved optimization efficiency, and reduced reliance on downstream
distillation, which consequently generalize on various NLU tasks and simplify
the downstream pipeline of BNNs. Our code and pretrained models are publicly
available at https://github.com/Xingrun-Xing/BiPFT.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Generative Model-based Feature Knowledge Distillation for Action Recognition. (arXiv:2312.08644v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08644">http://arxiv.org/abs/2312.08644</a></li>
<li>Code URL: https://github.com/aaai-24/generative-based-kd</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08644]] Generative Model-based Feature Knowledge Distillation for Action Recognition(http://arxiv.org/abs/2312.08644)</code></li>
<li>Summary: <p>Knowledge distillation (KD), a technique widely employed in computer vision,
has emerged as a de facto standard for improving the performance of small
neural networks. However, prevailing KD-based approaches in video tasks
primarily focus on designing loss functions and fusing cross-modal information.
This overlooks the spatial-temporal feature semantics, resulting in limited
advancements in model compression. Addressing this gap, our paper introduces an
innovative knowledge distillation framework, with the generative model for
training a lightweight student model. In particular, the framework is organized
into two steps: the initial phase is Feature Representation, wherein a
generative model-based attention module is trained to represent feature
semantics; Subsequently, the Generative-based Feature Distillation phase
encompasses both Generative Distillation and Attention Distillation, with the
objective of transferring attention-based feature semantics with the generative
model. The efficacy of our approach is demonstrated through comprehensive
experiments on diverse popular datasets, proving considerable enhancements in
video action recognition task. Moreover, the effectiveness of our proposed
framework is validated in the context of more intricate video action detection
task. Our code is available at https://github.com/aaai-24/Generative-based-KD.
</p></li>
</ul>

<h3>Title: ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks. (arXiv:2312.08583v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08583">http://arxiv.org/abs/2312.08583</a></li>
<li>Code URL: https://github.com/microsoft/DeepSpeed</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08583]] ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks(http://arxiv.org/abs/2312.08583)</code></li>
<li>Summary: <p>This study examines 4-bit quantization methods like GPTQ in large language
models (LLMs), highlighting GPTQ's overfitting and limited enhancement in
Zero-Shot tasks. While prior works merely focusing on zero-shot measurement, we
extend task scope to more generative categories such as code generation and
abstractive summarization, in which we found that INT4 quantization can
significantly underperform. However, simply shifting to higher precision
formats like FP6 has been particularly challenging, thus overlooked, due to
poor performance caused by the lack of sophisticated integration and system
acceleration strategies on current AI hardware. Our results show that FP6, even
with a coarse-grain quantization scheme, performs robustly across various
algorithms and tasks, demonstrating its superiority in accuracy and
versatility. Notably, with the FP6 quantization, \codestar-15B model performs
comparably to its FP16 counterpart in code generation, and for smaller models
like the 406M it closely matches their baselines in summarization. Neither can
be achieved by INT4. To better accommodate various AI hardware and achieve the
best system performance, we propose a novel 4+2 design for FP6 to achieve
similar latency to the state-of-the-art INT4 fine-grain quantization. With our
design, FP6 can become a promising solution to the current 4-bit quantization
methods used in LLMs.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens. (arXiv:2312.08870v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08870">http://arxiv.org/abs/2312.08870</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08870]] Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens(http://arxiv.org/abs/2312.08870)</code></li>
<li>Summary: <p>Recent advances in large video-language models have displayed promising
outcomes in video comprehension. Current approaches straightforwardly convert
video into language tokens and employ large language models for multi-modal
tasks. However, this method often leads to the generation of irrelevant
content, commonly known as "hallucination", as the length of the text increases
and the impact of the video diminishes. To address this problem, we propose
Vista-LLaMA, a novel framework that maintains the consistent distance between
all visual tokens and any language tokens, irrespective of the generated text
length. Vista-LLaMA omits relative position encoding when determining attention
weights between visual and text tokens, retaining the position encoding for
text and text tokens. This amplifies the effect of visual tokens on text
generation, especially when the relative distance is longer between visual and
text tokens. The proposed attention mechanism significantly reduces the chance
of producing irrelevant text related to the video content. Furthermore, we
present a sequential visual projector that projects the current video frame
into tokens of language space with the assistance of the previous frame. This
approach not only captures the temporal relationship within the video, but also
allows less visual tokens to encompass the entire video. Our approach
significantly outperforms various previous methods (e.g., Video-ChatGPT,
MovieChat) on four challenging open-ended video question answering benchmarks.
We reach an accuracy of 60.7 on the zero-shot NExT-QA and 60.5 on the zero-shot
MSRVTT-QA, setting a new state-of-the-art performance. This project is
available at https://jinxxian.github.io/Vista-LLaMA.
</p></li>
</ul>

<h3>Title: Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-modal Language Models. (arXiv:2312.08962v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08962">http://arxiv.org/abs/2312.08962</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08962]] Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-modal Language Models(http://arxiv.org/abs/2312.08962)</code></li>
<li>Summary: <p>We introduce a Depicted image Quality Assessment method (DepictQA),
overcoming the constraints of traditional score-based approaches. DepictQA
leverages Multi-modal Large Language Models (MLLMs), allowing for detailed,
language-based, human-like evaluation of image quality. Unlike conventional
Image Quality Assessment (IQA) methods relying on scores, DepictQA interprets
image content and distortions descriptively and comparatively, aligning closely
with humans' reasoning process. To build the DepictQA model, we establish a
hierarchical task framework, and collect a multi-modal IQA training dataset,
named M-BAPPS. To navigate the challenges in limited training data and
processing multiple images, we propose to use multi-source training data and
specialized image tags. Our DepictQA demonstrates a better performance than
score-based methods on the BAPPS benchmark. Moreover, compared with general
MLLMs, our DepictQA can generate more accurate reasoning descriptive languages.
Our research indicates that language-based IQA methods have the potential to be
customized for individual preferences. Datasets and codes will be released
publicly.
</p></li>
</ul>

<h3>Title: Beyond English: Evaluating LLMs for Arabic Grammatical Error Correction. (arXiv:2312.08400v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08400">http://arxiv.org/abs/2312.08400</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08400]] Beyond English: Evaluating LLMs for Arabic Grammatical Error Correction(http://arxiv.org/abs/2312.08400)</code></li>
<li>Summary: <p>Large language models (LLMs) finetuned to follow human instruction have
recently exhibited significant capabilities in various English NLP tasks.
However, their performance in grammatical error correction (GEC), especially on
languages other than English, remains significantly unexplored. In this work,
we evaluate the abilities of instruction finetuned LLMs in Arabic GEC, a
complex task due to Arabic's rich morphology. Our findings suggest that various
prompting methods, coupled with (in-context) few-shot learning, demonstrate
considerable effectiveness, with GPT-4 achieving up to $65.49$ F$_{1}$ score
under expert prompting (approximately $5$ points higher than our established
baseline). Despite these positive results, we find that instruction finetuned
models, regardless of their size, are still outperformed by fully finetuned
ones, even if they are significantly smaller in size. This disparity highlights
substantial room for improvements for LLMs. Inspired by methods used in
low-resource machine translation, we also develop a method exploiting synthetic
data that significantly outperforms previous models on two standard Arabic
benchmarks. Our best model achieves a new SOTA on Arabic GEC, with $73.29$ and
$73.26$ F$_{1}$ on the 2014 and 2015 QALB datasets, respectively, compared to
peer-reviewed published baselines.
</p></li>
</ul>

<h3>Title: Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach. (arXiv:2312.08579v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08579">http://arxiv.org/abs/2312.08579</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08579]] Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach(http://arxiv.org/abs/2312.08579)</code></li>
<li>Summary: <p>The automatic identification of planetary feature names in astronomy
publications presents numerous challenges. These features include craters,
defined as roughly circular depressions resulting from impact or volcanic
activity; dorsas, which are elongate raised structures or wrinkle ridges; and
lacus, small irregular patches of dark, smooth material on the Moon, referred
to as "lake" (Planetary Names Working Group, n.d.). Many feature names overlap
with places or people's names that they are named after, for example, Syria,
Tempe, Einstein, and Sagan, to name a few (U.S. Geological Survey, n.d.). Some
feature names have been used in many contexts, for instance, Apollo, which can
refer to mission, program, sample, astronaut, seismic, seismometers, core, era,
data, collection, instrument, and station, in addition to the crater on the
Moon. Some feature names can appear in the text as adjectives, like the lunar
craters Black, Green, and White. Some feature names in other contexts serve as
directions, like craters West and South on the Moon. Additionally, some
features share identical names across different celestial bodies, requiring
disambiguation, such as the Adams crater, which exists on both the Moon and
Mars. We present a multi-step pipeline combining rule-based filtering,
statistical relevance analysis, part-of-speech (POS) tagging, named entity
recognition (NER) model, hybrid keyword harvesting, knowledge graph (KG)
matching, and inference with a locally installed large language model (LLM) to
reliably identify planetary names despite these challenges. When evaluated on a
dataset of astronomy papers from the Astrophysics Data System (ADS), this
methodology achieves an F1-score over 0.97 in disambiguating planetary feature
names.
</p></li>
</ul>

<h3>Title: Metacognition-Enhanced Few-Shot Prompting With Positive Reinforcement. (arXiv:2312.08642v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08642">http://arxiv.org/abs/2312.08642</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08642]] Metacognition-Enhanced Few-Shot Prompting With Positive Reinforcement(http://arxiv.org/abs/2312.08642)</code></li>
<li>Summary: <p>Few-shot prompting elicits the remarkable abilities of large language models
by equipping them with a few demonstration examples in the input. However, the
traditional method of providing large language models with all demonstration
input-output pairs at once may not effectively guide large language models to
learn the specific input-output mapping relationship. In this paper, inspired
by the regulatory and supportive role of metacognition in students' learning,
we propose a novel metacognition-enhanced few-shot prompting, which guides
large language models to reflect on their thought processes to comprehensively
learn the given demonstration examples. Furthermore, considering that positive
reinforcement can improve students' learning motivation, we introduce positive
reinforcement into our metacognition-enhanced few-shot prompting to promote the
few-shot learning of large language models by providing response-based positive
feedback. The experimental results on two real-world datasets show that our
metacognition-enhanced few-shot prompting with positive reinforcement surpasses
traditional few-shot prompting in classification accuracy and macro F1.
</p></li>
</ul>

<h3>Title: TigerBot: An Open Multilingual Multitask LLM. (arXiv:2312.08688v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08688">http://arxiv.org/abs/2312.08688</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08688]] TigerBot: An Open Multilingual Multitask LLM(http://arxiv.org/abs/2312.08688)</code></li>
<li>Summary: <p>We release and introduce the TigerBot family of large language models (LLMs),
consisting of base and chat models, sized from 7, 13, 70 and 180 billion
parameters. We develop our models embarking from Llama-2 and BLOOM, and push
the boundary further in data, training algorithm, infrastructure, and
application tools. Our models yield meaningful performance gain over SOTA
open-source models, e.g., Llama-2, specifically 6\% gain in English and 20\%
gain in Chinese. TigerBot model family also achieves leading performance in
major academic and industrial benchmarks and leaderboards. We believe that
TigerBot represents just a snapshot of lightning-fast progression in LLM
open-source community. Therefore, we are thrilled to give back by publicly
releasing our models and reporting our approach behind, with additional
emphases on building SOTA LLMs in a democratized way and making LLMs of use in
real-world applications.
</p></li>
</ul>

<h3>Title: A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis. (arXiv:2312.08725v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08725">http://arxiv.org/abs/2312.08725</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08725]] A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis(http://arxiv.org/abs/2312.08725)</code></li>
<li>Summary: <p>Financial sentiment analysis plays a crucial role in uncovering latent
patterns and detecting emerging trends, enabling individuals to make
well-informed decisions that may yield substantial advantages within the
constantly changing realm of finance. Recently, Large Language Models (LLMs)
have demonstrated their effectiveness in diverse domains, showcasing remarkable
capabilities even in zero-shot and few-shot in-context learning for various
Natural Language Processing (NLP) tasks. Nevertheless, their potential and
applicability in the context of financial sentiment analysis have not been
thoroughly explored yet. To bridge this gap, we employ two approaches:
in-context learning (with a focus on gpt-3.5-turbo model) and fine-tuning LLMs
on a finance-domain dataset. Given the computational costs associated with
fine-tuning LLMs with large parameter sizes, our focus lies on smaller LLMs,
spanning from 250M to 3B parameters for fine-tuning. We then compare the
performances with state-of-the-art results to evaluate their effectiveness in
the finance-domain. Our results demonstrate that fine-tuned smaller LLMs can
achieve comparable performance to state-of-the-art fine-tuned LLMs, even with
models having fewer parameters and a smaller training dataset. Additionally,
the zero-shot and one-shot performance of LLMs produces comparable results with
fine-tuned smaller LLMs and state-of-the-art outcomes. Furthermore, our
analysis demonstrates that there is no observed enhancement in performance for
finance-domain sentiment analysis when the number of shots for in-context
learning is increased.
</p></li>
</ul>

<h3>Title: Evaluating Large Language Models for Health-related Queries with Presuppositions. (arXiv:2312.08800v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08800">http://arxiv.org/abs/2312.08800</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08800]] Evaluating Large Language Models for Health-related Queries with Presuppositions(http://arxiv.org/abs/2312.08800)</code></li>
<li>Summary: <p>As corporations rush to integrate large language models (LLMs) to their
search offerings, it is critical that they provide factually accurate
information that is robust to any presuppositions that a user may express. In
this work, we introduce UPHILL, a dataset consisting of health-related queries
with varying degrees of presuppositions. Using UPHILL, we evaluate the factual
accuracy and consistency of InstructGPT, ChatGPT, and BingChat models. We find
that while model responses rarely disagree with true health claims (posed as
questions), they often fail to challenge false claims: responses from
InstructGPT agree with 32% of the false claims, ChatGPT 26% and BingChat 23%.
As we increase the extent of presupposition in input queries, the responses
from InstructGPT and ChatGPT agree with the claim considerably more often,
regardless of its veracity. Responses from BingChat, which rely on retrieved
webpages, are not as susceptible. Given the moderate factual accuracy, and the
inability of models to consistently correct false assumptions, our work calls
for a careful assessment of current LLMs for use in high-stakes scenarios.
</p></li>
</ul>

<h3>Title: Boosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context Pruning. (arXiv:2312.08901v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08901">http://arxiv.org/abs/2312.08901</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08901]] Boosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context Pruning(http://arxiv.org/abs/2312.08901)</code></li>
<li>Summary: <p>Large language models (LLMs) have shown impressive capabilities in various
tasks, yet they still struggle with math reasoning. Despite efforts to optimize
Chain-of-Thoughts (CoT) prompts and fine-tune LLMs, the potential of few-shot
learning remains unexplored. In this work, we propose CoT-Max, a novel approach
pushing the boundaries of few-shot CoT learning to improve LLM math reasoning
capabilities. CoT-Max addresses the challenges of the selection of useful
examples and limited number of examples due to restricted context window
length. Inspired by our observation that natural language inputs contain many
redundancy, we propose a coarse-to-fine pruner as a plug-and-play module for
LLMs, which first identifies crucial CoT examples from a large batch and then
further prunes unimportant tokens. To train the pruner, we collect a math
reasoning dataset with diverse difficulty and steps, introduce a reward to
measure both the input's effectiveness for math reasoning and token length
constraints, and propose a novel training approach with reinforcement learning.
As a result, CoT-Max significantly outperforms CoT and few-shot prompting
baselines across various LLMs (LLaMA2-7B, 13B, 70B) and 5 mathematical
datasets, achieving up to 4.55% absolute improvements. Remarkably, without any
fine-tuning, LLaMA2-70B with CoT-Max surpasses GPT-3.5 and a wide range of
larger LLMs (PaLM, Minerva, etc.) on the GSM8K.
</p></li>
</ul>

<h3>Title: LDM$^2$: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement. (arXiv:2312.08402v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08402">http://arxiv.org/abs/2312.08402</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08402]] LDM$^2$: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement(http://arxiv.org/abs/2312.08402)</code></li>
<li>Summary: <p>With the rapid development of large language models (LLMs), it is highly
demanded that LLMs can be adopted to make decisions to enable the artificial
general intelligence. Most approaches leverage manually crafted examples to
prompt the LLMs to imitate the decision process of human. However, designing
optimal prompts is difficult and the patterned prompts can hardly be
generalized to more complex environments. In this paper, we propose a novel
model named Large Decision Model with Memory (LDM$^2$), which leverages a
dynamic memory mechanism to construct dynamic prompts, guiding the LLMs in
making proper decisions according to the faced state. LDM$^2$ consists of two
stages: memory formation and memory refinement. In the former stage, human
behaviors are decomposed into state-action tuples utilizing the powerful
summarizing ability of LLMs. Then, these tuples are stored in the memory, whose
indices are generated by the LLMs, to facilitate the retrieval of the most
relevant subset of memorized tuples based on the current state. In the latter
stage, our LDM$^2$ employs tree exploration to discover more suitable decision
processes and enrich the memory by adding valuable state-action tuples. The
dynamic circle of exploration and memory enhancement provides LDM$^2$ a better
understanding of the global environment. Extensive experiments conducted in two
interactive environments have shown that our LDM$^2$ outperforms the baselines
in terms of both score and success rate, which demonstrates its effectiveness.
</p></li>
</ul>

<h3>Title: LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers. (arXiv:2312.08958v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08958">http://arxiv.org/abs/2312.08958</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08958]] LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers(http://arxiv.org/abs/2312.08958)</code></li>
<li>Summary: <p>We propose a framework that leverages foundation models as teachers, guiding
a reinforcement learning agent to acquire semantically meaningful behavior
without human feedback. In our framework, the agent receives task instructions
grounded in a training environment from large language models. Then, a
vision-language model guides the agent in learning the multi-task
language-conditioned policy by providing reward feedback. We demonstrate that
our method can learn semantically meaningful skills in a challenging open-ended
MineDojo environment while prior unsupervised skill discovery methods struggle.
Additionally, we discuss observed challenges of using off-the-shelf foundation
models as teachers and our efforts to address them.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: A global optimization SAR image segmentation model can be easily transformed to a general ROF denoising model. (arXiv:2312.08376v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08376">http://arxiv.org/abs/2312.08376</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08376]] A global optimization SAR image segmentation model can be easily transformed to a general ROF denoising model(http://arxiv.org/abs/2312.08376)</code></li>
<li>Summary: <p>In this paper, we propose a novel locally statistical active contour model
(LACM) based on Aubert-Aujol (AA) denoising model and variational level set
method, which can be used for SAR images segmentation with intensity
inhomogeneity. Then we transform the proposed model into a global optimization
model by using convex relaxation technique. Firstly, we apply the Split Bregman
technique to transform the global optimization model into two alternating
optimization processes of Shrink operator and Laplace operator, which is called
SB_LACM model. Moreover, we propose two fast models to solve the global
optimization model , which are more efficient than the SB_LACM model. The first
model is: we add the proximal function to transform the global optimization
model to a general ROF model[29], which can be solved by a fast denoising
algorithm proposed by R.-Q.Jia, and H.Zhao; Thus we obtain a fast segmentation
algorithm with global optimization solver that does not involve partial
differential equations or difference equation, and only need simple difference
computation. The second model is: we use a different splitting approach than
one model to transform the global optimization model into a differentiable term
and a general ROF model term, which can be solved by the same technique as the
first model. Experiments using some challenging synthetic images and Envisat
SAR images demonstrate the superiority of our proposed models with respect to
the state-of-the-art models.
</p></li>
</ul>

<h3>Title: M3T: Multi-Scale Memory Matching for Video Object Segmentation and Tracking. (arXiv:2312.08514v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08514">http://arxiv.org/abs/2312.08514</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08514]] M3T: Multi-Scale Memory Matching for Video Object Segmentation and Tracking(http://arxiv.org/abs/2312.08514)</code></li>
<li>Summary: <p>Video Object Segmentation (VOS) has became increasingly important with
availability of larger datasets and more complex and realistic settings, which
involve long videos with global motion (e.g, in egocentric settings), depicting
small objects undergoing both rigid and non-rigid (including state)
deformations. While a number of recent approaches have been explored for this
task, these data characteristics still present challenges. In this work we
propose a novel, DETR-style encoder-decoder architecture, which focuses on
systematically analyzing and addressing aforementioned challenges.
Specifically, our model enables on-line inference with long videos in a
windowed fashion, by breaking the video into clips and propagating context
among them using time-coded memory. We illustrate that short clip length and
longer memory with learned time-coding are important design choices for
achieving state-of-the-art (SoTA) performance. Further, we propose multi-scale
matching and decoding to ensure sensitivity and accuracy for small objects.
Finally, we propose a novel training strategy that focuses learning on portions
of the video where an object undergoes significant deformations -- a form of
"soft" hard-negative mining, implemented as loss-reweighting. Collectively,
these technical contributions allow our model to achieve SoTA performance on
two complex datasets -- VISOR and VOST. A series of detailed ablations validate
our design choices as well as provide insights into the importance of parameter
choices and their impact on performance.
</p></li>
</ul>

<h3>Title: Semi-supervised Semantic Segmentation Meets Masked Modeling:Fine-grained Locality Learning Matters in Consistency Regularization. (arXiv:2312.08631v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08631">http://arxiv.org/abs/2312.08631</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08631]] Semi-supervised Semantic Segmentation Meets Masked Modeling:Fine-grained Locality Learning Matters in Consistency Regularization(http://arxiv.org/abs/2312.08631)</code></li>
<li>Summary: <p>Semi-supervised semantic segmentation aims to utilize limited labeled images
and abundant unlabeled images to achieve label-efficient learning, wherein the
weak-to-strong consistency regularization framework, popularized by FixMatch,
is widely used as a benchmark scheme. Despite its effectiveness, we observe
that such scheme struggles with satisfactory segmentation for the local
regions. This can be because it originally stems from the image classification
task and lacks specialized mechanisms to capture fine-grained local semantics
that prioritizes in dense prediction. To address this issue, we propose a novel
framework called \texttt{MaskMatch}, which enables fine-grained locality
learning to achieve better dense segmentation. On top of the original
teacher-student framework, we design a masked modeling proxy task that
encourages the student model to predict the segmentation given the unmasked
image patches (even with 30\% only) and enforces the predictions to be
consistent with pseudo-labels generated by the teacher model using the complete
image. Such design is motivated by the intuition that if the predictions are
more consistent given insufficient neighboring information, stronger
fine-grained locality perception is achieved. Besides, recognizing the
importance of reliable pseudo-labels in the above locality learning and the
original consistency learning scheme, we design a multi-scale ensembling
strategy that considers context at different levels of abstraction for
pseudo-label generation. Extensive experiments on benchmark datasets
demonstrate the superiority of our method against previous approaches and its
plug-and-play flexibility.
</p></li>
</ul>

<h3>Title: Segment Beyond View: Handling Partially Missing Modality for Audio-Visual Semantic Segmentation. (arXiv:2312.08673v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08673">http://arxiv.org/abs/2312.08673</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08673]] Segment Beyond View: Handling Partially Missing Modality for Audio-Visual Semantic Segmentation(http://arxiv.org/abs/2312.08673)</code></li>
<li>Summary: <p>Augmented Reality (AR) devices, emerging as prominent mobile interaction
platforms, face challenges in user safety, particularly concerning oncoming
vehicles. While some solutions leverage onboard camera arrays, these cameras
often have limited field-of-view (FoV) with front or downward perspectives.
Addressing this, we propose a new out-of-view semantic segmentation task and
Segment Beyond View (SBV), a novel audio-visual semantic segmentation method.
SBV supplements the visual modality, which miss the information beyond FoV,
with the auditory information using a teacher-student distillation model
(Omni2Ego). The model consists of a vision teacher utilising panoramic
information, an auditory teacher with 8-channel audio, and an audio-visual
student that takes views with limited FoV and binaural audio as input and
produce semantic segmentation for objects outside FoV. SBV outperforms existing
models in comparative evaluations and shows a consistent performance across
varying FoV ranges and in monaural audio settings.
</p></li>
</ul>

<h3>Title: Polyper: Boundary Sensitive Polyp Segmentation. (arXiv:2312.08735v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08735">http://arxiv.org/abs/2312.08735</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08735]] Polyper: Boundary Sensitive Polyp Segmentation(http://arxiv.org/abs/2312.08735)</code></li>
<li>Summary: <p>We present a new boundary sensitive framework for polyp segmentation, called
Polyper. Our method is motivated by a clinical approach that seasoned medical
practitioners often leverage the inherent features of interior polyp regions to
tackle blurred boundaries.Inspired by this, we propose explicitly leveraging
polyp regions to bolster the model's boundary discrimination capability while
minimizing computation. Our approach first extracts boundary and polyp regions
from the initial segmentation map through morphological operators. Then, we
design the boundary sensitive attention that concentrates on augmenting the
features near the boundary regions using the interior polyp regions's
characteristics to generate good segmentation results. Our proposed method can
be seamlessly integrated with classical encoder networks, like ResNet-50,
MiT-B1, and Swin Transformer. To evaluate the effectiveness of Polyper, we
conduct experiments on five publicly available challenging datasets, and
receive state-of-the-art performance on all of them. Code is available at
https://github.com/haoshao-nku/medical_seg.git.
</p></li>
</ul>

<h3>Title: CattleEyeView: A Multi-task Top-down View Cattle Dataset for Smarter Precision Livestock Farming. (arXiv:2312.08764v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08764">http://arxiv.org/abs/2312.08764</a></li>
<li>Code URL: https://github.com/animaleyeq/cattleeyeview</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08764]] CattleEyeView: A Multi-task Top-down View Cattle Dataset for Smarter Precision Livestock Farming(http://arxiv.org/abs/2312.08764)</code></li>
<li>Summary: <p>Cattle farming is one of the important and profitable agricultural
industries. Employing intelligent automated precision livestock farming systems
that can count animals, track the animals and their poses will raise
productivity and significantly reduce the heavy burden on its already limited
labor pool. To achieve such intelligent systems, a large cattle video dataset
is essential in developing and training such models. However, many current
animal datasets are tailored to few tasks or other types of animals, which
result in poorer model performance when applied to cattle. Moreover, they do
not provide top-down views of cattle. To address such limitations, we introduce
CattleEyeView dataset, the first top-down view multi-task cattle video dataset
for a variety of inter-related tasks (i.e., counting, detection, pose
estimation, tracking, instance segmentation) that are useful to count the
number of cows and assess their growth and well-being. The dataset contains 753
distinct top-down cow instances in 30,703 frames (14 video sequences). We
perform benchmark experiments to evaluate the model's performance for each
task. The dataset and codes can be found at
https://github.com/AnimalEyeQ/CattleEyeView.
</p></li>
</ul>

<h3>Title: Offshore Wind Plant Instance Segmentation Using Sentinel-1 Time Series, GIS, and Semantic Segmentation Models. (arXiv:2312.08773v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08773">http://arxiv.org/abs/2312.08773</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08773]] Offshore Wind Plant Instance Segmentation Using Sentinel-1 Time Series, GIS, and Semantic Segmentation Models(http://arxiv.org/abs/2312.08773)</code></li>
<li>Summary: <p>Offshore wind farms represent a renewable energy source with a significant
global growth trend, and their monitoring is strategic for territorial and
environmental planning. This study's primary objective is to detect offshore
wind plants at an instance level using semantic segmentation models and
Sentinel-1 time series. The secondary objectives are: (a) to develop a database
consisting of labeled data and S-1 time series; (b) to compare the performance
of five deep semantic segmentation architectures (U-Net, U-Net++, Feature
Pyramid Network - FPN, DeepLabv3+, and LinkNet); (c) develop a novel
augmentation strategy that shuffles the positions of the images within the time
series; (d) investigate different dimensions of time series intervals (1, 5,
10, and 15 images); and (e) evaluate the semantic-to-instance conversion
procedure. LinkNet was the top-performing model, followed by U-Net++ and U-Net,
while FPN and DeepLabv3+ presented the worst results. The evaluation of
semantic segmentation models reveals enhanced Intersection over Union (IoU)
(25%) and F-score metrics (18%) with the augmentation of time series images.
The study showcases the augmentation strategy's capability to mitigate biases
and precisely detect invariant targets. Furthermore, the conversion from
semantic to instance segmentation demonstrates its efficacy in accurately
isolating individual instances within classified regions - simplifying training
data and reducing annotation effort and complexity.
</p></li>
</ul>

<h3>Title: Progressive Uncertain Feature Self-reinforcement for Weakly Supervised Semantic Segmentation. (arXiv:2312.08916v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08916">http://arxiv.org/abs/2312.08916</a></li>
<li>Code URL: https://github.com/jessie459/feature-self-reinforcement</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08916]] Progressive Uncertain Feature Self-reinforcement for Weakly Supervised Semantic Segmentation(http://arxiv.org/abs/2312.08916)</code></li>
<li>Summary: <p>Compared to conventional semantic segmentation with pixel-level supervision,
Weakly Supervised Semantic Segmentation (WSSS) with image-level labels poses
the challenge that it always focuses on the most discriminative regions,
resulting in a disparity between fully supervised conditions. A typical
manifestation is the diminished precision on the object boundaries, leading to
a deteriorated accuracy of WSSS. To alleviate this issue, we propose to
adaptively partition the image content into deterministic regions (e.g.,
confident foreground and background) and uncertain regions (e.g., object
boundaries and misclassified categories) for separate processing. For uncertain
cues, we employ an activation-based masking strategy and seek to recover the
local information with self-distilled knowledge. We further assume that the
unmasked confident regions should be robust enough to preserve the global
semantics. Building upon this, we introduce a complementary self-enhancement
method that constrains the semantic consistency between these confident regions
and an augmented image with the same class labels. Extensive experiments
conducted on PASCAL VOC 2012 and MS COCO 2014 demonstrate that our proposed
single-stage approach for WSSS not only outperforms state-of-the-art benchmarks
remarkably but also surpasses multi-stage methodologies that trade complexity
for accuracy. The code can be found at
https://github.com/Jessie459/feature-self-reinforcement.
</p></li>
</ul>

<h3>Title: Influence of Prompting Strategies on Segment Anything Model (SAM) for Short-axis Cardiac MRI segmentation. (arXiv:2312.08932v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08932">http://arxiv.org/abs/2312.08932</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08932]] Influence of Prompting Strategies on Segment Anything Model (SAM) for Short-axis Cardiac MRI segmentation(http://arxiv.org/abs/2312.08932)</code></li>
<li>Summary: <p>The Segment Anything Model (SAM) has recently emerged as a significant
breakthrough in foundation models, demonstrating remarkable zero-shot
performance in object segmentation tasks. While SAM is designed for
generalization, it exhibits limitations in handling specific medical imaging
tasks that require fine-structure segmentation or precise boundaries. In this
paper, we focus on the task of cardiac magnetic resonance imaging (cMRI)
short-axis view segmentation using the SAM foundation model. We conduct a
comprehensive investigation of the impact of different prompting strategies
(including bounding boxes, positive points, negative points, and their
combinations) on segmentation performance. We evaluate on two public datasets
using the baseline model and models fine-tuned with varying amounts of
annotated data, ranging from a limited number of volumes to a fully annotated
dataset. Our findings indicate that prompting strategies significantly
influence segmentation performance. Combining positive points with either
bounding boxes or negative points shows substantial benefits, but little to no
benefit when combined simultaneously. We further observe that fine-tuning SAM
with a few annotated volumes improves segmentation performance when properly
prompted. Specifically, fine-tuning with bounding boxes has a positive impact,
while fine-tuning without bounding boxes leads to worse results compared to
baseline.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
