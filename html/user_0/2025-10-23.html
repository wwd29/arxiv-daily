<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-23</h1>
<h3>Title: Contextual Augmentation for Entity Linking using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Daniel Vollmers, Hamada M. Zahera, Diego Moussallem, Axel-Cyrille Ngonga Ngomo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18888">https://arxiv.org/abs/2510.18888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18888">https://arxiv.org/pdf/2510.18888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18888]] Contextual Augmentation for Entity Linking using Large Language Models(https://arxiv.org/abs/2510.18888)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Entity Linking involves detecting and linking entity mentions in natural language texts to a knowledge graph. Traditional methods use a two-step process with separate models for entity recognition and disambiguation, which can be computationally intensive and less effective. We propose a fine-tuned model that jointly integrates entity recognition and disambiguation in a unified framework. Furthermore, our approach leverages large language models to enrich the context of entity mentions, yielding better performance in entity disambiguation. We evaluated our approach on benchmark datasets and compared with several baselines. The evaluation results show that our approach achieves state-of-the-art performance on out-of-domain datasets.</li>
</ul>

<h3>Title: Small Language Models Offer Significant Potential for Science Community</h3>
<ul>
<li><strong>Authors: </strong>Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18890">https://arxiv.org/abs/2510.18890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18890">https://arxiv.org/pdf/2510.18890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18890]] Small Language Models Offer Significant Potential for Science Community(https://arxiv.org/abs/2510.18890)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in natural language processing, particularly with large language models (LLMs), are transforming how scientists engage with the literature. While the adoption of LLMs is increasing, concerns remain regarding potential information biases and computational costs. Rather than LLMs, I developed a framework to evaluate the feasibility of precise, rapid, and cost-effective information retrieval from extensive geoscience literature using freely available small language models (MiniLMs). A curated corpus of approximately 77 million high-quality sentences, extracted from 95 leading peer-reviewed geoscience journals such as Geophysical Research Letters and Earth and Planetary Science Letters published during years 2000 to 2024, was constructed. MiniLMs enable a computationally efficient approach for extracting relevant domain-specific information from these corpora through semantic search techniques and sentence-level indexing. This approach, unlike LLMs such as ChatGPT-4 that often produces generalized responses, excels at identifying substantial amounts of expert-verified information with established, multi-disciplinary sources, especially for information with quantitative findings. Furthermore, by analyzing emotional tone via sentiment analysis and topical clusters through unsupervised clustering within sentences, MiniLM provides a powerful tool for tracking the evolution of conclusions, research priorities, advancements, and emerging questions within geoscience communities. Overall, MiniLM holds significant potential within the geoscience community for applications such as fact and image retrievals, trend analyses, contradiction analyses, and educational purposes.</li>
</ul>

<h3>Title: When Models Can't Follow: Testing Instruction Adherence Across 256 LLMs</h3>
<ul>
<li><strong>Authors: </strong>Richard J. Young, Brandon Gillins, Alice M. Matthews</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18892">https://arxiv.org/abs/2510.18892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18892">https://arxiv.org/pdf/2510.18892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18892]] When Models Can't Follow: Testing Instruction Adherence Across 256 LLMs(https://arxiv.org/abs/2510.18892)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite widespread deployment of Large Language Models, systematic evaluation of instruction-following capabilities remains challenging. While comprehensive benchmarks exist, focused assessments that quickly diagnose specific instruction adherence patterns are valuable. As newer models may be trained on existing benchmarks, novel evaluation approaches are needed to assess genuine capabilities rather than memorized performance. This paper presents a streamlined evaluation framework using twenty carefully designed prompts to assess LLM instruction-following across diverse task categories. We demonstrate this framework through a large-scale empirical study conducted on October 14, 2025, testing 256 verified working models from 331 available via OpenRouter. To ensure methodological rigor and prevent selection bias, we first verified each model's basic functionality before inclusion. Unlike large-scale benchmarks requiring extensive computational resources, our approach offers a practical diagnostic tool researchers and practitioners can readily apply. Our methodology builds upon verifiable instructions while introducing a compact test suite balancing comprehensiveness with efficiency. Each prompt targets distinct aspects of instruction following, including format compliance, content constraints, logical sequencing, and multi-step task execution. We evaluate models from major providers (OpenAI, Anthropic, Google, Meta, Mistral) and emerging implementations (Qwen, DeepSeek, community models), providing comparative performance analysis. Our findings reveal consistent failure modes and identify specific instruction types posing particular challenges. This work contributes both a practical evaluation tool and one of the most comprehensive empirical analyses of instruction-following capabilities across the contemporary LLM landscape.</li>
</ul>

<h3>Title: Transformer-Based Low-Resource Language Translation: A Study on Standard Bengali to Sylheti</h3>
<ul>
<li><strong>Authors: </strong>Mangsura Kabir Oni, Tabia Tanzin Prama</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18898">https://arxiv.org/abs/2510.18898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18898">https://arxiv.org/pdf/2510.18898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18898]] Transformer-Based Low-Resource Language Translation: A Study on Standard Bengali to Sylheti(https://arxiv.org/abs/2510.18898)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Machine Translation (MT) has advanced from rule-based and statistical methods to neural approaches based on the Transformer architecture. While these methods have achieved impressive results for high-resource languages, low-resource varieties such as Sylheti remain underexplored. In this work, we investigate Bengali-to-Sylheti translation by fine-tuning multilingual Transformer models and comparing them with zero-shot large language models (LLMs). Experimental results demonstrate that fine-tuned models significantly outperform LLMs, with mBART-50 achieving the highest translation adequacy and MarianMT showing the strongest character-level fidelity. These findings highlight the importance of task-specific adaptation for underrepresented languages and contribute to ongoing efforts toward inclusive language technologies.</li>
</ul>

<h3>Title: DuoLens: A Framework for Robust Detection of Machine-Generated Multilingual Text and Code</h3>
<ul>
<li><strong>Authors: </strong>Shriyansh Agrawal, Aidan Lau, Sanyam Shah, Ahan M R, Kevin Zhu, Sunishchal Dev, Vasu Sharma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18904">https://arxiv.org/abs/2510.18904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18904">https://arxiv.org/pdf/2510.18904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18904]] DuoLens: A Framework for Robust Detection of Machine-Generated Multilingual Text and Code(https://arxiv.org/abs/2510.18904)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The prevalence of Large Language Models (LLMs) for generating multilingual text and source code has only increased the imperative for machine-generated content detectors to be accurate and efficient across domains. Current detectors, predominantly utilizing zero-shot methods, such as Fast DetectGPT or GPTZero, either incur high computational cost or lack sufficient accuracy, often with a trade-off between the two, leaving room for further improvement. To address these gaps, we propose the fine-tuning of encoder-only Small Language Models (SLMs), in particular, the pre-trained models of RoBERTA and CodeBERTa using specialized datasets on source code and other natural language to prove that for the task of binary classification, SLMs outperform LLMs by a huge margin whilst using a fraction of compute. Our encoders achieve AUROC $= 0.97$ to $0.99$ and macro-F1 $0.89$ to $0.94$ while reducing latency by $8$-$12\times$ and peak VRAM by $3$-$5\times$ at $512$-token inputs. Under cross-generator shifts and adversarial transformations (paraphrase, back-translation; code formatting/renaming), performance retains $\geq 92%$ of clean AUROC. We release training and evaluation scripts with seeds and configs; a reproducibility checklist is also included.</li>
</ul>

<h3>Title: 3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and Latency</h3>
<ul>
<li><strong>Authors: </strong>Minseok Jung, Abhas Ricky, Muhammad Rameez Chatni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18905">https://arxiv.org/abs/2510.18905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18905">https://arxiv.org/pdf/2510.18905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18905]] 3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and Latency(https://arxiv.org/abs/2510.18905)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>AI inference scaling is often tuned through 1D heuristics (a fixed reasoning passes) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail to consider cost and latency constraints. We introduce a 3D optimization framework that jointly calibrates accuracy, cost, and latency within a unified decision space, enabling constraints-aware inference scaling. Using Monte Carlo simulations across three representative scenarios and nine simulated large language models, we evaluate four optimization methods to address the 3D multi-objective optimization (MOO) problem. Framing inference scaling in MOO shapes a feasible space that 1D and 2D optimizations fail to capture, enabling environmentadaptive selection of the inference scaling k. Results show that knee-point optimization achieves the best balance, while accuracy-maximization remains favorable when precision is prioritized. The framework establishes a theoretical foundation for deployment-aware inference scaling across diverse operational contexts.</li>
</ul>

<h3>Title: Improving Topic Modeling of Social Media Short Texts with Rephrasing: A Case Study of COVID-19 Related Tweets</h3>
<ul>
<li><strong>Authors: </strong>Wangjiaxuan Xin, Shuhua Yin, Shi Chen, Yaorong Ge</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18908">https://arxiv.org/abs/2510.18908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18908">https://arxiv.org/pdf/2510.18908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18908]] Improving Topic Modeling of Social Media Short Texts with Rephrasing: A Case Study of COVID-19 Related Tweets(https://arxiv.org/abs/2510.18908)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Social media platforms such as Twitter (now X) provide rich data for analyzing public discourse, especially during crises such as the COVID-19 pandemic. However, the brevity, informality, and noise of social media short texts often hinder the effectiveness of traditional topic modeling, producing incoherent or redundant topics that are often difficult to interpret. To address these challenges, we have developed \emph{TM-Rephrase}, a model-agnostic framework that leverages large language models (LLMs) to rephrase raw tweets into more standardized and formal language prior to topic modeling. Using a dataset of 25,027 COVID-19-related Twitter posts, we investigate the effects of two rephrasing strategies, general- and colloquial-to-formal-rephrasing, on multiple topic modeling methods. Results demonstrate that \emph{TM-Rephrase} improves three metrics measuring topic modeling performance (i.e., topic coherence, topic uniqueness, and topic diversity) while reducing topic redundancy of most topic modeling algorithms, with the colloquial-to-formal strategy yielding the greatest performance gains and especially for the Latent Dirichlet Allocation (LDA) algorithm. This study contributes to a model-agnostic approach to enhancing topic modeling in public health related social media analysis, with broad implications for improved understanding of public discourse in health crisis as well as other important domains.</li>
</ul>

<h3>Title: Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection</h3>
<ul>
<li><strong>Authors: </strong>Hongyi He, Xiao Liu, Zhenghao Lin, Mingni Tang, Yi Cheng, Jintao Wang, Wenjie Li, Peng Cheng, Yeyun Gong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18909">https://arxiv.org/abs/2510.18909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18909">https://arxiv.org/pdf/2510.18909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18909]] Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection(https://arxiv.org/abs/2510.18909)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>High-quality pre-training data is crutial for large language models, where quality captures factual reliability and semantic value, and diversity ensures broad coverage and distributional heterogeneity. Existing approaches typically rely on single or multiple-dimensional score-based selection. However, directly selecting top-scored data often degrades performance, and sampling from a broader range is required to recover results. The above non-monotonicity between dataset scores and downstream benchmark results reveals a fundamental bias: score-based methods collapse correlated dimensions, causing top-scored data to appear high-quality while systematically overlooking diversity. We argue that ensuring diversity requires decomposing correlated metrics into orthogonal feature dimensions, from which the top-scored data can be directly selected. Therefore, we proposed the Orthogonal Diversity-Aware Selection (ODiS) algorithm, which preserves both quality and diversity during data selection. First, ODiS evaluates data from multiple dimensions, covering language quality, knowledge quality, and comprehension difficulty. The multi-dimensional scores are then decorrelated via Principal Component Analysis (PCA), yielding orthogonal evaluation dimensions. For each dimension, a Roberta-based scorer is trained to regress the data onto PCA-projected scores, enabling scalable inference on large corpora. Finally, ODiS constructs the training dataset by selecting top-scored data within each orthogonal dimension, thereby ensuring both quality and diversity. Empirical results show that ODiS-selected data exhibit less than 2\% inter-dimension overlap, confirming orthogonality between dimensions. More importantly, models trained with ODiS-selected data significantly outperform other baselines on downstream benchmarks, highlighting the necessity of orthogonal, diversity-aware data selection for LLMs.</li>
</ul>

<h3>Title: Context-aware Fairness Evaluation and Mitigation in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Afrozah Nadeem, Mark Dras, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18914">https://arxiv.org/abs/2510.18914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18914">https://arxiv.org/pdf/2510.18914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18914]] Context-aware Fairness Evaluation and Mitigation in LLMs(https://arxiv.org/abs/2510.18914)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large language models often display undesirable behaviors embedded in their internal representations, undermining fairness, inconsistency drift, amplification of harmful content, and the propagation of unwanted patterns during extended dialogue and conversations. Although training-time or data-centric methods attempt to reduce these effects, they are computationally expensive, irreversible once deployed, and slow to adapt to new conversational contexts. Pruning-based methods provide a flexible and transparent way to reduce bias by adjusting the neurons responsible for certain behaviors. However, most existing approaches are static; once a neuron is removed, the model loses the ability to adapt when the conversation or context changes. To address this, we propose a dynamic, reversible, pruning-based framework that detects context-aware neuron activations and applies adaptive masking to modulate their influence during generation. Our inference-time solution provides fine-grained, memory-aware mitigation with knowledge-preserved, more coherent behavior across multilingual single- and multi-turn dialogues, enabling dynamic fairness control in real-world conversational AI.</li>
</ul>

<h3>Title: Misinformation Detection using Large Language Models with Explainability</h3>
<ul>
<li><strong>Authors: </strong>Jainee Patel, Chintan Bhatt, Himani Trivedi, Thanh Thi Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18918">https://arxiv.org/abs/2510.18918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18918">https://arxiv.org/pdf/2510.18918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18918]] Misinformation Detection using Large Language Models with Explainability(https://arxiv.org/abs/2510.18918)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>The rapid spread of misinformation on online platforms undermines trust among individuals and hinders informed decision making. This paper shows an explainable and computationally efficient pipeline to detect misinformation using transformer-based pretrained language models (PLMs). We optimize both RoBERTa and DistilBERT using a two-step strategy: first, we freeze the backbone and train only the classification head; then, we progressively unfreeze the backbone layers while applying layer-wise learning rate decay. On two real-world benchmark datasets, COVID Fake News and FakeNewsNet GossipCop, we test the proposed approach with a unified protocol of preprocessing and stratified splits. To ensure transparency, we integrate the Local Interpretable Model-Agnostic Explanations (LIME) at the token level to present token-level rationales and SHapley Additive exPlanations (SHAP) at the global feature attribution level. It demonstrates that DistilBERT achieves accuracy comparable to RoBERTa while requiring significantly less computational resources. This work makes two key contributions: (1) it quantitatively shows that a lightweight PLM can maintain task performance while substantially reducing computational cost, and (2) it presents an explainable pipeline that retrieves faithful local and global justifications without compromising performance. The results suggest that PLMs combined with principled fine-tuning and interpretability can be an effective framework for scalable, trustworthy misinformation detection.</li>
</ul>

<h3>Title: Benchmarking On-Device Machine Learning on Apple Silicon with MLX</h3>
<ul>
<li><strong>Authors: </strong>Oluwaseun A. Ajayi, Ogundepo Odunayo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18921">https://arxiv.org/abs/2510.18921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18921">https://arxiv.org/pdf/2510.18921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18921]] Benchmarking On-Device Machine Learning on Apple Silicon with MLX(https://arxiv.org/abs/2510.18921)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The recent widespread adoption of Large Language Models (LLMs) and machine learning in general has sparked research interest in exploring the possibilities of deploying these models on smaller devices such as laptops and mobile phones. This creates a need for frameworks and approaches that are capable of taking advantage of on-device hardware. The MLX framework was created to address this need. It is a framework optimized for machine learning (ML) computations on Apple silicon devices, facilitating easier research, experimentation, and prototyping. This paper presents a performance evaluation of MLX, focusing on inference latency of transformer models. We compare the performance of different transformer architecture implementations in MLX with their Pytorch counterparts. For this research we create a framework called MLX-transformers which includes different transformer implementations in MLX and downloads the model checkpoints in pytorch and converts it to the MLX format. By leveraging the advanced architecture and capabilities of Apple Silicon, MLX-Transformers enables seamless execution of transformer models directly sourced from Hugging Face, eliminating the need for checkpoint conversion often required when porting models between frameworks. Our study benchmarks different transformer models on two Apple Silicon macbook devices against an NVIDIA CUDA GPU. Specifically, we compare the inference latency performance of models with the same parameter sizes and checkpoints. We evaluate the performance of BERT, RoBERTa, and XLM-RoBERTa models, with the intention of extending future work to include models of different modalities, thus providing a more comprehensive assessment of MLX's capabilities. The results highlight MLX's potential in enabling efficient and more accessible on-device ML applications within Apple's ecosystem.</li>
</ul>

<h3>Title: Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients</h3>
<ul>
<li><strong>Authors: </strong>Omar El mansouri, Mohamed El Amine Seddik, Salem Lahlou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18924">https://arxiv.org/abs/2510.18924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18924">https://arxiv.org/pdf/2510.18924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18924]] Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients(https://arxiv.org/abs/2510.18924)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) or verifiable rewards (RLVR), the standard paradigm for aligning LLMs or building recent SOTA reasoning models, is highly sensitive to noise from inconsistent or erroneous rewards. Yet, the interaction between such noise and widely used group-based policy optimization methods remains underexplored. We introduce a noise-robust Group Relative Policy Optimization (GRPO) and Done Right GRPO (this http URL) framework that explicitly models reward corruption as Bernoulli noise. Our method applies noise correction after estimating reward flip probabilities to debias the learning signal, yielding provably unbiased gradient estimates. Theoretical analysis shows that group-based methods inherently mitigate individual-level noise, and our correction strategy amplifies this robustness. Empirically, we observe consistent improvements across math and code tasks when applying our noise correction to standard reward model usage, with particular gains of up to 6.7 percentage points in accuracy on math tasks and 1.5 on code tasks under realistic reward model conditions. This work bridges label-noise correction from supervised learning with modern RLHF, offering both theoretical insights and a practical algorithm for noisy real-world deployment.</li>
</ul>

<h3>Title: BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping</h3>
<ul>
<li><strong>Authors: </strong>Zhiheng Xi, Xin Guo, Yang Nan, Enyu Zhou, Junrui Shen, Wenxiang Chen, Jiaqi Liu, Jixuan Huang, Zhihao Zhang, Honglin Guo, Xun Deng, Zhikai Lei, Miao Zheng, Guoteng Wang, Shuo Zhang, Peng Sun, Rui Zheng, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18927">https://arxiv.org/abs/2510.18927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18927">https://arxiv.org/pdf/2510.18927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18927]] BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping(https://arxiv.org/abs/2510.18927)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has recently become the core paradigm for aligning and strengthening large language models (LLMs). Yet, applying RL in off-policy settings--where stale data from past policies are used for training--improves sample efficiency, but remains challenging: policy entropy declines sharply, optimization often becomes unstable and may even collapse. Through theoretical and empirical analysis, we identify two key insights: (i) an imbalance in optimization, where negative-advantage samples dominate the policy gradient, suppressing useful behaviors and risking gradient explosions; and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping mechanism in PPO-like objectives systematically blocks entropy-increasing updates, thereby driving the policy toward over-exploitation at the expense of exploration. Building on these insights, we propose BAlanced Policy Optimization with Adaptive Clipping (BAPO), a simple yet effective method that dynamically adjusts clipping bounds to adaptively re-balance positive and negative contributions, preserve entropy, and stabilize RL optimization. Across diverse off-policy scenarios--including sample replay and partial rollout--BAPO achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025 benchmarks, our 7B BAPO model surpasses open-source counterparts such as SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art results among models of the same scale but also outperforms leading proprietary systems like o3-mini and Gemini-2.5-Flash-Thinking.</li>
</ul>

<h3>Title: Evaluating LLM Story Generation through Large-scale Network Analysis of Social Structures</h3>
<ul>
<li><strong>Authors: </strong>Hiroshi Nonaka, K. E. Perry</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18932">https://arxiv.org/abs/2510.18932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18932">https://arxiv.org/pdf/2510.18932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18932]] Evaluating LLM Story Generation through Large-scale Network Analysis of Social Structures(https://arxiv.org/abs/2510.18932)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluating the creative capabilities of large language models (LLMs) in complex tasks often requires human assessments that are difficult to scale. We introduce a novel, scalable methodology for evaluating LLM story generation by analyzing underlying social structures in narratives as signed character networks. To demonstrate its effectiveness, we conduct a large-scale comparative analysis using networks from over 1,200 stories, generated by four leading LLMs (GPT-4o, GPT-4o mini, Gemini 1.5 Pro, and Gemini 1.5 Flash) and a human-written corpus. Our findings, based on network properties like density, clustering, and signed edge weights, show that LLM-generated stories consistently exhibit a strong bias toward tightly-knit, positive relationships, which aligns with findings from prior research using human assessment. Our proposed approach provides a valuable tool for evaluating limitations and tendencies in the creative storytelling of current and future LLMs.</li>
</ul>

<h3>Title: Dimensionality Reduction for Remote Sensing Data Analysis: A Systematic Review of Methods and Applications</h3>
<ul>
<li><strong>Authors: </strong>Nathan Mankovich, Kai-Hendrik Cohrs, Homer Durand, Vasileios Sitokonstantinou, Tristan Williams, Gustau Camps-Valls</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18935">https://arxiv.org/abs/2510.18935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18935">https://arxiv.org/pdf/2510.18935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18935]] Dimensionality Reduction for Remote Sensing Data Analysis: A Systematic Review of Methods and Applications(https://arxiv.org/abs/2510.18935)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Earth observation involves collecting, analyzing, and processing an ever-growing mass of data. Automatically harvesting information is crucial for addressing significant societal, economic, and environmental challenges, ranging from environmental monitoring to urban planning and disaster management. However, the high dimensionality of these data poses challenges in terms of sparsity, inefficiency, and the curse of dimensionality, which limits the effectiveness of machine learning models. Dimensionality reduction (DR) techniques, specifically feature extraction, address these challenges by preserving essential data properties while reducing complexity and enhancing tasks such as data compression, cleaning, fusion, visualization, anomaly detection, and prediction. This review provides a handbook for leveraging DR across the RS data value chain and identifies opportunities for under-explored DR algorithms and their application in future research.</li>
</ul>

<h3>Title: ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge</h3>
<ul>
<li><strong>Authors: </strong>Zhilin Wang, Jaehun Jung, Ximing Lu, Shizhe Diao, Ellie Evans, Jiaqi Zeng, Pavlo Molchanov, Yejin Choi, Jan Kautz, Yi Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18941">https://arxiv.org/abs/2510.18941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18941">https://arxiv.org/pdf/2510.18941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18941]] ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge(https://arxiv.org/abs/2510.18941)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Evaluating progress in large language models (LLMs) is often constrained by the challenge of verifying responses, limiting assessments to tasks like mathematics, programming, and short-form question-answering. However, many real-world applications require evaluating LLMs in processing professional documents, synthesizing information, and generating comprehensive reports in response to user queries. We introduce ProfBench: a set of over 7000 response-criterion pairs as evaluated by human-experts with professional knowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We build robust and affordable LLM-Judges to evaluate ProfBench rubrics, by mitigating self-enhancement bias and reducing the cost of evaluation by 2-3 orders of magnitude, to make it fair and accessible to the broader community. Our findings reveal that ProfBench poses significant challenges even for state-of-the-art LLMs, with top-performing models like GPT-5-high achieving only 65.9\% overall performance. Furthermore, we identify notable performance disparities between proprietary and open-weight models and provide insights into the role that extended thinking plays in addressing complex, professional-domain tasks. Data: this https URL and Code: this https URL</li>
</ul>

<h3>Title: Ninja Codes: Neurally Generated Fiducial Markers for Stealthy 6-DoF Tracking</h3>
<ul>
<li><strong>Authors: </strong>Yuichiro Takeuchi, Yusuke Imoto, Shunya Kato</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18976">https://arxiv.org/abs/2510.18976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18976">https://arxiv.org/pdf/2510.18976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18976]] Ninja Codes: Neurally Generated Fiducial Markers for Stealthy 6-DoF Tracking(https://arxiv.org/abs/2510.18976)</code><input type="text"></li>
<li><strong>Keywords: </strong>steal</a></li>
<li><strong>Abstract: </strong>In this paper we describe Ninja Codes, neurally-generated fiducial markers that can be made to naturally blend into various real-world environments. An encoder network converts arbitrary images into Ninja Codes by applying visually modest alterations; the resulting codes, printed and pasted onto surfaces, can provide stealthy 6-DoF location tracking for a wide range of applications including augmented reality, robotics, motion-based user interfaces, etc. Ninja Codes can be printed using off-the-shelf color printers on regular printing paper, and can be detected using any device equipped with a modern RGB camera and capable of running inference. Using an end-to-end process inspired by prior work on deep steganography, we jointly train a series of network modules that perform the creation and detection of Ninja Codes. Through experiments, we demonstrate Ninja Codes' ability to provide reliable location tracking under common indoor lighting conditions, while successfully concealing themselves within diverse environmental textures. We expect Ninja Codes to offer particular value in scenarios where the conspicuous appearances of conventional fiducial markers make them undesirable for aesthetic and other reasons.</li>
</ul>

<h3>Title: Towards Universal Solvers: Using PGD Attack in Active Learning to Increase Generalizability of Neural Operators as Knowledge Distillation from Numerical PDE Solvers</h3>
<ul>
<li><strong>Authors: </strong>Yifei Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18989">https://arxiv.org/abs/2510.18989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18989">https://arxiv.org/pdf/2510.18989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18989]] Towards Universal Solvers: Using PGD Attack in Active Learning to Increase Generalizability of Neural Operators as Knowledge Distillation from Numerical PDE Solvers(https://arxiv.org/abs/2510.18989)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Nonlinear PDE solvers require fine space-time discretizations and local linearizations, leading to high memory cost and slow runtimes. Neural operators such as FNOs and DeepONets offer fast single-shot inference by learning function-to-function mappings and truncating high-frequency components, but they suffer from poor out-of-distribution (OOD) generalization, often failing on inputs outside the training distribution. We propose an adversarial teacher-student distillation framework in which a differentiable numerical solver supervises a compact neural operator while a PGD-style active sampling loop searches for worst-case inputs under smoothness and energy constraints to expand the training set. Using differentiable spectral solvers enables gradient-based adversarial search and stabilizes sample mining. Experiments on Burgers and Navier-Stokes systems demonstrate that adversarial distillation substantially improves OOD robustness while preserving the low parameter cost and fast inference of neural operators.</li>
</ul>

<h3>Title: The Black Tuesday Attack: how to crash the stock market with adversarial examples to financial forecasting models</h3>
<ul>
<li><strong>Authors: </strong>Thomas Hofweber, Jefrey Bergl, Ian Reyes, Amir Sadovnik</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18990">https://arxiv.org/abs/2510.18990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18990">https://arxiv.org/pdf/2510.18990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18990]] The Black Tuesday Attack: how to crash the stock market with adversarial examples to financial forecasting models(https://arxiv.org/abs/2510.18990)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>We investigate and defend the possibility of causing a stock market crash via small manipulations of individual stock values that together realize an adversarial example to financial forecasting models, causing these models to make the self-fulfilling prediction of a crash. Such a crash triggered by an adversarial example would likely be hard to detect, since the model's predictions would be accurate and the interventions that would cause it are minor. This possibility is a major risk to financial stability and an opportunity for hostile actors to cause great economic damage to an adversary. This threat also exists against individual stocks and the corresponding valuation of individual companies. We outline how such an attack might proceed, what its theoretical basis is, how it can be directed towards a whole economy or an individual company, and how one might defend against it. We conclude that this threat is vastly underappreciated and requires urgent research on how to defend against it.</li>
</ul>

<h3>Title: An Encode-then-Decompose Approach to Unsupervised Time Series Anomaly Detection on Contaminated Training Data--Extended Version</h3>
<ul>
<li><strong>Authors: </strong>Buang Zhang, Tung Kieu, Xiangfei Qiu, Chenjuan Guo, Jilin Hu, Aoying Zhou, Christian S. Jensen, Bin Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18998">https://arxiv.org/abs/2510.18998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18998">https://arxiv.org/pdf/2510.18998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18998]] An Encode-then-Decompose Approach to Unsupervised Time Series Anomaly Detection on Contaminated Training Data--Extended Version(https://arxiv.org/abs/2510.18998)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection is important in modern large-scale systems and is applied in a variety of domains to analyze and monitor the operation of diverse systems. Unsupervised approaches have received widespread interest, as they do not require anomaly labels during training, thus avoiding potentially high costs and having wider applications. Among these, autoencoders have received extensive attention. They use reconstruction errors from compressed representations to define anomaly scores. However, representations learned by autoencoders are sensitive to anomalies in training time series, causing reduced accuracy. We propose a novel encode-then-decompose paradigm, where we decompose the encoded representation into stable and auxiliary representations, thereby enhancing the robustness when training with contaminated time series. In addition, we propose a novel mutual information based metric to replace the reconstruction errors for identifying anomalies. Our proposal demonstrates competitive or state-of-the-art performance on eight commonly used multi- and univariate time series benchmarks and exhibits robustness to time series with different contamination ratios.</li>
</ul>

<h3>Title: Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts</h3>
<ul>
<li><strong>Authors: </strong>Seungjun Yu, Junsung Park, Youngsun Lim, Hyunjung Shim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19001">https://arxiv.org/abs/2510.19001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19001">https://arxiv.org/pdf/2510.19001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19001]] Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts(https://arxiv.org/abs/2510.19001)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a two-phase vision-language QA system for autonomous driving that answers high-level perception, prediction, and planning questions. In Phase-1, a large multimodal LLM (Qwen2.5-VL-32B) is conditioned on six-camera inputs, a short temporal window of history, and a chain-of-thought prompt with few-shot exemplars. A self-consistency ensemble (multiple sampled reasoning chains) further improves answer reliability. In Phase-2, we augment the prompt with nuScenes scene metadata (object annotations, ego-vehicle state, etc.) and category-specific question instructions (separate prompts for perception, prediction, planning tasks). In experiments on a driving QA benchmark, our approach significantly outperforms the baseline Qwen2.5 models. For example, using 5 history frames and 10-shot prompting in Phase-1 yields 65.1% overall accuracy (vs.62.61% with zero-shot); applying self-consistency raises this to 66.85%. Phase-2 achieves 67.37% overall. Notably, the system maintains 96% accuracy under severe visual corruption. These results demonstrate that carefully engineered prompts and contextual grounding can greatly enhance high-level driving QA with pretrained vision-language models.</li>
</ul>

<h3>Title: $Δ$t-Mamba3D: A Time-Aware Spatio-Temporal State-Space Model for Breast Cancer Risk Prediction</h3>
<ul>
<li><strong>Authors: </strong>Zhengbo Zhou, Dooman Arefan, Margarita Zuley, Shandong Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19003">https://arxiv.org/abs/2510.19003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19003">https://arxiv.org/pdf/2510.19003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19003]] $Δ$t-Mamba3D: A Time-Aware Spatio-Temporal State-Space Model for Breast Cancer Risk Prediction(https://arxiv.org/abs/2510.19003)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Longitudinal analysis of sequential radiological images is hampered by a fundamental data challenge: how to effectively model a sequence of high-resolution images captured at irregular time intervals. This data structure contains indispensable spatial and temporal cues that current methods fail to fully exploit. Models often compromise by either collapsing spatial information into vectors or applying spatio-temporal models that are computationally inefficient and incompatible with non-uniform time steps. We address this challenge with Time-Aware $\Delta$t-Mamba3D, a novel state-space architecture adapted for longitudinal medical imaging. Our model simultaneously encodes irregular inter-visit intervals and rich spatio-temporal context while remaining computationally efficient. Its core innovation is a continuous-time selective scanning mechanism that explicitly integrates the true time difference between exams into its state transitions. This is complemented by a multi-scale 3D neighborhood fusion module that robustly captures spatio-temporal relationships. In a comprehensive breast cancer risk prediction benchmark using sequential screening mammogram exams, our model shows superior performance, improving the validation c-index by 2-5 percentage points and achieving higher 1-5 year AUC scores compared to established variants of recurrent, transformer, and state-space models. Thanks to its linear complexity, the model can efficiently process long and complex patient screening histories of mammograms, forming a new framework for longitudinal image analysis.</li>
</ul>

<h3>Title: Prior-informed optimization of treatment recommendation via bandit algorithms trained on large language model-processed historical records</h3>
<ul>
<li><strong>Authors: </strong>Saman Nessari, Ali Bozorgi-Amiri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19014">https://arxiv.org/abs/2510.19014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19014">https://arxiv.org/pdf/2510.19014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19014]] Prior-informed optimization of treatment recommendation via bandit algorithms trained on large language model-processed historical records(https://arxiv.org/abs/2510.19014)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Current medical practice depends on standardized treatment frameworks and empirical methodologies that neglect individual patient variations, leading to suboptimal health outcomes. We develop a comprehensive system integrating Large Language Models (LLMs), Conditional Tabular Generative Adversarial Networks (CTGAN), T-learner counterfactual models, and contextual bandit approaches to provide customized, data-informed clinical recommendations. The approach utilizes LLMs to process unstructured medical narratives into structured datasets (93.2% accuracy), uses CTGANs to produce realistic synthetic patient data (55% accuracy via two-sample verification), deploys T-learners to forecast patient-specific treatment responses (84.3% accuracy), and integrates prior-informed contextual bandits to enhance online therapeutic selection by effectively balancing exploration of new possibilities with exploitation of existing knowledge. Testing on stage III colon cancer datasets revealed that our KernelUCB approach obtained 0.60-0.61 average reward scores across 5,000 rounds, exceeding other reference methods. This comprehensive system overcomes cold-start limitations in online learning environments, improves computational effectiveness, and constitutes notable progress toward individualized medicine adapted to specific patient characteristics.</li>
</ul>

<h3>Title: MoAlign: Motion-Centric Representation Alignment for Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Aritra Bhowmik, Denis Korzhenkov, Cees G. M. Snoek, Amirhossein Habibian, Mohsen Ghafoorian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19022">https://arxiv.org/abs/2510.19022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19022">https://arxiv.org/pdf/2510.19022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19022]] MoAlign: Motion-Centric Representation Alignment for Video Diffusion Models(https://arxiv.org/abs/2510.19022)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-video diffusion models have enabled high-quality video synthesis, yet often fail to generate temporally coherent and physically plausible motion. A key reason is the models' insufficient understanding of complex motions that natural videos often entail. Recent works tackle this problem by aligning diffusion model features with those from pretrained video encoders. However, these encoders mix video appearance and dynamics into entangled features, limiting the benefit of such alignment. In this paper, we propose a motion-centric alignment framework that learns a disentangled motion subspace from a pretrained video encoder. This subspace is optimized to predict ground-truth optical flow, ensuring it captures true motion dynamics. We then align the latent features of a text-to-video diffusion model to this new subspace, enabling the generative model to internalize motion knowledge and generate more plausible videos. Our method improves the physical commonsense in a state-of-the-art video diffusion model, while preserving adherence to textual prompts, as evidenced by empirical evaluations on VideoPhy, VideoPhy2, VBench, and VBench-2.0, along with a user study.</li>
</ul>

<h3>Title: Fusion of Machine Learning and Blockchain-based Privacy-Preserving Approach for Health Care Data in the Internet of Things</h3>
<ul>
<li><strong>Authors: </strong>Behnam Rezaei Bezanjani, Seyyed Hamid Ghafouri, Reza Gholamrezaei</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19026">https://arxiv.org/abs/2510.19026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19026">https://arxiv.org/pdf/2510.19026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19026]] Fusion of Machine Learning and Blockchain-based Privacy-Preserving Approach for Health Care Data in the Internet of Things(https://arxiv.org/abs/2510.19026)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, robust</a></li>
<li><strong>Abstract: </strong>In recent years, the rapid integration of Internet of Things (IoT) devices into the healthcare sector has brought about revolutionary advancements in patient care and data management. While these technological innovations hold immense promise, they concurrently raise critical security concerns, particularly in safeguarding medical data against potential cyber threats. The sensitive nature of health-related information requires robust measures to ensure the confidentiality, integrity, and availability of patient data in IoT-enabled medical environments. Addressing the imperative need for enhanced security in IoT-based healthcare systems, we propose a comprehensive method encompassing three distinct phases. In the first phase, we implement Blockchain-Enabled Request and Transaction Encryption to strengthen data transaction security, providing an immutable and transparent framework. In the second phase, we introduce a Request Pattern Recognition Check that leverages diverse data sources to identify and block potential unauthorized access attempts. Finally, the third phase incorporates Feature Selection and a BiLSTM network to enhance the accuracy and efficiency of intrusion detection using advanced machine learning techniques. We compared the simulation results of the proposed method with three recent related methods: AIBPSF-IoMT, OMLIDS-PBIoT, and AIMMFIDS. The evaluation criteria include detection rate, false alarm rate, precision, recall, and accuracy - crucial benchmarks for assessing the overall performance of intrusion detection systems. Our findings show that the proposed method outperforms existing approaches across all evaluated criteria, demonstrating its effectiveness in improving the security of IoT-based healthcare systems.</li>
</ul>

<h3>Title: Are they lovers or friends? Evaluating LLMs' Social Reasoning in English and Korean Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Eunsu Kim, Junyeong Park, Juhyun Oh, Kiwoong Park, Seyoung Song, A.Seza Dogruoz, Najoung Kim, Alice Oh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19028">https://arxiv.org/abs/2510.19028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19028">https://arxiv.org/pdf/2510.19028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19028]] Are they lovers or friends? Evaluating LLMs' Social Reasoning in English and Korean Dialogues(https://arxiv.org/abs/2510.19028)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are increasingly used in human-AI interactions, their social reasoning capabilities in interpersonal contexts are critical. We introduce SCRIPTS, a 1k-dialogue dataset in English and Korean, sourced from movie scripts. The task involves evaluating models' social reasoning capability to infer the interpersonal relationships (e.g., friends, sisters, lovers) between speakers in each dialogue. Each dialogue is annotated with probabilistic relational labels (Highly Likely, Less Likely, Unlikely) by native (or equivalent) Korean and English speakers from Korea and the U.S. Evaluating nine models on our task, current proprietary LLMs achieve around 75-80% on the English dataset, whereas their performance on Korean drops to 58-69%. More strikingly, models select Unlikely relationships in 10-25% of their responses. Furthermore, we find that thinking models and chain-of-thought prompting, effective for general reasoning, provide minimal benefits for social reasoning and occasionally amplify social biases. Our findings reveal significant limitations in current LLMs' social reasoning capabilities, highlighting the need for efforts to develop socially-aware language models.</li>
</ul>

<h3>Title: When Can We Trust LLMs in Mental Health? Large-Scale Benchmarks for Reliable LLM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Abeer Badawi, Elahe Rahimi, Md Tahmid Rahman Laskar, Sheri Grach, Lindsay Bertrand, Lames Danok, Jimmy Huang, Frank Rudzicz, Elham Dolatabadi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19032">https://arxiv.org/abs/2510.19032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19032">https://arxiv.org/pdf/2510.19032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19032]] When Can We Trust LLMs in Mental Health? Large-Scale Benchmarks for Reliable LLM Evaluation(https://arxiv.org/abs/2510.19032)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluating Large Language Models (LLMs) for mental health support is challenging due to the emotionally and cognitively complex nature of therapeutic dialogue. Existing benchmarks are limited in scale, reliability, often relying on synthetic or social media data, and lack frameworks to assess when automated judges can be trusted. To address the need for large-scale dialogue datasets and judge reliability assessment, we introduce two benchmarks that provide a framework for generation and evaluation. MentalBench-100k consolidates 10,000 one-turn conversations from three real scenarios datasets, each paired with nine LLM-generated responses, yielding 100,000 response pairs. MentalAlign-70k}reframes evaluation by comparing four high-performing LLM judges with human experts across 70,000 ratings on seven attributes, grouped into Cognitive Support Score (CSS) and Affective Resonance Score (ARS). We then employ the Affective Cognitive Agreement Framework, a statistical methodology using intraclass correlation coefficients (ICC) with confidence intervals to quantify agreement, consistency, and bias between LLM judges and human experts. Our analysis reveals systematic inflation by LLM judges, strong reliability for cognitive attributes such as guidance and informativeness, reduced precision for empathy, and some unreliability in safety and relevance. Our contributions establish new methodological and empirical foundations for reliable, large-scale evaluation of LLMs in mental health. We release the benchmarks and codes at: this https URL</li>
</ul>

<h3>Title: From Memorization to Generalization: Fine-Tuning Large Language Models for Biomedical Term-to-Identifier Normalization</h3>
<ul>
<li><strong>Authors: </strong>Suswitha Pericharla, Daniel B. Hier, Tayo Obafemi-Ajayi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19036">https://arxiv.org/abs/2510.19036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19036">https://arxiv.org/pdf/2510.19036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19036]] From Memorization to Generalization: Fine-Tuning Large Language Models for Biomedical Term-to-Identifier Normalization(https://arxiv.org/abs/2510.19036)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Effective biomedical data integration depends on automated term normalization, the mapping of natural language biomedical terms to standardized identifiers. This linking of terms to identifiers is essential for semantic interoperability. Large language models (LLMs) show promise for this task but perform unevenly across terminologies. We evaluated both memorization (training-term performance) and generalization (validation-term performance) across multiple biomedical ontologies. Fine-tuning Llama 3.1 8B revealed marked differences by terminology. GO mappings showed strong memorization gains (up to 77% improvement in term-to-identifier accuracy), whereas HPO showed minimal improvement. Generalization occurred only for protein-gene (GENE) mappings (13.9% gain), while fine-tuning for HPO and GO yielded negligible transfer. Baseline accuracy varied by model scale, with GPT-4o outperforming both Llama variants for all terminologies. Embedding analyses showed tight semantic alignment between gene symbols and protein names but weak alignment between terms and identifiers for GO or HPO, consistent with limited lexicalization. Fine-tuning success depended on two interacting factors: identifier popularity and lexicalization. Popular identifiers were more likely encountered during pretraining, enhancing memorization. Lexicalized identifiers, such as gene symbols, enabled semantic generalization. By contrast, arbitrary identifiers in GO and HPO constrained models to rote learning. These findings provide a predictive framework for when fine-tuning enhances factual recall versus when it fails due to sparse or non-lexicalized identifiers.</li>
</ul>

<h3>Title: Empowering Decision Trees via Shape Function Branching</h3>
<ul>
<li><strong>Authors: </strong>Nakul Upadhya, Eldan Cohen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19040">https://arxiv.org/abs/2510.19040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19040">https://arxiv.org/pdf/2510.19040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19040]] Empowering Decision Trees via Shape Function Branching(https://arxiv.org/abs/2510.19040)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Decision trees are prized for their interpretability and strong performance on tabular data. Yet, their reliance on simple axis-aligned linear splits often forces deep, complex structures to capture non-linear feature effects, undermining human comprehension of the constructed tree. To address this limitation, we propose a novel generalization of a decision tree, the Shape Generalized Tree (SGT), in which each internal node applies a learnable axis-aligned shape function to a single feature, enabling rich, non-linear partitioning in one split. As users can easily visualize each node's shape function, SGTs are inherently interpretable and provide intuitive, visual explanations of the model's decision mechanisms. To learn SGTs from data, we propose ShapeCART, an efficient induction algorithm for SGTs. We further extend the SGT framework to bivariate shape functions (S$^2$GT) and multi-way trees (SGT$_K$), and present Shape$^2$CART and ShapeCART$_K$, extensions to ShapeCART for learning S$^2$GTs and SGT$_K$s, respectively. Experiments on various datasets show that SGTs achieve superior performance with reduced model size compared to traditional axis-aligned linear trees.</li>
</ul>

<h3>Title: POLAR: Policy-based Layerwise Reinforcement Learning Method for Stealthy Backdoor Attacks in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Kuai Yu, Xiaoyu Wu, Peishen Yan, Qingqian Yang, Linshan Jiang, Hao Wang, Yang Hua, Tao Song, Haibing Guan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19056">https://arxiv.org/abs/2510.19056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19056">https://arxiv.org/pdf/2510.19056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19056]] POLAR: Policy-based Layerwise Reinforcement Learning Method for Stealthy Backdoor Attacks in Federated Learning(https://arxiv.org/abs/2510.19056)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, steal, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables decentralized model training across multiple clients without exposing local data, but its distributed feature makes it vulnerable to backdoor attacks. Despite early FL backdoor attacks modifying entire models, recent studies have explored the concept of backdoor-critical (BC) layers, which poison the chosen influential layers to maintain stealthiness while achieving high effectiveness. However, existing BC layers approaches rely on rule-based selection without consideration of the interrelations between layers, making them ineffective and prone to detection by advanced defenses. In this paper, we propose POLAR (POlicy-based LAyerwise Reinforcement learning), the first pipeline to creatively adopt RL to solve the BC layer selection problem in layer-wise backdoor attack. Different from other commonly used RL paradigm, POLAR is lightweight with Bernoulli sampling. POLAR dynamically learns an attack strategy, optimizing layer selection using policy gradient updates based on backdoor success rate (BSR) improvements. To ensure stealthiness, we introduce a regularization constraint that limits the number of modified layers by penalizing large attack footprints. Extensive experiments demonstrate that POLAR outperforms the latest attack methods by up to 40% against six state-of-the-art (SOTA) defenses.</li>
</ul>

<h3>Title: PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Amith Ananthram, Elias Stengel-Eskin, Lorena A. Bradford, Julia Demarest, Adam Purvis, Keith Krut, Robert Stein, Rina Elster Pantalony, Mohit Bansal, Kathleen McKeown</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19060">https://arxiv.org/abs/2510.19060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19060">https://arxiv.org/pdf/2510.19060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19060]] PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions(https://arxiv.org/abs/2510.19060)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While vision-language models (VLMs) have advanced into detailed image description, evaluation remains a challenge. Standard metrics (e.g. CIDEr, SPICE) were designed for short texts and tuned to recognize errors that are now uncommon, such as object misidentification. In contrast, long texts require sensitivity to attribute and relation attachments and scores that localize errors to particular text spans. In this work, we introduce PoSh, a metric for detailed image description that uses scene graphs as structured rubrics to guide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained errors (e.g. mistakes in compositional understanding). PoSh is replicable, interpretable and a better proxy for human raters than existing metrics (including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging new dataset, DOCENT. This novel benchmark contains artwork, paired with expert-written references, and model-generated descriptions, augmented with granular and coarse judgments of their quality from art history students. Thus, DOCENT enables evaluating both detailed image description metrics and detailed image description itself in a challenging new domain. We show that PoSh achieves stronger correlations (+0.05 Spearman $\rho$) with the human judgments in DOCENT than the best open-weight alternatives, is robust to image type (using CapArena, an existing dataset of web imagery) and is a capable reward function, outperforming standard supervised fine-tuning. Then, using PoSh, we characterize the performance of open and closed models in describing the paintings, sketches and statues in DOCENT and find that foundation models struggle to achieve full, error-free coverage of images with rich scene dynamics, establishing a demanding new task to gauge VLM progress. Through both PoSh and DOCENT, we hope to enable advances in important areas such as assistive text generation.</li>
</ul>

<h3>Title: What Makes a Good Curriculum? Disentangling the Effects of Data Ordering on LLM Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yaning Jia, Chunhui Zhang, Xingjian Diao, Xiangchi Yuan, Zhongyu Ouyang, soroush vosoughi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19099">https://arxiv.org/abs/2510.19099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19099">https://arxiv.org/pdf/2510.19099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19099]] What Makes a Good Curriculum? Disentangling the Effects of Data Ordering on LLM Mathematical Reasoning(https://arxiv.org/abs/2510.19099)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Curriculum learning (CL) - ordering training data from easy to hard - has become a popular strategy for improving reasoning in large language models (LLMs). Yet prior work employs disparate difficulty metrics and training setups, leaving open fundamental questions: When does curriculum help? Which direction - forward or reverse - is better? And does the answer depend on what we measure? We address these questions through a unified offline evaluation framework that decomposes curriculum difficulty into five complementary dimensions: Problem Difficulty, Model Surprisal, Confidence Margin, Predictive Uncertainty, and Decision Variability. Through controlled post-training experiments on mathematical reasoning benchmarks with Llama3.1-8B, Mistral-7B, and Gemma3-4B, we find that (i) no curriculum strategy dominates universally - the relative effectiveness of forward versus reverse CL depends jointly on model capability and task complexity; (ii) even within a single metric, samples at different difficulty levels produce distinct gains depending on task demands; and (iii) task-aligned curricula focus on shaping the model's final representations and generalization, whereas inner-state curricula modulate internal states such as confidence and uncertainty. Our findings challenge the notion of a universal curriculum strategy and offer actionable guidance across model and task regimes, with some metrics indicating that prioritizing decision-uncertain samples can further enhance learning outcomes.</li>
</ul>

<h3>Title: Advancing Brain Tumor Segmentation via Attention-based 3D U-Net Architecture and Digital Image Processing</h3>
<ul>
<li><strong>Authors: </strong>Eyad Gad, Seif Soliman, M. Saeed Darweesh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19109">https://arxiv.org/abs/2510.19109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19109">https://arxiv.org/pdf/2510.19109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19109]] Advancing Brain Tumor Segmentation via Attention-based 3D U-Net Architecture and Digital Image Processing(https://arxiv.org/abs/2510.19109)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In the realm of medical diagnostics, rapid advancements in Artificial Intelligence (AI) have significantly yielded remarkable improvements in brain tumor segmentation. Encoder-Decoder architectures, such as U-Net, have played a transformative role by effectively extracting meaningful representations in 3D brain tumor segmentation from Magnetic resonance imaging (MRI) scans. However, standard U-Net models encounter challenges in accurately delineating tumor regions, especially when dealing with irregular shapes and ambiguous boundaries. Additionally, training robust segmentation models on high-resolution MRI data, such as the BraTS datasets, necessitates high computational resources and often faces challenges associated with class imbalance. This study proposes the integration of the attention mechanism into the 3D U-Net model, enabling the model to capture intricate details and prioritize informative regions during the segmentation process. Additionally, a tumor detection algorithm based on digital image processing techniques is utilized to address the issue of imbalanced training data and mitigate bias. This study aims to enhance the performance of brain tumor segmentation, ultimately improving the reliability of diagnosis. The proposed model is thoroughly evaluated and assessed on the BraTS 2020 dataset using various performance metrics to accomplish this goal. The obtained results indicate that the model outperformed related studies, exhibiting dice of 0.975, specificity of 0.988, and sensitivity of 0.995, indicating the efficacy of the proposed model in improving brain tumor segmentation, offering valuable insights for reliable diagnosis in clinical settings.</li>
</ul>

<h3>Title: That's Deprecated! Understanding, Detecting, and Steering Knowledge Conflicts in Language Models for Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Jaesung Bae, Cameron Churchwell, Mitchell Hermon, Tsun-An Hsieh, Jocelyn Xu, Yekaterina Yegorova, Mark Hasegawa-Johnson, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19116">https://arxiv.org/abs/2510.19116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19116">https://arxiv.org/pdf/2510.19116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19116]] That's Deprecated! Understanding, Detecting, and Steering Knowledge Conflicts in Language Models for Code Generation(https://arxiv.org/abs/2510.19116)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates how large language models (LLMs) behave when faced with discrepancies between their parametric knowledge and conflicting information contained in a prompt. Building on prior question-answering (QA) research, we extend the investigation of knowledge conflicts to the realm of code generation. We propose a domain-agnostic framework for constructing and interpreting such conflicts, along with a novel evaluation method and dataset tailored to code conflict scenarios. Our experiments indicate that sufficiently large LLMs encode the notion of a knowledge conflict in their parameters, enabling us to detect knowledge conflicts with up to \textbf{80.65\%} accuracy. Building on these insights, we show that activation-level steering can achieve up to a \textbf{12.6\%} improvement in steering success over a random baseline. However, effectiveness depends critically on balancing model size, task domain, and steering direction. The experiment code and data will be made publicly available after acceptance.</li>
</ul>

<h3>Title: A Graph Signal Processing Framework for Hallucination Detection in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Valentin Noël</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, eess.SP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19117">https://arxiv.org/abs/2510.19117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19117">https://arxiv.org/pdf/2510.19117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19117]] A Graph Signal Processing Framework for Hallucination Detection in Large Language Models(https://arxiv.org/abs/2510.19117)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models achieve impressive results but distinguishing factual reasoning from hallucinations remains challenging. We propose a spectral analysis framework that models transformer layers as dynamic graphs induced by attention, with token embeddings as signals on these graphs. Through graph signal processing, we define diagnostics including Dirichlet energy, spectral entropy, and high-frequency energy ratios, with theoretical connections to computational stability. Experiments across GPT architectures suggest universal spectral patterns: factual statements exhibit consistent "energy mountain" behavior with low-frequency convergence, while different hallucination types show distinct signatures. Logical contradictions destabilize spectra with large effect sizes ($g>1.0$), semantic errors remain stable but show connectivity drift, and substitution hallucinations display intermediate perturbations. A simple detector using spectral signatures achieves 88.75% accuracy versus 75% for perplexity-based baselines, demonstrating practical utility. These findings indicate that spectral geometry may capture reasoning patterns and error behaviors, potentially offering a framework for hallucination detection in large language models.</li>
</ul>

<h3>Title: A Novel Approach to Breast Cancer Segmentation using U-Net Model with Attention Mechanisms and FedProx</h3>
<ul>
<li><strong>Authors: </strong>Eyad Gad, Mustafa Abou Khatwa, Mustafa A. Elattar, Sahar Selim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19118">https://arxiv.org/abs/2510.19118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19118">https://arxiv.org/pdf/2510.19118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19118]] A Novel Approach to Breast Cancer Segmentation using U-Net Model with Attention Mechanisms and FedProx(https://arxiv.org/abs/2510.19118)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, segmentation</a></li>
<li><strong>Abstract: </strong>Breast cancer is a leading cause of death among women worldwide, emphasizing the need for early detection and accurate diagnosis. As such Ultrasound Imaging, a reliable and cost-effective tool, is used for this purpose, however the sensitive nature of medical data makes it challenging to develop accurate and private artificial intelligence models. A solution is Federated Learning as it is a promising technique for distributed machine learning on sensitive medical data while preserving patient privacy. However, training on non-Independent and non-Identically Distributed (non-IID) local datasets can impact the accuracy and generalization of the trained model, which is crucial for accurate tumour boundary delineation in BC segmentation. This study aims to tackle this challenge by applying the Federated Proximal (FedProx) method to non-IID Ultrasonic Breast Cancer Imaging datasets. Moreover, we focus on enhancing tumour segmentation accuracy by incorporating a modified U-Net model with attention mechanisms. Our approach resulted in a global model with 96% accuracy, demonstrating the effectiveness of our method in enhancing tumour segmentation accuracy while preserving patient privacy. Our findings suggest that FedProx has the potential to be a promising approach for training precise machine learning models on non-IID local medical datasets.</li>
</ul>

<h3>Title: Learning Peer Influence Probabilities with Linear Contextual Bandits</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Sayeed Faruk, Mohammad Shahverdikondori, Elena Zheleva</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19119">https://arxiv.org/abs/2510.19119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19119">https://arxiv.org/pdf/2510.19119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19119]] Learning Peer Influence Probabilities with Linear Contextual Bandits(https://arxiv.org/abs/2510.19119)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In networked environments, users frequently share recommendations about content, products, services, and courses of action with others. The extent to which such recommendations are successful and adopted is highly contextual, dependent on the characteristics of the sender, recipient, their relationship, the recommended item, and the medium, which makes peer influence probabilities highly heterogeneous. Accurate estimation of these probabilities is key to understanding information diffusion processes and to improving the effectiveness of viral marketing strategies. However, learning these probabilities from data is challenging; static data may capture correlations between peer recommendations and peer actions but fails to reveal influence relationships. Online learning algorithms can learn these probabilities from interventions but either waste resources by learning from random exploration or optimize for rewards, thus favoring exploration of the space with higher influence probabilities. In this work, we study learning peer influence probabilities under a contextual linear bandit framework. We show that a fundamental trade-off can arise between regret minimization and estimation error, characterize all achievable rate pairs, and propose an uncertainty-guided exploration algorithm that, by tuning a parameter, attains any pair within this trade-off. Our experiments on semi-synthetic network datasets show the advantages of our method over static methods and contextual bandits that ignore this trade-off.</li>
</ul>

<h3>Title: Securing IoT Communications via Anomaly Traffic Detection: Synergy of Genetic Algorithm and Ensemble Method</h3>
<ul>
<li><strong>Authors: </strong>Behnam Seyedi, Octavian Postolache</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19121">https://arxiv.org/abs/2510.19121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19121">https://arxiv.org/pdf/2510.19121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19121]] Securing IoT Communications via Anomaly Traffic Detection: Synergy of Genetic Algorithm and Ensemble Method(https://arxiv.org/abs/2510.19121)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>The rapid growth of the Internet of Things (IoT) has transformed industries by enabling seamless data exchange among connected devices. However, IoT networks remain vulnerable to security threats such as denial of service (DoS) attacks, anomalous traffic, and data manipulation due to decentralized architectures and limited resources. To address these issues, this paper proposes an advanced anomaly detection framework with three main phases. First, data preprocessing is performed using the Median KS Test to remove noise, handle missing values, and balance datasets for cleaner input. Second, a feature selection phase employs a Genetic Algorithm combined with eagle inspired search strategies to identify the most relevant features, reduce dimensionality, and improve efficiency without sacrificing accuracy. Finally, an ensemble classifier integrates Decision Tree, Random Forest, and XGBoost algorithms to achieve accurate and reliable anomaly detection. The proposed model demonstrates high adaptability and scalability across diverse IoT environments. Experimental results show that it outperforms existing methods by achieving 98 percent accuracy, 95 percent detection rate, and reductions in false positive (10 percent) and false negative (5 percent) rates. These results confirm the framework effectiveness and robustness in improving IoT network security against evolving cyber threats.</li>
</ul>

<h3>Title: Training-Free Spectral Fingerprints of Voice Processing in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Valentin Noël</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, eess.SP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19131">https://arxiv.org/abs/2510.19131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19131">https://arxiv.org/pdf/2510.19131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19131]] Training-Free Spectral Fingerprints of Voice Processing in Transformers(https://arxiv.org/abs/2510.19131)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Different transformer architectures implement identical linguistic computations via distinct connectivity patterns, yielding model imprinted ``computational fingerprints'' detectable through spectral analysis. Using graph signal processing on attention induced token graphs, we track changes in algebraic connectivity (Fiedler value, $\Delta\lambda_2$) under voice alternation across 20 languages and three model families, with a prespecified early window (layers 2--5). Our analysis uncovers clear architectural signatures: Phi-3-Mini shows a dramatic English specific early layer disruption ($\overline{\Delta\lambda_2}_{[2,5]}\!\approx\!-0.446$) while effects in 19 other languages are minimal, consistent with public documentation that positions the model primarily for English use. Qwen2.5-7B displays small, distributed shifts that are largest for morphologically rich languages, and LLaMA-3.2-1B exhibits systematic but muted responses. These spectral signatures correlate strongly with behavioral differences (Phi-3: $r=-0.976$) and are modulated by targeted attention head ablations, linking the effect to early attention structure and confirming functional relevance. Taken together, the findings are consistent with the view that training emphasis can leave detectable computational imprints: specialized processing strategies that manifest as measurable connectivity patterns during syntactic transformations. Beyond voice alternation, the framework differentiates reasoning modes, indicating utility as a simple, training free diagnostic for revealing architectural biases and supporting model reliability analysis.</li>
</ul>

<h3>Title: HAMLOCK: HArdware-Model LOgically Combined attacK</h3>
<ul>
<li><strong>Authors: </strong>Sanskar Amgain, Daniel Lobo, Atri Chatterjee, Swarup Bhunia, Fnu Suya</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19145">https://arxiv.org/abs/2510.19145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19145">https://arxiv.org/pdf/2510.19145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19145]] HAMLOCK: HArdware-Model LOgically Combined attacK(https://arxiv.org/abs/2510.19145)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, steal</a></li>
<li><strong>Abstract: </strong>The growing use of third-party hardware accelerators (e.g., FPGAs, ASICs) for deep neural networks (DNNs) introduces new security vulnerabilities. Conventional model-level backdoor attacks, which only poison a model's weights to misclassify inputs with a specific trigger, are often detectable because the entire attack logic is embedded within the model (i.e., software), creating a traceable layer-by-layer activation path. This paper introduces the HArdware-Model Logically Combined Attack (HAMLOCK), a far stealthier threat that distributes the attack logic across the hardware-software boundary. The software (model) is now only minimally altered by tuning the activations of few neurons to produce uniquely high activation values when a trigger is present. A malicious hardware Trojan detects those unique activations by monitoring the corresponding neurons' most significant bit or the 8-bit exponents and triggers another hardware Trojan to directly manipulate the final output logits for misclassification. This decoupled design is highly stealthy, as the model itself contains no complete backdoor activation path as in conventional attacks and hence, appears fully benign. Empirically, across benchmarks like MNIST, CIFAR10, GTSRB, and ImageNet, HAMLOCK achieves a near-perfect attack success rate with a negligible clean accuracy drop. More importantly, HAMLOCK circumvents the state-of-the-art model-level defenses without any adaptive optimization. The hardware Trojan is also undetectable, incurring area and power overheads as low as 0.01%, which is easily masked by process and environmental noise. Our findings expose a critical vulnerability at the hardware-software interface, demanding new cross-layer defenses against this emerging threat.</li>
</ul>

<h3>Title: Subliminal Corruption: Mechanisms, Thresholds, and Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Reya Vir, Sarvesh Bhatnagar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19152">https://arxiv.org/abs/2510.19152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19152">https://arxiv.org/pdf/2510.19152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19152]] Subliminal Corruption: Mechanisms, Thresholds, and Interpretability(https://arxiv.org/abs/2510.19152)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>As machine learning models are increasingly fine-tuned on synthetic data, there is a critical risk of subtle misalignments spreading through interconnected AI systems. This paper investigates subliminal corruption, which we define as undesirable traits are transmitted through semantically neutral data, bypassing standard safety checks. While this phenomenon has been identified, a quantitative understanding of its dynamics is missing. To address this gap, we present a systematic study of the scaling laws, thresholds, and mechanisms of subliminal corruption using a teacher-student setup with GPT-2. Our experiments reveal three key findings: (1) subliminal corruption causes behavioral crossover, degrading the model's overall alignment, not just the targeted trait; (2) alignment fails in a sharp phase transition at a critical threshold of poisoned data, rather than degrading gradually; and (3) interpretability analysis shows the corruption mechanism mimics the model's natural fine-tuning process, making it difficult to detect. These results demonstrate a critical vulnerability in AI systems that rely on synthetic data and highlight the need for new safety protocols that can account for latent threats.</li>
</ul>

<h3>Title: Feature Space Adaptation for Robust Model Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Peng Wang, Minghao Gu, Qiang Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19155">https://arxiv.org/abs/2510.19155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19155">https://arxiv.org/pdf/2510.19155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19155]] Feature Space Adaptation for Robust Model Fine-Tuning(https://arxiv.org/abs/2510.19155)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Catastrophic forgetting is a common issue in model fine-tuning, especially when the downstream domain contains limited labeled data or differs greatly from the pre-training distribution. Existing parameter-efficient fine-tuning methods operate in the weight space by modifying or augmenting the pre-trained model's parameters, which can yield models overly specialized to the available downstream data. To mitigate the risk of overwriting pre-trained knowledge and enhance robustness, we propose to fine-tune the pre-trained model in the feature space. Two new fine-tuning methods are proposed: LoRFA (Low-Rank Feature Adaptation) and VeFA (Vector-Based Feature Adaptation). Feature space adaptation is inspired by the idea of effect equivalence modeling (EEM) of downstream lurking variables causing distribution shifts, which posits that unobserved factors can be represented as the total equivalent amount on observed features. By compensating for the effects of downstream lurking variables via a lightweight feature-level transformation, the pre-trained representations can be preserved, which improves model generalization under distribution shift. We evaluate LoRFA and VeFA versus LoRA on image classification, NLU, and NLG, covering both standard fine-tuning metrics and robustness. Feature space adaptation achieves comparable fine-tuning results and consistently stronger robustness.</li>
</ul>

<h3>Title: Preliminary Use of Vision Language Model Driven Extraction of Mouse Behavior Towards Understanding Fear Expression</h3>
<ul>
<li><strong>Authors: </strong>Paimon Goulart, Jordan Steinhauser, Kylene Shuler, Edward Korzus, Jia Chen, Evangelos E. Papalexakis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19160">https://arxiv.org/abs/2510.19160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19160">https://arxiv.org/pdf/2510.19160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19160]] Preliminary Use of Vision Language Model Driven Extraction of Mouse Behavior Towards Understanding Fear Expression(https://arxiv.org/abs/2510.19160)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Integration of diverse data will be a pivotal step towards improving scientific explorations in many disciplines. This work establishes a vision-language model (VLM) that encodes videos with text input in order to classify various behaviors of a mouse existing in and engaging with their environment. Importantly, this model produces a behavioral vector over time for each subject and for each session the subject undergoes. The output is a valuable dataset that few programs are able to produce with as high accuracy and with minimal user input. Specifically, we use the open-source Qwen2.5-VL model and enhance its performance through prompts, in-context learning (ICL) with labeled examples, and frame-level preprocessing. We found that each of these methods contributes to improved classification, and that combining them results in strong F1 scores across all behaviors, including rare classes like freezing and fleeing, without any model fine-tuning. Overall, this model will support interdisciplinary researchers studying mouse behavior by enabling them to integrate diverse behavioral features, measured across multiple time points and environments, into a comprehensive dataset that can address complex research questions.</li>
</ul>

<h3>Title: "You Are Rejected!": An Empirical Study of Large Language Models Taking Hiring Evaluations</h3>
<ul>
<li><strong>Authors: </strong>Dingjie Fu, Dianxing Shi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19167">https://arxiv.org/abs/2510.19167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19167">https://arxiv.org/pdf/2510.19167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19167]] "You Are Rejected!": An Empirical Study of Large Language Models Taking Hiring Evaluations(https://arxiv.org/abs/2510.19167)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the proliferation of the internet and the rapid advancement of Artificial Intelligence, leading technology companies face an urgent annual demand for a considerable number of software and algorithm engineers. To efficiently and effectively identify high-potential candidates from thousands of applicants, these firms have established a multi-stage selection process, which crucially includes a standardized hiring evaluation designed to assess job-specific competencies. Motivated by the demonstrated prowess of Large Language Models (LLMs) in coding and reasoning tasks, this paper investigates a critical question: Can LLMs successfully pass these hiring evaluations? To this end, we conduct a comprehensive examination of a widely used professional assessment questionnaire. We employ state-of-the-art LLMs to generate responses and subsequently evaluate their performance. Contrary to any prior expectation of LLMs being ideal engineers, our analysis reveals a significant inconsistency between the model-generated answers and the company-referenced solutions. Our empirical findings lead to a striking conclusion: All evaluated LLMs fails to pass the hiring evaluation.</li>
</ul>

<h3>Title: OpenGuardrails: An Open-Source Context-Aware AI Guardrails Platform</h3>
<ul>
<li><strong>Authors: </strong>Thomas Wang, Haowen Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19169">https://arxiv.org/abs/2510.19169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19169">https://arxiv.org/pdf/2510.19169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19169]] OpenGuardrails: An Open-Source Context-Aware AI Guardrails Platform(https://arxiv.org/abs/2510.19169)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, attack, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly integrated into real-world applications, safeguarding them against unsafe, malicious, or privacy-violating content is critically important. We present OpenGuardrails, the first open-source project to provide both a context-aware safety and manipulation detection model and a deployable platform for comprehensive AI guardrails. OpenGuardrails protects against content-safety risks, model-manipulation attacks (e.g., prompt injection, jailbreaking, code-interpreter abuse, and the generation/execution of malicious code), and data leakage. Content-safety and model-manipulation detection are implemented by a unified large model, while data-leakage identification and redaction are performed by a separate lightweight NER pipeline (e.g., Presidio-style models or regex-based detectors). The system can be deployed as a security gateway or an API-based service, with enterprise-grade, fully private deployment options. OpenGuardrails achieves state-of-the-art (SOTA) performance on safety benchmarks, excelling in both prompt and response classification across English, Chinese, and multilingual tasks. All models are released under the Apache 2.0 license for public use.</li>
</ul>

<h3>Title: When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA</h3>
<ul>
<li><strong>Authors: </strong>Nishanth Sridhar Nakshatri, Shamik Roy, Manoj Ghuhan Arivazhagan, Hanhan Zhou, Vinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19172">https://arxiv.org/abs/2510.19172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19172">https://arxiv.org/pdf/2510.19172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19172]] When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA(https://arxiv.org/abs/2510.19172)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>LLMs often fail to handle temporal knowledge conflicts--contradictions arising when facts evolve over time within their training data. Existing studies evaluate this phenomenon through benchmarks built on structured knowledge bases like Wikidata, but they focus on widely-covered, easily-memorized popular entities and lack the dynamic structure needed to fairly evaluate LLMs with different knowledge cut-off dates. We introduce evolveQA, a benchmark specifically designed to evaluate LLMs on temporally evolving knowledge, constructed from 3 real-world, time-stamped corpora: AWS updates, Azure changes, and WHO disease outbreak reports. Our framework identifies naturally occurring knowledge evolution and generates questions with gold answers tailored to different LLM knowledge cut-off dates. Through extensive evaluation of 12 open and closed-source LLMs across 3 knowledge probing formats, we demonstrate significant performance drops of up to 31% on evolveQA compared to static knowledge questions.</li>
</ul>

<h3>Title: Imbalanced Gradients in RL Post-Training of Multi-Task LLMs</h3>
<ul>
<li><strong>Authors: </strong>Runzhe Wu, Ankur Samanta, Ayush Jain, Scott Fujimoto, Jeongyeol Kwon, Ben Kretzu, Youliang Yu, Kaveh Hassani, Boris Vidolov, Yonathan Efroni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19178">https://arxiv.org/abs/2510.19178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19178">https://arxiv.org/pdf/2510.19178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19178]] Imbalanced Gradients in RL Post-Training of Multi-Task LLMs(https://arxiv.org/abs/2510.19178)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-task post-training of large language models (LLMs) is typically performed by mixing datasets from different tasks and optimizing them jointly. This approach implicitly assumes that all tasks contribute gradients of similar magnitudes; when this assumption fails, optimization becomes biased toward large-gradient tasks. In this paper, however, we show that this assumption fails in RL post-training: certain tasks produce significantly larger gradients, thus biasing updates toward those tasks. Such gradient imbalance would be justified only if larger gradients implied larger learning gains on the tasks (i.e., larger performance improvements) -- but we find this is not true. Large-gradient tasks can achieve similar or even much lower learning gains than small-gradient ones. Further analyses reveal that these gradient imbalances cannot be explained by typical training statistics such as training rewards or advantages, suggesting that they arise from the inherent differences between tasks. This cautions against naive dataset mixing and calls for future work on principled gradient-level corrections for LLMs.</li>
</ul>

<h3>Title: Interpretable Question Answering with Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Kartikeya Aneja, Manasvi Srivastava, Subhayan Das, Nagender Aneja</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19181">https://arxiv.org/abs/2510.19181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19181">https://arxiv.org/pdf/2510.19181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19181]] Interpretable Question Answering with Knowledge Graphs(https://arxiv.org/abs/2510.19181)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a question answering system that operates exclusively on a knowledge graph retrieval without relying on retrieval augmented generation (RAG) with large language models (LLMs). Instead, a small paraphraser model is used to paraphrase the entity relationship edges retrieved from querying the knowledge graph. The proposed pipeline is divided into two main stages. The first stage involves pre-processing a document to generate sets of question-answer (QA) pairs. The second stage converts these QAs into a knowledge graph from which graph-based retrieval is performed using embeddings and fuzzy techniques. The graph is queried, re-ranked, and paraphrased to generate a final answer. This work includes an evaluation using LLM-as-a-judge on the CRAG benchmark, which resulted in accuracies of 71.9% and 54.4% using LLAMA-3.2 and GPT-3.5-Turbo, respectively.</li>
</ul>

<h3>Title: PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning</h3>
<ul>
<li><strong>Authors: </strong>Fengyuan Sun, Hui Chen, Xinhao Xu, Dandan Zheng, Jingdong Chen, Jun Zhou, Jungong Han, Guiguang Ding</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19183">https://arxiv.org/abs/2510.19183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19183">https://arxiv.org/pdf/2510.19183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19183]] PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning(https://arxiv.org/abs/2510.19183)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>While multi-modal large language models (MLLMs) have made significant progress in recent years, the issue of hallucinations remains a major challenge. To mitigate this phenomenon, existing solutions either introduce additional data for further training or incorporate external or internal information during inference. However, these approaches inevitably introduce extra computational costs. In this paper, we observe that hallucinations in MLLMs are strongly associated with insufficient attention allocated to visual tokens. In particular, the presence of redundant visual tokens disperses the model's attention, preventing it from focusing on the most informative ones. As a result, critical visual cues are often under-attended, which in turn exacerbates the occurrence of hallucinations. Building on this observation, we propose \textbf{PruneHal}, a training-free, simple yet effective method that leverages adaptive KV cache pruning to enhance the model's focus on critical visual information, thereby mitigating hallucinations. To the best of our knowledge, we are the first to apply token pruning for hallucination mitigation in MLLMs. Notably, our method don't require additional training and incurs nearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be seamlessly integrated with different decoding strategies, including those specifically designed for hallucination mitigation. We evaluate PruneHal on several widely used hallucination evaluation benchmarks using four mainstream MLLMs, achieving robust and outstanding results that highlight the effectiveness and superiority of our method. Our code will be publicly available.</li>
</ul>

<h3>Title: Video Consistency Distance: Enhancing Temporal Consistency for Image-to-Video Generation via Reward-Based Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Takehiro Aoshima, Yusuke Shinohara, Park Byeongseon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19193">https://arxiv.org/abs/2510.19193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19193">https://arxiv.org/pdf/2510.19193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19193]] Video Consistency Distance: Enhancing Temporal Consistency for Image-to-Video Generation via Reward-Based Fine-Tuning(https://arxiv.org/abs/2510.19193)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reward-based fine-tuning of video diffusion models is an effective approach to improve the quality of generated videos, as it can fine-tune models without requiring real-world video datasets. However, it can sometimes be limited to specific performances because conventional reward functions are mainly aimed at enhancing the quality across the whole generated video sequence, such as aesthetic appeal and overall consistency. Notably, the temporal consistency of the generated video often suffers when applying previous approaches to image-to-video (I2V) generation tasks. To address this limitation, we propose Video Consistency Distance (VCD), a novel metric designed to enhance temporal consistency, and fine-tune a model with the reward-based fine-tuning framework. To achieve coherent temporal consistency relative to a conditioning image, VCD is defined in the frequency space of video frame features to capture frame information effectively through frequency-domain analysis. Experimental results across multiple I2V datasets demonstrate that fine-tuning a video generation model with VCD significantly enhances temporal consistency without degrading other performance compared to the previous method.</li>
</ul>

<h3>Title: An Active Diffusion Neural Network for Graphs</h3>
<ul>
<li><strong>Authors: </strong>Mengying Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19202">https://arxiv.org/abs/2510.19202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19202">https://arxiv.org/pdf/2510.19202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19202]] An Active Diffusion Neural Network for Graphs(https://arxiv.org/abs/2510.19202)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The analogy to heat diffusion has enhanced our understanding of information flow in graphs and inspired the development of Graph Neural Networks (GNNs). However, most diffusion-based GNNs emulate passive heat diffusion, which still suffers from over-smoothing and limits their ability to capture global graph information. Inspired by the heat death of the universe, which posits that energy distribution becomes uniform over time in a closed system, we recognize that, without external input, node representations in a graph converge to identical feature vectors as diffusion progresses. To address this issue, we propose the Active Diffusion-based Graph Neural Network (ADGNN). ADGNN achieves active diffusion by integrating multiple external information sources that dynamically influence the diffusion process, effectively overcoming the over-smoothing problem. Furthermore, our approach realizes true infinite diffusion by directly calculating the closed-form solution of the active diffusion iterative formula. This allows nodes to preserve their unique characteristics while efficiently gaining comprehensive insights into the graph's global structure. We evaluate ADGNN against several state-of-the-art GNN models across various graph tasks. The results demonstrate that ADGNN significantly improves both accuracy and efficiency, highlighting its effectiveness in capturing global graph information and maintaining node distinctiveness.</li>
</ul>

<h3>Title: Defending Against Prompt Injection with DataFilter</h3>
<ul>
<li><strong>Authors: </strong>Yizhu Wang, Sizhe Chen, Raghad Alkhudair, Basel Alomair, David Wagner</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19207">https://arxiv.org/abs/2510.19207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19207">https://arxiv.org/pdf/2510.19207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19207]] Defending Against Prompt Injection with DataFilter(https://arxiv.org/abs/2510.19207)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>When large language model (LLM) agents are increasingly deployed to automate tasks and interact with untrusted external data, prompt injection emerges as a significant security threat. By injecting malicious instructions into the data that LLMs access, an attacker can arbitrarily override the original user task and redirect the agent toward unintended, potentially harmful actions. Existing defenses either require access to model weights (fine-tuning), incur substantial utility loss (detection-based), or demand non-trivial system redesign (system-level). Motivated by this, we propose DataFilter, a test-time model-agnostic defense that removes malicious instructions from the data before it reaches the backend LLM. DataFilter is trained with supervised fine-tuning on simulated injections and leverages both the user's instruction and the data to selectively strip adversarial content while preserving benign information. Across multiple benchmarks, DataFilter consistently reduces the prompt injection attack success rates to near zero while maintaining the LLMs' utility. DataFilter delivers strong security, high utility, and plug-and-play deployment, making it a strong practical defense to secure black-box commercial LLMs against prompt injection. Our DataFilter model is released at this https URL for immediate use, with the code to reproduce our results at this https URL.</li>
</ul>

<h3>Title: DiSRouter: Distributed Self-Routing for LLM Selections</h3>
<ul>
<li><strong>Authors: </strong>Hang Zheng, Hongshen Xu, Yongkai Lin, Shuai Fan, Lu Chen, Kai Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19208">https://arxiv.org/abs/2510.19208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19208">https://arxiv.org/pdf/2510.19208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19208]] DiSRouter: Distributed Self-Routing for LLM Selections(https://arxiv.org/abs/2510.19208)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of Large Language Models (LLMs) has created a diverse ecosystem of models with highly varying performance and costs, necessitating effective query routing to balance performance and expense. Current routing systems often rely on a centralized external router trained on a fixed set of LLMs, making them inflexible and prone to poor performance since the small router can not fully understand the knowledge boundaries of different LLMs. We introduce DiSRouter (Distributed Self-Router), a novel paradigm that shifts from centralized control to distributed routing. In DiSRouter, a query traverses a network of LLM agents, each independently deciding whether to answer or route to other agents based on its own self-awareness, its ability to judge its competence. This distributed design offers superior flexibility, scalability, and generalizability. To enable this, we propose a two-stage Self-Awareness Training pipeline that enhances each LLM's self-awareness. Extensive experiments demonstrate that DiSRouter significantly outperforms existing routing methods in utility across various scenarios, effectively distinguishes between easy and hard queries, and shows strong generalization to out-of-domain tasks. Our work validates that leveraging an LLM's intrinsic self-awareness is more effective than external assessment, paving the way for more modular and efficient multi-agent systems.</li>
</ul>

<h3>Title: Modality Matching Matters: Calibrating Language Distances for Cross-Lingual Transfer in URIEL+</h3>
<ul>
<li><strong>Authors: </strong>York Hay Ng, Aditya Khan, Xiang Lu, Matteo Salloum, Michael Zhou, Phuong H. Hoang, A. Seza Doğruöz, En-Shiun Annie Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19217">https://arxiv.org/abs/2510.19217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19217">https://arxiv.org/pdf/2510.19217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19217]] Modality Matching Matters: Calibrating Language Distances for Cross-Lingual Transfer in URIEL+(https://arxiv.org/abs/2510.19217)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing linguistic knowledge bases such as URIEL+ provide valuable geographic, genetic and typological distances for cross-lingual transfer but suffer from two key limitations. One, their one-size-fits-all vector representations are ill-suited to the diverse structures of linguistic data, and two, they lack a principled method for aggregating these signals into a single, comprehensive score. In this paper, we address these gaps by introducing a framework for type-matched language distances. We propose novel, structure-aware representations for each distance type: speaker-weighted distributions for geography, hyperbolic embeddings for genealogy, and a latent variables model for typology. We unify these signals into a robust, task-agnostic composite distance. In selecting transfer languages, our representations and composite distances consistently improve performance across a wide range of NLP tasks, providing a more principled and effective toolkit for multilingual research.</li>
</ul>

<h3>Title: Brain-Inspired Perspective on Configurations: Unsupervised Similarity and Early Cognition</h3>
<ul>
<li><strong>Authors: </strong>Juntang Wang, Yihan Wang, Hao Wu, Dongmian Zou, Shixin Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19229">https://arxiv.org/abs/2510.19229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19229">https://arxiv.org/pdf/2510.19229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19229]] Brain-Inspired Perspective on Configurations: Unsupervised Similarity and Early Cognition(https://arxiv.org/abs/2510.19229)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Infants discover categories, detect novelty, and adapt to new contexts without supervision -- a challenge for current machine learning. We present a brain-inspired perspective on configurations, a finite-resolution clustering framework that uses a single resolution parameter and attraction-repulsion dynamics to yield hierarchical organization, novelty sensitivity, and flexible adaptation. To evaluate these properties, we introduce mheatmap, which provides proportional heatmaps and a reassignment algorithm to fairly assess multi-resolution and dynamic behavior. Across datasets, configurations are competitive on standard clustering metrics, achieve 87% AUC in novelty detection, and show 35% better stability during dynamic category evolution. These results position configurations as a principled computational model of early cognitive categorization and a step toward brain-inspired AI.</li>
</ul>

<h3>Title: SPOT: Scalable Policy Optimization with Trees for Markov Decision Processes</h3>
<ul>
<li><strong>Authors: </strong>Xuyuan Xiong, Pedro Chumpitaz-Flores, Kaixun Hua, Cheng Hua</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19241">https://arxiv.org/abs/2510.19241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19241">https://arxiv.org/pdf/2510.19241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19241]] SPOT: Scalable Policy Optimization with Trees for Markov Decision Processes(https://arxiv.org/abs/2510.19241)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Interpretable reinforcement learning policies are essential for high-stakes decision-making, yet optimizing decision tree policies in Markov Decision Processes (MDPs) remains challenging. We propose SPOT, a novel method for computing decision tree policies, which formulates the optimization problem as a mixed-integer linear program (MILP). To enhance efficiency, we employ a reduced-space branch-and-bound approach that decouples the MDP dynamics from tree-structure constraints, enabling efficient parallel search. This significantly improves runtime and scalability compared to previous methods. Our approach ensures that each iteration yields the optimal decision tree. Experimental results on standard benchmarks demonstrate that SPOT achieves substantial speedup and scales to larger MDPs with a significantly higher number of states. The resulting decision tree policies are interpretable and compact, maintaining transparency without compromising performance. These results demonstrate that our approach simultaneously achieves interpretability and scalability, delivering high-quality policies an order of magnitude faster than existing approaches.</li>
</ul>

<h3>Title: Interpret Policies in Deep Reinforcement Learning using SILVER with RL-Guided Labeling: A Model-level Approach to High-dimensional and Multi-action Environments</h3>
<ul>
<li><strong>Authors: </strong>Yiyu Qian, Su Nguyen, Chao Chen, Qinyue Zhou, Liyuan Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19244">https://arxiv.org/abs/2510.19244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19244">https://arxiv.org/pdf/2510.19244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19244]] Interpret Policies in Deep Reinforcement Learning using SILVER with RL-Guided Labeling: A Model-level Approach to High-dimensional and Multi-action Environments(https://arxiv.org/abs/2510.19244)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Deep reinforcement learning (RL) achieves remarkable performance but lacks interpretability, limiting trust in policy behavior. The existing SILVER framework (Li, Siddique, and Cao 2025) explains RL policy via Shapley-based regression but remains restricted to low-dimensional, binary-action domains. We propose SILVER with RL-guided labeling, an enhanced variant that extends SILVER to multi-action and high-dimensional environments by incorporating the RL policy's own action outputs into the boundary points identification. Our method first extracts compact feature representations from image observations, performs SHAP-based feature attribution, and then employs RL-guided labeling to generate behaviorally consistent boundary datasets. Surrogate models, such as decision trees and regression-based functions, are subsequently trained to interpret RL policy's decision structure. We evaluate the proposed framework on two Atari environments using three deep RL algorithms and conduct human-subject study to assess the clarity and trustworthiness of the derived interpretable policy. Results show that our approach maintains competitive task performance while substantially improving transparency and human understanding of agent behavior. This work advances explainable RL by transforming SILVER into a scalable and behavior-aware framework for interpreting deep RL agents in high-dimensional, multi-action settings.</li>
</ul>

<h3>Title: SheetBrain: A Neuro-Symbolic Agent for Accurate Reasoning over Complex and Large Spreadsheets</h3>
<ul>
<li><strong>Authors: </strong>Ziwei Wang, Jiayuan Su, Mengyu Zhou, Huaxing Zeng, Mengni Jia, Xiao Lv, Haoyu Dong, Xiaojun Ma, Shi Han, Dongmei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19247">https://arxiv.org/abs/2510.19247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19247">https://arxiv.org/pdf/2510.19247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19247]] SheetBrain: A Neuro-Symbolic Agent for Accurate Reasoning over Complex and Large Spreadsheets(https://arxiv.org/abs/2510.19247)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding and reasoning over complex spreadsheets remain fundamental challenges for large language models (LLMs), which often struggle with accurately capturing the complex structure of tables and ensuring reasoning correctness. In this work, we propose SheetBrain, a neuro-symbolic dual workflow agent framework designed for accurate reasoning over tabular data, supporting both spreadsheet question answering and manipulation tasks. SheetBrain comprises three core modules: an understanding module, which produces a comprehensive overview of the spreadsheet - including sheet summary and query-based problem insight to guide reasoning; an execution module, which integrates a Python sandbox with preloaded table-processing libraries and an Excel helper toolkit for effective multi-turn reasoning; and a validation module, which verifies the correctness of reasoning and answers, triggering re-execution when necessary. We evaluate SheetBrain on multiple public tabular QA and manipulation benchmarks, and introduce SheetBench, a new benchmark targeting large, multi-table, and structurally complex spreadsheets. Experimental results show that SheetBrain significantly improves accuracy on both existing benchmarks and the more challenging scenarios presented in SheetBench. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Mixing Configurations for Downstream Prediction</h3>
<ul>
<li><strong>Authors: </strong>Juntang Wang, Hao Wu, Runkun Guo, Yihan Wang, Dongmian Zou, Shixin Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19248">https://arxiv.org/abs/2510.19248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19248">https://arxiv.org/pdf/2510.19248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19248]] Mixing Configurations for Downstream Prediction(https://arxiv.org/abs/2510.19248)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Humans possess an innate ability to group objects by similarity, a cognitive mechanism that clustering algorithms aim to emulate. Recent advances in community detection have enabled the discovery of configurations -- valid hierarchical clusterings across multiple resolution scales -- without requiring labeled data. In this paper, we formally characterize these configurations and identify similar emergent structures in register tokens within Vision Transformers. Unlike register tokens, configurations exhibit lower redundancy and eliminate the need for ad hoc selection. They can be learned through unsupervised or self-supervised methods, yet their selection or composition remains specific to the downstream task and input. Building on these insights, we introduce GraMixC, a plug-and-play module that extracts configurations, aligns them using our Reverse Merge/Split (RMS) technique, and fuses them via attention heads before forwarding them to any downstream predictor. On the DSN1 16S rRNA cultivation-media prediction task, GraMixC improves the R2 score from 0.6 to 0.9 across multiple methods, setting a new state of the art. We further validate GraMixC on standard tabular benchmarks, where it consistently outperforms single-resolution and static-feature baselines.</li>
</ul>

<h3>Title: Advances in 4D Representation: Geometry, Motion, and Interaction</h3>
<ul>
<li><strong>Authors: </strong>Mingrui Zhao, Sauradip Nag, Kai Wang, Aditya Vora, Guangda Ji, Peter Chun, Ali Mahdavi-Amiri, Hao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19255">https://arxiv.org/abs/2510.19255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19255">https://arxiv.org/pdf/2510.19255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19255]] Advances in 4D Representation: Geometry, Motion, and Interaction(https://arxiv.org/abs/2510.19255)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>We present a survey on 4D generation and reconstruction, a fast-evolving subfield of computer graphics whose developments have been propelled by recent advances in neural fields, geometric and motion deep learning, as well 3D generative artificial intelligence (GenAI). While our survey is not the first of its kind, we build our coverage of the domain from a unique and distinctive perspective of 4D representations\/}, to model 3D geometry evolving over time while exhibiting motion and interaction. Specifically, instead of offering an exhaustive enumeration of many works, we take a more selective approach by focusing on representative works to highlight both the desirable properties and ensuing challenges of each representation under different computation, application, and data scenarios. The main take-away message we aim to convey to the readers is on how to select and then customize the appropriate 4D representations for their tasks. Organizationally, we separate the 4D representations based on three key pillars: geometry, motion, and interaction. Our discourse will not only encompass the most popular representations of today, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS), but also bring attention to relatively under-explored representations in the 4D context, such as structured models and long-range motions. Throughout our survey, we will reprise the role of large language models (LLMs) and video foundational models (VFMs) in a variety of 4D applications, while steering our discussion towards their current limitations and how they can be addressed. We also provide a dedicated coverage on what 4D datasets are currently available, as well as what is lacking, in driving the subfield forward. Project page:this https URL</li>
</ul>

<h3>Title: FnRGNN: Distribution-aware Fairness in Graph Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Soyoung Park, Sungsu Lim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19257">https://arxiv.org/abs/2510.19257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19257">https://arxiv.org/pdf/2510.19257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19257]] FnRGNN: Distribution-aware Fairness in Graph Neural Network(https://arxiv.org/abs/2510.19257)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) excel at learning from structured data, yet fairness in regression tasks remains underexplored. Existing approaches mainly target classification and representation-level debiasing, which cannot fully address the continuous nature of node-level regression. We propose FnRGNN, a fairness-aware in-processing framework for GNN-based node regression that applies interventions at three levels: (i) structure-level edge reweighting, (ii) representation-level alignment via MMD, and (iii) prediction-level normalization through Sinkhorn-based distribution matching. This multi-level strategy ensures robust fairness under complex graph topologies. Experiments on four real-world datasets demonstrate that FnRGNN reduces group disparities without sacrificing performance. Code is available at this https URL.</li>
</ul>

<h3>Title: LAPRAD: LLM-Assisted PRotocol Attack Discovery</h3>
<ul>
<li><strong>Authors: </strong>R.Can Aygun (UCLA), Yehuda Afek (Tel-Aviv University), Anat Bremler-Barr (Tel-Aviv University), Leonard Kleinrock (UCLA)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19264">https://arxiv.org/abs/2510.19264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19264">https://arxiv.org/pdf/2510.19264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19264]] LAPRAD: LLM-Assisted PRotocol Attack Discovery(https://arxiv.org/abs/2510.19264)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>With the goal of improving the security of Internet protocols, we seek faster, semi-automatic methods to discover new vulnerabilities in protocols such as DNS, BGP, and others. To this end, we introduce the LLM-Assisted Protocol Attack Discovery (LAPRAD) methodology, enabling security researchers with some DNS knowledge to efficiently uncover vulnerabilities that would otherwise be hard to detect. LAPRAD follows a three-stage process. In the first, we consult an LLM (GPT-o1) that has been trained on a broad corpus of DNS-related sources and previous DDoS attacks to identify potential exploits. In the second stage, a different LLM automatically constructs the corresponding attack configurations using the ReACT approach implemented via LangChain (DNS zone file generation). Finally, in the third stage, we validate the attack's functionality and effectiveness. Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and rediscovered two recently reported ones that were not included in the LLM's training data. The first new attack employs a bait-and-switch technique to trick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving capacity to as little as 6%. The second exploits large DNSSEC encryption algorithms (RSA-4096) with multiple keys, thereby bypassing a recently implemented default RRSet limit. The third leverages ANY-type responses to produce a similar effect. These variations of a cache-flushing DDoS attack, called SigCacheFlush, circumvent existing patches, severely degrade resolver query capacity, and impact the latest versions of major DNS resolver implementations.</li>
</ul>

<h3>Title: Difficulty-Controllable Multiple-Choice Question Generation Using Large Language Models and Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yuto Tomikawa, Masaki Uto</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19265">https://arxiv.org/abs/2510.19265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19265">https://arxiv.org/pdf/2510.19265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19265]] Difficulty-Controllable Multiple-Choice Question Generation Using Large Language Models and Direct Preference Optimization(https://arxiv.org/abs/2510.19265)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Difficulty-controllable question generation for reading comprehension has gained significant attention in the field of education as a fundamental tool for adaptive learning support. Although several neural question generation methods have recently succeeded in controlling difficulty, conventional approaches still face two major limitations. First, they cannot directly generate multiple-choice questions, which are the most widely used question type in educational contexts. Second, they are not explicitly trained to optimize the accuracy of difficulty control, leaving room for further improvement in difficulty controllability. To address these limitations, this study proposes a novel difficulty-controllable multiple-choice question generation method for reading comprehension which leverages a large language model trained using a direct preference optimization technique to improve the accuracy of difficulty control.</li>
</ul>

<h3>Title: Data Efficient Any Transformer-to-Mamba Distillation via Attention Bridge</h3>
<ul>
<li><strong>Authors: </strong>Penghao Wang, Yuhao Zhou, Mengxuan Wu, Panpan Zhang, Zhangyang Wang, Kai Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19266">https://arxiv.org/abs/2510.19266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19266">https://arxiv.org/pdf/2510.19266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19266]] Data Efficient Any Transformer-to-Mamba Distillation via Attention Bridge(https://arxiv.org/abs/2510.19266)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>State-space models (SSMs) have emerged as efficient alternatives to Transformers for sequence modeling, offering superior scalability through recurrent structures. However, their training remains costly and the ecosystem around them is far less mature than that of Transformers. Moreover, the structural heterogeneity between SSMs and Transformers makes it challenging to efficiently distill knowledge from pretrained attention models. In this work, we propose Cross-architecture distillation via Attention Bridge (CAB), a novel data-efficient distillation framework that efficiently transfers attention knowledge from Transformer teachers to state-space student models. Unlike conventional knowledge distillation that transfers knowledge only at the output level, CAB enables token-level supervision via a lightweight bridge and flexible layer-wise alignment, improving both efficiency and transferability. We further introduce flexible layer-wise alignment strategies to accommodate architectural discrepancies between teacher and student. Extensive experiments across vision and language domains demonstrate that our method consistently improves the performance of state-space models, even under limited training data, outperforming both standard and cross-architecture distillation methods. Our findings suggest that attention-based knowledge can be efficiently transferred to recurrent models, enabling rapid utilization of Transformer expertise for building a stronger SSM community.</li>
</ul>

<h3>Title: SCEESR: Semantic-Control Edge Enhancement for Diffusion-Based Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Yun Kai Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19272">https://arxiv.org/abs/2510.19272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19272">https://arxiv.org/pdf/2510.19272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19272]] SCEESR: Semantic-Control Edge Enhancement for Diffusion-Based Super-Resolution(https://arxiv.org/abs/2510.19272)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Real-world image super-resolution (Real-ISR) must handle complex degradations and inherent reconstruction ambiguities. While generative models have improved perceptual quality, a key trade-off remains with computational cost. One-step diffusion models offer speed but often produce structural inaccuracies due to distillation artifacts. To address this, we propose a novel SR framework that enhances a one-step diffusion model using a ControlNet mechanism for semantic edge guidance. This integrates edge information to provide dynamic structural control during single-pass inference. We also introduce a hybrid loss combining L2, LPIPS, and an edge-aware AME loss to optimize for pixel accuracy, perceptual quality, and geometric precision. Experiments show our method effectively improves structural integrity and realism while maintaining the efficiency of one-step generation, achieving a superior balance between output quality and inference speed. The results of test datasets will be published at this https URL and the related code will be published at this https URL.</li>
</ul>

<h3>Title: MobiAct: Efficient MAV Action Recognition Using MobileNetV4 with Contrastive Learning and Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Zhang Nengbo, Ho Hann Woei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19273">https://arxiv.org/abs/2510.19273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19273">https://arxiv.org/pdf/2510.19273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19273]] MobiAct: Efficient MAV Action Recognition Using MobileNetV4 with Contrastive Learning and Knowledge Distillation(https://arxiv.org/abs/2510.19273)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate and efficient recognition of Micro Air Vehicle (MAV) motion is essential for enabling real-time perception and coordination in autonomous aerial swarm. However, most existing approaches rely on large, computationally intensive models that are unsuitable for resource-limited MAV platforms, which results in a trade-off between recognition accuracy and inference speed. To address these challenges, this paper proposes a lightweight MAV action recognition framework, MobiAct, designed to achieve high accuracy with low computational cost. Specifically, MobiAct adopts MobileNetV4 as the backbone network and introduces a Stage-wise Orthogonal Knowledge Distillation (SOKD) strategy to effectively transfer MAV motion features from a teacher network (ResNet18) to a student network, thereby enhancing knowledge transfer efficiency. Furthermore, a parameter-free attention mechanism is integrated into the architecture to improve recognition accuracy without increasing model complexity. In addition, a hybrid loss training strategy is developed to combine multiple loss objectives, which ensures stable and robust optimization during training. Experimental results demonstrate that the proposed MobiAct achieves low-energy and low-computation MAV action recognition, while maintaining the fastest action decoding speed among compared methods. Across all three self-collected datasets, MobiAct achieves an average recognition accuracy of 92.12%, while consuming only 136.16 pJ of energy and processing recognition at a rate of 8.84 actions per second. Notably, MobiAct decodes actions up to 2 times faster than the leading method, with highly comparable recognition accuracy, highlighting its superior efficiency in MAV action recognition.</li>
</ul>

<h3>Title: D2D: Detector-to-Differentiable Critic for Improved Numeracy in Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Nobline Yoo, Olga Russakovsky, Ye Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19278">https://arxiv.org/abs/2510.19278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19278">https://arxiv.org/pdf/2510.19278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19278]] D2D: Detector-to-Differentiable Critic for Improved Numeracy in Text-to-Image Generation(https://arxiv.org/abs/2510.19278)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) diffusion models have achieved strong performance in semantic alignment, yet they still struggle with generating the correct number of objects specified in prompts. Existing approaches typically incorporate auxiliary counting networks as external critics to enhance numeracy. However, since these critics must provide gradient guidance during generation, they are restricted to regression-based models that are inherently differentiable, thus excluding detector-based models with superior counting ability, whose count-via-enumeration nature is non-differentiable. To overcome this limitation, we propose Detector-to-Differentiable (D2D), a novel framework that transforms non-differentiable detection models into differentiable critics, thereby leveraging their superior counting ability to guide numeracy generation. Specifically, we design custom activation functions to convert detector logits into soft binary indicators, which are then used to optimize the noise prior at inference time with pre-trained T2I models. Our extensive experiments on SDXL-Turbo, SD-Turbo, and Pixart-DMD across four benchmarks of varying complexity (low-density, high-density, and multi-object scenarios) demonstrate consistent and substantial improvements in object counting accuracy (e.g., boosting up to 13.7% on D2D-Small, a 400-prompt, low-density benchmark), with minimal degradation in overall image quality and computational overhead.</li>
</ul>

<h3>Title: Enhancing Early Alzheimer Disease Detection through Big Data and Ensemble Few-Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Safa Ben Atitallah, Maha Driss, Wadii Boulila, Anis Koubaa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19282">https://arxiv.org/abs/2510.19282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19282">https://arxiv.org/pdf/2510.19282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19282]] Enhancing Early Alzheimer Disease Detection through Big Data and Ensemble Few-Shot Learning(https://arxiv.org/abs/2510.19282)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Alzheimer disease is a severe brain disorder that causes harm in various brain areas and leads to memory damage. The limited availability of labeled medical data poses a significant challenge for accurate Alzheimer disease detection. There is a critical need for effective methods to improve the accuracy of Alzheimer disease detection, considering the scarcity of labeled data, the complexity of the disease, and the constraints related to data privacy. To address this challenge, our study leverages the power of big data in the form of pre-trained Convolutional Neural Networks (CNNs) within the framework of Few-Shot Learning (FSL) and ensemble learning. We propose an ensemble approach based on a Prototypical Network (ProtoNet), a powerful method in FSL, integrating various pre-trained CNNs as encoders. This integration enhances the richness of features extracted from medical images. Our approach also includes a combination of class-aware loss and entropy loss to ensure a more precise classification of Alzheimer disease progression levels. The effectiveness of our method was evaluated using two datasets, the Kaggle Alzheimer dataset and the ADNI dataset, achieving an accuracy of 99.72% and 99.86%, respectively. The comparison of our results with relevant state-of-the-art studies demonstrated that our approach achieved superior accuracy and highlighted its validity and potential for real-world applications in early Alzheimer disease detection.</li>
</ul>

<h3>Title: TheMCPCompany: Creating General-purpose Agents with Task-specific Tools</h3>
<ul>
<li><strong>Authors: </strong>Reza Esfandiarpoor, Vishwas Suryanarayanan, Stephen H. Bach, Vishal Chowdhary, Anthony Aue</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19286">https://arxiv.org/abs/2510.19286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19286">https://arxiv.org/pdf/2510.19286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19286]] TheMCPCompany: Creating General-purpose Agents with Task-specific Tools(https://arxiv.org/abs/2510.19286)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Since the introduction of the Model Context Protocol (MCP), the number of available tools for Large Language Models (LLMs) has increased significantly. These task-specific tool sets offer an alternative to general-purpose tools such as web browsers, while being easier to develop and maintain than GUIs. However, current general-purpose agents predominantly rely on web browsers for interacting with the environment. Here, we introduce TheMCPCompany, a benchmark for evaluating tool-calling agents on tasks that involve interacting with various real-world services. We use the REST APIs of these services to create MCP servers, which include over 18,000 tools. We also provide manually annotated ground-truth tools for each task. In our experiments, we use the ground truth tools to show the potential of tool-calling agents for both improving performance and reducing costs assuming perfect tool retrieval. Next, we explore agent performance using tool retrieval to study the real-world practicality of tool-based agents. While all models with tool retrieval perform similarly or better than browser-based agents, smaller models cannot take full advantage of the available tools through retrieval. On the other hand, GPT-5's performance with tool retrieval is very close to its performance with ground-truth tools. Overall, our work shows that the most advanced reasoning models are effective at discovering tools in simpler environments, but seriously struggle with navigating complex enterprise environments. TheMCPCompany reveals that navigating tens of thousands of tools and combining them in non-trivial ways to solve complex problems is still a challenging task for current models and requires both better reasoning and better retrieval models.</li>
</ul>

<h3>Title: Reliability and Resilience of AI-Driven Critical Network Infrastructure under Cyber-Physical Threats</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos A. Lizos, Leandros Maglaras, Elena Petrovik, Saied M. Abd El-atty, Georgios Tsachtsiris, Mohamed Amine Ferrag</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19295">https://arxiv.org/abs/2510.19295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19295">https://arxiv.org/pdf/2510.19295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19295]] Reliability and Resilience of AI-Driven Critical Network Infrastructure under Cyber-Physical Threats(https://arxiv.org/abs/2510.19295)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>The increasing reliance on AI-driven 5G/6G network infrastructures for mission-critical services highlights the need for reliability and resilience against sophisticated cyber-physical threats. These networks are highly exposed to novel attack surfaces due to their distributed intelligence, virtualized resources, and cross-domain integration. This paper proposes a fault-tolerant and resilience-aware framework that integrates AI-driven anomaly detection, adaptive routing, and redundancy mechanisms to mitigate cascading failures under cyber-physical attack conditions. A comprehensive validation is carried out using NS-3 simulations, where key performance indicators such as reliability, latency, resilience index, and packet loss rate are analyzed under various attack scenarios. The deduced results demonstrate that the proposed framework significantly improves fault recovery, stabilizes packet delivery, and reduces service disruption compared to baseline approaches.</li>
</ul>

<h3>Title: QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhang, Rui Zhang, Jiaming Guo, Lei Huang, Di Huang, Yunpu Zhao, Shuyao Cheng, Pengwei Jin, Chongxiao Li, Zidong Du, Xing Hu, Qi Guo, Yunji Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19296">https://arxiv.org/abs/2510.19296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19296">https://arxiv.org/pdf/2510.19296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19296]] QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation(https://arxiv.org/abs/2510.19296)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The remarkable progress of Large Language Models (LLMs) presents promising opportunities for Verilog code generation which is significantly important for automated circuit design. The lacking of meaningful functional rewards hinders the preference optimization based on Reinforcement Learning (RL) for producing functionally correct Verilog code. In this paper, we propose Signal-Aware Learning for Verilog code generation (QiMeng-SALV) by leveraging code segments of functionally correct output signal to optimize RL training. Considering Verilog code specifies the structural interconnection of hardware gates and wires so that different output signals are independent, the key insight of QiMeng-SALV is to extract verified signal-aware implementations in partially incorrect modules, so as to enhance the extraction of meaningful functional rewards. Roughly, we verify the functional correctness of signals in generated module by comparing with that of reference module in the training data. Then abstract syntax tree (AST) is employed to identify signal-aware code segments which can provide meaningful functional rewards from erroneous modules. Finally, we introduce signal-aware DPO which is optimized on the correct signal-level code segments, thereby preventing noise and interference from incorrect signals. The proposed QiMeng-SALV underscores the paradigm shift from conventional module-level to fine-grained signal-level optimization in Verilog code generation, addressing the issue of insufficient functional rewards. Experiments demonstrate that our method achieves state-of-the-art performance on VerilogEval and RTLLM, with a 7B parameter model matching the performance of the DeepSeek v3 671B model and significantly outperforming the leading open-source model CodeV trained on the same dataset. Our code is available at this https URL.</li>
</ul>

<h3>Title: Collaborative penetration testing suite for emerging generative AI algorithms</h3>
<ul>
<li><strong>Authors: </strong>Petar Radanliev</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.MA, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19303">https://arxiv.org/abs/2510.19303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19303">https://arxiv.org/pdf/2510.19303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19303]] Collaborative penetration testing suite for emerging generative AI algorithms(https://arxiv.org/abs/2510.19303)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, generative</a></li>
<li><strong>Abstract: </strong>Problem Space: AI Vulnerabilities and Quantum Threats Generative AI vulnerabilities: model inversion, data poisoning, adversarial inputs. Quantum threats Shor Algorithm breaking RSA ECC encryption. Challenge Secure generative AI models against classical and quantum cyberattacks. Proposed Solution Collaborative Penetration Testing Suite Five Integrated Components: DAST SAST OWASP ZAP, Burp Suite, SonarQube, Fortify. IAST Contrast Assess integrated with CI CD pipeline. Blockchain Logging Hyperledger Fabric for tamper-proof logs. Quantum Cryptography Lattice based RLWE protocols. AI Red Team Simulations Adversarial ML & Quantum-assisted attacks. Integration Layer: Unified workflow for AI, cybersecurity, and quantum experts. Key Results 300+ vulnerabilities identified across test environments. 70% reduction in high-severity issues within 2 weeks. 90% resolution efficiency for blockchain-logged vulnerabilities. Quantum-resistant cryptography maintained 100% integrity in tests. Outcome: Quantum AI Security Protocol integrating Blockchain Quantum Cryptography AI Red Teaming.</li>
</ul>

<h3>Title: Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Jo, Jaesik Yoon, Justin Deschenaux, Caglar Gulcehre, Sungjin Ahn</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19304">https://arxiv.org/abs/2510.19304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19304">https://arxiv.org/pdf/2510.19304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19304]] Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall(https://arxiv.org/abs/2510.19304)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models offer a promising alternative to autoregressive generation through parallel decoding, but they suffer from a sampling wall: once categorical sampling occurs, rich distributional information collapses into one-hot vectors and cannot be propagated across steps, forcing subsequent steps to operate with limited information. To mitigate this problem, we introduce Loopholing, a novel and simple mechanism that preserves this information via a deterministic latent pathway, leading to Loopholing Discrete Diffusion Models (LDDMs). Trained efficiently with a self-conditioning strategy, LDDMs achieve substantial gains-reducing generative perplexity by up to 61% over prior baselines, closing (and in some cases surpassing) the gap with autoregressive models, and producing more coherent text. Applied to reasoning tasks, LDDMs also improve performance on arithmetic benchmarks such as Countdown and Game of 24. These results also indicate that loopholing mitigates idle steps and oscillations, providing a scalable path toward high-quality non-autoregressive text generation.</li>
</ul>

<h3>Title: FrogDeepSDM: Improving Frog Counting and Occurrence Prediction Using Multimodal Data and Pseudo-Absence Imputation</h3>
<ul>
<li><strong>Authors: </strong>Chirag Padubidri, Pranesh Velmurugan, Andreas Lanitis, Andreas Kamilaris</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19305">https://arxiv.org/abs/2510.19305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19305">https://arxiv.org/pdf/2510.19305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19305]] FrogDeepSDM: Improving Frog Counting and Occurrence Prediction Using Multimodal Data and Pseudo-Absence Imputation(https://arxiv.org/abs/2510.19305)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Monitoring species distribution is vital for conservation efforts, enabling the assessment of environmental impacts and the development of effective preservation strategies. Traditional data collection methods, including citizen science, offer valuable insights but remain limited in coverage and completeness. Species Distribution Modelling (SDM) helps address these gaps by using occurrence data and environmental variables to predict species presence across large regions. In this study, we enhance SDM accuracy for frogs (Anura) by applying deep learning and data imputation techniques using data from the "EY - 2022 Biodiversity Challenge." Our experiments show that data balancing significantly improved model performance, reducing the Mean Absolute Error (MAE) from 189 to 29 in frog counting tasks. Feature selection identified key environmental factors influencing occurrence, optimizing inputs while maintaining predictive accuracy. The multimodal ensemble model, integrating land cover, NDVI, and other environmental inputs, outperformed individual models and showed robust generalization across unseen regions. The fusion of image and tabular data improved both frog counting and habitat classification, achieving 84.9% accuracy with an AUC of 0.90. This study highlights the potential of multimodal learning and data preprocessing techniques such as balancing and imputation to improve predictive ecological modeling when data are sparse or incomplete, contributing to more precise and scalable biodiversity monitoring.</li>
</ul>

<h3>Title: Unified Reinforcement and Imitation Learning for Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Byung-Kwan Lee, Ryo Hachiuma, Yong Man Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19307">https://arxiv.org/abs/2510.19307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19307">https://arxiv.org/pdf/2510.19307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19307]] Unified Reinforcement and Imitation Learning for Vision-Language Models(https://arxiv.org/abs/2510.19307)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have achieved remarkable progress, yet their large scale often renders them impractical for resource-constrained environments. This paper introduces Unified Reinforcement and Imitation Learning (RIL), a novel and efficient training algorithm designed to create powerful, lightweight VLMs. RIL distinctively combines the strengths of reinforcement learning with adversarial imitation learning. This enables smaller student VLMs not only to mimic the sophisticated text generation of large teacher models but also to systematically improve their generative capabilities through reinforcement signals. Key to our imitation framework is an LLM-based discriminator that adeptly distinguishes between student and teacher outputs, complemented by guidance from multiple large teacher VLMs to ensure diverse learning. This unified learning strategy, leveraging both reinforcement and imitation, empowers student models to achieve significant performance gains, making them competitive with leading closed-source VLMs. Extensive experiments on diverse vision-language benchmarks demonstrate that RIL significantly narrows the performance gap with state-of-the-art open- and closed-source VLMs and, in several instances, surpasses them.</li>
</ul>

<h3>Title: JointCQ: Improving Factual Hallucination Detection with Joint Claim and Query Generation</h3>
<ul>
<li><strong>Authors: </strong>Fan Xu, Huixuan Zhang, Zhenliang Zhang, Jiahao Wang, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19310">https://arxiv.org/abs/2510.19310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19310">https://arxiv.org/pdf/2510.19310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19310]] JointCQ: Improving Factual Hallucination Detection with Joint Claim and Query Generation(https://arxiv.org/abs/2510.19310)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Current large language models (LLMs) often suffer from hallucination issues, i,e, generating content that appears factual but is actually unreliable. A typical hallucination detection pipeline involves response decomposition (i.e., claim extraction), query generation, evidence collection (i.e., search or retrieval), and claim verification. However, existing methods exhibit limitations in the first two stages, such as context loss during claim extraction and low specificity in query generation, resulting in degraded performance across the hallucination detection pipeline. In this work, we introduce JointCQ this https URL, a joint claim-and-query generation framework designed to construct an effective and efficient claim-query generator. Our framework leverages elaborately designed evaluation criteria to filter synthesized training data, and finetunes a language model for joint claim extraction and query generation, providing reliable and informative inputs for downstream search and verification. Experimental results demonstrate that our method outperforms previous methods on multiple open-domain QA hallucination detection benchmarks, advancing the goal of more trustworthy and transparent language model systems.</li>
</ul>

<h3>Title: HAD: HAllucination Detection Language Models Based on a Comprehensive Hallucination Taxonomy</h3>
<ul>
<li><strong>Authors: </strong>Fan Xu, Xinyu Hu, Zhenghan Yu, Li Lin, Xu Zhang, Yang Zhang, Wei Zhou, Jinjie Gu, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19318">https://arxiv.org/abs/2510.19318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19318">https://arxiv.org/pdf/2510.19318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19318]] HAD: HAllucination Detection Language Models Based on a Comprehensive Hallucination Taxonomy(https://arxiv.org/abs/2510.19318)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The increasing reliance on natural language generation (NLG) models, particularly large language models, has raised concerns about the reliability and accuracy of their outputs. A key challenge is hallucination, where models produce plausible but incorrect information. As a result, hallucination detection has become a critical task. In this work, we introduce a comprehensive hallucination taxonomy with 11 categories across various NLG tasks and propose the HAllucination Detection (HAD) models this https URL, which integrate hallucination detection, span-level identification, and correction into a single inference process. Trained on an elaborate synthetic dataset of about 90K samples, our HAD models are versatile and can be applied to various NLG tasks. We also carefully annotate a test set for hallucination detection, called HADTest, which contains 2,248 samples. Evaluations on in-domain and out-of-domain test sets show that our HAD models generally outperform the existing baselines, achieving state-of-the-art results on HaluEval, FactCHD, and FaithBench, confirming their robustness and versatility.</li>
</ul>

<h3>Title: Online Handwritten Signature Verification Based on Temporal-Spatial Graph Attention Transformer</h3>
<ul>
<li><strong>Authors: </strong>Hai-jie Yuan, Heng Zhang, Fei Yin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19321">https://arxiv.org/abs/2510.19321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19321">https://arxiv.org/pdf/2510.19321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19321]] Online Handwritten Signature Verification Based on Temporal-Spatial Graph Attention Transformer(https://arxiv.org/abs/2510.19321)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Handwritten signature verification is a crucial aspect of identity authentication, with applications in various domains such as finance and e-commerce. However, achieving high accuracy in signature verification remains challenging due to intra-user variability and the risk of forgery. This paper introduces a novel approach for dynamic signature verification: the Temporal-Spatial Graph Attention Transformer (TS-GATR). TS-GATR combines the Graph Attention Network (GAT) and the Gated Recurrent Unit (GRU) to model both spatial and temporal dependencies in signature data. TS-GATR enhances verification performance by representing signatures as graphs, where each node captures dynamic features (e.g. position, velocity, pressure), and by using attention mechanisms to model their complex relationships. The proposed method further employs a Dual-Graph Attention Transformer (DGATR) module, which utilizes k-step and k-nearest neighbor adjacency graphs to model local and global spatial features, respectively. To capture long-term temporal dependencies, the model integrates GRU, thereby enhancing its ability to learn dynamic features during signature verification. Comprehensive experiments conducted on benchmark datasets such as MSDS and DeepSignDB show that TS-GATR surpasses current state-of-the-art approaches, consistently achieving lower Equal Error Rates (EER) across various scenarios.</li>
</ul>

<h3>Title: Authorization of Knowledge-base Agents in an Intent-based Management Function</h3>
<ul>
<li><strong>Authors: </strong>Loay Abdelrazek, Leyli Karaçay, Marin Orlic</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19324">https://arxiv.org/abs/2510.19324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19324">https://arxiv.org/pdf/2510.19324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19324]] Authorization of Knowledge-base Agents in an Intent-based Management Function(https://arxiv.org/abs/2510.19324)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>As networks move toward the next-generation 6G, Intent-based Management (IbM) systems are increasingly adopted to simplify and automate network management by translating high-level intents into low-level configurations. Within these systems, agents play a critical role in monitoring current state of the network, gathering data, and enforcing actions across the network to fulfill the intent. However, ensuring secure and fine-grained authorization of agents remains a significant challenge, especially in dynamic and multi-tenant environments. Traditional models such as Role-Based Access Control (RBAC), Attribute-Based Access Control (ABAC) and Relational-Based Access Control (RelBAC) often lack the flexibility to accommodate the evolving context and granularity required by intentbased operations. In this paper, we propose an enhanced authorization framework that integrates contextual and functional attributes with agent roles to achieve dynamic, policy-driven access control. By analyzing agent functionalities, our approach ensures that agents are granted only the minimal necessary privileges towards knowledge graphs.</li>
</ul>

<h3>Title: Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization</h3>
<ul>
<li><strong>Authors: </strong>Junjie Song, Yiwen Liu, Dapeng Li, Yin Sun, Shukun Fu, Siqi Chen, Yuji Cao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19325">https://arxiv.org/abs/2510.19325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19325">https://arxiv.org/pdf/2510.19325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19325]] Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization(https://arxiv.org/abs/2510.19325)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text summarization is a crucial task that requires the simultaneous optimization of multiple objectives, including consistency, coherence, relevance, and fluency, which presents considerable challenges. Although large language models (LLMs) have demonstrated remarkable performance, enhanced by reinforcement learning (RL), few studies have focused on optimizing the multi-objective problem of summarization through RL based on LLMs. In this paper, we introduce hypervolume optimization (HVO), a novel optimization strategy that dynamically adjusts the scores between groups during the reward process in RL by using the hypervolume method. This method guides the model's optimization to progressively approximate the pareto front, thereby generating balanced summaries across multiple objectives. Experimental results on several representative summarization datasets demonstrate that our method outperforms group relative policy optimization (GRPO) in overall scores and shows more balanced performance across different dimensions. Moreover, a 7B foundation model enhanced by HVO performs comparably to GPT-4 in the summarization task, while maintaining a shorter generation length. Our code is publicly available at this https URL</li>
</ul>

<h3>Title: Slot Filling as a Reasoning Task for SpeechLLMs</h3>
<ul>
<li><strong>Authors: </strong>Kadri Hacioglu, Manjunath K E, Andreas Stolcke</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19326">https://arxiv.org/abs/2510.19326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19326">https://arxiv.org/pdf/2510.19326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19326]] Slot Filling as a Reasoning Task for SpeechLLMs(https://arxiv.org/abs/2510.19326)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose integration of reasoning into speech large language models (speechLLMs) for the end-to-end slot-filling task. Inspired by the recent development of reasoning LLMs, we use a chain-of-thought framework to decompose the slot-filling task into multiple reasoning steps, create a reasoning dataset and apply the supervised fine-tuning strategy to a speechLLM. We distinguish between regular and reasoning speechLLMs and experiment with different types and sizes of LLMs as their text foundation models. We demonstrate performance improvements by introducing reasoning (intermediate) steps. However, we show that a reasoning textual LLM developed mainly for math, logic and coding domains might be inferior as a foundation model for a reasoning speechLLM. We further show that hybrid speechLLMs, built on a hybrid text foundation LLM and fine-tuned to preserve both direct and reasoning modes of operation, have better performance than those fine-tuned employing only one mode of operation.</li>
</ul>

<h3>Title: Seabed-Net: A multi-task network for joint bathymetry estimation and seabed classification from remote sensing imagery in shallow waters</h3>
<ul>
<li><strong>Authors: </strong>Panagiotis Agrafiotis, Begüm Demir</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19329">https://arxiv.org/abs/2510.19329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19329">https://arxiv.org/pdf/2510.19329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19329]] Seabed-Net: A multi-task network for joint bathymetry estimation and seabed classification from remote sensing imagery in shallow waters(https://arxiv.org/abs/2510.19329)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Accurate, detailed, and regularly updated bathymetry, coupled with complex semantic content, is essential for under-mapped shallow-water environments facing increasing climatological and anthropogenic pressures. However, existing approaches that derive either depth or seabed classes from remote sensing imagery treat these tasks in isolation, forfeiting the mutual benefits of their interaction and hindering the broader adoption of deep learning methods. To address these limitations, we introduce Seabed-Net, a unified multi-task framework that simultaneously predicts bathymetry and pixel-based seabed classification from remote sensing imagery of various resolutions. Seabed-Net employs dual-branch encoders for bathymetry estimation and pixel-based seabed classification, integrates cross-task features via an Attention Feature Fusion module and a windowed Swin-Transformer fusion block, and balances objectives through dynamic task uncertainty weighting. In extensive evaluations at two heterogeneous coastal sites, it consistently outperforms traditional empirical models and traditional machine learning regression methods, achieving up to 75\% lower RMSE. It also reduces bathymetric RMSE by 10-30\% compared to state-of-the-art single-task and multi-task baselines and improves seabed classification accuracy up to 8\%. Qualitative analyses further demonstrate enhanced spatial consistency, sharper habitat boundaries, and corrected depth biases in low-contrast regions. These results confirm that jointly modeling depth with both substrate and seabed habitats yields synergistic gains, offering a robust, open solution for integrated shallow-water mapping. Code and pretrained weights are available at this https URL.</li>
</ul>

<h3>Title: Algorithmic Fairness in NLP: Persona-Infused LLMs for Human-Centric Hate Speech Detection</h3>
<ul>
<li><strong>Authors: </strong>Ewelina Gajewska, Arda Derbent, Jaroslaw A Chudziak, Katarzyna Budzynska</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19331">https://arxiv.org/abs/2510.19331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19331">https://arxiv.org/pdf/2510.19331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19331]] Algorithmic Fairness in NLP: Persona-Infused LLMs for Human-Centric Hate Speech Detection(https://arxiv.org/abs/2510.19331)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate how personalising Large Language Models (Persona-LLMs) with annotator personas affects their sensitivity to hate speech, particularly regarding biases linked to shared or differing identities between annotators and targets. To this end, we employ Google's Gemini and OpenAI's GPT-4.1-mini models and two persona-prompting methods: shallow persona prompting and a deeply contextualised persona development based on Retrieval-Augmented Generation (RAG) to incorporate richer persona profiles. We analyse the impact of using in-group and out-group annotator personas on the models' detection performance and fairness across diverse social groups. This work bridges psychological insights on group identity with advanced NLP techniques, demonstrating that incorporating socio-demographic attributes into LLMs can address bias in automated hate speech detection. Our results highlight both the potential and limitations of persona-based approaches in reducing bias, offering valuable insights for developing more equitable hate speech detection systems.</li>
</ul>

<h3>Title: A Training-Free Framework for Open-Vocabulary Image Segmentation and Recognition with EfficientNet and CLIP</h3>
<ul>
<li><strong>Authors: </strong>Ying Dai, Wei Yu Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19333">https://arxiv.org/abs/2510.19333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19333">https://arxiv.org/pdf/2510.19333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19333]] A Training-Free Framework for Open-Vocabulary Image Segmentation and Recognition with EfficientNet and CLIP(https://arxiv.org/abs/2510.19333)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>This paper presents a novel training-free framework for open-vocabulary image segmentation and object recognition (OVSR), which leverages EfficientNetB0, a convolutional neural network, for unsupervised segmentation and CLIP, a vision-language model, for open-vocabulary object recognition. The proposed framework adopts a two stage pipeline: unsupervised image segmentation followed by segment-level recognition via vision-language alignment. In the first stage, pixel-wise features extracted from EfficientNetB0 are decomposed using singular value decomposition to obtain latent representations, which are then clustered using hierarchical clustering to segment semantically meaningful regions. The number of clusters is adaptively determined by the distribution of singular values. In the second stage, the segmented regions are localized and encoded into image embeddings using the Vision Transformer backbone of CLIP. Text embeddings are precomputed using CLIP's text encoder from category-specific prompts, including a generic something else prompt to support open set recognition. The image and text embeddings are concatenated and projected into a shared latent feature space via SVD to enhance cross-modal alignment. Recognition is performed by computing the softmax over the similarities between the projected image and text embeddings. The proposed method is evaluated on standard benchmarks, including COCO, ADE20K, and PASCAL VOC, achieving state-of-the-art performance in terms of Hungarian mIoU, precision, recall, and F1-score. These results demonstrate the effectiveness, flexibility, and generalizability of the proposed framework.</li>
</ul>

<h3>Title: DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents</h3>
<ul>
<li><strong>Authors: </strong>Kai Shi, Jun Yang, Ni Yang, Binqiang Pan, Qingsong Xie, Chao Zhang, Zhenyu Yang, Tianhuang Su, Haonan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19336">https://arxiv.org/abs/2510.19336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19336">https://arxiv.org/pdf/2510.19336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19336]] DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents(https://arxiv.org/abs/2510.19336)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Mobile Phone Agents (MPAs) have emerged as a promising research direction due to their broad applicability across diverse scenarios. While Multimodal Large Language Models (MLLMs) serve as the foundation for MPAs, their effectiveness in handling multiple mobile phone tasks simultaneously remains limited. Although multitask supervised fine-tuning (SFT) is widely adopted for multitask learning, existing approaches struggle to determine optimal training data compositions for peak performance. To address this challenge, we propose DaMo (Data Mixture Optimizer) - a novel solution employing a trainable network that predicts optimal data mixtures by forecasting downstream task performance for any given dataset ratio. To support comprehensive evaluation, we introduce PhoneAgentBench, the first specialized benchmark to evaluate MLLMs on multimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse real-world industrial mobile application scenarios. Demonstrating strong predictive capability (R^2=0.81) in small-scale pilot experiments, DaMo efficiently extrapolates optimal data mixing configurations. Our results show DaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to alternative methods. Furthermore, extensive experiments across established benchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench reveal DaMo's superior generalization, outperforming other approaches by 2.57% in terms of average score. When used solely for MLLM optimization on the BFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably, DaMo maintains robust scalability, preserving its effectiveness when applied to other model architectures. The code and dataset are available at this https URL</li>
</ul>

<h3>Title: Local Obfuscation by GLINER for Impartial Context Aware Lineage: Development and evaluation of PII Removal system</h3>
<ul>
<li><strong>Authors: </strong>Prakrithi Shivaprakash, Lekhansh Shukla, Animesh Mukherjee, Prabhat Chand, Pratima Murthy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19346">https://arxiv.org/abs/2510.19346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19346">https://arxiv.org/pdf/2510.19346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19346]] Local Obfuscation by GLINER for Impartial Context Aware Lineage: Development and evaluation of PII Removal system(https://arxiv.org/abs/2510.19346)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Removing Personally Identifiable Information (PII) from clinical notes in Electronic Health Records (EHRs) is essential for research and AI development. While Large Language Models (LLMs) are powerful, their high computational costs and the data privacy risks of API-based services limit their use, especially in low-resource settings. To address this, we developed LOGICAL (Local Obfuscation by GLINER for Impartial Context-Aware Lineage), an efficient, locally deployable PII removal system built on a fine-tuned Generalist and Lightweight Named Entity Recognition (GLiNER) model. We used 1515 clinical documents from a psychiatric hospital's EHR system. We defined nine PII categories for removal. A modern-gliner-bi-large-v1.0 model was fine-tuned on 2849 text instances and evaluated on a test set of 376 instances using character-level precision, recall, and F1-score. We compared its performance against Microsoft Azure NER, Microsoft Presidio, and zero-shot prompting with Gemini-Pro-2.5 and Llama-3.3-70B-Instruct. The fine-tuned GLiNER model achieved superior performance, with an overall micro-average F1-score of 0.980, significantly outperforming Gemini-Pro-2.5 (F1-score: 0.845). LOGICAL correctly sanitised 95% of documents completely, compared to 64% for the next-best solution. The model operated efficiently on a standard laptop without a dedicated GPU. However, a 2% entity-level false negative rate underscores the need for human-in-the-loop validation across all tested systems. Fine-tuned, specialised transformer models like GLiNER offer an accurate, computationally efficient, and secure solution for PII removal from clinical notes. This "sanitisation at the source" approach is a practical alternative to resource-intensive LLMs, enabling the creation of de-identified datasets for research and AI development while preserving data privacy, particularly in resource-constrained environments.</li>
</ul>

<h3>Title: A New Type of Adversarial Examples</h3>
<ul>
<li><strong>Authors: </strong>Xingyang Nie, Guojie Xiao, Su Pan, Biao Wang, Huilin Ge, Tao Fang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19347">https://arxiv.org/abs/2510.19347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19347">https://arxiv.org/pdf/2510.19347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19347]] A New Type of Adversarial Examples(https://arxiv.org/abs/2510.19347)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Most machine learning models are vulnerable to adversarial examples, which poses security concerns on these models. Adversarial examples are crafted by applying subtle but intentionally worst-case modifications to examples from the dataset, leading the model to output a different answer from the original example. In this paper, adversarial examples are formed in an exactly opposite manner, which are significantly different from the original examples but result in the same answer. We propose a novel set of algorithms to produce such adversarial examples, including the negative iterative fast gradient sign method (NI-FGSM) and the negative iterative fast gradient method (NI-FGM), along with their momentum variants: the negative momentum iterative fast gradient sign method (NMI-FGSM) and the negative momentum iterative fast gradient method (NMI-FGM). Adversarial examples constructed by these methods could be used to perform an attack on machine learning systems in certain occasions. Moreover, our results show that the adversarial examples are not merely distributed in the neighbourhood of the examples from the dataset; instead, they are distributed extensively in the sample space.</li>
</ul>

<h3>Title: ConvXformer: Differentially Private Hybrid ConvNeXt-Transformer for Inertial Navigation</h3>
<ul>
<li><strong>Authors: </strong>Omer Tariq, Muhammad Bilal, Muneeb Ul Hassan, Dongsoo Han, Jon Crowcroft</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19352">https://arxiv.org/abs/2510.19352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19352">https://arxiv.org/pdf/2510.19352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19352]] ConvXformer: Differentially Private Hybrid ConvNeXt-Transformer for Inertial Navigation(https://arxiv.org/abs/2510.19352)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, robust, transformer</a></li>
<li><strong>Abstract: </strong>Data-driven inertial sequence learning has revolutionized navigation in GPS-denied environments, offering superior odometric resolution compared to traditional Bayesian methods. However, deep learning-based inertial tracking systems remain vulnerable to privacy breaches that can expose sensitive training data. \hl{Existing differential privacy solutions often compromise model performance by introducing excessive noise, particularly in high-frequency inertial measurements.} In this article, we propose ConvXformer, a hybrid architecture that fuses ConvNeXt blocks with Transformer encoders in a hierarchical structure for robust inertial navigation. We propose an efficient differential privacy mechanism incorporating adaptive gradient clipping and gradient-aligned noise injection (GANI) to protect sensitive information while ensuring model performance. Our framework leverages truncated singular value decomposition for gradient processing, enabling precise control over the privacy-utility trade-off. Comprehensive performance evaluations on benchmark datasets (OxIOD, RIDI, RoNIN) demonstrate that ConvXformer surpasses state-of-the-art methods, achieving more than 40% improvement in positioning accuracy while ensuring $(\epsilon,\delta)$-differential privacy guarantees. To validate real-world performance, we introduce the Mech-IO dataset, collected from the mechanical engineering building at KAIST, where intense magnetic fields from industrial equipment induce significant sensor perturbations. This demonstrated robustness under severe environmental distortions makes our framework well-suited for secure and intelligent navigation in cyber-physical systems.</li>
</ul>

<h3>Title: DARE: A Deformable Adaptive Regularization Estimator for Learning-Based Medical Image Registration</h3>
<ul>
<li><strong>Authors: </strong>Ahsan Raza Siyal, Markus Haltmeier, Ruth Steiger, Malik Galijasevic, Elke Ruth Gizewski, Astrid Ellen Grams</a></li>
<li><strong>Subjects: </strong>cs.CV, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19353">https://arxiv.org/abs/2510.19353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19353">https://arxiv.org/pdf/2510.19353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19353]] DARE: A Deformable Adaptive Regularization Estimator for Learning-Based Medical Image Registration(https://arxiv.org/abs/2510.19353)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deformable medical image registration is a fundamental task in medical image analysis. While deep learning-based methods have demonstrated superior accuracy and computational efficiency compared to traditional techniques, they often overlook the critical role of regularization in ensuring robustness and anatomical plausibility. We propose DARE (Deformable Adaptive Regularization Estimator), a novel registration framework that dynamically adjusts elastic regularization based on the gradient norm of the deformation field. Our approach integrates strain and shear energy terms, which are adaptively modulated to balance stability and flexibility. To ensure physically realistic transformations, DARE includes a folding-prevention mechanism that penalizes regions with negative deformation Jacobian. This strategy mitigates non-physical artifacts such as folding, avoids over-smoothing, and improves both registration accuracy and anatomical plausibility</li>
</ul>

<h3>Title: M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yejin Kwon, Taewoo Kang, Hyunsoo Yoon, Changouk Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19358">https://arxiv.org/abs/2510.19358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19358">https://arxiv.org/pdf/2510.19358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19358]] M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models(https://arxiv.org/abs/2510.19358)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present M3-SLU, a new multimodal large language model (MLLM) benchmark for evaluating multi-speaker, multi-turn spoken language understanding. While recent models show strong performance in speech and text comprehension, they still struggle with speaker-attributed reasoning, the ability to understand who said what and when in natural conversations. M3-SLU is built from four open corpora (CHiME-6, MELD, MultiDialog, and AMI) and comprises over 12,000 validated instances with paired audio, transcripts, and metadata. It includes two tasks: (1) Speaker-Attributed Question Answering and (2) Speaker Attribution via Utterance Matching. We provide baseline results for both cascaded pipelines and end-to-end MLLMs, evaluated using an LLM-as-Judge and accuracy metrics. Results show that while models can capture what was said, they often fail to identify who said it, revealing a key gap in speaker-aware dialogue understanding. M3-SLU offers as a challenging benchmark to advance research in speaker-aware multimodal understanding.</li>
</ul>

<h3>Title: AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Xianyang Liu, Yilin Liu, Shuai Wang, Hao Cheng, Andrew Estornell, Yuzhi Zhao, Jiaheng Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19361">https://arxiv.org/abs/2510.19361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19361">https://arxiv.org/pdf/2510.19361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19361]] AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation(https://arxiv.org/abs/2510.19361)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The creation of high-quality datasets to improve Large Language Model (LLM) reasoning remains a significant challenge, as current methods often suffer from generating low-quality/incorrect answers and limited information richness from available data sources. To address this, we propose AgenticMath, a novel agentic pipeline for generating high-quality mathematical question-answer pairs to enhance the supervised fine-tuning of LLMs. Our method operates through four stages: (1) Seed Question Filter that selects questions with high information richness, complexity, and clarity; (2) an Agentic Question Rephrase step that employs a multi-agent system to generate diverse, logically consistent paraphrases; (3) an Answer Augment step where rewrite answers using chain-of-thought reasoning to enhance numerical and logical correctness, without reliance on human-provided labels; and (4) a final Question and Answer Evaluation that retains only the most superior pairs. Extensive experiments demonstrate that, fine-tuning 3B-8B parameter LLMs on AgenticMath generated datasets (comprising only 30-60K math samples) achieves competitive or superior performance on diverse in domain and out-of-domain mathematical reasoning benchmarks compared to baselines trained on much more data (e.g., 400K or 2.3M samples). Our work demonstrates that targeted, high-quality data generation is a more efficient path to improving mathematical reasoning in LLMs than large-scale, low-quality alternatives.</li>
</ul>

<h3>Title: LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Wang, Gaokai Zhang, Li Lyna Zhang, Ning Shang, Fan Yang, Dongyao Chen, Mao Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19363">https://arxiv.org/abs/2510.19363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19363">https://arxiv.org/pdf/2510.19363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19363]] LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts(https://arxiv.org/abs/2510.19363)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning over long contexts is essential for large language models. While reinforcement learning (RL) enhances short-context reasoning by inducing "Aha" moments in chain-of-thought, the advanced thinking patterns required for long-context reasoning remain largely unexplored, and high-difficulty RL data are scarce. In this paper, we introduce LoongRL, a data-driven RL method for advanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis approach that transforms short multi-hop QA into high-difficulty long-context tasks by inserting UUID chains that hide the true question among large collections of distracting documents. Solving these tasks requires the model to trace the correct chain step-by-step, identify the true question, retrieve relevant facts and reason over them to answer correctly. RL training on KeyChain data induces an emergent plan-retrieve-reason-recheck reasoning pattern that generalizes far beyond training length. Models trained at 16K effectively solve 128K tasks without prohibitive full-length RL rollout costs. On Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA accuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches a score of 74.2, rivaling much larger frontier models such as o3-mini (74.5) and DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all 128K needle-in-a-haystack stress tests, and preserves short-context reasoning capabilities.</li>
</ul>

<h3>Title: AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields</h3>
<ul>
<li><strong>Authors: </strong>Woo Jae Kim, Kyu Beom Han, Yoonki Cho, Youngju Na, Junsik Jung, Sooel Son, Sung-eui Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19371">https://arxiv.org/abs/2510.19371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19371">https://arxiv.org/pdf/2510.19371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19371]] AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields(https://arxiv.org/abs/2510.19371)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>As Neural Radiance Fields (NeRFs) have emerged as a powerful tool for 3D scene representation and novel view synthesis, protecting their intellectual property (IP) from unauthorized use is becoming increasingly crucial. In this work, we aim to protect the IP of NeRFs by injecting adversarial perturbations that disrupt their unauthorized applications. However, perturbing the 3D geometry of NeRFs can easily deform the underlying scene structure and thus substantially degrade the rendering quality, which has led existing attempts to avoid geometric perturbations or restrict them to explicit spaces like meshes. To overcome this limitation, we introduce a learnable sensitivity to quantify the spatially varying impact of geometric perturbations on rendering quality. Building upon this, we propose AegisRF, a novel framework that consists of a Perturbation Field, which injects adversarial perturbations into the pre-rendering outputs (color and volume density) of NeRF models to fool an unauthorized downstream target model, and a Sensitivity Field, which learns the sensitivity to adaptively constrain geometric perturbations, preserving rendering quality while disrupting unauthorized use. Our experimental evaluations demonstrate the generalized applicability of AegisRF across diverse downstream tasks and modalities, including multi-view image classification and voxel-based 3D localization, while maintaining high visual fidelity. Codes are available at this https URL.</li>
</ul>

<h3>Title: Optimization Benchmark for Diffusion Models on Dynamical Systems</h3>
<ul>
<li><strong>Authors: </strong>Fabian Schaipp</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19376">https://arxiv.org/abs/2510.19376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19376">https://arxiv.org/pdf/2510.19376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19376]] Optimization Benchmark for Diffusion Models on Dynamical Systems(https://arxiv.org/abs/2510.19376)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The training of diffusion models is often absent in the evaluation of new optimization techniques. In this work, we benchmark recent optimization algorithms for training a diffusion model for denoising flow trajectories. We observe that Muon and SOAP are highly efficient alternatives to AdamW (18% lower final loss). We also revisit several recent phenomena related to the training of models for text or image applications in the context of diffusion model training. This includes the impact of the learning-rate schedule on the training dynamics, and the performance gap between Adam and SGD.</li>
</ul>

<h3>Title: Learning Noise-Resilient and Transferable Graph-Text Alignment via Dynamic Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Liu, Minglai Shao, Zengyi Wo, Yunlong Chu, Bing Hao, Shengzhong Liu, Ruijie Wang, Jianxin Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19384">https://arxiv.org/abs/2510.19384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19384">https://arxiv.org/pdf/2510.19384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19384]] Learning Noise-Resilient and Transferable Graph-Text Alignment via Dynamic Quality Assessment(https://arxiv.org/abs/2510.19384)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Pre-training Graph Foundation Models (GFMs) on text-attributed graphs (TAGs) is central to web-scale applications such as search, recommendation, and knowledge discovery. However, existing CLIP-style graph-text aligners face two key limitations: they assume strict one-to-one correspondences between nodes and texts, overlooking the inherent many-to-many relations in real-world graphs; and they rely on static alignment objectives that cannot adapt to varying data quality, making them brittle under noisy supervision. Together, these limitations expose a core dilemma: embracing expressive many-to-many alignment amplifies noise, while reverting to strict one-to-one strategies sacrifices semantic diversity and fails to handle inherently mismatched pairs. To address these challenges, we propose ADAligner, a dynamic, quality-aware graph-text alignment framework that dynamically adjusts between expressive many-to-many and conservative one-to-one objectives according to supervision quality. ADAligner estimates batch-level alignment reliability in real time and adapts its optimization accordingly, promoting soft, subgraph-level many-to-many alignment when supervision is clean, while emphasizing reliable one-to-one alignment by dynamically filtering low-confidence pairs under noise. Theoretically, we prove that this dynamic mechanism forms a stable negative feedback process, ensuring convergence and robustness. Comprehensive experiments on nine diverse TAG datasets demonstrate that ADAligner consistently outperforms prior graph-text aligners on zero-/few-shot node classification, link prediction and cross-modal retrieval tasks. It maintains strong robustness under noisy supervision and accelerates pre-training by approximately 2 to 3 times compared to multimodal baselines, establishing a scalable and reliable foundation for graph-text representation learning in real-world web environments.</li>
</ul>

<h3>Title: CPSVD: Enhancing Large Language Model Compression via Column-Preserving Singular Value Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Lin Xv, Jingsheng Gao, Xian Gao, Ting Li, Yuzhuo Fu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19385">https://arxiv.org/abs/2510.19385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19385">https://arxiv.org/pdf/2510.19385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19385]] CPSVD: Enhancing Large Language Model Compression via Column-Preserving Singular Value Decomposition(https://arxiv.org/abs/2510.19385)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Large Language Models (LLMs) faces a critical bottleneck in their immense size, necessitating efficient compression techniques. While Singular Value Decomposition (SVD) is a promising approach, existing SVD-based methods treat the entire parameter matrix uniformly, overlooking that SVD approximation errors vary significantly across different matrix parts, which often leads to suboptimal compression. To address this, we propose \textbf{C}olumn-\textbf{P}reserving \textbf{S}ingular \textbf{V}alue \textbf{D}ecomposition (CPSVD), a novel method that refines SVD-based LLM compression by intelligently segmenting the parameter matrix. Unlike traditional SVD, CPSVD identifies and directly preserves matrix columns with high decomposition errors, applying SVD only to columns with low decomposition errors, while precisely determining the optimal balance point between these two strategies to minimize error. Furthermore, leveraging the inherent heterogeneity in decomposition errors across different matrices within an LLM, CPSVD adaptively allocates non-uniform compression rates to modules within that layer, while adhering to a target layer-wise compression ratio, thereby further enhancing compression performance. Extensive experiments demonstrate that CPSVD consistently outperforms state-of-the-art SVD-based LLM compression methods, achieving lower perplexity and higher accuracy on zero-shot tasks.</li>
</ul>

<h3>Title: ARA: Adaptive Rank Allocation for Efficient Large Language Model SVD Compression</h3>
<ul>
<li><strong>Authors: </strong>Lin Xv, Jingsheng Gao, Xian Gao, Ting Liu, Yuzhuo Fu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19389">https://arxiv.org/abs/2510.19389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19389">https://arxiv.org/pdf/2510.19389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19389]] ARA: Adaptive Rank Allocation for Efficient Large Language Model SVD Compression(https://arxiv.org/abs/2510.19389)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the field of large language model (LLM) compression, singular value decomposition (SVD) is a widely studied and adopted low-rank decomposition technique. Since SVD operates exclusively on linear modules, and these modules in LLMs are separated by nonlinear components, SVD can only be applied independently to each linear module. Under a global compression ratio constraint, determining the appropriate rank for different linear modules becomes a critical problem. Existing approaches, such as heuristic algorithms and mask-based training, have made progress in addressing this challenge. However, these methods still suffer from several limitations: heuristic algorithms explore the solution space within restricted regions, while mask-based training struggles to efficiently capture the relationship between singular value spectra and trainable parameters. More importantly, current methods overlook the key property that the gain function is non-smooth at a compression ratio of 1, which often leads the training process to suboptimal local minima. To address these issues, we propose an Adaptive Rank Allocation (ARA) method. Specifically, (1) ARA introduces a dedicated mask design that enables efficient mapping and updating between retained ranks and trainable parameters; and (2) it employs an additional loss function to guide parameter selection toward globally optimal solutions. Experimental results demonstrate that ARA achieves state-of-the-art performance. On the LLaMA2-7B model with a 80\% compression ratio, ARA reduces perplexity on WikiText2 from 8.38 to 6.42 and improves average zero-shot task accuracy by 9.72 percentage points compared with uniform compression. These results highlight the effectiveness of our method for rank allocation in SVD-based LLM compression.</li>
</ul>

<h3>Title: A Probabilistic Computing Approach to the Closest Vector Problem for Lattice-Based Factoring</h3>
<ul>
<li><strong>Authors: </strong>Max O. Al-Hasso, Marko von der Leyen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET, math.OC, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19390">https://arxiv.org/abs/2510.19390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19390">https://arxiv.org/pdf/2510.19390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19390]] A Probabilistic Computing Approach to the Closest Vector Problem for Lattice-Based Factoring(https://arxiv.org/abs/2510.19390)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The closest vector problem (CVP) is a fundamental optimization problem in lattice-based cryptography and its conjectured hardness underpins the security of lattice-based cryptosystems. Furthermore, Schnorr's lattice-based factoring algorithm reduces integer factoring (the foundation of current cryptosystems, including RSA) to the CVP. Recent work has investigated the inclusion of a heuristic CVP approximation `refinement' step in the lattice-based factoring algorithm, using quantum variational algorithms to perform the heuristic optimization. This coincides with the emergence of probabilistic computing as a hardware accelerator for randomized algorithms including tasks in combinatorial optimization. In this work we investigate the application of probabilistic computing to the heuristic optimization task of CVP approximation refinement in lattice-based factoring. We present the design of a probabilistic computing algorithm for this task, a discussion of `prime lattice' parameters, and experimental results showing the efficacy of probabilistic computing for solving the CVP as well as its efficacy as a subroutine for lattice-based factoring. The main results found that (a) this approach is capable of finding the maximal available CVP approximation refinement in time linear in problem size and (b) probabilistic computing used in conjunction with the lattice parameters presented can find the composite prime factors of a semiprime number using up to 100x fewer lattice instances than similar quantum and classical methods.</li>
</ul>

<h3>Title: SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision</h3>
<ul>
<li><strong>Authors: </strong>Yasser Hamidullah, Shakib Yazdani, Cennet Oguz, Josef van Genabith, Cristina España-Bonet</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19398">https://arxiv.org/abs/2510.19398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19398">https://arxiv.org/pdf/2510.19398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19398]] SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision(https://arxiv.org/abs/2510.19398)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Sign language translation (SLT) is typically trained with text in a single spoken language, which limits scalability and cross-language generalization. Earlier approaches have replaced gloss supervision with text-based sentence embeddings, but up to now, these remain tied to a specific language and modality. In contrast, here we employ language-agnostic, multimodal embeddings trained on text and speech from multiple languages to supervise SLT, enabling direct multilingual translation. To address data scarcity, we propose a coupled augmentation method that combines multilingual target augmentations (i.e. translations into many languages) with video-level perturbations, improving model robustness. Experiments show consistent BLEURT gains over text-only sentence embedding supervision, with larger improvements in low-resource settings. Our results demonstrate that language-agnostic embedding supervision, combined with coupled augmentation, provides a scalable and semantically robust alternative to traditional SLT training.</li>
</ul>

<h3>Title: ToMMeR -- Efficient Entity Mention Detection from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Victor Morand, Nadi Tomeh, Josiane Mothe, Benjamin Piwowarski</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19410">https://arxiv.org/abs/2510.19410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19410">https://arxiv.org/pdf/2510.19410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19410]] ToMMeR -- Efficient Entity Mention Detection from Large Language Models(https://arxiv.org/abs/2510.19410)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Identifying which text spans refer to entities -- mention detection -- is both foundational for information extraction and a known performance bottleneck. We introduce ToMMeR, a lightweight model (<300K parameters) probing mention detection capabilities from early LLM layers. Across 13 NER benchmarks, ToMMeR achieves 93\% recall zero-shot, with over 90\% precision using an LLM as a judge showing that ToMMeR rarely produces spurious predictions despite high recall. Cross-model analysis reveals that diverse architectures (14M-15B parameters) converge on similar mention boundaries (DICE >75\%), confirming that mention detection emerges naturally from language modeling. When extended with span classification heads, ToMMeR achieves near SOTA NER performance (80-87\% F1 on standard benchmarks). Our work provides evidence that structured entity representations exist in early transformer layers and can be efficiently recovered with minimal parameters.</li>
</ul>

<h3>Title: From See to Shield: ML-Assisted Fine-Grained Access Control for Visual Data</h3>
<ul>
<li><strong>Authors: </strong>Mete Harun Akcay, Buse Gul Atli, Siddharth Prakash Rao, Alexandros Bakas</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19418">https://arxiv.org/abs/2510.19418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19418">https://arxiv.org/pdf/2510.19418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19418]] From See to Shield: ML-Assisted Fine-Grained Access Control for Visual Data(https://arxiv.org/abs/2510.19418)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect</a></li>
<li><strong>Abstract: </strong>As the volume of stored data continues to grow, identifying and protecting sensitive information within large repositories becomes increasingly challenging, especially when shared with multiple users with different roles and permissions. This work presents a system architecture for trusted data sharing with policy-driven access control, enabling selective protection of sensitive regions while maintaining scalability. The proposed architecture integrates four core modules that combine automated detection of sensitive regions, post-correction, key management, and access control. Sensitive regions are secured using a hybrid scheme that employs symmetric encryption for efficiency and Attribute-Based Encryption for policy enforcement. The system supports efficient key distribution and isolates key storage to strengthen overall security. To demonstrate its applicability, we evaluate the system on visual datasets, where Privacy-Sensitive Objects in images are automatically detected, reassessed, and selectively encrypted prior to sharing in a data repository. Experimental results show that our system provides effective PSO detection, increases macro-averaged F1 score (5%) and mean Average Precision (10%), and maintains an average policy-enforced decryption time of less than 1 second per image. These results demonstrate the effectiveness, efficiency and scalability of our proposed solution for fine-grained access control.</li>
</ul>

<h3>Title: BLiSS 1.0: Evaluating Bilingual Learner Competence in Second Language Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuan Gao, Suchir Salhan, Andrew Caines, Paula Buttery, Weiwei Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19419">https://arxiv.org/abs/2510.19419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19419">https://arxiv.org/pdf/2510.19419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19419]] BLiSS 1.0: Evaluating Bilingual Learner Competence in Second Language Small Language Models(https://arxiv.org/abs/2510.19419)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>To bridge the gap between performance-oriented benchmarks and the evaluation of cognitively inspired models, we introduce BLiSS 1.0, a Benchmark of Learner Interlingual Syntactic Structure. Our benchmark operationalizes a new paradigm of selective tolerance, testing whether a model finds a naturalistic learner error more plausible than a matched, artificial error within the same sentence. Constructed from over 2.8 million naturalistic learner sentences, BLiSS provides 136,867 controlled triplets (corrected, learner, artificial) for this purpose. Experiments on a diverse suite of models demonstrate that selective tolerance is a distinct capability from standard grammaticality, with performance clustering strongly by training paradigm. This validates BLiSS as a robust tool for measuring how different training objectives impact a model's alignment with the systematic patterns of human language acquisition.</li>
</ul>

<h3>Title: Monitoring LLM-based Multi-Agent Systems Against Corruptions via Node Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Chengcan Wu, Zhixin Zhang, Mingqian Xu, Zeming Wei, Meng Sun</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.MA, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19420">https://arxiv.org/abs/2510.19420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19420">https://arxiv.org/pdf/2510.19420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19420]] Monitoring LLM-based Multi-Agent Systems Against Corruptions via Node Evaluation(https://arxiv.org/abs/2510.19420)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM)-based Multi-Agent Systems (MAS) have become a popular paradigm of AI applications. However, trustworthiness issues in MAS remain a critical concern. Unlike challenges in single-agent systems, MAS involve more complex communication processes, making them susceptible to corruption attacks. To mitigate this issue, several defense mechanisms have been developed based on the graph representation of MAS, where agents represent nodes and communications form edges. Nevertheless, these methods predominantly focus on static graph defense, attempting to either detect attacks in a fixed graph structure or optimize a static topology with certain defensive capabilities. To address this limitation, we propose a dynamic defense paradigm for MAS graph structures, which continuously monitors communication within the MAS graph, then dynamically adjusts the graph topology, accurately disrupts malicious communications, and effectively defends against evolving and diverse dynamic attacks. Experimental results in increasingly complex and dynamic MAS environments demonstrate that our method significantly outperforms existing MAS defense mechanisms, contributing an effective guardrail for their trustworthy applications. Our code is available at this https URL.</li>
</ul>

<h3>Title: FairNet: Dynamic Fairness Correction without Performance Loss via Contrastive Conditional LoRA</h3>
<ul>
<li><strong>Authors: </strong>Songqi Zhou, Zeyuan Liu, Benben Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19421">https://arxiv.org/abs/2510.19421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19421">https://arxiv.org/pdf/2510.19421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19421]] FairNet: Dynamic Fairness Correction without Performance Loss via Contrastive Conditional LoRA(https://arxiv.org/abs/2510.19421)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Ensuring fairness in machine learning models is a critical challenge. Existing debiasing methods often compromise performance, rely on static correction strategies, and struggle with data sparsity, particularly within minority groups. Furthermore, their utilization of sensitive attributes is often suboptimal, either depending excessively on complete attribute labeling or disregarding these attributes entirely. To overcome these limitations, we propose FairNet, a novel framework for dynamic, instance-level fairness correction. FairNet integrates a bias detector with conditional low-rank adaptation (LoRA), which enables selective activation of the fairness correction mechanism exclusively for instances identified as biased, and thereby preserve performance on unbiased instances. A key contribution is a new contrastive loss function for training the LoRA module, specifically designed to minimize intra-class representation disparities across different sensitive groups and effectively address underfitting in minority groups. The FairNet framework can flexibly handle scenarios with complete, partial, or entirely absent sensitive attribute labels. Theoretical analysis confirms that, under moderate TPR/FPR for the bias detector, FairNet can enhance the performance of the worst group without diminishing overall model performance, and potentially yield slight performance improvements. Comprehensive empirical evaluations across diverse vision and language benchmarks validate the effectiveness of FairNet.</li>
</ul>

<h3>Title: LLM Unlearning with LLM Beliefs</h3>
<ul>
<li><strong>Authors: </strong>Kemou Li, Qizhou Wang, Yue Wang, Fengpeng Li, Jun Liu, Bo Han, Jiantao Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19422">https://arxiv.org/abs/2510.19422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19422">https://arxiv.org/pdf/2510.19422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19422]] LLM Unlearning with LLM Beliefs(https://arxiv.org/abs/2510.19422)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models trained on vast corpora inherently risk memorizing sensitive or harmful content, which may later resurface in their outputs. Prevailing unlearning methods generally rely on gradient ascent and its variants to lower the probability of specific target responses. However, we find that this strategy induces a critical side effect: probability mass is redistributed into high-likelihood regions, often corresponding to semantically related rephrasings of the targets. We refer to this as the squeezing effect, which explains why many methods yield merely spurious unlearning, a problem further obscured by automated metrics (e.g., ROUGE, truth ratio) that misreport actual success. To address this, we propose a bootstrapping (BS) framework that explicitly links the squeezing effect with the model's own high-confidence generations, namely its model beliefs. Since model beliefs inherently capture the very high-likelihood regions where probability mass is squeezed, incorporating them into the unlearning objective directly counters the squeezing effect. By jointly suppressing both target responses and model beliefs, BS-T (token) attenuates high-probability tokens, whereas BS-S (sequence) removes entire high-confidence generations, together achieving more thorough forgetting while preserving utility. Extensive experiments across diverse benchmarks with various model families confirm the effectiveness of our approach.</li>
</ul>

<h3>Title: Neural Variational Dropout Processes</h3>
<ul>
<li><strong>Authors: </strong>Insu Jeon, Youngjin Park, Gunhee Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19425">https://arxiv.org/abs/2510.19425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19425">https://arxiv.org/pdf/2510.19425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19425]] Neural Variational Dropout Processes(https://arxiv.org/abs/2510.19425)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Learning to infer the conditional posterior model is a key step for robust meta-learning. This paper presents a new Bayesian meta-learning approach called Neural Variational Dropout Processes (NVDPs). NVDPs model the conditional posterior distribution based on a task-specific dropout; a low-rank product of Bernoulli experts meta-model is utilized for a memory-efficient mapping of dropout rates from a few observed contexts. It allows for a quick reconfiguration of a globally learned and shared neural network for new tasks in multi-task few-shot learning. In addition, NVDPs utilize a novel prior conditioned on the whole task data to optimize the conditional \textit{dropout} posterior in the amortized variational inference. Surprisingly, this enables the robust approximation of task-specific dropout rates that can deal with a wide range of functional ambiguities and uncertainties. We compared the proposed method with other meta-learning approaches in the few-shot learning tasks such as 1D stochastic regression, image inpainting, and classification. The results show the excellent performance of NVDPs.</li>
</ul>

<h3>Title: Revisiting the Relation Between Robustness and Universality</h3>
<ul>
<li><strong>Authors: </strong>M. Klabunde, L. Caspari, F. Lemmerich</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19427">https://arxiv.org/abs/2510.19427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19427">https://arxiv.org/pdf/2510.19427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19427]] Revisiting the Relation Between Robustness and Universality(https://arxiv.org/abs/2510.19427)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The modified universality hypothesis proposed by Jones et al. (2022) suggests that adversarially robust models trained for a given task are highly similar. We revisit the hypothesis and test its generality. While we verify Jones' main claim of high representational similarity in specific settings, results are not consistent across different datasets. We also discover that predictive behavior does not converge with increasing robustness and thus is not universal. We find that differing predictions originate in the classification layer, but show that more universal predictive behavior can be achieved with simple retraining of the classifiers. Overall, our work points towards partial universality of neural networks in specific settings and away from notions of strict universality.</li>
</ul>

<h3>Title: Transmitter Identification via Volterra Series Based Radio Frequency Fingerprint</h3>
<ul>
<li><strong>Authors: </strong>Rundong Jiang, Jun Hu, Zhiyuan Xie, Yunqi Song, Shiyou Xu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19440">https://arxiv.org/abs/2510.19440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19440">https://arxiv.org/pdf/2510.19440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19440]] Transmitter Identification via Volterra Series Based Radio Frequency Fingerprint(https://arxiv.org/abs/2510.19440)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, extraction, interpretability</a></li>
<li><strong>Abstract: </strong>The growing number of wireless devices increases the need for secure network access. Radio Frequency Fingerprinting (RFF), a physical-layer authentication method, offers a promising solution as it requires no cryptography and resists spoofing. However, existing RFF approaches often lack a unified theory and effective feature extraction. Many methods use handcrafted signal features or direct neural network classification, leading to limited generalization and interpretability. In this work, we model the transmitter as a black box and analyze its impact on transmitted signals. By treating the deviation from an ideal signal as hardware-induced distortion, we represent the received signal using a Volterra series, using its kernels to capture linear and nonlinear hardware traits. To manage the high dimensionality of these kernels, we approximate them via wavelet decomposition and estimate coefficients through least-squares fitting. The resulting wavelet coefficients provide compact yet informative hardware representations, which are classified using a complex-valued neural network. Experiments on a public LoRa dataset show state-of-the-art performance, with over 98% accuracy in static channels and above 90% under multipath and Doppler effects. The proposed approach improves both interpretability and generalization across varying channel conditions.</li>
</ul>

<h3>Title: Reasoning Like Experts: Leveraging Multimodal Large Language Models for Drawing-based Psychoanalysis</h3>
<ul>
<li><strong>Authors: </strong>Xueqi Ma, Yanbei Jiang, Sarah Erfani, James Bailey, Weifeng Liu, Krista A. Ehinger, Jey Han Lau</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19451">https://arxiv.org/abs/2510.19451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19451">https://arxiv.org/pdf/2510.19451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19451]] Reasoning Like Experts: Leveraging Multimodal Large Language Models for Drawing-based Psychoanalysis(https://arxiv.org/abs/2510.19451)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have demonstrated exceptional performance across various objective multimodal perception tasks, yet their application to subjective, emotionally nuanced domains, such as psychological analysis, remains largely unexplored. In this paper, we introduce PICK, a multi-step framework designed for Psychoanalytical Image Comprehension through hierarchical analysis and Knowledge injection with MLLMs, specifically focusing on the House-Tree-Person (HTP) Test, a widely used psychological assessment in clinical practice. First, we decompose drawings containing multiple instances into semantically meaningful sub-drawings, constructing a hierarchical representation that captures spatial structure and content across three levels: single-object level, multi-object level, and whole level. Next, we analyze these sub-drawings at each level with a targeted focus, extracting psychological or emotional insights from their visual cues. We also introduce an HTP knowledge base and design a feature extraction module, trained with reinforcement learning, to generate a psychological profile for single-object level analysis. This profile captures both holistic stylistic features and dynamic object-specific features (such as those of the house, tree, or person), correlating them with psychological states. Finally, we integrate these multi-faceted information to produce a well-informed assessment that aligns with expert-level reasoning. Our approach bridges the gap between MLLMs and specialized expert domains, offering a structured and interpretable framework for understanding human mental states through visual expression. Experimental results demonstrate that the proposed PICK significantly enhances the capability of MLLMs in psychological analysis. It is further validated as a general framework through extensions to emotion understanding tasks.</li>
</ul>

<h3>Title: MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Kailin Jiang, Ning Jiang, Yuchen Ren, Yuchen Li, Yifan Gao, Jinhe Bi, Yunpu Ma, Qingqing Liu, Xianhao Wang, Yifan Jia, Hongbo Jiang, Yaocong Hu, Bin Li, Lei Liu, Yuntao Du</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19457">https://arxiv.org/abs/2510.19457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19457">https://arxiv.org/pdf/2510.19457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19457]] MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models(https://arxiv.org/abs/2510.19457)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal pre-training, yet their static representations struggle to maintain an accurate understanding of time-sensitive factual knowledge. Existing benchmarks remain constrained by static designs, inadequately evaluating LMMs' ability to understand time-sensitive knowledge. To address this gap, we propose MINED, a comprehensive benchmark that evaluates temporal awareness along 6 key dimensions and 11 challenging tasks: cognition, awareness, trustworthiness, understanding, reasoning, and robustness. MINED is constructed from Wikipedia by two professional annotators, containing 2,104 time-sensitive knowledge samples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED shows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07, while most open-source LMMs still lack time understanding ability. Meanwhile, LMMs perform best on organization knowledge, whereas their performance is weakest on sport. To address these challenges, we investigate the feasibility of updating time-sensitive knowledge in LMMs through knowledge editing methods and observe that LMMs can effectively update knowledge via knowledge editing methods in single editing scenarios.</li>
</ul>

<h3>Title: AegisMCP: Online Graph Intrusion Detection for Tool-Augmented LLMs on Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Zhonghao Zhan, Amir Al Sadi, Krinos Li, Hamed Haddadi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19462">https://arxiv.org/abs/2510.19462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19462">https://arxiv.org/pdf/2510.19462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19462]] AegisMCP: Online Graph Intrusion Detection for Tool-Augmented LLMs on Edge Devices(https://arxiv.org/abs/2510.19462)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>In this work, we study security of Model Context Protocol (MCP) agent toolchains and their applications in smart homes. We introduce AegisMCP, a protocol-level intrusion detector. Our contributions are: (i) a minimal attack suite spanning instruction-driven escalation, chain-of-tool exfiltration, malicious MCP server registration, and persistence; (ii) NEBULA-Schema (Network-Edge Behavioral Learning for Untrusted LLM Agents), a reusable protocol-level instrumentation that represents MCP activity as a streaming heterogeneous temporal graph over agents, MCP servers, tools, devices, remotes, and sessions; and (iii) a CPU-only streaming detector that fuses novelty, session-DAG structure, and attribute cues for near-real-time edge inference, with optional fusion of local prompt-guardrail signals. On an emulated smart-home testbed spanning multiple MCP stacks and a physical bench, AegisMCP achieves sub-second per-window model inference and end-to-end alerting. The latency of AegisMCP is consistently sub-second on Intel N150-class edge hardware, while outperforming traffic-only and sequence baselines; ablations confirm the importance of DAG and install/permission signals. We release code, schemas, and generators for reproducible evaluation.</li>
</ul>

<h3>Title: PCP-GAN: Property-Constrained Pore-scale image reconstruction via conditional Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Ali Sadeghkhani, Brandon Bennett, Masoud Babaei, Arash Rabbani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19465">https://arxiv.org/abs/2510.19465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19465">https://arxiv.org/pdf/2510.19465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19465]] PCP-GAN: Property-Constrained Pore-scale image reconstruction via conditional Generative Adversarial Networks(https://arxiv.org/abs/2510.19465)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Obtaining truly representative pore-scale images that match bulk formation properties remains a fundamental challenge in subsurface characterization, as natural spatial heterogeneity causes extracted sub-images to deviate significantly from core-measured values. This challenge is compounded by data scarcity, where physical samples are only available at sparse well locations. This study presents a multi-conditional Generative Adversarial Network (cGAN) framework that generates representative pore-scale images with precisely controlled properties, addressing both the representativeness challenge and data availability constraints. The framework was trained on thin section samples from four depths (1879.50-1943.50 m) of a carbonate formation, simultaneously conditioning on porosity values and depth parameters within a single unified model. This approach captures both universal pore network principles and depth-specific geological characteristics, from grainstone fabrics with interparticle-intercrystalline porosity to crystalline textures with anhydrite inclusions. The model achieved exceptional porosity control (R^2=0.95) across all formations with mean absolute errors of 0.0099-0.0197. Morphological validation confirmed preservation of critical pore network characteristics including average pore radius, specific surface area, and tortuosity, with statistical differences remaining within acceptable geological tolerances. Most significantly, generated images demonstrated superior representativeness with dual-constraint errors of 1.9-11.3% compared to 36.4-578% for randomly extracted real sub-images. This capability provides transformative tools for subsurface characterization, particularly valuable for carbon storage, geothermal energy, and groundwater management applications where knowing the representative morphology of the pore space is critical for implementing digital rock physics.</li>
</ul>

<h3>Title: Predicting before Reconstruction: A generative prior framework for MRI acceleration</h3>
<ul>
<li><strong>Authors: </strong>Juhyung Park, Rokgi Hong, Roh-Eul Yoo, Jaehyeon Koo, Se Young Chun, Seung Hong Choi, Jongho Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19472">https://arxiv.org/abs/2510.19472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19472">https://arxiv.org/pdf/2510.19472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19472]] Predicting before Reconstruction: A generative prior framework for MRI acceleration(https://arxiv.org/abs/2510.19472)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in artificial intelligence have created transformative capabilities in image synthesis and generation, enabling diverse research fields to innovate at revolutionary speed and spectrum. In this study, we leverage this generative power to introduce a new paradigm for accelerating Magnetic Resonance Imaging (MRI), introducing a shift from image reconstruction to proactive predictive imaging. Despite being a cornerstone of modern patient care, MRI's lengthy acquisition times limit clinical throughput. Our novel framework addresses this challenge by first predicting a target contrast image, which then serves as a data-driven prior for reconstructing highly under-sampled data. This informative prior is predicted by a generative model conditioned on diverse data sources, such as other contrast images, previously scanned images, acquisition parameters, patient information. We demonstrate this approach with two key applications: (1) reconstructing FLAIR images using predictions from T1w and/or T2w scans, and (2) reconstructing T1w images using predictions from previously acquired T1w scans. The framework was evaluated on internal and multiple public datasets (total 14,921 scans; 1,051,904 slices), including multi-channel k-space data, for a range of high acceleration factors (x4, x8 and x12). The results demonstrate that our prediction-prior reconstruction method significantly outperforms other approaches, including those with alternative or no prior information. Through this framework we introduce a fundamental shift from image reconstruction towards a new paradigm of predictive imaging.</li>
</ul>

<h3>Title: PRGCN: A Graph Memory Network for Cross-Sequence Pattern Reuse in 3D Human Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zhuoyang Xie, Yibo Zhao, Hui Huang, Riwei Wang, Zan Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19475">https://arxiv.org/abs/2510.19475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19475">https://arxiv.org/pdf/2510.19475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19475]] PRGCN: A Graph Memory Network for Cross-Sequence Pattern Reuse in 3D Human Pose Estimation(https://arxiv.org/abs/2510.19475)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Monocular 3D human pose estimation remains a fundamentally ill-posed inverse problem due to the inherent depth ambiguity in 2D-to-3D lifting. While contemporary video-based methods leverage temporal context to enhance spatial reasoning, they operate under a critical paradigm limitation: processing each sequence in isolation, thereby failing to exploit the strong structural regularities and repetitive motion patterns that pervade human movement across sequences. This work introduces the Pattern Reuse Graph Convolutional Network (PRGCN), a novel framework that formalizes pose estimation as a problem of pattern retrieval and adaptation. At its core, PRGCN features a graph memory bank that learns and stores a compact set of pose prototypes, encoded as relational graphs, which are dynamically retrieved via an attention mechanism to provide structured priors. These priors are adaptively fused with hard-coded anatomical constraints through a memory-driven graph convolution, ensuring geometrical plausibility. To underpin this retrieval process with robust spatiotemporal features, we design a dual-stream hybrid architecture that synergistically combines the linear-complexity, local temporal modeling of Mamba-based state-space models with the global relational capacity of self-attention. Extensive evaluations on Human3.6M and MPI-INF-3DHP benchmarks demonstrate that PRGCN establishes a new state-of-the-art, achieving an MPJPE of 37.1mm and 13.4mm, respectively, while exhibiting enhanced cross-domain generalization capability. Our work posits that the long-overlooked mechanism of cross-sequence pattern reuse is pivotal to advancing the field, shifting the paradigm from per-sequence optimization towards cumulative knowledge learning.</li>
</ul>

<h3>Title: Mitigating representation bias caused by missing pixels in methane plume detection</h3>
<ul>
<li><strong>Authors: </strong>Julia Wąsala, Joannes D. Maasakkers, Ilse Aben, Rochelle Schneider, Holger Hoos, Mitra Baratchi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19478">https://arxiv.org/abs/2510.19478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19478">https://arxiv.org/pdf/2510.19478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19478]] Mitigating representation bias caused by missing pixels in methane plume detection(https://arxiv.org/abs/2510.19478)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Most satellite images have systematically missing pixels (i.e., missing data not at random (MNAR)) due to factors such as clouds. If not addressed, these missing pixels can lead to representation bias in automated feature extraction models. In this work, we show that spurious association between the label and the number of missing values in methane plume detection can cause the model to associate the coverage (i.e., the percentage of valid pixels in an image) with the label, subsequently under-detecting plumes in low-coverage images. We evaluate multiple imputation approaches to remove the dependence between the coverage and a label. Additionally, we propose a weighted resampling scheme during training that removes the association between the label and the coverage by enforcing class balance in each coverage bin. Our results show that both resampling and imputation can significantly reduce the representation bias without hurting balanced accuracy, precision, or recall. Finally, we evaluate the capability of the debiased models using these techniques in an operational scenario and demonstrate that the debiased models have a higher chance of detecting plumes in low-coverage images.</li>
</ul>

<h3>Title: Graph Unlearning Meets Influence-aware Negative Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Qiang Chen, Zhongze Wu, Ang He, Xi Lin, Shuo Jiang, Shan You, Chang Xu, Yi Chen, Xiu Su</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19479">https://arxiv.org/abs/2510.19479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19479">https://arxiv.org/pdf/2510.19479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19479]] Graph Unlearning Meets Influence-aware Negative Preference Optimization(https://arxiv.org/abs/2510.19479)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advancements in graph unlearning models have enhanced model utility by preserving the node representation essentially invariant, while using gradient ascent on the forget set to achieve unlearning. However, this approach causes a drastic degradation in model utility during the unlearning process due to the rapid divergence speed of gradient ascent. In this paper, we introduce \textbf{INPO}, an \textbf{I}nfluence-aware \textbf{N}egative \textbf{P}reference \textbf{O}ptimization framework that focuses on slowing the divergence speed and improving the robustness of the model utility to the unlearning process. Specifically, we first analyze that NPO has slower divergence speed and theoretically propose that unlearning high-influence edges can reduce impact of unlearning. We design an influence-aware message function to amplify the influence of unlearned edges and mitigate the tight topological coupling between the forget set and the retain set. The influence of each edge is quickly estimated by a removal-based method. Additionally, we propose a topological entropy loss from the perspective of topology to avoid excessive information loss in the local structure during unlearning. Extensive experiments conducted on five real-world datasets demonstrate that INPO-based model achieves state-of-the-art performance on all forget quality metrics while maintaining the model's utility. Codes are available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: ELUTQ: Efficient LUT-Aware Quantization for Deploying Large Language Models on Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Xin Nie, Liang Dong, HaiCheng Zhang, JiaWang Xiao, G. Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19482">https://arxiv.org/abs/2510.19482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19482">https://arxiv.org/pdf/2510.19482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19482]] ELUTQ: Efficient LUT-Aware Quantization for Deploying Large Language Models on Edge Devices(https://arxiv.org/abs/2510.19482)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The deployment of Large Language Models (LLMs) on CPU-based edge devices is crucial for enabling on-device intelligence and expanding AI accessibility. However, it remains challenging due to limited memory and computational resources. During edge inference, memory usage and latency are the primary bottlenecks. Although weight quantization can effectively reduce memory consumption, existing hardware-friendly approaches often rely on uniform quantization, which poorly fits weight distributions and incurs high dequantization overhead at low bit widths. To address these limitations, we propose ELUTQ, an efficient quantization framework introducing a novel quantization format, Hierarchical Linear Quantization (HLQ). HLQ better captures the statistical characteristics of weights without increasing the computational cost of Bit-serial LUT-based GEMM operations, thereby eliminating dequantization overhead. It is orthogonal to existing quantization algorithms and can be seamlessly integrated into various quantization pipelines. For efficient on-device deployment, ELUTQ provides optimized CPU kernels for end-to-end inference. Experiments show that for LLaMA3-8B, HLQ reduces perplexity by about 8% at 3-bit and 85% at 2-bit precision under post-training quantization, completing quantization within one hour. With efficient finetuning, HLQ further improves 2-bit performance within two hours. In terms of inference efficiency, our 2-bit LLaMA2-7B achieves over 25 tokens/s on an Apple M2 chip (4 threads, batch size = 1).</li>
</ul>

<h3>Title: Towards Single-Source Domain Generalized Object Detection via Causal Visual Prompts</h3>
<ul>
<li><strong>Authors: </strong>Chen Li, Huiying Xu, Changxin Gao, Zeyu Wang, Yun Liu, Xinzhong Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19487">https://arxiv.org/abs/2510.19487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19487">https://arxiv.org/pdf/2510.19487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19487]] Towards Single-Source Domain Generalized Object Detection via Causal Visual Prompts(https://arxiv.org/abs/2510.19487)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Single-source Domain Generalized Object Detection (SDGOD), as a cutting-edge research topic in computer vision, aims to enhance model generalization capability in unseen target domains through single-source domain training. Current mainstream approaches attempt to mitigate domain discrepancies via data augmentation techniques. However, due to domain shift and limited domain-specific knowledge, models tend to fall into the pitfall of spurious correlations. This manifests as the model's over-reliance on simplistic classification features (e.g., color) rather than essential domain-invariant representations like object contours. To address this critical challenge, we propose the Cauvis (Causal Visual Prompts) method. First, we introduce a Cross-Attention Prompts module that mitigates bias from spurious features by integrating visual prompts with cross-attention. To address the inadequate domain knowledge coverage and spurious feature entanglement in visual prompts for single-domain generalization, we propose a dual-branch adapter that disentangles causal-spurious features while achieving domain adaptation via high-frequency feature extraction. Cauvis achieves state-of-the-art performance with 15.9-31.4% gains over existing domain generalization methods on SDGOD datasets, while exhibiting significant robustness advantages in complex interference environments.</li>
</ul>

<h3>Title: Cross-Chain Sealed-Bid Auctions Using Confidential Compute Blockchains</h3>
<ul>
<li><strong>Authors: </strong>Jonas Gebele, Timm Mutzel, Burak Oez, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19491">https://arxiv.org/abs/2510.19491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19491">https://arxiv.org/pdf/2510.19491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19491]] Cross-Chain Sealed-Bid Auctions Using Confidential Compute Blockchains(https://arxiv.org/abs/2510.19491)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, fair</a></li>
<li><strong>Abstract: </strong>Sealed-bid auctions ensure fair competition and efficient allocation but are often deployed on centralized infrastructure, enabling opaque manipulation. Public blockchains eliminate central control, yet their inherent transparency conflicts with the confidentiality required for sealed bidding. Prior attempts struggle to reconcile privacy, verifiability, and scalability without relying on trusted intermediaries, multi-round protocols, or expensive cryptography. We present a sealed-bid auction protocol that executes sensitive bidding logic on a Trusted Execution Environment (TEE)-backed confidential compute blockchain while retaining settlement and enforcement on a public chain. Bidders commit funds to enclave-generated escrow addresses, ensuring confidentiality and binding commitments. After the deadline, any party can trigger resolution: the confidential blockchain determines the winner through verifiable off-chain computation and issues signed settlement transactions for execution on the public chain. Our design provides security, privacy, and scalability without trusted third parties or protocol modifications. We implement it on SUAVE with Ethereum settlement, evaluate its scalability and trust assumptions, and demonstrate deployment with minimal integration on existing infrastructure</li>
</ul>

<h3>Title: Machine Text Detectors are Membership Inference Attacks</h3>
<ul>
<li><strong>Authors: </strong>Ryuto Koike, Liam Dugan, Masahiro Kaneko, Chris Callison-Burch, Naoaki Okazaki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19492">https://arxiv.org/abs/2510.19492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19492">https://arxiv.org/pdf/2510.19492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19492]] Machine Text Detectors are Membership Inference Attacks(https://arxiv.org/abs/2510.19492)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, membership infer, fair</a></li>
<li><strong>Abstract: </strong>Although membership inference attacks (MIAs) and machine-generated text detection target different goals, identifying training samples and synthetic texts, their methods often exploit similar signals based on a language model's probability distribution. Despite this shared methodological foundation, the two tasks have been independently studied, which may lead to conclusions that overlook stronger methods and valuable insights developed in the other task. In this work, we theoretically and empirically investigate the transferability, i.e., how well a method originally developed for one task performs on the other, between MIAs and machine text detection. For our theoretical contribution, we prove that the metric that achieves the asymptotically highest performance on both tasks is the same. We unify a large proportion of the existing literature in the context of this optimal metric and hypothesize that the accuracy with which a given method approximates this metric is directly correlated with its transferability. Our large-scale empirical experiments, including 7 state-of-the-art MIA methods and 5 state-of-the-art machine text detectors across 13 domains and 10 generators, demonstrate very strong rank correlation (rho > 0.6) in cross-task performance. We notably find that Binoculars, originally designed for machine text detection, achieves state-of-the-art performance on MIA benchmarks as well, demonstrating the practical impact of the transferability. Our findings highlight the need for greater cross-task awareness and collaboration between the two research communities. To facilitate cross-task developments and fair evaluations, we introduce MINT, a unified evaluation suite for MIAs and machine-generated text detection, with implementation of 15 recent methods from both tasks.</li>
</ul>

<h3>Title: What is the Best Sequence Length for BABYLM?</h3>
<ul>
<li><strong>Authors: </strong>Suchir Salhan, Richard Diehl Martinez, Zébulon Goriely, Paula Buttery</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19493">https://arxiv.org/abs/2510.19493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19493">https://arxiv.org/pdf/2510.19493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19493]] What is the Best Sequence Length for BABYLM?(https://arxiv.org/abs/2510.19493)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer language models typically operate with a fixed-length context window, which has grown in step with large-scale pretraining datasets. In the BabyLM Challenge, however, many past submissions have defaulted to using much shorter sequence lengths. We examine the impact of sequence length on BabyLM pretraining, to answer the simple question: what sequence length should we be using when training Baby LMs? Using 100M-word training data and fixed compute budgets, we compare 125M-parameter Mamba and OPT models, finding that although longer is often better, the optimal length depends on both task and architecture. Shorter sequences are sufficient for grammatical generalization tasks whereas longer contexts benefit morphological analogical reasoning tasks.</li>
</ul>

<h3>Title: Energy-Efficient and Dequantization-Free Q-LLMs: A Spiking Neural Network Approach to Salient Value Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Wang, Zhanglu Yan, Zhi Zhou, Xu Chen, Weng-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19498">https://arxiv.org/abs/2510.19498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19498">https://arxiv.org/pdf/2510.19498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19498]] Energy-Efficient and Dequantization-Free Q-LLMs: A Spiking Neural Network Approach to Salient Value Mitigation(https://arxiv.org/abs/2510.19498)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the era of large language models (LLMs), weight-activation quantization helps fit models on edge device by reducing memory and compute bit-widths. However, three challenges persist for energy constrained hardware: (1) even after quantization, multiply-accumulate (MAC) operations remain unavoidable and continue to dominate energy consumption; (2) dequantization (or per-tensor/channel rescaling) introduces extra arithmetic and data movement, increasing latency and energy; (3) uniform parameters bit widths clip salient values-while intra-channel mixed precision is generally impractical on current matrix hardware and memory. In contrast, brain-inspired Spiking Neural Networks (SNNs), owing to their binary spike-based information representation and the Integrate-and-Fire (IF) paradigm, naturally support mixed-precision storage and energy-efficient computation by replacing complex MACs with temporal Accumulate (ACCs). Motivated by this property, we propose SpikeQuant, which selectively applies mixed-precision quantization to activations with salient values and re-encodes them into binary spike counts, thereby enabling dynamic mixed storage of different bitwidths. Furthermore, by embedding the quantization scale into the threshold of the IF mechanism, our approach performs energy-efficient linear transformations on weights and activations while avoiding explicit dequantization. Experimental results demonstrate that SpikeQuant consistently achieves near-FP16 perplexity under W4A4 quantization while reducing energy cost by up to 4.6 times compared to existing methods, highlighting its effectiveness for accurate and energy-efficient LLM deployment.</li>
</ul>

<h3>Title: Lookahead Routing for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Canbin Huang, Tianyuan Shi, Yuhua Zhu, Ruijun Chen, Xiaojun Quan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19506">https://arxiv.org/abs/2510.19506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19506">https://arxiv.org/pdf/2510.19506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19506]] Lookahead Routing for Large Language Models(https://arxiv.org/abs/2510.19506)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) routers improve the efficiency of multi-model systems by directing each query to the most appropriate model while leveraging the diverse strengths of heterogeneous LLMs. Most existing approaches frame routing as a classification problem based solely on the input query. While this reduces overhead by avoiding inference across all models, it overlooks valuable information that could be gleaned from potential outputs and fails to capture implicit intent or contextual nuances that often emerge only during response generation. These limitations can result in suboptimal routing decisions, particularly for complex or ambiguous queries that require deeper semantic understanding. To address this challenge, we propose Lookahead, a routing framework that "foresees" potential model outputs by predicting their latent representations and uses these predictions to guide model selection, thus enabling more informed routing without full inference. Within this framework, we implement two approaches based on causal and masked language models. Empirical evaluations across seven public benchmarks - spanning instruction following, mathematical reasoning, and code generation - show that Lookahead consistently outperforms existing routing baselines, achieving an average performance gain of 7.7% over the state-of-the-art. Our code is available at this https URL.</li>
</ul>

<h3>Title: Teaming LLMs to Detect and Mitigate Hallucinations</h3>
<ul>
<li><strong>Authors: </strong>Demian Till, John Smeaton, Peter Haubrick, Gouse Saheb, Florian Graef, David Berman</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19507">https://arxiv.org/abs/2510.19507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19507">https://arxiv.org/pdf/2510.19507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19507]] Teaming LLMs to Detect and Mitigate Hallucinations(https://arxiv.org/abs/2510.19507)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent work has demonstrated state-of-the-art results in large language model (LLM) hallucination detection and mitigation through consistency-based approaches which involve aggregating multiple responses sampled from a single LLM for a given prompt. These approaches help offset limitations stemming from the imperfect data on which LLMs are trained, which includes biases and under-representation of information required at deployment time among other limitations which can lead to hallucinations. We show that extending these single-model consistency methods to combine responses from multiple LLMs with different training data, training schemes and model architectures can result in substantial further improvements in hallucination detection and mitigation capabilities beyond their single-model consistency counterparts. We evaluate this \emph{consortium consistency} approach across many model teams from a pool of 15 LLMs and explore under what conditions it is beneficial to team together different LLMs in this manner. Further, we show that these performance improvements often come with reduced inference costs, offsetting a significant drawback with single-model consistency methods.</li>
</ul>

<h3>Title: From Prototypes to Sparse ECG Explanations: SHAP-Driven Counterfactuals for Multivariate Time-Series Multi-class Classification</h3>
<ul>
<li><strong>Authors: </strong>Maciej Mozolewski, Betül Bayrak, Kerstin Bach, Grzegorz J. Nalepa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19514">https://arxiv.org/abs/2510.19514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19514">https://arxiv.org/pdf/2510.19514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19514]] From Prototypes to Sparse ECG Explanations: SHAP-Driven Counterfactuals for Multivariate Time-Series Multi-class Classification(https://arxiv.org/abs/2510.19514)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>In eXplainable Artificial Intelligence (XAI), instance-based explanations for time series have gained increasing attention due to their potential for actionable and interpretable insights in domains such as healthcare. Addressing the challenges of explainability of state-of-the-art models, we propose a prototype-driven framework for generating sparse counterfactual explanations tailored to 12-lead ECG classification models. Our method employs SHAP-based thresholds to identify critical signal segments and convert them into interval rules, uses Dynamic Time Warping (DTW) and medoid clustering to extract representative prototypes, and aligns these prototypes to query R-peaks for coherence with the sample being explained. The framework generates counterfactuals that modify only 78% of the original signal while maintaining 81.3% validity across all classes and achieving 43% improvement in temporal stability. We evaluate three variants of our approach, Original, Sparse, and Aligned Sparse, with class-specific performance ranging from 98.9% validity for myocardial infarction (MI) to challenges with hypertrophy (HYP) detection (13.2%). This approach supports near realtime generation (< 1 second) of clinically valid counterfactuals and provides a foundation for interactive explanation platforms. Our findings establish design principles for physiologically-aware counterfactual explanations in AI-based diagnosis systems and outline pathways toward user-controlled explanation interfaces for clinical deployment.</li>
</ul>

<h3>Title: Optimizing the Unknown: Black Box Bayesian Optimization with Energy-Based Model and Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ruiyao Miao, Junren Xiao, Shiya Tsang, Hui Xiong, Yingnian Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19530">https://arxiv.org/abs/2510.19530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19530">https://arxiv.org/pdf/2510.19530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19530]] Optimizing the Unknown: Black Box Bayesian Optimization with Energy-Based Model and Reinforcement Learning(https://arxiv.org/abs/2510.19530)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing Bayesian Optimization (BO) methods typically balance exploration and exploitation to optimize costly objective functions. However, these methods often suffer from a significant one-step bias, which may lead to convergence towards local optima and poor performance in complex or high-dimensional tasks. Recently, Black-Box Optimization (BBO) has achieved success across various scientific and engineering domains, particularly when function evaluations are costly and gradients are unavailable. Motivated by this, we propose the Reinforced Energy-Based Model for Bayesian Optimization (REBMBO), which integrates Gaussian Processes (GP) for local guidance with an Energy-Based Model (EBM) to capture global structural information. Notably, we define each Bayesian Optimization iteration as a Markov Decision Process (MDP) and use Proximal Policy Optimization (PPO) for adaptive multi-step lookahead, dynamically adjusting the depth and direction of exploration to effectively overcome the limitations of traditional BO methods. We conduct extensive experiments on synthetic and real-world benchmarks, confirming the superior performance of REBMBO. Additional analyses across various GP configurations further highlight its adaptability and robustness.</li>
</ul>

<h3>Title: Insights into the Unknown: Federated Data Diversity Analysis on Molecular Data</h3>
<ul>
<li><strong>Authors: </strong>Markus Bujotzek, Evelyn Trautmann, Calum Hand, Ian Hales</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19535">https://arxiv.org/abs/2510.19535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19535">https://arxiv.org/pdf/2510.19535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19535]] Insights into the Unknown: Federated Data Diversity Analysis on Molecular Data(https://arxiv.org/abs/2510.19535)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, explainability</a></li>
<li><strong>Abstract: </strong>AI methods are increasingly shaping pharmaceutical drug discovery. However, their translation to industrial applications remains limited due to their reliance on public datasets, lacking scale and diversity of proprietary pharmaceutical data. Federated learning (FL) offers a promising approach to integrate private data into privacy-preserving, collaborative model training across data silos. This federated data access complicates important data-centric tasks such as estimating dataset diversity, performing informed data splits, and understanding the structure of the combined chemical space. To address this gap, we investigate how well federated clustering methods can disentangle and represent distributed molecular data. We benchmark three approaches, Federated kMeans (Fed-kMeans), Federated Principal Component Analysis combined with Fed-kMeans (Fed-PCA+Fed-kMeans), and Federated Locality-Sensitive Hashing (Fed-LSH), against their centralized counterparts on eight diverse molecular datasets. Our evaluation utilizes both, standard mathematical and a chemistry-informed evaluation metrics, SF-ICF, that we introduce in this work. The large-scale benchmarking combined with an in-depth explainability analysis shows the importance of incorporating domain knowledge through chemistry-informed metrics, and on-client explainability analyses for federated diversity analysis on molecular data.</li>
</ul>

<h3>Title: Privacy-Preserving Spiking Neural Networks: A Deep Dive into Encryption Parameter Optimisation</h3>
<ul>
<li><strong>Authors: </strong>Mahitha Pulivathi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19537">https://arxiv.org/abs/2510.19537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19537">https://arxiv.org/pdf/2510.19537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19537]] Privacy-Preserving Spiking Neural Networks: A Deep Dive into Encryption Parameter Optimisation(https://arxiv.org/abs/2510.19537)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect</a></li>
<li><strong>Abstract: </strong>Deep learning is widely applied to modern problems through neural networks, but the growing computational and energy demands of these models have driven interest in more efficient approaches. Spiking Neural Networks (SNNs), the third generation of neural networks, mimic the brain's event-driven behaviour, offering improved performance and reduced power use. At the same time, concerns about data privacy during cloud-based model execution have led to the adoption of cryptographic methods. This article introduces BioEncryptSNN, a spiking neural network based encryption-decryption framework for secure and noise-resilient data protection. Unlike conventional algorithms, BioEncryptSNN converts ciphertext into spike trains and exploits temporal neural dynamics to model encryption and decryption, optimising parameters such as key length, spike timing, and synaptic connectivity. Benchmarked against AES-128, RSA-2048, and DES, BioEncryptSNN preserved data integrity while achieving up to 4.1x faster encryption and decryption than PyCryptodome's AES implementation. The framework demonstrates scalability and adaptability across symmetric and asymmetric ciphers, positioning SNNs as a promising direction for secure, energy-efficient computing.</li>
</ul>

<h3>Title: The Intricate Dance of Prompt Complexity, Quality, Diversity, and Consistency in T2I Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaofeng Zhang, Aaron Courville, Michal Drozdzal, Adriana Romero-Soriano</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19557">https://arxiv.org/abs/2510.19557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19557">https://arxiv.org/pdf/2510.19557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19557]] The Intricate Dance of Prompt Complexity, Quality, Diversity, and Consistency in T2I Models(https://arxiv.org/abs/2510.19557)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) models offer great potential for creating virtually limitless synthetic data, a valuable resource compared to fixed and finite real datasets. Previous works evaluate the utility of synthetic data from T2I models on three key desiderata: quality, diversity, and consistency. While prompt engineering is the primary means of interacting with T2I models, the systematic impact of prompt complexity on these critical utility axes remains underexplored. In this paper, we first conduct synthetic experiments to motivate the difficulty of generalization w.r.t. prompt complexity and explain the observed difficulty with theoretical derivations. Then, we introduce a new evaluation framework that can compare the utility of real data and synthetic data, and present a comprehensive analysis of how prompt complexity influences the utility of synthetic data generated by commonly used T2I models. We conduct our study across diverse datasets, including CC12M, ImageNet-1k, and DCI, and evaluate different inference-time intervention methods. Our synthetic experiments show that generalizing to more general conditions is harder than the other way round, since the former needs an estimated likelihood that is not learned by diffusion models. Our large-scale empirical experiments reveal that increasing prompt complexity results in lower conditional diversity and prompt consistency, while reducing the synthetic-to-real distribution shift, which aligns with the synthetic experiments. Moreover, current inference-time interventions can augment the diversity of the generations at the expense of moving outside the support of real data. Among those interventions, prompt expansion, by deliberately using a pre-trained language model as a likelihood estimator, consistently achieves the highest performance in both image diversity and aesthetics, even higher than that of real data.</li>
</ul>

<h3>Title: Can You Trust What You See? Alpha Channel No-Box Attacks on Video Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Ariana Yi, Ce Zhou, Liyang Xiao, Qiben Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19574">https://arxiv.org/abs/2510.19574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19574">https://arxiv.org/pdf/2510.19574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19574]] Can You Trust What You See? Alpha Channel No-Box Attacks on Video Object Detection(https://arxiv.org/abs/2510.19574)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>As object detection models are increasingly deployed in cyber-physical systems such as autonomous vehicles (AVs) and surveillance platforms, ensuring their security against adversarial threats is essential. While prior work has explored adversarial attacks in the image domain, those attacks in the video domain remain largely unexamined, especially in the no-box setting. In this paper, we present {\alpha}-Cloak, the first no-box adversarial attack on object detectors that operates entirely through the alpha channel of RGBA videos. {\alpha}-Cloak exploits the alpha channel to fuse a malicious target video with a benign video, resulting in a fused video that appears innocuous to human viewers but consistently fools object detectors. Our attack requires no access to model architecture, parameters, or outputs, and introduces no perceptible artifacts. We systematically study the support for alpha channels across common video formats and playback applications, and design a fusion algorithm that ensures visual stealth and compatibility. We evaluate {\alpha}-Cloak on five state-of-the-art object detectors, a vision-language model, and a multi-modal large language model (Gemini-2.0-Flash), demonstrating a 100% attack success rate across all scenarios. Our findings reveal a previously unexplored vulnerability in video-based perception systems, highlighting the urgent need for defenses that account for the alpha channel in adversarial settings.</li>
</ul>

<h3>Title: Addressing the Depth-of-Field Constraint: A New Paradigm for High Resolution Multi-Focus Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Luca Piano, Peng Huanwen, Radu Ciprian Bilcu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19581">https://arxiv.org/abs/2510.19581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19581">https://arxiv.org/pdf/2510.19581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19581]] Addressing the Depth-of-Field Constraint: A New Paradigm for High Resolution Multi-Focus Image Fusion(https://arxiv.org/abs/2510.19581)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-focus image fusion (MFIF) addresses the depth-of-field (DOF) limitations of optical lenses, where only objects within a specific range appear sharp. Although traditional and deep learning methods have advanced the field, challenges persist, including limited training data, domain gaps from synthetic datasets, and difficulties with regions lacking information. We propose VAEEDOF, a novel MFIF method that uses a distilled variational autoencoder for high-fidelity, efficient image reconstruction. Our fusion module processes up to seven images simultaneously, enabling robust fusion across diverse focus points. To address data scarcity, we introduce MattingMFIF, a new syntetic 4K dataset, simulating realistic DOF effects from real photographs. Our method achieves state-of-the-art results, generating seamless artifact-free fused images and bridging the gap between synthetic and real-world scenarios, offering a significant step forward in addressing complex MFIF challenges. The code, and weights are available here:</li>
</ul>

<h3>Title: Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Yu Wu, Ke Shu, Jonas Fischer, Lidia Pivovarova, David Rosson, Eetu Mäkelä, Mikko Tolonen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19585">https://arxiv.org/abs/2510.19585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19585">https://arxiv.org/pdf/2510.19585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19585]] Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark(https://arxiv.org/abs/2510.19585)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a novel task of extracting Latin fragments from mixed-language historical documents with varied layouts. We benchmark and evaluate the performance of large foundation models against a multimodal dataset of 724 annotated pages. The results demonstrate that reliable Latin detection with contemporary models is achievable. Our study provides the first comprehensive analysis of these models' capabilities and limits for this task.</li>
</ul>

<h3>Title: Uncertainty evaluation of segmentation models for Earth observation</h3>
<ul>
<li><strong>Authors: </strong>Melanie Rey, Andriy Mnih, Maxim Neumann, Matt Overlan, Drew Purves</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19586">https://arxiv.org/abs/2510.19586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19586">https://arxiv.org/pdf/2510.19586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19586]] Uncertainty evaluation of segmentation models for Earth observation(https://arxiv.org/abs/2510.19586)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper investigates methods for estimating uncertainty in semantic segmentation predictions derived from satellite imagery. Estimating uncertainty for segmentation presents unique challenges compared to standard image classification, requiring scalable methods producing per-pixel estimates. While most research on this topic has focused on scene understanding or medical imaging, this work benchmarks existing methods specifically for remote sensing and Earth observation applications. Our evaluation focuses on the practical utility of uncertainty measures, testing their ability to identify prediction errors and noise-corrupted input image regions. Experiments are conducted on two remote sensing datasets, PASTIS and ForTy, selected for their differences in scale, geographic coverage, and label confidence. We perform an extensive evaluation featuring several models, such as Stochastic Segmentation Networks and ensembles, in combination with a number of neural architectures and uncertainty metrics. We make a number of practical recommendations based on our findings.</li>
</ul>

<h3>Title: Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Su Ho Han, Jeongseok Hyun, Pilhyeon Lee, Minho Shim, Dongyoon Wee, Seon Joo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19592">https://arxiv.org/abs/2510.19592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19592">https://arxiv.org/pdf/2510.19592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19592]] Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation(https://arxiv.org/abs/2510.19592)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) demonstrate strong video understanding by attending to visual tokens relevant to textual queries. To directly adapt this for localization in a training-free manner, we cast video reasoning segmentation as a video QA task and extract attention maps via rollout mechanism. However, raw attention maps are noisy and poorly aligned with object regions. We propose Decomposed Attention Fusion (DecAF), which refines these maps through two mechanisms: (1) contrastive object-background fusion and (2) complementary video-frame fusion. This method suppresses irrelevant activations and enhances object-focused cues, enabling direct conversion of attention maps into coarse segmentation masks. In addition, we introduce attention-guided SAM2 prompting for obtaining fine-grained masks. Unlike existing methods that jointly train MLLMs with SAM, our method operates entirely without retraining. DecAF outperforms training-free methods and achieves performance comparable to training-based methods on both referring and reasoning VOS benchmarks. The code will be available at this https URL.</li>
</ul>

<h3>Title: CBDiff:Conditional Bernoulli Diffusion Models for Image Forgery Localization</h3>
<ul>
<li><strong>Authors: </strong>Zhou Lei, Pan Gang, Wang Jiahao, Sun Di</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19597">https://arxiv.org/abs/2510.19597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19597">https://arxiv.org/pdf/2510.19597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19597]] CBDiff:Conditional Bernoulli Diffusion Models for Image Forgery Localization(https://arxiv.org/abs/2510.19597)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, diffusion</a></li>
<li><strong>Abstract: </strong>Image Forgery Localization (IFL) is a crucial task in image forensics, aimed at accurately identifying manipulated or tampered regions within an image at the pixel level. Existing methods typically generate a single deterministic localization map, which often lacks the precision and reliability required for high-stakes applications such as forensic analysis and security surveillance. To enhance the credibility of predictions and mitigate the risk of errors, we introduce an advanced Conditional Bernoulli Diffusion Model (CBDiff). Given a forged image, CBDiff generates multiple diverse and plausible localization maps, thereby offering a richer and more comprehensive representation of the forgery distribution. This approach addresses the uncertainty and variability inherent in tampered regions. Furthermore, CBDiff innovatively incorporates Bernoulli noise into the diffusion process to more faithfully reflect the inherent binary and sparse properties of forgery masks. Additionally, CBDiff introduces a Time-Step Cross-Attention (TSCAttention), which is specifically designed to leverage semantic feature guidance with temporal steps to improve manipulation detection. Extensive experiments on eight publicly benchmark datasets demonstrate that CBDiff significantly outperforms existing state-of-the-art methods, highlighting its strong potential for real-world deployment.</li>
</ul>

<h3>Title: XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography</h3>
<ul>
<li><strong>Authors: </strong>Haozhe Luo, Shelley Zixin Shu, Ziyu Zhou, Sebastian Otalora, Mauricio Reyes</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19599">https://arxiv.org/abs/2510.19599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19599">https://arxiv.org/pdf/2510.19599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19599]] XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography(https://arxiv.org/abs/2510.19599)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) have recently shown remarkable zero-shot performance in medical image understanding, yet their grounding ability, the extent to which textual concepts align with visual evidence, remains underexplored. In the medical domain, however, reliable grounding is essential for interpretability and clinical adoption. In this work, we present the first systematic benchmark for evaluating cross-modal interpretability in chest X-rays across seven CLIP-style VLM variants. We generate visual explanations using cross-attention and similarity-based localization maps, and quantitatively assess their alignment with radiologist-annotated regions across multiple pathologies. Our analysis reveals that: (1) while all VLM variants demonstrate reasonable localization for large and well-defined pathologies, their performance substantially degrades for small or diffuse lesions; (2) models that are pretrained on chest X-ray-specific datasets exhibit improved alignment compared to those trained on general-domain data. (3) The overall recognition ability and grounding ability of the model are strongly correlated. These findings underscore that current VLMs, despite their strong recognition ability, still fall short in clinically reliable grounding, highlighting the need for targeted interpretability benchmarks before deployment in medical practice. XBench code is available at this https URL</li>
</ul>

<h3>Title: PBBQ: A Persian Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Farhan Farsi, Shayan Bali, Fatemeh Valeh, Parsa Ghofrani, Alireza Pakniat, Kian Kashfipour, Amir H. Payberah</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19616">https://arxiv.org/abs/2510.19616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19616">https://arxiv.org/pdf/2510.19616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19616]] PBBQ: A Persian Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models(https://arxiv.org/abs/2510.19616)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the increasing adoption of large language models (LLMs), ensuring their alignment with social norms has become a critical concern. While prior research has examined bias detection in various languages, there remains a significant gap in resources addressing social biases within Persian cultural contexts. In this work, we introduce PBBQ, a comprehensive benchmark dataset designed to evaluate social biases in Persian LLMs. Our benchmark, which encompasses 16 cultural categories, was developed through questionnaires completed by 250 diverse individuals across multiple demographics, in close collaboration with social science experts to ensure its validity. The resulting PBBQ dataset contains over 37,000 carefully curated questions, providing a foundation for the evaluation and mitigation of bias in Persian language models. We benchmark several open-source LLMs, a closed-source model, and Persian-specific fine-tuned models on PBBQ. Our findings reveal that current LLMs exhibit significant social biases across Persian culture. Additionally, by comparing model outputs to human responses, we observe that LLMs often replicate human bias patterns, highlighting the complex interplay between learned representations and cultural this http URL acceptance of the paper, our PBBQ dataset will be publicly available for use in future work. Content warning: This paper contains unsafe content.</li>
</ul>

<h3>Title: Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Junfei Zhou, Penglin Dai, Quanmin Wei, Bingyi Liu, Xiao Wu, Jianping Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19618">https://arxiv.org/abs/2510.19618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19618">https://arxiv.org/pdf/2510.19618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19618]] Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism(https://arxiv.org/abs/2510.19618)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Multi-agent collaboration enhances the perception capabilities of individual agents through information sharing. However, in real-world applications, differences in sensors and models across heterogeneous agents inevitably lead to domain gaps during collaboration. Existing approaches based on adaptation and reconstruction fail to support pragmatic heterogeneous collaboration due to two key limitations: (1) Intrusive retraining of the encoder or core modules disrupts the established semantic consistency among agents; and (2) accommodating new agents incurs high computational costs, limiting scalability. To address these challenges, we present a novel Generative Communication mechanism (GenComm) that facilitates seamless perception across heterogeneous multi-agent systems through feature generation, without altering the original network, and employs lightweight numerical alignment of spatial information to efficiently integrate new agents at minimal cost. Specifically, a tailored Deformable Message Extractor is designed to extract spatial message for each collaborator, which is then transmitted in place of intermediate features. The Spatial-Aware Feature Generator, utilizing a conditional diffusion model, generates features aligned with the ego agent's semantic space while preserving the spatial information of the collaborators. These generated features are further refined by a Channel Enhancer before fusion. Experiments conducted on the OPV2V-H, DAIR-V2X and V2X-Real datasets demonstrate that GenComm outperforms existing state-of-the-art methods, achieving an 81\% reduction in both computational cost and parameter count when incorporating new agents. Our code is available at this https URL.</li>
</ul>

<h3>Title: Augmenting Moment Retrieval: Zero-Dependency Two-Stage Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhengxuan Wei, Jiajin Tang, Sibei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19622">https://arxiv.org/abs/2510.19622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19622">https://arxiv.org/pdf/2510.19622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19622]] Augmenting Moment Retrieval: Zero-Dependency Two-Stage Learning(https://arxiv.org/abs/2510.19622)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing Moment Retrieval methods face three critical bottlenecks: (1) data scarcity forces models into shallow keyword-feature associations; (2) boundary ambiguity in transition regions between adjacent events; (3) insufficient discrimination of fine-grained semantics (e.g., distinguishing ``kicking" vs. ``throwing" a ball). In this paper, we propose a zero-external-dependency Augmented Moment Retrieval framework, AMR, designed to overcome local optima caused by insufficient data annotations and the lack of robust boundary and semantic discrimination capabilities. AMR is built upon two key insights: (1) it resolves ambiguous boundary information and semantic confusion in existing annotations without additional data (avoiding costly manual labeling), and (2) it preserves boundary and semantic discriminative capabilities enhanced by training while generalizing to real-world scenarios, significantly improving performance. Furthermore, we propose a two-stage training framework with cold-start and distillation adaptation. The cold-start stage employs curriculum learning on augmented data to build foundational boundary/semantic awareness. The distillation stage introduces dual query sets: Original Queries maintain DETR-based localization using frozen Base Queries from the cold-start model, while Active Queries dynamically adapt to real-data distributions. A cross-stage distillation loss enforces consistency between Original and Base Queries, preventing knowledge forgetting while enabling real-world generalization. Experiments on multiple benchmarks show that AMR achieves improved performance over prior state-of-the-art approaches.</li>
</ul>

<h3>Title: Learning and Simulating Building Evacuation Patterns for Enhanced Safety Design Using Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jin Han, Zhe Zheng, Yi Gu, Jia-Rui Lin, Xin-Zheng Lu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19623">https://arxiv.org/abs/2510.19623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19623">https://arxiv.org/pdf/2510.19623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19623]] Learning and Simulating Building Evacuation Patterns for Enhanced Safety Design Using Generative Models(https://arxiv.org/abs/2510.19623)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Evacuation simulation is essential for building safety design, ensuring properly planned evacuation routes. However, traditional evacuation simulation relies heavily on refined modeling with extensive parameters, making it challenging to adopt such methods in a rapid iteration process in early design stages. Thus, this study proposes DiffEvac, a novel method to learn building evacuation patterns based on Generative Models (GMs), for efficient evacuation simulation and enhanced safety design. Initially, a dataset of 399 diverse functional layouts and corresponding evacuation heatmaps of buildings was established. Then, a decoupled feature representation is proposed to embed physical features like layouts and occupant density for GMs. Finally, a diffusion model based on image prompts is proposed to learn evacuation patterns from simulated evacuation heatmaps. Compared to existing research using Conditional GANs with RGB representation, DiffEvac achieves up to a 37.6% improvement in SSIM, 142% in PSNR, and delivers results 16 times faster, thereby cutting simulation time to 2 minutes. Case studies further demonstrate that the proposed method not only significantly enhances the rapid design iteration and adjustment process with efficient evacuation simulation but also offers new insights and technical pathways for future safety optimization in intelligent building design. The research implication is that the approach lowers the modeling burden, enables large-scale what-if exploration, and facilitates coupling with multi-objective design tools.</li>
</ul>

<h3>Title: CrossNews-UA: A Cross-lingual News Semantic Similarity Benchmark for Ukrainian, Polish, Russian, and English</h3>
<ul>
<li><strong>Authors: </strong>Daryna Dementieva, Evgeniya Sukhodolskaya, Alexander Fraser</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19628">https://arxiv.org/abs/2510.19628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19628">https://arxiv.org/pdf/2510.19628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19628]] CrossNews-UA: A Cross-lingual News Semantic Similarity Benchmark for Ukrainian, Polish, Russian, and English(https://arxiv.org/abs/2510.19628)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>In the era of social networks and rapid misinformation spread, news analysis remains a critical task. Detecting fake news across multiple languages, particularly beyond English, poses significant challenges. Cross-lingual news comparison offers a promising approach to verify information by leveraging external sources in different languages (Chen and Shu, 2024). However, existing datasets for cross-lingual news analysis (Chen et al., 2022a) were manually curated by journalists and experts, limiting their scalability and adaptability to new languages. In this work, we address this gap by introducing a scalable, explainable crowdsourcing pipeline for cross-lingual news similarity assessment. Using this pipeline, we collected a novel dataset CrossNews-UA of news pairs in Ukrainian as a central language with linguistically and contextually relevant languages-Polish, Russian, and English. Each news pair is annotated for semantic similarity with detailed justifications based on the 4W criteria (Who, What, Where, When). We further tested a range of models, from traditional bag-of-words, Transformer-based architectures to large language models (LLMs). Our results highlight the challenges in multilingual news analysis and offer insights into models performance.</li>
</ul>

<h3>Title: Matrix-Free Least Squares Solvers: Values, Gradients, and What to Do With Them</h3>
<ul>
<li><strong>Authors: </strong>Hrittik Roy, Søren Hauberg, Nicholas Krämer</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19634">https://arxiv.org/abs/2510.19634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19634">https://arxiv.org/pdf/2510.19634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19634]] Matrix-Free Least Squares Solvers: Values, Gradients, and What to Do With Them(https://arxiv.org/abs/2510.19634)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper argues that the method of least squares has significant unfulfilled potential in modern machine learning, far beyond merely being a tool for fitting linear models. To release its potential, we derive custom gradients that transform the solver into a differentiable operator, like a neural network layer, enabling many diverse applications. Empirically, we demonstrate: (i) scalability by enforcing weight sparsity on a 50 million parameter model; (ii) imposing conservativeness constraints in score-based generative models; and (iii) hyperparameter tuning of Gaussian processes based on predictive performance. By doing this, our work represents the next iteration in developing differentiable linear-algebra tools and making them widely accessible to machine learning practitioners.</li>
</ul>

<h3>Title: Latent Space Factorization in LoRA</h3>
<ul>
<li><strong>Authors: </strong>Shashi Kumar, Yacouba Kaloga, John Mitros, Petr Motlicek, Ina Kodrasi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19640">https://arxiv.org/abs/2510.19640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19640">https://arxiv.org/pdf/2510.19640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19640]] Latent Space Factorization in LoRA(https://arxiv.org/abs/2510.19640)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Low-rank adaptation (LoRA) is a widely used method for parameter-efficient finetuning. However, existing LoRA variants lack mechanisms to explicitly disambiguate task-relevant information within the learned low-rank subspace, potentially limiting downstream performance. We propose Factorized Variational Autoencoder LoRA (FVAE-LoRA), which leverages a VAE to learn two distinct latent spaces. Our novel Evidence Lower Bound formulation explicitly promotes factorization between the latent spaces, dedicating one latent space to task-salient features and the other to residual information. Extensive experiments on text, audio, and image tasks demonstrate that FVAE-LoRA consistently outperforms standard LoRA. Moreover, spurious correlation evaluations confirm that FVAE-LoRA better isolates task-relevant signals, leading to improved robustness under distribution shifts. Our code is publicly available at: this https URL</li>
</ul>

<h3>Title: Style Attack Disguise: When Fonts Become a Camouflage for Adversarial Intent</h3>
<ul>
<li><strong>Authors: </strong>Yangshijie Zhang, Xinda Wang, Jialin Liu, Wenqiang Wang, Zhicong Ma, Xingxing Jia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19641">https://arxiv.org/abs/2510.19641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19641">https://arxiv.org/pdf/2510.19641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19641]] Style Attack Disguise: When Fonts Become a Camouflage for Adversarial Intent(https://arxiv.org/abs/2510.19641)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>With social media growth, users employ stylistic fonts and font-like emoji to express individuality, creating visually appealing text that remains human-readable. However, these fonts introduce hidden vulnerabilities in NLP models: while humans easily read stylistic text, models process these characters as distinct tokens, causing interference. We identify this human-model perception gap and propose a style-based attack, Style Attack Disguise (SAD). We design two sizes: light for query efficiency and strong for superior attack performance. Experiments on sentiment classification and machine translation across traditional models, LLMs, and commercial services demonstrate SAD's strong attack performance. We also show SAD's potential threats to multimodal tasks including text-to-image and text-to-speech generation.</li>
</ul>

<h3>Title: Overlap-weighted orthogonal meta-learner for treatment effect estimation over time</h3>
<ul>
<li><strong>Authors: </strong>Konstantin Hess, Dennis Frauen, Mihaela van der Schaar, Stefan Feuerriegel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19643">https://arxiv.org/abs/2510.19643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19643">https://arxiv.org/pdf/2510.19643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19643]] Overlap-weighted orthogonal meta-learner for treatment effect estimation over time(https://arxiv.org/abs/2510.19643)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Estimating heterogeneous treatment effects (HTEs) in time-varying settings is particularly challenging, as the probability of observing certain treatment sequences decreases exponentially with longer prediction horizons. Thus, the observed data contain little support for many plausible treatment sequences, which creates severe overlap problems. Existing meta-learners for the time-varying setting typically assume adequate treatment overlap, and thus suffer from exploding estimation variance when the overlap is low. To address this problem, we introduce a novel overlap-weighted orthogonal (WO) meta-learner for estimating HTEs that targets regions in the observed data with high probability of receiving the interventional treatment sequences. This offers a fully data-driven approach through which our WO-learner can counteract instabilities as in existing meta-learners and thus obtain more reliable HTE estimates. Methodologically, we develop a novel Neyman-orthogonal population risk function that minimizes the overlap-weighted oracle risk. We show that our WO-learner has the favorable property of Neyman-orthogonality, meaning that it is robust against misspecification in the nuisance functions. Further, our WO-learner is fully model-agnostic and can be applied to any machine learning model. Through extensive experiments with both transformer and LSTM backbones, we demonstrate the benefits of our novel WO-learner.</li>
</ul>

<h3>Title: Unraveling Emotions with Pre-Trained Models</h3>
<ul>
<li><strong>Authors: </strong>Alejandro Pajón-Sanmartín, Francisco De Arriba-Pérez, Silvia García-Méndez, Fátima Leal, Benedita Malheiro, Juan Carlos Burguillo-Rial</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19668">https://arxiv.org/abs/2510.19668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19668">https://arxiv.org/pdf/2510.19668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19668]] Unraveling Emotions with Pre-Trained Models(https://arxiv.org/abs/2510.19668)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer models have significantly advanced the field of emotion recognition. However, there are still open challenges when exploring open-ended queries for Large Language Models (LLMs). Although current models offer good results, automatic emotion analysis in open texts presents significant challenges, such as contextual ambiguity, linguistic variability, and difficulty interpreting complex emotional expressions. These limitations make the direct application of generalist models difficult. Accordingly, this work compares the effectiveness of fine-tuning and prompt engineering in emotion detection in three distinct scenarios: (i) performance of fine-tuned pre-trained models and general-purpose LLMs using simple prompts; (ii) effectiveness of different emotion prompt designs with LLMs; and (iii) impact of emotion grouping techniques on these models. Experimental tests attain metrics above 70% with a fine-tuned pre-trained model for emotion recognition. Moreover, the findings highlight that LLMs require structured prompt engineering and emotion grouping to enhance their performance. These advancements improve sentiment analysis, human-computer interaction, and understanding of user behavior across various domains.</li>
</ul>

<h3>Title: DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Xiang Liu, Xuming Hu, Xiaowen Chu, Eunsol Choi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19669">https://arxiv.org/abs/2510.19669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19669">https://arxiv.org/pdf/2510.19669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19669]] DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference(https://arxiv.org/abs/2510.19669)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent reasoning Large Language Models (LLMs) demonstrate remarkable problem-solving abilities but often generate long thinking traces whose utility is unclear. Our work aims to improve their efficiency, enabling them to reach high performance without overthinking. First, we analyze the entropy of token probabilities in reasoning traces. Across three models, we observe a consistent U-shaped entropy pattern: high entropy on easy problems despite high accuracy, low entropy on problems with medium difficulty, and high entropy on hard problems reflecting uncertainty. Specifically, we notice 22--25\% entropy reduction from easy to medium difficulty regions, suggesting an {overthinking} phenomenon on easy instances. Building on these insights, we introduce \textbf{DiffAdapt}, a lightweight framework that selects Easy/Normal/Hard inference strategies per question based on their difficulty and reasoning trace entropy. Each inference strategy consists of a fixed prompt, temperature and maximum token length. In contrast to existing efficiency optimization methods, our approach does not fine-tune base LLM but a small probe that classifies LLM's final hidden state, allowing inexpensive adaptation. We comprehensively evaluate our method on five models and eight benchmarks. Our method achieves comparable or improved accuracy while reducing token usage by up to 22.4\%, establishing a practical path toward compute-efficient reasoning.</li>
</ul>

<h3>Title: CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation</h3>
<ul>
<li><strong>Authors: </strong>Hasan Akgul, Mari Eplik, Javier Rojas, Aina Binti Abdullah, Pieter van der Merwe</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19670">https://arxiv.org/abs/2510.19670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19670">https://arxiv.org/pdf/2510.19670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19670]] CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation(https://arxiv.org/abs/2510.19670)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer that grounds generation in site specific policies and notes; (iii) PromptRouter, a cost and uncertainty aware policy that selects edge only generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system works with modern serving optimizations, including paged or streaming KV caches, FlashAttention style kernels, speculative decoding, and quantized LoRA adapters, and supports on device personalization and federated updates under non IID drift. Across home, office, and clinic deployments, CoSense-LLM delivers grounded explanations while meeting tight service level objectives: it sustains sub second (p95) end to end latency on edge dominant paths, reduces inter tier token and bandwidth costs by preferring local retrieval grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge-RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV plus decoding accelerators lower energy per decision. The results support an edge first design that treats semantics, privacy, and predictable latency as co equal goals for large model deployments in interference prone environments.</li>
</ul>

<h3>Title: Policy Learning with Abstention</h3>
<ul>
<li><strong>Authors: </strong>Ayush Sawarni, Jikai Jin, Justin Whitehouse, Vasilis Syrgkanis</a></li>
<li><strong>Subjects: </strong>cs.LG, econ.EM, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19672">https://arxiv.org/abs/2510.19672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19672">https://arxiv.org/pdf/2510.19672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19672]] Policy Learning with Abstention(https://arxiv.org/abs/2510.19672)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Policy learning algorithms are widely used in areas such as personalized medicine and advertising to develop individualized treatment regimes. However, most methods force a decision even when predictions are uncertain, which is risky in high-stakes settings. We study policy learning with abstention, where a policy may defer to a safe default or an expert. When a policy abstains, it receives a small additive reward on top of the value of a random guess. We propose a two-stage learner that first identifies a set of near-optimal policies and then constructs an abstention rule from their disagreements. We establish fast O(1/n)-type regret guarantees when propensities are known, and extend these guarantees to the unknown-propensity case via a doubly robust (DR) objective. We further show that abstention is a versatile tool with direct applications to other core problems in policy learning: it yields improved guarantees under margin conditions without the common realizability assumption, connects to distributionally robust policy learning by hedging against small data shifts, and supports safe policy improvement by ensuring improvement over a baseline policy with high probability.</li>
</ul>

<h3>Title: CircuitGuard: Mitigating LLM Memorization in RTL Code Generation Against IP Leakage</h3>
<ul>
<li><strong>Authors: </strong>Nowfel Mashnoor, Mohammad Akyash, Hadi Kamali, Kimia Azar</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19676">https://arxiv.org/abs/2510.19676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19676">https://arxiv.org/pdf/2510.19676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19676]] CircuitGuard: Mitigating LLM Memorization in RTL Code Generation Against IP Leakage(https://arxiv.org/abs/2510.19676)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, robust, transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success in generative tasks, including register-transfer level (RTL) hardware synthesis. However, their tendency to memorize training data poses critical risks when proprietary or security-sensitive designs are unintentionally exposed during inference. While prior work has examined memorization in natural language, RTL introduces unique challenges: In RTL, structurally different implementations (e.g., behavioral vs. gate-level descriptions) can realize the same hardware, leading to intellectual property (IP) leakage (full or partial) even without verbatim overlap. Conversely, even small syntactic variations (e.g., operator precedence or blocking vs. non-blocking assignments) can drastically alter circuit behavior, making correctness preservation especially challenging. In this work, we systematically study memorization in RTL code generation and propose CircuitGuard, a defense strategy that balances leakage reduction with correctness preservation. CircuitGuard (1) introduces a novel RTL-aware similarity metric that captures both structural and functional equivalence beyond surface-level overlap, and (2) develops an activation-level steering method that identifies and attenuates transformer components most responsible for memorization. Our empirical evaluation demonstrates that CircuitGuard identifies (and isolates) 275 memorization-critical features across layers 18-28 of Llama 3.1-8B model, achieving up to 80% reduction in semantic similarity to proprietary patterns while maintaining generation quality. CircuitGuard further shows 78-85% cross-domain transfer effectiveness, enabling robust memorization mitigation across circuit categories without retraining.</li>
</ul>

<h3>Title: I Spy With My Model's Eye: Visual Search as a Behavioural Test for MLLMs</h3>
<ul>
<li><strong>Authors: </strong>John Burden, Jonathan Prunty, Ben Slater, Matthieu Tehenan, Greg Davis, Lucy Cheke</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19678">https://arxiv.org/abs/2510.19678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19678">https://arxiv.org/pdf/2510.19678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19678]] I Spy With My Model's Eye: Visual Search as a Behavioural Test for MLLMs(https://arxiv.org/abs/2510.19678)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) achieve strong performance on vision-language tasks, yet their visual processing is opaque. Most black-box evaluations measure task accuracy, but reveal little about underlying mechanisms. Drawing on cognitive psychology, we adapt classic visual search paradigms -- originally developed to study human perception -- to test whether MLLMs exhibit the ``pop-out'' effect, where salient visual features are detected independently of distractor set size. Using controlled experiments targeting colour, size and lighting features, we find that advanced MLLMs exhibit human-like pop-out effects in colour or size-based disjunctive (single feature) search, as well as capacity limits for conjunctive (multiple feature) search. We also find evidence to suggest that MLLMs, like humans, incorporate natural scene priors such as lighting direction into object representations. We reinforce our findings using targeted fine-tuning and mechanistic interpretability analyses. Our work shows how visual search can serve as a cognitively grounded diagnostic tool for evaluating perceptual capabilities in MLLMs.</li>
</ul>

<h3>Title: Curvilinear Structure-preserving Unpaired Cross-domain Medical Image Translation</h3>
<ul>
<li><strong>Authors: </strong>Zihao Chen, Yi Zhou, Xudong Jiang, Li Chen, Leopold Schmetterer, Bingyao Tan, Jun Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19679">https://arxiv.org/abs/2510.19679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19679">https://arxiv.org/pdf/2510.19679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19679]] Curvilinear Structure-preserving Unpaired Cross-domain Medical Image Translation(https://arxiv.org/abs/2510.19679)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Unpaired image-to-image translation has emerged as a crucial technique in medical imaging, enabling cross-modality synthesis, domain adaptation, and data augmentation without costly paired datasets. Yet, existing approaches often distort fine curvilinear structures, such as microvasculature, undermining both diagnostic reliability and quantitative analysis. This limitation is consequential in ophthalmic and vascular imaging, where subtle morphological changes carry significant clinical meaning. We propose Curvilinear Structure-preserving Translation (CST), a general framework that explicitly preserves fine curvilinear structures during unpaired translation by integrating structure consistency into the training. Specifically, CST augments baseline models with a curvilinear extraction module for topological supervision. It can be seamlessly incorporated into existing methods. We integrate it into CycleGAN and UNSB as two representative backbones. Comprehensive evaluation across three imaging modalities: optical coherence tomography angiography, color fundus and X-ray coronary angiography demonstrates that CST improves translation fidelity and achieves state-of-the-art performance. By reinforcing geometric integrity in learned mappings, CST establishes a principled pathway toward curvilinear structure-aware cross-domain translation in medical imaging.</li>
</ul>

<h3>Title: Are Large Language Models Sensitive to the Motives Behind Communication?</h3>
<ul>
<li><strong>Authors: </strong>Addison J. Wu, Ryan Liu, Kerem Oktar, Theodore R. Sumers, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19687">https://arxiv.org/abs/2510.19687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19687">https://arxiv.org/pdf/2510.19687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19687]] Are Large Language Models Sensitive to the Motives Behind Communication?(https://arxiv.org/abs/2510.19687)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Human communication is motivated: people speak, write, and create content with a particular communicative intent in mind. As a result, information that large language models (LLMs) and AI agents process is inherently framed by humans' intentions and incentives. People are adept at navigating such nuanced information: we routinely identify benevolent or self-serving motives in order to decide what statements to trust. For LLMs to be effective in the real world, they too must critically evaluate content by factoring in the motivations of the source -- for instance, weighing the credibility of claims made in a sales pitch. In this paper, we undertake a comprehensive study of whether LLMs have this capacity for motivational vigilance. We first employ controlled experiments from cognitive science to verify that LLMs' behavior is consistent with rational models of learning from motivated testimony, and find they successfully discount information from biased sources in a human-like manner. We then extend our evaluation to sponsored online adverts, a more naturalistic reflection of LLM agents' information ecosystems. In these settings, we find that LLMs' inferences do not track the rational models' predictions nearly as closely -- partly due to additional information that distracts them from vigilance-relevant considerations. However, a simple steering intervention that boosts the salience of intentions and incentives substantially increases the correspondence between LLMs and the rational model. These results suggest that LLMs possess a basic sensitivity to the motivations of others, but generalizing to novel real-world settings will require further improvements to these models.</li>
</ul>

<h3>Title: Explainable Face Presentation Attack Detection via Ensemble-CAM</h3>
<ul>
<li><strong>Authors: </strong>Rashik Shadman, M G Sarwar Murshed, Faraz Hussain</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19695">https://arxiv.org/abs/2510.19695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19695">https://arxiv.org/pdf/2510.19695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19695]] Explainable Face Presentation Attack Detection via Ensemble-CAM(https://arxiv.org/abs/2510.19695)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, biometric, explainability</a></li>
<li><strong>Abstract: </strong>Presentation attacks represent a critical security threat where adversaries use fake biometric data, such as face, fingerprint, or iris images, to gain unauthorized access to protected systems. Various presentation attack detection (PAD) systems have been designed leveraging deep learning (DL) models to mitigate this type of threat. Despite their effectiveness, most of the DL models function as black boxes - their decisions are opaque to their users. The purpose of explainability techniques is to provide detailed information about the reason behind the behavior or decision of DL models. In particular, visual explanation is necessary to better understand the decisions or predictions of DL-based PAD systems and determine the key regions due to which a biometric image is considered real or fake by the system. In this work, a novel technique, Ensemble-CAM, is proposed for providing visual explanations for the decisions made by deep learning-based face PAD systems. Our goal is to improve DL-based face PAD systems by providing a better understanding of their behavior. Our provided visual explanations will enhance the transparency and trustworthiness of DL-based face PAD systems.</li>
</ul>

<h3>Title: Fast Inference via Hierarchical Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Amir Globerson, Haim Kaplan, Yishay Mansour, Clara Mohri, Tal Schuster</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19705">https://arxiv.org/abs/2510.19705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19705">https://arxiv.org/pdf/2510.19705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19705]] Fast Inference via Hierarchical Speculative Decoding(https://arxiv.org/abs/2510.19705)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer language models generate text autoregressively, making inference latency proportional to the number of tokens generated. Speculative decoding reduces this latency without sacrificing output quality, by leveraging a small draft model to propose tokens that the larger target model verifies in parallel. In practice, however, there may exist a set of potential draft models- ranging from faster but less inaccurate, to slower yet more reliable. We introduce Hierarchical Speculative Decoding (HSD), an algorithm that stacks these draft models into a hierarchy, where each model proposes tokens, and the next larger model verifies them in a single forward pass, until finally the target model verifies tokens. We derive an expression for the expected latency of any such hierarchy and show that selecting the latency-optimal hierarchy can be done in polynomial time. Empirically, HSD gives up to 1.2x speed-up over the best single-draft baseline, demonstrating the practicality of our algorithm in reducing generation latency beyond previous techniques.</li>
</ul>

<h3>Title: SEMPO: Lightweight Foundation Models for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Hui He, Kun Yi, Yuanchi Ma, Qi Zhang, Zhendong Niu, Guansong Pang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19710">https://arxiv.org/abs/2510.19710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19710">https://arxiv.org/pdf/2510.19710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19710]] SEMPO: Lightweight Foundation Models for Time Series Forecasting(https://arxiv.org/abs/2510.19710)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The recent boom of large pre-trained models witnesses remarkable success in developing foundation models (FMs) for time series forecasting. Despite impressive performance across diverse downstream forecasting tasks, existing time series FMs possess massive network architectures and require substantial pre-training on large-scale datasets, which significantly hinders their deployment in resource-constrained environments. In response to this growing tension between versatility and affordability, we propose SEMPO, a novel lightweight foundation model that requires pretraining on relatively small-scale data, yet exhibits strong general time series forecasting. Concretely, SEMPO comprises two key modules: 1) energy-aware SpEctral decomposition module, that substantially improves the utilization of pre-training data by modeling not only the high-energy frequency signals but also the low-energy yet informative frequency signals that are ignored in current methods; and 2) Mixture-of-PrOmpts enabled Transformer, that learns heterogeneous temporal patterns through small dataset-specific prompts and adaptively routes time series tokens to prompt-based experts for parameter-efficient model adaptation across different datasets and domains. Equipped with these modules, SEMPO significantly reduces both pre-training data scale and model size, while achieving strong generalization. Extensive experiments on two large-scale benchmarks covering 16 datasets demonstrate the superior performance of SEMPO in both zero-shot and few-shot forecasting scenarios compared with state-of-the-art methods. Code and data are available at this https URL.</li>
</ul>

<h3>Title: LyTimeT: Towards Robust and Interpretable State-Variable Discovery</h3>
<ul>
<li><strong>Authors: </strong>Kuai Yu, Crystal Su, Xiang Liu, Judah Goldfeder, Mingyuan Shao, Hod Lipson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19716">https://arxiv.org/abs/2510.19716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19716">https://arxiv.org/pdf/2510.19716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19716]] LyTimeT: Towards Robust and Interpretable State-Variable Discovery(https://arxiv.org/abs/2510.19716)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Extracting the true dynamical variables of a system from high-dimensional video is challenging due to distracting visual factors such as background motion, occlusions, and texture changes. We propose LyTimeT, a two-phase framework for interpretable variable extraction that learns robust and stable latent representations of dynamical systems. In Phase 1, LyTimeT employs a spatio-temporal TimeSformer-based autoencoder that uses global attention to focus on dynamically relevant regions while suppressing nuisance variation, enabling distraction-robust latent state learning and accurate long-horizon video prediction. In Phase 2, we probe the learned latent space, select the most physically meaningful dimensions using linear correlation analysis, and refine the transition dynamics with a Lyapunov-based stability regularizer to enforce contraction and reduce error accumulation during roll-outs. Experiments on five synthetic benchmarks and four real-world dynamical systems, including chaotic phenomena, show that LyTimeT achieves mutual information and intrinsic dimension estimates closest to ground truth, remains invariant under background perturbations, and delivers the lowest analytical mean squared error among CNN-based (TIDE) and transformer-only baselines. Our results demonstrate that combining spatio-temporal attention with stability constraints yields predictive models that are not only accurate but also physically interpretable.</li>
</ul>

<h3>Title: Enabling Granular Subgroup Level Model Evaluations by Generating Synthetic Medical Time Series</h3>
<ul>
<li><strong>Authors: </strong>Mahmoud Ibrahim, Bart Elen, Chang Sun, Gökhan Ertaylan, Michel Dumontier</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19728">https://arxiv.org/abs/2510.19728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19728">https://arxiv.org/pdf/2510.19728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19728]] Enabling Granular Subgroup Level Model Evaluations by Generating Synthetic Medical Time Series(https://arxiv.org/abs/2510.19728)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, diffusion</a></li>
<li><strong>Abstract: </strong>We present a novel framework for leveraging synthetic ICU time-series data not only to train but also to rigorously and trustworthily evaluate predictive models, both at the population level and within fine-grained demographic subgroups. Building on prior diffusion and VAE-based generators (TimeDiff, HealthGen, TimeAutoDiff), we introduce \textit{Enhanced TimeAutoDiff}, which augments the latent diffusion objective with distribution-alignment penalties. We extensively benchmark all models on MIMIC-III and eICU, on 24-hour mortality and binary length-of-stay tasks. Our results show that Enhanced TimeAutoDiff reduces the gap between real-on-synthetic and real-on-real evaluation (``TRTS gap'') by over 70\%, achieving $\Delta_{TRTS} \leq 0.014$ AUROC, while preserving training utility ($\Delta_{TSTR} \approx 0.01$). Crucially, for 32 intersectional subgroups, large synthetic cohorts cut subgroup-level AUROC estimation error by up to 50\% relative to small real test sets, and outperform them in 72--84\% of subgroups. This work provides a practical, privacy-preserving roadmap for trustworthy, granular model evaluation in critical care, enabling robust and reliable performance analysis across diverse patient populations without exposing sensitive EHR data, contributing to the overall trustworthiness of Medical AI.</li>
</ul>

<h3>Title: Zhyper: Factorized Hypernetworks for Conditioned LLM Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>M. H. I. Abdalla, Zhipin Wang, Christian Frey, Steffen Eger, Josif Grabocka</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19733">https://arxiv.org/abs/2510.19733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19733">https://arxiv.org/pdf/2510.19733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19733]] Zhyper: Factorized Hypernetworks for Conditioned LLM Fine-Tuning(https://arxiv.org/abs/2510.19733)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) conditioning refers to instructing an LLM to generate content in accordance with the norms and values of a specific culture, beliefs of a particular political orientation, or any desired text-specified semantic conditioning. Unfortunately, prompt engineering does not ensure that LLMs behave in accordance with a desired conditioning due to the inductive bias of the pre-training and alignment datasets. Prior works have focused on fine-tuning LLMs by directly conditioning the LoRA weights; however, such methods introduce a large number of parameters. As a remedy, we propose Zhyper, a parameter-efficient factorized hypernetwork framework that generates context-aware LoRA adapters from textual descriptions. Experiments on multiple benchmarks show that Zhyper achieves competitive performance with up to 26x fewer parameters than the state-of-the-art baselines. Furthermore, we extend Zhyper to cultural alignment, demonstrating improved generalization to out-of-domain settings and a better capturing of fine-grained contextual values.</li>
</ul>

<h3>Title: When Do Transformers Learn Heuristics for Graph Connectivity?</h3>
<ul>
<li><strong>Authors: </strong>Qilin Ye, Deqing Fu, Robin Jia, Vatsal Sharan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19753">https://arxiv.org/abs/2510.19753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19753">https://arxiv.org/pdf/2510.19753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19753]] When Do Transformers Learn Heuristics for Graph Connectivity?(https://arxiv.org/abs/2510.19753)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers often fail to learn generalizable algorithms, instead relying on brittle heuristics. Using graph connectivity as a testbed, we explain this phenomenon both theoretically and empirically. We consider a simplified Transformer architecture, the disentangled Transformer, and prove that an $L$-layer model has capacity to solve for graphs with diameters up to exactly $3^L$, implementing an algorithm equivalent to computing powers of the adjacency matrix. We analyze the training-dynamics, and show that the learned strategy hinges on whether most training instances are within this model capacity. Within-capacity graphs (diameter $\leq 3^L$) drive the learning of a correct algorithmic solution while beyond-capacity graphs drive the learning of a simple heuristic based on node degrees. Finally, we empirically demonstrate that restricting training data within a model's capacity leads to both standard and disentangled transformers learning the exact algorithm rather than the degree-based heuristic.</li>
</ul>

<h3>Title: CONFEX: Uncertainty-Aware Counterfactual Explanations with Conformal Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Aman Bilkhoo (1), Milad Kazemi (1), Nicola Paoletti (1), Mehran Hosseini (1 and 2) ((1) Department of Informatics, King's College London, (2) Department of Computer Science, University of Manchester)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19754">https://arxiv.org/abs/2510.19754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19754">https://arxiv.org/pdf/2510.19754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19754]] CONFEX: Uncertainty-Aware Counterfactual Explanations with Conformal Guarantees(https://arxiv.org/abs/2510.19754)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Counterfactual explanations (CFXs) provide human-understandable justifications for model predictions, enabling actionable recourse and enhancing interpretability. To be reliable, CFXs must avoid regions of high predictive uncertainty, where explanations may be misleading or inapplicable. However, existing methods often neglect uncertainty or lack principled mechanisms for incorporating it with formal guarantees. We propose CONFEX, a novel method for generating uncertainty-aware counterfactual explanations using Conformal Prediction (CP) and Mixed-Integer Linear Programming (MILP). CONFEX explanations are designed to provide local coverage guarantees, addressing the issue that CFX generation violates exchangeability. To do so, we develop a novel localised CP procedure that enjoys an efficient MILP encoding by leveraging an offline tree-based partitioning of the input space. This way, CONFEX generates CFXs with rigorous guarantees on both predictive uncertainty and optimality. We evaluate CONFEX against state-of-the-art methods across diverse benchmarks and metrics, demonstrating that our uncertainty-aware approach yields robust and plausible explanations.</li>
</ul>

<h3>Title: A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19755">https://arxiv.org/abs/2510.19755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19755">https://arxiv.org/pdf/2510.19755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19755]] A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation(https://arxiv.org/abs/2510.19755)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \textit{multi-step iterations} and \textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation. Against this backdrop, \textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis. Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \textit{Efficient Generative Intelligence}.</li>
</ul>

<h3>Title: Exploring the Effect of DNN Depth on Adversarial Attacks in Network Intrusion Detection Systems</h3>
<ul>
<li><strong>Authors: </strong>Mohamed ElShehaby, Ashraf Matrawy</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19761">https://arxiv.org/abs/2510.19761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19761">https://arxiv.org/pdf/2510.19761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19761]] Exploring the Effect of DNN Depth on Adversarial Attacks in Network Intrusion Detection Systems(https://arxiv.org/abs/2510.19761)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Adversarial attacks pose significant challenges to Machine Learning (ML) systems and especially Deep Neural Networks (DNNs) by subtly manipulating inputs to induce incorrect predictions. This paper investigates whether increasing the layer depth of deep neural networks affects their robustness against adversarial attacks in the Network Intrusion Detection System (NIDS) domain. We compare the adversarial robustness of various deep neural networks across both \ac{NIDS} and computer vision domains (the latter being widely used in adversarial attack experiments). Our experimental results reveal that in the NIDS domain, adding more layers does not necessarily improve their performance, yet it may actually significantly degrade their robustness against adversarial attacks. Conversely, in the computer vision domain, adding more layers exhibits a more modest impact on robustness. These findings can guide the development of robust neural networks for (NIDS) applications and highlight the unique characteristics of network security domains within the (ML) landscape.</li>
</ul>

<h3>Title: SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via Promoting Deeper Thought Exploration</h3>
<ul>
<li><strong>Authors: </strong>Xichen Zhang, Sitong Wu, Haoru Tan, Shaozuo Yu, Yinghao Zhu, Ziyi He, Jiaya Jia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19767">https://arxiv.org/abs/2510.19767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19767">https://arxiv.org/pdf/2510.19767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19767]] SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via Promoting Deeper Thought Exploration(https://arxiv.org/abs/2510.19767)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The long chain-of-thought (LongCoT) capability is central to the recent breakthroughs achieved by large language models in complex reasoning tasks. However, the accompanying issue of ''underthinking'', where models exhibit shallow reasoning by frequently switching thoughts without sufficient exploration, limits both performance and token efficiency. To address this problem, we propose a simple yet effective reasoning strategy: the SmartSwitch inference framework. This framework can be easily integrated into any large language model as a plug-and-play solution, continuously monitoring the model's reasoning process to detect underthinking and guide it toward deeper exploration of promising but overlooked thoughts. Specifically, the perception module identifies points where thoughts switch and evaluates the potential of the preceding thought using an off-the-shelf process reward model (PRM). If a high-potential thought is found to be prematurely abandoned, the intervention module interrupts the ongoing inference, backtracks to the point before the switch, and inserts a "deepening prompt" to encourage further exploration along that promising path. Extensive experiments on challenging mathematical reasoning benchmarks demonstrate that our method significantly enhances the performance of various large language models of different sizes.</li>
</ul>

<h3>Title: Under Pressure: Security Analysis and Process Impacts of a Commercial Smart Air Compressor</h3>
<ul>
<li><strong>Authors: </strong>Jad Zarzour, Matthew Jablonski</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19772">https://arxiv.org/abs/2510.19772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19772">https://arxiv.org/pdf/2510.19772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19772]] Under Pressure: Security Analysis and Process Impacts of a Commercial Smart Air Compressor(https://arxiv.org/abs/2510.19772)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>The integration of Industrial Internet of Things (IIoT) devices into manufacturing environments has accelerated the transition to Industry 4.0, but has also introduced new cybersecurity risks. This paper conducts a comprehensive security analysis of a commercial smart air compressor, revealing critical vulnerabilities including hardcoded credentials, unauthenticated APIs, and an insecure update mechanism. It includes a formal threat model, demonstrates practical attack scenarios in a testbed environment, and evaluates their subsequent impact on an industrial process, leading to denial of service and the corruption of critical process telemetry. In addition, an analysis of the device's supply chain reveals how product integration from multiple vendors and limited security considerations can expose a device to threats. The findings underscore the necessity of incorporating cybersecurity principles into both IIoT device design and supply chain governance to enhance resilience against emerging industrial cyber threats.</li>
</ul>

<h3>Title: The Tail Tells All: Estimating Model-Level Membership Inference Vulnerability Without Reference Models</h3>
<ul>
<li><strong>Authors: </strong>Euodia Dodd, Nataša Krčo, Igor Shilov, Yves-Alexandre de Montjoye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19773">https://arxiv.org/abs/2510.19773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19773">https://arxiv.org/pdf/2510.19773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19773]] The Tail Tells All: Estimating Model-Level Membership Inference Vulnerability Without Reference Models(https://arxiv.org/abs/2510.19773)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Membership inference attacks (MIAs) have emerged as the standard tool for evaluating the privacy risks of AI models. However, state-of-the-art attacks require training numerous, often computationally expensive, reference models, limiting their practicality. We present a novel approach for estimating model-level vulnerability, the TPR at low FPR, to membership inference attacks without requiring reference models. Empirical analysis shows loss distributions to be asymmetric and heavy-tailed and suggests that most points at risk from MIAs have moved from the tail (high-loss region) to the head (low-loss region) of the distribution after training. We leverage this insight to propose a method to estimate model-level vulnerability from the training and testing distribution alone: using the absence of outliers from the high-loss region as a predictor of the risk. We evaluate our method, the TNR of a simple loss attack, across a wide range of architectures and datasets and show it to accurately estimate model-level vulnerability to the SOTA MIA attack (LiRA). We also show our method to outperform both low-cost (few reference models) attacks such as RMIA and other measures of distribution difference. We finally evaluate the use of non-linear functions to evaluate risk and show the approach to be promising to evaluate the risk in large-language models.</li>
</ul>

<h3>Title: GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters</h3>
<ul>
<li><strong>Authors: </strong>Anand Choudhary, Yasser Sulaıman, Lukas Mauch, Ghouthi Boukli Hacene, Fabien Cardinaux, Antoine Bosselut</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19778">https://arxiv.org/abs/2510.19778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19778">https://arxiv.org/pdf/2510.19778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19778]] GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters(https://arxiv.org/abs/2510.19778)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Sparse fine-tuning techniques adapt LLMs to downstream tasks by only tuning a sparse subset of model parameters. However, the effectiveness of sparse adaptation depends on optimally selecting the model parameters to be fine-tuned. In this work, we introduce a novel sparse fine-tuning technique named GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters, which fine-tunes only those model parameters which have the largest gradient magnitudes on downstream tasks and the smallest pre-trained magnitudes, intuitively prioritizing parameters that are highly task-relevant, but minimally disruptive to pre-trained knowledge. Our experimentation with LLaMA3 8B and Gemma 2B as base models shows that GaLLoP consistently improves or matches the in-distribution as well as out-of-distribution performance obtained via the usage of other leading parameter-efficient fine-tuning techniques, including LoRA, DoRA, and SAFT. Our analysis demonstrates that GaLLoP mitigates catastrophic forgetting and memorization of task data, as important pre-trained parameters remain unchanged, and stabilizes performance relative to other fine-tuning techniques, robustly generalizing across most random seeds.</li>
</ul>

<h3>Title: AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders</h3>
<ul>
<li><strong>Authors: </strong>Yuezhou Hu, Jiaxin Guo, Xinyu Feng, Tuo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19779">https://arxiv.org/abs/2510.19779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19779">https://arxiv.org/pdf/2510.19779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19779]] AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders(https://arxiv.org/abs/2510.19779)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\%). The code is publicly available at this https URL.</li>
</ul>

<h3>Title: Environment Inference for Learning Generalizable Dynamical System</h3>
<ul>
<li><strong>Authors: </strong>Shixuan Liu, Yue He, Haotian Wang, Wenjing Yang, Yunfei Wang, Peng Cui, Zhong Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19784">https://arxiv.org/abs/2510.19784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19784">https://arxiv.org/pdf/2510.19784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19784]] Environment Inference for Learning Generalizable Dynamical System(https://arxiv.org/abs/2510.19784)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Data-driven methods offer efficient and robust solutions for analyzing complex dynamical systems but rely on the assumption of I.I.D. data, driving the development of generalization techniques for handling environmental differences. These techniques, however, are limited by their dependence on environment labels, which are often unavailable during training due to data acquisition challenges, privacy concerns, and environmental variability, particularly in large public datasets and privacy-sensitive domains. In response, we propose DynaInfer, a novel method that infers environment specifications by analyzing prediction errors from fixed neural networks within each training round, enabling environment assignments directly from data. We prove our algorithm effectively solves the alternating optimization problem in unlabeled scenarios and validate it through extensive experiments across diverse dynamical systems. Results show that DynaInfer outperforms existing environment assignment techniques, converges rapidly to true labels, and even achieves superior performance when environment labels are available.</li>
</ul>

<h3>Title: OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Guowei Xu, Yuxuan Bian, Ailing Zeng, Mingyi Shi, Shaoli Huang, Wen Li, Lixin Duan, Qiang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19789">https://arxiv.org/abs/2510.19789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19789">https://arxiv.org/pdf/2510.19789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19789]] OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation(https://arxiv.org/abs/2510.19789)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>This paper introduces OmniMotion-X, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks, including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis), as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct OmniMoCap-X, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.</li>
</ul>

<h3>Title: ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers</h3>
<ul>
<li><strong>Authors: </strong>Saptarshi Sengupta, Zhengyu Zhou, Jun Araki, Xingbo Wang, Bingqing Wang, Suhang Wang, Zhe Feng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19791">https://arxiv.org/abs/2510.19791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19791">https://arxiv.org/pdf/2510.19791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19791]] ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers(https://arxiv.org/abs/2510.19791)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tool calling has become increasingly popular for Large Language Models (LLMs). However, for large tool sets, the resulting tokens would exceed the LLM's context window limit, making it impossible to include every tool. Hence, an external retriever is used to provide LLMs with the most relevant tools for a query. Existing retrieval models rank tools based on the similarity between a user query and a tool description (TD). This leads to suboptimal retrieval as user requests are often poorly aligned with the language of TD. To remedy the issue, we propose ToolDreamer, a framework to condition retriever models to fetch tools based on hypothetical (synthetic) TD generated using an LLM, i.e., description of tools that the LLM feels will be potentially useful for the query. The framework enables a more natural alignment between queries and tools within the language space of TD's. We apply ToolDreamer on the ToolRet dataset and show that our method improves the performance of sparse and dense retrievers with and without training, thus showcasing its flexibility. Through our proposed framework, our aim is to offload a portion of the reasoning burden to the retriever so that the LLM may effectively handle a large collection of tools without inundating its context window.</li>
</ul>

<h3>Title: Blackbox Model Provenance via Palimpsestic Membership Inference</h3>
<ul>
<li><strong>Authors: </strong>Rohith Kuditipudi, Jing Huang, Sally Zhu, Diyi Yang, Christopher Potts, Percy Liang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19796">https://arxiv.org/abs/2510.19796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19796">https://arxiv.org/pdf/2510.19796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19796]] Blackbox Model Provenance via Palimpsestic Membership Inference(https://arxiv.org/abs/2510.19796)</code><input type="text"></li>
<li><strong>Keywords: </strong>membership infer</a></li>
<li><strong>Abstract: </strong>Suppose Alice trains an open-weight language model and Bob uses a blackbox derivative of Alice's model to produce text. Can Alice prove that Bob is using her model, either by querying Bob's derivative model (query setting) or from the text alone (observational setting)? We formulate this question as an independence testing problem--in which the null hypothesis is that Bob's model or text is independent of Alice's randomized training run--and investigate it through the lens of palimpsestic memorization in language models: models are more likely to memorize data seen later in training, so we can test whether Bob is using Alice's model using test statistics that capture correlation between Bob's model or text and the ordering of training examples in Alice's training run. If Alice has randomly shuffled her training data, then any significant correlation amounts to exactly quantifiable statistical evidence against the null hypothesis, regardless of the composition of Alice's training data. In the query setting, we directly estimate (via prompting) the likelihood Bob's model gives to Alice's training examples and order; we correlate the likelihoods of over 40 fine-tunes of various Pythia and OLMo base models ranging from 1B to 12B parameters with the base model's training data order, achieving a p-value on the order of at most 1e-8 in all but six cases. In the observational setting, we try two approaches based on estimating 1) the likelihood of Bob's text overlapping with spans of Alice's training examples and 2) the likelihood of Bob's text with respect to different versions of Alice's model we obtain by repeating the last phase (e.g., 1%) of her training run on reshuffled data. The second approach can reliably distinguish Bob's text from as little as a few hundred tokens; the first does not involve any retraining but requires many more tokens (several hundred thousand) to achieve high power.</li>
</ul>

<h3>Title: Transformers are almost optimal metalearners for linear classification</h3>
<ul>
<li><strong>Authors: </strong>Roey Magen, Gal Vardi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19797">https://arxiv.org/abs/2510.19797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19797">https://arxiv.org/pdf/2510.19797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19797]] Transformers are almost optimal metalearners for linear classification(https://arxiv.org/abs/2510.19797)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers have demonstrated impressive in-context learning (ICL) capabilities, raising the question of whether they can serve as metalearners that adapt to new tasks using only a small number of in-context examples, without any further training. While recent theoretical work has studied transformers' ability to perform ICL, most of these analyses do not address the formal metalearning setting, where the objective is to solve a collection of related tasks more efficiently than would be possible by solving each task individually. In this paper, we provide the first theoretical analysis showing that a simplified transformer architecture trained via gradient descent can act as a near-optimal metalearner in a linear classification setting. We consider a natural family of tasks where each task corresponds to a class-conditional Gaussian mixture model, with the mean vectors lying in a shared $k$-dimensional subspace of $R^d$. After training on a sufficient number of such tasks, we show that the transformer can generalize to a new task using only $O(k / R^4)$ in-context examples, where $R$ denotes the signal strength at test time. This performance (almost) matches that of an optimal learner that knows exactly the shared subspace and significantly outperforms any learner that only has access to the in-context data, which requires $\Omega(d / R^4)$ examples to generalize. Importantly, our bounds on the number of training tasks and examples per task needed to achieve this result are independent of the ambient dimension $d$.</li>
</ul>

<h3>Title: The Art of Asking: Multilingual Prompt Optimization for Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>David Mora, Viraat Aryabumi, Wei-Yin Ko, Sara Hooker, Julia Kreutzer, Marzieh Fadaee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19806">https://arxiv.org/abs/2510.19806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19806">https://arxiv.org/pdf/2510.19806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19806]] The Art of Asking: Multilingual Prompt Optimization for Synthetic Data(https://arxiv.org/abs/2510.19806)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Synthetic data has become a cornerstone for scaling large language models, yet its multilingual use remains bottlenecked by translation-based prompts. This strategy inherits English-centric framing and style and neglects cultural dimensions, ultimately constraining model generalization. We argue that the overlooked prompt space-the very inputs that define training distributions-offers a more powerful lever for improving multilingual performance. We introduce a lightweight framework for prompt-space optimization, where translated prompts are systematically transformed for Naturalness, Cultural Adaptation, and Difficulty Enhancement. Using an off-the-shelf multilingual LLM, we apply these transformations to prompts for 12 languages spanning 7 families. Under identical data conditions, our approaches achieve substantial and consistent downstream improvements over the translation-only baseline: +4.7% on Global-MMLU accuracy, +2.4% on Flores XCometXL and +35.3% wins in preferences on mArenaHard. We establish prompt-space optimization as a simple yet powerful paradigm for building multilingual LLMs that are more robust, culturally grounded, and globally capable.</li>
</ul>

<h3>Title: Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xichen Zhang, Sitong Wu, Yinghao Zhu, Haoru Tan, Shaozuo Yu, Ziyi He, Jiaya Jia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19807">https://arxiv.org/abs/2510.19807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19807">https://arxiv.org/pdf/2510.19807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19807]] Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning(https://arxiv.org/abs/2510.19807)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from verifiable rewards has emerged as a powerful technique for enhancing the complex reasoning abilities of Large Language Models (LLMs). However, these methods are fundamentally constrained by the ''learning cliff'' phenomenon: when faced with problems far beyond their current capabilities, models consistently fail, yielding a persistent zero-reward signal. In policy optimization algorithms like GRPO, this collapses the advantage calculation to zero, rendering these difficult problems invisible to the learning gradient and stalling progress. To overcome this, we introduce Scaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive training framework that strategically provides minimal guidance only when a model's independent learning has plateaued. The framework first diagnoses learning stagnation and then intervenes by injecting tiered in-prompt hints, ranging from abstract concepts to concrete steps, enabling the model to construct a valid solution by itself. Extensive experiments on challenging mathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the pass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative 44.3% over a vanilla GRPO baseline. This result demonstrates our framework provides a robust and effective methodology for unlocking a model's ability to solve problems previously beyond its reach, a critical step towards extending the frontier of autonomous reasoning in LLM.</li>
</ul>

<h3>Title: Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Yusu Qian, Eli Bocek-Rivele, Liangchen Song, Jialing Tong, Yinfei Yang, Jiasen Lu, Wenze Hu, Zhe Gan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19808">https://arxiv.org/abs/2510.19808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19808">https://arxiv.org/pdf/2510.19808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19808]] Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing(https://arxiv.org/abs/2510.19808)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in multimodal models have demonstrated remarkable text-guided image editing capabilities, with systems like GPT-4o and Nano-Banana setting new benchmarks. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built from real images. We introduce Pico-Banana-400K, a comprehensive 400K-image dataset for instruction-based image editing. Our dataset is constructed by leveraging Nano-Banana to generate diverse edit pairs from real photographs in the OpenImages collection. What distinguishes Pico-Banana-400K from previous synthetic datasets is our systematic approach to quality and diversity. We employ a fine-grained image editing taxonomy to ensure comprehensive coverage of edit types while maintaining precise content preservation and instruction faithfulness through MLLM-based quality scoring and careful curation. Beyond single turn editing, Pico-Banana-400K enables research into complex editing scenarios. The dataset includes three specialized subsets: (1) a 72K-example multi-turn collection for studying sequential editing, reasoning, and planning across consecutive modifications; (2) a 56K-example preference subset for alignment research and reward model training; and (3) paired long-short editing instructions for developing instruction rewriting and summarization capabilities. By providing this large-scale, high-quality, and task-rich resource, Pico-Banana-400K establishes a robust foundation for training and benchmarking the next generation of text-guided image editing models.</li>
</ul>

<h3>Title: Hubble: a Model Suite to Advance the Study of LLM Memorization</h3>
<ul>
<li><strong>Authors: </strong>Johnny Tian-Zheng Wei, Ameya Godbole, Mohammad Aflah Khan, Ryan Wang, Xiaoyuan Zhu, James Flemings, Nitya Kashyap, Krishna P. Gummadi, Willie Neiswanger, Robin Jia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19811">https://arxiv.org/abs/2510.19811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19811">https://arxiv.org/pdf/2510.19811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19811]] Hubble: a Model Suite to Advance the Study of LLM Memorization(https://arxiv.org/abs/2510.19811)</code><input type="text"></li>
<li><strong>Keywords: </strong>membership infer, large language model</a></li>
<li><strong>Abstract: </strong>We present Hubble, a suite of fully open-source large language models (LLMs) for the scientific study of LLM memorization. Hubble models come in standard and perturbed variants: standard models are pretrained on a large English corpus, and perturbed models are trained in the same way but with controlled insertion of text (e.g., book passages, biographies, and test sets) designed to emulate key memorization risks. Our core release includes 8 models -- standard and perturbed models with 1B or 8B parameters, pretrained on 100B or 500B tokens -- establishing that memorization risks are determined by the frequency of sensitive data relative to size of the training corpus (i.e., a password appearing once in a smaller corpus is memorized better than the same password in a larger corpus). Our release also includes 6 perturbed models with text inserted at different pretraining phases, showing that sensitive data without continued exposure can be forgotten. These findings suggest two best practices for addressing memorization risks: to dilute sensitive data by increasing the size of the training corpus, and to order sensitive data to appear earlier in training. Beyond these general empirical findings, Hubble enables a broad range of memorization research; for example, analyzing the biographies reveals how readily different types of private information are memorized. We also demonstrate that the randomized insertions in Hubble make it an ideal testbed for membership inference and machine unlearning, and invite the community to further explore, benchmark, and build upon our work.</li>
</ul>

<h3>Title: Semantic World Models</h3>
<ul>
<li><strong>Authors: </strong>Jacob Berg, Chuning Zhu, Yanda Bao, Ishan Durugkar, Abhishek Gupta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19818">https://arxiv.org/abs/2510.19818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19818">https://arxiv.org/pdf/2510.19818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19818]] Semantic World Models(https://arxiv.org/abs/2510.19818)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Planning with world models offers a powerful paradigm for robotic control. Conventional approaches train a model to predict future frames conditioned on current frames and actions, which can then be used for planning. However, the objective of predicting future pixels is often at odds with the actual planning objective; strong pixel reconstruction does not always correlate with good planning decisions. This paper posits that instead of reconstructing future frames as pixels, world models only need to predict task-relevant semantic information about the future. For such prediction the paper poses world modeling as a visual question answering problem about semantic information in future frames. This perspective allows world modeling to be approached with the same tools underlying vision language models. Thus vision language models can be trained as "semantic" world models through a supervised finetuning process on image-action-text data, enabling planning for decision-making while inheriting many of the generalization and robustness properties from the pretrained vision-language models. The paper demonstrates how such a semantic world model can be used for policy improvement on open-ended robotics tasks, leading to significant generalization improvements over typical paradigms of reconstruction-based action-conditional world modeling. Website available at this https URL.</li>
</ul>

<h3>Title: Is This Tracker On? A Benchmark Protocol for Dynamic Tracking</h3>
<ul>
<li><strong>Authors: </strong>Ilona Demler, Saumya Chauhan, Georgia Gkioxari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19819">https://arxiv.org/abs/2510.19819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19819">https://arxiv.org/pdf/2510.19819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19819]] Is This Tracker On? A Benchmark Protocol for Dynamic Tracking(https://arxiv.org/abs/2510.19819)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce ITTO, a challenging new benchmark suite for evaluating and diagnosing the capabilities and limitations of point tracking methods. Our videos are sourced from existing datasets and egocentric real-world recordings, with high-quality human annotations collected through a multi-stage pipeline. ITTO captures the motion complexity, occlusion patterns, and object diversity characteristic of real-world scenes -- factors that are largely absent in current benchmarks. We conduct a rigorous analysis of state-of-the-art tracking methods on ITTO, breaking down performance along key axes of motion complexity. Our findings reveal that existing trackers struggle with these challenges, particularly in re-identifying points after occlusion, highlighting critical failure modes. These results point to the need for new modeling approaches tailored to real-world dynamics. We envision ITTO as a foundation testbed for advancing point tracking and guiding the development of more robust tracking algorithms.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
