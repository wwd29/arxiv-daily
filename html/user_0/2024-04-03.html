<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-03</h1>
<h3>Title: NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Amit Dhurandhar, Tejaswini Pedapati, Ronny Luss, Soham Dan, Aurelie Lozano, Payel Das, Georgios Kollias</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01306">https://arxiv.org/abs/2404.01306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01306">https://arxiv.org/pdf/2404.01306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01306]] NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for  Large Language Models(https://arxiv.org/abs/2404.01306)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based Language Models have become ubiquitous in Natural Language Processing (NLP) due to their impressive performance on various tasks. However, expensive training as well as inference remains a significant impediment to their widespread applicability. While enforcing sparsity at various levels of the model architecture has found promise in addressing scaling and efficiency issues, there remains a disconnect between how sparsity affects network topology. Inspired by brain neuronal networks, we explore sparsity approaches through the lens of network topology. Specifically, we exploit mechanisms seen in biological networks, such as preferential attachment and redundant synapse pruning, and show that principled, model-agnostic sparsity approaches are performant and efficient across diverse NLP tasks, spanning both classification (such as natural language inference) and generation (summarization, machine translation), despite our sole objective not being optimizing performance. NeuroPrune is competitive with (or sometimes superior to) baselines on performance and can be up to $10$x faster in terms of training time for a given level of sparsity, simultaneously exhibiting measurable improvements in inference time in many cases.</li>
</ul>

<h3>Title: Intelligent Learning Rate Distribution to reduce Catastrophic Forgetting  in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Philip Kenneweg, Alexander Schulz, Sarah Schr√∂der, Barbara Hammer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01317">https://arxiv.org/abs/2404.01317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01317">https://arxiv.org/pdf/2404.01317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01317]] Intelligent Learning Rate Distribution to reduce Catastrophic Forgetting  in Transformers(https://arxiv.org/abs/2404.01317)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Pretraining language models on large text corpora is a common practice in natural language processing. Fine-tuning of these models is then performed to achieve the best results on a variety of tasks. In this paper, we investigate the problem of catastrophic forgetting in transformer neural networks and question the common practice of fine-tuning with a flat learning rate for the entire network in this context. We perform a hyperparameter optimization process to find learning rate distributions that are better than a flat learning rate. We combine the learning rate distributions thus found and show that they generalize to better performance with respect to the problem of catastrophic forgetting. We validate these learning rate distributions with a variety of NLP benchmarks from the GLUE dataset.</li>
</ul>

<h3>Title: JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J. Pappas, Florian Tramer, Hamed Hassani, Eric Wong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01318">https://arxiv.org/abs/2404.01318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01318">https://arxiv.org/pdf/2404.01318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01318]] JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large  Language Models(https://arxiv.org/abs/2404.01318)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) a new jailbreaking dataset containing 100 unique behaviors, which we call JBB-Behaviors; (2) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (3) a standardized evaluation framework that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community. Over time, we will expand and adapt the benchmark to reflect technical and methodological advances in the research community.</li>
</ul>

<h3>Title: A Review of Multi-Modal Large Language and Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Kilian Carolan, Laura Fennelly, Alan F. Smeaton</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01322">https://arxiv.org/abs/2404.01322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01322">https://arxiv.org/pdf/2404.01322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01322]] A Review of Multi-Modal Large Language and Vision Models(https://arxiv.org/abs/2404.01322)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently emerged as a focal point of research and application, driven by their unprecedented ability to understand and generate text with human-like quality. Even more recently, LLMs have been extended into multi-modal large language models (MM-LLMs) which extends their capabilities to deal with image, video and audio information, in addition to text. This opens up applications like text-to-video generation, image captioning, text-to-speech, and more and is achieved either by retro-fitting an LLM with multi-modal capabilities, or building a MM-LLM from scratch. This paper provides an extensive review of the current state of those LLMs with multi-modal capabilities as well as the very recent MM-LLMs. It covers the historical development of LLMs especially the advances enabled by transformer-based architectures like OpenAI's GPT series and Google's BERT, as well as the role of attention mechanisms in enhancing model performance. The paper includes coverage of the major and most important of the LLMs and MM-LLMs and also covers the techniques of model tuning, including fine-tuning and prompt engineering, which tailor pre-trained models to specific tasks or domains. Ethical considerations and challenges, such as data bias and model misuse, are also analysed to underscore the importance of responsible AI development and deployment. Finally, we discuss the implications of open-source versus proprietary models in AI research. Through this review, we provide insights into the transformative potential of MM-LLMs in various applications.</li>
</ul>

<h3>Title: Holo-VQVAE: VQ-VAE for phase-only holograms</h3>
<ul>
<li><strong>Authors: </strong>Joohyun Park, Hyeongyeop Kang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01330">https://arxiv.org/abs/2404.01330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01330">https://arxiv.org/pdf/2404.01330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01330]] Holo-VQVAE: VQ-VAE for phase-only holograms(https://arxiv.org/abs/2404.01330)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Holography stands at the forefront of visual technology innovation, offering immersive, three-dimensional visualizations through the manipulation of light wave amplitude and phase. Contemporary research in hologram generation has predominantly focused on image-to-hologram conversion, producing holograms from existing images. These approaches, while effective, inherently limit the scope of innovation and creativity in hologram generation. In response to this limitation, we present Holo-VQVAE, a novel generative framework tailored for phase-only holograms (POHs). Holo-VQVAE leverages the architecture of Vector Quantized Variational AutoEncoders, enabling it to learn the complex distributions of POHs. Furthermore, it integrates the Angular Spectrum Method into the training process, facilitating learning in the image domain. This framework allows for the generation of unseen, diverse holographic content directly from its intricately learned latent space without requiring pre-existing images. This pioneering work paves the way for groundbreaking applications and methodologies in holographic content creation, opening a new era in the exploration of holographic content.</li>
</ul>

<h3>Title: LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact  Language Model</h3>
<ul>
<li><strong>Authors: </strong>Musashi Hinck, Matthew L. Olson, David Cobbley, Shao-Yen Tseng, Vasudev Lal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01331">https://arxiv.org/abs/2404.01331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01331">https://arxiv.org/pdf/2404.01331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01331]] LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact  Language Model(https://arxiv.org/abs/2404.01331)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We train a suite of multimodal foundation models (MMFM) using the popular LLaVA framework with the recently released Gemma family of large language models (LLMs). Of particular interest is the 2B parameter Gemma model, which provides opportunities to construct capable small-scale MMFMs. In line with findings from other papers in this space, we test the effect of ablating three design features: pretraining the connector, utilizing a more powerful image backbone, and increasing the size of the language backbone. The resulting models, which we call LLaVA-Gemma, exhibit moderate performance on an array of evaluations, but fail to improve past the current comparably sized SOTA models. Closer analysis of performance shows mixed effects; skipping pretraining tends to reduce performance, larger vision models sometimes improve performance, and increasing language model size has inconsistent effects. We publicly release training recipes, code and weights for our models for the LLaVA-Gemma models.</li>
</ul>

<h3>Title: Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior  Using Shapley Value</h3>
<ul>
<li><strong>Authors: </strong>Behnam Mohammadi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01332">https://arxiv.org/abs/2404.01332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01332">https://arxiv.org/pdf/2404.01332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01332]] Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior  Using Shapley Value(https://arxiv.org/abs/2404.01332)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The emergence of large language models (LLMs) has opened up exciting possibilities for simulating human behavior and cognitive processes, with potential applications in various domains, including marketing research and consumer behavior analysis. However, the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations. This paper presents a novel approach based on Shapley values from cooperative game theory to interpret LLM behavior and quantify the relative contribution of each prompt component to the model's output. Through two applications-a discrete choice experiment and an investigation of cognitive biases-we demonstrate how the Shapley value method can uncover what we term "token noise" effects, a phenomenon where LLM decisions are disproportionately influenced by tokens providing minimal informative content. This phenomenon raises concerns about the robustness and generalizability of insights obtained from LLMs in the context of human behavior simulation. Our model-agnostic approach extends its utility to proprietary LLMs, providing a valuable tool for marketers and researchers to strategically optimize prompts and mitigate apparent cognitive biases. Our findings underscore the need for a more nuanced understanding of the factors driving LLM responses before relying on them as substitutes for human subjects in research settings. We emphasize the importance of researchers reporting results conditioned on specific prompt templates and exercising caution when drawing parallels between human behavior and LLMs.</li>
</ul>

<h3>Title: Augmenting NER Datasets with LLMs: Towards Automated and Refined  Annotation</h3>
<ul>
<li><strong>Authors: </strong>Yuji Naraki, Ryosuke Yamaki, Yoshikazu Ikeda, Takafumi Horie, Hiroki Naganuma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01334">https://arxiv.org/abs/2404.01334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01334">https://arxiv.org/pdf/2404.01334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01334]] Augmenting NER Datasets with LLMs: Towards Automated and Refined  Annotation(https://arxiv.org/abs/2404.01334)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the field of Natural Language Processing (NLP), Named Entity Recognition (NER) is recognized as a critical technology, employed across a wide array of applications. Traditional methodologies for annotating datasets for NER models are challenged by high costs and variations in dataset quality. This research introduces a novel hybrid annotation approach that synergizes human effort with the capabilities of Large Language Models (LLMs). This approach not only aims to ameliorate the noise inherent in manual annotations, such as omissions, thereby enhancing the performance of NER models, but also achieves this in a cost-effective manner. Additionally, by employing a label mixing strategy, it addresses the issue of class imbalance encountered in LLM-based annotations. Through an analysis across multiple datasets, this method has been consistently shown to provide superior performance compared to traditional annotation methods, even under constrained budget conditions. This study illuminates the potential of leveraging LLMs to improve dataset quality, introduces a novel technique to mitigate class imbalances, and demonstrates the feasibility of achieving high-performance NER in a cost-effective way.</li>
</ul>

<h3>Title: Generative AI for Architectural Design: A Literature Review</h3>
<ul>
<li><strong>Authors: </strong>Chengyuan Li, Tianyu Zhang, Xusheng Du, Ye Zhang, Haoran Xie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01335">https://arxiv.org/abs/2404.01335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01335">https://arxiv.org/pdf/2404.01335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01335]] Generative AI for Architectural Design: A Literature Review(https://arxiv.org/abs/2404.01335)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Artificial Intelligence (AI) has pioneered new methodological paradigms in architectural design, significantly expanding the innovative potential and efficiency of the design process. This paper explores the extensive applications of generative AI technologies in architectural design, a trend that has benefited from the rapid development of deep generative models. This article provides a comprehensive review of the basic principles of generative AI and large-scale models and highlights the applications in the generation of 2D images, videos, and 3D models. In addition, by reviewing the latest literature from 2020, this paper scrutinizes the impact of generative AI technologies at different stages of architectural design, from generating initial architectural 3D forms to producing final architectural imagery. The marked trend of research growth indicates an increasing inclination within the architectural design community towards embracing generative AI, thereby catalyzing a shared enthusiasm for research. These research cases and methodologies have not only proven to enhance efficiency and innovation significantly but have also posed challenges to the conventional boundaries of architectural creativity. Finally, we point out new directions for design innovation and articulate fresh trajectories for applying generative AI in the architectural domain. This article provides the first comprehensive literature review about generative AI for architectural design, and we believe this work can facilitate more research work on this significant topic in architecture.</li>
</ul>

<h3>Title: Automatic detection of relevant information, predictions and forecasts  in financial news through topic modelling with Latent Dirichlet Allocation</h3>
<ul>
<li><strong>Authors: </strong>Silvia Garc√≠a-M√©ndez, Francisco de Arriba-P√©rez, Ana Barros-Vila, Francisco J. Gonz√°lez-Casta√±o, Enrique Costa-Montenegro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CE, cs.IR, cs.LG, q-fin.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01338">https://arxiv.org/abs/2404.01338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01338">https://arxiv.org/pdf/2404.01338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01338]] Automatic detection of relevant information, predictions and forecasts  in financial news through topic modelling with Latent Dirichlet Allocation(https://arxiv.org/abs/2404.01338)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Financial news items are unstructured sources of information that can be mined to extract knowledge for market screening applications. Manual extraction of relevant information from the continuous stream of finance-related news is cumbersome and beyond the skills of many investors, who, at most, can follow a few sources and authors. Accordingly, we focus on the analysis of financial news to identify relevant text and, within that text, forecasts and predictions. We propose a novel Natural Language Processing (NLP) system to assist investors in the detection of relevant financial events in unstructured textual sources by considering both relevance and temporality at the discursive level. Firstly, we segment the text to group together closely related text. Secondly, we apply co-reference resolution to discover internal dependencies within segments. Finally, we perform relevant topic modelling with Latent Dirichlet Allocation (LDA) to separate relevant from less relevant text and then analyse the relevant text using a Machine Learning-oriented temporal approach to identify predictions and speculative statements. We created an experimental data set composed of 2,158 financial news items that were manually labelled by NLP researchers to evaluate our solution. The ROUGE-L values for the identification of relevant text and predictions/forecasts were 0.662 and 0.982, respectively. To our knowledge, this is the first work to jointly consider relevance and temporality at the discursive level. It contributes to the transfer of human associative discourse capabilities to expert systems through the combination of multi-paragraph topic segmentation and co-reference resolution to separate author expression patterns, topic modelling with LDA to detect relevant text, and discursive temporality analysis to identify forecasts and predictions within this text.</li>
</ul>

<h3>Title: From Similarity to Superiority: Channel Clustering for Time Series  Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Jialin Chen, Jan Eric Lenssen, Aosong Feng, Weihua Hu, Matthias Fey, Leandros Tassiulas, Jure Leskovec, Rex Ying</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01340">https://arxiv.org/abs/2404.01340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01340">https://arxiv.org/pdf/2404.01340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01340]] From Similarity to Superiority: Channel Clustering for Time Series  Forecasting(https://arxiv.org/abs/2404.01340)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Time series forecasting has attracted significant attention in recent decades. Previous studies have demonstrated that the Channel-Independent (CI) strategy improves forecasting performance by treating different channels individually, while it leads to poor generalization on unseen instances and ignores potentially necessary interactions between channels. Conversely, the Channel-Dependent (CD) strategy mixes all channels with even irrelevant and indiscriminate information, which, however, results in oversmoothing issues and limits forecasting accuracy. There is a lack of channel strategy that effectively balances individual channel treatment for improved forecasting performance without overlooking essential interactions between channels. Motivated by our observation of a correlation between the time series model's performance boost against channel mixing and the intrinsic similarity on a pair of channels, we developed a novel and adaptable Channel Clustering Module (CCM). CCM dynamically groups channels characterized by intrinsic similarities and leverages cluster identity instead of channel identity, combining the best of CD and CI worlds. Extensive experiments on real-world datasets demonstrate that CCM can (1) boost the performance of CI and CD models by an average margin of 2.4% and 7.2% on long-term and short-term forecasting, respectively; (2) enable zero-shot forecasting with mainstream time series forecasting models; (3) uncover intrinsic time series patterns among channels and improve interpretability of complex time series models.</li>
</ul>

<h3>Title: Block-Diagonal Guided DBSCAN Clustering</h3>
<ul>
<li><strong>Authors: </strong>Zheng Xing, Weibing Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01341">https://arxiv.org/abs/2404.01341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01341">https://arxiv.org/pdf/2404.01341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01341]] Block-Diagonal Guided DBSCAN Clustering(https://arxiv.org/abs/2404.01341)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Cluster analysis plays a crucial role in database mining, and one of the most widely used algorithms in this field is DBSCAN. However, DBSCAN has several limitations, such as difficulty in handling high-dimensional large-scale data, sensitivity to input parameters, and lack of robustness in producing clustering results. This paper introduces an improved version of DBSCAN that leverages the block-diagonal property of the similarity graph to guide the clustering procedure of DBSCAN. The key idea is to construct a graph that measures the similarity between high-dimensional large-scale data points and has the potential to be transformed into a block-diagonal form through an unknown permutation, followed by a cluster-ordering procedure to generate the desired permutation. The clustering structure can be easily determined by identifying the diagonal blocks in the permuted graph. We propose a gradient descent-based method to solve the proposed problem. Additionally, we develop a DBSCAN-based points traversal algorithm that identifies clusters with high densities in the graph and generates an augmented ordering of clusters. The block-diagonal structure of the graph is then achieved through permutation based on the traversal order, providing a flexible foundation for both automatic and interactive cluster analysis. We introduce a split-and-refine algorithm to automatically search for all diagonal blocks in the permuted graph with theoretically optimal guarantees under specific cases. We extensively evaluate our proposed approach on twelve challenging real-world benchmark clustering datasets and demonstrate its superior performance compared to the state-of-the-art clustering method on every dataset.</li>
</ul>

<h3>Title: DiffAgent: Fast and Accurate Text-to-Image API Selection with Large  Language Model</h3>
<ul>
<li><strong>Authors: </strong>Lirui Zhao, Yue Yang, Kaipeng Zhang, Wenqi Shao, Yuxin Zhang, Yu Qiao, Ping Luo, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01342">https://arxiv.org/abs/2404.01342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01342">https://arxiv.org/pdf/2404.01342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01342]] DiffAgent: Fast and Accurate Text-to-Image API Selection with Large  Language Model(https://arxiv.org/abs/2404.01342)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) generative models have attracted significant attention and found extensive applications within and beyond academic research. For example, the Civitai community, a platform for T2I innovation, currently hosts an impressive array of 74,492 distinct models. However, this diversity presents a formidable challenge in selecting the most appropriate model and parameters, a process that typically requires numerous trials. Drawing inspiration from the tool usage research of large language models (LLMs), we introduce DiffAgent, an LLM agent designed to screen the accurate selection in seconds via API calls. DiffAgent leverages a novel two-stage training framework, SFTA, enabling it to accurately align T2I API responses with user input in accordance with human preferences. To train and evaluate DiffAgent's capabilities, we present DABench, a comprehensive dataset encompassing an extensive range of T2I APIs from the community. Our evaluations reveal that DiffAgent not only excels in identifying the appropriate T2I API but also underscores the effectiveness of the SFTA training framework. Codes are available at https://github.com/OpenGVLab/DiffAgent.</li>
</ul>

<h3>Title: CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jingzhe Shi, Jialuo Li, Qinwei Ma, Zaiwen Yang, Huan Ma, Lei Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01343">https://arxiv.org/abs/2404.01343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01343">https://arxiv.org/pdf/2404.01343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01343]] CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs(https://arxiv.org/abs/2404.01343)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Businesses and software platforms are increasingly turning to Large Language Models (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance with file access or as reasoning agents for customer service. However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios. To address these issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile in existing System), designed to: (1) efficiently utilize existing databases or systems for accessing user information or interacting with these systems following existing guidelines; (2) provide accurate and reasonable responses or carry out required operations in the system while avoiding harmful operations; and (3) leverage a combination of small and large LLMs to achieve satisfying performance at a reasonable inference cost. We introduce a practical dataset, the CPHOS-dataset, which includes a database, guiding files, and QA pairs collected from CPHOS, an online platform that facilitates the organization of simulated Physics Olympiads for high school teachers and students. We have conducted extensive experiments to validate the performance of our proposed CHOPS architecture using the CPHOS-dataset, with the aim of demonstrating how LLMs can enhance or serve as alternatives to human customer service. Our code and dataset will be open-sourced soon.</li>
</ul>

<h3>Title: Fairness in Large Language Models: A Taxonomic Survey</h3>
<ul>
<li><strong>Authors: </strong>Zhibo Chu, Zichong Wang, Wenbin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01349">https://arxiv.org/abs/2404.01349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01349">https://arxiv.org/pdf/2404.01349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01349]] Fairness in Large Language Models: A Taxonomic Survey(https://arxiv.org/abs/2404.01349)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations, prompting extensive study in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in traditional machine learning, entails exclusive backgrounds, taxonomies, and fulfillment techniques. To this end, this survey presents a comprehensive overview of recent advances in the existing literature concerning fair LLMs. Specifically, a brief introduction to LLMs is provided, followed by an analysis of factors contributing to bias in LLMs. Additionally, the concept of fairness in LLMs is discussed categorically, summarizing metrics for evaluating bias in LLMs and existing algorithms for promoting fairness. Furthermore, resources for evaluating bias in LLMs, including toolkits and datasets, are summarized. Finally, existing research challenges and open questions are discussed.</li>
</ul>

<h3>Title: Efficiently Distilling LLMs for Edge Applications</h3>
<ul>
<li><strong>Authors: </strong>Achintya Kundu, Fabian Lim, Aaron Chew, Laura Wynter, Penny Chong, Rhui Dih Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01353">https://arxiv.org/abs/2404.01353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01353">https://arxiv.org/pdf/2404.01353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01353]] Efficiently Distilling LLMs for Edge Applications(https://arxiv.org/abs/2404.01353)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Supernet training of LLMs is of great interest in industrial applications as it confers the ability to produce a palette of smaller models at constant cost, regardless of the number of models (of different size / latency) produced. We propose a new method called Multistage Low-rank Fine-tuning of Super-transformers (MLFS) for parameter-efficient supernet training. We show that it is possible to obtain high-quality encoder models that are suitable for commercial edge applications, and that while decoder-only models are resistant to a comparable degree of compression, decoders can be effectively sliced for a significant reduction in training time.</li>
</ul>

<h3>Title: The Double-Edged Sword of Input Perturbations to Robust Accurate  Fairness</h3>
<ul>
<li><strong>Authors: </strong>Xuran Li, Peng Wu, Yanting Chen, Xingjun Ma, Zhen Zhang, Kaixiang Dong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01356">https://arxiv.org/abs/2404.01356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01356">https://arxiv.org/pdf/2404.01356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01356]] The Double-Edged Sword of Input Perturbations to Robust Accurate  Fairness(https://arxiv.org/abs/2404.01356)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, fair</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) are known to be sensitive to adversarial input perturbations, leading to a reduction in either prediction accuracy or individual fairness. To jointly characterize the susceptibility of prediction accuracy and individual fairness to adversarial perturbations, we introduce a novel robustness definition termed robust accurate fairness. Informally, robust accurate fairness requires that predictions for an instance and its similar counterparts consistently align with the ground truth when subjected to input perturbations. We propose an adversarial attack approach dubbed RAFair to expose false or biased adversarial defects in DNN, which either deceive accuracy or compromise individual fairness. Then, we show that such adversarial instances can be effectively addressed by carefully designed benign perturbations, correcting their predictions to be accurate and fair. Our work explores the double-edged sword of input perturbations to robust accurate fairness in DNN and the potential of using benign perturbations to correct adversarial instances.</li>
</ul>

<h3>Title: LLM Attributor: Interactive Visual Attribution for LLM Generation</h3>
<ul>
<li><strong>Authors: </strong>Seongmin Lee, Zijie J. Wang, Aishwarya Chakravarthy, Alec Helbling, ShengYun Peng, Mansi Phute, Duen Horng Chau, Minsuk Kahng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01361">https://arxiv.org/abs/2404.01361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01361">https://arxiv.org/pdf/2404.01361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01361]] LLM Attributor: Interactive Visual Attribution for LLM Generation(https://arxiv.org/abs/2404.01361)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM's text generation. Our library offers a new way to quickly attribute an LLM's text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to LLM Attributor's broad support for computational notebooks, users can easily integrate it into their workflow to interactively visualize attributions of their models. For easier access and extensibility, we open-source LLM Attributor at https://github.com/poloclub/ LLM-Attribution. The video demo is available at https://youtu.be/mIG2MDQKQxM.</li>
</ul>

<h3>Title: Prompt-prompted Mixture of Experts for Efficient LLM Generation</h3>
<ul>
<li><strong>Authors: </strong>Harry Dong, Beidi Chen, Yuejie Chi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01365">https://arxiv.org/abs/2404.01365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01365">https://arxiv.org/pdf/2404.01365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01365]] Prompt-prompted Mixture of Experts for Efficient LLM Generation(https://arxiv.org/abs/2404.01365)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free MoE that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method's simplicity, we show with 50\% of the FF parameters, GRIFFIN maintains the original model's performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.25$\times$ speed-up in Llama 2 13B on an NVIDIA L40). Code will be available at https://github.com/hdong920/GRIFFIN.</li>
</ul>

<h3>Title: Bigger is not Always Better: Scaling Properties of Latent Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Kangfu Mei, Zhengzhong Tu, Mauricio Delbracio, Hossein Talebi, Vishal M. Patel, Peyman Milanfar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01367">https://arxiv.org/abs/2404.01367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01367">https://arxiv.org/pdf/2404.01367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01367]] Bigger is not Always Better: Scaling Properties of Latent Diffusion  Models(https://arxiv.org/abs/2404.01367)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We study the scaling properties of latent diffusion models (LDMs) with an emphasis on their sampling efficiency. While improved network architecture and inference algorithms have shown to effectively boost sampling efficiency of diffusion models, the role of model size -- a critical determinant of sampling efficiency -- has not been thoroughly examined. Through empirical analysis of established text-to-image diffusion models, we conduct an in-depth investigation into how model size influences sampling efficiency across varying sampling steps. Our findings unveil a surprising trend: when operating under a given inference budget, smaller models frequently outperform their larger equivalents in generating high-quality results. Moreover, we extend our study to demonstrate the generalizability of the these findings by applying various diffusion samplers, exploring diverse downstream tasks, evaluating post-distilled models, as well as comparing performance relative to training compute. These findings open up new pathways for the development of LDM scaling strategies which can be employed to enhance generative capabilities within limited inference budgets.</li>
</ul>

<h3>Title: Developing Safe and Responsible Large Language Models -- A Comprehensive  Framework</h3>
<ul>
<li><strong>Authors: </strong>Shaina Raza, Oluwanifemi Bamgbose, Shardul Ghuge, Fatemeh Tavakoli, Deepak John Reji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01399">https://arxiv.org/abs/2404.01399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01399">https://arxiv.org/pdf/2404.01399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01399]] Developing Safe and Responsible Large Language Models -- A Comprehensive  Framework(https://arxiv.org/abs/2404.01399)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Given the growing concerns around the safety and risks of Large Language Models (LLMs), it is essential to develop methods for mitigating these issues. We introduce Safe and Responsible Large Language Model (SR$_{\text{LLM}}$) , a model designed to enhance the safety of language generation using LLMs. Our approach incorporates a comprehensive LLM safety risk taxonomy and utilizes a dataset annotated by experts that align with this taxonomy. SR$_{\text{LLM}}$ is designed to identify potentially unsafe content and produce benign variations. It employs instruction-based and parameter-efficient fine-tuning methods, making the model not only effective in enhancing safety but also resource-efficient and straightforward to adjust. Through our testing on five benchmark datasets and two proprietary datasets, we observed notable reductions in the generation of unsafe content. Moreover, following the implementation of safety measures, there was a significant improvement in the production of safe content. We detail our fine-tuning processes and how we benchmark safety for SR$_{\text{LLM}}$ with the community engagement and promote the responsible advancement of LLMs. All the data and code are available anonymous at https://github.com/shainarazavi/Safe-Responsible-LLM .</li>
</ul>

<h3>Title: OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via  Image-Informed Textual Representation</h3>
<ul>
<li><strong>Authors: </strong>Xiongwei Wu, Sicheng Yu, Ee-Peng Lim, Chong-Wah Ngo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01409">https://arxiv.org/abs/2404.01409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01409">https://arxiv.org/pdf/2404.01409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01409]] OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via  Image-Informed Textual Representation(https://arxiv.org/abs/2404.01409)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In the realm of food computing, segmenting ingredients from images poses substantial challenges due to the large intra-class variance among the same ingredients, the emergence of new ingredients, and the high annotation costs associated with large food segmentation datasets. Existing approaches primarily utilize a closed-vocabulary and static text embeddings setting. These methods often fall short in effectively handling the ingredients, particularly new and diverse ones. In response to these limitations, we introduce OVFoodSeg, a framework that adopts an open-vocabulary setting and enhances text embeddings with visual context. By integrating vision-language models (VLMs), our approach enriches text embedding with image-specific information through two innovative modules, eg, an image-to-text learner FoodLearner and an Image-Informed Text Encoder. The training process of OVFoodSeg is divided into two stages: the pre-training of FoodLearner and the subsequent learning phase for segmentation. The pre-training phase equips FoodLearner with the capability to align visual information with corresponding textual representations that are specifically related to food, while the second phase adapts both the FoodLearner and the Image-Informed Text Encoder for the segmentation task. By addressing the deficiencies of previous models, OVFoodSeg demonstrates a significant improvement, achieving an 4.9\% increase in mean Intersection over Union (mIoU) on the FoodSeg103 dataset, setting a new milestone for food image segmentation.</li>
</ul>

<h3>Title: Is Model Collapse Inevitable? Breaking the Curse of Recursion by  Accumulating Real and Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Matthias Gerstgrasser, Rylan Schaeffer, Apratim Dey, Rafael Rafailov, Henry Sleight, John Hughes, Tomasz Korbak, Rajashree Agrawal, Dhruv Pai, Andrey Gromov, Daniel A. Roberts, Diyi Yang, David L. Donoho, Sanmi Koyejo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.ET, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01413">https://arxiv.org/abs/2404.01413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01413">https://arxiv.org/pdf/2404.01413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01413]] Is Model Collapse Inevitable? Breaking the Curse of Recursion by  Accumulating Real and Synthetic Data(https://arxiv.org/abs/2404.01413)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops discovered that such loops can lead to model collapse, a phenomenon where performance progressively degrades with each model-fitting iteration until the latest model becomes useless. However, several recent papers studying model collapse assumed that new data replace old data over time rather than assuming data accumulate over time. In this paper, we compare these two settings and show that accumulating data prevents model collapse. We begin by studying an analytically tractable setup in which a sequence of linear models are fit to the previous models' predictions. Previous work showed if data are replaced, the test error increases linearly with the number of model-fitting iterations; we extend this result by proving that if data instead accumulate, the test error has a finite upper bound independent of the number of iterations. We next empirically test whether accumulating data similarly prevents model collapse by pretraining sequences of language models on text corpora. We confirm that replacing data does indeed cause model collapse, then demonstrate that accumulating data prevents model collapse; these results hold across a range of model sizes, architectures and hyperparameters. We further show that similar results hold for other deep generative models on real data: diffusion models for molecule generation and variational autoencoders for image generation. Our work provides consistent theoretical and empirical evidence that data accumulation mitigates model collapse.</li>
</ul>

<h3>Title: On the Faithfulness of Vision Transformer Explanations</h3>
<ul>
<li><strong>Authors: </strong>Junyi Wu, Weitai Kang, Hao Tang, Yuan Hong, Yan Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01415">https://arxiv.org/abs/2404.01415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01415">https://arxiv.org/pdf/2404.01415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01415]] On the Faithfulness of Vision Transformer Explanations(https://arxiv.org/abs/2404.01415)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability, transformer</a></li>
<li><strong>Abstract: </strong>To interpret Vision Transformers, post-hoc explanations assign salience scores to input pixels, providing human-understandable heatmaps. However, whether these interpretations reflect true rationales behind the model's output is still underexplored. To address this gap, we study the faithfulness criterion of explanations: the assigned salience scores should represent the influence of the corresponding input pixels on the model's predictions. To evaluate faithfulness, we introduce Salience-guided Faithfulness Coefficient (SaCo), a novel evaluation metric leveraging essential information of salience distribution. Specifically, we conduct pair-wise comparisons among distinct pixel groups and then aggregate the differences in their salience scores, resulting in a coefficient that indicates the explanation's degree of faithfulness. Our explorations reveal that current metrics struggle to differentiate between advanced explanation methods and Random Attribution, thereby failing to capture the faithfulness property. In contrast, our proposed SaCo offers a reliable faithfulness measurement, establishing a robust metric for interpretations. Furthermore, our SaCo demonstrates that the use of gradient and multi-layer aggregation can markedly enhance the faithfulness of attention-based explanation, shedding light on potential paths for advancing Vision Transformer explainability.</li>
</ul>

<h3>Title: DPMesh: Exploiting Diffusion Prior for Occluded Human Mesh Recovery</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Zhu, Ao Li, Yansong Tang, Wenliang Zhao, Jie Zhou, Jiwen Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01424">https://arxiv.org/abs/2404.01424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01424">https://arxiv.org/pdf/2404.01424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01424]] DPMesh: Exploiting Diffusion Prior for Occluded Human Mesh Recovery(https://arxiv.org/abs/2404.01424)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, diffusion</a></li>
<li><strong>Abstract: </strong>The recovery of occluded human meshes presents challenges for current methods due to the difficulty in extracting effective image features under severe occlusion. In this paper, we introduce DPMesh, an innovative framework for occluded human mesh recovery that capitalizes on the profound diffusion prior about object structure and spatial relationships embedded in a pre-trained text-to-image diffusion model. Unlike previous methods reliant on conventional backbones for vanilla feature extraction, DPMesh seamlessly integrates the pre-trained denoising U-Net with potent knowledge as its image backbone and performs a single-step inference to provide occlusion-aware information. To enhance the perception capability for occluded poses, DPMesh incorporates well-designed guidance via condition injection, which produces effective controls from 2D observations for the denoising U-Net. Furthermore, we explore a dedicated noisy key-point reasoning approach to mitigate disturbances arising from occlusion and crowded scenarios. This strategy fully unleashes the perceptual capability of the diffusion prior, thereby enhancing accuracy. Extensive experiments affirm the efficacy of our framework, as we outperform state-of-the-art methods on both occlusion-specific and standard datasets. The persuasive results underscore its ability to achieve precise and robust 3D human mesh recovery, particularly in challenging scenarios involving occlusion and crowded scenes.</li>
</ul>

<h3>Title: Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing  Positional Bias in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zheng Zhang, Fan Yang, Ziyan Jiang, Zheng Chen, Zhengyang Zhao, Chengyuan Ma, Liang Zhao, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01430">https://arxiv.org/abs/2404.01430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01430">https://arxiv.org/pdf/2404.01430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01430]] Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing  Positional Bias in LLMs(https://arxiv.org/abs/2404.01430)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have enhanced their ability to process long input contexts. This development is particularly crucial for tasks that involve retrieving knowledge from an external datastore, which can result in long inputs. However, recent studies show a positional bias in LLMs, demonstrating varying performance depending on the location of useful information within the input sequence. In this study, we conduct extensive experiments to investigate the root causes of positional bias. Our findings indicate that the primary contributor to LLM positional bias stems from the inherent positional preferences of different models. We demonstrate that merely employing prompt-based solutions is inadequate for overcoming the positional preferences. To address this positional bias issue of a pre-trained LLM, we developed a Position-Aware Parameter Efficient Fine-Tuning (PAPEFT) approach which is composed of a data augmentation technique and a parameter efficient adapter, enhancing a uniform attention distribution across the input context. Our experiments demonstrate that the proposed approach effectively reduces positional bias, improving LLMs' effectiveness in handling long context sequences for various tasks that require externally retrieved knowledge.</li>
</ul>

<h3>Title: The Radar Ghost Dataset -- An Evaluation of Ghost Objects in Automotive  Radar Data</h3>
<ul>
<li><strong>Authors: </strong>Florian Kraus, Nicolas Scheiner, Werner Ritter, Klaus Dietmayer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01437">https://arxiv.org/abs/2404.01437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01437">https://arxiv.org/pdf/2404.01437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01437]] The Radar Ghost Dataset -- An Evaluation of Ghost Objects in Automotive  Radar Data(https://arxiv.org/abs/2404.01437)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Radar sensors have a long tradition in advanced driver assistance systems (ADAS) and also play a major role in current concepts for autonomous vehicles. Their importance is reasoned by their high robustness against meteorological effects, such as rain, snow, or fog, and the radar's ability to measure relative radial velocity differences via the Doppler effect. The cause for these advantages, namely the large wavelength, is also one of the drawbacks of radar sensors. Compared to camera or lidar sensor, a lot more surfaces in a typical traffic scenario appear flat relative to the radar's emitted signal. This results in multi-path reflections or so called ghost detections in the radar signal. Ghost objects pose a major source for potential false positive detections in a vehicle's perception pipeline. Therefore, it is important to be able to segregate multi-path reflections from direct ones. In this article, we present a dataset with detailed manual annotations for different kinds of ghost detections. Moreover, two different approaches for identifying these kinds of objects are evaluated. We hope that our dataset encourages more researchers to engage in the fields of multi-path object suppression or exploitation.</li>
</ul>

<h3>Title: Neural Implicit Representation for Building Digital Twins of Unknown  Articulated Objects</h3>
<ul>
<li><strong>Authors: </strong>Yijia Weng, Bowen Wen, Jonathan Tremblay, Valts Blukis, Dieter Fox, Leonidas Guibas, Stan Birchfield</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01440">https://arxiv.org/abs/2404.01440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01440">https://arxiv.org/pdf/2404.01440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01440]] Neural Implicit Representation for Building Digital Twins of Unknown  Articulated Objects(https://arxiv.org/abs/2404.01440)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We address the problem of building digital twins of unknown articulated objects from two RGBD scans of the object at different articulation states. We decompose the problem into two stages, each addressing distinct aspects. Our method first reconstructs object-level shape at each state, then recovers the underlying articulation model including part segmentation and joint articulations that associate the two states. By explicitly modeling point-level correspondences and exploiting cues from images, 3D reconstructions, and kinematics, our method yields more accurate and stable results compared to prior work. It also handles more than one movable part and does not rely on any object shape or structure priors. Project page: https://github.com/NVlabs/DigitalTwinArt</li>
</ul>

<h3>Title: Unveiling Divergent Inductive Biases of LLMs on Temporal Data</h3>
<ul>
<li><strong>Authors: </strong>Sindhu Kishore, Hangfeng He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01453">https://arxiv.org/abs/2404.01453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01453">https://arxiv.org/pdf/2404.01453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01453]] Unveiling Divergent Inductive Biases of LLMs on Temporal Data(https://arxiv.org/abs/2404.01453)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Unraveling the intricate details of events in natural language necessitates a subtle understanding of temporal dynamics. Despite the adeptness of Large Language Models (LLMs) in discerning patterns and relationships from data, their inherent comprehension of temporal dynamics remains a formidable challenge. This research meticulously explores these intrinsic challenges within LLMs, with a specific emphasis on evaluating the performance of GPT-3.5 and GPT-4 models in the analysis of temporal data. Employing two distinct prompt types, namely Question Answering (QA) format and Textual Entailment (TE) format, our analysis probes into both implicit and explicit events. The findings underscore noteworthy trends, revealing disparities in the performance of GPT-3.5 and GPT-4. Notably, biases toward specific temporal relationships come to light, with GPT-3.5 demonstrating a preference for "AFTER'' in the QA format for both implicit and explicit events, while GPT-4 leans towards "BEFORE''. Furthermore, a consistent pattern surfaces wherein GPT-3.5 tends towards "TRUE'', and GPT-4 exhibits a preference for "FALSE'' in the TE format for both implicit and explicit events. This persistent discrepancy between GPT-3.5 and GPT-4 in handling temporal data highlights the intricate nature of inductive bias in LLMs, suggesting that the evolution of these models may not merely mitigate bias but may introduce new layers of complexity.</li>
</ul>

<h3>Title: Will the Real Linda Please Stand up...to Large Language Models?  Examining the Representativeness Heuristic in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Pengda Wang, Zilin Xiao, Hanjie Chen, Frederick L. Oswald</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01461">https://arxiv.org/abs/2404.01461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01461">https://arxiv.org/pdf/2404.01461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01461]] Will the Real Linda Please Stand up...to Large Language Models?  Examining the Representativeness Heuristic in LLMs(https://arxiv.org/abs/2404.01461)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although large language models (LLMs) have demonstrated remarkable proficiency in understanding text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence. This work investigates the impact of the representativeness heuristic on LLM reasoning. We created REHEAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to REHEAT all exhibited representativeness heuristic biases. We further identify that the model's reasoning steps are often incorrectly based on a stereotype rather than the problem's description. Interestingly, the performance improves when adding a hint in the prompt to remind the model of using its knowledge. This suggests the uniqueness of the representativeness heuristic compared to traditional biases. It can occur even when LLMs possess the correct knowledge while failing in a cognitive trap. This highlights the importance of future research focusing on the representativeness heuristic in model reasoning and decision-making and on developing solutions to address it.</li>
</ul>

<h3>Title: OpenChemIE: An Information Extraction Toolkit For Chemistry Literature</h3>
<ul>
<li><strong>Authors: </strong>Vincent Fan, Yujie Qian, Alex Wang, Amber Wang, Connor W. Coley, Regina Barzilay</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01462">https://arxiv.org/abs/2404.01462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01462">https://arxiv.org/pdf/2404.01462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01462]] OpenChemIE: An Information Extraction Toolkit For Chemistry Literature(https://arxiv.org/abs/2404.01462)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Information extraction from chemistry literature is vital for constructing up-to-date reaction databases for data-driven chemistry. Complete extraction requires combining information across text, tables, and figures, whereas prior work has mainly investigated extracting reactions from single modalities. In this paper, we present OpenChemIE to address this complex challenge and enable the extraction of reaction data at the document level. OpenChemIE approaches the problem in two steps: extracting relevant information from individual modalities and then integrating the results to obtain a final list of reactions. For the first step, we employ specialized neural models that each address a specific task for chemistry information extraction, such as parsing molecules or reactions from text or figures. We then integrate the information from these modules using chemistry-informed algorithms, allowing for the extraction of fine-grained reaction data from reaction condition and substrate scope investigations. Our machine learning models attain state-of-the-art performance when evaluated individually, and we meticulously annotate a challenging dataset of reaction schemes with R-groups to evaluate our pipeline as a whole, achieving an F1 score of 69.5%. Additionally, the reaction extraction results of \ours attain an accuracy score of 64.3% when directly compared against the Reaxys chemical database. We provide OpenChemIE freely to the public as an open-source package, as well as through a web interface.</li>
</ul>

<h3>Title: Are large language models superhuman chemists?</h3>
<ul>
<li><strong>Authors: </strong>Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Benedict Emoekabu, Aswanth Krishnan, Mara Wilhelmi, Macjonathan Okereke, Juliane Eberhardt, Amir Mohammad Elahi, Maximilian Greiner, Caroline T. Holick, Tanya Gupta, Mehrdad Asgari, Christina Glaubitz, Lea C. Klepsch, Yannik K√∂ster, Jakob Meyer, Santiago Miret, Tim Hoffmann, Fabian Alexander Kreth, Michael Ringleb, Nicole Roesner, Ulrich S. Schubert, Leanne M. Stafast, Dinga Wonanke, Michael Pieler, Philippe Schwaller, Kevin Maik Jablonka</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.AI, physics.chem-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01475">https://arxiv.org/abs/2404.01475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01475">https://arxiv.org/pdf/2404.01475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01475]] Are large language models superhuman chemists?(https://arxiv.org/abs/2404.01475)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have gained widespread interest due to their ability to process human language and perform tasks on which they have not been explicitly trained. This is relevant for the chemical sciences, which face the problem of small and diverse datasets that are frequently in the form of text. LLMs have shown promise in addressing these issues and are increasingly being harnessed to predict chemical properties, optimize reactions, and even design and conduct experiments autonomously. However, we still have only a very limited systematic understanding of the chemical reasoning capabilities of LLMs, which would be required to improve models and mitigate potential harms. Here, we introduce "ChemBench," an automated framework designed to rigorously evaluate the chemical knowledge and reasoning abilities of state-of-the-art LLMs against the expertise of human chemists. We curated more than 7,000 question-answer pairs for a wide array of subfields of the chemical sciences, evaluated leading open and closed-source LLMs, and found that the best models outperformed the best human chemists in our study on average. The models, however, struggle with some chemical reasoning tasks that are easy for human experts and provide overconfident, misleading predictions, such as about chemicals' safety profiles. These findings underscore the dual reality that, although LLMs demonstrate remarkable proficiency in chemical tasks, further research is critical to enhancing their safety and utility in chemical sciences. Our findings also indicate a need for adaptations to chemistry curricula and highlight the importance of continuing to develop evaluation frameworks to improve safe and useful LLMs.</li>
</ul>

<h3>Title: A Study on Scaling Up Multilingual News Framing Analysis</h3>
<ul>
<li><strong>Authors: </strong>Syeda Sabrina Akter, Antonios Anastasopoulos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01481">https://arxiv.org/abs/2404.01481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01481">https://arxiv.org/pdf/2404.01481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01481]] A Study on Scaling Up Multilingual News Framing Analysis(https://arxiv.org/abs/2404.01481)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Media framing is the study of strategically selecting and presenting specific aspects of political issues to shape public opinion. Despite its relevance to almost all societies around the world, research has been limited due to the lack of available datasets and other resources. This study explores the possibility of dataset creation through crowdsourcing, utilizing non-expert annotators to develop training corpora. We first extend framing analysis beyond English news to a multilingual context (12 typologically diverse languages) through automatic translation. We also present a novel benchmark in Bengali and Portuguese on the immigration and same-sex marriage domains. Additionally, we show that a system trained on our crowd-sourced dataset, combined with other existing ones, leads to a 5.32 percentage point increase from the baseline, showing that crowdsourcing is a viable option. Last, we study the performance of large language models (LLMs) for this task, finding that task-specific fine-tuning is a better approach than employing bigger non-specialized models.</li>
</ul>

<h3>Title: Explainable AI Integrated Feature Engineering for Wildfire Prediction</h3>
<ul>
<li><strong>Authors: </strong>Di Fan, Ayan Biswas, James Paul Ahrens</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01487">https://arxiv.org/abs/2404.01487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01487">https://arxiv.org/pdf/2404.01487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01487]] Explainable AI Integrated Feature Engineering for Wildfire Prediction(https://arxiv.org/abs/2404.01487)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Wildfires present intricate challenges for prediction, necessitating the use of sophisticated machine learning techniques for effective modeling\cite{jain2020review}. In our research, we conducted a thorough assessment of various machine learning algorithms for both classification and regression tasks relevant to predicting wildfires. We found that for classifying different types or stages of wildfires, the XGBoost model outperformed others in terms of accuracy and robustness. Meanwhile, the Random Forest regression model showed superior results in predicting the extent of wildfire-affected areas, excelling in both prediction error and explained variance. Additionally, we developed a hybrid neural network model that integrates numerical data and image information for simultaneous classification and regression. To gain deeper insights into the decision-making processes of these models and identify key contributing features, we utilized eXplainable Artificial Intelligence (XAI) techniques, including TreeSHAP, LIME, Partial Dependence Plots (PDP), and Gradient-weighted Class Activation Mapping (Grad-CAM). These interpretability tools shed light on the significance and interplay of various features, highlighting the complex factors influencing wildfire predictions. Our study not only demonstrates the effectiveness of specific machine learning models in wildfire-related tasks but also underscores the critical role of model transparency and interpretability in environmental science applications.</li>
</ul>

<h3>Title: SUGAR: Pre-training 3D Visual Representations for Robotics</h3>
<ul>
<li><strong>Authors: </strong>Shizhe Chen, Ricardo Garcia, Ivan Laptev, Cordelia Schmid</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01491">https://arxiv.org/abs/2404.01491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01491">https://arxiv.org/pdf/2404.01491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01491]] SUGAR: Pre-training 3D Visual Representations for Robotics(https://arxiv.org/abs/2404.01491)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Learning generalizable visual representations from Internet data has yielded promising results for robotics. Yet, prevailing approaches focus on pre-training 2D representations, being sub-optimal to deal with occlusions and accurately localize objects in complex 3D scenes. Meanwhile, 3D representation learning has been limited to single-object understanding. To address these limitations, we introduce a novel 3D pre-training framework for robotics named SUGAR that captures semantic, geometric and affordance properties of objects through 3D point clouds. We underscore the importance of cluttered scenes in 3D representation learning, and automatically construct a multi-object dataset benefiting from cost-free supervision in simulation. SUGAR employs a versatile transformer-based model to jointly address five pre-training tasks, namely cross-modal knowledge distillation for semantic learning, masked point modeling to understand geometry structures, grasping pose synthesis for object affordance, 3D instance segmentation and referring expression grounding to analyze cluttered scenes. We evaluate our learned representation on three robotic-related tasks, namely, zero-shot 3D object recognition, referring expression grounding, and language-driven robotic manipulation. Experimental results show that SUGAR's 3D representation outperforms state-of-the-art 2D and 3D representations.</li>
</ul>

<h3>Title: Can Biases in ImageNet Models Explain Generalization?</h3>
<ul>
<li><strong>Authors: </strong>Paul Gavrikov, Janis Keuper</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01509">https://arxiv.org/abs/2404.01509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01509">https://arxiv.org/pdf/2404.01509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01509]] Can Biases in ImageNet Models Explain Generalization?(https://arxiv.org/abs/2404.01509)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>The robust generalization of models to rare, in-distribution (ID) samples drawn from the long tail of the training distribution and to out-of-training-distribution (OOD) samples is one of the major challenges of current deep learning methods. For image classification, this manifests in the existence of adversarial attacks, the performance drops on distorted images, and a lack of generalization to concepts such as sketches. The current understanding of generalization in neural networks is very limited, but some biases that differentiate models from human vision have been identified and might be causing these limitations. Consequently, several attempts with varying success have been made to reduce these biases during training to improve generalization. We take a step back and sanity-check these attempts. Fixing the architecture to the well-established ResNet-50, we perform a large-scale study on 48 ImageNet models obtained via different training methods to understand how and if these biases - including shape bias, spectral biases, and critical bands - interact with generalization. Our extensive study results reveal that contrary to previous findings, these biases are insufficient to accurately predict the generalization of a model holistically. We provide access to all checkpoints and evaluation code at https://github.com/paulgavrikov/biases_vs_generalization</li>
</ul>

<h3>Title: Addressing Heterogeneity in Federated Load Forecasting with  Personalization Layers</h3>
<ul>
<li><strong>Authors: </strong>Shourya Bose, Yu Zhang, Kibaek Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01517">https://arxiv.org/abs/2404.01517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01517">https://arxiv.org/pdf/2404.01517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01517]] Addressing Heterogeneity in Federated Load Forecasting with  Personalization Layers(https://arxiv.org/abs/2404.01517)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>The advent of smart meters has enabled pervasive collection of energy consumption data for training short-term load forecasting models. In response to privacy concerns, federated learning (FL) has been proposed as a privacy-preserving approach for training, but the quality of trained models degrades as client data becomes heterogeneous. In this paper we propose the use of personalization layers for load forecasting in a general framework called PL-FL. We show that PL-FL outperforms FL and purely local training, while requiring lower communication bandwidth than FL. This is done through extensive simulations on three different datasets from the NREL ComStock repository.</li>
</ul>

<h3>Title: Temporally Consistent Unbalanced Optimal Transport for Unsupervised  Action Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ming Xu, Stephen Gould</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01518">https://arxiv.org/abs/2404.01518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01518">https://arxiv.org/pdf/2404.01518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01518]] Temporally Consistent Unbalanced Optimal Transport for Unsupervised  Action Segmentation(https://arxiv.org/abs/2404.01518)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We propose a novel approach to the action segmentation task for long, untrimmed videos, based on solving an optimal transport problem. By encoding a temporal consistency prior into a Gromov-Wasserstein problem, we are able to decode a temporally consistent segmentation from a noisy affinity/matching cost matrix between video frames and action classes. Unlike previous approaches, our method does not require knowing the action order for a video to attain temporal consistency. Furthermore, our resulting (fused) Gromov-Wasserstein problem can be efficiently solved on GPUs using a few iterations of projected mirror descent. We demonstrate the effectiveness of our method in an unsupervised learning setting, where our method is used to generate pseudo-labels for self-training. We evaluate our segmentation approach and unsupervised learning pipeline on the Breakfast, 50-Salads, YouTube Instructions and Desktop Assembly datasets, yielding state-of-the-art results for the unsupervised video action segmentation task.</li>
</ul>

<h3>Title: Set-Aligning Framework for Auto-Regressive Event Temporal Graph  Generation</h3>
<ul>
<li><strong>Authors: </strong>Xingwei Tan, Yuxiang Zhou, Gabriele Pergola, Yulan He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01532">https://arxiv.org/abs/2404.01532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01532">https://arxiv.org/pdf/2404.01532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01532]] Set-Aligning Framework for Auto-Regressive Event Temporal Graph  Generation(https://arxiv.org/abs/2404.01532)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Event temporal graphs have been shown as convenient and effective representations of complex temporal relations between events in text. Recent studies, which employ pre-trained language models to auto-regressively generate linearised graphs for constructing event temporal graphs, have shown promising results. However, these methods have often led to suboptimal graph generation as the linearised graphs exhibit set characteristics which are instead treated sequentially by language models. This discrepancy stems from the conventional text generation objectives, leading to erroneous penalisation of correct predictions caused by the misalignment of elements in target sequences. To address these challenges, we reframe the task as a conditional set generation problem, proposing a Set-aligning Framework tailored for the effective utilisation of Large Language Models (LLMs). The framework incorporates data augmentations and set-property regularisations designed to alleviate text generation loss penalties associated with the linearised graph edge sequences, thus encouraging the generation of more relation edges. Experimental results show that our framework surpasses existing baselines for event temporal graph generation. Furthermore, under zero-shot settings, the structural knowledge introduced through our framework notably improves model generalisation, particularly when the training examples available are limited.</li>
</ul>

<h3>Title: Bidirectional Multi-Scale Implicit Neural Representations for Image  Deraining</h3>
<ul>
<li><strong>Authors: </strong>Xiang Chen, Jinshan Pan, Jiangxin Dong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01547">https://arxiv.org/abs/2404.01547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01547">https://arxiv.org/pdf/2404.01547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01547]] Bidirectional Multi-Scale Implicit Neural Representations for Image  Deraining(https://arxiv.org/abs/2404.01547)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>How to effectively explore multi-scale representations of rain streaks is important for image deraining. In contrast to existing Transformer-based methods that depend mostly on single-scale rain appearance, we develop an end-to-end multi-scale Transformer that leverages the potentially useful features in various scales to facilitate high-quality image reconstruction. To better explore the common degradation representations from spatially-varying rain streaks, we incorporate intra-scale implicit neural representations based on pixel coordinates with the degraded inputs in a closed-loop design, enabling the learned features to facilitate rain removal and improve the robustness of the model in complex scenarios. To ensure richer collaborative representation from different scales, we embed a simple yet effective inter-scale bidirectional feedback operation into our multi-scale Transformer by performing coarse-to-fine and fine-to-coarse information communication. Extensive experiments demonstrate that our approach, named as NeRD-Rain, performs favorably against the state-of-the-art ones on both synthetic and real-world benchmark datasets. The source code and trained models are available at https://github.com/cschenxiang/NeRD-Rain.</li>
</ul>

<h3>Title: Octopus: On-device language model for function calling of software APIs</h3>
<ul>
<li><strong>Authors: </strong>Wei Chen, Zhiyuan Li, Mingyuan Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01549">https://arxiv.org/abs/2404.01549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01549">https://arxiv.org/pdf/2404.01549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01549]] Octopus: On-device language model for function calling of software APIs(https://arxiv.org/abs/2404.01549)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving domain of artificial intelligence, Large Language Models (LLMs) play a crucial role due to their advanced text processing and generation abilities. This study introduces a new strategy aimed at harnessing on-device LLMs in invoking software APIs. We meticulously compile a dataset derived from software API documentation and apply fine-tuning to LLMs with capacities of 2B, 3B and 7B parameters, specifically to enhance their proficiency in software API interactions. Our approach concentrates on refining the models' grasp of API structures and syntax, significantly enhancing the accuracy of API function calls. Additionally, we propose \textit{conditional masking} techniques to ensure outputs in the desired formats and reduce error rates while maintaining inference speeds. We also propose a novel benchmark designed to evaluate the effectiveness of LLMs in API interactions, establishing a foundation for subsequent research. Octopus, the fine-tuned model, is proved to have better performance than GPT-4 for the software APIs calling. This research aims to advance automated software development and API integration, representing substantial progress in aligning LLM capabilities with the demands of practical software engineering applications.</li>
</ul>

<h3>Title: A Linear Time and Space Local Point Cloud Geometry Encoder via  Vectorized Kernel Mixture (VecKM)</h3>
<ul>
<li><strong>Authors: </strong>Dehao Yuan, Cornelia Ferm√ºller, Tahseen Rabbani, Furong Huang, Yiannis Aloimonos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01568">https://arxiv.org/abs/2404.01568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01568">https://arxiv.org/pdf/2404.01568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01568]] A Linear Time and Space Local Point Cloud Geometry Encoder via  Vectorized Kernel Mixture (VecKM)(https://arxiv.org/abs/2404.01568)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>We propose VecKM, a novel local point cloud geometry encoder that is descriptive, efficient and robust to noise. VecKM leverages a unique approach by vectorizing a kernel mixture to represent the local point clouds. Such representation is descriptive and robust to noise, which is supported by two theorems that confirm its ability to reconstruct and preserve the similarity of the local shape. Moreover, VecKM is the first successful attempt to reduce the computation and memory costs from $O(n^2+nKd)$ to $O(nd)$ by sacrificing a marginal constant factor, where $n$ is the size of the point cloud and $K$ is neighborhood size. The efficiency is primarily due to VecKM's unique factorizable property that eliminates the need of explicitly grouping points into neighborhoods. In the normal estimation task, VecKM demonstrates not only 100x faster inference speed but also strongest descriptiveness and robustness compared with existing popular encoders. In classification and segmentation tasks, integrating VecKM as a preprocessing module achieves consistently better performance than the PointNet, PointNet++, and point transformer baselines, and runs consistently faster by up to 10x.</li>
</ul>

<h3>Title: Evaluating Large Language Models Using Contrast Sets: An Experimental  Approach</h3>
<ul>
<li><strong>Authors: </strong>Manish Sanwal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01569">https://arxiv.org/abs/2404.01569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01569">https://arxiv.org/pdf/2404.01569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01569]] Evaluating Large Language Models Using Contrast Sets: An Experimental  Approach(https://arxiv.org/abs/2404.01569)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the domain of Natural Language Inference (NLI), especially in tasks involving the classification of multiple input texts, the Cross-Entropy Loss metric is widely employed as a standard for error measurement. However, this metric falls short in effectively evaluating a model's capacity to understand language entailments. In this study, we introduce an innovative technique for generating a contrast set for the Stanford Natural Language Inference (SNLI) dataset. Our strategy involves the automated substitution of verbs, adverbs, and adjectives with their synonyms to preserve the original meaning of sentences. This method aims to assess whether a model's performance is based on genuine language comprehension or simply on pattern recognition. We conducted our analysis using the ELECTRA-small model. The model achieved an accuracy of 89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5% on our contrast set, indicating a substantial 17% decline. This outcome led us to conduct a detailed examination of the model's learning behaviors. Following this, we improved the model's resilience by fine-tuning it with a contrast-enhanced training dataset specifically designed for SNLI, which increased its accuracy to 85.5% on the contrast sets. Our findings highlight the importance of incorporating diverse linguistic expressions into datasets for NLI tasks. We hope that our research will encourage the creation of more inclusive datasets, thereby contributing to the development of NLI models that are both more sophisticated and effective.</li>
</ul>

<h3>Title: Leveraging Digital Perceptual Technologies for Remote Perception and  Analysis of Human Biomechanical Processes: A Contactless Approach for  Workload and Joint Force Assessment</h3>
<ul>
<li><strong>Authors: </strong>Jesudara Omidokun, Darlington Egeonu, Bochen Jia, Liang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01576">https://arxiv.org/abs/2404.01576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01576">https://arxiv.org/pdf/2404.01576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01576]] Leveraging Digital Perceptual Technologies for Remote Perception and  Analysis of Human Biomechanical Processes: A Contactless Approach for  Workload and Joint Force Assessment(https://arxiv.org/abs/2404.01576)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This study presents an innovative computer vision framework designed to analyze human movements in industrial settings, aiming to enhance biomechanical analysis by integrating seamlessly with existing software. Through a combination of advanced imaging and modeling techniques, the framework allows for comprehensive scrutiny of human motion, providing valuable insights into kinematic patterns and kinetic data. Utilizing Convolutional Neural Networks (CNNs), Direct Linear Transform (DLT), and Long Short-Term Memory (LSTM) networks, the methodology accurately detects key body points, reconstructs 3D landmarks, and generates detailed 3D body meshes. Extensive evaluations across various movements validate the framework's effectiveness, demonstrating comparable results to traditional marker-based models with minor differences in joint angle estimations and precise estimations of weight and height. Statistical analyses consistently support the framework's reliability, with joint angle estimations showing less than a 5-degree difference for hip flexion, elbow flexion, and knee angle methods. Additionally, weight estimation exhibits an average error of less than 6 % for weight and less than 2 % for height when compared to ground-truth values from 10 subjects. The integration of the Biomech-57 landmark skeleton template further enhances the robustness and reinforces the framework's credibility. This framework shows significant promise for meticulous biomechanical analysis in industrial contexts, eliminating the need for cumbersome markers and extending its utility to diverse research domains, including the study of specific exoskeleton devices' impact on facilitating the prompt return of injured workers to their tasks.</li>
</ul>

<h3>Title: Diffusion Deepfake</h3>
<ul>
<li><strong>Authors: </strong>Chaitali Bhattacharyya, Hanxiao Wang, Feng Zhang, Sungho Kim, Xiatian Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01579">https://arxiv.org/abs/2404.01579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01579">https://arxiv.org/pdf/2404.01579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01579]] Diffusion Deepfake(https://arxiv.org/abs/2404.01579)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent progress in generative AI, primarily through diffusion models, presents significant challenges for real-world deepfake detection. The increased realism in image details, diverse content, and widespread accessibility to the general public complicates the identification of these sophisticated deepfakes. Acknowledging the urgency to address the vulnerability of current deepfake detectors to this evolving threat, our paper introduces two extensive deepfake datasets generated by state-of-the-art diffusion models as other datasets are less diverse and low in quality. Our extensive experiments also showed that our dataset is more challenging compared to the other face deepfake datasets. Our strategic dataset creation not only challenge the deepfake detectors but also sets a new benchmark for more evaluation. Our comprehensive evaluation reveals the struggle of existing detection methods, often optimized for specific image domains and manipulations, to effectively adapt to the intricate nature of diffusion deepfakes, limiting their practical utility. To address this critical issue, we investigate the impact of enhancing training data diversity on representative detection methods. This involves expanding the diversity of both manipulation techniques and image domains. Our findings underscore that increasing training data diversity results in improved generalizability. Moreover, we propose a novel momentum difficulty boosting strategy to tackle the additional challenge posed by training data heterogeneity. This strategy dynamically assigns appropriate sample weights based on learning difficulty, enhancing the model's adaptability to both easy and challenging samples. Extensive experiments on both existing and newly proposed benchmarks demonstrate that our model optimization approach surpasses prior alternatives significantly.</li>
</ul>

<h3>Title: Hallucination Diversity-Aware Active Learning for Text Summarization</h3>
<ul>
<li><strong>Authors: </strong>Yu Xia, Xu Liu, Tong Yu, Sungchul Kim, Ryan A. Rossi, Anup Rao, Tung Mai, Shuai Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01588">https://arxiv.org/abs/2404.01588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01588">https://arxiv.org/pdf/2404.01588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01588]] Hallucination Diversity-Aware Active Learning for Text Summarization(https://arxiv.org/abs/2404.01588)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown propensity to generate hallucinated outputs, i.e., texts that are factually incorrect or unsupported. Existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in LLM outputs. Moreover, most of these methods focus on a specific type of hallucination, e.g., entity or token errors, which limits their effectiveness in addressing various types of hallucinations exhibited in LLM outputs. To our best knowledge, in this paper we propose the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed. By measuring fine-grained hallucinations from errors in semantic frame, discourse and content verifiability in text summarization, we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning. Extensive experiments on three datasets and different backbone models demonstrate advantages of our method in effectively and efficiently mitigating LLM hallucinations.</li>
</ul>

<h3>Title: Classifying Cancer Stage with Open-Source Clinical Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chia-Hsuan Chang, Mary M. Lucas, Grace Lu-Yao, Christopher C. Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01589">https://arxiv.org/abs/2404.01589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01589">https://arxiv.org/pdf/2404.01589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01589]] Classifying Cancer Stage with Open-Source Clinical Large Language Models(https://arxiv.org/abs/2404.01589)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Cancer stage classification is important for making treatment and care management plans for oncology patients. Information on staging is often included in unstructured form in clinical, pathology, radiology and other free-text reports in the electronic health record system, requiring extensive work to parse and obtain. To facilitate the extraction of this information, previous NLP approaches rely on labeled training datasets, which are labor-intensive to prepare. In this study, we demonstrate that without any labeled training data, open-source clinical large language models (LLMs) can extract pathologic tumor-node-metastasis (pTNM) staging information from real-world pathology reports. Our experiments compare LLMs and a BERT-based model fine-tuned using the labeled data. Our findings suggest that while LLMs still exhibit subpar performance in Tumor (T) classification, with the appropriate adoption of prompting strategies, they can achieve comparable performance on Metastasis (M) classification and improved performance on Node (N) classification.</li>
</ul>

<h3>Title: Language Model Guided Interpretable Video Action Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Ning Wang, Guangming Zhu, HS Li, Liang Zhang, Syed Afaq Ali Shah, Mohammed Bennamoun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01591">https://arxiv.org/abs/2404.01591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01591">https://arxiv.org/pdf/2404.01591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01591]] Language Model Guided Interpretable Video Action Reasoning(https://arxiv.org/abs/2404.01591)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>While neural networks have excelled in video action recognition tasks, their black-box nature often obscures the understanding of their decision-making processes. Recent approaches used inherently interpretable models to analyze video actions in a manner akin to human reasoning. These models, however, usually fall short in performance compared to their black-box counterparts. In this work, we present a new framework named Language-guided Interpretable Action Recognition framework (LaIAR). LaIAR leverages knowledge from language models to enhance both the recognition capabilities and the interpretability of video models. In essence, we redefine the problem of understanding video model decisions as a task of aligning video and language models. Using the logical reasoning captured by the language model, we steer the training of the video model. This integrated approach not only improves the video model's adaptability to different domains but also boosts its overall performance. Extensive experiments on two complex video action datasets, Charades & CAD-120, validates the improved performance and interpretability of our LaIAR framework. The code of LaIAR is available at https://github.com/NingWang2049/LaIAR.</li>
</ul>

<h3>Title: What Can Transformer Learn with Varying Depth? Case Studies on Sequence  Learning Tasks</h3>
<ul>
<li><strong>Authors: </strong>Xingwu Chen, Difan Zou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01601">https://arxiv.org/abs/2404.01601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01601">https://arxiv.org/pdf/2404.01601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01601]] What Can Transformer Learn with Varying Depth? Case Studies on Sequence  Learning Tasks(https://arxiv.org/abs/2404.01601)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We study the capabilities of the transformer architecture with varying depth. Specifically, we designed a novel set of sequence learning tasks to systematically evaluate and comprehend how the depth of transformer affects its ability to perform memorization, reasoning, generalization, and contextual generalization. We show a transformer with only one attention layer can excel in memorization but falls short in other tasks. Then, we show that exhibiting reasoning and generalization ability requires the transformer to have at least two attention layers, while context generalization ability may necessitate three attention layers. Additionally, we identify a class of simple operations that a single attention layer can execute, and show that the complex tasks can be approached as the combinations of these simple operations and thus can be resolved by stacking multiple attention layers. This sheds light on studying more practical and complex tasks beyond our design. Numerical experiments corroborate our theoretical findings.</li>
</ul>

<h3>Title: Helmsman of the Masses? Evaluate the Opinion Leadership of Large  Language Models in the Werewolf Game</h3>
<ul>
<li><strong>Authors: </strong>Silin Du, Xiaowei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01602">https://arxiv.org/abs/2404.01602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01602">https://arxiv.org/pdf/2404.01602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01602]] Helmsman of the Masses? Evaluate the Opinion Leadership of Large  Language Models in the Werewolf Game(https://arxiv.org/abs/2404.01602)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have exhibited memorable strategic behaviors in social deductive games. However, the significance of opinion leadership exhibited by LLM-based agents has been overlooked, which is crucial for practical applications in multi-agent and human-AI interaction settings. Opinion leaders are individuals who have a noticeable impact on the beliefs and behaviors of others within a social group. In this work, we employ the Werewolf game as a simulation platform to assess the opinion leadership of LLMs. The game features the role of the Sheriff, tasked with summarizing arguments and recommending decision options, and therefore serves as a credible proxy for an opinion leader. We develop a framework integrating the Sheriff role and devise two novel metrics for evaluation based on the critical characteristics of opinion leaders. The first metric measures the reliability of the opinion leader, and the second assesses the influence of the opinion leader on other players' decisions. We conduct extensive experiments to evaluate LLMs of different scales. In addition, we collect a Werewolf question-answering dataset (WWQA) to assess and enhance LLM's grasp of the game rules, and we also incorporate human participants for further analysis. The results suggest that the Werewolf game is a suitable test bed to evaluate the opinion leadership of LLMs and few LLMs possess the capacity for opinion leadership.</li>
</ul>

<h3>Title: Haina Storage: A Decentralized Secure Storage Framework Based on  Improved Blockchain Structure</h3>
<ul>
<li><strong>Authors: </strong>Zijian Zhou, Caimei Wang, Xiaoheng Deng, Jianhao Lu, Qilue Wen, Chen Zhang, Hong Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01606">https://arxiv.org/abs/2404.01606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01606">https://arxiv.org/pdf/2404.01606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01606]] Haina Storage: A Decentralized Secure Storage Framework Based on  Improved Blockchain Structure(https://arxiv.org/abs/2404.01606)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, fair</a></li>
<li><strong>Abstract: </strong>Although the decentralized storage technology based on the blockchain can effectively realize secure data storage on cloud services. However, there are still some problems in the existing schemes, such as low storage capacity and low efficiency. To address related issues, we propose a novel decentralized storage framework, which mainly includes four aspects: (1) we proposed a Bi-direction Circular Linked Chain Structure (BCLCS), which improves data's storage capacity and applicability in decentralized storage. (2) A Proof of Resources (PoR) decision model is proposed. By introducing the network environment as an essential evaluation parameter of storage right decision, the energy and time consumption of decision-making are reduced, and the fairness of decision-making is improved. (3) A chain structure dynamic locking mechanism (CSDLM) is designed to realize anti-traverse and access control. (4) A Bi-directional data Access Mechanism (BDAM) is proposed, which improves the efficiency of data access and acquisition in decentralized storage mode. The experimental results show that the framework has significantly improved the shortcomings of the current decentralized storage.</li>
</ul>

<h3>Title: Audio Simulation for Sound Source Localization in Virtual Evironment</h3>
<ul>
<li><strong>Authors: </strong>Yi Di Yuan, Swee Liang Wong, Jonathan Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01611">https://arxiv.org/abs/2404.01611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01611">https://arxiv.org/pdf/2404.01611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01611]] Audio Simulation for Sound Source Localization in Virtual Evironment(https://arxiv.org/abs/2404.01611)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Non-line-of-sight localization in signal-deprived environments is a challenging yet pertinent problem. Acoustic methods in such predominantly indoor scenarios encounter difficulty due to the reverberant nature. In this study, we aim to locate sound sources to specific locations within a virtual environment by leveraging physically grounded sound propagation simulations and machine learning methods. This process attempts to overcome the issue of data insufficiency to localize sound sources to their location of occurrence especially in post-event localization. We achieve 0.786+/- 0.0136 F1-score using an audio transformer spectrogram approach.</li>
</ul>

<h3>Title: Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo</h3>
<ul>
<li><strong>Authors: </strong>Zongrui Li, Zhan Lu, Haojie Yan, Boxin Shi, Gang Pan, Qian Zheng, Xudong Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01612">https://arxiv.org/abs/2404.01612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01612">https://arxiv.org/pdf/2404.01612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01612]] Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo(https://arxiv.org/abs/2404.01612)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Natural Light Uncalibrated Photometric Stereo (NaUPS) relieves the strict environment and light assumptions in classical Uncalibrated Photometric Stereo (UPS) methods. However, due to the intrinsic ill-posedness and high-dimensional ambiguities, addressing NaUPS is still an open question. Existing works impose strong assumptions on the environment lights and objects' material, restricting the effectiveness in more general scenarios. Alternatively, some methods leverage supervised learning with intricate models while lacking interpretability, resulting in a biased estimation. In this work, we proposed Spin Light Uncalibrated Photometric Stereo (Spin-UP), an unsupervised method to tackle NaUPS in various environment lights and objects. The proposed method uses a novel setup that captures the object's images on a rotatable platform, which mitigates NaUPS's ill-posedness by reducing unknowns and provides reliable priors to alleviate NaUPS's ambiguities. Leveraging neural inverse rendering and the proposed training strategies, Spin-UP recovers surface normals, environment light, and isotropic reflectance under complex natural light with low computational cost. Experiments have shown that Spin-UP outperforms other supervised / unsupervised NaUPS methods and achieves state-of-the-art performance on synthetic and real-world datasets. Codes and data are available at https://github.com/LMozart/CVPR2024-SpinUP.</li>
</ul>

<h3>Title: LR-FPN: Enhancing Remote Sensing Object Detection with Location Refined  Feature Pyramid Network</h3>
<ul>
<li><strong>Authors: </strong>Hanqian Li, Ruinan Zhang, Ye Pan, Junchi Ren, Fei Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01614">https://arxiv.org/abs/2404.01614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01614">https://arxiv.org/pdf/2404.01614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01614]] LR-FPN: Enhancing Remote Sensing Object Detection with Location Refined  Feature Pyramid Network(https://arxiv.org/abs/2404.01614)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Remote sensing target detection aims to identify and locate critical targets within remote sensing images, finding extensive applications in agriculture and urban planning. Feature pyramid networks (FPNs) are commonly used to extract multi-scale features. However, existing FPNs often overlook extracting low-level positional information and fine-grained context interaction. To address this, we propose a novel location refined feature pyramid network (LR-FPN) to enhance the extraction of shallow positional information and facilitate fine-grained context interaction. The LR-FPN consists of two primary modules: the shallow position information extraction module (SPIEM) and the contextual interaction module (CIM). Specifically, SPIEM first maximizes the retention of solid location information of the target by simultaneously extracting positional and saliency information from the low-level feature map. Subsequently, CIM injects this robust location information into different layers of the original FPN through spatial and channel interaction, explicitly enhancing the object area. Moreover, in spatial interaction, we introduce a simple local and non-local interaction strategy to learn and retain the saliency information of the object. Lastly, the LR-FPN can be readily integrated into common object detection frameworks to improve performance significantly. Extensive experiments on two large-scale remote sensing datasets (i.e., DOTAV1.0 and HRSC2016) demonstrate that the proposed LR-FPN is superior to state-of-the-art object detection approaches. Our code and models will be publicly available.</li>
</ul>

<h3>Title: Transforming LLMs into Cross-modal and Cross-lingual RetrievalSystems</h3>
<ul>
<li><strong>Authors: </strong>Frank Palma Gomez, Ramon Sanabria, Yun-hsuan Sung, Daniel Cer, Siddharth Dalmia, Gustavo Hernandez Abrego</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01616">https://arxiv.org/abs/2404.01616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01616">https://arxiv.org/pdf/2404.01616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01616]] Transforming LLMs into Cross-modal and Cross-lingual RetrievalSystems(https://arxiv.org/abs/2404.01616)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn't require speech data during LLM pre-training and can exploit LLM's multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these languages. Additionally, our model demonstrates cross-lingual speech and text matching, which is further enhanced by readily available machine translation data.</li>
</ul>

<h3>Title: Making Privacy-preserving Federated Graph Analytics with Strong  Guarantees Practical (for Certain Queries)</h3>
<ul>
<li><strong>Authors: </strong>Kunlong Liu, Trinabh Gupta</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01619">https://arxiv.org/abs/2404.01619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01619">https://arxiv.org/pdf/2404.01619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01619]] Making Privacy-preserving Federated Graph Analytics with Strong  Guarantees Practical (for Certain Queries)(https://arxiv.org/abs/2404.01619)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate</a></li>
<li><strong>Abstract: </strong>Privacy-preserving federated graph analytics is an emerging area of research. The goal is to run graph analytics queries over a set of devices that are organized as a graph while keeping the raw data on the devices rather than centralizing it. Further, no entity may learn any new information except for the final query result. For instance, a device may not learn a neighbor's data. The state-of-the-art prior work for this problem provides privacy guarantees for a broad set of queries in a strong threat model where the devices can be malicious. However, it imposes an impractical overhead: each device locally requires over 8.79 hours of cpu time and 5.73 GiBs of network transfers per query. This paper presents Colo, a new, low-cost system for privacy-preserving federated graph analytics that requires minutes of cpu time and a few MiBs in network transfers, for a particular subset of queries. At the heart of Colo is a new secure computation protocol that enables a device to securely and efficiently evaluate a graph query in its local neighborhood while hiding device data, edge data, and topology data. An implementation and evaluation of Colo shows that for running a variety of COVID-19 queries over a population of 1M devices, it requires less than 8.4 minutes of a device's CPU time and 4.93 MiBs in network transfers - improvements of up to three orders of magnitude.</li>
</ul>

<h3>Title: AAA: an Adaptive Mechanism for Locally Differential Private Mean  Estimation</h3>
<ul>
<li><strong>Authors: </strong>Fei Wei, Ergute Bao, Xiaokui Xiao, Yin Yang, Bolin Ding</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01625">https://arxiv.org/abs/2404.01625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01625">https://arxiv.org/pdf/2404.01625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01625]] AAA: an Adaptive Mechanism for Locally Differential Private Mean  Estimation(https://arxiv.org/abs/2404.01625)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Local differential privacy (LDP) is a strong privacy standard that has been adopted by popular software systems. The main idea is that each individual perturbs their own data locally, and only submits the resulting noisy version to a data aggregator. Although much effort has been devoted to computing various types of aggregates and building machine learning applications under LDP, research on fundamental perturbation mechanisms has not achieved significant improvement in recent years. Towards a more refined result utility, existing works mainly focus on improving the worst-case guarantee. However, this approach does not necessarily promise a better average performance given the fact that the data in practice obey a certain distribution, which is not known beforehand. In this paper, we propose the advanced adaptive additive (AAA) mechanism, which is a distribution-aware approach that addresses the average utility and tackles the classical mean estimation problem. AAA is carried out in a two-step approach: first, as the global data distribution is not available beforehand, the data aggregator selects a random subset of individuals to compute a (noisy) quantized data descriptor; then, the data aggregator collects data from the remaining individuals, which are perturbed in a distribution-aware fashion. The perturbation involved in the latter step is obtained by solving an optimization problem, which is formulated with the data descriptor obtained in the former step and the desired properties of task-determined utilities. We provide rigorous privacy proofs, utility analyses, and extensive experiments comparing AAA with state-of-the-art mechanisms. The evaluation results demonstrate that the AAA mechanism consistently outperforms existing solutions with a clear margin in terms of result utility, on a wide range of privacy constraints and real-world and synthetic datasets.</li>
</ul>

<h3>Title: Entity Disambiguation via Fusion Entity Decoding</h3>
<ul>
<li><strong>Authors: </strong>Junxiong Wang, Ali Mousavi, Omar Attia, Saloni Potdar, Alexander M. Rush, Umar Farooq Minhas, Yunyao Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01626">https://arxiv.org/abs/2404.01626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01626">https://arxiv.org/pdf/2404.01626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01626]] Entity Disambiguation via Fusion Entity Decoding(https://arxiv.org/abs/2404.01626)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Entity disambiguation (ED), which links the mentions of ambiguous entities to their referent entities in a knowledge base, serves as a core component in entity linking (EL). Existing generative approaches demonstrate improved accuracy compared to classification approaches under the standardized ZELDA benchmark. Nevertheless, generative approaches suffer from the need for large-scale pre-training and inefficient generation. Most importantly, entity descriptions, which could contain crucial information to distinguish similar entities from each other, are often overlooked. We propose an encoder-decoder model to disambiguate entities with more detailed entity descriptions. Given text and candidate entities, the encoder learns interactions between the text and each candidate entity, producing representations for each entity candidate. The decoder then fuses the representations of entity candidates together and selects the correct entity. Our experiments, conducted on various entity disambiguation benchmarks, demonstrate the strong and robust performance of this model, particularly +1.5% in the ZELDA benchmark compared with GENRE. Furthermore, we integrate this approach into the retrieval/reader framework and observe +1.5% improvements in end-to-end entity linking in the GERBIL benchmark compared with EntQA.</li>
</ul>

<h3>Title: Enhancing Functional Safety in Automotive AMS Circuits through  Unsupervised Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Ayush Arunachalam, Ian Kintz, Suvadeep Banerjee, Arnab Raha, Xiankun Jin, Fei Su, Viswanathan Pillai Prasanth, Rubin A. Parekhji, Suriyaprakash Natarajan, Kanad Basu</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01632">https://arxiv.org/abs/2404.01632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01632">https://arxiv.org/pdf/2404.01632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01632]] Enhancing Functional Safety in Automotive AMS Circuits through  Unsupervised Machine Learning(https://arxiv.org/abs/2404.01632)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Given the widespread use of safety-critical applications in the automotive field, it is crucial to ensure the Functional Safety (FuSa) of circuits and components within automotive systems. The Analog and Mixed-Signal (AMS) circuits prevalent in these systems are more vulnerable to faults induced by parametric perturbations, noise, environmental stress, and other factors, in comparison to their digital counterparts. However, their continuous signal characteristics present an opportunity for early anomaly detection, enabling the implementation of safety mechanisms to prevent system failure. To address this need, we propose a novel framework based on unsupervised machine learning for early anomaly detection in AMS circuits. The proposed approach involves injecting anomalies at various circuit locations and individual components to create a diverse and comprehensive anomaly dataset, followed by the extraction of features from the observed circuit signals. Subsequently, we employ clustering algorithms to facilitate anomaly detection. Finally, we propose a time series framework to enhance and expedite anomaly detection performance. Our approach encompasses a systematic analysis of anomaly abstraction at multiple levels pertaining to the automotive domain, from hardware- to block-level, where anomalies are injected to create diverse fault scenarios. By monitoring the system behavior under these anomalous conditions, we capture the propagation of anomalies and their effects at different abstraction levels, thereby potentially paving the way for the implementation of reliable safety mechanisms to ensure the FuSa of automotive SoCs. Our experimental findings indicate that our approach achieves 100% anomaly detection accuracy and significantly optimizes the associated latency by 5X, underscoring the effectiveness of our devised solution.</li>
</ul>

<h3>Title: Learning to Control Camera Exposure via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Kyunghyun Lee, Ukcheol Shin, Byeong-Uk Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01636">https://arxiv.org/abs/2404.01636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01636">https://arxiv.org/pdf/2404.01636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01636]] Learning to Control Camera Exposure via Reinforcement Learning(https://arxiv.org/abs/2404.01636)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Adjusting camera exposure in arbitrary lighting conditions is the first step to ensure the functionality of computer vision applications. Poorly adjusted camera exposure often leads to critical failure and performance degradation. Traditional camera exposure control methods require multiple convergence steps and time-consuming processes, making them unsuitable for dynamic lighting conditions. In this paper, we propose a new camera exposure control framework that rapidly controls camera exposure while performing real-time processing by exploiting deep reinforcement learning. The proposed framework consists of four contributions: 1) a simplified training ground to simulate real-world's diverse and dynamic lighting changes, 2) flickering and image attribute-aware reward design, along with lightweight state design for real-time processing, 3) a static-to-dynamic lighting curriculum to gradually improve the agent's exposure-adjusting capability, and 4) domain randomization techniques to alleviate the limitation of the training ground and achieve seamless generalization in the wild.As a result, our proposed method rapidly reaches a desired exposure level within five steps with real-time processing (1 ms). Also, the acquired images are well-exposed and show superiority in various computer vision tasks, such as feature extraction and object detection.</li>
</ul>

<h3>Title: ADVREPAIR:Provable Repair of Adversarial Attack</h3>
<ul>
<li><strong>Authors: </strong>Zhiming Chi, Jianan Ma, Pengfei Yang, Cheng-Chao Huang, Renjue Li, Xiaowei Huang, Lijun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01642">https://arxiv.org/abs/2404.01642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01642">https://arxiv.org/pdf/2404.01642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01642]] ADVREPAIR:Provable Repair of Adversarial Attack(https://arxiv.org/abs/2404.01642)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) are increasingly deployed in safety-critical domains, but their vulnerability to adversarial attacks poses serious safety risks. Existing neuron-level methods using limited data lack efficacy in fixing adversaries due to the inherent complexity of adversarial attack mechanisms, while adversarial training, leveraging a large number of adversarial samples to enhance robustness, lacks provability. In this paper, we propose ADVREPAIR, a novel approach for provable repair of adversarial attacks using limited data. By utilizing formal verification, ADVREPAIR constructs patch modules that, when integrated with the original network, deliver provable and specialized repairs within the robustness neighborhood. Additionally, our approach incorporates a heuristic mechanism for assigning patch modules, allowing this defense against adversarial attacks to generalize to other inputs. ADVREPAIR demonstrates superior efficiency, scalability and repair success rate. Different from existing DNN repair methods, our repair can generalize to general inputs, thereby improving the robustness of the neural network globally, which indicates a significant breakthrough in the generalization capability of ADVREPAIR.</li>
</ul>

<h3>Title: ContrastCAD: Contrastive Learning-based Representation Learning for  Computer-Aided Design Models</h3>
<ul>
<li><strong>Authors: </strong>Minseop Jung, Minseong Kim, Jibum Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01645">https://arxiv.org/abs/2404.01645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01645">https://arxiv.org/pdf/2404.01645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01645]] ContrastCAD: Contrastive Learning-based Representation Learning for  Computer-Aided Design Models(https://arxiv.org/abs/2404.01645)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>The success of Transformer-based models has encouraged many researchers to learn CAD models using sequence-based approaches. However, learning CAD models is still a challenge, because they can be represented as complex shapes with long construction sequences. Furthermore, the same CAD model can be expressed using different CAD construction sequences. We propose a novel contrastive learning-based approach, named ContrastCAD, that effectively captures semantic information within the construction sequences of the CAD model. ContrastCAD generates augmented views using dropout techniques without altering the shape of the CAD model. We also propose a new CAD data augmentation method, called a Random Replace and Extrude (RRE) method, to enhance the learning performance of the model when training an imbalanced training CAD dataset. Experimental results show that the proposed RRE augmentation method significantly enhances the learning performance of Transformer-based autoencoders, even for complex CAD models having very long construction sequences. The proposed ContrastCAD model is shown to be robust to permutation changes of construction sequences and performs better representation learning by generating representation spaces where similar CAD models are more closely clustered. Our codes are available at https://github.com/cm8908/ContrastCAD.</li>
</ul>

<h3>Title: Transformer meets wcDTW to improve real-time battery bids: A new  approach to scenario selection</h3>
<ul>
<li><strong>Authors: </strong>Sujal Bhavsar, Vera Zaychik Moffitt, Justin Appleby</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01646">https://arxiv.org/abs/2404.01646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01646">https://arxiv.org/pdf/2404.01646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01646]] Transformer meets wcDTW to improve real-time battery bids: A new  approach to scenario selection(https://arxiv.org/abs/2404.01646)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Stochastic battery bidding in real-time energy markets is a nuanced process, with its efficacy depending on the accuracy of forecasts and the representative scenarios chosen for optimization. In this paper, we introduce a pioneering methodology that amalgamates Transformer-based forecasting with weighted constrained Dynamic Time Warping (wcDTW) to refine scenario selection. Our approach harnesses the predictive capabilities of Transformers to foresee Energy prices, while wcDTW ensures the selection of pertinent historical scenarios by maintaining the coherence between multiple uncertain products. Through extensive simulations in the PJM market for July 2023, our method exhibited a 10% increase in revenue compared to the conventional method, highlighting its potential to revolutionize battery bidding strategies in real-time markets.</li>
</ul>

<h3>Title: AI WALKUP: A Computer-Vision Approach to Quantifying MDS-UPDRS in  Parkinson's Disease</h3>
<ul>
<li><strong>Authors: </strong>Xiang Xiang, Zihan Zhang, Jing Ma, Yao Deng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01654">https://arxiv.org/abs/2404.01654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01654">https://arxiv.org/pdf/2404.01654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01654]] AI WALKUP: A Computer-Vision Approach to Quantifying MDS-UPDRS in  Parkinson's Disease(https://arxiv.org/abs/2404.01654)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Parkinson's Disease (PD) is the second most common neurodegenerative disorder. The existing assessment method for PD is usually the Movement Disorder Society - Unified Parkinson's Disease Rating Scale (MDS-UPDRS) to assess the severity of various types of motor symptoms and disease progression. However, manual assessment suffers from high subjectivity, lack of consistency, and high cost and low efficiency of manual communication. We want to use a computer vision based solution to capture human pose images based on a camera, reconstruct and perform motion analysis using algorithms, and extract the features of the amount of motion through feature engineering. The proposed approach can be deployed on different smartphones, and the video recording and artificial intelligence analysis can be done quickly and easily through our APP.</li>
</ul>

<h3>Title: FashionEngine: Interactive Generation and Editing of 3D Clothed Humans</h3>
<ul>
<li><strong>Authors: </strong>Tao Hu, Fangzhou Hong, Zhaoxi Chen, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01655">https://arxiv.org/abs/2404.01655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01655">https://arxiv.org/pdf/2404.01655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01655]] FashionEngine: Interactive Generation and Editing of 3D Clothed Humans(https://arxiv.org/abs/2404.01655)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present FashionEngine, an interactive 3D human generation and editing system that allows us to design 3D digital humans in a way that aligns with how humans interact with the world, such as natural languages, visual perceptions, and hand-drawing. FashionEngine automates the 3D human production with three key components: 1) A pre-trained 3D human diffusion model that learns to model 3D humans in a semantic UV latent space from 2D image training data, which provides strong priors for diverse generation and editing tasks. 2) Multimodality-UV Space encoding the texture appearance, shape topology, and textual semantics of human clothing in a canonical UV-aligned space, which faithfully aligns the user multimodal inputs with the implicit UV latent space for controllable 3D human editing. The multimodality-UV space is shared across different user inputs, such as texts, images, and sketches, which enables various joint multimodal editing tasks. 3) Multimodality-UV Aligned Sampler learns to sample high-quality and diverse 3D humans from the diffusion prior for multimodal user inputs. Extensive experiments validate FashionEngine's state-of-the-art performance for conditional generation/editing tasks. In addition, we present an interactive user interface for our FashionEngine that enables both conditional and unconditional generation tasks, and editing tasks including pose/view/shape control, text-, image-, and sketch-driven 3D human editing and 3D virtual try-on, in a unified framework. Our project page is at: https://taohuumd.github.io/projects/FashionEngine.</li>
</ul>

<h3>Title: Supporting Mitosis Detection AI Training with Inter-Observer Eye-Gaze  Consistencies</h3>
<ul>
<li><strong>Authors: </strong>Hongyan Gu, Zihan Yan, Ayesha Alvi, Brandon Day, Chunxu Yang, Zida Wu, Shino Magaki, Mohammad Haeri, Xiang 'Anthony' Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01656">https://arxiv.org/abs/2404.01656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01656">https://arxiv.org/pdf/2404.01656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01656]] Supporting Mitosis Detection AI Training with Inter-Observer Eye-Gaze  Consistencies(https://arxiv.org/abs/2404.01656)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The expansion of artificial intelligence (AI) in pathology tasks has intensified the demand for doctors' annotations in AI development. However, collecting high-quality annotations from doctors is costly and time-consuming, creating a bottleneck in AI progress. This study investigates eye-tracking as a cost-effective technology to collect doctors' behavioral data for AI training with a focus on the pathology task of mitosis detection. One major challenge in using eye-gaze data is the low signal-to-noise ratio, which hinders the extraction of meaningful information. We tackled this by levering the properties of inter-observer eye-gaze consistencies and creating eye-gaze labels from consistent eye-fixations shared by a group of observers. Our study involved 14 non-medical participants, from whom we collected eye-gaze data and generated eye-gaze labels based on varying group sizes. We assessed the efficacy of such eye-gaze labels by training Convolutional Neural Networks (CNNs) and comparing their performance to those trained with ground truth annotations and a heuristic-based baseline. Results indicated that CNNs trained with our eye-gaze labels closely followed the performance of ground-truth-based CNNs, and significantly outperformed the baseline. Although primarily focused on mitosis, we envision that insights from this study can be generalized to other medical imaging tasks.</li>
</ul>

<h3>Title: Release of Pre-Trained Models for the Japanese Language</h3>
<ul>
<li><strong>Authors: </strong>Kei Sawada, Tianyu Zhao, Makoto Shing, Kentaro Mitsui, Akio Kaga, Yukiya Hono, Toshiaki Wakatsuki, Koh Mitsuda</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01657">https://arxiv.org/abs/2404.01657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01657">https://arxiv.org/pdf/2404.01657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01657]] Release of Pre-Trained Models for the Japanese Language(https://arxiv.org/abs/2404.01657)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>AI democratization aims to create a world in which the average person can utilize AI techniques. To achieve this goal, numerous research institutes have attempted to make their results accessible to the public. In particular, large pre-trained models trained on large-scale data have shown unprecedented potential, and their release has had a significant impact. However, most of the released models specialize in the English language, and thus, AI democratization in non-English-speaking communities is lagging significantly. To reduce this gap in AI access, we released Generative Pre-trained Transformer (GPT), Contrastive Language and Image Pre-training (CLIP), Stable Diffusion, and Hidden-unit Bidirectional Encoder Representations from Transformers (HuBERT) pre-trained in Japanese. By providing these models, users can freely interface with AI that aligns with Japanese cultural values and ensures the identity of Japanese culture, thus enhancing the democratization of AI. Additionally, experiments showed that pre-trained models specialized for Japanese can efficiently achieve high performance in Japanese tasks.</li>
</ul>

<h3>Title: CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuechen Liang, Meiling Tao, Tianyu Shi, Yiting Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01663">https://arxiv.org/abs/2404.01663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01663">https://arxiv.org/pdf/2404.01663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01663]] CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small  Language Models(https://arxiv.org/abs/2404.01663)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset. We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback. This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory. In this research, we propose a new communication agent framework that integrates multi-agent systems with environmental feedback mechanisms, offering a scalable method to explore cooperative behaviors. Notably, our TinyAgent-7B model exhibits performance on par with GPT-3.5, despite having fewer parameters, signifying a substantial improvement in the efficiency and effectiveness of LLMs.</li>
</ul>

<h3>Title: METAL: Towards Multilingual Meta-Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Rishav Hada, Varun Gumma, Mohamed Ahmed, Kalika Bali, Sunayana Sitaram</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01667">https://arxiv.org/abs/2404.01667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01667">https://arxiv.org/pdf/2404.01667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01667]] METAL: Towards Multilingual Meta-Evaluation(https://arxiv.org/abs/2404.01667)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rising human-like precision of Large Language Models (LLMs) in numerous tasks, their utilization in a variety of real-world applications is becoming more prevalent. Several studies have shown that LLMs excel on many standard NLP benchmarks. However, it is challenging to evaluate LLMs due to test dataset contamination and the limitations of traditional metrics. Since human evaluations are difficult to collect, there is a growing interest in the community to use LLMs themselves as reference-free evaluators for subjective metrics. However, past work has shown that LLM-based evaluators can exhibit bias and have poor alignment with human judgments. In this study, we propose a framework for an end-to-end assessment of LLMs as evaluators in multilingual scenarios. We create a carefully curated dataset, covering 10 languages containing native speaker judgments for the task of summarization. This dataset is created specifically to evaluate LLM-based evaluators, which we refer to as meta-evaluation (METAL). We compare the performance of LLM-based evaluators created using GPT-3.5-Turbo, GPT-4, and PaLM2. Our results indicate that LLM-based evaluators based on GPT-4 perform the best across languages, while GPT-3.5-Turbo performs poorly. Additionally, we perform an analysis of the reasoning provided by LLM-based evaluators and find that it often does not match the reasoning provided by human judges.</li>
</ul>

<h3>Title: Incentives in Private Collaborative Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Rachael Hwee Ling Sim, Yehong Zhang, Trong Nghia Hoang, Xinyi Xu, Bryan Kian Hsiang Low, Patrick Jaillet</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01676">https://arxiv.org/abs/2404.01676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01676">https://arxiv.org/pdf/2404.01676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01676]] Incentives in Private Collaborative Machine Learning(https://arxiv.org/abs/2404.01676)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, fair</a></li>
<li><strong>Abstract: </strong>Collaborative machine learning involves training models on data from multiple parties but must incentivize their participation. Existing data valuation methods fairly value and reward each party based on shared data or model parameters but neglect the privacy risks involved. To address this, we introduce differential privacy (DP) as an incentive. Each party can select its required DP guarantee and perturb its sufficient statistic (SS) accordingly. The mediator values the perturbed SS by the Bayesian surprise it elicits about the model parameters. As our valuation function enforces a privacy-valuation trade-off, parties are deterred from selecting excessive DP guarantees that reduce the utility of the grand coalition's model. Finally, the mediator rewards each party with different posterior samples of the model parameters. Such rewards still satisfy existing incentives like fairness but additionally preserve DP and a high similarity to the grand coalition's posterior. We empirically demonstrate the effectiveness and practicality of our approach on synthetic and real-world datasets.</li>
</ul>

<h3>Title: JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic  Dataset in Crowded Human Environments</h3>
<ul>
<li><strong>Authors: </strong>Duy-Tho Le, Chenhui Gou, Stavya Datta, Hengcan Shi, Ian Reid, Jianfei Cai, Hamid Rezatofighi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01686">https://arxiv.org/abs/2404.01686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01686">https://arxiv.org/pdf/2404.01686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01686]] JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic  Dataset in Crowded Human Environments(https://arxiv.org/abs/2404.01686)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Autonomous robot systems have attracted increasing research attention in recent years, where environment understanding is a crucial step for robot navigation, human-robot interaction, and decision. Real-world robot systems usually collect visual data from multiple sensors and are required to recognize numerous objects and their movements in complex human-crowded settings. Traditional benchmarks, with their reliance on single sensors and limited object classes and scenarios, fail to provide the comprehensive environmental understanding robots need for accurate navigation, interaction, and decision-making. As an extension of JRDB dataset, we unveil JRDB-PanoTrack, a novel open-world panoptic segmentation and tracking benchmark, towards more comprehensive environmental perception. JRDB-PanoTrack includes (1) various data involving indoor and outdoor crowded scenes, as well as comprehensive 2D and 3D synchronized data modalities; (2) high-quality 2D spatial panoptic segmentation and temporal tracking annotations, with additional 3D label projections for further spatial understanding; (3) diverse object classes for closed- and open-world recognition benchmarks, with OSPA-based metrics for evaluation. Extensive evaluation of leading methods shows significant challenges posed by our dataset.</li>
</ul>

<h3>Title: A Lightweight Security Solution for Mitigation of Hatchetman Attack in  RPL-based 6LoWPAN</h3>
<ul>
<li><strong>Authors: </strong>Girish Sharma, Jyoti Grover, Abhishek Verma</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01689">https://arxiv.org/abs/2404.01689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01689">https://arxiv.org/pdf/2404.01689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01689]] A Lightweight Security Solution for Mitigation of Hatchetman Attack in  RPL-based 6LoWPAN(https://arxiv.org/abs/2404.01689)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>In recent times, the Internet of Things (IoT) has a significant rise in industries, and we live in the era of Industry 4.0, where each device is connected to the Internet from small to big. These devices are Artificial Intelligence (AI) enabled and are capable of perspective analytics. By 2023, it's anticipated that over 14 billion smart devices will be available on the Internet. These applications operate in a wireless environment where memory, power, and other resource limitations apply to the nodes. In addition, the conventional routing method is ineffective in networks with limited resource devices, lossy links, and slow data rates. Routing Protocol for Low Power and Lossy Networks (RPL), a new routing protocol for such networks, was proposed by the IETF's ROLL group. RPL operates in two modes: Storing and Non-Storing. In Storing mode, each node have the information to reach to other node. In Non-Storing mode, the routing information lies with the root node only. The attacker may exploit the Non-Storing feature of the RPL. When the root node transmits User Datagram Protocol~(UDP) or control message packet to the child nodes, the routing information is stored in the extended header of the IPv6 packet. The attacker may modify the address from the source routing header which leads to Denial of Service (DoS) attack. This attack is RPL specific which is known as Hatchetman attack. This paper shows significant degradation in terms of network performance when an attacker exploits this feature. We also propose a lightweight mitigation of Hatchetman attack using game theoretic approach to detect the Hatchetman attack in IoT.</li>
</ul>

<h3>Title: Beyond Image Super-Resolution for Image Recognition with Task-Driven  Perceptual Loss</h3>
<ul>
<li><strong>Authors: </strong>Jaeha Kim, Junghun Oh, Kyoung Mu Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01692">https://arxiv.org/abs/2404.01692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01692">https://arxiv.org/pdf/2404.01692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01692]] Beyond Image Super-Resolution for Image Recognition with Task-Driven  Perceptual Loss(https://arxiv.org/abs/2404.01692)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In real-world scenarios, image recognition tasks, such as semantic segmentation and object detection, often pose greater challenges due to the lack of information available within low-resolution (LR) content. Image super-resolution (SR) is one of the promising solutions for addressing the challenges. However, due to the ill-posed property of SR, it is challenging for typical SR methods to restore task-relevant high-frequency contents, which may dilute the advantage of utilizing the SR method. Therefore, in this paper, we propose Super-Resolution for Image Recognition (SR4IR) that effectively guides the generation of SR images beneficial to achieving satisfactory image recognition performance when processing LR images. The critical component of our SR4IR is the task-driven perceptual (TDP) loss that enables the SR network to acquire task-specific knowledge from a network tailored for a specific task. Moreover, we propose a cross-quality patch mix and an alternate training framework that significantly enhances the efficacy of the TDP loss by addressing potential problems when employing the TDP loss. Through extensive experiments, we demonstrate that our SR4IR achieves outstanding task performance by generating SR images useful for a specific image recognition task, including semantic segmentation, object detection, and image classification. The implementation code is available at https://github.com/JaehaKim97/SR4IR.</li>
</ul>

<h3>Title: MotionChain: Conversational Motion Controllers via Multimodal Prompts</h3>
<ul>
<li><strong>Authors: </strong>Biao Jiang, Xin Chen, Chi Zhang, Fukun Yin, Zhuoyuan Li, Gang YU, Jiayuan Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01700">https://arxiv.org/abs/2404.01700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01700">https://arxiv.org/pdf/2404.01700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01700]] MotionChain: Conversational Motion Controllers via Multimodal Prompts(https://arxiv.org/abs/2404.01700)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in language models have demonstrated their adeptness in conducting multi-turn dialogues and retaining conversational context. However, this proficiency remains largely unexplored in other multimodal generative models, particularly in human motion models. By integrating multi-turn conversations in controlling continuous virtual human movements, generative human motion models can achieve an intuitive and step-by-step process of human task execution for humanoid robotics, game agents, or other embodied systems. In this work, we present MotionChain, a conversational human motion controller to generate continuous and long-term human motion through multimodal prompts. Specifically, MotionChain consists of multi-modal tokenizers that transform various data types such as text, image, and motion, into discrete tokens, coupled with a Vision-Motion-aware Language model. By leveraging large-scale language, vision-language, and vision-motion data to assist motion-related generation tasks, MotionChain thus comprehends each instruction in multi-turn conversation and generates human motions followed by these prompts. Extensive experiments validate the efficacy of MotionChain, demonstrating state-of-the-art performance in conversational motion generation, as well as more intuitive manners of controlling and interacting with virtual humans.</li>
</ul>

<h3>Title: On the Role of Summary Content Units in Text Summarization Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Marcel Nawrath, Agnieszka Nowak, Tristan Ratz, Danilo C. Walenta, Juri Opitz, Leonardo F. R. Ribeiro, Jo√£o Sedoc, Daniel Deutsch, Simon Mille, Yixin Liu, Lining Zhang, Sebastian Gehrmann, Saad Mahamood, Miruna Clinciu, Khyathi Chandu, Yufang Hou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01701">https://arxiv.org/abs/2404.01701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01701">https://arxiv.org/pdf/2404.01701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01701]] On the Role of Summary Content Units in Text Summarization Evaluation(https://arxiv.org/abs/2404.01701)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>At the heart of the Pyramid evaluation method for text summarization lie human written summary content units (SCUs). These SCUs are concise sentences that decompose a summary into small facts. Such SCUs can be used to judge the quality of a candidate summary, possibly partially automated via natural language inference (NLI) systems. Interestingly, with the aim to fully automate the Pyramid evaluation, Zhang and Bansal (2021) show that SCUs can be approximated by automatically generated semantic role triplets (STUs). However, several questions currently lack answers, in particular: i) Are there other ways of approximating SCUs that can offer advantages? ii) Under which conditions are SCUs (or their approximations) offering the most value? In this work, we examine two novel strategies to approximate SCUs: generating SCU approximations from AMR meaning representations (SMUs) and from large language models (SGUs), respectively. We find that while STUs and SMUs are competitive, the best approximation quality is achieved by SGUs. We also show through a simple sentence-decomposition baseline (SSUs) that SCUs (and their approximations) offer the most value when ranking short summaries, but may not help as much when ranking systems or longer summaries.</li>
</ul>

<h3>Title: Samba: Semantic Segmentation of Remotely Sensed Images with State Space  Model</h3>
<ul>
<li><strong>Authors: </strong>Qinfeng Zhu, Yuanzhi Cai, Yuan Fang, Yihan Yang, Cheng Chen, Lei Fan, Anh Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01705">https://arxiv.org/abs/2404.01705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01705">https://arxiv.org/pdf/2404.01705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01705]] Samba: Semantic Segmentation of Remotely Sensed Images with State Space  Model(https://arxiv.org/abs/2404.01705)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>High-resolution remotely sensed images poses a challenge for commonly used semantic segmentation methods such as Convolutional Neural Network (CNN) and Vision Transformer (ViT). CNN-based methods struggle with handling such high-resolution images due to their limited receptive field, while ViT faces challenges to handle long sequences. Inspired by Mamba, which adopts a State Space Model (SSM) to efficiently capture global semantic information, we propose a semantic segmentation framework for high-resolution remotely sensed images, named Samba. Samba utilizes an encoder-decoder architecture, with Samba blocks serving as the encoder for efficient multi-level semantic information extraction, and UperNet functioning as the decoder. We evaluate Samba on the LoveDA dataset, comparing its performance against top-performing CNN and ViT methods. The results reveal that Samba achieved unparalleled performance on LoveDA. This represents that the proposed Samba is an effective application of the SSM in semantic segmentation of remotely sensed images, setting a new benchmark in performance for Mamba-based techniques in this specific application. The source code and baseline implementations are available at https://github.com/zhuqinfeng1999/Samba.</li>
</ul>

<h3>Title: Upsample Guidance: Scale Up Diffusion Models without Training</h3>
<ul>
<li><strong>Authors: </strong>Juno Hwang, Yong-Hyun Park, Junghyo Jo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01709">https://arxiv.org/abs/2404.01709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01709">https://arxiv.org/pdf/2404.01709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01709]] Upsample Guidance: Scale Up Diffusion Models without Training(https://arxiv.org/abs/2404.01709)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated superior performance across various generative tasks including images, videos, and audio. However, they encounter difficulties in directly generating high-resolution samples. Previously proposed solutions to this issue involve modifying the architecture, further training, or partitioning the sampling process into multiple stages. These methods have the limitation of not being able to directly utilize pre-trained models as-is, requiring additional work. In this paper, we introduce upsample guidance, a technique that adapts pretrained diffusion model (e.g., $512^2$) to generate higher-resolution images (e.g., $1536^2$) by adding only a single term in the sampling process. Remarkably, this technique does not necessitate any additional training or relying on external models. We demonstrate that upsample guidance can be applied to various models, such as pixel-space, latent space, and video diffusion models. We also observed that the proper selection of guidance scale can improve image quality, fidelity, and prompt alignment.</li>
</ul>

<h3>Title: Generative AI for Immersive Communication: The Next Frontier in  Internet-of-Senses Through 6G</h3>
<ul>
<li><strong>Authors: </strong>Nassim Sehad, Lina Bariah, Wassim Hamidouche, Hamed Hellaoui, Riku J√§ntti, M√©rouane Debbah</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.MM, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01713">https://arxiv.org/abs/2404.01713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01713">https://arxiv.org/pdf/2404.01713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01713]] Generative AI for Immersive Communication: The Next Frontier in  Internet-of-Senses Through 6G(https://arxiv.org/abs/2404.01713)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Over the past two decades, the Internet-of-Things (IoT) has been a transformative concept, and as we approach 2030, a new paradigm known as the Internet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR), IoS seeks to provide multi-sensory experiences, acknowledging that in our physical reality, our perception extends far beyond just sight and sound; it encompasses a range of senses. This article explores existing technologies driving immersive multi-sensory media, delving into their capabilities and potential applications. This exploration includes a comparative analysis between conventional immersive media streaming and a proposed use case that lever- ages semantic communication empowered by generative Artificial Intelligence (AI). The focal point of this analysis is the substantial reduction in bandwidth consumption by 99.93% in the proposed scheme. Through this comparison, we aim to underscore the practical applications of generative AI for immersive media while addressing the challenges and outlining future trajectories.</li>
</ul>

<h3>Title: AddSR: Accelerating Diffusion-based Blind Super-Resolution with  Adversarial Diffusion Distillation</h3>
<ul>
<li><strong>Authors: </strong>Rui Xie, Ying Tai, Kai Zhang, Zhenyu Zhang, Jun Zhou, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01717">https://arxiv.org/abs/2404.01717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01717">https://arxiv.org/pdf/2404.01717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01717]] AddSR: Accelerating Diffusion-based Blind Super-Resolution with  Adversarial Diffusion Distillation(https://arxiv.org/abs/2404.01717)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Blind super-resolution methods based on stable diffusion showcase formidable generative capabilities in reconstructing clear high-resolution images with intricate details from low-resolution inputs. However, their practical applicability is often hampered by poor efficiency, stemming from the requirement of thousands or hundreds of sampling steps. Inspired by the efficient text-to-image approach adversarial diffusion distillation (ADD), we design AddSR to address this issue by incorporating the ideas of both distillation and ControlNet. Specifically, we first propose a prediction-based self-refinement strategy to provide high-frequency information in the student model output with marginal additional time cost. Furthermore, we refine the training process by employing HR images, rather than LR images, to regulate the teacher model, providing a more robust constraint for distillation. Second, we introduce a timestep-adapting loss to address the perception-distortion imbalance problem introduced by ADD. Extensive experiments demonstrate our AddSR generates better restoration results, while achieving faster speed than previous SD-based state-of-the-art models (e.g., 7x faster than SeeSR).</li>
</ul>

<h3>Title: Self-Improvement Programming for Temporal Knowledge Graph Question  Answering</h3>
<ul>
<li><strong>Authors: </strong>Zhuo Chen, Zhao Zhang, Zixuan Li, Fei Wang, Yutao Zeng, Xiaolong Jin, Yongjun Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01720">https://arxiv.org/abs/2404.01720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01720">https://arxiv.org/pdf/2404.01720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01720]] Self-Improvement Programming for Temporal Knowledge Graph Question  Answering(https://arxiv.org/abs/2404.01720)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Temporal Knowledge Graph Question Answering (TKGQA) aims to answer questions with temporal intent over Temporal Knowledge Graphs (TKGs). The core challenge of this task lies in understanding the complex semantic information regarding multiple types of time constraints (e.g., before, first) in questions. Existing end-to-end methods implicitly model the time constraints by learning time-aware embeddings of questions and candidate answers, which is far from understanding the question comprehensively. Motivated by semantic-parsing-based approaches that explicitly model constraints in questions by generating logical forms with symbolic operators, we design fundamental temporal operators for time constraints and introduce a novel self-improvement Programming method for TKGQA (Prog-TQA). Specifically, Prog-TQA leverages the in-context learning ability of Large Language Models (LLMs) to understand the combinatory time constraints in the questions and generate corresponding program drafts with a few examples given. Then, it aligns these drafts to TKGs with the linking module and subsequently executes them to generate the answers. To enhance the ability to understand questions, Prog-TQA is further equipped with a self-improvement strategy to effectively bootstrap LLMs using high-quality self-generated drafts. Extensive experiments demonstrate the superiority of the proposed Prog-TQA on MultiTQ and CronQuestions datasets, especially in the Hits@1 metric.</li>
</ul>

<h3>Title: Asymptotics of Language Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Joy Qiping Yang, Salman Salamatian, Ziteng Sun, Ananda Theertha Suresh, Ahmad Beirami</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01730">https://arxiv.org/abs/2404.01730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01730">https://arxiv.org/pdf/2404.01730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01730]] Asymptotics of Language Model Alignment(https://arxiv.org/abs/2404.01730)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Let $p$ denote a generative language model. Let $r$ denote a reward model that returns a scalar that captures the degree at which a draw from $p$ is preferred. The goal of language model alignment is to alter $p$ to a new distribution $\phi$ that results in a higher expected reward while keeping $\phi$ close to $p.$ A popular alignment method is the KL-constrained reinforcement learning (RL), which chooses a distribution $\phi_\Delta$ that maximizes $E_{\phi_{\Delta}} r(y)$ subject to a relative entropy constraint $KL(\phi_\Delta || p) \leq \Delta.$ Another simple alignment method is best-of-$N$, where $N$ samples are drawn from $p$ and one with highest reward is selected. In this paper, we offer a closed-form characterization of the optimal KL-constrained RL solution. We demonstrate that any alignment method that achieves a comparable trade-off between KL divergence and reward must approximate the optimal KL-constrained RL solution in terms of relative entropy. To further analyze the properties of alignment methods, we introduce two simplifying assumptions: we let the language model be memoryless, and the reward model be linear. Although these assumptions may not reflect complex real-world scenarios, they enable a precise characterization of the asymptotic behavior of both the best-of-$N$ alignment, and the KL-constrained RL method, in terms of information-theoretic quantities. We prove that the reward of the optimal KL-constrained RL solution satisfies a large deviation principle, and we fully characterize its rate function. We also show that the rate of growth of the scaled cumulants of the reward is characterized by a proper Renyi cross entropy. Finally, we show that best-of-$N$ is asymptotically equivalent to KL-constrained RL solution by proving that their expected rewards are asymptotically equal, and concluding that the two distributions must be close in KL divergence.</li>
</ul>

<h3>Title: Octopus v2: On-device language model for super agent</h3>
<ul>
<li><strong>Authors: </strong>Wei Chen, Zhiyuan Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01744">https://arxiv.org/abs/2404.01744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01744">https://arxiv.org/pdf/2404.01744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01744]] Octopus v2: On-device language model for super agent(https://arxiv.org/abs/2404.01744)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Language models have shown effectiveness in a variety of software applications, particularly in tasks related to automatic workflow. These models possess the crucial ability to call functions, which is essential in creating AI agents. Despite the high performance of large-scale language models in cloud environments, they are often associated with concerns over privacy and cost. Current on-device models for function calling face issues with latency and accuracy. Our research presents a new method that empowers an on-device model with 2 billion parameters to surpass the performance of GPT-4 in both accuracy and latency, and decrease the context length by 95\%. When compared to Llama-7B with a RAG-based function calling mechanism, our method enhances latency by 35-fold. This method reduces the latency to levels deemed suitable for deployment across a variety of edge devices in production environments, aligning with the performance requisites for real-world applications.</li>
</ul>

<h3>Title: Unleash the Potential of CLIP for Video Highlight Detection</h3>
<ul>
<li><strong>Authors: </strong>Donghoon Han, Seunghyeon Seo, Eunhwan Park, Seong-Uk Nam, Nojun Kwak</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01745">https://arxiv.org/abs/2404.01745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01745">https://arxiv.org/pdf/2404.01745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01745]] Unleash the Potential of CLIP for Video Highlight Detection(https://arxiv.org/abs/2404.01745)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal and large language models (LLMs) have revolutionized the utilization of open-world knowledge, unlocking novel potentials across various tasks and applications. Among these domains, the video domain has notably benefited from their capabilities. In this paper, we present Highlight-CLIP (HL-CLIP), a method designed to excel in the video highlight detection task by leveraging the pre-trained knowledge embedded in multimodal models. By simply fine-tuning the multimodal encoder in combination with our innovative saliency pooling technique, we have achieved the state-of-the-art performance in the highlight detection task, the QVHighlight Benchmark, to the best of our knowledge.</li>
</ul>

<h3>Title: Exploring Latent Pathways: Enhancing the Interpretability of Autonomous  Driving with a Variational Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Anass Bairouk, Mirjana Maras, Simon Herlin, Alexander Amini, Marc Blanchon, Ramin Hasani, Patrick Chareyre, Daniela Rus</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01750">https://arxiv.org/abs/2404.01750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01750">https://arxiv.org/pdf/2404.01750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01750]] Exploring Latent Pathways: Enhancing the Interpretability of Autonomous  Driving with a Variational Autoencoder(https://arxiv.org/abs/2404.01750)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Autonomous driving presents a complex challenge, which is usually addressed with artificial intelligence models that are end-to-end or modular in nature. Within the landscape of modular approaches, a bio-inspired neural circuit policy model has emerged as an innovative control module, offering a compact and inherently interpretable system to infer a steering wheel command from abstract visual features. Here, we take a leap forward by integrating a variational autoencoder with the neural circuit policy controller, forming a solution that directly generates steering commands from input camera images. By substituting the traditional convolutional neural network approach to feature extraction with a variational autoencoder, we enhance the system's interpretability, enabling a more transparent and understandable decision-making process. In addition to the architectural shift toward a variational autoencoder, this study introduces the automatic latent perturbation tool, a novel contribution designed to probe and elucidate the latent features within the variational autoencoder. The automatic latent perturbation tool automates the interpretability process, offering granular insights into how specific latent variables influence the overall model's behavior. Through a series of numerical experiments, we demonstrate the interpretative power of the variational autoencoder-neural circuit policy model and the utility of the automatic latent perturbation tool in making the inner workings of autonomous driving systems more transparent.</li>
</ul>

<h3>Title: M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets</h3>
<ul>
<li><strong>Authors: </strong>Gaurish Thakkar, Sherzod Hakimov, Marko Tadiƒá</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01753">https://arxiv.org/abs/2404.01753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01753">https://arxiv.org/pdf/2404.01753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01753]] M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets(https://arxiv.org/abs/2404.01753)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, multimodal natural language processing, aimed at learning from diverse data types, has garnered significant attention. However, there needs to be more clarity when it comes to analysing multimodal tasks in multi-lingual contexts. While prior studies on sentiment analysis of tweets have predominantly focused on the English language, this paper addresses this gap by transforming an existing textual Twitter sentiment dataset into a multimodal format through a straightforward curation process. Our work opens up new avenues for sentiment-related research within the research community. Additionally, we conduct baseline experiments utilising this augmented dataset and report the findings. Notably, our evaluations reveal that when comparing unimodal and multimodal configurations, using a sentiment-tuned large language model as a text encoder performs exceptionally well.</li>
</ul>

<h3>Title: GEARS: Local Geometry-aware Hand-object Interaction Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Keyang Zhou, Bharat Lal Bhatnagar, Jan Eric Lenssen, Gerard Pons-moll</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01758">https://arxiv.org/abs/2404.01758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01758">https://arxiv.org/pdf/2404.01758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01758]] GEARS: Local Geometry-aware Hand-object Interaction Synthesis(https://arxiv.org/abs/2404.01758)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Generating realistic hand motion sequences in interaction with objects has gained increasing attention with the growing interest in digital humans. Prior work has illustrated the effectiveness of employing occupancy-based or distance-based virtual sensors to extract hand-object interaction features. Nonetheless, these methods show limited generalizability across object categories, shapes and sizes. We hypothesize that this is due to two reasons: 1) the limited expressiveness of employed virtual sensors, and 2) scarcity of available training data. To tackle this challenge, we introduce a novel joint-centered sensor designed to reason about local object geometry near potential interaction regions. The sensor queries for object surface points in the neighbourhood of each hand joint. As an important step towards mitigating the learning complexity, we transform the points from global frame to hand template frame and use a shared module to process sensor features of each individual joint. This is followed by a spatio-temporal transformer network aimed at capturing correlation among the joints in different dimensions. Moreover, we devise simple heuristic rules to augment the limited training sequences with vast static hand grasping samples. This leads to a broader spectrum of grasping types observed during training, in turn enhancing our model's generalization capability. We evaluate on two public datasets, GRAB and InterCap, where our method shows superiority over baselines both quantitatively and perceptually.</li>
</ul>

<h3>Title: Security for adversarial wiretap channels</h3>
<ul>
<li><strong>Authors: </strong>Esther H√§nggi, Iy√°n M√©ndez Veiga, Ligong Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01760">https://arxiv.org/abs/2404.01760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01760">https://arxiv.org/pdf/2404.01760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01760]] Security for adversarial wiretap channels(https://arxiv.org/abs/2404.01760)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>We consider the wiretap channel, where the individual channel uses have memory or are influenced by an adversary. We analyze the explicit and computationally efficient construction of information-theoretically secure coding schemes which use the inverse of an extractor and an error-correcting code. These schemes are known to achieve secrecy capacity on a large class of memoryless wiretap channels. We show that this also holds for certain channel types with memory. In particular, they can achieve secrecy capacity on channels where an adversary can pick a sequence of ``states'' governing the channel's behavior, as long as, given every possible state, the channel is strongly symmetric.</li>
</ul>

<h3>Title: Class-Incremental Few-Shot Event Detection</h3>
<ul>
<li><strong>Authors: </strong>Kailin Zhao, Xiaolong Jin, Long Bai, Jiafeng Guo, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01767">https://arxiv.org/abs/2404.01767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01767">https://arxiv.org/pdf/2404.01767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01767]] Class-Incremental Few-Shot Event Detection(https://arxiv.org/abs/2404.01767)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Event detection is one of the fundamental tasks in information extraction and knowledge graph. However, a realistic event detection system often needs to deal with new event classes constantly. These new classes usually have only a few labeled instances as it is time-consuming and labor-intensive to annotate a large number of unlabeled instances. Therefore, this paper proposes a new task, called class-incremental few-shot event detection. Nevertheless, this task faces two problems, i.e., old knowledge forgetting and new class overfitting. To solve these problems, this paper further presents a novel knowledge distillation and prompt learning based method, called Prompt-KD. Specifically, to handle the forgetting problem about old knowledge, Prompt-KD develops an attention based multi-teacher knowledge distillation framework, where the ancestor teacher model pre-trained on base classes is reused in all learning sessions, and the father teacher model derives the current student model via adaptation. On the other hand, in order to cope with the few-shot learning scenario and alleviate the corresponding new class overfitting problem, Prompt-KD is also equipped with a prompt learning mechanism. Extensive experiments on two benchmark datasets, i.e., FewEvent and MAVEN, demonstrate the superior performance of Prompt-KD.</li>
</ul>

<h3>Title: Auditing Large Language Models for Enhanced Text-Based Stereotype  Detection and Probing-Based Bias Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Zekun Wu, Sahan Bulathwela, Maria Perez-Ortiz, Adriano Soares Koshiyama</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01768">https://arxiv.org/abs/2404.01768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01768">https://arxiv.org/pdf/2404.01768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01768]] Auditing Large Language Models for Enhanced Text-Based Stereotype  Detection and Probing-Based Bias Evaluation(https://arxiv.org/abs/2404.01768)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have significantly increased their presence in human-facing Artificial Intelligence (AI) applications. However, LLMs could reproduce and even exacerbate stereotypical outputs from training data. This work introduces the Multi-Grain Stereotype (MGS) dataset, encompassing 51,867 instances across gender, race, profession, religion, and stereotypical text, collected by fusing multiple previously publicly available stereotype detection datasets. We explore different machine learning approaches aimed at establishing baselines for stereotype detection, and fine-tune several language models of various architectures and model sizes, presenting in this work a series of stereotypes classifier models for English text trained on MGS. To understand whether our stereotype detectors capture relevant features (aligning with human common sense) we utilise a variety of explanainable AI tools, including SHAP, LIME, and BertViz, and analyse a series of example cases discussing the results. Finally, we develop a series of stereotype elicitation prompts and evaluate the presence of stereotypes in text generation tasks with popular LLMs, using one of our best performing previously presented stereotypes detectors. Our experiments yielded several key findings: i) Training stereotype detectors in a multi-dimension setting yields better results than training multiple single-dimension classifiers.ii) The integrated MGS Dataset enhances both the in-dataset and cross-dataset generalisation ability of stereotype detectors compared to using the datasets separately. iii) There is a reduction in stereotypes in the content generated by GPT Family LLMs with newer versions.</li>
</ul>

<h3>Title: A noisy elephant in the room: Is your out-of-distribution detector  robust to label noise?</h3>
<ul>
<li><strong>Authors: </strong>Galadrielle Humblot-Renaux, Sergio Escalera, Thomas B. Moeslund</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01775">https://arxiv.org/abs/2404.01775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01775">https://arxiv.org/pdf/2404.01775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01775]] A noisy elephant in the room: Is your out-of-distribution detector  robust to label noise?(https://arxiv.org/abs/2404.01775)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The ability to detect unfamiliar or unexpected images is essential for safe deployment of computer vision systems. In the context of classification, the task of detecting images outside of a model's training domain is known as out-of-distribution (OOD) detection. While there has been a growing research interest in developing post-hoc OOD detection methods, there has been comparably little discussion around how these methods perform when the underlying classifier is not trained on a clean, carefully curated dataset. In this work, we take a closer look at 20 state-of-the-art OOD detection methods in the (more realistic) scenario where the labels used to train the underlying classifier are unreliable (e.g. crowd-sourced or web-scraped labels). Extensive experiments across different datasets, noise types & levels, architectures and checkpointing strategies provide insights into the effect of class label noise on OOD detection, and show that poor separation between incorrectly classified ID samples vs. OOD samples is an overlooked yet important limitation of existing methods. Code: https://github.com/glhr/ood-labelnoise</li>
</ul>

<h3>Title: Generative AI-Based Text Generation Methods Using Pre-Trained GPT-2  Model</h3>
<ul>
<li><strong>Authors: </strong>Rohit Pandey, Hetvi Waghela, Sneha Rakshit, Aparna Rangari, Anjali Singh, Rahul Kumar, Ratnadeep Ghosal, Jaydip Sen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01786">https://arxiv.org/abs/2404.01786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01786">https://arxiv.org/pdf/2404.01786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01786]] Generative AI-Based Text Generation Methods Using Pre-Trained GPT-2  Model(https://arxiv.org/abs/2404.01786)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This work delved into the realm of automatic text generation, exploring a variety of techniques ranging from traditional deterministic approaches to more modern stochastic methods. Through analysis of greedy search, beam search, top-k sampling, top-p sampling, contrastive searching, and locally typical searching, this work has provided valuable insights into the strengths, weaknesses, and potential applications of each method. Each text-generating method is evaluated using several standard metrics and a comparative study has been made on the performance of the approaches. Finally, some future directions of research in the field of automatic text generation are also identified.</li>
</ul>

<h3>Title: PATCH -- Psychometrics-AssisTed benCHmarking of Large Language Models: A  Case Study of Mathematics Proficiency</h3>
<ul>
<li><strong>Authors: </strong>Qixiang Fang, Daniel L. Oberski, Dong Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01799">https://arxiv.org/abs/2404.01799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01799">https://arxiv.org/pdf/2404.01799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01799]] PATCH -- Psychometrics-AssisTed benCHmarking of Large Language Models: A  Case Study of Mathematics Proficiency(https://arxiv.org/abs/2404.01799)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Many existing benchmarks of large (multimodal) language models (LLMs) focus on measuring LLMs' academic proficiency, often with also an interest in comparing model performance with human test takers. While these benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?). In response to these challenges, we propose leveraging knowledge from psychometrics - a field dedicated to the measurement of latent variables like academic proficiency - into LLM benchmarking. We make three primary contributions. First, we introduce PATCH: a novel framework for Psychometrics-AssisTed benCHmarking of LLMs. PATCH addresses the aforementioned limitations, presenting a new direction for LLM benchmark research. Second, we implement PATCH by measuring GPT-4 and Gemini-Pro-Vision's proficiency in 8th grade mathematics against 56 human populations. We show that adopting a psychometrics-based approach yields evaluation outcomes that diverge from those based on existing benchmarking practices. Third, we release 4 datasets to support measuring and comparing LLM proficiency in grade school mathematics and science against human populations.</li>
</ul>

<h3>Title: EventSleep: Sleep Activity Recognition with Event Cameras</h3>
<ul>
<li><strong>Authors: </strong>Carlos Plou, Nerea Gallego, Alberto Sabater, Eduardo Montijano, Pablo Urcola, Luis Montesano, Ruben Martinez-Cantin, Ana C. Murillo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01801">https://arxiv.org/abs/2404.01801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01801">https://arxiv.org/pdf/2404.01801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01801]] EventSleep: Sleep Activity Recognition with Event Cameras(https://arxiv.org/abs/2404.01801)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Event cameras are a promising technology for activity recognition in dark environments due to their unique properties. However, real event camera datasets under low-lighting conditions are still scarce, which also limits the number of approaches to solve these kind of problems, hindering the potential of this technology in many applications. We present EventSleep, a new dataset and methodology to address this gap and study the suitability of event cameras for a very relevant medical application: sleep monitoring for sleep disorders analysis. The dataset contains synchronized event and infrared recordings emulating common movements that happen during the sleep, resulting in a new challenging and unique dataset for activity recognition in dark environments. Our novel pipeline is able to achieve high accuracy under these challenging conditions and incorporates a Bayesian approach (Laplace ensembles) to increase the robustness in the predictions, which is fundamental for medical applications. Our work is the first application of Bayesian neural networks for event cameras, the first use of Laplace ensembles in a realistic problem, and also demonstrates for the first time the potential of event cameras in a new application domain: to enhance current sleep evaluation procedures. Our activity recognition results highlight the potential of event cameras under dark conditions, and its capacity and robustness for sleep activity recognition, and open problems as the adaptation of event data pre-processing techniques to dark environments.</li>
</ul>

<h3>Title: Systematic Solutions to Login and Authentication Security: A  Dual-Password Login-Authentication Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Yun Su, Mo Xi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01803">https://arxiv.org/abs/2404.01803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01803">https://arxiv.org/pdf/2404.01803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01803]] Systematic Solutions to Login and Authentication Security: A  Dual-Password Login-Authentication Mechanism(https://arxiv.org/abs/2404.01803)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>Credential theft and remote attacks are the most serious threats to authentication mechanisms. The crux of the problems is that we cannot control such behaviors. However, if a password does not contain user's secrets, stealing it is useless. If unauthorized inputs are disabled, the remote attacks can be invalidated. Thereby, credential secrets and input fields to our accounts can be controlled. Rather than encrypting passwords, we design a dual-password login-authentication mechanism, where a user-selected secret-free login password is converted into an untypable authentication password. Subsequently, the authenticatable functionality of the login password and the typable functionality of the authentication password may be disabled or invalidated so that the credential theft and remote attacks can be prevented. Thus, the usability-security trade-off and password reuse are resolved; local storage of authentication passwords is no longer necessary. More importantly, the password converter acts as an open hash algorithm, meaning that its intermediate elements can be used to define a truly unique identity of the login process to implement a novel dual-identity authentication. Particularly, the elements are concealed, inaccessible, and independent of any personal information, and therefore can be used to define a perfect unforgeable process identifier to identify and disable the unauthorized inputs.</li>
</ul>

<h3>Title: Improved Text Emotion Prediction Using Combined Valence and Arousal  Ordinal Classification</h3>
<ul>
<li><strong>Authors: </strong>Michael Mitsios, Georgios Vamvoukakis, Georgia Maniati, Nikolaos Ellinas, Georgios Dimitriou, Konstantinos Markopoulos, Panos Kakoulidis, Alexandra Vioni, Myrsini Christidou, Junkwang Oh, Gunu Jho, Inchul Hwang, Georgios Vardaxoglou, Aimilios Chalamandaris, Pirros Tsiakoulis, Spyros Raptis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01805">https://arxiv.org/abs/2404.01805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01805">https://arxiv.org/pdf/2404.01805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01805]] Improved Text Emotion Prediction Using Combined Valence and Arousal  Ordinal Classification(https://arxiv.org/abs/2404.01805)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Emotion detection in textual data has received growing interest in recent years, as it is pivotal for developing empathetic human-computer interaction systems. This paper introduces a method for categorizing emotions from text, which acknowledges and differentiates between the diversified similarities and distinctions of various emotions. Initially, we establish a baseline by training a transformer-based model for standard emotion classification, achieving state-of-the-art performance. We argue that not all misclassifications are of the same importance, as there are perceptual similarities among emotional classes. We thus redefine the emotion labeling problem by shifting it from a traditional classification model to an ordinal classification one, where discrete emotions are arranged in a sequential order according to their valence levels. Finally, we propose a method that performs ordinal classification in the two-dimensional emotion space, considering both valence and arousal scales. The results show that our approach not only preserves high accuracy in emotion prediction but also significantly reduces the magnitude of errors in cases of misclassification.</li>
</ul>

<h3>Title: Software-Defined Cryptography: A Design Feature of Cryptographic Agility</h3>
<ul>
<li><strong>Authors: </strong>Jihoon Cho, Changhoon Lee, Eunkyung Kim, Jieun Lee, Beumjin Cho</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01808">https://arxiv.org/abs/2404.01808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01808">https://arxiv.org/pdf/2404.01808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01808]] Software-Defined Cryptography: A Design Feature of Cryptographic Agility(https://arxiv.org/abs/2404.01808)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Cryptographic agility, or crypto-agility, is a design feature that enables agile updates to new cryptographic algorithms and standards without the need to modify or replace the surrounding infrastructure. This paper examines the prerequisites for crypto-agility and proposes its desired design feature. More specifically, we investigate the design characteristics of widely deployed cybersecurity paradigms, i.e., zero trust, and apply its design feature to crypto-agility, achieving greater visibility and automation in cryptographic management.</li>
</ul>

<h3>Title: Sparse Semi-DETR: Sparse Learnable Queries for Semi-Supervised Object  Detection</h3>
<ul>
<li><strong>Authors: </strong>Tahira Shehzadi, Khurram Azeem Hashmi, Didier Stricker, Muhammad Zeshan Afzal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01819">https://arxiv.org/abs/2404.01819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01819">https://arxiv.org/pdf/2404.01819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01819]] Sparse Semi-DETR: Sparse Learnable Queries for Semi-Supervised Object  Detection(https://arxiv.org/abs/2404.01819)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we address the limitations of the DETR-based semi-supervised object detection (SSOD) framework, particularly focusing on the challenges posed by the quality of object queries. In DETR-based SSOD, the one-to-one assignment strategy provides inaccurate pseudo-labels, while the one-to-many assignments strategy leads to overlapping predictions. These issues compromise training efficiency and degrade model performance, especially in detecting small or occluded objects. We introduce Sparse Semi-DETR, a novel transformer-based, end-to-end semi-supervised object detection solution to overcome these challenges. Sparse Semi-DETR incorporates a Query Refinement Module to enhance the quality of object queries, significantly improving detection capabilities for small and partially obscured objects. Additionally, we integrate a Reliable Pseudo-Label Filtering Module that selectively filters high-quality pseudo-labels, thereby enhancing detection accuracy and consistency. On the MS-COCO and Pascal VOC object detection benchmarks, Sparse Semi-DETR achieves a significant improvement over current state-of-the-art methods that highlight Sparse Semi-DETR's effectiveness in semi-supervised object detection, particularly in challenging scenarios involving small or partially obscured objects.</li>
</ul>

<h3>Title: Defense without Forgetting: Continual Adversarial Defense with  Anisotropic & Isotropic Pseudo Replay</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Zhou, Zhongyun Hua</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01828">https://arxiv.org/abs/2404.01828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01828">https://arxiv.org/pdf/2404.01828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01828]] Defense without Forgetting: Continual Adversarial Defense with  Anisotropic & Isotropic Pseudo Replay(https://arxiv.org/abs/2404.01828)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks have demonstrated susceptibility to adversarial attacks. Adversarial defense techniques often focus on one-shot setting to maintain robustness against attack. However, new attacks can emerge in sequences in real-world deployment scenarios. As a result, it is crucial for a defense model to constantly adapt to new attacks, but the adaptation process can lead to catastrophic forgetting of previously defended against attacks. In this paper, we discuss for the first time the concept of continual adversarial defense under a sequence of attacks, and propose a lifelong defense baseline called Anisotropic \& Isotropic Replay (AIR), which offers three advantages: (1) Isotropic replay ensures model consistency in the neighborhood distribution of new data, indirectly aligning the output preference between old and new tasks. (2) Anisotropic replay enables the model to learn a compromise data manifold with fresh mixed semantics for further replay constraints and potential future attacks. (3) A straightforward regularizer mitigates the 'plasticity-stability' trade-off by aligning model output between new and old tasks. Experiment results demonstrate that AIR can approximate or even exceed the empirical performance upper bounds achieved by Joint Training.</li>
</ul>

<h3>Title: Great, Now Write an Article About That: The Crescendo Multi-Turn LLM  Jailbreak Attack</h3>
<ul>
<li><strong>Authors: </strong>Mark Russinovich, Ahmed Salem, Ronen Eldan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01833">https://arxiv.org/abs/2404.01833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01833">https://arxiv.org/pdf/2404.01833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01833]] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM  Jailbreak Attack(https://arxiv.org/abs/2404.01833)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as "jailbreaks", seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies, progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b Chat, and Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, we introduce Crescendomation, a tool that automates the Crescendo attack, and our evaluation showcases its effectiveness against state-of-the-art models.</li>
</ul>

<h3>Title: Semi-Supervised Domain Adaptation for Wildfire Detection</h3>
<ul>
<li><strong>Authors: </strong>JooYoung Jang, Youngseo Cha, Jisu Kim, SooHyung Lee, Geonu Lee, Minkook Cho, Young Hwang, Nojun Kwak</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01842">https://arxiv.org/abs/2404.01842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01842">https://arxiv.org/pdf/2404.01842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01842]] Semi-Supervised Domain Adaptation for Wildfire Detection(https://arxiv.org/abs/2404.01842)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recently, both the frequency and intensity of wildfires have increased worldwide, primarily due to climate change. In this paper, we propose a novel protocol for wildfire detection, leveraging semi-supervised Domain Adaptation for object detection, accompanied by a corresponding dataset designed for use by both academics and industries. Our dataset encompasses 30 times more diverse labeled scenes for the current largest benchmark wildfire dataset, HPWREN, and introduces a new labeling policy for wildfire detection. Inspired by CoordConv, we propose a robust baseline, Location-Aware Object Detection for Semi-Supervised Domain Adaptation (LADA), utilizing a teacher-student based framework capable of extracting translational variance features characteristic of wildfires. With only using 1% target domain labeled data, our framework significantly outperforms our source-only baseline by a notable margin of 3.8% in mean Average Precision on the HPWREN wildfire dataset. Our dataset is available at https://github.com/BloomBerry/LADA.</li>
</ul>

<h3>Title: Accelerating Transformer Pre-Training with 2:4 Sparsity</h3>
<ul>
<li><strong>Authors: </strong>Yuezhou Hu, Kang Zhao, Weiyu Huang, Jianfei Chen, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01847">https://arxiv.org/abs/2404.01847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01847">https://arxiv.org/pdf/2404.01847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01847]] Accelerating Transformer Pre-Training with 2:4 Sparsity(https://arxiv.org/abs/2404.01847)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Training large Transformers is slow, but recent innovations on GPU architecture gives us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of Transformers in pre-training. First, we define a "flip rate" to monitor the stability of a 2:4 training process. Utilizing this metric, we suggest two techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the mask decay term on gradients, and to enhance the model's quality by a simple yet effective dense fine-tuning procedure near the end of pre-training. Besides, we devise two effective techniques to practically accelerate training: to calculate transposable 2:4 mask by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that a combination of our methods reaches the best performance on multiple Transformers among different 2:4 training methods, while actual acceleration can be observed on different shapes of Transformer block.</li>
</ul>

<h3>Title: Pairwise Similarity Distribution Clustering for Noisy Label Learning</h3>
<ul>
<li><strong>Authors: </strong>Sihan Bai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01853">https://arxiv.org/abs/2404.01853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01853">https://arxiv.org/pdf/2404.01853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01853]] Pairwise Similarity Distribution Clustering for Noisy Label Learning(https://arxiv.org/abs/2404.01853)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Noisy label learning aims to train deep neural networks using a large amount of samples with noisy labels, whose main challenge comes from how to deal with the inaccurate supervision caused by wrong labels. Existing works either take the label correction or sample selection paradigm to involve more samples with accurate labels into the training process. In this paper, we propose a simple yet effective sample selection algorithm, termed as Pairwise Similarity Distribution Clustering~(PSDC), to divide the training samples into one clean set and another noisy set, which can power any of the off-the-shelf semi-supervised learning regimes to further train networks for different downstream tasks. Specifically, we take the pairwise similarity between sample pairs to represent the sample structure, and the Gaussian Mixture Model~(GMM) to model the similarity distribution between sample pairs belonging to the same noisy cluster, therefore each sample can be confidently divided into the clean set or noisy set. Even under severe label noise rate, the resulting data partition mechanism has been proved to be more robust in judging the label confidence in both theory and practice. Experimental results on various benchmark datasets, such as CIFAR-10, CIFAR-100 and Clothing1M, demonstrate significant improvements over state-of-the-art methods.</li>
</ul>

<h3>Title: Poro 34B and the Blessing of Multilinguality</h3>
<ul>
<li><strong>Authors: </strong>Risto Luukkonen, Jonathan Burdge, Elaine Zosa, Aarne Talman, Ville Komulainen, V√§in√∂ Hatanp√§√§, Peter Sarlin, Sampo Pyysalo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01856">https://arxiv.org/abs/2404.01856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01856">https://arxiv.org/pdf/2404.01856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01856]] Poro 34B and the Blessing of Multilinguality(https://arxiv.org/abs/2404.01856)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at https://huggingface.co/LumiOpen/Poro-34B.</li>
</ul>

<h3>Title: Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Xu He, Qiaochu Huang, Zhensong Zhang, Zhiwei Lin, Zhiyong Wu, Sicheng Yang, Minglei Li, Zhiyi Chen, Songcen Xu, Xiaofei Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01862">https://arxiv.org/abs/2404.01862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01862">https://arxiv.org/pdf/2404.01862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01862]] Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model(https://arxiv.org/abs/2404.01862)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Co-speech gestures, if presented in the lively form of videos, can achieve superior visual effects in human-machine interaction. While previous works mostly generate structural human skeletons, resulting in the omission of appearance information, we focus on the direct generation of audio-driven co-speech gesture videos in this work. There are two main challenges: 1) A suitable motion feature is needed to describe complex human movements with crucial appearance information. 2) Gestures and speech exhibit inherent dependencies and should be temporally aligned even of arbitrary length. To solve these problems, we present a novel motion-decoupled framework to generate co-speech gesture videos. Specifically, we first introduce a well-designed nonlinear TPS transformation to obtain latent motion features preserving essential appearance information. Then a transformer-based diffusion model is proposed to learn the temporal correlation between gestures and speech, and performs generation in the latent motion space, followed by an optimal motion selection module to produce long-term coherent and consistent gesture videos. For better visual perception, we further design a refinement network focusing on missing details of certain areas. Extensive experimental results show that our proposed framework significantly outperforms existing approaches in both motion and video-related evaluations. Our code, demos, and more resources are available at https://github.com/thuhcsi/S2G-MDDiffusion.</li>
</ul>

<h3>Title: Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language  Models -- A Survey</h3>
<ul>
<li><strong>Authors: </strong>Philipp Mondorf, Barbara Plank</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01869">https://arxiv.org/abs/2404.01869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01869">https://arxiv.org/pdf/2404.01869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01869]] Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language  Models -- A Survey(https://arxiv.org/abs/2404.01869)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans. However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior. This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses. Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training data, rather than on genuine reasoning abilities. Additionally, we identify the need for further research that delineates the key differences between human and LLM-based reasoning. Through this survey, we aim to shed light on the complex reasoning processes within LLMs.</li>
</ul>

<h3>Title: Procedural Fairness in Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Ziming Wang, Changwu Huang, Xin Yao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01877">https://arxiv.org/abs/2404.01877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01877">https://arxiv.org/pdf/2404.01877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01877]] Procedural Fairness in Machine Learning(https://arxiv.org/abs/2404.01877)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Fairness in machine learning (ML) has received much attention. However, existing studies have mainly focused on the distributive fairness of ML models. The other dimension of fairness, i.e., procedural fairness, has been neglected. In this paper, we first define the procedural fairness of ML models, and then give formal definitions of individual and group procedural fairness. We propose a novel metric to evaluate the group procedural fairness of ML models, called $GPF_{FAE}$, which utilizes a widely used explainable artificial intelligence technique, namely feature attribution explanation (FAE), to capture the decision process of the ML models. We validate the effectiveness of $GPF_{FAE}$ on a synthetic dataset and eight real-world datasets. Our experiments reveal the relationship between procedural and distributive fairness of the ML model. Based on our analysis, we propose a method for identifying the features that lead to the procedural unfairness of the model and propose two methods to improve procedural fairness after identifying unfair features. Our experimental results demonstrate that we can accurately identify the features that lead to procedural unfairness in the ML model, and both of our proposed methods can significantly improve procedural fairness with a slight impact on model performance, while also improving distributive fairness.</li>
</ul>

<h3>Title: Real, fake and synthetic faces - does the coin have three sides?</h3>
<ul>
<li><strong>Authors: </strong>Shahzeb Naeem, Ramzi Al-Sharawi, Muhammad Riyyan Khan, Usman Tariq, Abhinav Dhall, Hasan Al-Nashash</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01878">https://arxiv.org/abs/2404.01878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01878">https://arxiv.org/pdf/2404.01878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01878]] Real, fake and synthetic faces - does the coin have three sides?(https://arxiv.org/abs/2404.01878)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the ever-growing power of generative artificial intelligence, deepfake and artificially generated (synthetic) media have continued to spread online, which creates various ethical and moral concerns regarding their usage. To tackle this, we thus present a novel exploration of the trends and patterns observed in real, deepfake and synthetic facial images. The proposed analysis is done in two parts: firstly, we incorporate eight deep learning models and analyze their performances in distinguishing between the three classes of images. Next, we look to further delve into the similarities and differences between these three sets of images by investigating their image properties both in the context of the entire image as well as in the context of specific regions within the image. ANOVA test was also performed and provided further clarity amongst the patterns associated between the images of the three classes. From our findings, we observe that the investigated deeplearning models found it easier to detect synthetic facial images, with the ViT Patch-16 model performing best on this task with a class-averaged sensitivity, specificity, precision, and accuracy of 97.37%, 98.69%, 97.48%, and 98.25%, respectively. This observation was supported by further analysis of various image properties. We saw noticeable differences across the three category of images. This analysis can help us build better algorithms for facial image generation, and also shows that synthetic, deepfake and real face images are indeed three different classes.</li>
</ul>

<h3>Title: Scene Adaptive Sparse Transformer for Event-based Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Yansong Peng, Hebei Li, Yueyi Zhang, Xiaoyan Sun, Feng Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01882">https://arxiv.org/abs/2404.01882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01882">https://arxiv.org/pdf/2404.01882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01882]] Scene Adaptive Sparse Transformer for Event-based Object Detection(https://arxiv.org/abs/2404.01882)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While recent Transformer-based approaches have shown impressive performances on event-based object detection tasks, their high computational costs still diminish the low power consumption advantage of event cameras. Image-based works attempt to reduce these costs by introducing sparse Transformers. However, they display inadequate sparsity and adaptability when applied to event-based object detection, since these approaches cannot balance the fine granularity of token-level sparsification and the efficiency of window-based Transformers, leading to reduced performance and efficiency. Furthermore, they lack scene-specific sparsity optimization, resulting in information loss and a lower recall rate. To overcome these limitations, we propose the Scene Adaptive Sparse Transformer (SAST). SAST enables window-token co-sparsification, significantly enhancing fault tolerance and reducing computational overhead. Leveraging the innovative scoring and selection modules, along with the Masked Sparse Window Self-Attention, SAST showcases remarkable scene-aware adaptability: It focuses only on important objects and dynamically optimizes sparsity level according to scene complexity, maintaining a remarkable balance between performance and computational cost. The evaluation results show that SAST outperforms all other dense and sparse networks in both performance and efficiency on two large-scale event-based object detection datasets (1Mpx and Gen1). Code: https://github.com/Peterande/SAST</li>
</ul>

<h3>Title: 3D Scene Generation from Scene Graphs and Self-Attention</h3>
<ul>
<li><strong>Authors: </strong>Pietro Bonazzi, Mengqi Wang, Diego Martin Arroyo, Fabian Manhardt, Federico Tombari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01887">https://arxiv.org/abs/2404.01887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01887">https://arxiv.org/pdf/2404.01887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01887]] 3D Scene Generation from Scene Graphs and Self-Attention(https://arxiv.org/abs/2404.01887)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Synthesizing realistic and diverse indoor 3D scene layouts in a controllable fashion opens up applications in simulated navigation and virtual reality. As concise and robust representations of a scene, scene graphs have proven to be well-suited as the semantic control on the generated layout. We present a variant of the conditional variational autoencoder (cVAE) model to synthesize 3D scenes from scene graphs and floor plans. We exploit the properties of self-attention layers to capture high-level relationships between objects in a scene, and use these as the building blocks of our model. Our model, leverages graph transformers to estimate the size, dimension and orientation of the objects in a room while satisfying relationships in the given scene graph. Our experiments shows self-attention layers leads to sparser (HOW MUCH) and more diverse scenes (HOW MUCH)\. Included in this work, we publish the first large-scale dataset for conditioned scene generation from scene graphs, containing over XXX rooms (of floor plans and scene graphs).</li>
</ul>

<h3>Title: ASTRA: An Action Spotting TRAnsformer for Soccer Videos</h3>
<ul>
<li><strong>Authors: </strong>Artur Xarles, Sergio Escalera, Thomas B. Moeslund, Albert Clap√©s</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01891">https://arxiv.org/abs/2404.01891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01891">https://arxiv.org/pdf/2404.01891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01891]] ASTRA: An Action Spotting TRAnsformer for Soccer Videos(https://arxiv.org/abs/2404.01891)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce ASTRA, a Transformer-based model designed for the task of Action Spotting in soccer matches. ASTRA addresses several challenges inherent in the task and dataset, including the requirement for precise action localization, the presence of a long-tail data distribution, non-visibility in certain actions, and inherent label noise. To do so, ASTRA incorporates (a) a Transformer encoder-decoder architecture to achieve the desired output temporal resolution and to produce precise predictions, (b) a balanced mixup strategy to handle the long-tail distribution of the data, (c) an uncertainty-aware displacement head to capture the label variability, and (d) input audio signal to enhance detection of non-visible actions. Results demonstrate the effectiveness of ASTRA, achieving a tight Average-mAP of 66.82 on the test set. Moreover, in the SoccerNet 2023 Action Spotting challenge, we secure the 3rd position with an Average-mAP of 70.21 on the challenge set.</li>
</ul>

<h3>Title: Minimize Quantization Output Error with Bias Compensation</h3>
<ul>
<li><strong>Authors: </strong>Cheng Gong, Haoshuai Zheng, Mengting Hu, Zheng Lin, Deng-Ping Fan, Yuzhi Zhang, Tao Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01892">https://arxiv.org/abs/2404.01892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01892">https://arxiv.org/pdf/2404.01892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01892]] Minimize Quantization Output Error with Bias Compensation(https://arxiv.org/abs/2404.01892)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Quantization is a promising method that reduces memory usage and computational intensity of Deep Neural Networks (DNNs), but it often leads to significant output error that hinder model deployment. In this paper, we propose Bias Compensation (BC) to minimize the output error, thus realizing ultra-low-precision quantization without model fine-tuning. Instead of optimizing the non-convex quantization process as in most previous methods, the proposed BC bypasses the step to directly minimize the quantizing output error by identifying a bias vector for compensation. We have established that the minimization of output error through BC is a convex problem and provides an efficient strategy to procure optimal solutions associated with minimal output error,without the need for training or fine-tuning. We conduct extensive experiments on Vision Transformer models and Large Language Models, and the results show that our method notably reduces quantization output error, thereby permitting ultra-low-precision post-training quantization and enhancing the task performance of models. Especially, BC improves the accuracy of ViT-B with 4-bit PTQ4ViT by 36.89% on the ImageNet-1k task, and decreases the perplexity of OPT-350M with 3-bit GPTQ by 5.97 on WikiText2.The code is in https://github.com/GongCheng1919/bias-compensation.</li>
</ul>

<h3>Title: Activation Steering for Robust Type Prediction in CodeLLMs</h3>
<ul>
<li><strong>Authors: </strong>Francesca Lucchetti, Arjun Guha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01903">https://arxiv.org/abs/2404.01903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01903">https://arxiv.org/pdf/2404.01903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01903]] Activation Steering for Robust Type Prediction in CodeLLMs(https://arxiv.org/abs/2404.01903)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Contemporary LLMs pretrained on code are capable of succeeding at a wide variety of programming tasks. However, their performance is very sensitive to syntactic features, such as the names of variables and types, the structure of code, and presence of type hints. We contribute an inference-time technique to make CodeLLMs more robust to syntactic distractors that are semantically irrelevant. Our methodology relies on activation steering, which involves editing internal model activations to steer the model towards the correct prediction. We contribute a novel way to construct steering vectors by taking inspiration from mutation testing, which constructs minimal semantics-breaking code edits. In contrast, we construct steering vectors from semantics-preserving code edits. We apply our approach to the task of type prediction for the gradually typed languages Python and TypeScript. This approach corrects up to 90% of type mispredictions. Finally, we show that steering vectors calculated from Python activations reliably correct type mispredictions in TypeScript, and vice versa. This result suggests that LLMs may be learning to transfer knowledge of types across programming languages.</li>
</ul>

<h3>Title: Humanizing Machine-Generated Content: Evading AI-Text Detection through  Adversarial Attack</h3>
<ul>
<li><strong>Authors: </strong>Ying Zhou, Ben He, Le Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01907">https://arxiv.org/abs/2404.01907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01907">https://arxiv.org/pdf/2404.01907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01907]] Humanizing Machine-Generated Content: Evading AI-Text Detection through  Adversarial Attack(https://arxiv.org/abs/2404.01907)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>With the development of large language models (LLMs), detecting whether text is generated by a machine becomes increasingly challenging in the face of malicious use cases like the spread of false information, protection of intellectual property, and prevention of academic plagiarism. While well-trained text detectors have demonstrated promising performance on unseen test data, recent research suggests that these detectors have vulnerabilities when dealing with adversarial attacks such as paraphrasing. In this paper, we propose a framework for a broader class of adversarial attacks, designed to perform minor perturbations in machine-generated content to evade detection. We consider two attack settings: white-box and black-box, and employ adversarial learning in dynamic scenarios to assess the potential enhancement of the current detection model's robustness against such attacks. The empirical results reveal that the current detection models can be compromised in as little as 10 seconds, leading to the misclassification of machine-generated text as human-written content. Furthermore, we explore the prospect of improving the model's robustness over iterative adversarial learning. Although some improvements in model robustness are observed, practical applications still face significant challenges. These findings shed light on the future development of AI-text detectors, emphasizing the need for more accurate and robust detection methods.</li>
</ul>

<h3>Title: Multicore DRAM Bank-& Row-Conflict Bomb for Timing Attacks in  Mixed-Criticality Systems</h3>
<ul>
<li><strong>Authors: </strong>Antonio Savino, Gautam Gala, Marcello Cinque, Gerhard Fohler</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01910">https://arxiv.org/abs/2404.01910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01910">https://arxiv.org/pdf/2404.01910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01910]] Multicore DRAM Bank-& Row-Conflict Bomb for Timing Attacks in  Mixed-Criticality Systems(https://arxiv.org/abs/2404.01910)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>With the increasing use of multicore platforms to realize mixed-criticality systems, understanding the underlying shared resources, such as the memory hierarchy shared among cores, and achieving isolation between co-executing tasks running on the same platform with different criticality levels becomes relevant. In addition to safety considerations, a malicious entity can exploit shared resources to create timing attacks on critical applications. In this paper, we focus on understanding the shared DRAM dual in-line memory module and created a timing attack, that we named the "bank & row conflict bomb", to target a victim task in a multicore platform. We also created a "navigate" algorithm to understand how victim requests are managed by the Memory Controller and provide valuable inputs for designing the bank & row conflict bomb. We performed experimental tests on a 2nd Gen Intel Xeon Processor with an 8GB DDR4-2666 DRAM module to show that such an attack can produce a significant increase in the execution time of the victim task by about 150%, motivating the need for proper countermeasures to help ensure the safety and security of critical applications.</li>
</ul>

<h3>Title: SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity  Recognition of Unseen Entities</h3>
<ul>
<li><strong>Authors: </strong>Hyunjong Ok, Taeho Kil, Sukmin Seo, Jaeho Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01914">https://arxiv.org/abs/2404.01914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01914">https://arxiv.org/pdf/2404.01914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01914]] SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity  Recognition of Unseen Entities(https://arxiv.org/abs/2404.01914)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in named entity recognition (NER) have pushed the boundary of the task to incorporate visual signals, leading to many variants, including multi-modal NER (MNER) or grounded MNER (GMNER). A key challenge to these tasks is that the model should be able to generalize to the entities unseen during the training, and should be able to handle the training samples with noisy annotations. To address this obstacle, we propose SCANNER (Span CANdidate detection and recognition for NER), a model capable of effectively handling all three NER variants. SCANNER is a two-stage structure; we extract entity candidates in the first stage and use it as a query to get knowledge, effectively pulling knowledge from various sources. We can boost our performance by utilizing this entity-centric extracted knowledge to address unseen entities. Furthermore, to tackle the challenges arising from noisy annotations in NER datasets, we introduce a novel self-distillation method, enhancing the robustness and accuracy of our model in processing training data with inherent uncertainties. Our approach demonstrates competitive performance on the NER benchmark and surpasses existing methods on both MNER and GMNER benchmarks. Further analysis shows that the proposed distillation and knowledge utilization methods improve the performance of our model on various benchmarks.</li>
</ul>

<h3>Title: A Rationale-centric Counterfactual Data Augmentation Method for  Cross-Document Event Coreference Resolution</h3>
<ul>
<li><strong>Authors: </strong>Bowen Ding, Qingkai Min, Shengkun Ma, Yingjie Li, Linyi Yang, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01921">https://arxiv.org/abs/2404.01921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01921">https://arxiv.org/pdf/2404.01921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01921]] A Rationale-centric Counterfactual Data Augmentation Method for  Cross-Document Event Coreference Resolution(https://arxiv.org/abs/2404.01921)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Based on Pre-trained Language Models (PLMs), event coreference resolution (ECR) systems have demonstrated outstanding performance in clustering coreferential events across documents. However, the existing system exhibits an excessive reliance on the `triggers lexical matching' spurious pattern in the input mention pair text. We formalize the decision-making process of the baseline ECR system using a Structural Causal Model (SCM), aiming to identify spurious and causal associations (i.e., rationales) within the ECR task. Leveraging the debiasing capability of counterfactual data augmentation, we develop a rationale-centric counterfactual data augmentation method with LLM-in-the-loop. This method is specialized for pairwise input in the ECR system, where we conduct direct interventions on triggers and context to mitigate the spurious association while emphasizing the causation. Our approach achieves state-of-the-art performance on three popular cross-document ECR benchmarks and demonstrates robustness in out-of-domain scenarios.</li>
</ul>

<h3>Title: SGSH: Stimulate Large Language Models with Skeleton Heuristics for  Knowledge Base Question Generation</h3>
<ul>
<li><strong>Authors: </strong>Shasha Guo, Lizi Liao, Jing Zhang, Yanling Wang, Cuiping Li, Hong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01923">https://arxiv.org/abs/2404.01923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01923">https://arxiv.org/pdf/2404.01923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01923]] SGSH: Stimulate Large Language Models with Skeleton Heuristics for  Knowledge Base Question Generation(https://arxiv.org/abs/2404.01923)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge base question generation (KBQG) aims to generate natural language questions from a set of triplet facts extracted from KB. Existing methods have significantly boosted the performance of KBQG via pre-trained language models (PLMs) thanks to the richly endowed semantic knowledge. With the advance of pre-training techniques, large language models (LLMs) (e.g., GPT-3.5) undoubtedly possess much more semantic knowledge. Therefore, how to effectively organize and exploit the abundant knowledge for KBQG becomes the focus of our study. In this work, we propose SGSH--a simple and effective framework to Stimulate GPT-3.5 with Skeleton Heuristics to enhance KBQG. The framework incorporates "skeleton heuristics", which provides more fine-grained guidance associated with each input to stimulate LLMs to generate optimal questions, encompassing essential elements like the question phrase and the auxiliary verb.More specifically, we devise an automatic data construction strategy leveraging ChatGPT to construct a skeleton training dataset, based on which we employ a soft prompting approach to train a BART model dedicated to generating the skeleton associated with each input. Subsequently, skeleton heuristics are encoded into the prompt to incentivize GPT-3.5 to generate desired questions. Extensive experiments demonstrate that SGSH derives the new state-of-the-art performance on the KBQG tasks.</li>
</ul>

<h3>Title: Toward Efficient Visual Gyroscopes: Spherical Moments, Harmonics  Filtering, and Masking Techniques for Spherical Camera Applications</h3>
<ul>
<li><strong>Authors: </strong>Yao Du, Carlos M. Mateo, Mirjana Maras, Tsun-Hsuan Wang, Marc Blanchon, Alexander Amini, Daniela Rus, Omar Tahri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01924">https://arxiv.org/abs/2404.01924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01924">https://arxiv.org/pdf/2404.01924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01924]] Toward Efficient Visual Gyroscopes: Spherical Moments, Harmonics  Filtering, and Masking Techniques for Spherical Camera Applications(https://arxiv.org/abs/2404.01924)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Unlike a traditional gyroscope, a visual gyroscope estimates camera rotation through images. The integration of omnidirectional cameras, offering a larger field of view compared to traditional RGB cameras, has proven to yield more accurate and robust results. However, challenges arise in situations that lack features, have substantial noise causing significant errors, and where certain features in the images lack sufficient strength, leading to less precise prediction results. Here, we address these challenges by introducing a novel visual gyroscope, which combines an analytical method with a neural network approach to provide a more efficient and accurate rotation estimation from spherical images. The presented method relies on three key contributions: an adapted analytical approach to compute the spherical moments coefficients, introduction of masks for better global feature representation, and the use of a multilayer perceptron to adaptively choose the best combination of masks and filters. Experimental results demonstrate superior performance of the proposed approach in terms of accuracy. The paper emphasizes the advantages of integrating machine learning to optimize analytical solutions, discusses limitations, and suggests directions for future research.</li>
</ul>

<h3>Title: Improving Bird's Eye View Semantic Segmentation by Task Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Tianhao Zhao, Yongcan Chen, Yu Wu, Tianyang Liu, Bo Du, Peilun Xiao, Shi Qiu, Hongda Yang, Guozhen Li, Yi Yang, Yutian Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01925">https://arxiv.org/abs/2404.01925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01925">https://arxiv.org/pdf/2404.01925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01925]] Improving Bird's Eye View Semantic Segmentation by Task Decomposition(https://arxiv.org/abs/2404.01925)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation in bird's eye view (BEV) plays a crucial role in autonomous driving. Previous methods usually follow an end-to-end pipeline, directly predicting the BEV segmentation map from monocular RGB inputs. However, the challenge arises when the RGB inputs and BEV targets from distinct perspectives, making the direct point-to-point predicting hard to optimize. In this paper, we decompose the original BEV segmentation task into two stages, namely BEV map reconstruction and RGB-BEV feature alignment. In the first stage, we train a BEV autoencoder to reconstruct the BEV segmentation maps given corrupted noisy latent representation, which urges the decoder to learn fundamental knowledge of typical BEV patterns. The second stage involves mapping RGB input images into the BEV latent space of the first stage, directly optimizing the correlations between the two views at the feature level. Our approach simplifies the complexity of combining perception and generation into distinct steps, equipping the model to handle intricate and challenging scenes effectively. Besides, we propose to transform the BEV segmentation map from the Cartesian to the polar coordinate system to establish the column-wise correspondence between RGB images and BEV maps. Moreover, our method requires neither multi-scale features nor camera intrinsic parameters for depth estimation and saves computational overhead. Extensive experiments on nuScenes and Argoverse show the effectiveness and efficiency of our method. Code is available at https://github.com/happytianhao/TaDe.</li>
</ul>

<h3>Title: Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs  in Translation</h3>
<ul>
<li><strong>Authors: </strong>Veronica Valeros, Anna ≈†irokova, Carlos Catania, Sebastian Garcia</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01940">https://arxiv.org/abs/2404.01940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01940">https://arxiv.org/pdf/2404.01940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01940]] Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs  in Translation(https://arxiv.org/abs/2404.01940)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Understanding cybercrime communications is paramount for cybersecurity defence. This often involves translating communications into English for processing, interpreting, and generating timely intelligence. The problem is that translation is hard. Human translation is slow, expensive, and scarce. Machine translation is inaccurate and biased. We propose using fine-tuned Large Language Models (LLM) to generate translations that can accurately capture the nuances of cybercrime language. We apply our technique to public chats from the NoName057(16) Russian-speaking hacktivist group. Our results show that our fine-tuned LLM model is better, faster, more accurate, and able to capture nuances of the language. Our method shows it is possible to achieve high-fidelity translations and significantly reduce costs by a factor ranging from 430 to 23,000 compared to a human translator.</li>
</ul>

<h3>Title: LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging</h3>
<ul>
<li><strong>Authors: </strong>Haoyang Ge, Qiao Feng, Hailong Jia, Xiongzheng Li, Xiangjun Yin, You Zhou, Jingyu Yang, Kun Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01941">https://arxiv.org/abs/2404.01941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01941">https://arxiv.org/pdf/2404.01941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01941]] LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging(https://arxiv.org/abs/2404.01941)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, extraction</a></li>
<li><strong>Abstract: </strong>Human pose and shape (HPS) estimation with lensless imaging is not only beneficial to privacy protection but also can be used in covert surveillance scenarios due to the small size and simple structure of this device. However, this task presents significant challenges due to the inherent ambiguity of the captured measurements and lacks effective methods for directly estimating human pose and shape from lensless data. In this paper, we propose the first end-to-end framework to recover 3D human poses and shapes from lensless measurements to our knowledge. We specifically design a multi-scale lensless feature decoder to decode the lensless measurements through the optically encoded mask for efficient feature extraction. We also propose a double-head auxiliary supervision mechanism to improve the estimation accuracy of human limb ends. Besides, we establish a lensless imaging system and verify the effectiveness of our method on various datasets acquired by our lensless imaging system.</li>
</ul>

<h3>Title: Lookahead Exploration with Neural Radiance Representation for Continuous  Vision-Language Navigation</h3>
<ul>
<li><strong>Authors: </strong>Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, Junjie Hu, Ming Jiang, Shuqiang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01943">https://arxiv.org/abs/2404.01943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01943">https://arxiv.org/pdf/2404.01943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01943]] Lookahead Exploration with Neural Radiance Representation for Continuous  Vision-Language Navigation(https://arxiv.org/abs/2404.01943)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. At each navigation step, the agent selects from possible candidate locations and then makes the move. For better navigation planning, the lookahead exploration strategy aims to effectively evaluate the agent's next action by accurately anticipating the future environment of candidate locations. To this end, some existing works predict RGB images for future environments, while this strategy suffers from image distortion and high computational cost. To address these issues, we propose the pre-trained hierarchical neural radiance representation model (HNR) to produce multi-level semantic features for future environments, which are more robust and efficient than pixel-wise RGB reconstruction. Furthermore, with the predicted future environmental representations, our lookahead VLN model is able to construct the navigable future path tree and select the optimal path via efficient parallel evaluation. Extensive experiments on the VLN-CE datasets confirm the effectiveness of our method.</li>
</ul>

<h3>Title: Event-assisted Low-Light Video Object Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hebei Li, Jin Wang, Jiahui Yuan, Yue Li, Wenming Weng, Yansong Peng, Yueyi Zhang, Zhiwei Xiong, Xiaoyan Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01945">https://arxiv.org/abs/2404.01945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01945">https://arxiv.org/pdf/2404.01945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01945]] Event-assisted Low-Light Video Object Segmentation(https://arxiv.org/abs/2404.01945)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In the realm of video object segmentation (VOS), the challenge of operating under low-light conditions persists, resulting in notably degraded image quality and compromised accuracy when comparing query and memory frames for similarity computation. Event cameras, characterized by their high dynamic range and ability to capture motion information of objects, offer promise in enhancing object visibility and aiding VOS methods under such low-light conditions. This paper introduces a pioneering framework tailored for low-light VOS, leveraging event camera data to elevate segmentation accuracy. Our approach hinges on two pivotal components: the Adaptive Cross-Modal Fusion (ACMF) module, aimed at extracting pertinent features while fusing image and event modalities to mitigate noise interference, and the Event-Guided Memory Matching (EGMM) module, designed to rectify the issue of inaccurate matching prevalent in low-light settings. Additionally, we present the creation of a synthetic LLE-DAVIS dataset and the curation of a real-world LLE-VOS dataset, encompassing frames and events. Experimental evaluations corroborate the efficacy of our method across both datasets, affirming its effectiveness in low-light scenarios.</li>
</ul>

<h3>Title: Automatic Wood Pith Detector: Local Orientation Estimation and Robust  Accumulation</h3>
<ul>
<li><strong>Authors: </strong>Henry Marichal, Diego Passarella, Gregory Randall</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01952">https://arxiv.org/abs/2404.01952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01952">https://arxiv.org/pdf/2404.01952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01952]] Automatic Wood Pith Detector: Local Orientation Estimation and Robust  Accumulation(https://arxiv.org/abs/2404.01952)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A fully automated technique for wood pith detection (APD), relying on the concentric shape of the structure of wood ring slices, is introduced. The method estimates the ring's local orientations using the 2D structure tensor and finds the pith position, optimizing a cost function designed for this problem. We also present a variant (APD-PCL), using the parallel coordinates space, that enhances the method's effectiveness when there are no clear tree ring patterns. Furthermore, refining previous work by Kurdthongmee, a YoloV8 net is trained for pith detection, producing a deep learning-based approach to the same problem (APD-DL). All methods were tested on seven datasets, including images captured under diverse conditions (controlled laboratory settings, sawmill, and forest) and featuring various tree species (Pinus taeda, Douglas fir, Abies alba, and Gleditsia triacanthos). All proposed approaches outperform existing state-of-the-art methods and can be used in CPU-based real-time applications. Additionally, we provide a novel dataset comprising images of gymnosperm and angiosperm species. Dataset and source code are available at this http URL</li>
</ul>

<h3>Title: HyperCLOVA X Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Kang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook Kang, Hyunwook Kim, Kyung-Min Kim, Munhyong Kim, Sungju Kim, Donghyun Kwak, Hanock Kwak, Se Jung Kwon, Bado Lee, Dongsoo Lee, Gichang Lee, Jooho Lee, Baeseong Park, Seongjin Shin, Joonsang Yu, Seolki Baek, Sumin Byeon, Eungsup Cho, Dooseok Choe, Jeesung Han, Youngkyun Jin, Hyein Jun, Jaeseung Jung, Chanwoong Kim, Jinhong Kim, Jinuk Kim, Dokyeong Lee, Dongwook Park, Jeong Min Sohn, Sujung Han, Jiae Heo, Sungju Hong, Mina Jeon, Hyunhoon Jung, Jungeun Jung, Wangkyo Jung, Chungjoon Kim, Hyeri Kim, Jonghyun Kim, Min Young Kim, Soeun Lee, Joonhee Park, Jieun Shin, Sojin Yang, Jungsoon Yoon, Hwaran Lee, Sanghwan Bae, Jeehwan Cha, Donghoon Ham, Youngki Hong, Yunki Hong, Myunggeun Ji, Yeguk Jin, Chansong Jo, Shinyoung Joo, Seunghwan Jung,  et al. (316 additional authors not shown)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01954">https://arxiv.org/abs/2404.01954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01954">https://arxiv.org/pdf/2404.01954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01954]] HyperCLOVA X Technical Report(https://arxiv.org/abs/2404.01954)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce HyperCLOVA X, a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding. HyperCLOVA X was trained on a balanced mix of Korean, English, and code data, followed by instruction-tuning with high-quality human-annotated datasets while abiding by strict safety guidelines reflecting our commitment to responsible AI. The model is evaluated across various benchmarks, including comprehensive reasoning, knowledge, commonsense, factuality, coding, math, chatting, instruction-following, and harmlessness, in both Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in Korean backed by a deep understanding of the language and cultural nuances. Further analysis of the inherent bilingual nature and its extension to multilingualism highlights the model's cross-lingual proficiency and strong generalization ability to untargeted languages, including machine translation between several language pairs and cross-lingual inference tasks. We believe that HyperCLOVA X can provide helpful guidance for regions or countries in developing their sovereign LLMs.</li>
</ul>

<h3>Title: MESEN: Exploit Multimodal Data to Design Unimodal Human Activity  Recognition with Few Labels</h3>
<ul>
<li><strong>Authors: </strong>Lilin Xu, Chaojie Gu, Rui Tan, Shibo He, Jiming Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01958">https://arxiv.org/abs/2404.01958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01958">https://arxiv.org/pdf/2404.01958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01958]] MESEN: Exploit Multimodal Data to Design Unimodal Human Activity  Recognition with Few Labels(https://arxiv.org/abs/2404.01958)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Human activity recognition (HAR) will be an essential function of various emerging applications. However, HAR typically encounters challenges related to modality limitations and label scarcity, leading to an application gap between current solutions and real-world requirements. In this work, we propose MESEN, a multimodal-empowered unimodal sensing framework, to utilize unlabeled multimodal data available during the HAR model design phase for unimodal HAR enhancement during the deployment phase. From a study on the impact of supervised multimodal fusion on unimodal feature extraction, MESEN is designed to feature a multi-task mechanism during the multimodal-aided pre-training stage. With the proposed mechanism integrating cross-modal feature contrastive learning and multimodal pseudo-classification aligning, MESEN exploits unlabeled multimodal data to extract effective unimodal features for each modality. Subsequently, MESEN can adapt to downstream unimodal HAR with only a few labeled samples. Extensive experiments on eight public multimodal datasets demonstrate that MESEN achieves significant performance improvements over state-of-the-art baselines in enhancing unimodal HAR by exploiting multimodal data.</li>
</ul>

<h3>Title: Bi-LORA: A Vision-Language Approach for Synthetic Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdenour Hadid, Abdelmalik Taleb-Ahmed</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01959">https://arxiv.org/abs/2404.01959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01959">https://arxiv.org/pdf/2404.01959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01959]] Bi-LORA: A Vision-Language Approach for Synthetic Image Detection(https://arxiv.org/abs/2404.01959)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs). We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM, notably bootstrapping language image pre-training (BLIP2). Rigorous and comprehensive experiments are conducted to validate the effectiveness of our proposed approach, particularly in detecting unseen diffusion-generated images from unknown diffusion-based generative models during training, showcasing robustness to noise, and demonstrating generalization capabilities to GANs. The obtained results showcase an impressive average accuracy of 93.41% in synthetic image detection on unseen generation models. The code and models associated with this research can be publicly accessed at https://github.com/Mamadou-Keita/VLM-DETECT.</li>
</ul>

<h3>Title: CAM-Based Methods Can See through Walls</h3>
<ul>
<li><strong>Authors: </strong>Magamed Taimeskhanov, Ronan Sicre, Damien Garreau</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01964">https://arxiv.org/abs/2404.01964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01964">https://arxiv.org/pdf/2404.01964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01964]] CAM-Based Methods Can See through Walls(https://arxiv.org/abs/2404.01964)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>CAM-based methods are widely-used post-hoc interpretability method that produce a saliency map to explain the decision of an image classification model. The saliency map highlights the important areas of the image relevant to the prediction. In this paper, we show that most of these methods can incorrectly attribute an important score to parts of the image that the model cannot see. We show that this phenomenon occurs both theoretically and experimentally. On the theory side, we analyze the behavior of GradCAM on a simple masked CNN model at initialization. Experimentally, we train a VGG-like model constrained to not use the lower part of the image and nevertheless observe positive scores in the unseen part of the image. This behavior is evaluated quantitatively on two new datasets. We believe that this is problematic, potentially leading to mis-interpretation of the model's behavior.</li>
</ul>

<h3>Title: Fashion Style Editing with Generative Human Prior</h3>
<ul>
<li><strong>Authors: </strong>Chaerin Kong, Seungyong Lee, Soohyeok Im, Wonsuk Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01984">https://arxiv.org/abs/2404.01984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01984">https://arxiv.org/pdf/2404.01984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01984]] Fashion Style Editing with Generative Human Prior(https://arxiv.org/abs/2404.01984)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Image editing has been a long-standing challenge in the research community with its far-reaching impact on numerous applications. Recently, text-driven methods started to deliver promising results in domains like human faces, but their applications to more complex domains have been relatively limited. In this work, we explore the task of fashion style editing, where we aim to manipulate the fashion style of human imagery using text descriptions. Specifically, we leverage a generative human prior and achieve fashion style editing by navigating its learned latent space. We first verify that the existing text-driven editing methods fall short for our problem due to their overly simplified guidance signal, and propose two directions to reinforce the guidance: textual augmentation and visual referencing. Combined with our empirical findings on the latent space structure, our Fashion Style Editing framework (FaSE) successfully projects abstract fashion concepts onto human images and introduces exciting new applications to the field.</li>
</ul>

<h3>Title: What is Point Supervision Worth in Video Instance Segmentation?</h3>
<ul>
<li><strong>Authors: </strong>Shuaiyi Huang, De-An Huang, Zhiding Yu, Shiyi Lan, Subhashree Radhakrishnan, Jose M. Alvarez, Abhinav Shrivastava, Anima Anandkumar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01990">https://arxiv.org/abs/2404.01990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01990">https://arxiv.org/pdf/2404.01990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01990]] What is Point Supervision Worth in Video Instance Segmentation?(https://arxiv.org/abs/2404.01990)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Video instance segmentation (VIS) is a challenging vision task that aims to detect, segment, and track objects in videos. Conventional VIS methods rely on densely-annotated object masks which are expensive. We reduce the human annotations to only one point for each object in a video frame during training, and obtain high-quality mask predictions close to fully supervised models. Our proposed training method consists of a class-agnostic proposal generation module to provide rich negative samples and a spatio-temporal point-based matcher to match the object queries with the provided point annotations. Comprehensive experiments on three VIS benchmarks demonstrate competitive performance of the proposed framework, nearly matching fully supervised methods.</li>
</ul>

<h3>Title: Kallaama: A Transcribed Speech Dataset about Agriculture in the Three  Most Widely Spoken Languages in Senegal</h3>
<ul>
<li><strong>Authors: </strong>Elodie Gauthier, Aminata Ndiaye, Abdoulaye Guiss√©</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01991">https://arxiv.org/abs/2404.01991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01991">https://arxiv.org/pdf/2404.01991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01991]] Kallaama: A Transcribed Speech Dataset about Agriculture in the Three  Most Widely Spoken Languages in Senegal(https://arxiv.org/abs/2404.01991)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>This work is part of the Kallaama project, whose objective is to produce and disseminate national languages corpora for speech technologies developments, in the field of agriculture. Except for Wolof, which benefits from some language data for natural language processing, national languages of Senegal are largely ignored by language technology providers. However, such technologies are keys to the protection, promotion and teaching of these languages. Kallaama focuses on the 3 main spoken languages by Senegalese people: Wolof, Pulaar and Sereer. These languages are widely spoken by the population, with around 10 million of native Senegalese speakers, not to mention those outside the country. However, they remain under-resourced in terms of machine-readable data that can be used for automatic processing and language technologies, all the more so in the agricultural sector. We release a transcribed speech dataset containing 125 hours of recordings, about agriculture, in each of the above-mentioned languages. These resources are specifically designed for Automatic Speech Recognition purpose, including traditional approaches. To build such technologies, we provide textual corpora in Wolof and Pulaar, and a pronunciation lexicon containing 49,132 entries from the Wolof dataset.</li>
</ul>

<h3>Title: AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug  Design</h3>
<ul>
<li><strong>Authors: </strong>Xinze Li, Penglei Wang, Tianfan Fu, Wenhao Gao, Chengtao Li, Leilei Shi, Junhong Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02003">https://arxiv.org/abs/2404.02003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02003">https://arxiv.org/pdf/2404.02003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02003]] AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug  Design(https://arxiv.org/abs/2404.02003)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, diffusion</a></li>
<li><strong>Abstract: </strong>Structure-based drug design (SBDD), which aims to generate molecules that can bind tightly to the target protein, is an essential problem in drug discovery, and previous approaches have achieved initial success. However, most existing methods still suffer from invalid local structure or unrealistic conformation issues, which are mainly due to the poor leaning of bond angles or torsional angles. To alleviate these problems, we propose AUTODIFF, a diffusion-based fragment-wise autoregressive generation model. Specifically, we design a novel molecule assembly strategy named conformal motif that preserves the conformation of local structures of molecules first, then we encode the interaction of the protein-ligand complex with an SE(3)-equivariant convolutional network and generate molecules motif-by-motif with diffusion modeling. In addition, we also improve the evaluation framework of SBDD by constraining the molecular weights of the generated molecules in the same range, together with some new metrics, which make the evaluation more fair and practical. Extensive experiments on CrossDocked2020 demonstrate that our approach outperforms the existing models in generating realistic molecules with valid structures and conformations while maintaining high binding affinity.</li>
</ul>

<h3>Title: Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi,  Tamil, and Indian English Online Spaces</h3>
<ul>
<li><strong>Authors: </strong>Advaitha Vetagiri, Gyandeep Kalita, Eisha Halder, Chetna Taparia, Partha Pakray, Riyanka Manna</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02013">https://arxiv.org/abs/2404.02013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02013">https://arxiv.org/pdf/2404.02013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02013]] Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi,  Tamil, and Indian English Online Spaces(https://arxiv.org/abs/2404.02013)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Online gender-based harassment is a widespread issue limiting the free expression and participation of women and marginalized genders in digital spaces. Detecting such abusive content can enable platforms to curb this menace. We participated in the Gendered Abuse Detection in Indic Languages shared task at ICON2023 that provided datasets of annotated Twitter posts in English, Hindi and Tamil for building classifiers to identify gendered abuse. Our team CNLP-NITS-PP developed an ensemble approach combining CNN and BiLSTM networks that can effectively model semantic and sequential patterns in textual data. The CNN captures localized features indicative of abusive language through its convolution filters applied on embedded input text. To determine context-based offensiveness, the BiLSTM analyzes this sequence for dependencies among words and phrases. Multiple variations were trained using FastText and GloVe word embeddings for each language dataset comprising over 7,600 crowdsourced annotations across labels for explicit abuse, targeted minority attacks and general offences. The validation scores showed strong performance across f1-measures, especially for English 0.84. Our experiments reveal how customizing embeddings and model hyperparameters can improve detection capability. The proposed architecture ranked 1st in the competition, proving its ability to handle real-world noisy text with code-switching. This technique has a promising scope as platforms aim to combat cyber harassment facing Indic language internet users. Our Code is at https://github.com/advaithavetagiri/CNLP-NITS-PP</li>
</ul>

<h3>Title: Improving Retrieval Augmented Open-Domain Question-Answering with  Vectorized Contexts</h3>
<ul>
<li><strong>Authors: </strong>Zhuo Chen, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, Kewei Tu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02022">https://arxiv.org/abs/2404.02022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02022">https://arxiv.org/pdf/2404.02022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02022]] Improving Retrieval Augmented Open-Domain Question-Answering with  Vectorized Contexts(https://arxiv.org/abs/2404.02022)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the era of large language models, applying techniques such as Retrieval Augmented Generation can better address Open-Domain Question-Answering problems. Due to constraints including model sizes and computing resources, the length of context is often limited, and it becomes challenging to empower the model to cover overlong contexts while answering questions from open domains. This paper proposes a general and convenient method to covering longer contexts in Open-Domain Question-Answering tasks. It leverages a small encoder language model that effectively encodes contexts, and the encoding applies cross-attention with origin inputs. With our method, the origin language models can cover several times longer contexts while keeping the computing requirements close to the baseline. Our experiments demonstrate that after fine-tuning, there is improved performance across two held-in datasets, four held-out datasets, and also in two In Context Learning settings.</li>
</ul>

<h3>Title: MultiParaDetox: Extending Text Detoxification with Parallel Data to New  Languages</h3>
<ul>
<li><strong>Authors: </strong>Daryna Dementieva, Nikolay Babakov, Alexander Panchenko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02037">https://arxiv.org/abs/2404.02037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02037">https://arxiv.org/pdf/2404.02037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02037]] MultiParaDetox: Extending Text Detoxification with Parallel Data to New  Languages(https://arxiv.org/abs/2404.02037)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text detoxification is a textual style transfer (TST) task where a text is paraphrased from a toxic surface form, e.g. featuring rude words, to the neutral register. Recently, text detoxification methods found their applications in various task such as detoxification of Large Language Models (LLMs) (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxic speech combating in social networks (Deng et al., 2023; Mun et al., 2023; Agarwal et al., 2023). All these applications are extremely important to ensure safe communication in modern digital worlds. However, the previous approaches for parallel text detoxification corpora collection -- ParaDetox (Logacheva et al., 2022) and APPADIA (Atwell et al., 2022) -- were explored only in monolingual setup. In this work, we aim to extend ParaDetox pipeline to multiple languages presenting MultiParaDetox to automate parallel detoxification corpus collection for potentially any language. Then, we experiment with different text detoxification models -- from unsupervised baselines to LLMs and fine-tuned models on the presented parallel corpora -- showing the great benefit of parallel corpus presence to obtain state-of-the-art text detoxification models for any language.</li>
</ul>

<h3>Title: Universal representations for financial transactional data: embracing  local, global, and external contexts</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Bazarova, Maria Kovaleva, Ilya Kuleshov, Evgenia Romanenkova, Alexander Stepikin, Alexandr Yugay, Dzhambulat Mollaev, Ivan Kireev, Andrey Savchenko, Alexey Zaytsev</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02047">https://arxiv.org/abs/2404.02047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02047">https://arxiv.org/pdf/2404.02047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02047]] Universal representations for financial transactional data: embracing  local, global, and external contexts(https://arxiv.org/abs/2404.02047)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Effective processing of financial transactions is essential for banking data analysis. However, in this domain, most methods focus on specialized solutions to stand-alone problems instead of constructing universal representations suitable for many problems. We present a representation learning framework that addresses diverse business challenges. We also suggest novel generative models that account for data specifics, and a way to integrate external information into a client's representation, leveraging insights from other customers' actions. Finally, we offer a benchmark, describing representation quality globally, concerning the entire transaction history; locally, reflecting the client's current state; and dynamically, capturing representation evolution over time. Our generative approach demonstrates superior performance in local tasks, with an increase in ROC-AUC of up to 14\% for the next MCC prediction task and up to 46\% for downstream tasks from existing contrastive baselines. Incorporating external information improves the scores by an additional 20\%.</li>
</ul>

<h3>Title: Noise Masking Attacks and Defenses for Pretrained Speech Models</h3>
<ul>
<li><strong>Authors: </strong>Matthew Jagielski, Om Thakkar, Lun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02052">https://arxiv.org/abs/2404.02052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02052">https://arxiv.org/pdf/2404.02052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02052]] Noise Masking Attacks and Defenses for Pretrained Speech Models(https://arxiv.org/abs/2404.02052)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack</a></li>
<li><strong>Abstract: </strong>Speech models are often trained on sensitive data in order to improve model performance, leading to potential privacy leakage. Our work considers noise masking attacks, introduced by Amid et al. 2022, which attack automatic speech recognition (ASR) models by requesting a transcript of an utterance which is partially replaced with noise. They show that when a record has been seen at training time, the model will transcribe the noisy record with its memorized sensitive transcript. In our work, we extend these attacks beyond ASR models, to attack pretrained speech encoders. Our method fine-tunes the encoder to produce an ASR model, and then performs noise masking on this model, which we find recovers private information from the pretraining data, despite the model never having seen transcripts at pretraining time! We show how to improve the precision of these attacks and investigate a number of countermeasures to our attacks.</li>
</ul>

<h3>Title: Deconstructing In-Context Learning: Understanding Prompts via Corruption</h3>
<ul>
<li><strong>Authors: </strong>Namrata Shivagunde, Vladislav Lialin, Sherin Muckatira, Anna Rumshisky</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02054">https://arxiv.org/abs/2404.02054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02054">https://arxiv.org/pdf/2404.02054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02054]] Deconstructing In-Context Learning: Understanding Prompts via Corruption(https://arxiv.org/abs/2404.02054)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The ability of large language models (LLMs) to "learn in context" based on the provided prompt has led to an explosive growth in their use, culminating in the proliferation of AI assistants such as ChatGPT, Claude, and Bard. These AI assistants are known to be robust to minor prompt modifications, mostly due to alignment techniques that use human feedback. In contrast, the underlying pre-trained LLMs they use as a backbone are known to be brittle in this respect. Building high-quality backbone models remains a core challenge, and a common approach to assessing their quality is to conduct few-shot evaluation. Such evaluation is notorious for being highly sensitive to minor prompt modifications, as well as the choice of specific in-context examples. Prior work has examined how modifying different elements of the prompt can affect model performance. However, these earlier studies tended to concentrate on a limited number of specific prompt attributes and often produced contradictory results. Additionally, previous research either focused on models with fewer than 15 billion parameters or exclusively examined black-box models like GPT-3 or PaLM, making replication challenging. In the present study, we decompose the entire prompt into four components: task description, demonstration inputs, labels, and inline instructions provided for each demonstration. We investigate the effects of structural and semantic corruptions of these elements on model performance. We study models ranging from 1.5B to 70B in size, using ten datasets covering classification and generation tasks. We find that repeating text within the prompt boosts model performance, and bigger models ($\geq$30B) are more sensitive to the semantics of the prompt. Finally, we observe that adding task and inline instructions to the demonstrations enhances model performance even when the instructions are semantically corrupted.</li>
</ul>

<h3>Title: Long-context LLMs Struggle with Long In-context Learning</h3>
<ul>
<li><strong>Authors: </strong>Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, Wenhu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02060">https://arxiv.org/abs/2404.02060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02060">https://arxiv.org/pdf/2404.02060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02060]] Long-context LLMs Struggle with Long In-context Learning(https://arxiv.org/abs/2404.02060)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification. We meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction. We evaluate 13 long-context LLMs on our benchmarks. We find that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window. However, after the context window exceeds 20K, most LLMs except GPT-4 will dip dramatically. This suggests a notable gap in current LLM capabilities for processing and understanding long, context-rich sequences. Further analysis revealed a tendency among models to favor predictions for labels presented towards the end at the sequence. Their ability to reason over multiple pieces in the long sequence is yet to be improved. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LIConBench could serve as a more realistic evaluation for the future long context LLMs.</li>
</ul>

<h3>Title: Digital Forgetting in Large Language Models: A Survey of Unlearning  Methods</h3>
<ul>
<li><strong>Authors: </strong>Alberto Blanco-Justicia, Najeeb Jebreel, Benet Manzanares, David S√°nchez, Josep Domingo-Ferrer, Guillem Collell, Kuan Eeik Tan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02062">https://arxiv.org/abs/2404.02062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02062">https://arxiv.org/pdf/2404.02062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02062]] Digital Forgetting in Large Language Models: A Survey of Unlearning  Methods(https://arxiv.org/abs/2404.02062)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>The objective of digital forgetting is, given a model with undesirable knowledge or behavior, obtain a new model where the detected issues are no longer present. The motivations for forgetting include privacy protection, copyright protection, elimination of biases and discrimination, and prevention of harmful content generation. Effective digital forgetting has to be effective (meaning how well the new model has forgotten the undesired knowledge/behavior), retain the performance of the original model on the desirable tasks, and be scalable (in particular forgetting has to be more efficient than retraining from scratch on just the tasks/data to be retained). This survey focuses on forgetting in large language models (LLMs). We first provide background on LLMs, including their components, the types of LLMs, and their usual training pipeline. Second, we describe the motivations, types, and desired properties of digital forgetting. Third, we introduce the approaches to digital forgetting in LLMs, among which unlearning methodologies stand out as the state of the art. Fourth, we provide a detailed taxonomy of machine unlearning methods for LLMs, and we survey and compare current approaches. Fifth, we detail datasets, models and metrics used for the evaluation of forgetting, retaining and runtime. Sixth, we discuss challenges in the area. Finally, we provide some concluding remarks.</li>
</ul>

<h3>Title: Multi-Level Label Correction by Distilling Proximate Patterns for  Semi-supervised Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hui Xiao, Yuting Hong, Li Dong, Diqun Yan, Jiayan Zhuang, Junjie Xiong, Dongtai Liang, Chengbin Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02065">https://arxiv.org/abs/2404.02065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02065">https://arxiv.org/pdf/2404.02065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02065]] Multi-Level Label Correction by Distilling Proximate Patterns for  Semi-supervised Semantic Segmentation(https://arxiv.org/abs/2404.02065)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semi-supervised semantic segmentation relieves the reliance on large-scale labeled data by leveraging unlabeled data. Recent semi-supervised semantic segmentation approaches mainly resort to pseudo-labeling methods to exploit unlabeled data. However, unreliable pseudo-labeling can undermine the semi-supervision processes. In this paper, we propose an algorithm called Multi-Level Label Correction (MLLC), which aims to use graph neural networks to capture structural relationships in Semantic-Level Graphs (SLGs) and Class-Level Graphs (CLGs) to rectify erroneous pseudo-labels. Specifically, SLGs represent semantic affinities between pairs of pixel features, and CLGs describe classification consistencies between pairs of pixel labels. With the support of proximate pattern information from graphs, MLLC can rectify incorrectly predicted pseudo-labels and can facilitate discriminative feature representations. We design an end-to-end network to train and perform this effective label corrections mechanism. Experiments demonstrate that MLLC can significantly improve supervised baselines and outperforms state-of-the-art approaches in different scenarios on Cityscapes and PASCAL VOC 2012 datasets. Specifically, MLLC improves the supervised baseline by at least 5% and 2% with DeepLabV2 and DeepLabV3+ respectively under different partition protocols.</li>
</ul>

<h3>Title: Red-Teaming Segment Anything Model</h3>
<ul>
<li><strong>Authors: </strong>Krzysztof Jankowski, Bartlomiej Sobieski, Mateusz Kwiatkowski, Jakub Szulc, Michal Janik, Hubert Baniecki, Przemyslaw Biecek</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02067">https://arxiv.org/abs/2404.02067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02067">https://arxiv.org/pdf/2404.02067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02067]] Red-Teaming Segment Anything Model(https://arxiv.org/abs/2404.02067)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, segmentation</a></li>
<li><strong>Abstract: </strong>Foundation models have emerged as pivotal tools, tackling many complex tasks through pre-training on vast datasets and subsequent fine-tuning for specific applications. The Segment Anything Model is one of the first and most well-known foundation models for computer vision segmentation tasks. This work presents a multi-faceted red-teaming analysis that tests the Segment Anything Model against challenging tasks: (1) We analyze the impact of style transfer on segmentation masks, demonstrating that applying adverse weather conditions and raindrops to dashboard images of city roads significantly distorts generated masks. (2) We focus on assessing whether the model can be used for attacks on privacy, such as recognizing celebrities' faces, and show that the model possesses some undesired knowledge in this task. (3) Finally, we check how robust the model is to adversarial attacks on segmentation masks under text prompts. We not only show the effectiveness of popular white-box attacks and resistance to black-box attacks but also introduce a novel approach - Focused Iterative Gradient Attack (FIGA) that combines white-box approaches to construct an efficient attack resulting in a smaller number of modified pixels. All of our testing methods and analyses indicate a need for enhanced safety measures in foundation models for image segmentation.</li>
</ul>

<h3>Title: EGTR: Extracting Graph from Transformer for Scene Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinbae Im, JeongYeon Nam, Nokyung Park, Hyungmin Lee, Seunghyun Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02072">https://arxiv.org/abs/2404.02072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02072">https://arxiv.org/pdf/2404.02072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02072]] EGTR: Extracting Graph from Transformer for Scene Graph Generation(https://arxiv.org/abs/2404.02072)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Scene Graph Generation (SGG) is a challenging task of detecting objects and predicting relationships between objects. After DETR was developed, one-stage SGG models based on a one-stage object detector have been actively studied. However, complex modeling is used to predict the relationship between objects, and the inherent relationship between object queries learned in the multi-head self-attention of the object detector has been neglected. We propose a lightweight one-stage SGG model that extracts the relation graph from the various relationships learned in the multi-head self-attention layers of the DETR decoder. By fully utilizing the self-attention by-products, the relation graph can be extracted effectively with a shallow relation extraction head. Considering the dependency of the relation extraction task on the object detection task, we propose a novel relation smoothing technique that adjusts the relation label adaptively according to the quality of the detected objects. By the relation smoothing, the model is trained according to the continuous curriculum that focuses on object detection task at the beginning of training and performs multi-task learning as the object detection performance gradually improves. Furthermore, we propose a connectivity prediction task that predicts whether a relation exists between object pairs as an auxiliary task of the relation extraction. We demonstrate the effectiveness and efficiency of our method for the Visual Genome and Open Image V6 datasets. Our code is publicly available at https://github.com/naver-ai/egtr .</li>
</ul>

<h3>Title: WcDT: World-centric Diffusion Transformer for Traffic Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Chen Yang, Aaron Xuxiang Tian, Dong Chen, Tianyu Shi, Arsalan Heydarian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02082">https://arxiv.org/abs/2404.02082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02082">https://arxiv.org/pdf/2404.02082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02082]] WcDT: World-centric Diffusion Transformer for Traffic Scene Generation(https://arxiv.org/abs/2404.02082)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce a novel approach for autonomous driving trajectory generation by harnessing the complementary strengths of diffusion probabilistic models (a.k.a., diffusion models) and transformers. Our proposed framework, termed the "World-Centric Diffusion Transformer" (WcDT), optimizes the entire trajectory generation process, from feature extraction to model inference. To enhance the scene diversity and stochasticity, the historical trajectory data is first preprocessed and encoded into latent space using Denoising Diffusion Probabilistic Models (DDPM) enhanced with Diffusion with Transformer (DiT) blocks. Then, the latent features, historical trajectories, HD map features, and historical traffic signal information are fused with various transformer-based encoders. The encoded traffic scenes are then decoded by a trajectory decoder to generate multimodal future trajectories. Comprehensive experimental results show that the proposed approach exhibits superior performance in generating both realistic and diverse trajectories, showing its potential for integration into automatic driving simulation systems.</li>
</ul>

<h3>Title: Adaptive Feature Fusion Neural Network for Glaucoma Segmentation on  Unseen Fundus Images</h3>
<ul>
<li><strong>Authors: </strong>Jiyuan Zhong, Hu Ke, Ming Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02084">https://arxiv.org/abs/2404.02084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02084">https://arxiv.org/pdf/2404.02084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02084]] Adaptive Feature Fusion Neural Network for Glaucoma Segmentation on  Unseen Fundus Images(https://arxiv.org/abs/2404.02084)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Fundus image segmentation on unseen domains is challenging, especially for the over-parameterized deep models trained on the small medical datasets. To address this challenge, we propose a method named Adaptive Feature-fusion Neural Network (AFNN) for glaucoma segmentation on unseen domains, which mainly consists of three modules: domain adaptor, feature-fusion network, and self-supervised multi-task learning. Specifically, the domain adaptor helps the pretrained-model fast adapt from other image domains to the medical fundus image domain. Feature-fusion network and self-supervised multi-task learning for the encoder and decoder are introduced to improve the domain generalization ability. In addition, we also design the weighted-dice-loss to improve model performance on complex optic-cup segmentation tasks. Our proposed method achieves a competitive performance over existing fundus segmentation methods on four public glaucoma datasets.</li>
</ul>

<h3>Title: LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause  Pair Extraction as Sequence Labelling Task</h3>
<ul>
<li><strong>Authors: </strong>Suyash Vardhan Mathur, Akshett Rai Jindal, Hardik Mittal, Manish Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02088">https://arxiv.org/abs/2404.02088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02088">https://arxiv.org/pdf/2404.02088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02088]] LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause  Pair Extraction as Sequence Labelling Task(https://arxiv.org/abs/2404.02088)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Conversation is the most natural form of human communication, where each utterance can range over a variety of possible emotions. While significant work has been done towards the detection of emotions in text, relatively little work has been done towards finding the cause of the said emotions, especially in multimodal settings. SemEval 2024 introduces the task of Multimodal Emotion Cause Analysis in Conversations, which aims to extract emotions reflected in individual utterances in a conversation involving multiple modalities (textual, audio, and visual modalities) along with the corresponding utterances that were the cause for the emotion. In this paper, we propose models that tackle this task as an utterance labeling and a sequence labeling problem and perform a comparative study of these models, involving baselines using different encoders, using BiLSTM for adding contextual information of the conversation, and finally adding a CRF layer to try to model the inter-dependencies between adjacent utterances more effectively. In the official leaderboard for the task, our architecture was ranked 8th, achieving an F1-score of 0.1759 on the leaderboard.</li>
</ul>

<h3>Title: CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions  for RAG systems</h3>
<ul>
<li><strong>Authors: </strong>Sara Rosenthal, Avirup Sil, Radu Florian, Salim Roukos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02103">https://arxiv.org/abs/2404.02103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02103">https://arxiv.org/pdf/2404.02103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02103]] CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions  for RAG systems(https://arxiv.org/abs/2404.02103)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval Augmented Generation (RAG) has become a popular application for large language models. It is preferable that successful RAG systems provide accurate answers that are supported by being grounded in a passage without any hallucinations. While considerable work is required for building a full RAG pipeline, being able to benchmark performance is also necessary. We present ClapNQ, a benchmark Long-form Question Answering dataset for the full RAG pipeline. ClapNQ includes long answers with grounded gold passages from Natural Questions (NQ) and a corpus to perform either retrieval, generation, or the full RAG pipeline. The ClapNQ answers are concise, 3x smaller than the full passage, and cohesive, with multiple pieces of the passage that are not contiguous. RAG models must adapt to these properties to be successful at ClapNQ. We present baseline experiments and analysis for ClapNQ that highlight areas where there is still significant room for improvement in grounded RAG. CLAPNQ is publicly available at https://github.com/primeqa/clapnq</li>
</ul>

<h3>Title: Pre-trained Vision and Language Transformers Are Few-Shot Incremental  Learners</h3>
<ul>
<li><strong>Authors: </strong>Keon-Hee Park, Kyungwoo Song, Gyeong-Moon Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02117">https://arxiv.org/abs/2404.02117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02117">https://arxiv.org/pdf/2404.02117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02117]] Pre-trained Vision and Language Transformers Are Few-Shot Incremental  Learners(https://arxiv.org/abs/2404.02117)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Few-Shot Class Incremental Learning (FSCIL) is a task that requires a model to learn new classes incrementally without forgetting when only a few samples for each class are given. FSCIL encounters two significant challenges: catastrophic forgetting and overfitting, and these challenges have driven prior studies to primarily rely on shallow models, such as ResNet-18. Even though their limited capacity can mitigate both forgetting and overfitting issues, it leads to inadequate knowledge transfer during few-shot incremental sessions. In this paper, we argue that large models such as vision and language transformers pre-trained on large datasets can be excellent few-shot incremental learners. To this end, we propose a novel FSCIL framework called PriViLege, Pre-trained Vision and Language transformers with prompting functions and knowledge distillation. Our framework effectively addresses the challenges of catastrophic forgetting and overfitting in large models through new pre-trained knowledge tuning (PKT) and two losses: entropy-based divergence loss and semantic knowledge distillation loss. Experimental results show that the proposed PriViLege significantly outperforms the existing state-of-the-art methods with a large margin, e.g., +9.38% in CUB200, +20.58% in CIFAR-100, and +13.36% in miniImageNet. Our implementation code is available at https://github.com/KHU-AGI/PriViLege.</li>
</ul>

<h3>Title: Exploring Automated Distractor Generation for Math Multiple-choice  Questions via Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wanyong Feng, Jaewook Lee, Hunter McNichols, Alexander Scarlatos, Digory Smith, Simon Woodhead, Nancy Otero Ornelas, Andrew Lan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02124">https://arxiv.org/abs/2404.02124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02124">https://arxiv.org/pdf/2404.02124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02124]] Exploring Automated Distractor Generation for Math Multiple-choice  Questions via Large Language Models(https://arxiv.org/abs/2404.02124)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable format in assessments and practices. One of the most important aspects of MCQs is the distractors, i.e., incorrect options that are designed to target common errors or misconceptions among real students. To date, the task of crafting high-quality distractors largely remains a labor and time-intensive process for teachers and learning content designers, which has limited scalability. In this work, we study the task of automated distractor generation in the domain of math MCQs and explore a wide variety of large language model (LLM)-based approaches, from in-context learning to fine-tuning. We conduct extensive experiments using a real-world math MCQ dataset and find that although LLMs can generate some mathematically valid distractors, they are less adept at anticipating common errors or misconceptions among real students.</li>
</ul>

<h3>Title: 3D Congealing: 3D-Aware Image Alignment in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Yunzhi Zhang, Zizhang Li, Amit Raj, Andreas Engelhardt, Yuanzhen Li, Tingbo Hou, Jiajun Wu, Varun Jampani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02125">https://arxiv.org/abs/2404.02125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02125">https://arxiv.org/pdf/2404.02125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02125]] 3D Congealing: 3D-Aware Image Alignment in the Wild(https://arxiv.org/abs/2404.02125)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose 3D Congealing, a novel problem of 3D-aware alignment for 2D images capturing semantically similar objects. Given a collection of unlabeled Internet images, our goal is to associate the shared semantic parts from the inputs and aggregate the knowledge from 2D images to a shared 3D canonical space. We introduce a general framework that tackles the task without assuming shape templates, poses, or any camera parameters. At its core is a canonical 3D representation that encapsulates geometric and semantic information. The framework optimizes for the canonical representation together with the pose for each input image, and a per-image coordinate map that warps 2D pixel coordinates to the 3D canonical frame to account for the shape matching. The optimization procedure fuses prior knowledge from a pre-trained image generative model and semantic information from input images. The former provides strong knowledge guidance for this under-constraint task, while the latter provides the necessary information to mitigate the training data bias from the pre-trained model. Our framework can be used for various tasks such as correspondence matching, pose estimation, and image editing, achieving strong results on real-world image datasets under challenging illumination conditions and on in-the-wild online image collections.</li>
</ul>

<h3>Title: Rematch: Robust and Efficient Matching of Local Knowledge Graphs to  Improve Structural and Semantic Similarity</h3>
<ul>
<li><strong>Authors: </strong>Zoher Kachwala, Jisun An, Haewoon Kwak, Filippo Menczer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02126">https://arxiv.org/abs/2404.02126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02126">https://arxiv.org/pdf/2404.02126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02126]] Rematch: Robust and Efficient Matching of Local Knowledge Graphs to  Improve Structural and Semantic Similarity(https://arxiv.org/abs/2404.02126)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Knowledge graphs play a pivotal role in various applications, such as question-answering and fact-checking. Abstract Meaning Representation (AMR) represents text as knowledge graphs. Evaluating the quality of these graphs involves matching them structurally to each other and semantically to the source text. Existing AMR metrics are inefficient and struggle to capture semantic similarity. We also lack a systematic evaluation benchmark for assessing structural similarity between AMR graphs. To overcome these limitations, we introduce a novel AMR similarity metric, rematch, alongside a new evaluation for structural similarity called RARE. Among state-of-the-art metrics, rematch ranks second in structural similarity; and first in semantic similarity by 1--5 percentage points on the STS-B and SICK-R benchmarks. Rematch is also five times faster than the next most efficient metric.</li>
</ul>

<h3>Title: ViTamin: Designing Scalable Vision Models in the Vision-Language Era</h3>
<ul>
<li><strong>Authors: </strong>Jienneg Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, Liang-Chieh Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02132">https://arxiv.org/abs/2404.02132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02132">https://arxiv.org/pdf/2404.02132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02132]] ViTamin: Designing Scalable Vision Models in the Vision-Language Era(https://arxiv.org/abs/2404.02132)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in vision-language models (VLMs) start a new page in the vision community. The VLMs provide stronger and more generalizable feature embeddings compared to those from ImageNet-pretrained models, thanks to the training on the large-scale Internet image-text pairs. However, despite the amazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remain the default choice for the image encoder. Although pure transformer proves its effectiveness in the text encoding area, it remains questionable whether it is also the case for image encoding, especially considering that various types of networks are proposed on the ImageNet benchmark, which, unfortunately, are rarely studied in VLMs. Due to small data/model scale, the original conclusions of model design on ImageNet can be limited and biased. In this paper, we aim at building an evaluation protocol of vision models in the vision-language era under the contrastive language-image pretraining (CLIP) framework. We provide a comprehensive way to benchmark different vision models, covering their zero-shot performance and scalability in both model and training data sizes. To this end, we introduce ViTamin, a new vision models tailored for VLMs. ViTamin-L significantly outperforms ViT-L by 2.0% ImageNet zero-shot accuracy, when using the same publicly available DataComp-1B dataset and the same OpenCLIP training scheme. ViTamin-L presents promising results on 60 diverse benchmarks, including classification, retrieval, open-vocabulary detection and segmentation, and large multi-modal models. When further scaling up the model size, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shot accuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters (4.4B).</li>
</ul>

<h3>Title: Topic-based Watermarks for LLM-Generated Text</h3>
<ul>
<li><strong>Authors: </strong>Alexander Nemecek, Yuzhou Jiang, Erman Ayday</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02138">https://arxiv.org/abs/2404.02138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02138">https://arxiv.org/pdf/2404.02138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02138]] Topic-based Watermarks for LLM-Generated Text(https://arxiv.org/abs/2404.02138)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a "topic-based watermarking algorithm" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked LLM. Inspired from previous work, we propose using a pair of lists (that are generated based on the specified extracted topic(s)) that specify certain tokens to be included or excluded while generating the watermarked output of the LLM. Using the proposed watermarking algorithm, we show the practicality of a watermark detection algorithm. Furthermore, we discuss a wide range of attacks that can emerge against watermarking algorithms for LLMs and the benefit of the proposed watermarking scheme for the feasibility of modeling a potential attacker considering its benefit vs. loss.</li>
</ul>

<h3>Title: Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of  Orthogonal Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Yang, Zijie Pan, Chun Gu, Li Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02148">https://arxiv.org/abs/2404.02148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02148">https://arxiv.org/pdf/2404.02148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02148]] Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of  Orthogonal Diffusion Models(https://arxiv.org/abs/2404.02148)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in 3D generation are predominantly propelled by improvements in 3D-aware image diffusion models which are pretrained on Internet-scale image data and fine-tuned on massive 3D data, offering the capability of producing highly consistent multi-view images. However, due to the scarcity of synchronized multi-view video data, it is impractical to adapt this paradigm to 4D generation directly. Despite that, the available video and 3D data are adequate for training video and multi-view diffusion models that can provide satisfactory dynamic and geometric priors respectively. In this paper, we present Diffusion$^2$, a novel framework for dynamic 3D content creation that leverages the knowledge about geometric consistency and temporal smoothness from these models to directly sample dense multi-view and multi-frame images which can be employed to optimize continuous 4D representation. Specifically, we design a simple yet effective denoising strategy via score composition of video and multi-view diffusion models based on the probability structure of the images to be generated. Owing to the high parallelism of the image generation and the efficiency of the modern 4D reconstruction pipeline, our framework can generate 4D content within few minutes. Furthermore, our method circumvents the reliance on 4D data, thereby having the potential to benefit from the scalability of the foundation video and multi-view diffusion models. Extensive experiments demonstrate the efficacy of our proposed framework and its capability to flexibly adapt to various types of prompts.</li>
</ul>

<h3>Title: From Seaweed to Security: The Emergence of Alginate in Compromising IoT  Fingerprint Sensors</h3>
<ul>
<li><strong>Authors: </strong>Pouria Rad, Gokila Dorai, Mohsen Jozani</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02150">https://arxiv.org/abs/2404.02150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02150">https://arxiv.org/pdf/2404.02150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02150]] From Seaweed to Security: The Emergence of Alginate in Compromising IoT  Fingerprint Sensors(https://arxiv.org/abs/2404.02150)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, biometric</a></li>
<li><strong>Abstract: </strong>The increasing integration of capacitive fingerprint recognition sensors in IoT devices presents new challenges in digital forensics, particularly in the context of advanced fingerprint spoofing. Previous research has highlighted the effectiveness of materials such as latex and silicone in deceiving biometric systems. In this study, we introduce Alginate, a biopolymer derived from brown seaweed, as a novel material with the potential for spoofing IoT-specific capacitive fingerprint sensors. Our research uses Alginate and cutting-edge image recognition techniques to unveil a nuanced IoT vulnerability that raises significant security and privacy concerns. Our proof-of-concept experiments employed authentic fingerprint molds to create Alginate replicas, which exhibited remarkable visual and tactile similarities to real fingerprints. The conductivity and resistivity properties of Alginate, closely resembling human skin, make it a subject of interest in the digital forensics field, especially regarding its ability to spoof IoT device sensors. This study calls upon the digital forensics community to develop advanced anti-spoofing strategies to protect the evolving IoT infrastructure against such sophisticated threats.</li>
</ul>

<h3>Title: Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks</h3>
<ul>
<li><strong>Authors: </strong>Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02151">https://arxiv.org/abs/2404.02151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02151">https://arxiv.org/pdf/2404.02151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02151]] Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks(https://arxiv.org/abs/2404.02151)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the SaTML'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and in some settings it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). We provide the code, prompts, and logs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks.</li>
</ul>

<h3>Title: GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from  a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02152">https://arxiv.org/abs/2404.02152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02152">https://arxiv.org/pdf/2404.02152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02152]] GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from  a Single Image(https://arxiv.org/abs/2404.02152)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: https://zju3dv.github.io/geneavatar/</li>
</ul>

<h3>Title: Dynamic Pre-training: Towards Efficient and Scalable All-in-One Image  Restoration</h3>
<ul>
<li><strong>Authors: </strong>Akshay Dudhane, Omkar Thawakar, Syed Waqas Zamir, Salman Khan, Fahad Shahbaz Khan, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02154">https://arxiv.org/abs/2404.02154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02154">https://arxiv.org/pdf/2404.02154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02154]] Dynamic Pre-training: Towards Efficient and Scalable All-in-One Image  Restoration(https://arxiv.org/abs/2404.02154)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>All-in-one image restoration tackles different types of degradations with a unified model instead of having task-specific, non-generic models for each degradation. The requirement to tackle multiple degradations using the same model can lead to high-complexity designs with fixed configuration that lack the adaptability to more efficient alternatives. We propose DyNet, a dynamic family of networks designed in an encoder-decoder style for all-in-one image restoration tasks. Our DyNet can seamlessly switch between its bulkier and lightweight variants, thereby offering flexibility for efficient model deployment with a single round of training. This seamless switching is enabled by our weights-sharing mechanism, forming the core of our architecture and facilitating the reuse of initialized module weights. Further, to establish robust weights initialization, we introduce a dynamic pre-training strategy that trains variants of the proposed DyNet concurrently, thereby achieving a 50% reduction in GPU hours. To tackle the unavailability of large-scale dataset required in pre-training, we curate a high-quality, high-resolution image dataset named Million-IRD having 2M image samples. We validate our DyNet for image denoising, deraining, and dehazing in all-in-one setting, achieving state-of-the-art results with 31.34% reduction in GFlops and a 56.75% reduction in parameters compared to baseline models. The source codes and trained models are available at https://github.com/akshaydudhane16/DyNet.</li>
</ul>

<h3>Title: Alpha Invariance: On Inverse Scaling Between Distance and Volume Density  in Neural Radiance Fields</h3>
<ul>
<li><strong>Authors: </strong>Joshua Ahn, Haochen Wang, Raymond A. Yeh, Greg Shakhnarovich</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02155">https://arxiv.org/abs/2404.02155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02155">https://arxiv.org/pdf/2404.02155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02155]] Alpha Invariance: On Inverse Scaling Between Distance and Volume Density  in Neural Radiance Fields(https://arxiv.org/abs/2404.02155)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of volumetric densities in neural radiance fields, i.e., the densities double when scene size is halved, and vice versa. We call this property alpha invariance. For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing both distance and volume densities in log space, and 2) a discretization-agnostic initialization strategy to guarantee high ray transmittance. We revisit a few popular radiance field models and find that these systems use various heuristics to deal with issues arising from scene scaling. We test their behaviors and show our recipe to be more robust.</li>
</ul>

<h3>Title: Segment Any 3D Object with Language</h3>
<ul>
<li><strong>Authors: </strong>Seungjun Lee, Yuyang Zhao, Gim Hee Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02157">https://arxiv.org/abs/2404.02157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02157">https://arxiv.org/pdf/2404.02157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02157]] Segment Any 3D Object with Language(https://arxiv.org/abs/2404.02157)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate Open-Vocabulary 3D Instance Segmentation (OV-3DIS) with free-form language instructions. Earlier works that rely on only annotated base categories for training suffer from limited generalization to unseen novel categories. Recent works mitigate poor generalizability to novel categories by generating class-agnostic masks or projecting generalized masks from 2D to 3D, but disregard semantic or geometry information, leading to sub-optimal performance. Instead, generating generalizable but semantic-related masks directly from 3D point clouds would result in superior outcomes. In this paper, we introduce Segment any 3D Object with LanguagE (SOLE), which is a semantic and geometric-aware visual-language learning framework with strong generalizability by generating semantic-related masks directly from 3D point clouds. Specifically, we propose a multimodal fusion network to incorporate multimodal semantics in both backbone and decoder. In addition, to align the 3D segmentation model with various language instructions and enhance the mask quality, we introduce three types of multimodal associations as supervision. Our SOLE outperforms previous methods by a large margin on ScanNetv2, ScanNet200, and Replica benchmarks, and the results are even close to the fully-supervised counterpart despite the absence of class annotations in the training. Furthermore, extensive qualitative results demonstrate the versatility of our SOLE to language instructions.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
