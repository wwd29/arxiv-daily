<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-08-20</h1>
<h3>Title: Fair Play in the Newsroom: Actor-Based Filtering Gender Discrimination in Text Corpora</h3>
<ul>
<li><strong>Authors: </strong>Stefanie Urchs, Veronika Thurner, Matthias Aßenmacher, Christian Heumann, Stephanie Thiemichen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13169">https://arxiv.org/abs/2508.13169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13169">https://arxiv.org/pdf/2508.13169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13169]] Fair Play in the Newsroom: Actor-Based Filtering Gender Discrimination in Text Corpora(https://arxiv.org/abs/2508.13169)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large language models are increasingly shaping digital communication, yet their outputs often reflect structural gender imbalances that originate from their training data. This paper presents an extended actor-level pipeline for detecting and mitigating gender discrimination in large-scale text corpora. Building on prior work in discourse-aware fairness analysis, we introduce new actor-level metrics that capture asymmetries in sentiment, syntactic agency, and quotation styles. The pipeline supports both diagnostic corpus analysis and exclusion-based balancing, enabling the construction of fairer corpora. We apply our approach to the taz2024full corpus of German newspaper articles from 1980 to 2024, demonstrating substantial improvements in gender balance across multiple linguistic dimensions. Our results show that while surface-level asymmetries can be mitigated through filtering and rebalancing, subtler forms of bias persist, particularly in sentiment and framing. We release the tools and reports to support further research in discourse-based fairness auditing and equitable corpus construction.</li>
</ul>

<h3>Title: Contextual Attention-Based Multimodal Fusion of LLM and CNN for Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Meriem Zerkouk, Miloud Mihoubi, Belkacem Chikhaoui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13196">https://arxiv.org/abs/2508.13196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13196">https://arxiv.org/pdf/2508.13196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13196]] Contextual Attention-Based Multimodal Fusion of LLM and CNN for Sentiment Analysis(https://arxiv.org/abs/2508.13196)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach for multimodal sentiment analysis on social media, particularly in the context of natural disasters, where understanding public sentiment is crucial for effective crisis management. Unlike conventional methods that process text and image modalities separately, our approach seamlessly integrates Convolutional Neural Network (CNN) based image analysis with Large Language Model (LLM) based text processing, leveraging Generative Pre-trained Transformer (GPT) and prompt engineering to extract sentiment relevant features from the CrisisMMD dataset. To effectively model intermodal relationships, we introduce a contextual attention mechanism within the fusion process. Leveraging contextual-attention layers, this mechanism effectively captures intermodality interactions, enhancing the model's comprehension of complex relationships between textual and visual data. The deep neural network architecture of our model learns from these fused features, leading to improved accuracy compared to existing baselines. Experimental results demonstrate significant advancements in classifying social media data into informative and noninformative categories across various natural disasters. Our model achieves a notable 2.43% increase in accuracy and 5.18% in F1-score, highlighting its efficacy in processing complex multimodal data. Beyond quantitative metrics, our approach provides deeper insight into the sentiments expressed during crises. The practical implications extend to real time disaster management, where enhanced sentiment analysis can optimize the accuracy of emergency interventions. By bridging the gap between multimodal analysis, LLM powered text understanding, and disaster response, our work presents a promising direction for Artificial Intelligence (AI) driven crisis management solutions. Keywords:</li>
</ul>

<h3>Title: YOLO11-CR: a Lightweight Convolution-and-Attention Framework for Accurate Fatigue Driving Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhebin Jin, Ligang Dong</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13205">https://arxiv.org/abs/2508.13205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13205">https://arxiv.org/pdf/2508.13205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13205]] YOLO11-CR: a Lightweight Convolution-and-Attention Framework for Accurate Fatigue Driving Detection(https://arxiv.org/abs/2508.13205)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Driver fatigue detection is of paramount importance for intelligent transportation systems due to its critical role in mitigating road traffic accidents. While physiological and vehicle dynamics-based methods offer accuracy, they are often intrusive, hardware-dependent, and lack robustness in real-world environments. Vision-based techniques provide a non-intrusive and scalable alternative, but still face challenges such as poor detection of small or occluded objects and limited multi-scale feature modeling. To address these issues, this paper proposes YOLO11-CR, a lightweight and efficient object detection model tailored for real-time fatigue detection. YOLO11-CR introduces two key modules: the Convolution-and-Attention Fusion Module (CAFM), which integrates local CNN features with global Transformer-based context to enhance feature expressiveness; and the Rectangular Calibration Module (RCM), which captures horizontal and vertical contextual information to improve spatial localization, particularly for profile faces and small objects like mobile phones. Experiments on the DSM dataset demonstrated that YOLO11-CR achieves a precision of 87.17%, recall of 83.86%, mAP@50 of 88.09%, and mAP@50-95 of 55.93%, outperforming baseline models significantly. Ablation studies further validate the effectiveness of the CAFM and RCM modules in improving both sensitivity and localization accuracy. These results demonstrate that YOLO11-CR offers a practical and high-performing solution for in-vehicle fatigue monitoring, with strong potential for real-world deployment and future enhancements involving temporal modeling, multi-modal data integration, and embedded optimization.</li>
</ul>

<h3>Title: Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions</h3>
<ul>
<li><strong>Authors: </strong>Xuyang Guo, Zekai Huang, Zhao Song, Jiahao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13214">https://arxiv.org/abs/2508.13214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13214">https://arxiv.org/pdf/2508.13214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13214]] Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions(https://arxiv.org/abs/2508.13214)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently demonstrated strong emergent abilities in complex reasoning and zero-shot generalization, showing unprecedented potential for LLM-as-a-judge applications in education, peer review, and data quality evaluation. However, their robustness under prompt injection attacks, where malicious instructions are embedded into the content to manipulate outputs, remains a significant concern. In this work, we explore a frustratingly simple yet effective attack setting to test whether LLMs can be easily misled. Specifically, we evaluate LLMs on basic arithmetic questions (e.g., "What is 3 + 2?") presented as either multiple-choice or true-false judgment problems within PDF files, where hidden prompts are injected into the file. Our results reveal that LLMs are indeed vulnerable to such hidden prompt injection attacks, even in these trivial scenarios, highlighting serious robustness risks for LLM-as-a-judge applications.</li>
</ul>

<h3>Title: MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Yang, Daoyuan Wu, Yufan Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13220">https://arxiv.org/abs/2508.13220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13220">https://arxiv.org/pdf/2508.13220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13220]] MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols(https://arxiv.org/abs/2508.13220)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly integrated into real-world applications via the Model Context Protocol (MCP), a universal, open standard for connecting AI agents with data sources and external tools. While MCP enhances the capabilities of LLM-based agents, it also introduces new security risks and expands their attack surfaces. In this paper, we present the first systematic taxonomy of MCP security, identifying 17 attack types across 4 primary attack surfaces. We introduce MCPSecBench, a comprehensive security benchmark and playground that integrates prompt datasets, MCP servers, MCP clients, and attack scripts to evaluate these attacks across three major MCP providers. Our benchmark is modular and extensible, allowing researchers to incorporate custom implementations of clients, servers, and transport protocols for systematic security assessment. Experimental results show that over 85% of the identified attacks successfully compromise at least one platform, with core vulnerabilities universally affecting Claude, OpenAI, and Cursor, while prompt-based and tool-centric attacks exhibit considerable variability across different hosts and models. Overall, MCPSecBench standardizes the evaluation of MCP security and enables rigorous testing across all MCP layers.</li>
</ul>

<h3>Title: MIRAGE: Towards AI-Generated Image Detection in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Cheng Xia, Manxi Lin, Jiexiang Tan, Xiaoxiong Du, Yang Qiu, Junjun Zheng, Xiangheng Kong, Yuning Jiang, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13223">https://arxiv.org/abs/2508.13223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13223">https://arxiv.org/pdf/2508.13223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13223]] MIRAGE: Towards AI-Generated Image Detection in the Wild(https://arxiv.org/abs/2508.13223)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, generative</a></li>
<li><strong>Abstract: </strong>The spreading of AI-generated images (AIGI), driven by advances in generative AI, poses a significant threat to information security and public trust. Existing AIGI detectors, while effective against images in clean laboratory settings, fail to generalize to in-the-wild scenarios. These real-world images are noisy, varying from ``obviously fake" images to realistic ones derived from multiple generative models and further edited for quality control. We address in-the-wild AIGI detection in this paper. We introduce Mirage, a challenging benchmark designed to emulate the complexity of in-the-wild AIGI. Mirage is constructed from two sources: (1) a large corpus of Internet-sourced AIGI verified by human experts, and (2) a synthesized dataset created through the collaboration between multiple expert generators, closely simulating the realistic AIGI in the wild. Building on this benchmark, we propose Mirage-R1, a vision-language model with heuristic-to-analytic reasoning, a reflective reasoning mechanism for AIGI detection. Mirage-R1 is trained in two stages: a supervised-fine-tuning cold start, followed by a reinforcement learning stage. By further adopting an inference-time adaptive thinking strategy, Mirage-R1 is able to provide either a quick judgment or a more robust and accurate conclusion, effectively balancing inference speed and performance. Extensive experiments show that our model leads state-of-the-art detectors by 5% and 10% on Mirage and the public benchmark, respectively. The benchmark and code will be made publicly available.</li>
</ul>

<h3>Title: RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Suhang Hu, Wei Hu, Yuhang Su, Fan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13229">https://arxiv.org/abs/2508.13229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13229">https://arxiv.org/pdf/2508.13229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13229]] RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning(https://arxiv.org/abs/2508.13229)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) struggle with complex image annotation tasks, such as emotion classification and context-driven object detection, which demand sophisticated reasoning. Standard Supervised Fine-Tuning (SFT) focuses solely on annotation outcomes, ignoring underlying rationales, while Visual Reinforcement Fine-Tuning (Visual-RFT) produces inconsistent Chains of Thought (CoTs) due to the absence of high-quality, verified CoTs during pre-training. We introduce RISE (Reason-Inspire-Strengthen-Expertise), a two-stage framework to overcome these limitations. In the Reason stage (RISE-CoT), a reinforcement learning-driven "annotation-reasoning-annotation" closed-loop generates visually grounded, logically consistent CoTs by verifying their ability to reconstruct original annotations without direct leakage. The Inspire and Strengthen stage (RISE-R1) leverages a high-quality CoT subset, filtered by RISE-CoT rewards, for supervised fine-tuning, followed by reinforcement fine-tuning to produce interpretable reasoning and accurate annotations, achieving Expertise in complex visual tasks. Evaluated on complex and simple image annotation tasks, RISE-trained Qwen2-VL-2B outperforms SFT and Visual-RFT, achieving robust performance and enhanced explainability. RISE offers a self-supervised solution for advancing VLM reasoning without requiring manually annotated CoTs.</li>
</ul>

<h3>Title: DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool Interleaved Vision-Language Model</h3>
<ul>
<li><strong>Authors: </strong>Qian Chen, Xianyin Zhang, Lifan Guo, Feng Chen, Chi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13238">https://arxiv.org/abs/2508.13238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13238">https://arxiv.org/pdf/2508.13238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13238]] DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool Interleaved Vision-Language Model(https://arxiv.org/abs/2508.13238)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large vision-language models (LVLMs) have enabled a new paradigm of end-to-end document image parsing, excelling in Optical Character Recognition (OCR) tasks such as text, table, and formula recognition. However, generative LVLMs, similarly to large language models (LLMs), are prone to hallucinations--generating words that do not exist in input images. Furthermore, LVLMs are designed for general purposes and tend to be less effective on OCR tasks compared to expert models that are trained on domain-specific datasets. In this paper, we propose DianJin-OCR-R1, a reasoning-enhanced framework designed to address these limitations through training reasoning-and-tool interleaved VLMs. Given a recognition instruction, our DianJin-OCR-R1 model first recognizes the content in the input image by its own OCR capabilities, and then calls other tools (i.e., other expert models) to obtain their results as references, finally looks again the image and rethinks about the reasoning process to provide the final recognized content. Since architectures of expert models are tailored for specific OCR tasks, which makes them less prone to hallucinations, their results can help VLMs mitigate hallucinations. Additionally, expert models are typically smaller in scale and easy to iterate, enabling performance improvements for VLMs at a lower cost. We evaluate our model on ReST and OmniDocBench, and experimental results show that our DianJin-OCR-R1 models consistently outperform their non-reasoning counterparts and expert OCR models, which proves the effectiveness of our method.</li>
</ul>

<h3>Title: Quantifying Loss Aversion in Cyber Adversaries via LLM Analysis</h3>
<ul>
<li><strong>Authors: </strong>Soham Hans, Nikolos Gurney, Stacy Marsella, Sofia Hirschmann</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13240">https://arxiv.org/abs/2508.13240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13240">https://arxiv.org/pdf/2508.13240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13240]] Quantifying Loss Aversion in Cyber Adversaries via LLM Analysis(https://arxiv.org/abs/2508.13240)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Understanding and quantifying human cognitive biases from empirical data has long posed a formidable challenge, particularly in cybersecurity, where defending against unknown adversaries is paramount. Traditional cyber defense strategies have largely focused on fortification, while some approaches attempt to anticipate attacker strategies by mapping them to cognitive vulnerabilities, yet they fall short in dynamically interpreting attacks in progress. In recognition of this gap, IARPA's ReSCIND program seeks to infer, defend against, and even exploit attacker cognitive traits. In this paper, we present a novel methodology that leverages large language models (LLMs) to extract quantifiable insights into the cognitive bias of loss aversion from hacker behavior. Our data are collected from an experiment in which hackers were recruited to attack a controlled demonstration network. We process the hacker generated notes using LLMs using it to segment the various actions and correlate the actions to predefined persistence mechanisms used by hackers. By correlating the implementation of these mechanisms with various operational triggers, our analysis provides new insights into how loss aversion manifests in hacker decision-making. The results demonstrate that LLMs can effectively dissect and interpret nuanced behavioral patterns, thereby offering a transformative approach to enhancing cyber defense strategies through real-time, behavior-based analysis.</li>
</ul>

<h3>Title: Exploration of Deep Learning Based Recognition for Urdu Text</h3>
<ul>
<li><strong>Authors: </strong>Sumaiya Fazal, Sheeraz Ahmed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13245">https://arxiv.org/abs/2508.13245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13245">https://arxiv.org/pdf/2508.13245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13245]] Exploration of Deep Learning Based Recognition for Urdu Text(https://arxiv.org/abs/2508.13245)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Urdu is a cursive script language and has similarities with Arabic and many other South Asian languages. Urdu is difficult to classify due to its complex geometrical and morphological structure. Character classification can be processed further if segmentation technique is efficient, but due to context sensitivity in Urdu, segmentation-based recognition often results with high error rate. Our proposed approach for Urdu optical character recognition system is a component-based classification relying on automatic feature learning technique called convolutional neural network. CNN is trained and tested on Urdu text dataset, which is generated through permutation process of three characters and further proceeds to discarding unnecessary images by applying connected component technique in order to obtain ligature only. Hierarchical neural network is implemented with two levels to deal with three degrees of character permutations and component classification Our model successfully achieved 0.99% for component classification.</li>
</ul>

<h3>Title: Involuntary Jailbreak</h3>
<ul>
<li><strong>Authors: </strong>Yangyang Guo, Yangyan Li, Mohan Kankanhalli</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13246">https://arxiv.org/abs/2508.13246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13246">https://arxiv.org/pdf/2508.13246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13246]] Involuntary Jailbreak(https://arxiv.org/abs/2508.13246)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>In this study, we disclose a worrying new vulnerability in Large Language Models (LLMs), which we term \textbf{involuntary jailbreak}. Unlike existing jailbreak attacks, this weakness is distinct in that it does not involve a specific attack objective, such as generating instructions for \textit{building a bomb}. Prior attack methods predominantly target localized components of the LLM guardrail. In contrast, involuntary jailbreaks may potentially compromise the entire guardrail structure, which our method reveals to be surprisingly fragile. We merely employ a single universal prompt to achieve this goal. In particular, we instruct LLMs to generate several questions that would typically be rejected, along with their corresponding in-depth responses (rather than a refusal). Remarkably, this simple prompt strategy consistently jailbreaks the majority of leading LLMs, including Claude Opus 4.1, Grok 4, Gemini 2.5 Pro, and GPT 4.1. We hope this problem can motivate researchers and practitioners to re-evaluate the robustness of LLM guardrails and contribute to stronger safety alignment in future.</li>
</ul>

<h3>Title: CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification</h3>
<ul>
<li><strong>Authors: </strong>Zeynep Ozdemir, Hacer Yalim Keles, Omer Ozgur Tanriover</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13280">https://arxiv.org/abs/2508.13280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13280">https://arxiv.org/pdf/2508.13280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13280]] CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification(https://arxiv.org/abs/2508.13280)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Estimating disease severity from endoscopic images is essential in assessing ulcerative colitis, where the Mayo Endoscopic Subscore (MES) is widely used to grade inflammation. However, MES classification remains challenging due to label noise from inter-observer variability and the ordinal nature of the score, which standard models often ignore. We propose CLoE, a curriculum learning framework that accounts for both label reliability and ordinal structure. Image quality, estimated via a lightweight model trained on Boston Bowel Preparation Scale (BBPS) labels, is used as a proxy for annotation confidence to order samples from easy (clean) to hard (noisy). This curriculum is further combined with ResizeMix augmentation to improve robustness. Experiments on the LIMUC and HyperKvasir datasets, using both CNNs and Transformers, show that CLoE consistently improves performance over strong supervised and self-supervised baselines. For instance, ConvNeXt-Tiny reaches 82.5\% accuracy and a QWK of 0.894 on LIMUC with low computational cost. These results highlight the potential of difficulty-aware training strategies for improving ordinal classification under label uncertainty. Code will be released at this https URL.</li>
</ul>

<h3>Title: GaitCrafter: Diffusion Model for Biometric Preserving Gait Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Sirshapan Mitra, Yogesh S. Rawat</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13300">https://arxiv.org/abs/2508.13300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13300">https://arxiv.org/pdf/2508.13300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13300]] GaitCrafter: Diffusion Model for Biometric Preserving Gait Synthesis(https://arxiv.org/abs/2508.13300)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, biometric, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Gait recognition is a valuable biometric task that enables the identification of individuals from a distance based on their walking patterns. However, it remains limited by the lack of large-scale labeled datasets and the difficulty of collecting diverse gait samples for each individual while preserving privacy. To address these challenges, we propose GaitCrafter, a diffusion-based framework for synthesizing realistic gait sequences in the silhouette domain. Unlike prior works that rely on simulated environments or alternative generative models, GaitCrafter trains a video diffusion model from scratch, exclusively on gait silhouette data. Our approach enables the generation of temporally consistent and identity-preserving gait sequences. Moreover, the generation process is controllable-allowing conditioning on various covariates such as clothing, carried objects, and view angle. We show that incorporating synthetic samples generated by GaitCrafter into the gait recognition pipeline leads to improved performance, especially under challenging conditions. Additionally, we introduce a mechanism to generate novel identities-synthetic individuals not present in the original dataset-by interpolating identity embeddings. These novel identities exhibit unique, consistent gait patterns and are useful for training models while maintaining privacy of real subjects. Overall, our work takes an important step toward leveraging diffusion models for high-quality, controllable, and privacy-aware gait data generation.</li>
</ul>

<h3>Title: DAASH: A Meta-Attack Framework for Synthesizing Effective and Stealthy Adversarial Examples</h3>
<ul>
<li><strong>Authors: </strong>Abdullah Al Nomaan Nafi, Habibur Rahaman, Zafaryab Haider, Tanzim Mahfuz, Fnu Suya, Swarup Bhunia, Prabuddha Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13309">https://arxiv.org/abs/2508.13309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13309">https://arxiv.org/pdf/2508.13309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13309]] DAASH: A Meta-Attack Framework for Synthesizing Effective and Stealthy Adversarial Examples(https://arxiv.org/abs/2508.13309)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>Numerous techniques have been proposed for generating adversarial examples in white-box settings under strict Lp-norm constraints. However, such norm-bounded examples often fail to align well with human perception, and only recently have a few methods begun specifically exploring perceptually aligned adversarial examples. Moreover, it remains unclear whether insights from Lp-constrained attacks can be effectively leveraged to improve perceptual efficacy. In this paper, we introduce DAASH, a fully differentiable meta-attack framework that generates effective and perceptually aligned adversarial examples by strategically composing existing Lp-based attack methods. DAASH operates in a multi-stage fashion: at each stage, it aggregates candidate adversarial examples from multiple base attacks using learned, adaptive weights and propagates the result to the next stage. A novel meta-loss function guides this process by jointly minimizing misclassification loss and perceptual distortion, enabling the framework to dynamically modulate the contribution of each base attack throughout the stages. We evaluate DAASH on adversarially trained models across CIFAR-10, CIFAR-100, and ImageNet. Despite relying solely on Lp-constrained based methods, DAASH significantly outperforms state-of-the-art perceptual attacks such as AdvAD -- achieving higher attack success rates (e.g., 20.63\% improvement) and superior visual quality, as measured by SSIM, LPIPS, and FID (improvements $\approx$ of 11, 0.015, and 5.7, respectively). Furthermore, DAASH generalizes well to unseen defenses, making it a practical and strong baseline for evaluating robustness without requiring handcrafted adaptive attacks for each new defense.</li>
</ul>

<h3>Title: A Dual-Attention Graph Network for fMRI Data Classification</h3>
<ul>
<li><strong>Authors: </strong>Amirali Arbab, Zeinab Davarani, Mehran Safayani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13328">https://arxiv.org/abs/2508.13328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13328">https://arxiv.org/pdf/2508.13328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13328]] A Dual-Attention Graph Network for fMRI Data Classification(https://arxiv.org/abs/2508.13328)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Understanding the complex neural activity dynamics is crucial for the development of the field of neuroscience. Although current functional MRI classification approaches tend to be based on static functional connectivity or cannot capture spatio-temporal relationships comprehensively, we present a new framework that leverages dynamic graph creation and spatiotemporal attention mechanisms for Autism Spectrum Disorder(ASD) diagnosis. The approach used in this research dynamically infers functional brain connectivity in each time interval using transformer-based attention mechanisms, enabling the model to selectively focus on crucial brain regions and time segments. By constructing time-varying graphs that are then processed with Graph Convolutional Networks (GCNs) and transformers, our method successfully captures both localized interactions and global temporal dependencies. Evaluated on the subset of ABIDE dataset, our model achieves 63.2 accuracy and 60.0 AUC, outperforming static graph-based approaches (e.g., GCN:51.8). This validates the efficacy of joint modeling of dynamic connectivity and spatio-temporal context for fMRI classification. The core novelty arises from (1) attention-driven dynamic graph creation that learns temporal brain region interactions and (2) hierarchical spatio-temporal feature fusion through GCNtransformer fusion.</li>
</ul>

<h3>Title: X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms</h3>
<ul>
<li><strong>Authors: </strong>Yueming Yuan, Ahan Gupta, Jianping Li, Sajal Dash, Feiyi Wang, Minjia Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13337">https://arxiv.org/abs/2508.13337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13337">https://arxiv.org/pdf/2508.13337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13337]] X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms(https://arxiv.org/abs/2508.13337)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Emerging expert-specialized Mixture-of-Experts (MoE) architectures, such as DeepSeek-MoE, deliver strong model quality through fine-grained expert segmentation and large top-k routing. However, their scalability is limited by substantial activation memory overhead and costly all-to-all communication. Furthermore, current MoE training systems - primarily optimized for NVIDIA GPUs - perform suboptimally on non-NVIDIA platforms, leaving significant computational potential untapped. In this work, we present X-MoE, a novel MoE training system designed to deliver scalable training performance for next-generation MoE architectures. X-MoE achieves this via several novel techniques, including efficient padding-free MoE training with cross-platform kernels, redundancy-bypassing dispatch, and hybrid parallelism with sequence-sharded MoE blocks. Our evaluation on the Frontier supercomputer, powered by AMD MI250X GPUs, shows that X-MoE scales DeepSeek-style MoEs up to 545 billion parameters across 1024 GPUs - 10x larger than the largest trainable model with existing methods under the same hardware budget, while maintaining high training throughput. The source code of X-MoE is available at this https URL.</li>
</ul>

<h3>Title: Counterfactual Probabilistic Diffusion with Expert Models</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Mu, Zhi Cao, Mehmed Uludag, Alexander Rodríguez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13355">https://arxiv.org/abs/2508.13355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13355">https://arxiv.org/pdf/2508.13355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13355]] Counterfactual Probabilistic Diffusion with Expert Models(https://arxiv.org/abs/2508.13355)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Predicting counterfactual distributions in complex dynamical systems is essential for scientific modeling and decision-making in domains such as public health and medicine. However, existing methods often rely on point estimates or purely data-driven models, which tend to falter under data scarcity. We propose a time series diffusion-based framework that incorporates guidance from imperfect expert models by extracting high-level signals to serve as structured priors for generative modeling. Our method, ODE-Diff, bridges mechanistic and data-driven approaches, enabling more reliable and interpretable causal inference. We evaluate ODE-Diff across semi-synthetic COVID-19 simulations, synthetic pharmacological dynamics, and real-world case studies, demonstrating that it consistently outperforms strong baselines in both point prediction and distributional accuracy.</li>
</ul>

<h3>Title: Silentflow: Leveraging Trusted Execution for Resource-Limited MPC via Hardware-Algorithm Co-design</h3>
<ul>
<li><strong>Authors: </strong>Zhuoran Li, Hanieh Totonchi Asl, Ebrahim Nouri, Yifei Cai, Danella Zhao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13357">https://arxiv.org/abs/2508.13357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13357">https://arxiv.org/pdf/2508.13357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13357]] Silentflow: Leveraging Trusted Execution for Resource-Limited MPC via Hardware-Algorithm Co-design(https://arxiv.org/abs/2508.13357)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>Secure Multi-Party Computation (MPC) offers a practical foundation for privacy-preserving machine learning at the edge, with MPC commonly employed to support nonlinear operations. These MPC protocols fundamentally rely on Oblivious Transfer (OT), particularly Correlated OT (COT), to generate correlated randomness essential for secure computation. Although COT generation is efficient in conventional two-party settings with resource-rich participants, it becomes a critical bottleneck in real-world inference on resource-constrained devices (e.g., IoT sensors and wearables), due to both communication latency and limited computational capacity. To enable real-time secure inference, we introduce Silentflow, a highly efficient Trusted Execution Environment (TEE)-assisted protocol that eliminates communication in COT generation. We tackle the core performance bottleneck-low computational intensity-through structured algorithmic decomposition: kernel fusion for parallelism, Blocked On-chip eXpansion (BOX) to improve memory access patterns, and vectorized batch operations to maximize memory bandwidth utilization. Through design space exploration, we balance end-to-end latency and resource demands, achieving up to 39.51x speedup over state-of-the-art protocols. By offloading COT computations to a Zynq-7000 SoC, SilentFlow accelerates PPMLaaS inference on the ImageNet dataset under resource constraints, achieving a 4.62x and 3.95x speedup over Cryptflow2 and Cheetah, respectively.</li>
</ul>

<h3>Title: A Risk Manager for Intrusion Tolerant Systems: Enhancing HAL 9000 with New Scoring and Data Sources</h3>
<ul>
<li><strong>Authors: </strong>Tadeu Freitas, Carlos Novo, Inês Dutra, João Soares, Manuel Correia, Benham Shariati, Rolando Martins</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13364">https://arxiv.org/abs/2508.13364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13364">https://arxiv.org/pdf/2508.13364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13364]] A Risk Manager for Intrusion Tolerant Systems: Enhancing HAL 9000 with New Scoring and Data Sources(https://arxiv.org/abs/2508.13364)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Intrusion Tolerant Systems (ITSs) have become increasingly critical due to the rise of multi-domain adversaries exploiting diverse attack surfaces. ITS architectures aim to tolerate intrusions, ensuring system compromise is prevented or mitigated even with adversary presence. Existing ITS solutions often employ Risk Managers leveraging public security intelligence to adjust system defenses dynamically against emerging threats. However, these approaches rely heavily on databases like NVD and ExploitDB, which require manual analysis for newly discovered vulnerabilities. This dependency limits the system's responsiveness to rapidly evolving threats. HAL 9000, an ITS Risk Manager introduced in our prior work, addressed these challenges through machine learning. By analyzing descriptions of known vulnerabilities, HAL 9000 predicts and assesses new vulnerabilities automatically. To calculate the risk of a system, it also incorporates the Exploitability Probability Scoring system to estimate the likelihood of exploitation within 30 days, enhancing proactive defense capabilities. Despite its success, HAL 9000's reliance on NVD and ExploitDB knowledge is a limitation, considering the availability of other sources of information. This extended work introduces a custom-built scraper that continuously mines diverse threat sources, including security advisories, research forums, and real-time exploit proofs-of-concept. This significantly expands HAL 9000's intelligence base, enabling earlier detection and assessment of unverified vulnerabilities. Our evaluation demonstrates that integrating scraper-derived intelligence with HAL 9000's risk management framework substantially improves its ability to address emerging threats. This paper details the scraper's integration into the architecture, its role in providing additional information on new threats, and the effects on HAL 9000's management.</li>
</ul>

<h3>Title: Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity Detection</h3>
<ul>
<li><strong>Authors: </strong>Dylan Phelps, Rodrigo Wilkens, Edward Gow-Smith, Thomas Pickard, Maggie Mi, Aline Villavicencio</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13365">https://arxiv.org/abs/2508.13365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13365">https://arxiv.org/pdf/2508.13365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13365]] Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity Detection(https://arxiv.org/abs/2508.13365)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The recent trend towards utilisation of reasoning models has improved the performance of Large Language Models (LLMs) across many tasks which involve logical steps. One linguistic task that could benefit from this framing is idiomaticity detection, as a potentially idiomatic expression must first be understood before it can be disambiguated and serves as a basis for reasoning. In this paper, we explore how reasoning capabilities in LLMs affect idiomaticity detection performance and examine the effect of model size. We evaluate, as open source representative models, the suite of DeepSeek-R1 distillation models ranging from 1.5B to 70B parameters across four idiomaticity detection datasets. We find the effect of reasoning to be smaller and more varied than expected. For smaller models, producing chain-of-thought (CoT) reasoning increases performance from Math-tuned intermediate models, but not to the levels of the base models, whereas larger models (14B, 32B, and 70B) show modest improvements. Our in-depth analyses reveal that larger models demonstrate good understanding of idiomaticity, successfully producing accurate definitions of expressions, while smaller models often fail to output the actual meaning. For this reason, we also experiment with providing definitions in the prompts of smaller models, which we show can improve performance in some cases.</li>
</ul>

<h3>Title: Whispering Context: Distilling Syntax and Semantics for Long Speech Transcripts</h3>
<ul>
<li><strong>Authors: </strong>Duygu Altinok</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13376">https://arxiv.org/abs/2508.13376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13376">https://arxiv.org/pdf/2508.13376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13376]] Whispering Context: Distilling Syntax and Semantics for Long Speech Transcripts(https://arxiv.org/abs/2508.13376)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>ASR systems often struggle with maintaining syntactic and semantic accuracy in long audio transcripts, impacting tasks like Named Entity Recognition (NER), capitalization, and punctuation. We propose a novel approach that enhances ASR by distilling contextual knowledge from LLaMA models into Whisper. Our method uses two strategies: (1) token level distillation with optimal transport to align dimensions and sequence lengths, and (2) representation loss minimization between sentence embeddings of Whisper and LLaMA, blending syntax and semantics. Evaluations on the Spoken Wikipedia dataset, a benchmark with long audios and rich entities demonstrate significant improvements in Word Error Rate (WER), NER, capitalization, and punctuation success. By introducing novel NER metrics and exploring semantics aware ASR, our work highlights the value of integrating linguistic context into transcription, setting a foundation for robust, context-aware ASR in longform speech.</li>
</ul>

<h3>Title: Applications of Small Language Models in Medical Imaging Classification with a Focus on Prompt Strategies</h3>
<ul>
<li><strong>Authors: </strong>Yiting Wang, Ziwei Wang, Jiachen Zhong, Di Zhu, Weiyi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13378">https://arxiv.org/abs/2508.13378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13378">https://arxiv.org/pdf/2508.13378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13378]] Applications of Small Language Models in Medical Imaging Classification with a Focus on Prompt Strategies(https://arxiv.org/abs/2508.13378)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable capabilities in natural language processing and multi-modal understanding. However, their high computational cost, limited accessibility, and data privacy concerns hinder their adoption in resource-constrained healthcare environments. This study investigates the performance of small language models (SLMs) in a medical imaging classification task, comparing different models and prompt designs to identify the optimal combination for accuracy and usability. Using the NIH Chest X-ray dataset, we evaluate multiple SLMs on the task of classifying chest X-ray positions (anteroposterior [AP] vs. posteroanterior [PA]) under three prompt strategies: baseline instruction, incremental summary prompts, and correction-based reflective prompts. Our results show that certain SLMs achieve competitive accuracy with well-crafted prompts, suggesting that prompt engineering can substantially enhance SLM performance in healthcare applications without requiring deep AI expertise from end users.</li>
</ul>

<h3>Title: Batching-Aware Joint Model Onloading and Offloading for Hierarchical Multi-Task Inference</h3>
<ul>
<li><strong>Authors: </strong>Seohyeon Cha, Kevin Chan, Gustavo de Veciana, Haris Vikalo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13380">https://arxiv.org/abs/2508.13380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13380">https://arxiv.org/pdf/2508.13380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13380]] Batching-Aware Joint Model Onloading and Offloading for Hierarchical Multi-Task Inference(https://arxiv.org/abs/2508.13380)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The growing demand for intelligent services on resource-constrained edge devices has spurred the development of collaborative inference systems that distribute workloads across end devices, edge servers, and the cloud. While most existing frameworks focus on single-task, single-model scenarios, many real-world applications (e.g., autonomous driving and augmented reality) require concurrent execution of diverse tasks including detection, segmentation, and depth estimation. In this work, we propose a unified framework to jointly decide which multi-task models to deploy (onload) at clients and edge servers, and how to route queries across the hierarchy (offload) to maximize overall inference accuracy under memory, compute, and communication constraints. We formulate this as a mixed-integer program and introduce J3O (Joint Optimization of Onloading and Offloading), an alternating algorithm that (i) greedily selects models to onload via Lagrangian-relaxed submodular optimization and (ii) determines optimal offloading via constrained linear programming. We further extend J3O to account for batching at the edge, maintaining scalability under heterogeneous task loads. Experiments show J3O consistently achieves over $97\%$ of the optimal accuracy while incurring less than $15\%$ of the runtime required by the optimal solver across multi-task benchmarks.</li>
</ul>

<h3>Title: AIM 2025 Rip Current Segmentation (RipSeg) Challenge Report</h3>
<ul>
<li><strong>Authors: </strong>Andrei Dumitriu, Florin Miron, Florin Tatui, Radu Tudor Ionescu, Radu Timofte, Aakash Ralhan, Florin-Alexandru Vasluianu, Shenyang Qian, Mitchell Harley, Imran Razzak, Yang Song, Pu Luo, Yumei Li, Cong Xu, Jinming Chai, Kexin Zhang, Licheng Jiao, Lingling Li, Siqi Yu, Chao Zhang, Kehuan Song, Fang Liu, Puhua Chen, Xu Liu, Jin Hu, Jinyang Xu, Biao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13401">https://arxiv.org/abs/2508.13401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13401">https://arxiv.org/pdf/2508.13401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13401]] AIM 2025 Rip Current Segmentation (RipSeg) Challenge Report(https://arxiv.org/abs/2508.13401)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>This report presents an overview of the AIM 2025 RipSeg Challenge, a competition designed to advance techniques for automatic rip current segmentation in still images. Rip currents are dangerous, fast-moving flows that pose a major risk to beach safety worldwide, making accurate visual detection an important and underexplored research task. The challenge builds on RipVIS, the largest available rip current dataset, and focuses on single-class instance segmentation, where precise delineation is critical to fully capture the extent of rip currents. The dataset spans diverse locations, rip current types, and camera orientations, providing a realistic and challenging benchmark. In total, $75$ participants registered for this first edition, resulting in $5$ valid test submissions. Teams were evaluated on a composite score combining $F_1$, $F_2$, $AP_{50}$, and $AP_{[50:95]}$, ensuring robust and application-relevant rankings. The top-performing methods leveraged deep learning architectures, domain adaptation techniques, pretrained models, and domain generalization strategies to improve performance under diverse conditions. This report outlines the dataset details, competition framework, evaluation metrics, and final results, providing insights into the current state of rip current segmentation. We conclude with a discussion of key challenges, lessons learned from the submissions, and future directions for expanding RipSeg.</li>
</ul>

<h3>Title: NovoMolGen: Rethinking Molecular Language Model Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Kamran Chitsaz, Roshan Balaji, Quentin Fournier, Nirav Pravinbhai Bhatt, Sarath Chandar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13408">https://arxiv.org/abs/2508.13408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13408">https://arxiv.org/pdf/2508.13408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13408]] NovoMolGen: Rethinking Molecular Language Model Pretraining(https://arxiv.org/abs/2508.13408)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Designing de-novo molecules with desired property profiles requires efficient exploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$ possible synthesizable candidates. While various deep generative models have been developed to design small molecules using diverse input representations, Molecular Large Language Models (Mol-LLMs) based on string representations have emerged as a scalable approach capable of exploring billions of molecules. However, there remains limited understanding regarding how standard language modeling practices such as textual representations, tokenization strategies, model size, and dataset scale impact molecular generation performance. In this work, we systematically investigate these critical aspects by introducing NovoMolGen, a family of transformer-based foundation models pretrained on 1.5 billion molecules for de-novo molecule generation. Through extensive empirical analyses, we identify a weak correlation between performance metrics measured during pretraining and actual downstream performance, revealing important distinctions between molecular and general NLP training dynamics. NovoMolGen establishes new state-of-the-art results, substantially outperforming prior Mol-LLMs and specialized generative models in both unconstrained and goal-directed molecular generation tasks, thus providing a robust foundation for advancing efficient and effective molecular modeling strategies.</li>
</ul>

<h3>Title: Decentralized Contextual Bandits with Network Adaptivity</h3>
<ul>
<li><strong>Authors: </strong>Chuyun Deng, Huiwen Jia</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13411">https://arxiv.org/abs/2508.13411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13411">https://arxiv.org/pdf/2508.13411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13411]] Decentralized Contextual Bandits with Network Adaptivity(https://arxiv.org/abs/2508.13411)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We consider contextual linear bandits over networks, a class of sequential decision-making problems where learning occurs simultaneously across multiple locations and the reward distributions share structural similarities while also exhibiting local differences. While classical contextual bandits assume either fully centralized data or entirely isolated learners, much remains unexplored in networked environments when information is partially shared. In this paper, we address this gap by developing two network-aware Upper Confidence Bound (UCB) algorithms, NetLinUCB and Net-SGD-UCB, which enable adaptive information sharing guided by dynamically updated network weights. Our approach decompose learning into global and local components and as a result allow agents to benefit from shared structure without full synchronization. Both algorithms incur lighter communication costs compared to a fully centralized setting as agents only share computed summaries regarding the homogeneous features. We establish regret bounds showing that our methods reduce the learning complexity associated with the shared structure from $O(N)$ to sublinear $O(\sqrt{N})$, where $N$ is the size of the network. The two algorithms reveal complementary strengths: NetLinUCB excels in low-noise regimes with fine-grained heterogeneity, while Net-SGD-UCB is robust to high-dimensional, high-variance contexts. We further demonstrate the effectiveness of our methods across simulated pricing environments compared to standard benchmarks.</li>
</ul>

<h3>Title: MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search</h3>
<ul>
<li><strong>Authors: </strong>Jeremy Carleton, Debajoy Mukherjee, Srinivas Shakkottai, Dileep Kalathil</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13415">https://arxiv.org/abs/2508.13415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13415">https://arxiv.org/pdf/2508.13415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13415]] MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search(https://arxiv.org/abs/2508.13415)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly deployed across diverse applications that demand balancing multiple, often conflicting, objectives -- such as helpfulness, harmlessness, or humor. Aligning outputs to user-specific preferences in such multi-objective settings typically requires fine-tuning models for each objective or preference configuration, which is computationally expensive and inflexible. We introduce MAVIS -- Multi-Objective Alignment via Value-Guided Inference-Time Search -- a lightweight inference-time alignment framework that enables dynamic control over LLM behavior without modifying the base model's weights. MAVIS trains a set of small value models, each corresponding to a distinct objective. At inference time, these value models are combined using user-specified weights to produce a tilting function that adjusts the base model's output distribution toward desired trade-offs. The value models are trained using a simple iterative algorithm that ensures monotonic improvement of the KL-regularized policy. We show empirically that MAVIS outperforms baselines that fine-tune per-objective models and combine them post hoc, and even approaches the performance of the idealized setting where models are fine-tuned for a user's exact preferences.</li>
</ul>

<h3>Title: When Secure Aggregation Falls Short: Achieving Long-Term Privacy in Asynchronous Federated Learning for LEO Satellite Networks</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Elmahallawy, Tie Luo</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13425">https://arxiv.org/abs/2508.13425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13425">https://arxiv.org/pdf/2508.13425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13425]] When Secure Aggregation Falls Short: Achieving Long-Term Privacy in Asynchronous Federated Learning for LEO Satellite Networks(https://arxiv.org/abs/2508.13425)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, federate, fair</a></li>
<li><strong>Abstract: </strong>Secure aggregation is a common technique in federated learning (FL) for protecting data privacy from both curious internal entities (clients or server) and external adversaries (eavesdroppers). However, in dynamic and resource-constrained environments such as low Earth orbit (LEO) satellite networks, traditional secure aggregation methods fall short in two aspects: (1) they assume continuous client availability while LEO satellite visibility is intermittent and irregular; (2) they consider privacy in each communication round but have overlooked the possible privacy leakage through multiple rounds. To address these limitations, we propose LTP-FLEO, an asynchronous FL framework that preserves long-term privacy (LTP) for LEO satellite networks. LTP-FLEO introduces (i) privacy-aware satellite partitioning, which groups satellites based on their predictable visibility to the server and enforces joint participation; (ii) model age balancing, which mitigates the adverse impact of stale model updates; and (iii) fair global aggregation, which treats satellites of different visibility durations in an equitable manner. Theoretical analysis and empirical validation demonstrate that LTP-FLEO effectively safeguards both model and data privacy across multi-round training, promotes fairness in line with satellite contributions, accelerates global convergence, and achieves competitive model accuracy.</li>
</ul>

<h3>Title: ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chunhua Liu, Kabir Manandhar Shrestha, Sukai Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13426">https://arxiv.org/abs/2508.13426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13426">https://arxiv.org/pdf/2508.13426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13426]] ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models(https://arxiv.org/abs/2508.13426)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) increasingly mediate cross-cultural communication, their behavior still reflects the distributional bias of the languages and viewpoints that are over-represented in their pre-training corpora. Yet, it remains a challenge to model and align culture due to limited cultural knowledge and a lack of exploration into effective learning approaches. We introduce a cost-efficient, cognitively grounded remedy: parameter-efficient fine-tuning on native speakers' free word-association norms, which encode implicit cultural schemas. Leveraging English-US and Mandarin associations from the Small-World-of-Words project, we adapt Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning (SFT) and PPO-based preference optimization. SFT boosts held-out association Precision at 5 by 16-20% in English and 43-165% in Mandarin, lifts median concreteness by +0.20, and attains human-level valence and arousal. These lexical gains transfer: on World-Values-Survey questions, fine-tuned models shift answer distributions toward the target culture, and on a 50-item high-tension subset, Qwen's Chinese-aligned responses double while Llama's US bias drops by one-third. Our 7-8B models rival or beat vanilla 70B baselines, showing that a few million culture-grounded associations can instill value alignment without costly retraining. Our work highlights both the promise and the need for future research grounded in human cognition in improving cultural alignment in AI models.</li>
</ul>

<h3>Title: Mitigating Easy Option Bias in Multiple-Choice Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Hao Zhang, Chen Li, Basura Fernando</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13428">https://arxiv.org/abs/2508.13428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13428">https://arxiv.org/pdf/2508.13428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13428]] Mitigating Easy Option Bias in Multiple-Choice Question Answering(https://arxiv.org/abs/2508.13428)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>In this early study, we observe an Easy-Options Bias (EOB) issue in some multiple-choice Visual Question Answering (VQA) benchmarks such as MMStar, RealWorldQA, SEED-Bench, Next-QA, STAR benchmark and Video-MME. This bias allows vision-language models (VLMs) to select the correct answer using only the vision (V) and options (O) as inputs, without the need for the question (Q). Through grounding experiments, we attribute the bias to an imbalance in visual relevance: the correct answer typically aligns more closely with the visual contents than the negative options in feature space, creating a shortcut for VLMs to infer the answer via simply vision-option similarity matching. To fix this, we introduce GroundAttack, a toolkit that automatically generates hard negative options as visually plausible as the correct answer. We apply it to the NExT-QA and MMStar datasets, creating new EOB-free annotations. On these EOB-free annotations, current VLMs approach to random accuracies under (V+O) settings, and drop to non-saturated accuracies under (V+Q+O) settings, providing a more realistic evaluation of VLMs' QA ability. Codes and new annotations will be released soon.</li>
</ul>

<h3>Title: EventTSF: Event-Aware Non-Stationary Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yunfeng Ge, Ming Jin, Yiji Zhao, Hongyan Li, Bo Du, Chang Xu, Shirui Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13434">https://arxiv.org/abs/2508.13434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13434">https://arxiv.org/pdf/2508.13434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13434]] EventTSF: Event-Aware Non-Stationary Time Series Forecasting(https://arxiv.org/abs/2508.13434)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Time series forecasting plays a vital role in critical domains like energy and transportation, where non-stationary dynamics are deeply intertwined with events in other modalities such as texts. However, incorporating natural language-based external events to improve non-stationary forecasting remains largely unexplored, as most approaches still rely on a single modality, resulting in limited contextual knowledge and model underperformance. Enabling fine-grained multimodal interactions between temporal and textual data is challenged by three fundamental issues: (1) the difficulty of fine-grained synchronization between time-varying discrete textual events and continuous time series; (2) the inherent temporal uncertainty introduced by textual semantics; and (3) the misalignment between textual event embeddings and multi-resolution temporal patterns. In this work, we address these challenges by introducing event-aware non-stationary time series forecasting (EventTSF), an autoregressive generation framework that integrates historical time series with textual events to make subsequent forecasts. Specifically, EventTSF uses autoregressive diffusion with flow matching at each step to capture nuanced temporal-event interactions. To handle event-induced uncertainty, flow matching timesteps are adaptively controlled according to event semantic signals. The underlying denoiser employs a multimodal U-shaped diffusion transformer that efficiently fuses temporal and textual modalities across different resolutions. Extensive experiments on 8 synthetic and real-world datasets show that EventTSF outperforms 12 baselines across diverse event-aware non-stationary time series forecasting scenarios, achieving substantial improvements of 10.7% higher forecasting accuracy and $1.13\times$ faster training efficiency.</li>
</ul>

<h3>Title: SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer</h3>
<ul>
<li><strong>Authors: </strong>Jiayu Fang, Zhiqi Shao, S T Boris Choy, Junbin Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13435">https://arxiv.org/abs/2508.13435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13435">https://arxiv.org/pdf/2508.13435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13435]] SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer(https://arxiv.org/abs/2508.13435)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Directed graphs are widely used to model asymmetric relationships in real-world systems. However, existing directed graph neural networks often struggle to jointly capture directional semantics and global structural patterns due to their isotropic aggregation mechanisms and localized filtering mechanisms. To address this limitation, this paper proposes SVDformer, a novel framework that synergizes SVD and Transformer architecture for direction-aware graph representation learning. SVDformer first refines singular value embeddings through multi-head self-attention, adaptively enhancing critical spectral components while suppressing high-frequency noise. This enables learnable low-pass/high-pass graph filtering without requiring spectral kernels. Furthermore, by treating singular vectors as directional projection bases and singular values as scaling factors, SVDformer uses the Transformer to model multi-scale interactions between incoming/outgoing edge patterns through attention weights, thereby explicitly preserving edge directionality during feature propagation. Extensive experiments on six directed graph benchmarks demonstrate that SVDformer consistently outperforms state-of-the-art GNNs and direction-aware baselines on node classification tasks, establishing a new paradigm for learning representations on directed graphs.</li>
</ul>

<h3>Title: Dynamic Design of Machine Learning Pipelines via Metalearning</h3>
<ul>
<li><strong>Authors: </strong>Edesio Alcobaça, André C. P. L. F. de Carvalho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13436">https://arxiv.org/abs/2508.13436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13436">https://arxiv.org/pdf/2508.13436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13436]] Dynamic Design of Machine Learning Pipelines via Metalearning(https://arxiv.org/abs/2508.13436)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Automated machine learning (AutoML) has democratized the design of machine learning based systems, by automating model selection, hyperparameter tuning and feature engineering. However, the high computational cost associated with traditional search and optimization strategies, such as Random Search, Particle Swarm Optimization and Bayesian Optimization, remains a significant challenge. Moreover, AutoML systems typically explore a large search space, which can lead to overfitting. This paper introduces a metalearning method for dynamically designing search spaces for AutoML system. The proposed method uses historical metaknowledge to select promising regions of the search space, accelerating the optimization process. According to experiments conducted for this study, the proposed method can reduce runtime by 89\% in Random Search and search space by (1.8/13 preprocessor and 4.3/16 classifier), without compromising significant predictive performance. Moreover, the proposed method showed competitive performance when adapted to Auto-Sklearn, reducing its search space. Furthermore, this study encompasses insights into meta-feature selection, meta-model explainability, and the trade-offs inherent in search space reduction strategies.</li>
</ul>

<h3>Title: Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference</h3>
<ul>
<li><strong>Authors: </strong>Yunxiang Yang, Ningning Xu, Jidong J. Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13439">https://arxiv.org/abs/2508.13439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13439">https://arxiv.org/pdf/2508.13439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13439]] Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference(https://arxiv.org/abs/2508.13439)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Comprehensive highway scene understanding and robust traffic risk inference are vital for advancing Intelligent Transportation Systems (ITS) and autonomous driving. Traditional approaches often struggle with scalability and generalization, particularly under the complex and dynamic conditions of real-world environments. To address these challenges, we introduce a novel structured prompting and knowledge distillation framework that enables automatic generation of high-quality traffic scene annotations and contextual risk assessments. Our framework orchestrates two large Vision-Language Models (VLMs): GPT-4o and o3-mini, using a structured Chain-of-Thought (CoT) strategy to produce rich, multi-perspective outputs. These outputs serve as knowledge-enriched pseudo-annotations for supervised fine-tuning of a much smaller student VLM. The resulting compact 3B-scale model, named VISTA (Vision for Intelligent Scene and Traffic Analysis), is capable of understanding low-resolution traffic videos and generating semantically faithful, risk-aware captions. Despite its significantly reduced parameter count, VISTA achieves strong performance across established captioning metrics (BLEU-4, METEOR, ROUGE-L, and CIDEr) when benchmarked against its teacher models. This demonstrates that effective knowledge distillation and structured multi-agent supervision can empower lightweight VLMs to capture complex reasoning capabilities. The compact architecture of VISTA facilitates efficient deployment on edge devices, enabling real-time risk monitoring without requiring extensive infrastructure upgrades.</li>
</ul>

<h3>Title: Hierarchy-Consistent Learning and Adaptive Loss Balancing for Hierarchical Multi-Label Classification</h3>
<ul>
<li><strong>Authors: </strong>Ruobing Jiang, Mengzhe Liu, Haobing Liu, Yanwei Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13452">https://arxiv.org/abs/2508.13452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13452">https://arxiv.org/pdf/2508.13452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13452]] Hierarchy-Consistent Learning and Adaptive Loss Balancing for Hierarchical Multi-Label Classification(https://arxiv.org/abs/2508.13452)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Hierarchical Multi-Label Classification (HMC) faces critical challenges in maintaining structural consistency and balancing loss weighting in Multi-Task Learning (MTL). In order to address these issues, we propose a classifier called HCAL based on MTL integrated with prototype contrastive learning and adaptive task-weighting mechanisms. The most significant advantage of our classifier is semantic consistency including both prototype with explicitly modeling label and feature aggregation from child classes to parent classes. The other important advantage is an adaptive loss-weighting mechanism that dynamically allocates optimization resources by monitoring task-specific convergence rates. It effectively resolves the "one-strong-many-weak" optimization bias inherent in traditional MTL approaches. To further enhance robustness, a prototype perturbation mechanism is formulated by injecting controlled noise into prototype to expand decision boundaries. Additionally, we formalize a quantitative metric called Hierarchical Violation Rate (HVR) as to evaluate hierarchical consistency and generalization. Extensive experiments across three datasets demonstrate both the higher classification accuracy and reduced hierarchical violation rate of the proposed classifier over baseline models.</li>
</ul>

<h3>Title: Revisiting MLLM Token Technology through the Lens of Classical Visual Coding</h3>
<ul>
<li><strong>Authors: </strong>Jinming Liu, Junyan Lin, Yuntao Wei, Kele Shao, Keda Tao, Jianguo Huang, Xudong Yang, Zhibo Chen, Huan Wang, Xin Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13460">https://arxiv.org/abs/2508.13460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13460">https://arxiv.org/pdf/2508.13460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13460]] Revisiting MLLM Token Technology through the Lens of Classical Visual Coding(https://arxiv.org/abs/2508.13460)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Classical visual coding and Multimodal Large Language Model (MLLM) token technology share the core objective - maximizing information fidelity while minimizing computational cost. Therefore, this paper reexamines MLLM token technology, including tokenization, token compression, and token reasoning, through the established principles of long-developed visual coding area. From this perspective, we (1) establish a unified formulation bridging token technology and visual coding, enabling a systematic, module-by-module comparative analysis; (2) synthesize bidirectional insights, exploring how visual coding principles can enhance MLLM token techniques' efficiency and robustness, and conversely, how token technology paradigms can inform the design of next-generation semantic visual codecs; (3) prospect for promising future research directions and critical unsolved challenges. In summary, this study presents the first comprehensive and structured technology comparison of MLLM token and visual coding, paving the way for more efficient multimodal models and more powerful visual codecs simultaneously.</li>
</ul>

<h3>Title: Vision Transformers for Kidney Stone Image Classification: A Comparative Study with CNNs</h3>
<ul>
<li><strong>Authors: </strong>Ivan Reyes-Amezcua, Francisco Lopez-Tiro, Clement Larose, Andres Mendez-Vazquez, Gilberto Ochoa-Ruiz, Christian Daul</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13461">https://arxiv.org/abs/2508.13461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13461">https://arxiv.org/pdf/2508.13461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13461]] Vision Transformers for Kidney Stone Image Classification: A Comparative Study with CNNs(https://arxiv.org/abs/2508.13461)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Kidney stone classification from endoscopic images is critical for personalized treatment and recurrence prevention. While convolutional neural networks (CNNs) have shown promise in this task, their limited ability to capture long-range dependencies can hinder performance under variable imaging conditions. This study presents a comparative analysis between Vision Transformers (ViTs) and CNN-based models, evaluating their performance on two ex vivo datasets comprising CCD camera and flexible ureteroscope images. The ViT-base model pretrained on ImageNet-21k consistently outperformed a ResNet50 baseline across multiple imaging conditions. For instance, in the most visually complex subset (Section patches from endoscopic images), the ViT model achieved 95.2% accuracy and 95.1% F1-score, compared to 64.5% and 59.3% with ResNet50. In the mixed-view subset from CCD-camera images, ViT reached 87.1% accuracy versus 78.4% with CNN. These improvements extend across precision and recall as well. The results demonstrate that ViT-based architectures provide superior classification performance and offer a scalable alternative to conventional CNNs for kidney stone image analysis.</li>
</ul>

<h3>Title: MINR: Efficient Implicit Neural Representations for Multi-Image Encoding</h3>
<ul>
<li><strong>Authors: </strong>Wenyong Zhou, Taiqiang Wu, Zhengwu Liu, Yuxin Cheng, Chen Zhang, Ngai Wong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13471">https://arxiv.org/abs/2508.13471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13471">https://arxiv.org/pdf/2508.13471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13471]] MINR: Efficient Implicit Neural Representations for Multi-Image Encoding(https://arxiv.org/abs/2508.13471)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Implicit Neural Representations (INRs) aim to parameterize discrete signals through implicit continuous functions. However, formulating each image with a separate neural network~(typically, a Multi-Layer Perceptron (MLP)) leads to computational and storage inefficiencies when encoding multi-images. To address this issue, we propose MINR, sharing specific layers to encode multi-image efficiently. We first compare the layer-wise weight distributions for several trained INRs and find that corresponding intermediate layers follow highly similar distribution patterns. Motivated by this, we share these intermediate layers across multiple images while preserving the input and output layers as input-specific. In addition, we design an extra novel projection layer for each image to capture its unique features. Experimental results on image reconstruction and super-resolution tasks demonstrate that MINR can save up to 60\% parameters while maintaining comparable performance. Particularly, MINR scales effectively to handle 100 images, maintaining an average peak signal-to-noise ratio (PSNR) of 34 dB. Further analysis of various backbones proves the robustness of the proposed MINR.</li>
</ul>

<h3>Title: Enhancing Robustness of Implicit Neural Representations Against Weight Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Wenyong Zhou, Yuxin Cheng, Zhengwu Liu, Taiqiang Wu, Chen Zhang, Ngai Wong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13481">https://arxiv.org/abs/2508.13481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13481">https://arxiv.org/pdf/2508.13481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13481]] Enhancing Robustness of Implicit Neural Representations Against Weight Perturbations(https://arxiv.org/abs/2508.13481)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Implicit Neural Representations (INRs) encode discrete signals in a continuous manner using neural networks, demonstrating significant value across various multimedia applications. However, the vulnerability of INRs presents a critical challenge for their real-world deployments, as the network weights might be subjected to unavoidable perturbations. In this work, we investigate the robustness of INRs for the first time and find that even minor perturbations can lead to substantial performance degradation in the quality of signal reconstruction. To mitigate this issue, we formulate the robustness problem in INRs by minimizing the difference between loss with and without weight perturbations. Furthermore, we derive a novel robust loss function to regulate the gradient of the reconstruction loss with respect to weights, thereby enhancing the robustness. Extensive experiments on reconstruction tasks across multiple modalities demonstrate that our method achieves up to a 7.5~dB improvement in peak signal-to-noise ratio (PSNR) values compared to original INRs under noisy conditions.</li>
</ul>

<h3>Title: CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Fuyang Liu, Jilin Mei, Fangyuan Mao, Chen Min, Yan Xing, Yu Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13485">https://arxiv.org/abs/2508.13485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13485">https://arxiv.org/pdf/2508.13485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13485]] CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving(https://arxiv.org/abs/2508.13485)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>4D radar-based object detection has garnered great attention for its robustness in adverse weather conditions and capacity to deliver rich spatial information across diverse driving scenarios. Nevertheless, the sparse and noisy nature of 4D radar point clouds poses substantial challenges for effective perception. To address the limitation, we present CORENet, a novel cross-modal denoising framework that leverages LiDAR supervision to identify noise patterns and extract discriminative features from raw 4D radar data. Designed as a plug-and-play architecture, our solution enables seamless integration into voxel-based detection frameworks without modifying existing pipelines. Notably, the proposed method only utilizes LiDAR data for cross-modal supervision during training while maintaining full radar-only operation during inference. Extensive evaluation on the challenging Dual-Radar dataset, which is characterized by elevated noise level, demonstrates the effectiveness of our framework in enhancing detection robustness. Comprehensive experiments validate that CORENet achieves superior performance compared to existing mainstream approaches.</li>
</ul>

<h3>Title: DyMixOp: Guiding Neural Operator Design for PDEs from a Complex Dynamics Perspective with Local-Global-Mixing</h3>
<ul>
<li><strong>Authors: </strong>Pengyu Lai, Yixiao Chen, Hui Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, nlin.CD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13490">https://arxiv.org/abs/2508.13490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13490">https://arxiv.org/pdf/2508.13490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13490]] DyMixOp: Guiding Neural Operator Design for PDEs from a Complex Dynamics Perspective with Local-Global-Mixing(https://arxiv.org/abs/2508.13490)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>A primary challenge in using neural networks to approximate nonlinear dynamical systems governed by partial differential equations (PDEs) is transforming these systems into a suitable format, especially when dealing with non-linearizable dynamics or the need for infinite-dimensional spaces for linearization. This paper introduces DyMixOp, a novel neural operator framework for PDEs that integrates insights from complex dynamical systems to address this challenge. Grounded in inertial manifold theory, DyMixOp transforms infinite-dimensional nonlinear PDE dynamics into a finite-dimensional latent space, establishing a structured foundation that maintains essential nonlinear interactions and enhances physical interpretability. A key innovation is the Local-Global-Mixing (LGM) transformation, inspired by convection dynamics in turbulence. This transformation effectively captures both fine-scale details and nonlinear interactions, while mitigating spectral bias commonly found in existing neural operators. The framework is further strengthened by a dynamics-informed architecture that connects multiple LGM layers to approximate linear and nonlinear dynamics, reflecting the temporal evolution of dynamical systems. Experimental results across diverse PDE benchmarks demonstrate that DyMixOp achieves state-of-the-art performance, significantly reducing prediction errors, particularly in convection-dominated scenarios reaching up to 86.7\%, while maintaining computational efficiency and scalability.</li>
</ul>

<h3>Title: Bridging the Gap: Doubles Badminton Analysis with Singles-Trained Models</h3>
<ul>
<li><strong>Authors: </strong>Seungheon Baek, Jinhyuk Yun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13507">https://arxiv.org/abs/2508.13507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13507">https://arxiv.org/pdf/2508.13507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13507]] Bridging the Gap: Doubles Badminton Analysis with Singles-Trained Models(https://arxiv.org/abs/2508.13507)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Badminton is known as one of the fastest racket sports in the world. Despite doubles matches being more prevalent in international tournaments than singles, previous research has mainly focused on singles due to the challenges in data availability and multi-person tracking. To address this gap, we designed an approach that transfers singles-trained models to doubles analysis. We extracted keypoints from the ShuttleSet single matches dataset using ViT-Pose and embedded them through a contrastive learning framework based on ST-GCN. To improve tracking stability, we incorporated a custom multi-object tracking algorithm that resolves ID switching issues from fast and overlapping player movements. A Transformer-based classifier then determines shot occurrences based on the learned embeddings. Our findings demonstrate the feasibility of extending pose-based shot recognition to doubles badminton, broadening analytics capabilities. This work establishes a foundation for doubles-specific datasets to enhance understanding of this predominant yet understudied format of the fast racket sport.</li>
</ul>

<h3>Title: ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hongxin Ding, Baixiang Huang, Yue Fang, Weibin Liao, Xinke Jiang, Zheng Li, Junfeng Zhao, Yasha Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13514">https://arxiv.org/abs/2508.13514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13514">https://arxiv.org/pdf/2508.13514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13514]] ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs(https://arxiv.org/abs/2508.13514)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Interactive medical questioning is essential in real-world clinical consultations, where physicians must actively gather information from patients. While medical Large Language Models (LLMs) have shown impressive capabilities in static medical question answering, they predominantly operate under a reactive paradigm: generating answers directly without seeking additional information, which risks incorrect diagnoses in such interactive settings. To address this limitation, we propose ProMed, a reinforcement learning (RL) framework that transitions medical LLMs toward a proactive paradigm, equipping them with the ability to ask clinically valuable questions before decision-making. At the core of ProMed is the Shapley Information Gain (SIG) reward, which quantifies the clinical utility of each question by combining the amount of newly acquired information with its contextual importance, estimated via Shapley values. We integrate SIG into a two-stage training pipeline: (1) SIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to construct high-reward interaction trajectories to supervise the model, and (2) SIG-Augmented Policy Optimization, which integrates SIG and enhances RL with a novel SIG-guided Reward Distribution Mechanism that assigns higher rewards to informative questions for targeted optimization. Extensive experiments on two newly curated partial-information medical benchmarks demonstrate that ProMed significantly outperforms state-of-the-art methods by an average of 6.29% and delivers a 54.45% gain over the reactive paradigm, while also generalizing robustly to out-of-domain cases.</li>
</ul>

<h3>Title: Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency</h3>
<ul>
<li><strong>Authors: </strong>Yanbiao Ma, Wei Dai, Bowei Liu, Jiayi Chen, Wenke Huang, Guancheng Wan, Zhiwu Lu, Junchi Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13518">https://arxiv.org/abs/2508.13518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13518">https://arxiv.org/pdf/2508.13518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13518]] Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency(https://arxiv.org/abs/2508.13518)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction, federate</a></li>
<li><strong>Abstract: </strong>Despite the fast progress of deep learning, one standing challenge is the gap of the observed training samples and the underlying true distribution. There are multiple reasons for the causing of this gap e.g. sampling bias, noise etc. In the era of foundation models, we show that when leveraging the off-the-shelf (vision) foundation models (e.g., CLIP, DINOv2) for feature extraction, the geometric shapes of the resulting feature distributions exhibit remarkable transferability across domains and datasets. To verify its practical usefulness, we embody our geometric knowledge-guided distribution calibration framework in two popular and challenging settings: federated learning and long-tailed recognition. In the federated setting, we devise a technique of acquiring the global geometric shape under privacy constraints, then leverage this knowledge to generate new samples for clients, in the aim of bridging the gap between local and global observations. In long-tailed learning, it utilizes the geometric knowledge transferred from sample-rich categories to recover the true distribution for sample-scarce tail classes. Comprehensive experiments show that our proposed geometric knowledge-guided distribution calibration effectively overcomes information deficits caused by data heterogeneity and sample imbalance, with boosted performance across benchmarks.</li>
</ul>

<h3>Title: Optimizing Scalar Selection in Elliptic Curve Cryptography Using Differential Evolution for Enhanced Security</h3>
<ul>
<li><strong>Authors: </strong>Takreem Haider</a></li>
<li><strong>Subjects: </strong>cs.CR, math.NT, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13520">https://arxiv.org/abs/2508.13520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13520">https://arxiv.org/pdf/2508.13520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13520]] Optimizing Scalar Selection in Elliptic Curve Cryptography Using Differential Evolution for Enhanced Security(https://arxiv.org/abs/2508.13520)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Elliptic Curve Cryptography (ECC) is a fundamental component of modern public-key cryptosystems that enable efficient and secure digital signatures, key exchanges, and encryption. Its core operation, scalar multiplication, denoted as $k \cdot P$, where $P$ is a base point and $k$ is a private scalar, relies heavily on the secrecy and unpredictability of $k$. Conventionally, $k$ is selected using user input or pseudorandom number generators. However, in resource-constrained environments with weak entropy sources, these approaches may yield low-entropy or biased scalars, increasing susceptibility to side-channel and key recovery attacks. To mitigate these vulnerabilities, we introduce an optimization-driven scalar generation method that explicitly maximizes bit-level entropy. Our approach uses differential evolution (DE), a population-based metaheuristic algorithm, to search for scalars whose binary representations exhibit maximal entropy, defined by an even and statistically uniform distribution of ones and zeros. This reformulation of scalar selection as an entropy-optimization problem enhances resistance to entropy-based cryptanalytic techniques and improves overall unpredictability. Experimental results demonstrate that DE-optimized scalars achieve entropy significantly higher than conventionally generated scalars. The proposed method can be integrated into existing ECC-based protocols, offering a deterministic, tunable alternative to traditional randomness, ideal for applications in blockchain, secure messaging, IoT, and other resource-constrained environments.</li>
</ul>

<h3>Title: Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation</h3>
<ul>
<li><strong>Authors: </strong>Hassan Barmandah</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13525">https://arxiv.org/abs/2508.13525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13525">https://arxiv.org/pdf/2508.13525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13525]] Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation(https://arxiv.org/abs/2508.13525)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) for Arabic are still dominated by Modern Standard Arabic (MSA), with limited support for Saudi dialects such as Najdi and Hijazi. This underrepresentation hinders their ability to capture authentic dialectal variation. Using a privately curated Saudi Dialect Instruction dataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50 split), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation model developed in Saudi Arabia, for Saudi dialect generation. We investigate two variants: (i) Dialect-Token training, which prepends an explicit dialect tag to the instruction, and (ii) No-Token training, which omits the tag at formatting time. Evaluation on a held-out test set combines an external dialect classifier with text fidelity metrics (chrF++ and BERTScore) and diversity measures. The Dialect-Token model achieves the best control, raising the Saudi rate from 47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity also improves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform strong generic instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct, Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control and fidelity, while avoiding metadata-tag echoing that these baselines frequently exhibit. We do not release the dataset or any model weights/adapters; instead, we release training/evaluation/inference code and a detailed datasheet (schema and aggregate statistics) to support independent verification.</li>
</ul>

<h3>Title: MATA (māta): Mindful Assessment of the Telugu Abilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chalamalasetti Kranti, Sowmya Vajjala</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13526">https://arxiv.org/abs/2508.13526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13526">https://arxiv.org/pdf/2508.13526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13526]] MATA (māta): Mindful Assessment of the Telugu Abilities of Large Language Models(https://arxiv.org/abs/2508.13526)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce MATA, a novel evaluation dataset to assess the ability of Large Language Models (LLMs) in Telugu language, comprising 729 carefully curated multiple-choice and open-ended questions that span diverse linguistic dimensions. We evaluate 11 open-weight and closed-source LLMs on our dataset and present a fine-grained analysis of their performance. Further, we empirically show how LLMs rely on superficial heuristics such as answer position and distractor patterns for multiple-choice questions. Finally, we also compare LLM-as-a-judge evaluation with human evaluation for open-ended questions and draw some conclusions on its reliability in a low-resource language. We argue that such fine-grained evaluation is essential for understanding model limitations and can inform the development of more linguistically capable LLMs, while also serving as a foundation for future research in Telugu NLP.</li>
</ul>

<h3>Title: Explainability of Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Andrés Páez</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13529">https://arxiv.org/abs/2508.13529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13529">https://arxiv.org/pdf/2508.13529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13529]] Explainability of Algorithms(https://arxiv.org/abs/2508.13529)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>The opaqueness of many complex machine learning algorithms is often mentioned as one of the main obstacles to the ethical development of artificial intelligence (AI). But what does it mean for an algorithm to be opaque? Highly complex algorithms such as artificial neural networks process enormous volumes of data in parallel along multiple hidden layers of interconnected nodes, rendering their inner workings epistemically inaccessible to any human being, including their designers and developers; they are "black boxes" for all their stakeholders. But opaqueness is not always the inevitable result of technical complexity. Sometimes, the way an algorithm works is intentionally hidden from view for proprietary reasons, especially in commercial automated decision systems, creating an entirely different type of opaqueness. In the first part of the chapter, we will examine these two ways of understanding opacity and the ethical implications that stem from each of them. In the second part, we explore the different explanatory methods that have been developed in computer science to overcome an AI system's technical opaqueness. As the analysis shows, explainable AI (XAI) still faces numerous challenges.</li>
</ul>

<h3>Title: MuFlex: A Scalable, Physics-based Platform for Multi-Building Flexibility Analysis and Coordination</h3>
<ul>
<li><strong>Authors: </strong>Ziyan Wu, Ivan Korolija, Rui Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13532">https://arxiv.org/abs/2508.13532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13532">https://arxiv.org/pdf/2508.13532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13532]] MuFlex: A Scalable, Physics-based Platform for Multi-Building Flexibility Analysis and Coordination(https://arxiv.org/abs/2508.13532)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>With the increasing penetration of renewable generation on the power grid, maintaining system balance requires coordinated demand flexibility from aggregations of buildings. Reinforcement learning (RL) has been widely explored for building controls because of its model-free nature. Open-source simulation testbeds are essential not only for training RL agents but also for fairly benchmarking control strategies. However, most building-sector testbeds target single buildings; multi-building platforms are relatively limited and typically rely on simplified models (e.g., Resistance-Capacitance) or data-driven approaches, which lack the ability to fully capture the physical intricacies and intermediate variables necessary for interpreting control performance. Moreover, these platforms often impose fixed inputs, outputs, and model formats, restricting their applicability as benchmarking tools across diverse control scenarios. To address these gaps, MuFlex, a scalable, open-source platform for benchmarking and testing control strategies for multi-building flexibility coordination, was developed in this study. MuFlex enables synchronous information exchange across EnergyPlus building models and adheres to the latest OpenAI Gym interface, providing a modular, standardized RL implementation. The platform capabilities were demonstrated in a case study coordinating demand flexibility across four office buildings using the Soft Actor-Critic algorithm with carefully fine-tuned hyperparameters. The results show that aggregating the four buildings flexibility reduced total peak demand below a specified threshold while maintaining indoor environmental quality.</li>
</ul>

<h3>Title: Compressed Models are NOT Trust-equivalent to Their Large Counterparts</h3>
<ul>
<li><strong>Authors: </strong>Rohit Raj Rai, Chirag Kothari, Siddhesh Shelke, Amit Awekar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13533">https://arxiv.org/abs/2508.13533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13533">https://arxiv.org/pdf/2508.13533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13533]] Compressed Models are NOT Trust-equivalent to Their Large Counterparts(https://arxiv.org/abs/2508.13533)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Large Deep Learning models are often compressed before being deployed in a resource-constrained environment. Can we trust the prediction of compressed models just as we trust the prediction of the original large model? Existing work has keenly studied the effect of compression on accuracy and related performance measures. However, performance parity does not guarantee trust-equivalence. We propose a two-dimensional framework for trust-equivalence evaluation. First, interpretability alignment measures whether the models base their predictions on the same input features. We use LIME and SHAP tests to measure the interpretability alignment. Second, calibration similarity measures whether the models exhibit comparable reliability in their predicted probabilities. It is assessed via ECE, MCE, Brier Score, and reliability diagrams. We conducted experiments using BERT-base as the large model and its multiple compressed variants. We focused on two text classification tasks: natural language inference and paraphrase identification. Our results reveal low interpretability alignment and significant mismatch in calibration similarity. It happens even when the accuracies are nearly identical between models. These findings show that compressed models are not trust-equivalent to their large counterparts. Deploying compressed models as a drop-in replacement for large models requires careful assessment, going beyond performance parity.</li>
</ul>

<h3>Title: EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors</h3>
<ul>
<li><strong>Authors: </strong>Shikun Zhang, Cunjian Chen, Yiqun Wang, Qiuhong Ke, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13537">https://arxiv.org/abs/2508.13537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13537">https://arxiv.org/pdf/2508.13537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13537]] EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors(https://arxiv.org/abs/2508.13537)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>High-fidelity head avatar reconstruction plays a crucial role in AR/VR, gaming, and multimedia content creation. Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated effectiveness in modeling complex geometry with real-time rendering capability and are now widely used in high-fidelity head avatar reconstruction tasks. However, existing 3DGS-based methods still face significant challenges in capturing fine-grained facial expressions and preserving local texture continuity, especially in highly deformable regions. To mitigate these limitations, we propose a novel 3DGS-based framework termed EAvatar for head reconstruction that is both expression-aware and deformation-aware. Our method introduces a sparse expression control mechanism, where a small number of key Gaussians are used to influence the deformation of their neighboring Gaussians, enabling accurate modeling of local deformations and fine-scale texture transitions. Furthermore, we leverage high-quality 3D priors from pretrained generative models to provide a more reliable facial geometry, offering structural guidance that improves convergence stability and shape accuracy during training. Experimental results demonstrate that our method produces more accurate and visually coherent head reconstructions with improved expression controllability and detail fidelity.</li>
</ul>

<h3>Title: GazeProphet: Software-Only Gaze Prediction for VR Foveated Rendering</h3>
<ul>
<li><strong>Authors: </strong>Farhaan Ebadulla, Chiraag Mudlapur, Gaurav BV</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13546">https://arxiv.org/abs/2508.13546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13546">https://arxiv.org/pdf/2508.13546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13546]] GazeProphet: Software-Only Gaze Prediction for VR Foveated Rendering(https://arxiv.org/abs/2508.13546)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Foveated rendering significantly reduces computational demands in virtual reality applications by concentrating rendering quality where users focus their gaze. Current approaches require expensive hardware-based eye tracking systems, limiting widespread adoption due to cost, calibration complexity, and hardware compatibility constraints. This paper presents GazeProphet, a software-only approach for predicting gaze locations in VR environments without requiring dedicated eye tracking hardware. The approach combines a Spherical Vision Transformer for processing 360-degree VR scenes with an LSTM-based temporal encoder that captures gaze sequence patterns. A multi-modal fusion network integrates spatial scene features with temporal gaze dynamics to predict future gaze locations with associated confidence estimates. Experimental evaluation on a comprehensive VR dataset demonstrates that GazeProphet achieves a median angular error of 3.83 degrees, outperforming traditional saliency-based baselines by 24% while providing reliable confidence calibration. The approach maintains consistent performance across different spatial regions and scene types, enabling practical deployment in VR systems without additional hardware requirements. Statistical analysis confirms the significance of improvements across all evaluation metrics. These results show that software-only gaze prediction can work for VR foveated rendering, making this performance boost more accessible to different VR platforms and apps.</li>
</ul>

<h3>Title: A Lightweight Dual-Mode Optimization for Generative Face Video Coding</h3>
<ul>
<li><strong>Authors: </strong>Zihan Zhang, Shanzhi Yin, Bolin Chen, Ru-Ling Liao, Shiqi Wang, Yan Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13547">https://arxiv.org/abs/2508.13547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13547">https://arxiv.org/pdf/2508.13547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13547]] A Lightweight Dual-Mode Optimization for Generative Face Video Coding(https://arxiv.org/abs/2508.13547)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Face Video Coding (GFVC) achieves superior rate-distortion performance by leveraging the strong inference capabilities of deep generative models. However, its practical deployment is hindered by large model parameters and high computational costs. To address this, we propose a lightweight GFVC framework that introduces dual-mode optimization - combining architectural redesign and operational refinement - to reduce complexity whilst preserving reconstruction quality. Architecturally, we replace traditional 3 x 3 convolutions with slimmer and more efficient layers, reducing complexity without compromising feature expressiveness. Operationally, we develop a two-stage adaptive channel pruning strategy: (1) soft pruning during training identifies redundant channels via learnable thresholds, and (2) hard pruning permanently eliminates these channels post-training using a derived mask. This dual-phase approach ensures both training stability and inference efficiency. Experimental results demonstrate that the proposed lightweight dual-mode optimization for GFVC can achieve 90.4% parameter reduction and 88.9% computation saving compared to the baseline, whilst achieving superior performance compared to state-of-the-art video coding standard Versatile Video Coding (VVC) in terms of perceptual-level quality metrics. As such, the proposed method is expected to enable efficient GFVC deployment in resource-constrained environments such as mobile edge devices.</li>
</ul>

<h3>Title: CALYPSO: Forecasting and Analyzing MRSA Infection Patterns with Community and Healthcare Transmission Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Rituparna Datta, Jiaming Cui, Gregory R. Madden, Anil Vullikanti</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13548">https://arxiv.org/abs/2508.13548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13548">https://arxiv.org/pdf/2508.13548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13548]] CALYPSO: Forecasting and Analyzing MRSA Infection Patterns with Community and Healthcare Transmission Dynamics(https://arxiv.org/abs/2508.13548)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Methicillin-resistant Staphylococcus aureus (MRSA) is a critical public health threat within hospitals as well as long-term care facilities. Better understanding of MRSA risks, evaluation of interventions and forecasting MRSA rates are important public health problems. Existing forecasting models rely on statistical or neural network approaches, which lack epidemiological interpretability, and have limited performance. Mechanistic epidemic models are difficult to calibrate and limited in incorporating diverse datasets. We present CALYPSO, a hybrid framework that integrates neural networks with mechanistic metapopulation models to capture the spread dynamics of infectious diseases (i.e., MRSA) across healthcare and community settings. Our model leverages patient-level insurance claims, commuting data, and healthcare transfer patterns to learn region- and time-specific parameters governing MRSA spread. This enables accurate, interpretable forecasts at multiple spatial resolutions (county, healthcare facility, region, state) and supports counterfactual analyses of infection control policies and outbreak risks. We also show that CALYPSO improves statewide forecasting performance by over 4.5% compared to machine learning baselines, while also identifying high-risk regions and cost-effective strategies for allocating infection prevention resources.</li>
</ul>

<h3>Title: DictAS: A Framework for Class-Generalizable Few-Shot Anomaly Segmentation via Dictionary Lookup</h3>
<ul>
<li><strong>Authors: </strong>Zhen Qu, Xian Tao, Xinyi Gong, ShiChen Qu, Xiaopei Zhang, Xingang Wang, Fei Shen, Zhengtao Zhang, Mukesh Prasad, Guiguang Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13560">https://arxiv.org/abs/2508.13560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13560">https://arxiv.org/pdf/2508.13560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13560]] DictAS: A Framework for Class-Generalizable Few-Shot Anomaly Segmentation via Dictionary Lookup(https://arxiv.org/abs/2508.13560)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent vision-language models (e.g., CLIP) have demonstrated remarkable class-generalizable ability to unseen classes in few-shot anomaly segmentation (FSAS), leveraging supervised prompt learning or fine-tuning on seen classes. However, their cross-category generalization largely depends on prior knowledge of real seen anomaly samples. In this paper, we propose a novel framework, namely DictAS, which enables a unified model to detect visual anomalies in unseen object categories without any retraining on the target data, only employing a few normal reference images as visual prompts. The insight behind DictAS is to transfer dictionary lookup capabilities to the FSAS task for unseen classes via self-supervised learning, instead of merely memorizing the normal and abnormal feature patterns from the training set. Specifically, DictAS mainly consists of three components: (1) **Dictionary Construction** - to simulate the index and content of a real dictionary using features from normal reference images. (2) **Dictionary Lookup** - to retrieve queried region features from the dictionary via a sparse lookup strategy. When a query feature cannot be retrieved, it is classified as an anomaly. (3) **Query Discrimination Regularization**- to enhance anomaly discrimination by making abnormal features harder to retrieve from the dictionary. To achieve this, Contrastive Query Constraint and Text Alignment Constraint are further proposed. Extensive experiments on seven public industrial and medical datasets demonstrate that DictAS consistently outperforms state-of-the-art FSAS methods.</li>
</ul>

<h3>Title: Prediction of Hospital Associated Infections During Continuous Hospital Stays</h3>
<ul>
<li><strong>Authors: </strong>Rituparna Datta, Methun Kamruzzaman, Eili Y. Klein, Gregory R Madden, Xinwei Deng, Anil Vullikanti, Parantapa Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13561">https://arxiv.org/abs/2508.13561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13561">https://arxiv.org/pdf/2508.13561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13561]] Prediction of Hospital Associated Infections During Continuous Hospital Stays(https://arxiv.org/abs/2508.13561)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The US Centers for Disease Control and Prevention (CDC), in 2019, designated Methicillin-resistant Staphylococcus aureus (MRSA) as a serious antimicrobial resistance threat. The risk of acquiring MRSA and suffering life-threatening consequences due to it remains especially high for hospitalized patients due to a unique combination of factors, including: co-morbid conditions, immuno suppression, antibiotic use, and risk of contact with contaminated hospital workers and equipment. In this paper, we present a novel generative probabilistic model, GenHAI, for modeling sequences of MRSA test results outcomes for patients during a single hospitalization. This model can be used to answer many important questions from the perspectives of hospital administrators for mitigating the risk of MRSA infections. Our model is based on the probabilistic programming paradigm, and can be used to approximately answer a variety of predictive, causal, and counterfactual questions. We demonstrate the efficacy of our model by comparing it against discriminative and generative machine learning models using two real-world datasets.</li>
</ul>

<h3>Title: Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose Inverse Kinematics</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Yang, Linfeng Dong, Wei Wang, Zhihang Zhong, Xiao Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13562">https://arxiv.org/abs/2508.13562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13562">https://arxiv.org/pdf/2508.13562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13562]] Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose Inverse Kinematics(https://arxiv.org/abs/2508.13562)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In 3D human pose and shape estimation, SMPLify remains a robust baseline that solves inverse kinematics (IK) through iterative optimization. However, its high computational cost limits its practicality. Recent advances across domains have shown that replacing iterative optimization with data-driven neural networks can achieve significant runtime improvements without sacrificing accuracy. Motivated by this trend, we propose Learnable SMPLify, a neural framework that replaces the iterative fitting process in SMPLify with a single-pass regression model. The design of our framework targets two core challenges in neural IK: data construction and generalization. To enable effective training, we propose a temporal sampling strategy that constructs initialization-target pairs from sequential frames. To improve generalization across diverse motions and unseen poses, we propose a human-centric normalization scheme and residual learning to narrow the solution space. Learnable SMPLify supports both sequential inference and plug-in post-processing to refine existing image-based estimators. Extensive experiments demonstrate that our method establishes itself as a practical and simple baseline: it achieves nearly 200x faster runtime compared to SMPLify, generalizes well to unseen 3DPW and RICH, and operates in a model-agnostic manner when used as a plug-in tool on LucidAction. The code is available at this https URL.</li>
</ul>

<h3>Title: The 9th AI City Challenge</h3>
<ul>
<li><strong>Authors: </strong>Zheng Tang, Shuo Wang, David C. Anastasiu, Ming-Ching Chang, Anuj Sharma, Quan Kong, Norimasa Kobori, Munkhjargal Gochoo, Ganzorig Batnasan, Munkh-Erdene Otgonbold, Fady Alnajjar, Jun-Wei Hsieh, Tomasz Kornuta, Xiaolong Li, Yilin Zhao, Han Zhang, Subhashree Radhakrishnan, Arihant Jain, Ratnesh Kumar, Vidya N. Murali, Yuxing Wang, Sameer Satish Pusegaonkar, Yizhou Wang, Sujit Biswas, Xunlei Wu, Zhedong Zheng, Pranamesh Chakraborty, Rama Chellappa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13564">https://arxiv.org/abs/2508.13564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13564">https://arxiv.org/pdf/2508.13564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13564]] The 9th AI City Challenge(https://arxiv.org/abs/2508.13564)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The ninth AI City Challenge continues to advance real-world applications of computer vision and AI in transportation, industrial automation, and public safety. The 2025 edition featured four tracks and saw a 17% increase in participation, with 245 teams from 15 countries registered on the evaluation server. Public release of challenge datasets led to over 30,000 downloads to date. Track 1 focused on multi-class 3D multi-camera tracking, involving people, humanoids, autonomous mobile robots, and forklifts, using detailed calibration and 3D bounding box annotations. Track 2 tackled video question answering in traffic safety, with multi-camera incident understanding enriched by 3D gaze labels. Track 3 addressed fine-grained spatial reasoning in dynamic warehouse environments, requiring AI systems to interpret RGB-D inputs and answer spatial questions that combine perception, geometry, and language. Both Track 1 and Track 3 datasets were generated in NVIDIA Omniverse. Track 4 emphasized efficient road object detection from fisheye cameras, supporting lightweight, real-time deployment on edge devices. The evaluation framework enforced submission limits and used a partially held-out test set to ensure fair benchmarking. Final rankings were revealed after the competition concluded, fostering reproducibility and mitigating overfitting. Several teams achieved top-tier results, setting new benchmarks in multiple tasks.</li>
</ul>

<h3>Title: Generative Model-Based Feature Attention Module for Video Action Analysis</h3>
<ul>
<li><strong>Authors: </strong>Guiqin Wang, Peng Zhao, Cong Zhao, Jing Huang, Siyan Guo, Shusen Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13565">https://arxiv.org/abs/2508.13565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13565">https://arxiv.org/pdf/2508.13565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13565]] Generative Model-Based Feature Attention Module for Video Action Analysis(https://arxiv.org/abs/2508.13565)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, generative</a></li>
<li><strong>Abstract: </strong>Video action analysis is a foundational technology within the realm of intelligent video comprehension, particularly concerning its application in Internet of Things(IoT). However, existing methodologies overlook feature semantics in feature extraction and focus on optimizing action proposals, thus these solutions are unsuitable for widespread adoption in high-performance IoT applications due to the limitations in precision, such as autonomous driving, which necessitate robust and scalable intelligent video analytics analysis. To address this issue, we propose a novel generative attention-based model to learn the relation of feature semantics. Specifically, by leveraging the differences of actions' foreground and background, our model simultaneously learns the frame- and segment-dependencies of temporal action feature semantics, which takes advantage of feature semantics in the feature extraction effectively. To evaluate the effectiveness of our model, we conduct extensive experiments on two benchmark video task, action recognition and action detection. In the context of action detection tasks, we substantiate the superiority of our approach through comprehensive validation on widely recognized datasets. Moreover, we extend the validation of the effectiveness of our proposed method to a broader task, video action recognition. Our code is available at this https URL.</li>
</ul>

<h3>Title: A Comparative Study of Decoding Strategies in Medical Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Oriana Presacan, Alireza Nik, Vajira Thambawita, Bogdan Ionescu, Michael Riegler</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13580">https://arxiv.org/abs/2508.13580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13580">https://arxiv.org/pdf/2508.13580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13580]] A Comparative Study of Decoding Strategies in Medical Text Generation(https://arxiv.org/abs/2508.13580)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) rely on various decoding strategies to generate text, and these choices can significantly affect output quality. In healthcare, where accuracy is critical, the impact of decoding strategies remains underexplored. We investigate this effect in five open-ended medical tasks, including translation, summarization, question answering, dialogue, and image captioning, evaluating 11 decoding strategies with medically specialized and general-purpose LLMs of different sizes. Our results show that deterministic strategies generally outperform stochastic ones: beam search achieves the highest scores, while {\eta} and top-k sampling perform worst. Slower decoding methods tend to yield better quality. Larger models achieve higher scores overall but have longer inference times and are no more robust to decoding. Surprisingly, while medical LLMs outperform general ones in two of the five tasks, statistical analysis shows no overall performance advantage and reveals greater sensitivity to decoding choice. We further compare multiple evaluation metrics and find that correlations vary by task, with MAUVE showing weak agreement with BERTScore and ROUGE, as well as greater sensitivity to the decoding strategy. These results highlight the need for careful selection of decoding methods in medical applications, as their influence can sometimes exceed that of model choice.</li>
</ul>

<h3>Title: Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Ruixin Zhang, Jiaqing Fan, Yifan Liao, Qian Qiao, Fanzhang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13584">https://arxiv.org/abs/2508.13584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13584">https://arxiv.org/pdf/2508.13584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13584]] Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model(https://arxiv.org/abs/2508.13584)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Referring Video Object Segmentation (RVOS) aims to segment specific objects in a video according to textual descriptions. We observe that recent RVOS approaches often place excessive emphasis on feature extraction and temporal modeling, while relatively neglecting the design of the segmentation head. In fact, there remains considerable room for improvement in segmentation head design. To address this, we propose a Temporal-Conditional Referring Video Object Segmentation model, which innovatively integrates existing segmentation methods to effectively enhance boundary segmentation capability. Furthermore, our model leverages a text-to-video diffusion model for feature extraction. On top of this, we remove the traditional noise prediction module to avoid the randomness of noise from degrading segmentation accuracy, thereby simplifying the model while improving performance. Finally, to overcome the limited feature extraction capability of the VAE, we design a Temporal Context Mask Refinement (TCMR) module, which significantly improves segmentation quality without introducing complex designs. We evaluate our method on four public RVOS benchmarks, where it consistently achieves state-of-the-art performance.</li>
</ul>

<h3>Title: CAI Fluency: A Framework for Cybersecurity AI Fluency</h3>
<ul>
<li><strong>Authors: </strong>Víctor Mayoral-Vilches, Jasmin Wachter, Cristóbal R. J. Veas Chavez, Cathrin Schachner, Luis Javier Navarrete-Lozano, María Sanz-Gómez</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13588">https://arxiv.org/abs/2508.13588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13588">https://arxiv.org/pdf/2508.13588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13588]] CAI Fluency: A Framework for Cybersecurity AI Fluency(https://arxiv.org/abs/2508.13588)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>This work introduces CAI Fluency, an an educational platform of the Cybersecurity AI (CAI) framework dedicated to democratizing the knowledge and application of cybersecurity AI tools in the global security community. The main objective of the CAI framework is to accelerate the widespread adoption and effective use of artificial intelligence-based cybersecurity solutions, pathing the way to vibe-hacking, the cybersecurity analogon to vibe-coding. CAI Fluency builds upon the Framework for AI Fluency, adapting its three modalities of human-AI interaction and four core competencies specifically for cybersecurity applications. This theoretical foundation ensures that practitioners develop not just technical skills, but also the critical thinking and ethical awareness necessary for responsible AI use in security contexts. This technical report serves as a white-paper, as well as detailed educational and practical guide that helps users understand the principles behind the CAI framework, and educates them how to apply this knowledge in their projects and real-world security contexts.</li>
</ul>

<h3>Title: Bridging Clear and Adverse Driving Conditions</h3>
<ul>
<li><strong>Authors: </strong>Yoel Shapiro, Yahia Showgan, Koustav Mullick</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13592">https://arxiv.org/abs/2508.13592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13592">https://arxiv.org/pdf/2508.13592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13592]] Bridging Clear and Adverse Driving Conditions(https://arxiv.org/abs/2508.13592)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Autonomous Driving (AD) systems exhibit markedly degraded performance under adverse environmental conditions, such as low illumination and precipitation. The underrepresentation of adverse conditions in AD datasets makes it challenging to address this deficiency. To circumvent the prohibitive cost of acquiring and annotating adverse weather data, we propose a novel Domain Adaptation (DA) pipeline that transforms clear-weather images into fog, rain, snow, and nighttime images. Here, we systematically develop and evaluate several novel data-generation pipelines, including simulation-only, GAN-based, and hybrid diffusion-GAN approaches, to synthesize photorealistic adverse images from labelled clear images. We leverage an existing DA GAN, extend it to support auxiliary inputs, and develop a novel training recipe that leverages both simulated and real images. The simulated images facilitate exact supervision by providing perfectly matched image pairs, while the real images help bridge the simulation-to-real (sim2real) gap. We further introduce a method to mitigate hallucinations and artifacts in Stable-Diffusion Image-to-Image (img2img) outputs by blending them adaptively with their progenitor images. We finetune downstream models on our synthetic data and evaluate them on the Adverse Conditions Dataset with Correspondences (ACDC). We achieve 1.85 percent overall improvement in semantic segmentation, and 4.62 percent on nighttime, demonstrating the efficacy of our hybrid method for robust AD perception under challenging conditions.</li>
</ul>

<h3>Title: Towards Efficient Vision State Space Models via Token Merging</h3>
<ul>
<li><strong>Authors: </strong>Jinyoung Park, Minseok Son, Changick Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13599">https://arxiv.org/abs/2508.13599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13599">https://arxiv.org/pdf/2508.13599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13599]] Towards Efficient Vision State Space Models via Token Merging(https://arxiv.org/abs/2508.13599)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>State Space Models (SSMs) have emerged as powerful architectures in computer vision, yet improving their computational efficiency remains crucial for practical and scalable this http URL token reduction serves as an effective approach for model efficiency, applying it to SSMs requires careful consideration of their unique sequential modeling this http URL this work, we propose MaMe, a token-merging strategy tailored for SSM-based vision this http URL addresses two key challenges: quantifying token importance and preserving sequential properties. Our approach leverages the state transition parameter $\mathbf{\Delta}$ as an informativeness measure and introduces strategic token arrangements to preserve sequential information this http URL experiments demonstrate that MaMe achieves superior efficiency-performance trade-offs for both fine-tuned and off-the-shelf models. Particularly, our approach maintains robustness even under aggressive token reduction where existing methods undergo significant performance this http URL image classification, MaMe shows strong generalization capabilities across video and audio domains, establishing an effective approach for enhancing efficiency in diverse SSM applications.</li>
</ul>

<h3>Title: PersonaVlog: Personalized Multimodal Vlog Generation with Multi-Agent Collaboration and Iterative Self-Correction</h3>
<ul>
<li><strong>Authors: </strong>Xiaolu Hou, Bing Ma, Jiaxiang Cheng, Xuhua Ren, Kai Yu, Wenyue Li, Tianxiang Zheng, Qinglin Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13602">https://arxiv.org/abs/2508.13602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13602">https://arxiv.org/pdf/2508.13602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13602]] PersonaVlog: Personalized Multimodal Vlog Generation with Multi-Agent Collaboration and Iterative Self-Correction(https://arxiv.org/abs/2508.13602)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>With the growing demand for short videos and personalized content, automated Video Log (Vlog) generation has become a key direction in multimodal content creation. Existing methods mostly rely on predefined scripts, lacking dynamism and personal expression. Therefore, there is an urgent need for an automated Vlog generation approach that enables effective multimodal collaboration and high personalization. To this end, we propose PersonaVlog, an automated multimodal stylized Vlog generation framework that can produce personalized Vlogs featuring videos, background music, and inner monologue speech based on a given theme and reference image. Specifically, we propose a multi-agent collaboration framework based on Multimodal Large Language Models (MLLMs). This framework efficiently generates high-quality prompts for multimodal content creation based on user input, thereby improving the efficiency and creativity of the process. In addition, we incorporate a feedback and rollback mechanism that leverages MLLMs to evaluate and provide feedback on generated results, thereby enabling iterative self-correction of multimodal content. We also propose ThemeVlogEval, a theme-based automated benchmarking framework that provides standardized metrics and datasets for fair evaluation. Comprehensive experiments demonstrate the significant advantages and potential of our framework over several baselines, highlighting its effectiveness and great potential for generating automated Vlogs.</li>
</ul>

<h3>Title: Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM</h3>
<ul>
<li><strong>Authors: </strong>Dariia Puhach, Amir H. Payberah, Éva Székely</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13603">https://arxiv.org/abs/2508.13603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13603">https://arxiv.org/pdf/2508.13603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13603]] Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM(https://arxiv.org/abs/2508.13603)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Similar to text-based Large Language Models (LLMs), Speech-LLMs exhibit emergent abilities and context awareness. However, whether these similarities extend to gender bias remains an open question. This study proposes a methodology leveraging speaker assignment as an analytic tool for bias investigation. Unlike text-based models, which encode gendered associations implicitly, Speech-LLMs must produce a gendered voice, making speaker selection an explicit bias cue. We evaluate Bark, a Text-to-Speech (TTS) model, analyzing its default speaker assignments for textual prompts. If Bark's speaker selection systematically aligns with gendered associations, it may reveal patterns in its training data or model design. To test this, we construct two datasets: (i) Professions, containing gender-stereotyped occupations, and (ii) Gender-Colored Words, featuring gendered connotations. While Bark does not exhibit systematic bias, it demonstrates gender awareness and has some gender inclinations.</li>
</ul>

<h3>Title: AdaDocVQA: Adaptive Framework for Long Document Visual Question Answering in Low-Resource Settings</h3>
<ul>
<li><strong>Authors: </strong>Haoxuan Li, Wei Song, Aofan Liu, Peiwu Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13606">https://arxiv.org/abs/2508.13606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13606">https://arxiv.org/pdf/2508.13606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13606]] AdaDocVQA: Adaptive Framework for Long Document Visual Question Answering in Low-Resource Settings(https://arxiv.org/abs/2508.13606)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Document Visual Question Answering (Document VQA) faces significant challenges when processing long documents in low-resource environments due to context limitations and insufficient training data. This paper presents AdaDocVQA, a unified adaptive framework addressing these challenges through three core innovations: a hybrid text retrieval architecture for effective document segmentation, an intelligent data augmentation pipeline that automatically generates high-quality reasoning question-answer pairs with multi-level verification, and adaptive ensemble inference with dynamic configuration generation and early stopping mechanisms. Experiments on Japanese document VQA benchmarks demonstrate substantial improvements with 83.04\% accuracy on Yes/No questions, 52.66\% on factual questions, and 44.12\% on numerical questions in JDocQA, and 59\% accuracy on LAVA dataset. Ablation studies confirm meaningful contributions from each component, and our framework establishes new state-of-the-art results for Japanese document VQA while providing a scalable foundation for other low-resource languages and specialized domains. Our code available at: this https URL.</li>
</ul>

<h3>Title: Bounding Causal Effects and Counterfactuals</h3>
<ul>
<li><strong>Authors: </strong>Tobias Maringgele</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13607">https://arxiv.org/abs/2508.13607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13607">https://arxiv.org/pdf/2508.13607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13607]] Bounding Causal Effects and Counterfactuals(https://arxiv.org/abs/2508.13607)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Causal inference often hinges on strong assumptions - such as no unmeasured confounding or perfect compliance - that are rarely satisfied in practice. Partial identification offers a principled alternative: instead of relying on unverifiable assumptions to estimate causal effects precisely, it derives bounds that reflect the uncertainty inherent in the data. Despite its theoretical appeal, partial identification remains underutilized in applied work, in part due to the fragmented nature of existing methods and the lack of practical guidance. This thesis addresses these challenges by systematically comparing a diverse set of bounding algorithms across multiple causal scenarios. We implement, extend, and unify state-of-the-art methods - including symbolic, optimization-based, and information-theoretic approaches - within a common evaluation framework. In particular, we propose an extension of a recently introduced entropy-bounded method, making it applicable to counterfactual queries such as the Probability of Necessity and Sufficiency (PNS). Our empirical study spans thousands of randomized simulations involving both discrete and continuous data-generating processes. We assess each method in terms of bound tightness, computational efficiency, and robustness to assumption violations. To support practitioners, we distill our findings into a practical decision tree for algorithm selection and train a machine learning model to predict the best-performing method based on observable data characteristics. All implementations are released as part of an open-source Python package, CausalBoundingEngine, which enables users to apply and compare bounding methods through a unified interface.</li>
</ul>

<h3>Title: RCGNet: RGB-based Category-Level 6D Object Pose Estimation with Geometric Guidance</h3>
<ul>
<li><strong>Authors: </strong>Sheng Yu, Di-Hua Zhai, Yuanqing Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13623">https://arxiv.org/abs/2508.13623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13623">https://arxiv.org/pdf/2508.13623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13623]] RCGNet: RGB-based Category-Level 6D Object Pose Estimation with Geometric Guidance(https://arxiv.org/abs/2508.13623)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While most current RGB-D-based category-level object pose estimation methods achieve strong performance, they face significant challenges in scenes lacking depth information. In this paper, we propose a novel category-level object pose estimation approach that relies solely on RGB images. This method enables accurate pose estimation in real-world scenarios without the need for depth data. Specifically, we design a transformer-based neural network for category-level object pose estimation, where the transformer is employed to predict and fuse the geometric features of the target object. To ensure that these predicted geometric features faithfully capture the object's geometry, we introduce a geometric feature-guided algorithm, which enhances the network's ability to effectively represent the object's geometric information. Finally, we utilize the RANSAC-PnP algorithm to compute the object's pose, addressing the challenges associated with variable object scales in pose estimation. Experimental results on benchmark datasets demonstrate that our approach is not only highly efficient but also achieves superior accuracy compared to previous RGB-based methods. These promising results offer a new perspective for advancing category-level object pose estimation using RGB images.</li>
</ul>

<h3>Title: Towards a Larger Model via One-Shot Federated Learning on Heterogeneous Client Models</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Ye, Xueli An, Onur Ayan, Junfan Wang, Xueqiang Yan, Georg Carle</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13625">https://arxiv.org/abs/2508.13625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13625">https://arxiv.org/pdf/2508.13625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13625]] Towards a Larger Model via One-Shot Federated Learning on Heterogeneous Client Models(https://arxiv.org/abs/2508.13625)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Large models, renowned for superior performance, outperform smaller ones even without billion-parameter scales. While mobile network servers have ample computational resources to support larger models than client devices, privacy constraints prevent clients from directly sharing their raw data. Federated Learning (FL) enables decentralized clients to collaboratively train a shared model by exchanging model parameters instead of transmitting raw data. Yet, it requires a uniform model architecture and multiple communication rounds, which neglect resource heterogeneity, impose heavy computational demands on clients, and increase communication overhead. To address these challenges, we propose FedOL, to construct a larger and more comprehensive server model in one-shot settings (i.e., in a single communication round). Instead of model parameter sharing, FedOL employs knowledge distillation, where clients only exchange model prediction outputs on an unlabeled public dataset. This reduces communication overhead by transmitting compact predictions instead of full model weights and enables model customization by allowing heterogeneous model architectures. A key challenge in this setting is that client predictions may be biased due to skewed local data distributions, and the lack of ground-truth labels in the public dataset further complicates reliable learning. To mitigate these issues, FedOL introduces a specialized objective function that iteratively refines pseudo-labels and the server model, improving learning reliability. To complement this, FedOL incorporates a tailored pseudo-label generation and knowledge distillation strategy that effectively integrates diverse knowledge. Simulation results show that FedOL significantly outperforms existing baselines, offering a cost-effective solution for mobile networks where clients possess valuable private data but limited computational resources.</li>
</ul>

<h3>Title: DiffIER: Optimizing Diffusion Models with Iterative Error Reduction</h3>
<ul>
<li><strong>Authors: </strong>Ao Chen, Lihe Ding, Tianfan Xue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13628">https://arxiv.org/abs/2508.13628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13628">https://arxiv.org/pdf/2508.13628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13628]] DiffIER: Optimizing Diffusion Models with Iterative Error Reduction(https://arxiv.org/abs/2508.13628)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable capabilities in generating high-quality samples and enhancing performance across diverse domains through Classifier-Free Guidance (CFG). However, the quality of generated samples is highly sensitive to the selection of the guidance weight. In this work, we identify a critical ``training-inference gap'' and we argue that it is the presence of this gap that undermines the performance of conditional generation and renders outputs highly sensitive to the guidance weight. We quantify this gap by measuring the accumulated error during the inference stage and establish a correlation between the selection of guidance weight and minimizing this gap. Furthermore, to mitigate this gap, we propose DiffIER, an optimization-based method for high-quality generation. We demonstrate that the accumulated error can be effectively reduced by an iterative error minimization at each step during inference. By introducing this novel plug-and-play optimization framework, we enable the optimization of errors at every single inference step and enhance generation quality. Empirical results demonstrate that our proposed method outperforms baseline approaches in conditional generation tasks. Furthermore, the method achieves consistent success in text-to-image generation, image super-resolution, and text-to-speech generation, underscoring its versatility and potential for broad applications in future research.</li>
</ul>

<h3>Title: Text2Weight: Bridging Natural Language and Neural Network Weight Spaces</h3>
<ul>
<li><strong>Authors: </strong>Bowen Tian, Wenshuo Chen, Zexi Li, Songning Lai, Jiemin Wu, Yutao Yue</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13633">https://arxiv.org/abs/2508.13633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13633">https://arxiv.org/pdf/2508.13633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13633]] Text2Weight: Bridging Natural Language and Neural Network Weight Spaces(https://arxiv.org/abs/2508.13633)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>How far are we really from automatically generating neural networks? While neural network weight generation shows promise, current approaches struggle with generalization to unseen tasks and practical application exploration. To address this, we propose T2W, a diffusion transformer framework that generates task-specific weights conditioned on natural language descriptions. T2W hierarchically processes network parameters into uniform blocks, integrates text embeddings from CLIP via a prior attention mechanism, and employs adversarial training with weight-space augmentation to enhance generalization. Experiments on Cifar100, Caltech256, and TinyImageNet demonstrate T2W's ability to produce high-quality weights for unseen tasks, outperforming optimization-based initialization and enabling novel applications such as weight enhancement and text-guided model fusion. Our work bridges textual semantics with weight-space dynamics, supported by an open-source dataset of text-weight pairs, advancing the practicality of generative models in neural network parameter synthesis. Our code is available on Github.</li>
</ul>

<h3>Title: Explainable Learning Rate Regimes for Stochastic Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zhuang Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13639">https://arxiv.org/abs/2508.13639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13639">https://arxiv.org/pdf/2508.13639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13639]] Explainable Learning Rate Regimes for Stochastic Optimization(https://arxiv.org/abs/2508.13639)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Modern machine learning is trained by stochastic gradient descent (SGD), whose performance critically depends on how the learning rate (LR) is adjusted and decreased over time. Yet existing LR regimes may be intricate, or need to tune one or more additional hyper-parameters manually whose bottlenecks include huge computational expenditure, time and power in practice. This work, in a natural and direct manner, clarifies how LR should be updated automatically only according to the intrinsic variation of stochastic gradients. An explainable LR regime by leveraging stochastic second-order algorithms is developed, behaving a similar pattern to heuristic algorithms but implemented simply without any parameter tuning requirement, where it is of an automatic procedure that LR should increase (decrease) as the norm of stochastic gradients decreases (increases). The resulting LR regime shows its efficiency, robustness, and scalability in different classical stochastic algorithms, containing SGD, SGDM, and SIGNSGD, on machine learning tasks.</li>
</ul>

<h3>Title: Personalized Subgraph Federated Learning with Sheaf Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Wenfei Liang, Yanan Zhao, Rui She, Yiming Li, Wee Peng Tay</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13642">https://arxiv.org/abs/2508.13642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13642">https://arxiv.org/pdf/2508.13642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13642]] Personalized Subgraph Federated Learning with Sheaf Collaboration(https://arxiv.org/abs/2508.13642)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, diffusion</a></li>
<li><strong>Abstract: </strong>Graph-structured data is prevalent in many applications. In subgraph federated learning (FL), this data is distributed across clients, each with a local subgraph. Personalized subgraph FL aims to develop a customized model for each client to handle diverse data distributions. However, performance variation across clients remains a key issue due to the heterogeneity of local subgraphs. To overcome the challenge, we propose FedSheafHN, a novel framework built on a sheaf collaboration mechanism to unify enhanced client descriptors with efficient personalized model generation. Specifically, FedSheafHN embeds each client's local subgraph into a server-constructed collaboration graph by leveraging graph-level embeddings and employing sheaf diffusion within the collaboration graph to enrich client representations. Subsequently, FedSheafHN generates customized client models via a server-optimized hypernetwork. Empirical evaluations demonstrate that FedSheafHN outperforms existing personalized subgraph FL methods on various graph datasets. Additionally, it exhibits fast model convergence and effectively generalizes to new clients.</li>
</ul>

<h3>Title: CRISP: Persistent Concept Unlearning via Sparse Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Tomer Ashuach, Dana Arad, Aaron Mueller, Martin Tutek, Yonatan Belinkov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13650">https://arxiv.org/abs/2508.13650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13650">https://arxiv.org/pdf/2508.13650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13650]] CRISP: Persistent Concept Unlearning via Sparse Autoencoders(https://arxiv.org/abs/2508.13650)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are increasingly deployed in real-world applications, the need to selectively remove unwanted knowledge while preserving model utility has become paramount. Recent work has explored sparse autoencoders (SAEs) to perform precise interventions on monosemantic features. However, most SAE-based methods operate at inference time, which does not create persistent changes in the model's parameters. Such interventions can be bypassed or reversed by malicious actors with parameter access. We introduce CRISP, a parameter-efficient method for persistent concept unlearning using SAEs. CRISP automatically identifies salient SAE features across multiple layers and suppresses their activations. We experiment with two LLMs and show that our method outperforms prior approaches on safety-critical unlearning tasks from the WMDP benchmark, successfully removing harmful knowledge while preserving general and in-domain capabilities. Feature-level analysis reveals that CRISP achieves semantically coherent separation between target and benign concepts, allowing precise suppression of the target features.</li>
</ul>

<h3>Title: Input Time Scaling</h3>
<ul>
<li><strong>Authors: </strong>Rapheal Huang (Yuming), Weilong Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13654">https://arxiv.org/abs/2508.13654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13654">https://arxiv.org/pdf/2508.13654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13654]] Input Time Scaling(https://arxiv.org/abs/2508.13654)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Current Large Language Models (LLMs) are usually post-trained on large-scale carefully curated datasets (data & training scaling) and doing reasoning in test time (inference time scaling). In this work, we present a new scaling paradigm, Input Time Scaling, to complement previous scaling methods by putting resources on queries (input time). During training and testing, we combine meta-knowledge from LLMs to refine inputs with different strategies. We also find a new phenomenon, training-testing co-design there. We need to apply query strategies during both training and testing. Only applying strategies on training or testing would seriously degrade the performance. We are also surprised to find that seemingly low data quality datasets can gain high performance. Adding irrelevant information to the queries, randomly selecting examples from a minimally filtered dataset, can even perform the best. These findings contradict the widely held inductive bias, "garbage in, garbage out". Curating datasets with seemingly high-quality data can even potentially limit the performance ceiling. In addition, models trained on more data with similar quality (15k VS 1k) perform worse, simple dataset size scaling should also be carefully inspected. The good news is that our findings are compatible with the Less is More phenomenon. A small set of examples is enough to evoke high-level reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct, we are able to reach SOTA performance among 32B models on AIME24(76.7%) and AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B, the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate reproducibility and further research, we are working on open-source our datasets, data pipelines, evaluation results, and checkpoints.</li>
</ul>

<h3>Title: DeH4R: A Decoupled and Hybrid Method for Road Network Graph Extraction</h3>
<ul>
<li><strong>Authors: </strong>Dengxian Gong, Shunping Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13669">https://arxiv.org/abs/2508.13669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13669">https://arxiv.org/pdf/2508.13669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13669]] DeH4R: A Decoupled and Hybrid Method for Road Network Graph Extraction(https://arxiv.org/abs/2508.13669)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>The automated extraction of complete and precise road network graphs from remote sensing imagery remains a critical challenge in geospatial computer vision. Segmentation-based approaches, while effective in pixel-level recognition, struggle to maintain topology fidelity after vectorization postprocessing. Graph-growing methods build more topologically faithful graphs but suffer from computationally prohibitive iterative ROI cropping. Graph-generating methods first predict global static candidate road network vertices, and then infer possible edges between vertices. They achieve fast topology-aware inference, but limits the dynamic insertion of vertices. To address these challenges, we propose DeH4R, a novel hybrid model that combines graph-generating efficiency and graph-growing dynamics. This is achieved by decoupling the task into candidate vertex detection, adjacent vertex prediction, initial graph contruction, and graph expansion. This architectural innovation enables dynamic vertex (edge) insertions while retaining fast inference speed and enhancing both topology fidelity and spatial consistency. Comprehensive evaluations on CityScale and SpaceNet benchmarks demonstrate state-of-the-art (SOTA) performance. DeH4R outperforms the prior SOTA graph-growing method RNGDet++ by 4.62 APLS and 10.18 IoU on CityScale, while being approximately 10 $\times$ faster. The code will be made publicly available at this https URL.</li>
</ul>

<h3>Title: Heavy-tailed Linear Bandits: Adversarial Robustness, Best-of-both-worlds, and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Canzhe Zhao, Shinji Ito, Shuai Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13679">https://arxiv.org/abs/2508.13679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13679">https://arxiv.org/pdf/2508.13679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13679]] Heavy-tailed Linear Bandits: Adversarial Robustness, Best-of-both-worlds, and Beyond(https://arxiv.org/abs/2508.13679)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Heavy-tailed bandits have been extensively studied since the seminal work of \citet{Bubeck2012BanditsWH}. In particular, heavy-tailed linear bandits, enabling efficient learning with both a large number of arms and heavy-tailed noises, have recently attracted significant attention \citep{ShaoYKL18,XueWWZ20,ZhongHYW21,Wang2025heavy,tajdini2025improved}. However, prior studies focus almost exclusively on stochastic regimes, with few exceptions limited to the special case of heavy-tailed multi-armed bandits (MABs) \citep{Huang0H22,ChengZ024,Chen2024uniINF}. In this work, we propose a general framework for adversarial heavy-tailed bandit problems, which performs follow-the-regularized-leader (FTRL) over the loss estimates shifted by a bonus function. Via a delicate setup of the bonus function, we devise the first FTRL-type best-of-both-worlds (BOBW) algorithm for heavy-tailed MABs, which does not require the truncated non-negativity assumption and achieves an $\widetilde{O}(T^{\frac{1}{\varepsilon}})$ worst-case regret in the adversarial regime as well as an $\widetilde{O}(\log T)$ gap-dependent regret in the stochastic regime. We then extend our framework to the linear case, proposing the first algorithm for adversarial heavy-tailed linear bandits with finite arm sets. This algorithm achieves an $\widetilde{O}(d^{\frac{1}{2}}T^{\frac{1}{\varepsilon}})$ regret, matching the best-known worst-case regret bound in stochastic regimes. Moreover, we propose a general data-dependent learning rate, termed \textit{heavy-tailed noise aware stability-penalty matching} (HT-SPM). We prove that HT-SPM guarantees BOBW regret bounds for general heavy-tailed bandit problems once certain conditions are satisfied. By using HT-SPM and, in particular, a variance-reduced linear loss estimator, we obtain the first BOBW result for heavy-tailed linear bandits.</li>
</ul>

<h3>Title: Know Me by My Pulse: Toward Practical Continuous Authentication on Wearable Devices via Wrist-Worn PPG</h3>
<ul>
<li><strong>Authors: </strong>Wei Shao, Zequan Liang, Ruoyu Zhang, Ruijie Fang, Ning Miao, Ehsan Kourkchi, Setareh Rafatirad, Houman Homayoun, Chongzhou Fang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13690">https://arxiv.org/abs/2508.13690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13690">https://arxiv.org/pdf/2508.13690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13690]] Know Me by My Pulse: Toward Practical Continuous Authentication on Wearable Devices via Wrist-Worn PPG(https://arxiv.org/abs/2508.13690)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust, biometric</a></li>
<li><strong>Abstract: </strong>Biometric authentication using physiological signals offers a promising path toward secure and user-friendly access control in wearable devices. While electrocardiogram (ECG) signals have shown high discriminability, their intrusive sensing requirements and discontinuous acquisition limit practicality. Photoplethysmography (PPG), on the other hand, enables continuous, non-intrusive authentication with seamless integration into wrist-worn wearable devices. However, most prior work relies on high-frequency PPG (e.g., 75 - 500 Hz) and complex deep models, which incur significant energy and computational overhead, impeding deployment in power-constrained real-world systems. In this paper, we present the first real-world implementation and evaluation of a continuous authentication system on a smartwatch, We-Be Band, using low-frequency (25 Hz) multi-channel PPG signals. Our method employs a Bi-LSTM with attention mechanism to extract identity-specific features from short (4 s) windows of 4-channel PPG. Through extensive evaluations on both public datasets (PTTPPG) and our We-Be Dataset (26 subjects), we demonstrate strong classification performance with an average test accuracy of 88.11%, macro F1-score of 0.88, False Acceptance Rate (FAR) of 0.48%, False Rejection Rate (FRR) of 11.77%, and Equal Error Rate (EER) of 2.76%. Our 25 Hz system reduces sensor power consumption by 53% compared to 512 Hz and 19% compared to 128 Hz setups without compromising performance. We find that sampling at 25 Hz preserves authentication accuracy, whereas performance drops sharply at 20 Hz while offering only trivial additional power savings, underscoring 25 Hz as the practical lower bound. Additionally, we find that models trained exclusively on resting data fail under motion, while activity-diverse training improves robustness across physiological states.</li>
</ul>

<h3>Title: Diversity-enhanced Collaborative Mamba for Semi-supervised Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Shumeng Li, Jian Zhang, Lei Qi, Luping Zhou, Yinghuan Shi, Yang Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13712">https://arxiv.org/abs/2508.13712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13712">https://arxiv.org/pdf/2508.13712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13712]] Diversity-enhanced Collaborative Mamba for Semi-supervised Medical Image Segmentation(https://arxiv.org/abs/2508.13712)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Acquiring high-quality annotated data for medical image segmentation is tedious and costly. Semi-supervised segmentation techniques alleviate this burden by leveraging unlabeled data to generate pseudo labels. Recently, advanced state space models, represented by Mamba, have shown efficient handling of long-range dependencies. This drives us to explore their potential in semi-supervised medical image segmentation. In this paper, we propose a novel Diversity-enhanced Collaborative Mamba framework (namely DCMamba) for semi-supervised medical image segmentation, which explores and utilizes the diversity from data, network, and feature perspectives. Firstly, from the data perspective, we develop patch-level weak-strong mixing augmentation with Mamba's scanning modeling characteristics. Moreover, from the network perspective, we introduce a diverse-scan collaboration module, which could benefit from the prediction discrepancies arising from different scanning directions. Furthermore, from the feature perspective, we adopt an uncertainty-weighted contrastive learning mechanism to enhance the diversity of feature representation. Experiments demonstrate that our DCMamba significantly outperforms other semi-supervised medical image segmentation methods, e.g., yielding the latest SSM-based method by 6.69% on the Synapse dataset with 20% labeled data.</li>
</ul>

<h3>Title: Trans-XFed: An Explainable Federated Learning for Supply Chain Credit Assessment</h3>
<ul>
<li><strong>Authors: </strong>Jie Shi, Arno P. J. M. Siebes, Siamak Mehrkanoon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13715">https://arxiv.org/abs/2508.13715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13715">https://arxiv.org/pdf/2508.13715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13715]] Trans-XFed: An Explainable Federated Learning for Supply Chain Credit Assessment(https://arxiv.org/abs/2508.13715)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>This paper proposes a Trans-XFed architecture that combines federated learning with explainable AI techniques for supply chain credit assessment. The proposed model aims to address several key challenges, including privacy, information silos, class imbalance, non-identically and independently distributed (Non-IID) data, and model interpretability in supply chain credit assessment. We introduce a performance-based client selection strategy (PBCS) to tackle class imbalance and Non-IID problems. This strategy achieves faster convergence by selecting clients with higher local F1 scores. The FedProx architecture, enhanced with homomorphic encryption, is used as the core model, and further incorporates a transformer encoder. The transformer encoder block provides insights into the learned features. Additionally, we employ the integrated gradient explainable AI technique to offer insights into decision-making. We demonstrate the effectiveness of Trans-XFed through experimental evaluations on real-world supply chain datasets. The obtained results show its ability to deliver accurate credit assessments compared to several baselines, while maintaining transparency and privacy.</li>
</ul>

<h3>Title: Generics and Default Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>James Ravi Kirkpatrick, Rachel Katharine Sterken</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13718">https://arxiv.org/abs/2508.13718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13718">https://arxiv.org/pdf/2508.13718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13718]] Generics and Default Reasoning in Large Language Models(https://arxiv.org/abs/2508.13718)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper evaluates the capabilities of 28 large language models (LLMs) to reason with 20 defeasible reasoning patterns involving generic generalizations (e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic. Generics are of special interest to linguists, philosophers, logicians, and cognitive scientists because of their complex exception-permitting behaviour and their centrality to default reasoning, cognition, and concept acquisition. We find that while several frontier models handle many default reasoning problems well, performance varies widely across models and prompting styles. Few-shot prompting modestly improves performance for some models, but chain-of-thought (CoT) prompting often leads to serious performance degradation (mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy in zero-shot condition, temperature 0). Most models either struggle to distinguish between defeasible and deductive inference or misinterpret generics as universal statements. These findings underscore both the promise and limits of current LLMs for default reasoning.</li>
</ul>

<h3>Title: Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Hanna Herasimchyk, Alhassan Abdelhalim, Sören Laue, Michaela Regneri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13729">https://arxiv.org/abs/2508.13729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13729">https://arxiv.org/pdf/2508.13729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13729]] Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings(https://arxiv.org/abs/2508.13729)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Understanding what knowledge is implicitly encoded in deep learning models is essential for improving the interpretability of AI systems. This paper examines common methods to explain the knowledge encoded in word embeddings, which are core elements of large language models (LLMs). These methods typically involve mapping embeddings onto collections of human-interpretable semantic features, known as feature norms. Prior work assumes that accurately predicting these semantic features from the word embeddings implies that the embeddings contain the corresponding knowledge. We challenge this assumption by demonstrating that prediction accuracy alone does not reliably indicate genuine feature-based interpretability. We show that these methods can successfully predict even random information, concluding that the results are predominantly determined by an algorithmic upper bound rather than meaningful semantic representation in the word embeddings. Consequently, comparisons between datasets based solely on prediction performance do not reliably indicate which dataset is better captured by the word embeddings. Our analysis illustrates that such mappings primarily reflect geometric similarity within vector spaces rather than indicating the genuine emergence of semantic properties.</li>
</ul>

<h3>Title: On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Daniel M. Jimenez-Gutierrez, Yelizaveta Falkouskaya, Jose L. Hernandez-Ramos, Aris Anagnostopoulos, Ioannis Chatzigiannakis, Andrea Vitaletti</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13730">https://arxiv.org/abs/2508.13730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13730">https://arxiv.org/pdf/2508.13730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13730]] On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions(https://arxiv.org/abs/2508.13730)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is an emerging distributed machine learning paradigm enabling multiple clients to train a global model collaboratively without sharing their raw data. While FL enhances data privacy by design, it remains vulnerable to various security and privacy threats. This survey provides a comprehensive overview of more than 200 papers regarding the state-of-the-art attacks and defense mechanisms developed to address these challenges, categorizing them into security-enhancing and privacy-preserving techniques. Security-enhancing methods aim to improve FL robustness against malicious behaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same time, privacy-preserving techniques focus on protecting sensitive data through cryptographic approaches, differential privacy, and secure aggregation. We critically analyze the strengths and limitations of existing methods, highlight the trade-offs between privacy, security, and model performance, and discuss the implications of non-IID data distributions on the effectiveness of these defenses. Furthermore, we identify open research challenges and future directions, including the need for scalable, adaptive, and energy-efficient solutions operating in dynamic and heterogeneous FL environments. Our survey aims to guide researchers and practitioners in developing robust and privacy-preserving FL systems, fostering advancements safeguarding collaborative learning frameworks' integrity and confidentiality.</li>
</ul>

<h3>Title: Enhancing Targeted Adversarial Attacks on Large Vision-Language Models through Intermediate Projector Guidance</h3>
<ul>
<li><strong>Authors: </strong>Yiming Cao, Yanjie Li, Kaisheng Liang, Yuni Lai, Bin Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13739">https://arxiv.org/abs/2508.13739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13739">https://arxiv.org/pdf/2508.13739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13739]] Enhancing Targeted Adversarial Attacks on Large Vision-Language Models through Intermediate Projector Guidance(https://arxiv.org/abs/2508.13739)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Targeted adversarial attacks are essential for proactively identifying security flaws in Vision-Language Models before real-world deployment. However, current methods perturb images to maximize global similarity with the target text or reference image at the encoder level, collapsing rich visual semantics into a single global vector. This limits attack granularity, hindering fine-grained manipulations such as modifying a car while preserving its background. Furthermore, these methods largely overlook the projector module, a critical semantic bridge between the visual encoder and the language model in VLMs, thereby failing to disrupt the full vision-language alignment pipeline within VLMs and limiting attack effectiveness. To address these issues, we propose the Intermediate Projector Guided Attack (IPGA), the first method to attack using the intermediate stage of the projector module, specifically the widely adopted Q-Former, which transforms global image embeddings into fine-grained visual features. This enables more precise control over adversarial perturbations by operating on semantically meaningful visual tokens rather than a single global representation. Specifically, IPGA leverages the Q-Former pretrained solely on the first vision-language alignment stage, without LLM fine-tuning, which improves both attack effectiveness and transferability across diverse VLMs. Furthermore, we propose Residual Query Alignment (RQA) to preserve unrelated visual content, thereby yielding more controlled and precise adversarial manipulations. Extensive experiments show that our attack method consistently outperforms existing methods in both standard global image captioning tasks and fine-grained visual question-answering tasks in black-box environment. Additionally, IPGA successfully transfers to multiple commercial VLMs, including Google Gemini and OpenAI GPT.</li>
</ul>

<h3>Title: Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias via Adversarial Dialogues in Scientific QA</h3>
<ul>
<li><strong>Authors: </strong>Kaiwei Zhang, Qi Jia, Zijian Chen, Wei Sun, Xiangyang Zhu, Chunyi Li, Dandan Zhu, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13743">https://arxiv.org/abs/2508.13743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13743">https://arxiv.org/pdf/2508.13743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13743]] Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias via Adversarial Dialogues in Scientific QA(https://arxiv.org/abs/2508.13743)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), while increasingly used in domains requiring factual rigor, often display a troubling behavior: sycophancy, the tendency to align with user beliefs regardless of correctness. This tendency is reinforced by preference-based alignment techniques that optimize for user satisfaction but can undermine truthfulness. While relatively benign in casual dialogue, sycophancy poses serious risks in high-stakes settings such as scientific question answering (QA), where model outputs may shape collaborative reasoning, decision-making, and knowledge formation. Despite its importance, this phenomenon remains underexamined in factual QA contexts. We address this gap by introducing a unified evaluation framework to quantify the impact of sycophantic context on model behavior in scientific QA, measuring how much user-imposed social pressure distorts model outputs. The framework incorporates adversarial prompting setups and targeted metrics, such as misleading resistance and sycophancy resistance, that capture a model's ability to maintain factual consistency under misleading cues. Systematic evaluations across open-source and proprietary models reveal pervasive sycophantic tendencies, driven more by alignment strategy than by model size. To mitigate this issue, we propose Pressure-Tune, a lightweight post-training method that fine-tunes models on synthetic adversarial dialogues paired with chain-of-thought rationales. These rationales reject user misinformation while reinforcing factual commitments. Experiments on challenging scientific QA benchmarks show that Pressure-Tune significantly enhances sycophancy resistance without compromising accuracy or responsiveness to valid feedback, offering a practical pathway toward more truthful and principled model behavior.</li>
</ul>

<h3>Title: NodeShield: Runtime Enforcement of Security-Enhanced SBOMs for Node.js</h3>
<ul>
<li><strong>Authors: </strong>Eric Cornelissen, Musard Balliu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13750">https://arxiv.org/abs/2508.13750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13750">https://arxiv.org/pdf/2508.13750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13750]] NodeShield: Runtime Enforcement of Security-Enhanced SBOMs for Node.js(https://arxiv.org/abs/2508.13750)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, steal</a></li>
<li><strong>Abstract: </strong>The software supply chain is an increasingly common attack vector for malicious actors. The this http URL ecosystem has been subject to a wide array of attacks, likely due to its size and prevalence. To counter such attacks, the research community and practitioners have proposed a range of static and dynamic mechanisms, including process- and language-level sandboxing, permission systems, and taint tracking. Drawing on valuable insight from these works, this paper studies a runtime protection mechanism for (the supply chain of) this http URL applications with the ambitious goals of compatibility, automation, minimal overhead, and policy conciseness. Specifically, we design, implement and evaluate NodeShield, a protection mechanism for this http URL that enforces an application's dependency hierarchy and controls access to system resources at runtime. We leverage the up-and-coming SBOM standard as the source of truth for the dependency hierarchy of the application, thus preventing components from stealthily abusing undeclared components. We propose to enhance the SBOM with a notion of capabilities that represents a set of related system resources a component may access. Our proposed SBOM extension, the Capability Bill of Materials or CBOM, records the required capabilities of each component, providing valuable insight into the potential privileged behavior. NodeShield enforces the SBOM and CBOM at runtime via code outlining (as opposed to inlining) with no modifications to the original code or this http URL runtime, thus preventing unexpected, potentially malicious behavior. Our evaluation shows that NodeShield can prevent over 98% out of 67 known supply chain attacks while incurring minimal overhead on servers at less than 1ms per request. We achieve this while maintaining broad compatibility with vanilla this http URL and a concise policy language that consists of at most 7 entries per dependency.</li>
</ul>

<h3>Title: Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration</h3>
<ul>
<li><strong>Authors: </strong>Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Dongchun Xie, Yiwei Wang, Xiaodan Liang, Jing Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13755">https://arxiv.org/abs/2508.13755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13755">https://arxiv.org/pdf/2508.13755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13755]] Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration(https://arxiv.org/abs/2508.13755)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO's mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR.</li>
</ul>

<h3>Title: MGT-Prism: Enhancing Domain Generalization for Machine-Generated Text Detection via Spectral Alignment</h3>
<ul>
<li><strong>Authors: </strong>Shengchao Liu, Xiaoming Liu, Chengzhengxu Li, Zhaohan Zhang, Guoxin Ma, Yu Lan, Shuai Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13768">https://arxiv.org/abs/2508.13768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13768">https://arxiv.org/pdf/2508.13768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13768]] MGT-Prism: Enhancing Domain Generalization for Machine-Generated Text Detection via Spectral Alignment(https://arxiv.org/abs/2508.13768)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models have shown growing ability to generate fluent and coherent texts that are highly similar to the writing style of humans. Current detectors for Machine-Generated Text (MGT) perform well when they are trained and tested in the same domain but generalize poorly to unseen domains, due to domain shift between data from different sources. In this work, we propose MGT-Prism, an MGT detection method from the perspective of the frequency domain for better domain generalization. Our key insight stems from analyzing text representations in the frequency domain, where we observe consistent spectral patterns across diverse domains, while significant discrepancies in magnitude emerge between MGT and human-written texts (HWTs). The observation initiates the design of a low frequency domain filtering module for filtering out the document-level features that are sensitive to domain shift, and a dynamic spectrum alignment strategy to extract the task-specific and domain-invariant features for improving the detector's performance in domain generalization. Extensive experiments demonstrate that MGT-Prism outperforms state-of-the-art baselines by an average of 0.90% in accuracy and 0.92% in F1 score on 11 test datasets across three domain-generalization scenarios.</li>
</ul>

<h3>Title: Can Large Language Models (LLMs) Describe Pictures Like Children? A Comparative Corpus Study</h3>
<ul>
<li><strong>Authors: </strong>Hanna Woloszyn, Benjamin Gagl</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13769">https://arxiv.org/abs/2508.13769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13769">https://arxiv.org/pdf/2508.13769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13769]] Can Large Language Models (LLMs) Describe Pictures Like Children? A Comparative Corpus Study(https://arxiv.org/abs/2508.13769)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The role of large language models (LLMs) in education is increasing, yet little attention has been paid to whether LLM-generated text resembles child language. This study evaluates how LLMs replicate child-like language by comparing LLM-generated texts to a collection of German children's descriptions of picture stories. We generated two LLM-based corpora using the same picture stories and two prompt types: zero-shot and few-shot prompts specifying a general age from the children corpus. We conducted a comparative analysis across psycholinguistic text properties, including word frequency, lexical richness, sentence and word length, part-of-speech tags, and semantic similarity with word embeddings. The results show that LLM-generated texts are longer but less lexically rich, rely more on high-frequency words, and under-represent nouns. Semantic vector space analysis revealed low similarity, highlighting differences between the two corpora on the level of corpus semantics. Few-shot prompt increased similarities between children and LLM text to a minor extent, but still failed to replicate lexical and semantic patterns. The findings contribute to our understanding of how LLMs approximate child language through multimodal prompting (text + image) and give insights into their use in psycholinguistic research and education while raising important questions about the appropriateness of LLM-generated language in child-directed educational tools.</li>
</ul>

<h3>Title: PENGUIN: Enhancing Transformer with Periodic-Nested Group Attention for Long-term Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Tian Sun, Yuqi Chen, Weiwei Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13773">https://arxiv.org/abs/2508.13773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13773">https://arxiv.org/pdf/2508.13773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13773]] PENGUIN: Enhancing Transformer with Periodic-Nested Group Attention for Long-term Time Series Forecasting(https://arxiv.org/abs/2508.13773)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Long-term time series forecasting (LTSF) is a fundamental task with wide-ranging applications. Although Transformer-based models have made significant breakthroughs in forecasting, their effectiveness for time series forecasting remains debatable. In this paper, we revisit the significance of self-attention and propose a simple yet effective mechanism, Periodic-Nested Group Attention, namely PENGUIN. Our approach highlights the importance of explicitly modeling periodic patterns and incorporating relative attention bias for effective time series modeling. To this end, we introduce a periodic-nested relative attention bias that captures periodic structures directly. To handle multiple coexisting periodicities (e.g., daily and weekly cycles), we design a grouped attention mechanism, where each group targets a specific periodicity using a multi-query attention mechanism. Extensive experiments across diverse benchmarks demonstrate that PENGUIN consistently outperforms both MLP-based and Transformer-based models.</li>
</ul>

<h3>Title: MR6D: Benchmarking 6D Pose Estimation for Mobile Robots</h3>
<ul>
<li><strong>Authors: </strong>Anas Gouda, Shrutarv Awasthi, Christian Blesing, Lokeshwaran Manohar, Frank Hoffmann, Alice Kirchheim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13775">https://arxiv.org/abs/2508.13775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13775">https://arxiv.org/pdf/2508.13775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13775]] MR6D: Benchmarking 6D Pose Estimation for Mobile Robots(https://arxiv.org/abs/2508.13775)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Existing 6D pose estimation datasets primarily focus on small household objects typically handled by robot arm manipulators, limiting their relevance to mobile robotics. Mobile platforms often operate without manipulators, interact with larger objects, and face challenges such as long-range perception, heavy self-occlusion, and diverse camera perspectives. While recent models generalize well to unseen objects, evaluations remain confined to household-like settings that overlook these factors. We introduce MR6D, a dataset designed for 6D pose estimation for mobile robots in industrial environments. It includes 92 real-world scenes featuring 16 unique objects across static and dynamic interactions. MR6D captures the challenges specific to mobile platforms, including distant viewpoints, varied object configurations, larger object sizes, and complex occlusion/self-occlusion patterns. Initial experiments reveal that current 6D pipelines underperform in these settings, with 2D segmentation being another hurdle. MR6D establishes a foundation for developing and evaluating pose estimation methods tailored to the demands of mobile robotics. The dataset is available at this https URL.</li>
</ul>

<h3>Title: VisionLaw: Inferring Interpretable Intrinsic Dynamics from Visual Observations via Bilevel Optimization</h3>
<ul>
<li><strong>Authors: </strong>Jailing Lin, Shu Jiang, Qingyuan Zeng, Zhenzhong Wang, Min Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13792">https://arxiv.org/abs/2508.13792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13792">https://arxiv.org/pdf/2508.13792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13792]] VisionLaw: Inferring Interpretable Intrinsic Dynamics from Visual Observations via Bilevel Optimization(https://arxiv.org/abs/2508.13792)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The intrinsic dynamics of an object governs its physical behavior in the real world, playing a critical role in enabling physically plausible interactive simulation with 3D assets. Existing methods have attempted to infer the intrinsic dynamics of objects from visual observations, but generally face two major challenges: one line of work relies on manually defined constitutive priors, making it difficult to generalize to complex scenarios; the other models intrinsic dynamics using neural networks, resulting in limited interpretability and poor generalization. To address these challenges, we propose VisionLaw, a bilevel optimization framework that infers interpretable expressions of intrinsic dynamics from visual observations. At the upper level, we introduce an LLMs-driven decoupled constitutive evolution strategy, where LLMs are prompted as a knowledgeable physics expert to generate and revise constitutive laws, with a built-in decoupling mechanism that substantially reduces the search complexity of LLMs. At the lower level, we introduce a vision-guided constitutive evaluation mechanism, which utilizes visual simulation to evaluate the consistency between the generated constitutive law and the underlying intrinsic dynamics, thereby guiding the upper-level evolution. Experiments on both synthetic and real-world datasets demonstrate that VisionLaw can effectively infer interpretable intrinsic dynamics from visual observations. It significantly outperforms existing state-of-the-art methods and exhibits strong generalization for interactive simulation in novel scenarios.</li>
</ul>

<h3>Title: A Fully Transformer Based Multimodal Framework for Explainable Cancer Image Segmentation Using Radiology Reports</h3>
<ul>
<li><strong>Authors: </strong>Enobong Adahada, Isabel Sassoon, Kate Hone, Yongmin Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13796">https://arxiv.org/abs/2508.13796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13796">https://arxiv.org/pdf/2508.13796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13796]] A Fully Transformer Based Multimodal Framework for Explainable Cancer Image Segmentation Using Radiology Reports(https://arxiv.org/abs/2508.13796)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>We introduce Med-CTX, a fully transformer based multimodal framework for explainable breast cancer ultrasound segmentation. We integrate clinical radiology reports to boost both performance and interpretability. Med-CTX achieves exact lesion delineation by using a dual-branch visual encoder that combines ViT and Swin transformers, as well as uncertainty aware fusion. Clinical language structured with BI-RADS semantics is encoded by BioClinicalBERT and combined with visual features utilising cross-modal attention, allowing the model to provide clinically grounded, model generated explanations. Our methodology generates segmentation masks, uncertainty maps, and diagnostic rationales all at once, increasing confidence and transparency in computer assisted diagnosis. On the BUS-BRA dataset, Med-CTX achieves a Dice score of 99% and an IoU of 95%, beating existing baselines U-Net, ViT, and Swin. Clinical text plays a key role in segmentation accuracy and explanation quality, as evidenced by ablation studies that show a -5.4% decline in Dice score and -31% in CIDEr. Med-CTX achieves good multimodal alignment (CLIP score: 85%) and increased confi dence calibration (ECE: 3.2%), setting a new bar for trustworthy, multimodal medical architecture.</li>
</ul>

<h3>Title: Communication-Efficient Federated Learning with Adaptive Number of Participants</h3>
<ul>
<li><strong>Authors: </strong>Sergey Skorik, Vladislav Dorofeev, Gleb Molodtsov, Aram Avetisyan, Dmitry Bylinkin, Daniil Medyakov, Aleksandr Beznosikov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13803">https://arxiv.org/abs/2508.13803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13803">https://arxiv.org/pdf/2508.13803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13803]] Communication-Efficient Federated Learning with Adaptive Number of Participants(https://arxiv.org/abs/2508.13803)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, transformer</a></li>
<li><strong>Abstract: </strong>Rapid scaling of deep learning models has enabled performance gains across domains, yet it introduced several challenges. Federated Learning (FL) has emerged as a promising framework to address these concerns by enabling decentralized training. Nevertheless, communication efficiency remains a key bottleneck in FL, particularly under heterogeneous and dynamic client participation. Existing methods, such as FedAvg and FedProx, or other approaches, including client selection strategies, attempt to mitigate communication costs. However, the problem of choosing the number of clients in a training round remains extremely underexplored. We introduce Intelligent Selection of Participants (ISP), an adaptive mechanism that dynamically determines the optimal number of clients per round to enhance communication efficiency without compromising model accuracy. We validate the effectiveness of ISP across diverse setups, including vision transformers, real-world ECG classification, and training with gradient compression. Our results show consistent communication savings of up to 30\% without losing the final quality. Applying ISP to different real-world ECG classification setups highlighted the selection of the number of clients as a separate task of federated learning.</li>
</ul>

<h3>Title: Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding</h3>
<ul>
<li><strong>Authors: </strong>Maciej Skorski, Alina Landowska</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13804">https://arxiv.org/abs/2508.13804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13804">https://arxiv.org/pdf/2508.13804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13804]] Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding(https://arxiv.org/abs/2508.13804)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>How do large language models understand moral dimensions compared to humans? This first large-scale Bayesian evaluation of market-leading language models provides the answer. In contrast to prior work using deterministic ground truth (majority or inclusion rules), we model annotator disagreements to capture both aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty (model domain sensitivity). We evaluate top language models (Claude Sonnet 4, DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on 100K+ texts spanning social media, news, and forums. Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing that AI models typically rank among the top 25\% of human annotators, achieving much better-than-average balanced accuracy. Importantly, we find that AI produces far fewer false negatives than humans, highlighting their more sensitive moral detection capabilities.</li>
</ul>

<h3>Title: Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Juncheng Xie, Hung-yi Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13805">https://arxiv.org/abs/2508.13805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13805">https://arxiv.org/pdf/2508.13805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13805]] Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs(https://arxiv.org/abs/2508.13805)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Controlling the length of text produced by large language models (LLMs) remains challenging: models frequently overshoot or undershoot explicit length instructions because they cannot reliably keep an internal token count. We present a prompt-based, one-shot strategy that compels an off-the-shelf LLM to generate exactly a desired number of tokens - words (English) or characters (Chinese) - without any fine-tuning or iterative sampling. The prompt appends countdown markers and explicit counting rules so that the model "writes while counting." We evaluate on four settings: open-ended generation (1-1000 tokens), XSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH equal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps from below 30% under naive prompts to above 95% with our countdown prompt, surpassing the popular draft-then-revise baseline, while judged answer quality is preserved. These results show that precise length control can be achieved through prompt engineering alone, offering a lightweight alternative to training- or decoding-based methods.</li>
</ul>

<h3>Title: Timestep-Compressed Attack on Spiking Neural Networks through Timestep-Level Backpropagation</h3>
<ul>
<li><strong>Authors: </strong>Donghwa Kang, Doohyun Kim, Sang-Ki Ko, Jinkyu Lee, Hyeongboo Baek, Brent ByungHoon Kang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13812">https://arxiv.org/abs/2508.13812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13812">https://arxiv.org/pdf/2508.13812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13812]] Timestep-Compressed Attack on Spiking Neural Networks through Timestep-Level Backpropagation(https://arxiv.org/abs/2508.13812)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>State-of-the-art (SOTA) gradient-based adversarial attacks on spiking neural networks (SNNs), which largely rely on extending FGSM and PGD frameworks, face a critical limitation: substantial attack latency from multi-timestep processing, rendering them infeasible for practical real-time applications. This inefficiency stems from their design as direct extensions of ANN paradigms, which fail to exploit key SNN properties. In this paper, we propose the timestep-compressed attack (TCA), a novel framework that significantly reduces attack latency. TCA introduces two components founded on key insights into SNN behavior. First, timestep-level backpropagation (TLBP) is based on our finding that global temporal information in backpropagation to generate perturbations is not critical for an attack's success, enabling per-timestep evaluation for early stopping. Second, adversarial membrane potential reuse (A-MPR) is motivated by the observation that initial timesteps are inefficiently spent accumulating membrane potential, a warm-up phase that can be pre-calculated and reused. Our experiments on VGG-11 and ResNet-17 with the CIFAR-10/100 and CIFAR10-DVS datasets show that TCA significantly reduces the required attack latency by up to 56.6% and 57.1% compared to SOTA methods in white-box and black-box settings, respectively, while maintaining a comparable attack success rate.</li>
</ul>

<h3>Title: Assessing Trustworthiness of AI Training Dataset using Subjective Logic -- A Use Case on Bias</h3>
<ul>
<li><strong>Authors: </strong>Koffi Ismael Ouattara, Ioannis Krontiris, Theo Dimitrakos, Frank Kargl</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13813">https://arxiv.org/abs/2508.13813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13813">https://arxiv.org/pdf/2508.13813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13813]] Assessing Trustworthiness of AI Training Dataset using Subjective Logic -- A Use Case on Bias(https://arxiv.org/abs/2508.13813)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate, fair</a></li>
<li><strong>Abstract: </strong>As AI systems increasingly rely on training data, assessing dataset trustworthiness has become critical, particularly for properties like fairness or bias that emerge at the dataset level. Prior work has used Subjective Logic to assess trustworthiness of individual data, but not to evaluate trustworthiness properties that emerge only at the level of the dataset as a whole. This paper introduces the first formal framework for assessing the trustworthiness of AI training datasets, enabling uncertainty-aware evaluations of global properties such as bias. Built on Subjective Logic, our approach supports trust propositions and quantifies uncertainty in scenarios where evidence is incomplete, distributed, and/or conflicting. We instantiate this framework on the trustworthiness property of bias, and we experimentally evaluate it based on a traffic sign recognition dataset. The results demonstrate that our method captures class imbalance and remains interpretable and robust in both centralized and federated contexts.</li>
</ul>

<h3>Title: Disentangled Deep Smoothed Bootstrap for Fair Imbalanced Regression</h3>
<ul>
<li><strong>Authors: </strong>Samuel Stocksieker, Denys pommeret, Arthur Charpentier</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13829">https://arxiv.org/abs/2508.13829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13829">https://arxiv.org/pdf/2508.13829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13829]] Disentangled Deep Smoothed Bootstrap for Fair Imbalanced Regression(https://arxiv.org/abs/2508.13829)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Imbalanced distribution learning is a common and significant challenge in predictive modeling, often reducing the performance of standard algorithms. Although various approaches address this issue, most are tailored to classification problems, with a limited focus on regression. This paper introduces a novel method to improve learning on tabular data within the Imbalanced Regression (IR) framework, which is a critical problem. We propose using Variational Autoencoders (VAEs) to model and define a latent representation of data distributions. However, VAEs can be inefficient with imbalanced data like other standard approaches. To address this, we develop an innovative data generation method that combines a disentangled VAE with a Smoothed Bootstrap applied in the latent space. We evaluate the efficiency of this method through numerical comparisons with competitors on benchmark datasets for IR.</li>
</ul>

<h3>Title: Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling</h3>
<ul>
<li><strong>Authors: </strong>Insaf Nahri, Romain Pinquié, Philippe Véron, Nicolas Bus, Mathieu Thorel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13833">https://arxiv.org/abs/2508.13833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13833">https://arxiv.org/pdf/2508.13833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13833]] Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling(https://arxiv.org/abs/2508.13833)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>This study explores the integration of Building Information Modeling (BIM) with Natural Language Processing (NLP) to automate the extraction of requirements from unstructured French Building Technical Specification (BTS) documents within the construction industry. Employing Named Entity Recognition (NER) and Relation Extraction (RE) techniques, the study leverages the transformer-based model CamemBERT and applies transfer learning with the French language model Fr\_core\_news\_lg, both pre-trained on a large French corpus in the general domain. To benchmark these models, additional approaches ranging from rule-based to deep learning-based methods are developed. For RE, four different supervised models, including Random Forest, are implemented using a custom feature vector. A hand-crafted annotated dataset is used to compare the effectiveness of NER approaches and RE models. Results indicate that CamemBERT and Fr\_core\_news\_lg exhibited superior performance in NER, achieving F1-scores over 90\%, while Random Forest proved most effective in RE, with an F1 score above 80\%. The outcomes are intended to be represented as a knowledge graph in future work to further enhance automatic verification systems.</li>
</ul>

<h3>Title: FedUP: Efficient Pruning-based Federated Unlearning for Model Poisoning Attacks</h3>
<ul>
<li><strong>Authors: </strong>Nicolò Romandini, Cristian Borcea, Rebecca Montanari, Luca Foschini</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13853">https://arxiv.org/abs/2508.13853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13853">https://arxiv.org/pdf/2508.13853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13853]] FedUP: Efficient Pruning-based Federated Unlearning for Model Poisoning Attacks(https://arxiv.org/abs/2508.13853)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) can be vulnerable to attacks, such as model poisoning, where adversaries send malicious local weights to compromise the global model. Federated Unlearning (FU) is emerging as a solution to address such vulnerabilities by selectively removing the influence of detected malicious contributors on the global model without complete retraining. However, unlike typical FU scenarios where clients are trusted and cooperative, applying FU with malicious and possibly colluding clients is challenging because their collaboration in unlearning their data cannot be assumed. This work presents FedUP, a lightweight FU algorithm designed to efficiently mitigate malicious clients' influence by pruning specific connections within the attacked model. Our approach achieves efficiency by relying only on clients' weights from the last training round before unlearning to identify which connections to inhibit. Isolating malicious influence is non-trivial due to overlapping updates from benign and malicious clients. FedUP addresses this by carefully selecting and zeroing the highest magnitude weights that diverge the most between the latest updates from benign and malicious clients while preserving benign information. FedUP is evaluated under a strong adversarial threat model, where up to 50%-1 of the clients could be malicious and have full knowledge of the aggregation process. We demonstrate the effectiveness, robustness, and efficiency of our solution through experiments across IID and Non-IID data, under label-flipping and backdoor attacks, and by comparing it with state-of-the-art (SOTA) FU solutions. In all scenarios, FedUP reduces malicious influence, lowering accuracy on malicious data to match that of a model retrained from scratch while preserving performance on benign data. FedUP achieves effective unlearning while consistently being faster and saving storage compared to the SOTA.</li>
</ul>

<h3>Title: SAGA: Learning Signal-Aligned Distributions for Improved Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Paul Grimal, Michaël Soumm, Hervé Le Borgne, Olivier Ferret, Akihiro Sugimoto</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13866">https://arxiv.org/abs/2508.13866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13866">https://arxiv.org/pdf/2508.13866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13866]] SAGA: Learning Signal-Aligned Distributions for Improved Text-to-Image Generation(https://arxiv.org/abs/2508.13866)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>State-of-the-art text-to-image models produce visually impressive results but often struggle with precise alignment to text prompts, leading to missing critical elements or unintended blending of distinct concepts. We propose a novel approach that learns a high-success-rate distribution conditioned on a target prompt, ensuring that generated images faithfully reflect the corresponding prompts. Our method explicitly models the signal component during the denoising process, offering fine-grained control that mitigates over-optimization and out-of-distribution artifacts. Moreover, our framework is training-free and seamlessly integrates with both existing diffusion and flow matching architectures. It also supports additional conditioning modalities -- such as bounding boxes -- for enhanced spatial alignment. Extensive experiments demonstrate that our approach outperforms current state-of-the-art methods. The code is available at this https URL.</li>
</ul>

<h3>Title: A Comprehensive Re-Evaluation of Biometric Modality Properties in the Modern Era</h3>
<ul>
<li><strong>Authors: </strong>Rouqaiah Al-Refai, Pankaja Priya Ramasamy, Ragini Ramesh, Patricia Arias-Cabarcos, Philipp Terhörst</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13874">https://arxiv.org/abs/2508.13874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13874">https://arxiv.org/pdf/2508.13874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13874]] A Comprehensive Re-Evaluation of Biometric Modality Properties in the Modern Era(https://arxiv.org/abs/2508.13874)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, biometric</a></li>
<li><strong>Abstract: </strong>The rapid advancement of authentication systems and their increasing reliance on biometrics for faster and more accurate user verification experience, highlight the critical need for a reliable framework to evaluate the suitability of biometric modalities for specific applications. Currently, the most widely known evaluation framework is a comparative table from 1998, which no longer adequately captures recent technological developments or emerging vulnerabilities in biometric systems. To address these challenges, this work revisits the evaluation of biometric modalities through an expert survey involving 24 biometric specialists. The findings indicate substantial shifts in property ratings across modalities. For example, face recognition, shows improved ratings due to technological progress, while fingerprint, shows decreased reliability because of emerging vulnerabilities and attacks. Further analysis of expert agreement levels across rated properties highlighted the consistency of the provided evaluations and ensured the reliability of the ratings. Finally, expert assessments are compared with dataset-level uncertainty across 55 biometric datasets, revealing strong alignment in most modalities and underscoring the importance of integrating empirical evidence with expert insight. Moreover, the identified expert disagreements reveal key open challenges and help guide future research toward resolving them.</li>
</ul>

<h3>Title: RICO: Two Realistic Benchmarks and an In-Depth Analysis for Incremental Learning in Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Matthias Neuwirth-Trapp, Maarten Bieshaar, Danda Pani Paudel, Luc Van Gool</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13878">https://arxiv.org/abs/2508.13878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13878">https://arxiv.org/pdf/2508.13878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13878]] RICO: Two Realistic Benchmarks and an In-Depth Analysis for Incremental Learning in Object Detection(https://arxiv.org/abs/2508.13878)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Incremental Learning (IL) trains models sequentially on new data without full retraining, offering privacy, efficiency, and scalability. IL must balance adaptability to new data with retention of old knowledge. However, evaluations often rely on synthetic, simplified benchmarks, obscuring real-world IL performance. To address this, we introduce two Realistic Incremental Object Detection Benchmarks (RICO): Domain RICO (D-RICO) features domain shifts with a fixed class set, and Expanding-Classes RICO (EC-RICO) integrates new domains and classes per IL step. Built from 14 diverse datasets covering real and synthetic domains, varying conditions (e.g., weather, time of day), camera sensors, perspectives, and labeling policies, both benchmarks capture challenges absent in existing evaluations. Our experiments show that all IL methods underperform in adaptability and retention, while replaying a small amount of previous data already outperforms all methods. However, individual training on the data remains superior. We heuristically attribute this gap to weak teachers in distillation, single models' inability to manage diverse tasks, and insufficient plasticity. Our code will be made publicly available.</li>
</ul>

<h3>Title: In-hoc Concept Representations to Regularise Deep Learning in Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Valentina Corbetta, Floris Six Dijkstra, Regina Beets-Tan, Hoel Kervadec, Kristoffer Wickstrøm, Wilson Silva</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13880">https://arxiv.org/abs/2508.13880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13880">https://arxiv.org/pdf/2508.13880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13880]] In-hoc Concept Representations to Regularise Deep Learning in Medical Imaging(https://arxiv.org/abs/2508.13880)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning models in medical imaging often achieve strong in-distribution performance but struggle to generalise under distribution shifts, frequently relying on spurious correlations instead of clinically meaningful features. We introduce LCRReg, a novel regularisation approach that leverages Latent Concept Representations (LCRs) (e.g., Concept Activation Vectors (CAVs)) to guide models toward semantically grounded representations. LCRReg requires no concept labels in the main training set and instead uses a small auxiliary dataset to synthesise high-quality, disentangled concept examples. We extract LCRs for predefined relevant features, and incorporate a regularisation term that guides a Convolutional Neural Network (CNN) to activate within latent subspaces associated with those concepts. We evaluate LCRReg across synthetic and real-world medical tasks. On a controlled toy dataset, it significantly improves robustness to injected spurious correlations and remains effective even in multi-concept and multiclass settings. On the diabetic retinopathy binary classification task, LCRReg enhances performance under both synthetic spurious perturbations and out-of-distribution (OOD) generalisation. Compared to baselines, including multitask learning, linear probing, and post-hoc concept-based models, LCRReg offers a lightweight, architecture-agnostic strategy for improving model robustness without requiring dense concept supervision. Code is available at the following link: this https URL\_regularization</li>
</ul>

<h3>Title: SCRNet: Spatial-Channel Regulation Network for Medical Ultrasound Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Weixin Xu, Ziliang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13899">https://arxiv.org/abs/2508.13899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13899">https://arxiv.org/pdf/2508.13899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13899]] SCRNet: Spatial-Channel Regulation Network for Medical Ultrasound Image Segmentation(https://arxiv.org/abs/2508.13899)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Medical ultrasound image segmentation presents a formidable challenge in the realm of computer vision. Traditional approaches rely on Convolutional Neural Networks (CNNs) and Transformer-based methods to address the intricacies of medical image segmentation. Nevertheless, inherent limitations persist, as CNN-based methods tend to disregard long-range dependencies, while Transformer-based methods may overlook local contextual information. To address these deficiencies, we propose a novel Feature Aggregation Module (FAM) designed to process two input features from the preceding layer. These features are seamlessly directed into two branches of the Convolution and Cross-Attention Parallel Module (CCAPM) to endow them with different roles in each of the two branches to help establish a strong connection between the two input features. This strategy enables our module to focus concurrently on both long-range dependencies and local contextual information by judiciously merging convolution operations with cross-attention mechanisms. Moreover, by integrating FAM within our proposed Spatial-Channel Regulation Module (SCRM), the ability to discern salient regions and informative features warranting increased attention is enhanced. Furthermore, by incorporating the SCRM into the encoder block of the UNet architecture, we introduce a novel framework dubbed Spatial-Channel Regulation Network (SCRNet). The results of our extensive experiments demonstrate the superiority of SCRNet, which consistently achieves state-of-the-art (SOTA) performance compared to existing methods.</li>
</ul>

<h3>Title: Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation</h3>
<ul>
<li><strong>Authors: </strong>Thanh Nguyen, Chang D. Yoo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13904">https://arxiv.org/abs/2508.13904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13904">https://arxiv.org/pdf/2508.13904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13904]] Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation(https://arxiv.org/abs/2508.13904)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The generative power of diffusion models (DMs) has recently enabled high-performing decision-making algorithms in offline reinforcement learning (RL), achieving state-of-the-art results across standard benchmarks. Among them, Diffusion Q-Learning (DQL) stands out as a leading method for its consistently strong performance. Nevertheless, DQL remains limited in practice due to its reliance on multi-step denoising for action generation during both training and inference. Although one-step denoising is desirable, simply applying it to DQL leads to a drastic performance drop. In this work, we revisit DQL and identify its core limitations. We then propose One-Step Flow Q-Learning (OFQL), a novel framework that enables efficient one-step action generation during both training and inference, without requiring auxiliary models, distillation, or multi-phase training. Specifically, OFQL reformulates DQL within the sample-efficient Flow Matching (FM) framework. While conventional FM induces curved generative trajectories that impede one-step generation, OFQL instead learns an average velocity field that facilitates direct, accurate action generation. Collectively, OFQL eliminates the need for multi-step sampling and recursive gradient updates in DQL, resulting in faster and more robust training and inference. Extensive experiments on the D4RL benchmark demonstrate that OFQL outperforms DQL and other diffusion-based baselines, while substantially reducing both training and inference time compared to DQL.</li>
</ul>

<h3>Title: Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management</h3>
<ul>
<li><strong>Authors: </strong>Tianheng Ling, Vipin Singh, Chao Qian, Felix Biessmann, Gregor Schiele</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13905">https://arxiv.org/abs/2508.13905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13905">https://arxiv.org/pdf/2508.13905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13905]] Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management(https://arxiv.org/abs/2508.13905)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Extreme weather events, intensified by climate change, increasingly challenge aging combined sewer systems, raising the risk of untreated wastewater overflow. Accurate forecasting of sewer overflow basin filling levels can provide actionable insights for early intervention, helping mitigating uncontrolled discharge. In recent years, AI-based forecasting methods have offered scalable alternatives to traditional physics-based models, but their reliance on cloud computing limits their reliability during communication outages. To address this, we propose an end-to-end forecasting framework that enables energy-efficient inference directly on edge devices. Our solution integrates lightweight Transformer and Long Short-Term Memory (LSTM) models, compressed via integer-only quantization for efficient on-device execution. Moreover, an automated hardware-aware deployment pipeline is used to search for optimal model configurations by jointly minimizing prediction error and energy consumption on an AMD Spartan-7 XC7S15 FPGA. Evaluated on real-world sewer data, the selected 8-bit Transformer model, trained on 24 hours of historical measurements, achieves high accuracy (MSE 0.0376) at an energy cost of 0.370 mJ per inference. In contrast, the optimal 8-bit LSTM model requires significantly less energy (0.009 mJ, over 40x lower) but yields 14.89% worse accuracy (MSE 0.0432) and much longer training time. This trade-off highlights the need to align model selection with deployment priorities, favoring LSTM for ultra-low energy consumption or Transformer for higher predictive accuracy. In general, our work enables local, energy-efficient forecasting, contributing to more resilient combined sewer systems. All code can be found in the GitHub Repository (this https URL).</li>
</ul>

<h3>Title: DIME-Net: A Dual-Illumination Adaptive Enhancement Network Based on Retinex and Mixture-of-Experts</h3>
<ul>
<li><strong>Authors: </strong>Ziang Wang, Xiaoqin Wang, Dingyi Wang, Qiang Li, Shushan Qiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13921">https://arxiv.org/abs/2508.13921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13921">https://arxiv.org/pdf/2508.13921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13921]] DIME-Net: A Dual-Illumination Adaptive Enhancement Network Based on Retinex and Mixture-of-Experts(https://arxiv.org/abs/2508.13921)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Image degradation caused by complex lighting conditions such as low-light and backlit scenarios is commonly encountered in real-world environments, significantly affecting image quality and downstream vision tasks. Most existing methods focus on a single type of illumination degradation and lack the ability to handle diverse lighting conditions in a unified manner. To address this issue, we propose a dual-illumination enhancement framework called DIME-Net. The core of our method is a Mixture-of-Experts illumination estimator module, where a sparse gating mechanism adaptively selects suitable S-curve expert networks based on the illumination characteristics of the input image. By integrating Retinex theory, this module effectively performs enhancement tailored to both low-light and backlit images. To further correct illumination-induced artifacts and color distortions, we design a damage restoration module equipped with Illumination-Aware Cross Attention and Sequential-State Global Attention mechanisms. In addition, we construct a hybrid illumination dataset, MixBL, by integrating existing datasets, allowing our model to achieve robust illumination adaptability through a single training process. Experimental results show that DIME-Net achieves competitive performance on both synthetic and real-world low-light and backlit datasets without any retraining. These results demonstrate its generalization ability and potential for practical multimedia applications under diverse and complex illumination conditions.</li>
</ul>

<h3>Title: Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control</h3>
<ul>
<li><strong>Authors: </strong>SM Mazharul Islam, Manfred Huber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13922">https://arxiv.org/abs/2508.13922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13922">https://arxiv.org/pdf/2508.13922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13922]] Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control(https://arxiv.org/abs/2508.13922)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A policy in deep reinforcement learning (RL), either deterministic or stochastic, is commonly parameterized as a Gaussian distribution alone, limiting the learned behavior to be unimodal. However, the nature of many practical decision-making problems favors a multimodal policy that facilitates robust exploration of the environment and thus to address learning challenges arising from sparse rewards, complex dynamics, or the need for strategic adaptation to varying contexts. This issue is exacerbated in continuous control domains where exploration usually takes place in the vicinity of the predicted optimal action, either through an additive Gaussian noise or the sampling process of a stochastic policy. In this paper, we introduce Categorical Policies to model multimodal behavior modes with an intermediate categorical distribution, and then generate output action that is conditioned on the sampled mode. We explore two sampling schemes that ensure differentiable discrete latent structure while maintaining efficient gradient-based optimization. By utilizing a latent categorical distribution to select the behavior mode, our approach naturally expresses multimodality while remaining fully differentiable via the sampling tricks. We evaluate our multimodal policy on a set of DeepMind Control Suite environments, demonstrating that through better exploration, our learned policies converge faster and outperform standard Gaussian policies. Our results indicate that the Categorical distribution serves as a powerful tool for structured exploration and multimodal behavior representation in continuous control.</li>
</ul>

<h3>Title: MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Ruan, Dan Jiang, Xian Gao, Ting Liu, Yuzhuo Fu, Yangyang Kang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13938">https://arxiv.org/abs/2508.13938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13938">https://arxiv.org/pdf/2508.13938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13938]] MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models(https://arxiv.org/abs/2508.13938)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, multimodal large language models (MLLMs) have achieved significant advancements across various domains, and corresponding evaluation benchmarks have been continuously refined and improved. In this process, benchmarks in the scientific domain have played an important role in assessing the reasoning capabilities of MLLMs. However, existing benchmarks still face three key challenges: 1) Insufficient evaluation of models' reasoning abilities in multilingual scenarios; 2) Inadequate assessment of MLLMs' comprehensive modality coverage; 3) Lack of fine-grained annotation of scientific knowledge points. To address these gaps, we propose MME-SCI, a comprehensive and challenging benchmark. We carefully collected 1,019 high-quality question-answer pairs, which involve 3 distinct evaluation modes. These pairs cover four subjects, namely mathematics, physics, chemistry, and biology, and support five languages: Chinese, English, French, Spanish, and Japanese. We conducted extensive experiments on 16 open-source models and 4 closed-source models, and the results demonstrate that MME-SCI is widely challenging for existing MLLMs. For instance, under the Image-only evaluation mode, o4-mini achieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics, physics, chemistry, and biology, respectively, indicating a significantly higher difficulty level compared to existing benchmarks. More importantly, using MME-SCI's multilingual and fine-grained knowledge attributes, we analyzed existing models' performance in depth and identified their weaknesses in specific domains. The Data and Evaluation Code are available at this https URL.</li>
</ul>

<h3>Title: ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features</h3>
<ul>
<li><strong>Authors: </strong>A.J.W. de Vink, Natalia Amat-Lefort, Lifeng Han</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13953">https://arxiv.org/abs/2508.13953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13953">https://arxiv.org/pdf/2508.13953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13953]] ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features(https://arxiv.org/abs/2508.13953)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>In the hospitality industry, understanding the factors that drive customer review ratings is critical for improving guest satisfaction and business performance. This work proposes ReviewGraph for Review Rating Prediction (RRP), a novel framework that transforms textual customer reviews into knowledge graphs by extracting (subject, predicate, object) triples and associating sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the framework predicts review rating scores through machine learning classifiers. We compare ReviewGraph performance with traditional NLP baselines (such as Bag of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating them in the HotelRec dataset. In comparison to the state of the art literature, our proposed model performs similar to their best performing model but with lower computational cost (without ensemble). While ReviewGraph achieves comparable predictive performance to LLMs and outperforms baselines on agreement-based metrics such as Cohen's Kappa, it offers additional advantages in interpretability, visual exploration, and potential integration into Retrieval-Augmented Generation (RAG) systems. This work highlights the potential of graph-based representations for enhancing review analytics and lays the groundwork for future research integrating advanced graph neural networks and fine-tuned LLM-based extraction methods. We will share ReviewGraph output and platform open-sourced on our GitHub page this https URL</li>
</ul>

<h3>Title: ViT-FIQA: Assessing Face Image Quality using Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Andrea Atzori, Fadi Boutros, Naser Damer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13957">https://arxiv.org/abs/2508.13957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13957">https://arxiv.org/pdf/2508.13957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13957]] ViT-FIQA: Assessing Face Image Quality using Vision Transformers(https://arxiv.org/abs/2508.13957)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Face Image Quality Assessment (FIQA) aims to predict the utility of a face image for face recognition (FR) systems. State-of-the-art FIQA methods mainly rely on convolutional neural networks (CNNs), leaving the potential of Vision Transformer (ViT) architectures underexplored. This work proposes ViT-FIQA, a novel approach that extends standard ViT backbones, originally optimized for FR, through a learnable quality token designed to predict a scalar utility score for any given face image. The learnable quality token is concatenated with the standard image patch tokens, and the whole sequence is processed via global self-attention by the ViT encoders to aggregate contextual information across all patches. At the output of the backbone, ViT-FIQA branches into two heads: (1) the patch tokens are passed through a fully connected layer to learn discriminative face representations via a margin-penalty softmax loss, and (2) the quality token is fed into a regression head to learn to predict the face sample's utility. Extensive experiments on challenging benchmarks and several FR models, including both CNN- and ViT-based architectures, demonstrate that ViT-FIQA consistently achieves top-tier performance. These results underscore the effectiveness of transformer-based architectures in modeling face image utility and highlight the potential of ViTs as a scalable foundation for future FIQA research this https URL.</li>
</ul>

<h3>Title: Red Teaming Methodology for Design Obfuscation</h3>
<ul>
<li><strong>Authors: </strong>Yuntao Liu, Abir Akib, Zelin Lu, Qian Xu, Ankur Srivastava, Gang Qu, David Kehlet, Nij Dorairaj</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13965">https://arxiv.org/abs/2508.13965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13965">https://arxiv.org/pdf/2508.13965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13965]] Red Teaming Methodology for Design Obfuscation(https://arxiv.org/abs/2508.13965)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>The main goal of design obfuscation schemes is to protect sensitive design details from untrusted parties in the VLSI supply chain, including but not limited to off-shore foundries and untrusted end users. In this work, we provide a systematic red teaming approach to evaluate the security of design obfuscation approaches. Specifically, we propose security metrics and evaluation methodology for the scenarios where the adversary does not have access to a working chip. A case study on the RIPPER tool developed by the University of Florida indicates that more information is leaked about the structure of the original design than commonly considered.</li>
</ul>

<h3>Title: RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Niu, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13968">https://arxiv.org/abs/2508.13968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13968">https://arxiv.org/pdf/2508.13968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13968]] RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation(https://arxiv.org/abs/2508.13968)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0°, 90°, 180°, and 270°. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench -- a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information -- including captions, depth maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0°) images, while certain models are able to identify upside-down (180°) images. None can reliably distinguish between 90° and 270°. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models' ability to distinguish 90° and 270° rotations, despite substantially improving the identification of 180° images. Together, these results reveal a significant gap between MLLMs' spatial reasoning capabilities and human perception in identifying rotation.</li>
</ul>

<h3>Title: ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Xianda Guo, Ruijun Zhang, Yiqun Duan, Ruilin Wang, Keyuan Zhou, Wenzhao Zheng, Wenke Huang, Gangwei Xu, Mike Horton, Yuan Si, Hao Zhao, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13977">https://arxiv.org/abs/2508.13977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13977">https://arxiv.org/pdf/2508.13977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13977]] ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving(https://arxiv.org/abs/2508.13977)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Depth estimation is a fundamental task for 3D scene understanding in autonomous driving, robotics, and augmented reality. Existing depth datasets, such as KITTI, nuScenes, and DDAD, have advanced the field but suffer from limitations in diversity and scalability. As benchmark performance on these datasets approaches saturation, there is an increasing need for a new generation of large-scale, diverse, and cost-efficient datasets to support the era of foundation models and multi-modal learning. To address these challenges, we introduce a large-scale, diverse, frame-wise continuous dataset for depth estimation in dynamic outdoor driving environments, comprising 20K video frames to evaluate existing methods. Our lightweight acquisition pipeline ensures broad scene coverage at low cost, while sparse yet statistically sufficient ground truth enables robust training. Compared to existing datasets, ours presents greater diversity in driving scenarios and lower depth density, creating new challenges for generalization. Benchmark experiments with standard monocular depth estimation models validate the dataset's utility and highlight substantial performance gaps in challenging conditions, establishing a new platform for advancing depth estimation research.</li>
</ul>

<h3>Title: Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Shaohua Duan, Xinze Li, Zhenghao Liu, Xiaoyuan Yi, Yukun Yan, Shuo Wang, Yu Gu, Ge Yu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13993">https://arxiv.org/abs/2508.13993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13993">https://arxiv.org/pdf/2508.13993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13993]] Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization(https://arxiv.org/abs/2508.13993)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long-context modeling is critical for a wide range of real-world tasks, including long-context question answering, summarization, and complex reasoning tasks. Recent studies have explored fine-tuning Large Language Models (LLMs) with synthetic data to enhance their long-context capabilities. However, the effectiveness of such approaches is often limited by the low diversity and factual inconsistencies in the generated data. To address these challenges, we propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB) rollout strategy to identify the most informative chunks from the given long context for sampling high-quality and diverse responses and constructing preference data pairs for Direct Preference Optimization (DPO) training. Specifically, we treat context chunks as arms of MAB, select chunks based on their expected reward scores to input into LLMs to generate responses, and iteratively update these scores based on reward feedback. This exploration and exploitation process enables the model to focus on the most relevant context segments, thereby generating and collecting high-quality and diverse responses. Finally, we collect these generated responses from the rollout process and apply the DPO method to further optimize the LLM. Experimental results show that LongMab-PO significantly improves the diversity and quality of preference data pairs, achieving state-of-the-art performance on long-context reasoning benchmarks. All code and data will be released on this https URL.</li>
</ul>

<h3>Title: ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Izadi, Mehran Safayani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14005">https://arxiv.org/abs/2508.14005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14005">https://arxiv.org/pdf/2508.14005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14005]] ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery(https://arxiv.org/abs/2508.14005)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition marked by disruptions in brain connectivity. Functional MRI (fMRI) offers a non-invasive window into large-scale neural dynamics by measuring blood-oxygen-level-dependent (BOLD) signals across the brain. These signals can be modeled as interactions among Regions of Interest (ROIs), which are grouped into functional communities based on their underlying roles in brain function. Emerging evidence suggests that connectivity patterns within and between these communities are particularly sensitive to ASD-related alterations. Effectively capturing these patterns and identifying interactions that deviate from typical development is essential for improving ASD diagnosis and enabling biomarker discovery. In this work, we introduce ASDFormer, a Transformer-based architecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to capture neural signatures associated with ASD. By integrating multiple specialized expert branches with attention mechanisms, ASDFormer adaptively emphasizes different brain regions and connectivity patterns relevant to autism. This enables both improved classification performance and more interpretable identification of disorder-related biomarkers. Applied to the ABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and reveals robust insights into functional connectivity disruptions linked to ASD, highlighting its potential as a tool for biomarker discovery.</li>
</ul>

<h3>Title: ResPlan: A Large-Scale Vector-Graph Dataset of 17,000 Residential Floor Plans</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Abouagour, Eleftherios Garyfallidis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14006">https://arxiv.org/abs/2508.14006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14006">https://arxiv.org/pdf/2508.14006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14006]] ResPlan: A Large-Scale Vector-Graph Dataset of 17,000 Residential Floor Plans(https://arxiv.org/abs/2508.14006)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>We introduce ResPlan, a large-scale dataset of 17,000 detailed, structurally rich, and realistic residential floor plans, created to advance spatial AI research. Each plan includes precise annotations of architectural elements (walls, doors, windows, balconies) and functional spaces (such as kitchens, bedrooms, and bathrooms). ResPlan addresses key limitations of existing datasets such as RPLAN (Wu et al., 2019) and MSD (van Engelenburg et al., 2024) by offering enhanced visual fidelity and greater structural diversity, reflecting realistic and non-idealized residential layouts. Designed as a versatile, general-purpose resource, ResPlan supports a wide range of applications including robotics, reinforcement learning, generative AI, virtual and augmented reality, simulations, and game development. Plans are provided in both geometric and graph-based formats, enabling direct integration into simulation engines and fast 3D conversion. A key contribution is an open-source pipeline for geometry cleaning, alignment, and annotation refinement. Additionally, ResPlan includes structured representations of room connectivity, supporting graph-based spatial reasoning tasks. Finally, we present comparative analyses with existing benchmarks and outline several open benchmark tasks enabled by ResPlan. Ultimately, ResPlan offers a significant advance in scale, realism, and usability, providing a robust foundation for developing and benchmarking next-generation spatial intelligence systems.</li>
</ul>

<h3>Title: Backdooring Self-Supervised Contrastive Learning by Noisy Alignment</h3>
<ul>
<li><strong>Authors: </strong>Tuo Chen, Jie Gui, Minjing Dong, Ju Jia, Lanting Fang, Jian Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14015">https://arxiv.org/abs/2508.14015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14015">https://arxiv.org/pdf/2508.14015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14015]] Backdooring Self-Supervised Contrastive Learning by Noisy Alignment(https://arxiv.org/abs/2508.14015)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Self-supervised contrastive learning (CL) effectively learns transferable representations from unlabeled data containing images or image-text pairs but suffers vulnerability to data poisoning backdoor attacks (DPCLs). An adversary can inject poisoned images into pretraining datasets, causing compromised CL encoders to exhibit targeted misbehavior in downstream tasks. Existing DPCLs, however, achieve limited efficacy due to their dependence on fragile implicit co-occurrence between backdoor and target object and inadequate suppression of discriminative features in backdoored images. We propose Noisy Alignment (NA), a DPCL method that explicitly suppresses noise components in poisoned images. Inspired by powerful training-controllable CL attacks, we identify and extract the critical objective of noisy alignment, adapting it effectively into data-poisoning scenarios. Our method implements noisy alignment by strategically manipulating contrastive learning's random cropping mechanism, formulating this process as an image layout optimization problem with theoretically derived optimal parameters. The resulting method is simple yet effective, achieving state-of-the-art performance compared to existing DPCLs, while maintaining clean-data accuracy. Furthermore, Noisy Alignment demonstrates robustness against common backdoor defenses. Codes can be found at this https URL.</li>
</ul>

<h3>Title: Ask Good Questions for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qi Wu, Zhongqi Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14025">https://arxiv.org/abs/2508.14025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14025">https://arxiv.org/pdf/2508.14025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14025]] Ask Good Questions for Large Language Models(https://arxiv.org/abs/2508.14025)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have significantly improved the performance of dialog systems, yet current approaches often fail to provide accurate guidance of topic due to their inability to discern user confusion in related concepts. To address this, we introduce the Ask-Good-Question (AGQ) framework, which features an improved Concept-Enhanced Item Response Theory (CEIRT) model to better identify users' knowledge levels. Our contributions include applying the CEIRT model along with LLMs to directly generate guiding questions based on the inspiring text, greatly improving information retrieval efficiency during the question & answer process. Through comparisons with other baseline methods, our approach outperforms by significantly enhencing the users' information retrieval experiences.</li>
</ul>

<h3>Title: Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR</h3>
<ul>
<li><strong>Authors: </strong>Xiao Liang, Zhongzhi Li, Yeyun Gong, Yelong Shen, Ying Nian Wu, Zhijiang Guo, Weizhu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14029">https://arxiv.org/abs/2508.14029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14029">https://arxiv.org/pdf/2508.14029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14029]] Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR(https://arxiv.org/abs/2508.14029)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a key paradigm for post-training Large Language Models (LLMs), particularly for complex reasoning tasks. However, vanilla RLVR training has been shown to improve Pass@1 performance at the expense of policy entropy, leading to reduced generation diversity and limiting the Pass@k performance, which typically represents the upper bound of LLM reasoning capability. In this paper, we systematically analyze the policy's generation diversity from the perspective of training problems and find that augmenting and updating training problems helps mitigate entropy collapse during training. Based on these observations, we propose an online Self-play with Variational problem Synthesis (SvS) strategy for RLVR training, which uses the policy's correct solutions to synthesize variational problems while ensuring their reference answers remain identical to the originals. This self-improving strategy effectively maintains policy entropy during training and substantially improves Pass@k compared with standard RLVR, sustaining prolonged improvements and achieving absolute gains of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model sizes from 3B to 32B consistently demonstrate the generalizability and robustness of SvS.</li>
</ul>

<h3>Title: Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Dongyoon Hahm, Taywon Min, Woogyeol Jin, Kimin Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14031">https://arxiv.org/abs/2508.14031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14031">https://arxiv.org/pdf/2508.14031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14031]] Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation(https://arxiv.org/abs/2508.14031)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature.</li>
</ul>

<h3>Title: The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities</h3>
<ul>
<li><strong>Authors: </strong>Xiancheng Li, Georgios D. Karampatakis, Helen E. Wood, Chris J. Griffiths, Borislava Mihaylova, Neil S. Coulson, Alessio Pasinato, Pietro Panzarasa, Marco Viviani, Anna De Simoni</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14032">https://arxiv.org/abs/2508.14032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14032">https://arxiv.org/pdf/2508.14032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14032]] The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities(https://arxiv.org/abs/2508.14032)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Digital health analytics face critical challenges nowadays. The sophisticated analysis of patient-generated health content, which contains complex emotional and medical contexts, requires scarce domain expertise, while traditional ML approaches are constrained by data shortage and privacy limitations in healthcare settings. Online Health Communities (OHCs) exemplify these challenges with mixed-sentiment posts, clinical terminology, and implicit emotional expressions that demand specialised knowledge for accurate Sentiment Analysis (SA). To address these challenges, this study explores how Large Language Models (LLMs) can integrate expert knowledge through in-context learning for SA, providing a scalable solution for sophisticated health data analysis. Specifically, we develop a structured codebook that systematically encodes expert interpretation guidelines, enabling LLMs to apply domain-specific knowledge through targeted prompting rather than extensive training. Six GPT models validated alongside DeepSeek and LLaMA 3.1 are compared with pre-trained language models (BioBERT variants) and lexicon-based methods, using 400 expert-annotated posts from two OHCs. LLMs achieve superior performance while demonstrating expert-level agreement. This high agreement, with no statistically significant difference from inter-expert agreement levels, suggests knowledge integration beyond surface-level pattern recognition. The consistent performance across diverse LLM models, supported by in-context learning, offers a promising solution for digital health analytics. This approach addresses the critical challenge of expert knowledge shortage in digital health research, enabling real-time, expert-quality analysis for patient monitoring, intervention assessment, and evidence-based health strategies.</li>
</ul>

<h3>Title: GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ken Deng, Yunhan Yang, Jingxiang Sun, Xihui Liu, Yebin Liu, Ding Liang, Yan-Pei Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14036">https://arxiv.org/abs/2508.14036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14036">https://arxiv.org/pdf/2508.14036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14036]] GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation(https://arxiv.org/abs/2508.14036)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Modern 3D generation methods can rapidly create shapes from sparse or single views, but their outputs often lack geometric detail due to computational constraints. We present DetailGen3D, a generative approach specifically designed to enhance these generated 3D shapes. Our key insight is to model the coarse-to-fine transformation directly through data-dependent flows in latent space, avoiding the computational overhead of large-scale 3D generative models. We introduce a token matching strategy that ensures accurate spatial correspondence during refinement, enabling local detail synthesis while preserving global structure. By carefully designing our training data to match the characteristics of synthesized coarse shapes, our method can effectively enhance shapes produced by various 3D generation and reconstruction approaches, from single-view to sparse multi-view inputs. Extensive experiments demonstrate that DetailGen3D achieves high-fidelity geometric detail synthesis while maintaining efficiency in training.</li>
</ul>

<h3>Title: LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos</h3>
<ul>
<li><strong>Authors: </strong>Chin-Yang Lin, Cheng Sun, Fu-En Yang, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14041">https://arxiv.org/abs/2508.14041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14041">https://arxiv.org/pdf/2508.14041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14041]] LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos(https://arxiv.org/abs/2508.14041)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>LongSplat addresses critical challenges in novel view synthesis (NVS) from casually captured long videos characterized by irregular camera motion, unknown camera poses, and expansive scenes. Current methods often suffer from pose drift, inaccurate geometry initialization, and severe memory limitations. To address these issues, we introduce LongSplat, a robust unposed 3D Gaussian Splatting framework featuring: (1) Incremental Joint Optimization that concurrently optimizes camera poses and 3D Gaussians to avoid local minima and ensure global consistency; (2) a robust Pose Estimation Module leveraging learned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that converts dense point clouds into anchors based on spatial density. Extensive experiments on challenging benchmarks demonstrate that LongSplat achieves state-of-the-art results, substantially improving rendering quality, pose accuracy, and computational efficiency compared to prior approaches. Project page: this https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
