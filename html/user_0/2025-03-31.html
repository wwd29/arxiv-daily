<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-03-31</h1>
<h3>Title: Leveraging Large Language Models for Automated Causal Loop Diagram Generation: Enhancing System Dynamics Modeling through Curated Prompting Techniques</h3>
<ul>
<li><strong>Authors: </strong>Ning-Yuan Georgia Liu, David R. Keith</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21798">https://arxiv.org/abs/2503.21798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21798">https://arxiv.org/pdf/2503.21798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21798]] Leveraging Large Language Models for Automated Causal Loop Diagram Generation: Enhancing System Dynamics Modeling through Curated Prompting Techniques(https://arxiv.org/abs/2503.21798)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Transforming a dynamic hypothesis into a causal loop diagram (CLD) is crucial for System Dynamics Modelling. Extracting key variables and causal relationships from text to build a CLD is often challenging and time-consuming for novice modelers, limiting SD tool adoption. This paper introduces and tests a method for automating the translation of dynamic hypotheses into CLDs using large language models (LLMs) with curated prompting techniques. We first describe how LLMs work and how they can make the inferences needed to build CLDs using a standard digraph structure. Next, we develop a set of simple dynamic hypotheses and corresponding CLDs from leading SD textbooks. We then compare the four different combinations of prompting techniques, evaluating their performance against CLDs labeled by expert modelers. Results show that for simple model structures and using curated prompting techniques, LLMs can generate CLDs of a similar quality to expert-built ones, accelerating CLD creation.</li>
</ul>

<h3>Title: ELM: Ensemble of Language Models for Predicting Tumor Group from Pathology Reports</h3>
<ul>
<li><strong>Authors: </strong>Lovedeep Gondara, Jonathan Simkin, Shebnum Devji, Gregory Arbour, Raymond Ng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21800">https://arxiv.org/abs/2503.21800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21800">https://arxiv.org/pdf/2503.21800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21800]] ELM: Ensemble of Language Models for Predicting Tumor Group from Pathology Reports(https://arxiv.org/abs/2503.21800)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Population-based cancer registries (PBCRs) face a significant bottleneck in manually extracting data from unstructured pathology reports, a process crucial for tasks like tumor group assignment, which can consume 900 person-hours for approximately 100,000 reports. To address this, we introduce ELM (Ensemble of Language Models), a novel ensemble-based approach leveraging both small language models (SLMs) and large language models (LLMs). ELM utilizes six fine-tuned SLMs, where three SLMs use the top part of the pathology report and three SLMs use the bottom part. This is done to maximize report coverage. ELM requires five-out-of-six agreement for a tumor group classification. Disagreements are arbitrated by an LLM with a carefully curated prompt. Our evaluation across nineteen tumor groups demonstrates ELM achieves an average precision and recall of 0.94, outperforming single-model and ensemble-without-LLM approaches. Deployed at the British Columbia Cancer Registry, ELM demonstrates how LLMs can be successfully applied in a PBCR setting to achieve state-of-the-art results and significantly enhance operational efficiencies, saving hundreds of person-hours annually.</li>
</ul>

<h3>Title: Comparison of Metadata Representation Models for Knowledge Graph Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Shusaku Egami, Kyoumoto Matsushita, Takanori Ugai, Ken Fukuda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21804">https://arxiv.org/abs/2503.21804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21804">https://arxiv.org/pdf/2503.21804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21804]] Comparison of Metadata Representation Models for Knowledge Graph Embeddings(https://arxiv.org/abs/2503.21804)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Hyper-relational Knowledge Graphs (HRKGs) extend traditional KGs beyond binary relations, enabling the representation of contextual, provenance, and temporal information in domains, such as historical events, sensor data, video content, and narratives. HRKGs can be structured using several Metadata Representation Models (MRMs), including Reification (REF), Singleton Property (SGP), and RDF-star (RDR). However, the effects of different MRMs on KG Embedding (KGE) and Link Prediction (LP) models remain unclear. This study evaluates MRMs in the context of LP tasks, identifies the limitations of existing evaluation frameworks, and introduces a new task that ensures fair comparisons across MRMs. Furthermore, we propose a framework that effectively reflects the knowledge representations of the three MRMs in latent space. Experiments on two types of datasets reveal that REF performs well in simple HRKGs, whereas SGP is less effective. However, in complex HRKGs, the differences among MRMs in the LP tasks are minimal. Our findings contribute to an optimal knowledge representation strategy for HRKGs in LP tasks.</li>
</ul>

<h3>Title: ImF: Implicit Fingerprint for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wu jiaxuan, Peng Wanli, Fu hang, Xue Yiming, Wen juan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21805">https://arxiv.org/abs/2503.21805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21805">https://arxiv.org/pdf/2503.21805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21805]] ImF: Implicit Fingerprint for Large Language Models(https://arxiv.org/abs/2503.21805)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Training large language models (LLMs) is resource-intensive and expensive, making intellectual property (IP) protection essential. Most existing model fingerprint methods inject fingerprints into LLMs to protect model ownership. These methods create fingerprint pairs with weak semantic correlations, lacking the contextual coherence and semantic relatedness founded in normal question-answer (QA) pairs in LLMs. In this paper, we propose a Generation Revision Intervention (GRI) attack that can effectively exploit this flaw to erase fingerprints, highlighting the need for more secure model fingerprint methods. Thus, we propose a novel injected fingerprint paradigm called Implicit Fingerprints (ImF). ImF constructs fingerprint pairs with strong semantic correlations, disguising them as natural QA pairs within LLMs. This ensures the fingerprints are consistent with normal model behavior, making them indistinguishable and robust against detection and removal. Our experiment on multiple LLMs demonstrates that ImF retains high verification success rates under adversarial conditions, offering a reliable solution for protecting LLM ownership.</li>
</ul>

<h3>Title: Large Language Models Meet Contrastive Learning: Zero-Shot Emotion Recognition Across Languages</h3>
<ul>
<li><strong>Authors: </strong>Heqing Zou, Fengmao Lv, Desheng Zheng, Eng Siong Chng, Deepu Rajan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21806">https://arxiv.org/abs/2503.21806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21806">https://arxiv.org/pdf/2503.21806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21806]] Large Language Models Meet Contrastive Learning: Zero-Shot Emotion Recognition Across Languages(https://arxiv.org/abs/2503.21806)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multilingual speech emotion recognition aims to estimate a speaker's emotional state using a contactless method across different languages. However, variability in voice characteristics and linguistic diversity poses significant challenges for zero-shot speech emotion recognition, especially with multilingual datasets. In this paper, we propose leveraging contrastive learning to refine multilingual speech features and extend large language models for zero-shot multilingual speech emotion estimation. Specifically, we employ a novel two-stage training framework to align speech signals with linguistic features in the emotional space, capturing both emotion-aware and language-agnostic speech representations. To advance research in this field, we introduce a large-scale synthetic multilingual speech emotion dataset, M5SER. Our experiments demonstrate the effectiveness of the proposed method in both speech emotion recognition and zero-shot multilingual speech emotion recognition, including previously unseen datasets and languages.</li>
</ul>

<h3>Title: LERO: LLM-driven Evolutionary framework with Hybrid Rewards and Enhanced Observation for Multi-Agent Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuan Wei, Xiaohan Shan, Jianmin Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21807">https://arxiv.org/abs/2503.21807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21807">https://arxiv.org/pdf/2503.21807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21807]] LERO: LLM-driven Evolutionary framework with Hybrid Rewards and Enhanced Observation for Multi-Agent Reinforcement Learning(https://arxiv.org/abs/2503.21807)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-agent reinforcement learning (MARL) faces two critical bottlenecks distinct from single-agent RL: credit assignment in cooperative tasks and partial observability of environmental states. We propose LERO, a framework integrating Large language models (LLMs) with evolutionary optimization to address these MARL-specific challenges. The solution centers on two LLM-generated components: a hybrid reward function that dynamically allocates individual credit through reward decomposition, and an observation enhancement function that augments partial observations with inferred environmental context. An evolutionary algorithm optimizes these components through iterative MARL training cycles, where top-performing candidates guide subsequent LLM generations. Evaluations in Multi-Agent Particle Environments (MPE) demonstrate LERO's superiority over baseline methods, with improved task performance and training efficiency.</li>
</ul>

<h3>Title: IPGO: Indirect Prompt Gradient Optimization on Text-to-Image Generative Models with High Data Efficiency</h3>
<ul>
<li><strong>Authors: </strong>Jianping Ye, Michel Wedel, Kunpeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21812">https://arxiv.org/abs/2503.21812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21812">https://arxiv.org/pdf/2503.21812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21812]] IPGO: Indirect Prompt Gradient Optimization on Text-to-Image Generative Models with High Data Efficiency(https://arxiv.org/abs/2503.21812)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-Image Diffusion models excel at generating images from text prompts but often lack optimal alignment with content semantics, aesthetics, and human preferences. To address these issues, in this study we introduce a novel framework, Indirect Prompt Gradient Optimization (IPGO), for prompt-level fine-tuning. IPGO enhances prompt embeddings by injecting continuously differentiable tokens at the beginning and end of the prompt embeddings, while exploiting low-rank benefits and flexibility from rotations. It allows for gradient-based optimization of injected tokens while enforcing value, orthonormality, and conformity constraints, facilitating continuous updates and empowering computational efficiency. To evaluate the performance of IPGO, we conduct prompt-wise and prompt-batch training with three reward models targeting image aesthetics, image-text alignment, and human preferences under three datasets of different complexity. The results show that IPGO consistently matches or outperforms cutting-edge benchmarks, including stable diffusion v1.5 with raw prompts, training-based approaches (DRaFT and DDPO), and training-free methods (DPO-Diffusion, Promptist, and ChatGPT-4o). Furthermore, we demonstrate IPGO's effectiveness in enhancing image generation quality while requiring minimal training data and limited computational resources.</li>
</ul>

<h3>Title: OAEI-LLM-T: A TBox Benchmark Dataset for Understanding LLM Hallucinations in Ontology Matching Systems</h3>
<ul>
<li><strong>Authors: </strong>Zhangcheng Qiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21813">https://arxiv.org/abs/2503.21813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21813">https://arxiv.org/pdf/2503.21813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21813]] OAEI-LLM-T: A TBox Benchmark Dataset for Understanding LLM Hallucinations in Ontology Matching Systems(https://arxiv.org/abs/2503.21813)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hallucinations are inevitable in downstream tasks using large language models (LLMs). While addressing hallucinations becomes a substantial challenge for LLM-based ontology matching (OM) systems, we introduce a new benchmark dataset called OAEI-LLM-T. The dataset evolves from the TBox (i.e. schema-matching) datasets in the Ontology Alignment Evaluation Initiative (OAEI), capturing hallucinations of different LLMs performing OM tasks. These OM-specific hallucinations are carefully classified into two primary categories and six sub-categories. We showcase the usefulness of the dataset in constructing the LLM leaderboard and fine-tuning foundational LLMs for LLM-based OM systems.</li>
</ul>

<h3>Title: Skip-Vision: A Comprehensive Framework for Accelerating Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weili Zeng, Ziyuan Huang, Kaixiang Ji, Yichao Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21817">https://arxiv.org/abs/2503.21817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21817">https://arxiv.org/pdf/2503.21817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21817]] Skip-Vision: A Comprehensive Framework for Accelerating Vision-Language Models(https://arxiv.org/abs/2503.21817)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based models have driven significant advancements in Multimodal Large Language Models (MLLMs), yet their computational costs surge drastically when scaling resolution, training data, and model parameters. A key bottleneck stems from the proliferation of visual tokens required for fine-grained image understanding. We propose Skip-Vision, a unified framework addressing both training and inference inefficiencies in vision-language models. On top of conventional token compression approaches, our method introduces two complementary acceleration strategies. For training acceleration, we observe that Feed-Forward Network (FFN) computations on visual tokens induce marginal feature updates. This motivates our Skip-FFN strategy, which bypasses FFN layers for redundant visual tokens. For inference acceleration, we design a selective KV-cache removal mechanism that prunes the skipped key-value pairs during decoding while preserving model performance. Experimental results demonstrate that Skip-Vision reduces training time by up to 35\%, inference FLOPs by 75\%, and latency by 45\%, while achieving comparable or superior performance to existing methods. Our work provides a practical solution for scaling high-performance MLLMs with enhanced efficiency.</li>
</ul>

<h3>Title: Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO Approach</h3>
<ul>
<li><strong>Authors: </strong>Xuying Li, Zhuo Li, Yuji Kosuga, Victor Bian</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21819">https://arxiv.org/abs/2503.21819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21819">https://arxiv.org/pdf/2503.21819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21819]] Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO Approach(https://arxiv.org/abs/2503.21819)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLMs) with human values and safety constraints is challenging, especially when objectives like helpfulness, truthfulness, and avoidance of harm conflict. Reinforcement Learning from Human Feedback (RLHF) has achieved notable success in steering models, but is complex and can be unstable. Recent approaches such as Direct Preference Optimization (DPO) simplify preference-based fine-tuning but may introduce bias or trade-off certain objectives~\cite{dpo}. In this work, we propose a Group Relative Policy Optimization (GRPO) framework with a multi-label reward regression model to achieve safe and aligned language generation. The GRPO algorithm optimizes a policy by comparing groups of sampled responses, eliminating the need for a separate value critic and improving training efficiency~\cite{grpo}. We train a reward model to predict multiple alignment scores (e.g., safety, helpfulness, etc.), which are combined into a single reward signal. We provide a theoretical derivation for using this learned multi-aspect reward within GRPO and discuss its advantages and limitations. Empirically, our approach improves all the safety and quality metrics evaluated in language generation tasks on model scales (0.5B, 7B, and 14B parameters), demonstrating a robust balance of objectives. We compare GRPO to PPO-based RLHF and DPO, highlighting that GRPO achieves alignment with significantly lower computational cost and explicit multi-objective handling. \textbf{We will open-source all trained models at this https URL.</li>
</ul>

<h3>Title: UFM: Unified Feature Matching Pre-training with Multi-Modal Image Assistants</h3>
<ul>
<li><strong>Authors: </strong>Yide Di, Yun Liao, Hao Zhou, Kaijun Zhu, Qing Duan, Junhui Liu, Mingyu Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21820">https://arxiv.org/abs/2503.21820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21820">https://arxiv.org/pdf/2503.21820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21820]] UFM: Unified Feature Matching Pre-training with Multi-Modal Image Assistants(https://arxiv.org/abs/2503.21820)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Image feature matching, a foundational task in computer vision, remains challenging for multimodal image applications, often necessitating intricate training on specific datasets. In this paper, we introduce a Unified Feature Matching pre-trained model (UFM) designed to address feature matching challenges across a wide spectrum of modal images. We present Multimodal Image Assistant (MIA) transformers, finely tunable structures adept at handling diverse feature matching problems. UFM exhibits versatility in addressing both feature matching tasks within the same modal and those across different modals. Additionally, we propose a data augmentation algorithm and a staged pre-training strategy to effectively tackle challenges arising from sparse data in specific modals and imbalanced modal datasets. Experimental results demonstrate that UFM excels in generalization and performance across various feature matching tasks. The code will be released at:this https URL.</li>
</ul>

<h3>Title: Low-Rank Adaptation of Pre-Trained Stable Diffusion for Rigid-Body Target ISAR Imaging</h3>
<ul>
<li><strong>Authors: </strong>Boan Zhang, Hang Dong, Jiongge Zhang, Long Tian, Rongrong Wang, Zhenhua Wu, Xiyang Liu, Hongwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21823">https://arxiv.org/abs/2503.21823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21823">https://arxiv.org/pdf/2503.21823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21823]] Low-Rank Adaptation of Pre-Trained Stable Diffusion for Rigid-Body Target ISAR Imaging(https://arxiv.org/abs/2503.21823)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Traditional range-instantaneous Doppler (RID) methods for rigid-body target imaging often suffer from low resolution due to the limitations of time-frequency analysis (TFA). To address this challenge, our primary focus is on obtaining high resolution time-frequency representations (TFRs) from their low resolution counterparts. Recognizing that the curve features of TFRs are a specific type of texture feature, we argue that pre trained generative models such as Stable Diffusion (SD) are well suited for enhancing TFRs, thanks to their powerful capability in capturing texture representations. Building on this insight, we propose a novel inverse synthetic aperture radar (ISAR) imaging method for rigid-body targets, leveraging the low-rank adaptation (LoRA) of a pre-trained SD model. Our approach adopts the basic structure and pre-trained parameters of SD Turbo while incorporating additional linear operations for LoRA and adversarial training to achieve super-resolution and noise suppression. Then we integrate LoRA-SD into the RID-based ISAR imaging, enabling sharply focused and denoised imaging with super-resolution capabilities. We evaluate our method using both simulated and real radar data. The experimental results demonstrate the superiority of our approach in frequency es timation and ISAR imaging compared to traditional methods. Notably, the generalization capability is verified by training on simulated radar data and testing on measured radar data.</li>
</ul>

<h3>Title: Protecting Your Video Content: Disrupting Automated Video-based LLM Annotations</h3>
<ul>
<li><strong>Authors: </strong>Haitong Liu, Kuofeng Gao, Yang Bai, Jinmin Li, Jinxiao Shan, Tao Dai, Shu-Tao Xia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21824">https://arxiv.org/abs/2503.21824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21824">https://arxiv.org/pdf/2503.21824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21824]] Protecting Your Video Content: Disrupting Automated Video-based LLM Annotations(https://arxiv.org/abs/2503.21824)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, robust, steal, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Recently, video-based large language models (video-based LLMs) have achieved impressive performance across various video comprehension tasks. However, this rapid advancement raises significant privacy and security concerns, particularly regarding the unauthorized use of personal video data in automated annotation by video-based LLMs. These unauthorized annotated video-text pairs can then be used to improve the performance of downstream tasks, such as text-to-video generation. To safeguard personal videos from unauthorized use, we propose two series of protective video watermarks with imperceptible adversarial perturbations, named Ramblings and Mutes. Concretely, Ramblings aim to mislead video-based LLMs into generating inaccurate captions for the videos, thereby degrading the quality of video annotations through inconsistencies between video content and captions. Mutes, on the other hand, are designed to prompt video-based LLMs to produce exceptionally brief captions, lacking descriptive detail. Extensive experiments demonstrate that our video watermarking methods effectively protect video data by significantly reducing video annotation performance across various video-based LLMs, showcasing both stealthiness and robustness in protecting personal video content. Our code is available at this https URL.</li>
</ul>

<h3>Title: Hybrid Multi-Stage Learning Framework for Edge Detection: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Mark Phil Pacot, Jayno Juventud, Gleen Dalaorao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21827">https://arxiv.org/abs/2503.21827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21827">https://arxiv.org/pdf/2503.21827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21827]] Hybrid Multi-Stage Learning Framework for Edge Detection: A Survey(https://arxiv.org/abs/2503.21827)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Edge detection remains a fundamental yet challenging task in computer vision, especially under varying illumination, noise, and complex scene conditions. This paper introduces a Hybrid Multi-Stage Learning Framework that integrates Convolutional Neural Network (CNN) feature extraction with a Support Vector Machine (SVM) classifier to improve edge localization and structural accuracy. Unlike conventional end-to-end deep learning models, our approach decouples feature representation and classification stages, enhancing robustness and interpretability. Extensive experiments conducted on benchmark datasets such as BSDS500 and NYUDv2 demonstrate that the proposed framework outperforms traditional edge detectors and even recent learning-based methods in terms of Optimal Dataset Scale (ODS) and Optimal Image Scale (OIS), while maintaining competitive Average Precision (AP). Both qualitative and quantitative results highlight enhanced performance on edge continuity, noise suppression, and perceptual clarity achieved by our method. This work not only bridges classical and deep learning paradigms but also sets a new direction for scalable, interpretable, and high-quality edge detection solutions.</li>
</ul>

<h3>Title: Shape Generation via Weight Space Learning</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Plattner, Arturs Berzins, Johannes Brandstetter</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21830">https://arxiv.org/abs/2503.21830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21830">https://arxiv.org/pdf/2503.21830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21830]] Shape Generation via Weight Space Learning(https://arxiv.org/abs/2503.21830)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Foundation models for 3D shape generation have recently shown a remarkable capacity to encode rich geometric priors across both global and local dimensions. However, leveraging these priors for downstream tasks can be challenging as real-world data are often scarce or noisy, and traditional fine-tuning can lead to catastrophic forgetting. In this work, we treat the weight space of a large 3D shape-generative model as a data modality that can be explored directly. We hypothesize that submanifolds within this high-dimensional weight space can modulate topological properties or fine-grained part features separately, demonstrating early-stage evidence via two experiments. First, we observe a sharp phase transition in global connectivity when interpolating in conditioning space, suggesting that small changes in weight space can drastically alter topology. Second, we show that low-dimensional reparameterizations yield controlled local geometry changes even with very limited data. These results highlight the potential of weight space learning to unlock new approaches for 3D shape generation and specialized fine-tuning.</li>
</ul>

<h3>Title: Refining Time Series Anomaly Detectors using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alan Yang, Yulin Chen, Sean Lee, Venus Montes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21833">https://arxiv.org/abs/2503.21833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21833">https://arxiv.org/pdf/2503.21833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21833]] Refining Time Series Anomaly Detectors using Large Language Models(https://arxiv.org/abs/2503.21833)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection (TSAD) is of widespread interest across many industries, including finance, healthcare, and manufacturing. Despite the development of numerous automatic methods for detecting anomalies, human oversight remains necessary to review and act upon detected anomalies, as well as verify their accuracy. We study the use of multimodal large language models (LLMs) to partially automate this process. We find that LLMs can effectively identify false alarms by integrating visual inspection of time series plots with text descriptions of the data-generating process. By leveraging the capabilities of LLMs, we aim to reduce the reliance on human effort required to maintain a TSAD system</li>
</ul>

<h3>Title: A Multi-Modal Knowledge-Enhanced Framework for Vessel Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Haomin Yu, Tianyi Li, Kristian Torp, Christian S. Jensen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21834">https://arxiv.org/abs/2503.21834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21834">https://arxiv.org/pdf/2503.21834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21834]] A Multi-Modal Knowledge-Enhanced Framework for Vessel Trajectory Prediction(https://arxiv.org/abs/2503.21834)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>Accurate vessel trajectory prediction facilitates improved navigational safety, routing, and environmental protection. However, existing prediction methods are challenged by the irregular sampling time intervals of the vessel tracking data from the global AIS system and the complexity of vessel movement. These aspects render model learning and generalization difficult. To address these challenges and improve vessel trajectory prediction, we propose the multi-modal knowledge-enhanced framework (MAKER) for vessel trajectory prediction. To contend better with the irregular sampling time intervals, MAKER features a Large language model-guided Knowledge Transfer (LKT) module that leverages pre-trained language models to transfer trajectory-specific contextual knowledge effectively. To enhance the ability to learn complex trajectory patterns, MAKER incorporates a Knowledge-based Self-paced Learning (KSL) module. This module employs kinematic knowledge to progressively integrate complex patterns during training, allowing for adaptive learning and enhanced generalization. Experimental results on two vessel trajectory datasets show that MAKER can improve the prediction accuracy of state-of-the-art methods by 12.08%-17.86%.</li>
</ul>

<h3>Title: iMedImage Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Ran Wei, ZhiXiong Lan, Qing Yan, Ning Song, Ming Lv, LongQing Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21836">https://arxiv.org/abs/2503.21836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21836">https://arxiv.org/pdf/2503.21836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21836]] iMedImage Technical Report(https://arxiv.org/abs/2503.21836)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Background: Chromosome karyotype analysis is crucial for diagnosing hereditary diseases, yet detecting structural abnormalities remains challenging. While AI has shown promise in medical imaging, its effectiveness varies across modalities. Leveraging advances in Foundation Models that integrate multimodal medical imaging for robust feature extraction and accurate diagnosis, we developed iMedImage, an end-to-end model for general medical image recognition, demonstrating strong performance across multiple imaging tasks, including chromosome abnormality detection. Materials and Methods: We constructed a comprehensive medical image dataset encompassing multiple modalities from common medical domains, including chromosome, cell, pathology, ultrasound, X-ray, CT, and MRI images. Based on this dataset, we developed the iMedImage model, which incorporates the following key features: (1) a unified representation method for diverse modality inputs and medical imaging tasks; (2) multi-level (case-level, image-level, patch-level) image recognition capabilities enhanced by Chain of Thought (CoT) embedding and Mixture of Experts (MoE) strategies. Results: The test set comprised data from 12 institutions across six regions in China, covering three mainstream scanning devices, and included naturally distributed, unscreened abnormal cases. On this diverse dataset, the model achieved a fully automated chromosome analysis workflow, including segmentation, karyotyping, and abnormality detection, reaching a sensitivity of 92.75% and a specificity of 91.5%. Conclusion: We propose iMedImage, an end-to-end foundation model for medical image analysis, demonstrating its superior performance across various medical imaging tasks. iMedImage provides clinicians with a precise imaging analysis tool and contributes to improving diagnostic accuracy and disease screening.</li>
</ul>

<h3>Title: MSPLoRA: A Multi-Scale Pyramid Low-Rank Adaptation for Efficient Model Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Jiancheng Zhao, Xingda Yu, Zhen Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21838">https://arxiv.org/abs/2503.21838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21838">https://arxiv.org/pdf/2503.21838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21838]] MSPLoRA: A Multi-Scale Pyramid Low-Rank Adaptation for Efficient Model Fine-Tuning(https://arxiv.org/abs/2503.21838)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Parameter-Efficient Fine-Tuning (PEFT) has become an essential approach for adapting large-scale pre-trained models while reducing computational costs. Among PEFT methods, LoRA significantly reduces trainable parameters by decomposing weight updates into low-rank matrices. However, traditional LoRA applies a fixed rank across all layers, failing to account for the varying complexity of hierarchical information, which leads to inefficient adaptation and redundancy. To address this, we propose MSPLoRA (Multi-Scale Pyramid LoRA), which introduces Global Shared LoRA, Mid-Level Shared LoRA, and Layer-Specific LoRA to capture global patterns, mid-level features, and fine-grained information, respectively. This hierarchical structure reduces inter-layer redundancy while maintaining strong adaptation capability. Experiments on various NLP tasks demonstrate that MSPLoRA achieves more efficient adaptation and better performance while significantly reducing the number of trainable parameters. Furthermore, additional analyses based on Singular Value Decomposition validate its information decoupling ability, highlighting MSPLoRA as a scalable and effective optimization strategy for parameter-efficient fine-tuning in large language models. Our code is available at this https URL.</li>
</ul>

<h3>Title: M-DocSum: Do LVLMs Genuinely Comprehend Interleaved Image-Text in Document Summarization?</h3>
<ul>
<li><strong>Authors: </strong>Haolong Yan, Kaijun Tan, Yeqing Shen, Xin Huang, Zheng Ge, Xiangyu Zhang, Si Li, Daxin Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21839">https://arxiv.org/abs/2503.21839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21839">https://arxiv.org/pdf/2503.21839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21839]] M-DocSum: Do LVLMs Genuinely Comprehend Interleaved Image-Text in Document Summarization?(https://arxiv.org/abs/2503.21839)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We investigate a critical yet under-explored question in Large Vision-Language Models (LVLMs): Do LVLMs genuinely comprehend interleaved image-text in the document? Existing document understanding benchmarks often assess LVLMs using question-answer formats, which are information-sparse and difficult to guarantee the coverage of long-range dependencies. To address this issue, we introduce a novel and challenging Multimodal Document Summarization Benchmark (M-DocSum-Bench), which comprises 500 high-quality arXiv papers, along with interleaved multimodal summaries aligned with human preferences. M-DocSum-Bench is a reference-based generation task and necessitates the generation of interleaved image-text summaries using provided reference images, thereby simultaneously evaluating capabilities in understanding, reasoning, localization, and summarization within complex multimodal document scenarios. To facilitate this benchmark, we develop an automated framework to construct summaries and propose a fine-grained evaluation method called M-DocEval. Moreover, we further develop a robust summarization baseline, i.e., M-DocSum-7B, by progressive two-stage training with diverse instruction and preference data. The extensive results on our M-DocSum-Bench reveal that the leading LVLMs struggle to maintain coherence and accurately integrate information within long and interleaved contexts, often exhibiting confusion between similar images and a lack of robustness. Notably, M-DocSum-7B achieves state-of-the-art performance compared to larger and closed-source models (including GPT-4o, Gemini Pro, Claude-3.5-Sonnet and Qwen2.5-VL-72B, etc.), demonstrating the potential of LVLMs for improved interleaved image-text understanding. The code, data, and models are available at this https URL.</li>
</ul>

<h3>Title: CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Hanyu Liu, Siyao Li, Ying Yu, Yixuan Jiang, Hang Xiao, Jingxi Long, Haotian Tang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21843">https://arxiv.org/abs/2503.21843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21843">https://arxiv.org/pdf/2503.21843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21843]] CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity Recognition(https://arxiv.org/abs/2503.21843)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Human Activity Recognition (HAR) is a fundamental technology for numerous human - centered intelligent applications. Although deep learning methods have been utilized to accelerate feature extraction, issues such as multimodal data mixing, activity heterogeneity, and complex model deployment remain largely unresolved. The aim of this paper is to address issues such as multimodal data mixing, activity heterogeneity, and complex model deployment in sensor-based human activity recognition. We propose a spatiotemporal attention modal decomposition alignment fusion strategy to tackle the problem of the mixed distribution of sensor data. Key discriminative features of activities are captured through cross-modal spatio-temporal disentangled representation, and gradient modulation is combined to alleviate data heterogeneity. In addition, a wearable deployment simulation system is constructed. We conducted experiments on a large number of public datasets, demonstrating the effectiveness of the model.</li>
</ul>

<h3>Title: Comparative Analysis of Image, Video, and Audio Classifiers for Automated News Video Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Attard, Dylan Seychell</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21848">https://arxiv.org/abs/2503.21848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21848">https://arxiv.org/pdf/2503.21848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21848]] Comparative Analysis of Image, Video, and Audio Classifiers for Automated News Video Segmentation(https://arxiv.org/abs/2503.21848)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>News videos require efficient content organisation and retrieval systems, but their unstructured nature poses significant challenges for automated processing. This paper presents a comprehensive comparative analysis of image, video, and audio classifiers for automated news video segmentation. This work presents the development and evaluation of multiple deep learning approaches, including ResNet, ViViT, AST, and multimodal architectures, to classify five distinct segment types: advertisements, stories, studio scenes, transitions, and visualisations. Using a custom-annotated dataset of 41 news videos comprising 1,832 scene clips, our experiments demonstrate that image-based classifiers achieve superior performance (84.34\% accuracy) compared to more complex temporal models. Notably, the ResNet architecture outperformed state-of-the-art video classifiers while requiring significantly fewer computational resources. Binary classification models achieved high accuracy for transitions (94.23\%) and advertisements (92.74\%). These findings advance the understanding of effective architectures for news video segmentation and provide practical insights for implementing automated content organisation systems in media applications. These include media archiving, personalised content delivery, and intelligent video search.</li>
</ul>

<h3>Title: Foveated Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hongyi Zeng, Wenxuan Liu, Tianhua Xia, Jinhui Chen, Ziyun Li, Sai Qian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21854">https://arxiv.org/abs/2503.21854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21854">https://arxiv.org/pdf/2503.21854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21854]] Foveated Instance Segmentation(https://arxiv.org/abs/2503.21854)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Instance segmentation is essential for augmented reality and virtual reality (AR/VR) as it enables precise object recognition and interaction, enhancing the integration of virtual and real-world elements for an immersive experience. However, the high computational overhead of segmentation limits its application on resource-constrained AR/VR devices, causing large processing latency and degrading user experience. In contrast to conventional scenarios, AR/VR users typically focus on only a few regions within their field of view before shifting perspective, allowing segmentation to be concentrated on gaze-specific areas. This insight drives the need for efficient segmentation methods that prioritize processing instance of interest, reducing computational load and enhancing real-time performance. In this paper, we present a foveated instance segmentation (FovealSeg) framework that leverages real-time user gaze data to perform instance segmentation exclusively on instance of interest, resulting in substantial computational savings. Evaluation results show that FSNet achieves an IoU of 0.56 on ADE20K and 0.54 on LVIS, notably outperforming the baseline. The code is available at this https URL</li>
</ul>

<h3>Title: RedditESS: A Mental Health Social Support Interaction Dataset -- Understanding Effective Social Support to Refine AI-Driven Support Tools</h3>
<ul>
<li><strong>Authors: </strong>Zeyad Alghamdi, Tharindu Kumarage, Garima Agrawal, Mansooreh Karami, Ibrahim Almuteb, Huan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21888">https://arxiv.org/abs/2503.21888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21888">https://arxiv.org/pdf/2503.21888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21888]] RedditESS: A Mental Health Social Support Interaction Dataset -- Understanding Effective Social Support to Refine AI-Driven Support Tools(https://arxiv.org/abs/2503.21888)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Effective mental health support is crucial for alleviating psychological distress. While large language model (LLM)-based assistants have shown promise in mental health interventions, existing research often defines "effective" support primarily in terms of empathetic acknowledgments, overlooking other essential dimensions such as informational guidance, community validation, and tangible coping strategies. To address this limitation and better understand what constitutes effective support, we introduce RedditESS, a novel real-world dataset derived from Reddit posts, including supportive comments and original posters' follow-up responses. Grounded in established social science theories, we develop an ensemble labeling mechanism to annotate supportive comments as effective or not and perform qualitative assessments to ensure the reliability of the annotations. Additionally, we demonstrate the practical utility of RedditESS by using it to guide LLM alignment toward generating more context-sensitive and genuinely helpful supportive responses. By broadening the understanding of effective support, our study paves the way for advanced AI-driven mental health interventions.</li>
</ul>

<h3>Title: StarFlow: Generating Structured Workflow Outputs From Sketch Images</h3>
<ul>
<li><strong>Authors: </strong>Patrice Bechard, Chao Wang, Amirhossein Abaskohi, Juan Rodriguez, Christopher Pal, David Vazquez, Spandana Gella, Sai Rajeswar, Perouz Taslakian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21889">https://arxiv.org/abs/2503.21889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21889">https://arxiv.org/pdf/2503.21889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21889]] StarFlow: Generating Structured Workflow Outputs From Sketch Images(https://arxiv.org/abs/2503.21889)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Workflows are a fundamental component of automation in enterprise platforms, enabling the orchestration of tasks, data processing, and system integrations. Despite being widely used, building workflows can be complex, often requiring manual configuration through low-code platforms or visual programming tools. To simplify this process, we explore the use of generative foundation models, particularly vision-language models (VLMs), to automatically generate structured workflows from visual inputs. Translating hand-drawn sketches or computer-generated diagrams into executable workflows is challenging due to the ambiguity of free-form drawings, variations in diagram styles, and the difficulty of inferring execution logic from visual elements. To address this, we introduce StarFlow, a framework for generating structured workflow outputs from sketches using vision-language models. We curate a diverse dataset of workflow diagrams -- including synthetic, manually annotated, and real-world samples -- to enable robust training and evaluation. We finetune and benchmark multiple vision-language models, conducting a series of ablation studies to analyze the strengths and limitations of our approach. Our results show that finetuning significantly enhances structured workflow generation, outperforming large vision-language models on this task.</li>
</ul>

<h3>Title: Poster Abstract: Time Attacks using Kernel Vulnerabilities</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Abdullah Soomro, Adeel Nasrullah, Fatima Muhammad Anwar</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21891">https://arxiv.org/abs/2503.21891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21891">https://arxiv.org/pdf/2503.21891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21891]] Poster Abstract: Time Attacks using Kernel Vulnerabilities(https://arxiv.org/abs/2503.21891)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Timekeeping is a fundamental component of modern computing; however, the security of system time remains an overlooked attack surface, leaving critical systems vulnerable to manipulation.</li>
</ul>

<h3>Title: AssistPDA: An Online Video Surveillance Assistant for Video Anomaly Prediction, Detection, and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Yang, Chen Gao, Jing Liu, Peng Wu, Guansong Pang, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21904">https://arxiv.org/abs/2503.21904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21904">https://arxiv.org/pdf/2503.21904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21904]] AssistPDA: An Online Video Surveillance Assistant for Video Anomaly Prediction, Detection, and Analysis(https://arxiv.org/abs/2503.21904)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancements in large language models (LLMs) have spurred growing interest in LLM-based video anomaly detection (VAD). However, existing approaches predominantly focus on video-level anomaly question answering or offline detection, ignoring the real-time nature essential for practical VAD applications. To bridge this gap and facilitate the practical deployment of LLM-based VAD, we introduce AssistPDA, the first online video anomaly surveillance assistant that unifies video anomaly prediction, detection, and analysis (VAPDA) within a single framework. AssistPDA enables real-time inference on streaming videos while supporting interactive user engagement. Notably, we introduce a novel event-level anomaly prediction task, enabling proactive anomaly forecasting before anomalies fully unfold. To enhance the ability to model intricate spatiotemporal relationships in anomaly events, we propose a Spatio-Temporal Relation Distillation (STRD) module. STRD transfers the long-term spatiotemporal modeling capabilities of vision-language models (VLMs) from offline settings to real-time scenarios. Thus it equips AssistPDA with a robust understanding of complex temporal dependencies and long-sequence memory. Additionally, we construct VAPDA-127K, the first large-scale benchmark designed for VLM-based online VAPDA. Extensive experiments demonstrate that AssistPDA outperforms existing offline VLM-based approaches, setting a new state-of-the-art for real-time VAPDA. Our dataset and code will be open-sourced to facilitate further research in the community.</li>
</ul>

<h3>Title: KernelFusion: Assumption-Free Blind Super-Resolution via Patch Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Oliver Heinimann, Assaf Shocher, Tal Zimbalist, Michal Irani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21907">https://arxiv.org/abs/2503.21907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21907">https://arxiv.org/pdf/2503.21907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21907]] KernelFusion: Assumption-Free Blind Super-Resolution via Patch Diffusion(https://arxiv.org/abs/2503.21907)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Traditional super-resolution (SR) methods assume an ``ideal'' downscaling SR-kernel (e.g., bicubic downscaling) between the high-resolution (HR) image and the low-resolution (LR) image. Such methods fail once the LR images are generated differently. Current blind-SR methods aim to remove this assumption, but are still fundamentally restricted to rather simplistic downscaling SR-kernels (e.g., anisotropic Gaussian kernels), and fail on more complex (out of distribution) downscaling degradations. However, using the correct SR-kernel is often more important than using a sophisticated SR algorithm. In ``KernelFusion'' we introduce a zero-shot diffusion-based method that makes no assumptions about the kernel. Our method recovers the unique image-specific SR-kernel directly from the LR input image, while simultaneously recovering its corresponding HR image. KernelFusion exploits the principle that the correct SR-kernel is the one that maximizes patch similarity across different scales of the LR image. We first train an image-specific patch-based diffusion model on the single LR input image, capturing its unique internal patch statistics. We then reconstruct a larger HR image with the same learned patch distribution, while simultaneously recovering the correct downscaling SR-kernel that maintains this cross-scale relation between the HR and LR images. Empirical results show that KernelFusion vastly outperforms all SR baselines on complex downscaling degradations, where existing SotA Blind-SR methods fail miserably. By breaking free from predefined kernel assumptions, KernelFusion pushes Blind-SR into a new assumption-free paradigm, handling downscaling kernels previously thought impossible.</li>
</ul>

<h3>Title: AutoPsyC: Automatic Recognition of Psychodynamic Conflicts from Semi-structured Interviews with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sayed Muddashir Hossain, Simon Ostermann, Patrick Gebhard, Cord Benecke, Josef van Genabith, Philipp Mller</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21911">https://arxiv.org/abs/2503.21911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21911">https://arxiv.org/pdf/2503.21911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21911]] AutoPsyC: Automatic Recognition of Psychodynamic Conflicts from Semi-structured Interviews with Large Language Models(https://arxiv.org/abs/2503.21911)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Psychodynamic conflicts are persistent, often unconscious themes that shape a person's behaviour and experiences. Accurate diagnosis of psychodynamic conflicts is crucial for effective patient treatment and is commonly done via long, manually scored semi-structured interviews. Existing automated solutions for psychiatric diagnosis tend to focus on the recognition of broad disorder categories such as depression, and it is unclear to what extent psychodynamic conflicts which even the patient themselves may not have conscious access to could be automatically recognised from conversation. In this paper, we propose AutoPsyC, the first method for recognising the presence and significance of psychodynamic conflicts from full-length Operationalized Psychodynamic Diagnostics (OPD) interviews using Large Language Models (LLMs). Our approach combines recent advances in parameter-efficient fine-tuning and Retrieval-Augmented Generation (RAG) with a summarisation strategy to effectively process entire 90 minute long conversations. In evaluations on a dataset of 141 diagnostic interviews we show that AutoPsyC consistently outperforms all baselines and ablation conditions on the recognition of four highly relevant psychodynamic conflicts.</li>
</ul>

<h3>Title: Hybrid Emotion Recognition: Enhancing Customer Interactions Through Acoustic and Textual Analysis</h3>
<ul>
<li><strong>Authors: </strong>Sahan Hewage Wewelwala, T.G.D.K. Sumanathilaka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21927">https://arxiv.org/abs/2503.21927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21927">https://arxiv.org/pdf/2503.21927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21927]] Hybrid Emotion Recognition: Enhancing Customer Interactions Through Acoustic and Textual Analysis(https://arxiv.org/abs/2503.21927)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This research presents a hybrid emotion recognition system integrating advanced Deep Learning, Natural Language Processing (NLP), and Large Language Models (LLMs) to analyze audio and textual data for enhancing customer interactions in contact centers. By combining acoustic features with textual sentiment analysis, the system achieves nuanced emotion detection, addressing the limitations of traditional approaches in understanding complex emotional states. Leveraging LSTM and CNN models for audio analysis and DistilBERT for textual evaluation, the methodology accommodates linguistic and cultural variations while ensuring real-time processing. Rigorous testing on diverse datasets demonstrates the system's robustness and accuracy, highlighting its potential to transform customer service by enabling personalized, empathetic interactions and improving operational efficiency. This research establishes a foundation for more intelligent and human-centric digital communication, redefining customer service standards.</li>
</ul>

<h3>Title: Local Normalization Distortion and the Thermodynamic Formalism of Decoding Strategies for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tom Kempton, Stuart Burrell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, math.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21929">https://arxiv.org/abs/2503.21929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21929">https://arxiv.org/pdf/2503.21929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21929]] Local Normalization Distortion and the Thermodynamic Formalism of Decoding Strategies for Large Language Models(https://arxiv.org/abs/2503.21929)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Advances in hardware and language model architecture have spurred a revolution in natural language generation. However, autoregressive models compute probability distributions over next-token choices, and sampling from these distributions, known as decoding, has received significantly less attention than other design choices. Existing decoding strategies are largely based on heuristics, resulting in methods that are hard to apply or improve in a principled manner. We develop the theory of decoding strategies for language models by expressing popular decoding algorithms as equilibrium states in the language of ergodic theory and stating the functions they optimize. Using this, we analyze the effect of the local normalization step of top-k, nucleus, and temperature sampling, used to make probabilities sum to one. We argue that local normalization distortion is a fundamental defect of decoding strategies and quantify the size of this distortion and its effect on mathematical proxies for the quality and diversity of generated text. Contrary to the prevailing explanation, we argue that the major cause of the under-performance of top-k sampling relative to nucleus sampling is local normalization distortion. This yields conclusions for the future design of decoding algorithms and the detection of machine-generated text.</li>
</ul>

<h3>Title: Multimodal Data Integration for Sustainable Indoor Gardening: Tracking Anyplant with Time Series Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Seyed Hamidreza Nabaei, Zeyang Zheng, Dong Chen, Arsalan Heydarian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CE, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21932">https://arxiv.org/abs/2503.21932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21932">https://arxiv.org/pdf/2503.21932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21932]] Multimodal Data Integration for Sustainable Indoor Gardening: Tracking Anyplant with Time Series Foundation Model(https://arxiv.org/abs/2503.21932)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Indoor gardening within sustainable buildings offers a transformative solution to urban food security and environmental sustainability. By 2030, urban farming, including Controlled Environment Agriculture (CEA) and vertical farming, is expected to grow at a compound annual growth rate (CAGR) of 13.2% from 2024 to 2030, according to market reports. This growth is fueled by advancements in Internet of Things (IoT) technologies, sustainable innovations such as smart growing systems, and the rising interest in green interior design. This paper presents a novel framework that integrates computer vision, machine learning (ML), and environmental sensing for the automated monitoring of plant health and growth. Unlike previous approaches, this framework combines RGB imagery, plant phenotyping data, and environmental factors such as temperature and humidity, to predict plant water stress in a controlled growth environment. The system utilizes high-resolution cameras to extract phenotypic features, such as RGB, plant area, height, and width while employing the Lag-Llama time series model to analyze and predict water stress. Experimental results demonstrate that integrating RGB, size ratios, and environmental data significantly enhances predictive accuracy, with the Fine-tuned model achieving the lowest errors (MSE = 0.420777, MAE = 0.595428) and reduced uncertainty. These findings highlight the potential of multimodal data and intelligent systems to automate plant care, optimize resource consumption, and align indoor gardening with sustainable building management practices, paving the way for resilient, green urban spaces.</li>
</ul>

<h3>Title: Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad</h3>
<ul>
<li><strong>Authors: </strong>Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunovi, Nikola Jovanovi, Martin Vechev</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21934">https://arxiv.org/abs/2503.21934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21934">https://arxiv.org/pdf/2503.21934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21934]] Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad(https://arxiv.org/abs/2503.21934)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent math benchmarks for large language models (LLMs) such as MathArena indicate that state-of-the-art reasoning models achieve impressive performance on mathematical competitions like AIME, with the leading model, o3-mini, achieving scores comparable to top human competitors. However, these benchmarks evaluate models solely based on final numerical answers, neglecting rigorous reasoning and proof generation which are essential for real-world mathematical tasks. To address this, we introduce the first comprehensive evaluation of full-solution reasoning for challenging mathematical problems. Using expert human annotators, we evaluated several state-of-the-art reasoning models on the six problems from the 2025 USAMO within hours of their release. Our results reveal that all tested models struggled significantly, achieving less than 5% on average. Through detailed analysis of reasoning traces, we identify the most common failure modes and find several unwanted artifacts arising from the optimization strategies employed during model training. Overall, our results suggest that current LLMs are inadequate for rigorous mathematical reasoning tasks, highlighting the need for substantial improvements in reasoning and proof generation capabilities.</li>
</ul>

<h3>Title: Flexible Moment-Invariant Bases from Irreducible Tensors</h3>
<ul>
<li><strong>Authors: </strong>Roxana Bujack, Emily Shinkle, Alice Allen, Tomas Suk, Nicholas Lubbers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21939">https://arxiv.org/abs/2503.21939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21939">https://arxiv.org/pdf/2503.21939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21939]] Flexible Moment-Invariant Bases from Irreducible Tensors(https://arxiv.org/abs/2503.21939)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Moment invariants are a powerful tool for the generation of rotation-invariant descriptors needed for many applications in pattern detection, classification, and machine learning. A set of invariants is optimal if it is complete, independent, and robust against degeneracy in the input. In this paper, we show that the current state of the art for the generation of these bases of moment invariants, despite being robust against moment tensors being identically zero, is vulnerable to a degeneracy that is common in real-world applications, namely spherical functions. We show how to overcome this vulnerability by combining two popular moment invariant approaches: one based on spherical harmonics and one based on Cartesian tensor algebra.</li>
</ul>

<h3>Title: Parametric Shadow Control for Portrait Generationin Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Haoming Cai, Tsung-Wei Huang, Shiv Gehlot, Brandon Y. Feng, Sachin Shah, Guan-Ming Su, Christopher Metzler</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21943">https://arxiv.org/abs/2503.21943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21943">https://arxiv.org/pdf/2503.21943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21943]] Parametric Shadow Control for Portrait Generationin Text-to-Image Diffusion Models(https://arxiv.org/abs/2503.21943)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models excel at generating diverse portraits, but lack intuitive shadow control. Existing editing approaches, as post-processing, struggle to offer effective manipulation across diverse styles. Additionally, these methods either rely on expensive real-world light-stage data collection or require extensive computational resources for training. To address these limitations, we introduce Shadow Director, a method that extracts and manipulates hidden shadow attributes within well-trained diffusion models. Our approach uses a small estimation network that requires only a few thousand synthetic images and hours of training-no costly real-world light-stage data needed. Shadow Director enables parametric and intuitive control over shadow shape, placement, and intensity during portrait generation while preserving artistic integrity and identity across diverse styles. Despite training only on synthetic data built on real-world identities, it generalizes effectively to generated portraits with diverse styles, making it a more accessible and resource-friendly solution.</li>
</ul>

<h3>Title: Entropy-Aware Branching for Improved Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xianzhi Li, Ethan Callanan, Xiaodan Zhu, Mathieu Sibue, Antony Papadimitriou, Mahmoud Mahfouz, Zhiqiang Ma, Xiaomo Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21961">https://arxiv.org/abs/2503.21961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21961">https://arxiv.org/pdf/2503.21961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21961]] Entropy-Aware Branching for Improved Mathematical Reasoning(https://arxiv.org/abs/2503.21961)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) are effectively aligned through extensive pre-training and fine-tuning, they still struggle with varying levels of uncertainty during token generation. In our investigation of mathematical reasoning, we observe that errors are more likely to arise at tokens exhibiting high entropy and variance of entropy in the model's output distribution. Based on the observation, we propose a novel approach that dynamically branches the generation process on demand instead of defaulting to the single most probable token. By exploring in parallel multiple branches stemming from high probability tokens of critical decision points, the model can discover diverse reasoning paths that might otherwise be missed. We further harness external feedback from larger models to rank and select the most coherent and accurate reasoning branch. Our experimental results on mathematical word problems and calculation questions show that this branching strategy boosts the reasoning capabilities of small LLMs up to 4.6% compared to conventional argmax decoding.</li>
</ul>

<h3>Title: NeuroLIP: Interpretable and Fair Cross-Modal Alignment of fMRI and Phenotypic Text</h3>
<ul>
<li><strong>Authors: </strong>Yanting Yang, Xiaoxiao Li</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21964">https://arxiv.org/abs/2503.21964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21964">https://arxiv.org/pdf/2503.21964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21964]] NeuroLIP: Interpretable and Fair Cross-Modal Alignment of fMRI and Phenotypic Text(https://arxiv.org/abs/2503.21964)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability</a></li>
<li><strong>Abstract: </strong>Integrating functional magnetic resonance imaging (fMRI) connectivity data with phenotypic textual descriptors (e.g., disease label, demographic data) holds significant potential to advance our understanding of neurological conditions. However, existing cross-modal alignment methods often lack interpretability and risk introducing biases by encoding sensitive attributes together with diagnostic-related features. In this work, we propose NeuroLIP, a novel cross-modal contrastive learning framework. We introduce text token-conditioned attention (TTCA) and cross-modal alignment via localized tokens (CALT) to the brain region-level embeddings with each disease-related phenotypic token. It improves interpretability via token-level attention maps, revealing brain region-disease associations. To mitigate bias, we propose a loss for sensitive attribute disentanglement that maximizes the attention distance between disease tokens and sensitive attribute tokens, reducing unintended correlations in downstream predictions. Additionally, we incorporate a negative gradient technique that reverses the sign of CALT loss on sensitive attributes, further discouraging the alignment of these features. Experiments on neuroimaging datasets (ABIDE and ADHD-200) demonstrate NeuroLIP's superiority in terms of fairness metrics while maintaining the overall best standard metric performance. Qualitative visualization of attention maps highlights neuroanatomical patterns aligned with diagnostic characteristics, validated by the neuroscientific literature. Our work advances the development of transparent and equitable neuroimaging AI.</li>
</ul>

<h3>Title: Q-MambaIR: Accurate Quantized Mamba for Efficient Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Yujie Chen, Haotong Qin, Zhang Zhang, Michelo Magno, Luca Benini, Yawei Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21970">https://arxiv.org/abs/2503.21970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21970">https://arxiv.org/pdf/2503.21970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21970]] Q-MambaIR: Accurate Quantized Mamba for Efficient Image Restoration(https://arxiv.org/abs/2503.21970)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>State-Space Models (SSMs) have attracted considerable attention in Image Restoration (IR) due to their ability to scale linearly sequence length while effectively capturing long-distance dependencies. However, deploying SSMs to edge devices is challenging due to the constraints in memory, computing capacity, and power consumption, underscoring the need for efficient compression strategies. While low-bit quantization is an efficient model compression strategy for reducing size and accelerating IR tasks, SSM suffers substantial performance drops at ultra-low bit-widths (2-4 bits), primarily due to outliers that exacerbate quantization error. To address this challenge, we propose Q-MambaIR, an accurate, efficient, and flexible Quantized Mamba for IR tasks. Specifically, we introduce a Statistical Dynamic-balancing Learnable Scalar (DLS) to dynamically adjust the quantization mapping range, thereby mitigating the peak truncation loss caused by extreme values. Furthermore, we design a Range-floating Flexible Allocator (RFA) with an adaptive threshold to flexibly round values. This approach preserves high-frequency details and maintains the SSM's feature extraction capability. Notably, RFA also enables pre-deployment weight quantization, striking a balance between computational efficiency and model accuracy. Extensive experiments on IR tasks demonstrate that Q-MambaIR consistently outperforms existing quantized SSMs, achieving much higher state-of-the-art (SOTA) accuracy results with only a negligible increase in training computation and storage saving.</li>
</ul>

<h3>Title: RocketPPA: Ultra-Fast LLM-Based PPA Estimator at Code-Level Abstraction</h3>
<ul>
<li><strong>Authors: </strong>Armin Abdollahi, Mehdi Kamal, Massoud Pedram</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21971">https://arxiv.org/abs/2503.21971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21971">https://arxiv.org/pdf/2503.21971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21971]] RocketPPA: Ultra-Fast LLM-Based PPA Estimator at Code-Level Abstraction(https://arxiv.org/abs/2503.21971)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have recently transformed hardware design, yet bridging the gap between code synthesis and PPA (power, performance, and area) estimation remains a challenge. In this work, we introduce a novel framework that leverages a 21k dataset of thoroughly cleaned and synthesizable Verilog modules, each annotated with detailed power, delay, and area metrics. By employing chain-of-thought techniques, we automatically debug and curate this dataset to ensure high fidelity in downstream applications. We then fine-tune CodeLlama using LoRA-based parameter-efficient methods, framing the task as a regression problem to accurately predict PPA metrics from Verilog code. Furthermore, we augment our approach with a mixture-of-experts architecture-integrating both LoRA and an additional MLP expert layer-to further refine predictions. Experimental results demonstrate significant improvements: power estimation accuracy is enhanced by 5.9% at a 20% error threshold and by 7.2% at a 10% threshold, delay estimation improves by 5.1% and 3.9%, and area estimation sees gains of 4% and 7.9% for the 20% and 10% thresholds, respectively. Notably, the incorporation of the mixture-of-experts module contributes an additional 3--4% improvement across these tasks. Our results establish a new benchmark for PPA-aware Verilog generation, highlighting the effectiveness of our integrated dataset and modeling strategies for next-generation EDA workflows.</li>
</ul>

<h3>Title: Improving Equivariant Networks with Probabilistic Symmetry Breaking</h3>
<ul>
<li><strong>Authors: </strong>Hannah Lawrence, Vasco Portilheiro, Yan Zhang, Skou-Oumar Kaba</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21985">https://arxiv.org/abs/2503.21985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21985">https://arxiv.org/pdf/2503.21985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21985]] Improving Equivariant Networks with Probabilistic Symmetry Breaking(https://arxiv.org/abs/2503.21985)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Equivariance encodes known symmetries into neural networks, often enhancing generalization. However, equivariant networks cannot break symmetries: the output of an equivariant network must, by definition, have at least the same self-symmetries as the input. This poses an important problem, both (1) for prediction tasks on domains where self-symmetries are common, and (2) for generative models, which must break symmetries in order to reconstruct from highly symmetric latent spaces. This fundamental limitation can be addressed by considering equivariant conditional distributions, instead of equivariant functions. We present novel theoretical results that establish necessary and sufficient conditions for representing such distributions. Concretely, this representation provides a practical framework for breaking symmetries in any equivariant network via randomized canonicalization. Our method, SymPE (Symmetry-breaking Positional Encodings), admits a simple interpretation in terms of positional encodings. This approach expands the representational power of equivariant networks while retaining the inductive bias of symmetry, which we justify through generalization bounds. Experimental results demonstrate that SymPE significantly improves performance of group-equivariant and graph neural networks across diffusion models for graphs, graph autoencoders, and lattice spin system modeling.</li>
</ul>

<h3>Title: BOOTPLACE: Bootstrapped Object Placement with Detection Transformers</h3>
<ul>
<li><strong>Authors: </strong>Hang Zhou, Xinxin Zuo, Rui Ma, Li Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21991">https://arxiv.org/abs/2503.21991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21991">https://arxiv.org/pdf/2503.21991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21991]] BOOTPLACE: Bootstrapped Object Placement with Detection Transformers(https://arxiv.org/abs/2503.21991)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we tackle the copy-paste image-to-image composition problem with a focus on object placement learning. Prior methods have leveraged generative models to reduce the reliance for dense supervision. However, this often limits their capacity to model complex data distributions. Alternatively, transformer networks with a sparse contrastive loss have been explored, but their over-relaxed regularization often leads to imprecise object placement. We introduce BOOTPLACE, a novel paradigm that formulates object placement as a placement-by-detection problem. Our approach begins by identifying suitable regions of interest for object placement. This is achieved by training a specialized detection transformer on object-subtracted backgrounds, enhanced with multi-object supervisions. It then semantically associates each target compositing object with detected regions based on their complementary characteristics. Through a boostrapped training approach applied to randomly object-subtracted images, our model enforces meaningful placements through extensive paired data augmentation. Experimental results on established benchmarks demonstrate BOOTPLACE's superior performance in object repositioning, markedly surpassing state-of-the-art baselines on Cityscapes and OPA datasets with notable improvements in IOU scores. Additional ablation studies further showcase the compositionality and generalizability of our approach, supported by user study evaluations.</li>
</ul>

<h3>Title: Monte Carlo Sampling for Analyzing In-Context Examples</h3>
<ul>
<li><strong>Authors: </strong>Stephanie Schoch, Yangfeng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22002">https://arxiv.org/abs/2503.22002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22002">https://arxiv.org/pdf/2503.22002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22002]] Monte Carlo Sampling for Analyzing In-Context Examples(https://arxiv.org/abs/2503.22002)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Prior works have shown that in-context learning is brittle to presentation factors such as the order, number, and choice of selected examples. However, ablation-based guidance on selecting the number of examples may ignore the interplay between different presentation factors. In this work we develop a Monte Carlo sampling-based method to study the impact of number of examples while explicitly accounting for effects from order and selected examples. We find that previous guidance on how many in-context examples to select does not always generalize across different sets of selected examples and orderings, and whether one-shot settings outperform zero-shot settings is highly dependent on the selected example. Additionally, inspired by data valuation, we apply our sampling method to in-context example selection to select examples that perform well across different orderings. We find a negative result, that while performance is robust to ordering and number of examples, there is an unexpected performance degradation compared to random sampling.</li>
</ul>

<h3>Title: Towards Privacy-Preserving Revocation of Verifiable Credentials with Time-Flexibility</h3>
<ul>
<li><strong>Authors: </strong>Francesco Buccafurri, Carmen Licciardi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22010">https://arxiv.org/abs/2503.22010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22010">https://arxiv.org/pdf/2503.22010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22010]] Towards Privacy-Preserving Revocation of Verifiable Credentials with Time-Flexibility(https://arxiv.org/abs/2503.22010)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Self-Sovereign Identity (SSI) is an emerging paradigm for authentication and credential presentation that aims to give users control over their data and prevent any kind of tracking by (even trusted) third parties. In the European Union, the EUDI Digital Identity wallet is about to become a concrete implementation of this paradigm. However, a debate is still ongoing, partially reflecting some aspects that are not yet consolidated in the scientific state of the art. Among these, an effective, efficient, and privacy-preserving implementation of verifiable credential revocation remains a subject of discussion. In this work-in-progress paper, we propose the basis of a novel method that customizes the use of anonymous hierarchical identity-based encryption to restrict the Verifier access to the temporal authorizations granted by the Holder. This way, the Issuer cannot track the Holder's credential presentations, and the Verifier cannot check revocation information beyond what is permitted by the Holder.</li>
</ul>

<h3>Title: AGILE: A Diffusion-Based Attention-Guided Image and Label Translation for Efficient Cross-Domain Plant Trait Identification</h3>
<ul>
<li><strong>Authors: </strong>Earl Ranario, Lars Lundqvist, Heesup Yun, Brian N. Bailey, J. Mason Earles</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22019">https://arxiv.org/abs/2503.22019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22019">https://arxiv.org/pdf/2503.22019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22019]] AGILE: A Diffusion-Based Attention-Guided Image and Label Translation for Efficient Cross-Domain Plant Trait Identification(https://arxiv.org/abs/2503.22019)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Semantically consistent cross-domain image translation facilitates the generation of training data by transferring labels across different domains, making it particularly useful for plant trait identification in agriculture. However, existing generative models struggle to maintain object-level accuracy when translating images between domains, especially when domain gaps are significant. In this work, we introduce AGILE (Attention-Guided Image and Label Translation for Efficient Cross-Domain Plant Trait Identification), a diffusion-based framework that leverages optimized text embeddings and attention guidance to semantically constrain image translation. AGILE utilizes pretrained diffusion models and publicly available agricultural datasets to improve the fidelity of translated images while preserving critical object semantics. Our approach optimizes text embeddings to strengthen the correspondence between source and target images and guides attention maps during the denoising process to control object placement. We evaluate AGILE on cross-domain plant datasets and demonstrate its effectiveness in generating semantically accurate translated images. Quantitative experiments show that AGILE enhances object detection performance in the target domain while maintaining realism and consistency. Compared to prior image translation methods, AGILE achieves superior semantic alignment, particularly in challenging cases where objects vary significantly or domain gaps are substantial.</li>
</ul>

<h3>Title: Cognitive Prompts Using Guilford's Structure of Intellect Model</h3>
<ul>
<li><strong>Authors: </strong>Oliver Kramer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22036">https://arxiv.org/abs/2503.22036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22036">https://arxiv.org/pdf/2503.22036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22036]] Cognitive Prompts Using Guilford's Structure of Intellect Model(https://arxiv.org/abs/2503.22036)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate strong language generation capabilities but often struggle with structured reasoning, leading to inconsistent or suboptimal problem-solving. To mitigate this limitation, Guilford's Structure of Intellect (SOI) model - a foundational framework from intelligence theory - is leveraged as the basis for cognitive prompt engineering. The SOI model categorizes cognitive operations such as pattern recognition, memory retrieval, and evaluation, offering a systematic approach to enhancing LLM reasoning and decision-making. This position paper presents a novel cognitive prompting approach for enforcing SOI-inspired reasoning for improving clarity, coherence, and adaptability in model responses.</li>
</ul>

<h3>Title: The Risks of Using Large Language Models for Text Annotation in Social Science Research</h3>
<ul>
<li><strong>Authors: </strong>Hao Lin, Yongjun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22040">https://arxiv.org/abs/2503.22040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22040">https://arxiv.org/pdf/2503.22040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22040]] The Risks of Using Large Language Models for Text Annotation in Social Science Research(https://arxiv.org/abs/2503.22040)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative artificial intelligence (GenAI) or large language models (LLMs) have the potential to revolutionize computational social science, particularly in automated textual analysis. In this paper, we conduct a systematic evaluation of the promises and risks of using LLMs for diverse coding tasks, with social movement studies serving as a case example. We propose a framework for social scientists to incorporate LLMs into text annotation, either as the primary coding decision-maker or as a coding assistant. This framework provides tools for researchers to develop the optimal prompt, and to examine and report the validity and reliability of LLMs as a methodological tool. Additionally, we discuss the associated epistemic risks related to validity, reliability, replicability, and transparency. We conclude with several practical guidelines for using LLMs in text annotation tasks, and how we can better communicate the epistemic risks in research.</li>
</ul>

<h3>Title: ThinkEdit: Interpretable Weight Editing to Mitigate Overly Short Thinking in Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Chung-En Sun, Ge Yan, Tsui-Wei Weng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22048">https://arxiv.org/abs/2503.22048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22048">https://arxiv.org/pdf/2503.22048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22048]] ThinkEdit: Interpretable Weight Editing to Mitigate Overly Short Thinking in Reasoning Models(https://arxiv.org/abs/2503.22048)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent studies have shown that Large Language Models (LLMs) augmented with chain-of-thought (CoT) reasoning demonstrate impressive problem-solving abilities. However, in this work, we identify a recurring issue where these models occasionally generate overly short reasoning, leading to degraded performance on even simple mathematical problems. Specifically, we investigate how reasoning length is embedded in the hidden representations of reasoning models and its impact on accuracy. Our analysis reveals that reasoning length is governed by a linear direction in the representation space, allowing us to induce overly short reasoning by steering the model along this direction. Building on this insight, we introduce ThinkEdit, a simple yet effective weight-editing approach to mitigate the issue of overly short reasoning. We first identify a small subset of attention heads (approximately 2%) that predominantly drive short reasoning behavior. We then edit the output projection weights of these heads to suppress the short reasoning direction. With changes to only 0.1% of the model's parameters, ThinkEdit effectively reduces overly short reasoning and yields notable accuracy gains for short reasoning outputs (+5.44%), along with an overall improvement across multiple math benchmarks (+2.43%). Our findings provide new mechanistic insights into how reasoning length is controlled within LLMs and highlight the potential of fine-grained model interventions to improve reasoning quality. Our code is available at this https URL</li>
</ul>

<h3>Title: A Deep Learning Framework for Boundary-Aware Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Tai An, Weiqiang Huang, Da Xu, Qingyuan He, Jiacheng Hu, Yujia Lou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22050">https://arxiv.org/abs/2503.22050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22050">https://arxiv.org/pdf/2503.22050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22050]] A Deep Learning Framework for Boundary-Aware Semantic Segmentation(https://arxiv.org/abs/2503.22050)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>As a fundamental task in computer vision, semantic segmentation is widely applied in fields such as autonomous driving, remote sensing image analysis, and medical image processing. In recent years, Transformer-based segmentation methods have demonstrated strong performance in global feature modeling. However, they still struggle with blurred target boundaries and insufficient recognition of small targets. To address these issues, this study proposes a Mask2Former-based semantic segmentation algorithm incorporating a boundary enhancement feature bridging module (BEFBM). The goal is to improve target boundary accuracy and segmentation consistency. Built upon the Mask2Former framework, this method constructs a boundary-aware feature map and introduces a feature bridging mechanism. This enables effective cross-scale feature fusion, enhancing the model's ability to focus on target boundaries. Experiments on the Cityscapes dataset demonstrate that, compared to mainstream segmentation methods, the proposed approach achieves significant improvements in metrics such as mIOU, mDICE, and mRecall. It also exhibits superior boundary retention in complex scenes. Visual analysis further confirms the model's advantages in fine-grained regions. Future research will focus on optimizing computational efficiency and exploring its potential in other high-precision segmentation tasks.</li>
</ul>

<h3>Title: Low Rank and Sparse Fourier Structure in Recurrent Networks Trained on Modular Addition</h3>
<ul>
<li><strong>Authors: </strong>Akshay Rangamani</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22059">https://arxiv.org/abs/2503.22059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22059">https://arxiv.org/pdf/2503.22059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22059]] Low Rank and Sparse Fourier Structure in Recurrent Networks Trained on Modular Addition(https://arxiv.org/abs/2503.22059)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Modular addition tasks serve as a useful test bed for observing empirical phenomena in deep learning, including the phenomenon of \emph{grokking}. Prior work has shown that one-layer transformer architectures learn Fourier Multiplication circuits to solve modular addition tasks. In this paper, we show that Recurrent Neural Networks (RNNs) trained on modular addition tasks also use a Fourier Multiplication strategy. We identify low rank structures in the model weights, and attribute model components to specific Fourier frequencies, resulting in a sparse representation in the Fourier space. We also show empirically that the RNN is robust to removing individual frequencies, while the performance degrades drastically as more frequencies are ablated from the model.</li>
</ul>

<h3>Title: Deep Depth Estimation from Thermal Image: Dataset, Benchmark, and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Ukcheol Shin, Jinsun Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22060">https://arxiv.org/abs/2503.22060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22060">https://arxiv.org/pdf/2503.22060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22060]] Deep Depth Estimation from Thermal Image: Dataset, Benchmark, and Challenges(https://arxiv.org/abs/2503.22060)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Achieving robust and accurate spatial perception under adverse weather and lighting conditions is crucial for the high-level autonomy of self-driving vehicles and robots. However, existing perception algorithms relying on the visible spectrum are highly affected by weather and lighting conditions. A long-wave infrared camera (i.e., thermal imaging camera) can be a potential solution to achieve high-level robustness. However, the absence of large-scale datasets and standardized benchmarks remains a significant bottleneck to progress in active research for robust visual perception from thermal images. To this end, this manuscript provides a large-scale Multi-Spectral Stereo (MS$^2$) dataset that consists of stereo RGB, stereo NIR, stereo thermal, stereo LiDAR data, and GNSS/IMU information along with semi-dense depth ground truth. MS$^2$ dataset includes 162K synchronized multi-modal data pairs captured across diverse locations (e.g., urban city, residential area, campus, and high-way road) at different times (e.g., morning, daytime, and nighttime) and under various weather conditions (e.g., clear-sky, cloudy, and rainy). Secondly, we conduct a thorough evaluation of monocular and stereo depth estimation networks across RGB, NIR, and thermal modalities to establish standardized benchmark results on MS$^2$ depth test sets (e.g., day, night, and rainy). Lastly, we provide in-depth analyses and discuss the challenges revealed by the benchmark results, such as the performance variability for each modality under adverse conditions, domain shift between different sensor modalities, and potential research direction for thermal perception. Our dataset and source code are publicly available at this https URL and this https URL.</li>
</ul>

<h3>Title: Arch-LLM: Taming LLMs for Neural Architecture Generation via Unsupervised Discrete Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Deshani Geethika Poddenige, Sachith Seneviratne, Damith Senanayake, Mahesan Niranjan, PN Suganthan, Saman Halgamuge</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22063">https://arxiv.org/abs/2503.22063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22063">https://arxiv.org/pdf/2503.22063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22063]] Arch-LLM: Taming LLMs for Neural Architecture Generation via Unsupervised Discrete Representation Learning(https://arxiv.org/abs/2503.22063)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Unsupervised representation learning has been widely explored across various modalities, including neural architectures, where it plays a key role in downstream applications like Neural Architecture Search (NAS). These methods typically learn an unsupervised representation space before generating/ sampling architectures for the downstream search. A common approach involves the use of Variational Autoencoders (VAEs) to map discrete architectures onto a continuous representation space, however, sampling from these spaces often leads to a high percentage of invalid or duplicate neural architectures. This could be due to the unnatural mapping of inherently discrete architectural space onto a continuous space, which emphasizes the need for a robust discrete representation of these architectures. To address this, we introduce a Vector Quantized Variational Autoencoder (VQ-VAE) to learn a discrete latent space more naturally aligned with the discrete neural architectures. In contrast to VAEs, VQ-VAEs (i) map each architecture into a discrete code sequence and (ii) allow the prior to be learned by any generative model rather than assuming a normal distribution. We then represent these architecture latent codes as numerical sequences and train a text-to-text model leveraging a Large Language Model to learn and generate sequences representing architectures. We experiment our method with Inception/ ResNet-like cell-based search spaces, namely NAS-Bench-101 and NAS-Bench-201. Compared to VAE-based methods, our approach improves the generation of valid and unique architectures by over 80% on NASBench-101 and over 8% on NASBench-201. Finally, we demonstrate the applicability of our method in NAS employing a sequence-modeling-based NAS algorithm.</li>
</ul>

<h3>Title: Federated Intrusion Detection System Based on Unsupervised Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Maxime Gourceyraud, Rim Ben Salem, Christopher Neal, Frdric Cuppens, Nora Boulahia Cuppens</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22065">https://arxiv.org/abs/2503.22065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22065">https://arxiv.org/pdf/2503.22065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22065]] Federated Intrusion Detection System Based on Unsupervised Machine Learning(https://arxiv.org/abs/2503.22065)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, federate</a></li>
<li><strong>Abstract: </strong>Recent Intrusion Detection System (IDS) research has increasingly moved towards the adoption of machine learning methods. However, most of these systems rely on supervised learning approaches, necessitating a fully labeled training set. In the realm of network intrusion detection, the requirement for extensive labeling can become impractically burdensome. Moreover, while IDS training could benefit from inter-company knowledge sharing, the sensitive nature of cybersecurity data often precludes such cooperation. To address these challenges, we propose an IDS architecture that utilizes unsupervised learning to reduce the need for labeling. We further facilitate collaborative learning through the implementation of a federated learning framework. To enhance privacy beyond what current federated clustering models offer, we introduce an innovative federated K-means++ initialization technique. Our findings indicate that transitioning from a centralized to a federated setup does not significantly diminish performance.</li>
</ul>

<h3>Title: Penrose Tiled Low-Rank Compression and Section-Wise Q&A Fine-Tuning: A General Framework for Domain-Specific Large Language Model Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Chuan-Wei Kuo, Siyu Chen, Chenqi Yan, Yu Yang Fredrik Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22074">https://arxiv.org/abs/2503.22074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22074">https://arxiv.org/pdf/2503.22074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22074]] Penrose Tiled Low-Rank Compression and Section-Wise Q&A Fine-Tuning: A General Framework for Domain-Specific Large Language Model Adaptation(https://arxiv.org/abs/2503.22074)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) hold great promise for specialized scientific domains such as materials science, yet adapting them efficiently and accurately to domain-specific knowledge remains challenging due to limited data and high knowledge density. We propose a two-stage framework that combines structured model compression with a scientific fine-tuning regimen to address this challenge. In the compression stage, we decompose the LLM's weight matrices into local low-rank "rank blocks" and arrange these blocks in a Penrose-like non-periodic tiling pattern. Each block is then compacted via spectral transformations (e.g., discrete cosine or Fourier transforms), and a Kullback-Leibler (KL) divergence-based alignment loss preserves the distributional similarity between the compressed model's representations and those of the original full model. In the adaptation stage, the compressed model is further tuned using a human-like scientific reading protocol: it processes technical materials science documents section by section, engaging in a structured question-and-answer routine for each section. This section-wise Q&A fine-tuning strategy extracts explicit reasoning traces and gradually injects domain knowledge, while minimizing catastrophic forgetting of the model's general language capabilities. By balancing efficient compression with targeted adaptation, our two-stage approach enables precise specialization of LLMs to high-value domains under data-scarce conditions. We present this principled yet exploratory pipeline and outline its potential for advancing materials science knowledge integration, laying the groundwork for comprehensive empirical evaluation in future work.</li>
</ul>

<h3>Title: Concise One-Layer Transformers Can Do Function Evaluation (Sometimes)</h3>
<ul>
<li><strong>Authors: </strong>Lena Strobl, Dana Angluin, Robert Frank</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22076">https://arxiv.org/abs/2503.22076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22076">https://arxiv.org/pdf/2503.22076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22076]] Concise One-Layer Transformers Can Do Function Evaluation (Sometimes)(https://arxiv.org/abs/2503.22076)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While transformers have proven enormously successful in a range of tasks, their fundamental properties as models of computation are not well understood. This paper contributes to the study of the expressive capacity of transformers, focusing on their ability to perform the fundamental computational task of evaluating an arbitrary function from $[n]$ to $[n]$ at a given argument. We prove that concise 1-layer transformers (i.e., with a polylog bound on the product of the number of heads, the embedding dimension, and precision) are capable of doing this task under some representations of the input, but not when the function's inputs and values are only encoded in different input positions. Concise 2-layer transformers can perform the task even with the more difficult input representation. Experimentally, we find a rough alignment between what we have proven can be computed by concise transformers and what can be practically learned.</li>
</ul>

<h3>Title: Leveraging LLMs for Predicting Unknown Diagnoses from Clinical Notes</h3>
<ul>
<li><strong>Authors: </strong>Dina Albassam, Adam Cross, Chengxiang Zhai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22092">https://arxiv.org/abs/2503.22092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22092">https://arxiv.org/pdf/2503.22092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22092]] Leveraging LLMs for Predicting Unknown Diagnoses from Clinical Notes(https://arxiv.org/abs/2503.22092)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Electronic Health Records (EHRs) often lack explicit links between medications and diagnoses, making clinical decision-making and research more difficult. Even when links exist, diagnosis lists may be incomplete, especially during early patient visits. Discharge summaries tend to provide more complete information, which can help infer accurate diagnoses, especially with the help of large language models (LLMs). This study investigates whether LLMs can predict implicitly mentioned diagnoses from clinical notes and link them to corresponding medications. We address two research questions: (1) Does majority voting across diverse LLM configurations outperform the best single configuration in diagnosis prediction? (2) How sensitive is majority voting accuracy to LLM hyperparameters such as temperature, top-p, and summary length? To evaluate, we created a new dataset of 240 expert-annotated medication-diagnosis pairs from 20 MIMIC-IV notes. Using GPT-3.5 Turbo, we ran 18 prompting configurations across short and long summary lengths, generating 8568 test cases. Results show that majority voting achieved 75 percent accuracy, outperforming the best single configuration at 66 percent. No single hyperparameter setting dominated, but combining deterministic, balanced, and exploratory strategies improved performance. Shorter summaries generally led to higher this http URL conclusion, ensemble-style majority voting with diverse LLM configurations improves diagnosis prediction in EHRs and offers a promising method to link medications and diagnoses in clinical texts.</li>
</ul>

<h3>Title: Few-Shot Graph Out-of-Distribution Detection with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Haoyan Xu, Zhengtao Yao, Yushun Dong, Ziyi Wang, Ryan A. Rossi, Mengyuan Li, Yue Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22097">https://arxiv.org/abs/2503.22097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22097">https://arxiv.org/pdf/2503.22097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22097]] Few-Shot Graph Out-of-Distribution Detection with LLMs(https://arxiv.org/abs/2503.22097)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing methods for graph out-of-distribution (OOD) detection typically depend on training graph neural network (GNN) classifiers using a substantial amount of labeled in-distribution (ID) data. However, acquiring high-quality labeled nodes in text-attributed graphs (TAGs) is challenging and costly due to their complex textual and structural characteristics. Large language models (LLMs), known for their powerful zero-shot capabilities in textual tasks, show promise but struggle to naturally capture the critical structural information inherent to TAGs, limiting their direct effectiveness. To address these challenges, we propose LLM-GOOD, a general framework that effectively combines the strengths of LLMs and GNNs to enhance data efficiency in graph OOD detection. Specifically, we first leverage LLMs' strong zero-shot capabilities to filter out likely OOD nodes, significantly reducing the human annotation burden. To minimize the usage and cost of the LLM, we employ it only to annotate a small subset of unlabeled nodes. We then train a lightweight GNN filter using these noisy labels, enabling efficient predictions of ID status for all other unlabeled nodes by leveraging both textual and structural information. After obtaining node embeddings from the GNN filter, we can apply informativeness-based methods to select the most valuable nodes for precise human annotation. Finally, we train the target ID classifier using these accurately annotated ID nodes. Extensive experiments on four real-world TAG datasets demonstrate that LLM-GOOD significantly reduces human annotation costs and outperforms state-of-the-art baselines in terms of both ID classification accuracy and OOD detection performance.</li>
</ul>

<h3>Title: Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks with Dialogues and Stories</h3>
<ul>
<li><strong>Authors: </strong>Yazhou Zhang, Qimeng Liu, Qiuchi Li, Peng Zhang, Jing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22115">https://arxiv.org/abs/2503.22115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22115">https://arxiv.org/pdf/2503.22115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22115]] Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks with Dialogues and Stories(https://arxiv.org/abs/2503.22115)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, steal, large language model</a></li>
<li><strong>Abstract: </strong>Evaluating the value alignment of large language models (LLMs) has traditionally relied on single-sentence adversarial prompts, which directly probe models with ethically sensitive or controversial questions. However, with the rapid advancements in AI safety techniques, models have become increasingly adept at circumventing these straightforward tests, limiting their effectiveness in revealing underlying biases and ethical stances. To address this limitation, we propose an upgraded value alignment benchmark that moves beyond single-sentence prompts by incorporating multi-turn dialogues and narrative-based scenarios. This approach enhances the stealth and adversarial nature of the evaluation, making it more robust against superficial safeguards implemented in modern LLMs. We design and implement a dataset that includes conversational traps and ethically ambiguous storytelling, systematically assessing LLMs' responses in more nuanced and context-rich settings. Experimental results demonstrate that this enhanced methodology can effectively expose latent biases that remain undetected in traditional single-shot evaluations. Our findings highlight the necessity of contextual and dynamic testing for value alignment in LLMs, paving the way for more sophisticated and realistic assessments of AI ethics and safety.</li>
</ul>

<h3>Title: Multimodal Machine Learning for Real Estate Appraisal: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Chenya Huang, Zhidong Li, Fang Chen, Bin Liang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22119">https://arxiv.org/abs/2503.22119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22119">https://arxiv.org/pdf/2503.22119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22119]] Multimodal Machine Learning for Real Estate Appraisal: A Comprehensive Survey(https://arxiv.org/abs/2503.22119)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Real estate appraisal has undergone a significant transition from manual to automated valuation and is entering a new phase of evolution. Leveraging comprehensive attention to various data sources, a novel approach to automated valuation, multimodal machine learning, has taken shape. This approach integrates multimodal data to deeply explore the diverse factors influencing housing prices. Furthermore, multimodal machine learning significantly outperforms single-modality or fewer-modality approaches in terms of prediction accuracy, with enhanced interpretability. However, systematic and comprehensive survey work on the application in the real estate domain is still lacking. In this survey, we aim to bridge this gap by reviewing the research efforts. We begin by reviewing the background of real estate appraisal and propose two research questions from the perspecve of performance and fusion aimed at improving the accuracy of appraisal results. Subsequently, we explain the concept of multimodal machine learning and provide a comprehensive classification and definition of modalities used in real estate appraisal for the first time. To ensure clarity, we explore works related to data and techniques, along with their evaluation methods, under the framework of these two research questions. Furthermore, specific application domains are summarized. Finally, we present insights into future research directions including multimodal complementarity, technology and modality contribution.</li>
</ul>

<h3>Title: Camera Model Identification with SPAIR-Swin and Entropy based Non-Homogeneous Patches</h3>
<ul>
<li><strong>Authors: </strong>Protyay Dey, Rejoy Chakraborty, Abhilasha S. Jadhav, Kapil Rana, Gaurav Sharma, Puneet Goyal</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22120">https://arxiv.org/abs/2503.22120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22120">https://arxiv.org/pdf/2503.22120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22120]] Camera Model Identification with SPAIR-Swin and Entropy based Non-Homogeneous Patches(https://arxiv.org/abs/2503.22120)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, transformer</a></li>
<li><strong>Abstract: </strong>Source camera model identification (SCMI) plays a pivotal role in image forensics with applications including authenticity verification and copyright protection. For identifying the camera model used to capture a given image, we propose SPAIR-Swin, a novel model combining a modified spatial attention mechanism and inverted residual block (SPAIR) with a Swin Transformer. SPAIR-Swin effectively captures both global and local features, enabling robust identification of artifacts such as noise patterns that are particularly effective for SCMI. Additionally, unlike conventional methods focusing on homogeneous patches, we propose a patch selection strategy for SCMI that emphasizes high-entropy regions rich in patterns and textures. Extensive evaluations on four benchmark SCMI datasets demonstrate that SPAIR-Swin outperforms existing methods, achieving patch-level accuracies of 99.45%, 98.39%, 99.45%, and 97.46% and image-level accuracies of 99.87%, 99.32%, 100%, and 98.61% on the Dresden, Vision, Forchheim, and Socrates datasets, respectively. Our findings highlight that high-entropy patches, which contain high-frequency information such as edge sharpness, noise, and compression artifacts, are more favorable in improving SCMI accuracy. Code will be made available upon request.</li>
</ul>

<h3>Title: Detecting Localized Deepfake Manipulations Using Action Unit-Guided Video Representations</h3>
<ul>
<li><strong>Authors: </strong>Tharun Anand, Siva Sankar, Pravin Nair</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22121">https://arxiv.org/abs/2503.22121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22121">https://arxiv.org/pdf/2503.22121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22121]] Detecting Localized Deepfake Manipulations Using Action Unit-Guided Video Representations(https://arxiv.org/abs/2503.22121)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, robust, generative</a></li>
<li><strong>Abstract: </strong>With rapid advancements in generative modeling, deepfake techniques are increasingly narrowing the gap between real and synthetic videos, raising serious privacy and security concerns. Beyond traditional face swapping and reenactment, an emerging trend in recent state-of-the-art deepfake generation methods involves localized edits such as subtle manipulations of specific facial features like raising eyebrows, altering eye shapes, or modifying mouth expressions. These fine-grained manipulations pose a significant challenge for existing detection models, which struggle to capture such localized variations. To the best of our knowledge, this work presents the first detection approach explicitly designed to generalize to localized edits in deepfake videos by leveraging spatiotemporal representations guided by facial action units. Our method leverages a cross-attention-based fusion of representations learned from pretext tasks like random masking and action unit detection, to create an embedding that effectively encodes subtle, localized changes. Comprehensive evaluations across multiple deepfake generation methods demonstrate that our approach, despite being trained solely on the traditional FF+ dataset, sets a new benchmark in detecting recent deepfake-generated videos with fine-grained local edits, achieving a $20\%$ improvement in accuracy over current state-of-the-art detection methods. Additionally, our method delivers competitive performance on standard datasets, highlighting its robustness and generalization across diverse types of local and global forgeries.</li>
</ul>

<h3>Title: Semantic segmentation for building houses from wooden cubes</h3>
<ul>
<li><strong>Authors: </strong>Ivan Beleacov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22125">https://arxiv.org/abs/2503.22125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22125">https://arxiv.org/pdf/2503.22125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22125]] Semantic segmentation for building houses from wooden cubes(https://arxiv.org/abs/2503.22125)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Automated construction is one of the most promising areas that can improve efficiency, reduce costs and minimize errors in the process of building construction. In this paper, a comparative analysis of three neural network models for semantic segmentation, U-Net(light), LinkNet and PSPNet, is performed. Two specialized datasets with images of houses built from wooden cubes were created for the experiments. The first dataset contains 4 classes (background, foundation, walls, roof ) and is designed for basic model evaluation, while the second dataset includes 44 classes where each cube is labeled as a separate object. The models were trained with the same hyperparameters and their accuracy was evaluated using MeanIoU and F1 Score metrics. According to the results obtained, U-Net(light) showed the best performance with 78% MeanIoU and 87% F1 Score on the first dataset and 17% and 25% respectively on the second dataset. The poor results on the second dataset are due to the limited amount of data, the complexity of the partitioning and the imbalance of classes, making it difficult to accurately select individual cubes. In addition, overtraining was observed in all experiments, manifested by high accuracy on the training dataset and its significant decrease on the validation dataset. The present work is the basis for the development of algorithms for automatic generation of staged building plans, which can be further scaled to design complete buildings. Future research is planned to extend the datasets and apply methods to combat overfitting (L1/L2 regularization, Early Stopping). The next stage of work will be the development of algorithms for automatic generation of a step-by-step plan for building houses from cubes using manipulators. Index Terms-Deep Learning, Computer vision, CNN, Semantic segmentation, Construction materials.</li>
</ul>

<h3>Title: Beyond Background Shift: Rethinking Instance Replay in Continual Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hongmei Yin, Tingliang Feng, Fan Lyu, Fanhua Shang, Hongying Liu, Wei Feng, Liang Wan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22136">https://arxiv.org/abs/2503.22136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22136">https://arxiv.org/pdf/2503.22136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22136]] Beyond Background Shift: Rethinking Instance Replay in Continual Semantic Segmentation(https://arxiv.org/abs/2503.22136)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this work, we focus on continual semantic segmentation (CSS), where segmentation networks are required to continuously learn new classes without erasing knowledge of previously learned ones. Although storing images of old classes and directly incorporating them into the training of new models has proven effective in mitigating catastrophic forgetting in classification tasks, this strategy presents notable limitations in CSS. Specifically, the stored and new images with partial category annotations leads to confusion between unannotated categories and the background, complicating model fitting. To tackle this issue, this paper proposes a novel Enhanced Instance Replay (EIR) method, which not only preserves knowledge of old classes while simultaneously eliminating background confusion by instance storage of old classes, but also mitigates background shifts in the new images by integrating stored instances with new images. By effectively resolving background shifts in both stored and new images, EIR alleviates catastrophic forgetting in the CSS task, thereby enhancing the model's capacity for CSS. Experimental results validate the efficacy of our approach, which significantly outperforms state-of-the-art CSS methods.</li>
</ul>

<h3>Title: FRASE: Structured Representations for Generalizable SPARQL Query Generation</h3>
<ul>
<li><strong>Authors: </strong>Papa Abdou Karim Karou Diallo, Amal Zouaq</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22144">https://arxiv.org/abs/2503.22144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22144">https://arxiv.org/pdf/2503.22144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22144]] FRASE: Structured Representations for Generalizable SPARQL Query Generation(https://arxiv.org/abs/2503.22144)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Translating natural language questions into SPARQL queries enables Knowledge Base querying for factual and up-to-date responses. However, existing datasets for this task are predominantly template-based, leading models to learn superficial mappings between question and query templates rather than developing true generalization capabilities. As a result, models struggle when encountering naturally phrased, template-free questions. This paper introduces FRASE (FRAme-based Semantic Enhancement), a novel approach that leverages Frame Semantic Role Labeling (FSRL) to address this limitation. We also present LC-QuAD 3.0, a new dataset derived from LC-QuAD 2.0, in which each question is enriched using FRASE through frame detection and the mapping of frame-elements to their argument. We evaluate the impact of this approach through extensive experiments on recent large language models (LLMs) under different fine-tuning configurations. Our results demonstrate that integrating frame-based structured representations consistently improves SPARQL generation performance, particularly in challenging generalization scenarios when test questions feature unseen templates (unknown template splits) and when they are all naturally phrased (reformulated questions).</li>
</ul>

<h3>Title: Tokenization of Gaze Data</h3>
<ul>
<li><strong>Authors: </strong>Tim Rolff, Jurik Karimian, Niklas Hypki, Susanne Schmidt, Markus Lappe, Frank Steinicke</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22145">https://arxiv.org/abs/2503.22145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22145">https://arxiv.org/pdf/2503.22145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22145]] Tokenization of Gaze Data(https://arxiv.org/abs/2503.22145)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>A considerable part of the performance of today's large language models (LLM's) and multimodal large language models (MLLM's) depends on their tokenization strategies. While tokenizers are extensively researched for textual and visual input, there is no research on tokenization strategies for gaze data due to its nature. However, a corresponding tokenization strategy would allow using the vision capabilities of pre-trained MLLM's for gaze data, for example, through fine-tuning. In this paper, we aim to close this research gap by analyzing five different tokenizers for gaze data on three different datasets for the forecasting and generation of gaze data through LLMs (cf.~\cref{fig:teaser}). We evaluate the tokenizers regarding their reconstruction and compression abilities. Further, we train an LLM for each tokenization strategy, measuring its generative and predictive performance. Overall, we found that a quantile tokenizer outperforms all others in predicting the gaze positions and k-means is best when predicting gaze velocities.</li>
</ul>

<h3>Title: EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Li, Vijay Veerabadran, Michael L. Iuzzolino, Brett D. Roads, Asli Celikyilmaz, Karl Ridgeway</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22152">https://arxiv.org/abs/2503.22152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22152">https://arxiv.org/pdf/2503.22152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22152]] EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos(https://arxiv.org/abs/2503.22152)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce EgoToM, a new video question-answering benchmark that extends Theory-of-Mind (ToM) evaluation to egocentric domains. Using a causal ToM model, we generate multi-choice video QA instances for the Ego4D dataset to benchmark the ability to predict a camera wearer's goals, beliefs, and next actions. We study the performance of both humans and state of the art multimodal large language models (MLLMs) on these three interconnected inference problems. Our evaluation shows that MLLMs achieve close to human-level accuracy on inferring goals from egocentric videos. However, MLLMs (including the largest ones we tested with over 100B parameters) fall short of human performance when inferring the camera wearers' in-the-moment belief states and future actions that are most consistent with the unseen video future. We believe that our results will shape the future design of an important class of egocentric digital assistants which are equipped with a reasonable model of the user's internal mental states.</li>
</ul>

<h3>Title: SoK: Security Analysis of Blockchain-based Cryptocurrency</h3>
<ul>
<li><strong>Authors: </strong>Zekai Liu, Xiaoqi Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22156">https://arxiv.org/abs/2503.22156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22156">https://arxiv.org/pdf/2503.22156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22156]] SoK: Security Analysis of Blockchain-based Cryptocurrency(https://arxiv.org/abs/2503.22156)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, attack</a></li>
<li><strong>Abstract: </strong>Cryptocurrency is a novel exploration of a form of currency that proposes a decentralized electronic payment scheme based on blockchain technology and cryptographic theory. While cryptocurrency has the security characteristics of being distributed and tamper-proof, increasing market demand has led to a rise in malicious transactions and attacks, thereby exposing cryptocurrency to vulnerabilities, privacy issues, and security threats. Particularly concerning are the emerging types of attacks and threats, which have made securing cryptocurrency increasingly urgent. Therefore, this paper classifies existing cryptocurrency security threats and attacks into five fundamental categories based on the blockchain infrastructure and analyzes in detail the vulnerability principles exploited by each type of threat and attack. Additionally, the paper examines the attackers' logic and methods and successfully reproduces the vulnerabilities. Furthermore, the author summarizes the existing detection and defense solutions and evaluates them, all of which provide important references for ensuring the security of cryptocurrency. Finally, the paper discusses the future development trends of cryptocurrency, as well as the public challenges it may face.</li>
</ul>

<h3>Title: Traffic Modeling for Network Security and Privacy: Challenges Ahead</h3>
<ul>
<li><strong>Authors: </strong>Dinil Mon Divakaran</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22161">https://arxiv.org/abs/2503.22161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22161">https://arxiv.org/pdf/2503.22161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22161]] Traffic Modeling for Network Security and Privacy: Challenges Ahead(https://arxiv.org/abs/2503.22161)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>Traffic analysis using machine learning and deep learning models has made significant progress over the past decades. These models address various tasks in network security and privacy, including detection of anomalies and attacks, countering censorship, etc. They also reveal privacy risks to users as demonstrated by the research on LLM token inference as well as fingerprinting (and counter-fingerprinting) of user-visiting websites, IoT devices, and different applications. However, challenges remain in securing our networks from threats and attacks. After briefly reviewing the tasks and recent ML models in network security and privacy, we discuss the challenges that lie ahead.</li>
</ul>

<h3>Title: Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhanke Zhou, Zhaocheng Zhu, Xuan Li, Mikhail Galkin, Xiao Feng, Sanmi Koyejo, Jian Tang, Bo Han</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22165">https://arxiv.org/abs/2503.22165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22165">https://arxiv.org/pdf/2503.22165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22165]] Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models(https://arxiv.org/abs/2503.22165)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Numerous applications of large language models (LLMs) rely on their ability to perform step-by-step reasoning. However, the reasoning behavior of LLMs remains poorly understood, posing challenges to research, development, and safety. To address this gap, we introduce landscape of thoughts-the first visualization tool for users to inspect the reasoning paths of chain-of-thought and its derivatives on any multi-choice dataset. Specifically, we represent the states in a reasoning path as feature vectors that quantify their distances to all answer choices. These features are then visualized in two-dimensional plots using t-SNE. Qualitative and quantitative analysis with the landscape of thoughts effectively distinguishes between strong and weak models, correct and incorrect answers, as well as different reasoning tasks. It also uncovers undesirable reasoning patterns, such as low consistency and high uncertainty. Additionally, users can adapt our tool to a model that predicts the property they observe. We showcase this advantage by adapting our tool to a lightweight verifier that evaluates the correctness of reasoning paths. The code is publicly available at: this https URL.</li>
</ul>

<h3>Title: Reasoning of Large Language Models over Knowledge Graphs with Super-Relations</h3>
<ul>
<li><strong>Authors: </strong>Song Wang, Junhong Lin, Xiaojie Guo, Julian Shun, Jundong Li, Yada Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22166">https://arxiv.org/abs/2503.22166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22166">https://arxiv.org/pdf/2503.22166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22166]] Reasoning of Large Language Models over Knowledge Graphs with Super-Relations(https://arxiv.org/abs/2503.22166)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have made significant progress in processing and reasoning over knowledge graphs, current methods suffer from a high non-retrieval rate. This limitation reduces the accuracy of answering questions based on these graphs. Our analysis reveals that the combination of greedy search and forward reasoning is a major contributor to this issue. To overcome these challenges, we introduce the concept of super-relations, which enables both forward and backward reasoning by summarizing and connecting various relational paths within the graph. This holistic approach not only expands the search space, but also significantly improves retrieval efficiency. In this paper, we propose the ReKnoS framework, which aims to Reason over Knowledge Graphs with Super-Relations. Our framework's key advantages include the inclusion of multiple relation paths through super-relations, enhanced forward and backward reasoning capabilities, and increased efficiency in querying LLMs. These enhancements collectively lead to a substantial improvement in the successful retrieval rate and overall reasoning performance. We conduct extensive experiments on nine real-world datasets to evaluate ReKnoS, and the results demonstrate the superior performance of ReKnoS over existing state-of-the-art baselines, with an average accuracy gain of 2.92%.</li>
</ul>

<h3>Title: Spatial Transport Optimization by Repositioning Attention Map for Training-Free Text-to-Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Woojung Han, Yeonkyung Lee, Chanyoung Kim, Kwanghyun Park, Seong Jae Hwang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22168">https://arxiv.org/abs/2503.22168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22168">https://arxiv.org/pdf/2503.22168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22168]] Spatial Transport Optimization by Repositioning Attention Map for Training-Free Text-to-Image Synthesis(https://arxiv.org/abs/2503.22168)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based text-to-image (T2I) models have recently excelled in high-quality image generation, particularly in a training-free manner, enabling cost-effective adaptability and generalization across diverse tasks. However, while the existing methods have been continuously focusing on several challenges, such as "missing objects" and "mismatched attributes," another critical issue of "mislocated objects" remains where generated spatial positions fail to align with text prompts. Surprisingly, ensuring such seemingly basic functionality remains challenging in popular T2I models due to the inherent difficulty of imposing explicit spatial guidance via text forms. To address this, we propose STORM (Spatial Transport Optimization by Repositioning Attention Map), a novel training-free approach for spatially coherent T2I synthesis. STORM employs Spatial Transport Optimization (STO), rooted in optimal transport theory, to dynamically adjust object attention maps for precise spatial adherence, supported by a Spatial Transport (ST) Cost function that enhances spatial understanding. Our analysis shows that integrating spatial awareness is most effective in the early denoising stages, while later phases refine details. Extensive experiments demonstrate that STORM surpasses existing methods, effectively mitigating mislocated objects while improving missing and mismatched attributes, setting a new benchmark for spatial alignment in T2I synthesis.</li>
</ul>

<h3>Title: An Empirical Study of Validating Synthetic Data for Text-Based Person Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Min Cao, ZiYin Zeng, YuXin Lu, Mang Ye, Dong Yi, Jinqiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22171">https://arxiv.org/abs/2503.22171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22171">https://arxiv.org/pdf/2503.22171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22171]] An Empirical Study of Validating Synthetic Data for Text-Based Person Retrieval(https://arxiv.org/abs/2503.22171)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, generative</a></li>
<li><strong>Abstract: </strong>Data plays a pivotal role in Text-Based Person Retrieval (TBPR) research. Mainstream research paradigm necessitates real-world person images with manual textual annotations for training models, posing privacy-sensitive and labor-intensive issues. Several pioneering efforts explore synthetic data for TBPR but still rely on real data, keeping the aforementioned issues and also resulting in diversity-deficient issue in synthetic datasets, thus impacting TBPR performance. Moreover, these works tend to explore synthetic data for TBPR through limited perspectives, leading to exploration-restricted issue. In this paper, we conduct an empirical study to explore the potential of synthetic data for TBPR, highlighting three key aspects. (1) We propose an inter-class image generation pipeline, in which an automatic prompt construction strategy is introduced to guide generative Artificial Intelligence (AI) models in generating various inter-class images without reliance on original data. (2) We develop an intra-class image augmentation pipeline, in which the generative AI models are applied to further edit the images for obtaining various intra-class images. (3) Building upon the proposed pipelines and an automatic text generation pipeline, we explore the effectiveness of synthetic data in diverse scenarios through extensive experiments. Additionally, we experimentally investigate various noise-robust learning strategies to mitigate the inherent noise in synthetic data. We will release the code, along with the synthetic large-scale dataset generated by our pipelines, which are expected to advance practical TBPR research.</li>
</ul>

<h3>Title: Concept-Aware LoRA for Domain-Aligned Segmentation Dataset Generation</h3>
<ul>
<li><strong>Authors: </strong>Minho Park, Sunghyun Park, Jungsoo Lee, Hyojin Park, Kyuwoong Hwang, Fatih Porikli, Jaegul Choo, Sungha Choi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22172">https://arxiv.org/abs/2503.22172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22172">https://arxiv.org/pdf/2503.22172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22172]] Concept-Aware LoRA for Domain-Aligned Segmentation Dataset Generation(https://arxiv.org/abs/2503.22172)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper addresses the challenge of data scarcity in semantic segmentation by generating datasets through text-to-image (T2I) generation models, reducing image acquisition and labeling costs. Segmentation dataset generation faces two key challenges: 1) aligning generated samples with the target domain and 2) producing informative samples beyond the training data. Fine-tuning T2I models can help generate samples aligned with the target domain. However, it often overfits and memorizes training data, limiting their ability to generate diverse and well-aligned samples. To overcome these issues, we propose Concept-Aware LoRA (CA-LoRA), a novel fine-tuning approach that selectively identifies and updates only the weights associated with necessary concepts (e.g., style or viewpoint) for domain alignment while preserving the pretrained knowledge of the T2I model to produce informative samples. We demonstrate its effectiveness in generating datasets for urban-scene segmentation, outperforming baseline and state-of-the-art methods in in-domain (few-shot and fully-supervised) settings, as well as in domain generalization tasks, especially under challenging conditions such as adverse weather and varying illumination, further highlighting its superiority.</li>
</ul>

<h3>Title: High-Fidelity Diffusion Face Swapping with ID-Constrained Facial Conditioning</h3>
<ul>
<li><strong>Authors: </strong>Dailan He, Xiahong Wang, Shulun Wang, Guanglu Song, Bingqi Ma, Hao Shao, Yu Liu, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22179">https://arxiv.org/abs/2503.22179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22179">https://arxiv.org/pdf/2503.22179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22179]] High-Fidelity Diffusion Face Swapping with ID-Constrained Facial Conditioning(https://arxiv.org/abs/2503.22179)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Face swapping aims to seamlessly transfer a source facial identity onto a target while preserving target attributes such as pose and expression. Diffusion models, known for their superior generative capabilities, have recently shown promise in advancing face-swapping quality. This paper addresses two key challenges in diffusion-based face swapping: the prioritized preservation of identity over target attributes and the inherent conflict between identity and attribute conditioning. To tackle these issues, we introduce an identity-constrained attribute-tuning framework for face swapping that first ensures identity preservation and then fine-tunes for attribute alignment, achieved through a decoupled condition injection. We further enhance fidelity by incorporating identity and adversarial losses in a post-training refinement stage. Our proposed identity-constrained diffusion-based face-swapping model outperforms existing methods in both qualitative and quantitative evaluations, demonstrating superior identity similarity and attribute consistency, achieving a new state-of-the-art performance in high-fidelity face swapping.</li>
</ul>

<h3>Title: Unbiased Max-Min Embedding Classification for Transductive Few-Shot Learning: Clustering and Classification Are All You Need</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Feixiang Liu, Jiale Du, Xinbo Gao, Jungong Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22193">https://arxiv.org/abs/2503.22193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22193">https://arxiv.org/pdf/2503.22193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22193]] Unbiased Max-Min Embedding Classification for Transductive Few-Shot Learning: Clustering and Classification Are All You Need(https://arxiv.org/abs/2503.22193)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Convolutional neural networks and supervised learning have achieved remarkable success in various fields but are limited by the need for large annotated datasets. Few-shot learning (FSL) addresses this limitation by enabling models to generalize from only a few labeled examples. Transductive few-shot learning (TFSL) enhances FSL by leveraging both labeled and unlabeled data, though it faces challenges like the hubness problem. To overcome these limitations, we propose the Unbiased Max-Min Embedding Classification (UMMEC) Method, which addresses the key challenges in few-shot learning through three innovative contributions. First, we introduce a decentralized covariance matrix to mitigate the hubness problem, ensuring a more uniform distribution of embeddings. Second, our method combines local alignment and global uniformity through adaptive weighting and nonlinear transformation, balancing intra-class clustering with inter-class separation. Third, we employ a Variational Sinkhorn Few-Shot Classifier to optimize the distances between samples and class prototypes, enhancing classification accuracy and robustness. These combined innovations allow the UMMEC method to achieve superior performance with minimal labeled data. Our UMMEC method significantly improves classification performance with minimal labeled data, advancing the state-of-the-art in TFSL.</li>
</ul>

<h3>Title: ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yunhong Min, Daehyeon Choi, Kyeongmin Yeo, Jihyun Lee, Minhyuk Sung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22194">https://arxiv.org/abs/2503.22194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22194">https://arxiv.org/pdf/2503.22194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22194]] ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation(https://arxiv.org/abs/2503.22194)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce ORIGEN, the first zero-shot method for 3D orientation grounding in text-to-image generation across multiple objects and diverse categories. While previous work on spatial grounding in image generation has mainly focused on 2D positioning, it lacks control over 3D orientation. To address this, we propose a reward-guided sampling approach using a pretrained discriminative model for 3D orientation estimation and a one-step text-to-image generative flow model. While gradient-ascent-based optimization is a natural choice for reward-based guidance, it struggles to maintain image realism. Instead, we adopt a sampling-based approach using Langevin dynamics, which extends gradient ascent by simply injecting random noise--requiring just a single additional line of code. Additionally, we introduce adaptive time rescaling based on the reward function to accelerate convergence. Our experiments show that ORIGEN outperforms both training-based and test-time guidance methods across quantitative metrics and user studies.</li>
</ul>

<h3>Title: EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Jiyu Chen, Shuang Peng, Daxiong Luo, Fan Yang, Renshou Wu, Fangyuan Li, Xiaoxin Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22196">https://arxiv.org/abs/2503.22196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22196">https://arxiv.org/pdf/2503.22196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22196]] EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge Devices(https://arxiv.org/abs/2503.22196)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based large language models (LLMs) encounter challenges in processing long sequences on edge devices due to the quadratic complexity of attention mechanisms and growing memory demands from Key-Value (KV) cache. Existing KV cache optimizations struggle with irreversible token eviction in long-output tasks, while alternative sequence modeling architectures prove costly to adopt within established Transformer infrastructure. We present EdgeInfinite, a memory-efficient solution for infinite contexts that integrates compressed memory into Transformer-based LLMs through a trainable memory-gating module. This approach maintains full compatibility with standard Transformer architectures, requiring fine-tuning only a small part of parameters, and enables selective activation of the memory-gating module for long and short context task routing. The experimental result shows that EdgeInfinite achieves comparable performance to baseline Transformer-based LLM on long context benchmarks while optimizing memory consumption and time to first token.</li>
</ul>

<h3>Title: Extremely Simple Out-of-distribution Detection for Audio-visual Generalized Zero-shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Xun Zhang, Jiale Du, Xinbo Gao, Jungong Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22197">https://arxiv.org/abs/2503.22197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22197">https://arxiv.org/pdf/2503.22197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22197]] Extremely Simple Out-of-distribution Detection for Audio-visual Generalized Zero-shot Learning(https://arxiv.org/abs/2503.22197)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Zero-shot Learning(ZSL) attains knowledge transfer from seen classes to unseen classes by exploring auxiliary category information, which is a promising yet difficult research topic. In this field, Audio-Visual Generalized Zero-Shot Learning~(AV-GZSL) has aroused researchers' great interest in which intricate relations within triple modalities~(audio, video, and natural language) render this task quite challenging but highly research-worthy. However, both existing embedding-based and generative-based AV-GZSL methods tend to suffer from domain shift problem a lot and we propose an extremely simple Out-of-distribution~(OOD) detection based AV-GZSL method~(EZ-AVOOD) to further mitigate bias problem by differentiating seen and unseen samples at the initial beginning. EZ-AVOOD accomplishes effective seen-unseen separation by exploiting the intrinsic discriminative information held in class-specific logits and class-agnostic feature subspace without training an extra OOD detector network. Followed by seen-unseen binary classification, we employ two expert models to classify seen samples and unseen samples separately. Compared to existing state-of-the-art methods, our model achieves superior ZSL and GZSL performances on three audio-visual datasets and becomes the new SOTA, which comprehensively demonstrates the effectiveness of the proposed EZ-AVOOD.</li>
</ul>

<h3>Title: Multi-modal Knowledge Distillation-based Human Trajectory Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Jaewoo Jeong, Seohee Lee, Daehee Park, Giwon Lee, Kuk-Jin Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22201">https://arxiv.org/abs/2503.22201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22201">https://arxiv.org/pdf/2503.22201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22201]] Multi-modal Knowledge Distillation-based Human Trajectory Forecasting(https://arxiv.org/abs/2503.22201)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Pedestrian trajectory forecasting is crucial in various applications such as autonomous driving and mobile robot navigation. In such applications, camera-based perception enables the extraction of additional modalities (human pose, text) to enhance prediction accuracy. Indeed, we find that textual descriptions play a crucial role in integrating additional modalities into a unified understanding. However, online extraction of text requires the use of VLM, which may not be feasible for resource-constrained systems. To address this challenge, we propose a multi-modal knowledge distillation framework: a student model with limited modality is distilled from a teacher model trained with full range of modalities. The comprehensive knowledge of a teacher model trained with trajectory, human pose, and text is distilled into a student model using only trajectory or human pose as a sole supplement. In doing so, we separately distill the core locomotion insights from intra-agent multi-modality and inter-agent interaction. Our generalizable framework is validated with two state-of-the-art models across three datasets on both ego-view (JRDB, SIT) and BEV-view (ETH/UCY) setups, utilizing both annotated and VLM-generated text captions. Distilled student models show consistent improvement in all prediction metrics for both full and instantaneous observations, improving up to ~13%. The code is available at this https URL.</li>
</ul>

<h3>Title: Segment then Splat: A Unified Approach for 3D Open-Vocabulary Segmentation based on Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Yiren Lu, Yunlai Zhou, Yiran Qiao, Chaoda Song, Tuo Liang, Jing Ma, Yu Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22204">https://arxiv.org/abs/2503.22204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22204">https://arxiv.org/pdf/2503.22204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22204]] Segment then Splat: A Unified Approach for 3D Open-Vocabulary Segmentation based on Gaussian Splatting(https://arxiv.org/abs/2503.22204)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Open-vocabulary querying in 3D space is crucial for enabling more intelligent perception in applications such as robotics, autonomous systems, and augmented reality. However, most existing methods rely on 2D pixel-level parsing, leading to multi-view inconsistencies and poor 3D object retrieval. Moreover, they are limited to static scenes and struggle with dynamic scenes due to the complexities of motion modeling. In this paper, we propose Segment then Splat, a 3D-aware open vocabulary segmentation approach for both static and dynamic scenes based on Gaussian Splatting. Segment then Splat reverses the long established approach of "segmentation after reconstruction" by dividing Gaussians into distinct object sets before reconstruction. Once the reconstruction is complete, the scene is naturally segmented into individual objects, achieving true 3D segmentation. This approach not only eliminates Gaussian-object misalignment issues in dynamic scenes but also accelerates the optimization process, as it eliminates the need for learning a separate language field. After optimization, a CLIP embedding is assigned to each object to enable open-vocabulary querying. Extensive experiments on various datasets demonstrate the effectiveness of our proposed method in both static and dynamic scenarios.</li>
</ul>

<h3>Title: Data-Free Universal Attack by Exploiting the Intrinsic Vulnerability of Deep Models</h3>
<ul>
<li><strong>Authors: </strong>YangTian Yan, Jinyu Tian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22205">https://arxiv.org/abs/2503.22205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22205">https://arxiv.org/pdf/2503.22205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22205]] Data-Free Universal Attack by Exploiting the Intrinsic Vulnerability of Deep Models(https://arxiv.org/abs/2503.22205)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, data-free</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) are susceptible to Universal Adversarial Perturbations (UAPs), which are instance agnostic perturbations that can deceive a target model across a wide range of samples. Unlike instance-specific adversarial examples, UAPs present a greater challenge as they must generalize across different samples and models. Generating UAPs typically requires access to numerous examples, which is a strong assumption in real-world tasks. In this paper, we propose a novel data-free method called Intrinsic UAP (IntriUAP), by exploiting the intrinsic vulnerabilities of deep models. We analyze a series of popular deep models composed of linear and nonlinear layers with a Lipschitz constant of 1, revealing that the vulnerability of these models is predominantly influenced by their linear components. Based on this observation, we leverage the ill-conditioned nature of the linear components by aligning the UAP with the right singular vectors corresponding to the maximum singular value of each linear layer. Remarkably, our method achieves highly competitive performance in attacking popular image classification deep models without using any image samples. We also evaluate the black-box attack performance of our method, showing that it matches the state-of-the-art baseline for data-free methods on models that conform to our theoretical framework. Beyond the data-free assumption, IntriUAP also operates under a weaker assumption, where the adversary only can access a few of the victim model's layers. Experiments demonstrate that the attack success rate decreases by only 4% when the adversary has access to just 50% of the linear layers in the victim model.</li>
</ul>

<h3>Title: Intrinsic Image Decomposition for Robust Self-supervised Monocular Depth Estimation on Reflective Surfaces</h3>
<ul>
<li><strong>Authors: </strong>Wonhyeok Choi, Kyumin Hwang, Minwoo Choi, Kiljoon Han, Wonjoon Choi, Mingyu Shin, Sunghoon Im</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22209">https://arxiv.org/abs/2503.22209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22209">https://arxiv.org/pdf/2503.22209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22209]] Intrinsic Image Decomposition for Robust Self-supervised Monocular Depth Estimation on Reflective Surfaces(https://arxiv.org/abs/2503.22209)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Self-supervised monocular depth estimation (SSMDE) has gained attention in the field of deep learning as it estimates depth without requiring ground truth depth maps. This approach typically uses a photometric consistency loss between a synthesized image, generated from the estimated depth, and the original image, thereby reducing the need for extensive dataset acquisition. However, the conventional photometric consistency loss relies on the Lambertian assumption, which often leads to significant errors when dealing with reflective surfaces that deviate from this model. To address this limitation, we propose a novel framework that incorporates intrinsic image decomposition into SSMDE. Our method synergistically trains for both monocular depth estimation and intrinsic image decomposition. The accurate depth estimation facilitates multi-image consistency for intrinsic image decomposition by aligning different view coordinate systems, while the decomposition process identifies reflective areas and excludes corrupted gradients from the depth training process. Furthermore, our framework introduces a pseudo-depth generation and knowledge distillation technique to further enhance the performance of the student model across both reflective and non-reflective surfaces. Comprehensive evaluations on multiple datasets show that our approach significantly outperforms existing SSMDE baselines in depth prediction, especially on reflective surfaces.</li>
</ul>

<h3>Title: Fuzzy Cluster-Aware Contrastive Clustering for Time Series</h3>
<ul>
<li><strong>Authors: </strong>Congyu Wang, Mingjing Du, Xiang Jiang, Yongquan Dong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22211">https://arxiv.org/abs/2503.22211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22211">https://arxiv.org/pdf/2503.22211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22211]] Fuzzy Cluster-Aware Contrastive Clustering for Time Series(https://arxiv.org/abs/2503.22211)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The rapid growth of unlabeled time series data, driven by the Internet of Things (IoT), poses significant challenges in uncovering underlying patterns. Traditional unsupervised clustering methods often fail to capture the complex nature of time series data. Recent deep learning-based clustering approaches, while effective, struggle with insufficient representation learning and the integration of clustering objectives. To address these issues, we propose a fuzzy cluster-aware contrastive clustering framework (FCACC) that jointly optimizes representation learning and clustering. Our approach introduces a novel three-view data augmentation strategy to enhance feature extraction by leveraging various characteristics of time series data. Additionally, we propose a cluster-aware hard negative sample generation mechanism that dynamically constructs high-quality negative samples using clustering structure information, thereby improving the model's discriminative ability. By leveraging fuzzy clustering, FCACC dynamically generates cluster structures to guide the contrastive learning process, resulting in more accurate clustering. Extensive experiments on 40 benchmark datasets show that FCACC outperforms the selected baseline methods (eight in total), providing an effective solution for unsupervised time series learning.</li>
</ul>

<h3>Title: Interpretable Deep Learning Paradigm for Airborne Transient Electromagnetic Inversion</h3>
<ul>
<li><strong>Authors: </strong>Shuang Wang, Xuben Wang, Fei Deng, Xiaodong Yu, Peifan Jiang, Lifeng Mao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22214">https://arxiv.org/abs/2503.22214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22214">https://arxiv.org/pdf/2503.22214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22214]] Interpretable Deep Learning Paradigm for Airborne Transient Electromagnetic Inversion(https://arxiv.org/abs/2503.22214)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability</a></li>
<li><strong>Abstract: </strong>The extraction of geoelectric structural information from airborne transient electromagnetic(ATEM)data primarily involves data processing and inversion. Conventional methods rely on empirical parameter selection, making it difficult to process complex field data with high noise levels. Additionally, inversion computations are time consuming and often suffer from multiple local minima. Existing deep learning-based approaches separate the data processing steps, where independently trained denoising networks struggle to ensure the reliability of subsequent inversions. Moreover, end to end networks lack interpretability. To address these issues, we propose a unified and interpretable deep learning inversion paradigm based on disentangled representation learning. The network explicitly decomposes noisy data into noise and signal factors, completing the entire data processing workflow based on the signal factors while incorporating physical information for guidance. This approach enhances the network's reliability and interpretability. The inversion results on field data demonstrate that our method can directly use noisy data to accurately reconstruct the subsurface electrical structure. Furthermore, it effectively processes data severely affected by environmental noise, which traditional methods struggle with, yielding improved lateral structural resolution.</li>
</ul>

<h3>Title: ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Wenjie Liu, Zhongliang Liu, Xiaoyan Yang, Man Sha, Yang Li</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22218">https://arxiv.org/abs/2503.22218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22218">https://arxiv.org/pdf/2503.22218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22218]] ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian Splatting(https://arxiv.org/abs/2503.22218)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>3D scene stylization approaches based on Neural Radiance Fields (NeRF) achieve promising results by optimizing with Nearest Neighbor Feature Matching (NNFM) loss. However, NNFM loss does not consider global style information. In addition, the implicit representation of NeRF limits their fine-grained control over the resulting scenes. In this paper, we introduce ABC-GS, a novel framework based on 3D Gaussian Splatting to achieve high-quality 3D style transfer. To this end, a controllable matching stage is designed to achieve precise alignment between scene content and style features through segmentation masks. Moreover, a style transfer loss function based on feature alignment is proposed to ensure that the outcomes of style transfer accurately reflect the global style of the reference image. Furthermore, the original geometric information of the scene is preserved with the depth loss and Gaussian regularization terms. Extensive experiments show that our ABC-GS provides controllability of style transfer and achieves stylization results that are more faithfully aligned with the global style of the chosen artistic reference. Our homepage is available at this https URL.</li>
</ul>

<h3>Title: DREMnet: An Interpretable Denoising Framework for Semi-Airborne Transient Electromagnetic Signal</h3>
<ul>
<li><strong>Authors: </strong>Shuang Wang, Ming Guo, Xuben Wang, Fei Deng, Lifeng Mao, Bin Wang, Wenlong Gao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22223">https://arxiv.org/abs/2503.22223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22223">https://arxiv.org/pdf/2503.22223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22223]] DREMnet: An Interpretable Denoising Framework for Semi-Airborne Transient Electromagnetic Signal(https://arxiv.org/abs/2503.22223)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>The semi-airborne transient electromagnetic method (SATEM) is capable of conducting rapid surveys over large-scale and hard-to-reach areas. However, the acquired signals are often contaminated by complex noise, which can compromise the accuracy of subsequent inversion interpretations. Traditional denoising techniques primarily rely on parameter selection strategies, which are insufficient for processing field data in noisy environments. With the advent of deep learning, various neural networks have been employed for SATEM signal denoising. However, existing deep learning methods typically use single-mapping learning approaches that struggle to effectively separate signal from noise. These methods capture only partial information and lack interpretability. To overcome these limitations, we propose an interpretable decoupled representation learning framework, termed DREMnet, that disentangles data into content and context factors, enabling robust and interpretable denoising in complex conditions. To address the limitations of CNN and Transformer architectures, we utilize the RWKV architecture for data processing and introduce the Contextual-WKV mechanism, which allows unidirectional WKV to perform bidirectional signal modeling. Our proposed Covering Embedding technique retains the strong local perception of convolutional networks through stacked embedding. Experimental results on test datasets demonstrate that the DREMnet method outperforms existing techniques, with processed field data that more accurately reflects the theoretical signal, offering improved identification of subsurface electrical structures.</li>
</ul>

<h3>Title: Follow Your Motion: A Generic Temporal Consistency Portrait Editing Framework with Trajectory Guidance</h3>
<ul>
<li><strong>Authors: </strong>Haijie Yang, Zhenyu Zhang, Hao Tang, Jianjun Qian, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22225">https://arxiv.org/abs/2503.22225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22225">https://arxiv.org/pdf/2503.22225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22225]] Follow Your Motion: A Generic Temporal Consistency Portrait Editing Framework with Trajectory Guidance(https://arxiv.org/abs/2503.22225)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Pre-trained conditional diffusion models have demonstrated remarkable potential in image editing. However, they often face challenges with temporal consistency, particularly in the talking head domain, where continuous changes in facial expressions intensify the level of difficulty. These issues stem from the independent editing of individual images and the inherent loss of temporal continuity during the editing process. In this paper, we introduce Follow Your Motion (FYM), a generic framework for maintaining temporal consistency in portrait editing. Specifically, given portrait images rendered by a pre-trained 3D Gaussian Splatting model, we first develop a diffusion model that intuitively and inherently learns motion trajectory changes at different scales and pixel coordinates, from the first frame to each subsequent frame. This approach ensures that temporally inconsistent edited avatars inherit the motion information from the rendered avatars. Secondly, to maintain fine-grained expression temporal consistency in talking head editing, we propose a dynamic re-weighted attention mechanism. This mechanism assigns higher weight coefficients to landmark points in space and dynamically updates these weights based on landmark loss, achieving more consistent and refined facial expressions. Extensive experiments demonstrate that our method outperforms existing approaches in terms of temporal consistency and can be used to optimize and compensate for temporally inconsistent outputs in a range of applications, such as text-driven editing, relighting, and various other applications.</li>
</ul>

<h3>Title: CAT: A GPU-Accelerated FHE Framework with Its Application to High-Precision Private Dataset Query</h3>
<ul>
<li><strong>Authors: </strong>Qirui Li, Rui Zong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22227">https://arxiv.org/abs/2503.22227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22227">https://arxiv.org/pdf/2503.22227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22227]] CAT: A GPU-Accelerated FHE Framework with Its Application to High-Precision Private Dataset Query(https://arxiv.org/abs/2503.22227)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, segmentation</a></li>
<li><strong>Abstract: </strong>We introduce an open-source GPU-accelerated fully homomorphic encryption (FHE) framework CAT, which surpasses existing solutions in functionality and efficiency. \emph{CAT} features a three-layer architecture: a foundation of core math, a bridge of pre-computed elements and combined operations, and an API-accessible layer of FHE operators. It utilizes techniques such as parallel executed operations, well-defined layout patterns of cipher data, kernel fusion/segmentation, and dual GPU pools to enhance the overall execution efficiency. In addition, a memory management mechanism ensures server-side suitability and prevents data leakage. Based on our framework, we implement three widely used FHE schemes: CKKS, BFV, and BGV. The results show that our implementation on Nvidia 4090 can achieve up to 2173$\times$ speedup over CPU implementation and 1.25$\times$ over state-of-the-art GPU acceleration work for specific operations. What's more, we offer a scenario validation with CKKS-based Privacy Database Queries, achieving a 33$\times$ speedup over its CPU counterpart. All query tasks can handle datasets up to $10^3$ rows on a single GPU within 1 second, using 2-5 GB storage. Our implementation has undergone extensive stability testing and can be easily deployed on commercial GPUs. We hope that our work will significantly advance the integration of state-of-the-art FHE algorithms into diverse real-world systems by providing a robust, industry-ready, and open-source tool.</li>
</ul>

<h3>Title: Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback</h3>
<ul>
<li><strong>Authors: </strong>Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, Yu Yue, Lin Yan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22230">https://arxiv.org/abs/2503.22230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22230">https://arxiv.org/pdf/2503.22230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22230]] Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback(https://arxiv.org/abs/2503.22230)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked. This paper addresses this gap by exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity. We introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. We also propose a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, we find that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate our methods' effectiveness and scalability. Results show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Our strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF.</li>
</ul>

<h3>Title: CoGen: 3D Consistent Video Generation via Adaptive Conditioning for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Yishen Ji, Ziyue Zhu, Zhenxin Zhu, Kaixin Xiong, Ming Lu, Zhiqi Li, Lijun Zhou, Haiyang Sun, Bing Wang, Tong Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22231">https://arxiv.org/abs/2503.22231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22231">https://arxiv.org/pdf/2503.22231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22231]] CoGen: 3D Consistent Video Generation via Adaptive Conditioning for Autonomous Driving(https://arxiv.org/abs/2503.22231)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent progress in driving video generation has shown significant potential for enhancing self-driving systems by providing scalable and controllable training data. Although pretrained state-of-the-art generation models, guided by 2D layout conditions (e.g., HD maps and bounding boxes), can produce photorealistic driving videos, achieving controllable multi-view videos with high 3D consistency remains a major challenge. To tackle this, we introduce a novel spatial adaptive generation framework, CoGen, which leverages advances in 3D generation to improve performance in two key aspects: (i) To ensure 3D consistency, we first generate high-quality, controllable 3D conditions that capture the geometry of driving scenes. By replacing coarse 2D conditions with these fine-grained 3D representations, our approach significantly enhances the spatial consistency of the generated videos. (ii) Additionally, we introduce a consistency adapter module to strengthen the robustness of the model to multi-condition control. The results demonstrate that this method excels in preserving geometric fidelity and visual realism, offering a reliable video generation solution for autonomous driving.</li>
</ul>

<h3>Title: Privacy-Preserving Secure Neighbor Discovery for Wireless Networks</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Mohamed Hussain, Panos Papadimitratos</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22232">https://arxiv.org/abs/2503.22232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22232">https://arxiv.org/pdf/2503.22232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22232]] Privacy-Preserving Secure Neighbor Discovery for Wireless Networks(https://arxiv.org/abs/2503.22232)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, steal</a></li>
<li><strong>Abstract: </strong>Traditional Neighbor Discovery (ND) and Secure Neighbor Discovery (SND) are key elements for network functionality. SND is a hard problem, satisfying not only typical security properties (authentication, integrity) but also verification of direct communication, which involves distance estimation based on time measurements and device coordinates. Defeating relay attacks, also known as "wormholes", leading to stealthy Byzantine links and significant degradation of communication and adversarial control, is key in many wireless networked systems. However, SND is not concerned with privacy; it necessitates revealing the identity and location of the device(s) participating in the protocol execution. This can be a deterrent for deployment, especially involving user-held devices in the emerging Internet of Things (IoT) enabled smart environments. To address this challenge, we present a novel Privacy-Preserving Secure Neighbor Discovery (PP-SND) protocol, enabling devices to perform SND without revealing their actual identities and locations, effectively decoupling discovery from the exposure of sensitive information. We use Homomorphic Encryption (HE) for computing device distances without revealing their actual coordinates, as well as employing a pseudonymous device authentication to hide identities while preserving communication integrity. PP-SND provides SND [1] along with pseudonymity, confidentiality, and unlinkability. Our presentation here is not specific to one wireless technology, and we assess the performance of the protocols (cryptographic overhead) on a Raspberry Pi 4 and provide a security and privacy analysis.</li>
</ul>

<h3>Title: WeatherMesh-3: Fast and accurate operational global weather forecasting</h3>
<ul>
<li><strong>Authors: </strong>Haoxing Du, Lyna Kim, Joan Creus-Costa, Jack Michaels, Anuj Shetty, Todd Hutchinson, Christopher Riedel, John Dean</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22235">https://arxiv.org/abs/2503.22235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22235">https://arxiv.org/pdf/2503.22235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22235]] WeatherMesh-3: Fast and accurate operational global weather forecasting(https://arxiv.org/abs/2503.22235)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present WeatherMesh-3 (WM-3), an operational transformer-based global weather forecasting system that improves the state of the art in both accuracy and computational efficiency. We introduce the following advances: 1) a latent rollout that enables arbitrary-length predictions in latent space without intermediate encoding or decoding; and 2) a modular architecture that flexibly utilizes mixed-horizon processors and encodes multiple real-time analyses to create blended initial conditions. WM-3 generates 14-day global forecasts at 0.25-degree resolution in 12 seconds on a single RTX 4090. This represents a >100,000-fold speedup over traditional NWP approaches while achieving superior accuracy with up to 37.7% improvement in RMSE over operational models, requiring only a single consumer-grade GPU for deployment. We aim for WM-3 to democratize weather forecasting by providing an accessible, lightweight model for operational use while pushing the performance boundaries of machine learning-based weather prediction.</li>
</ul>

<h3>Title: SCHNet: SAM Marries CLIP for Human Parsing</h3>
<ul>
<li><strong>Authors: </strong>Kunliang Liu, Jianming Wang, Rize Jin, Wonjun Hwang, Tae-Sun Chung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22237">https://arxiv.org/abs/2503.22237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22237">https://arxiv.org/pdf/2503.22237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22237]] SCHNet: SAM Marries CLIP for Human Parsing(https://arxiv.org/abs/2503.22237)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Vision Foundation Model (VFM) such as the Segment Anything Model (SAM) and Contrastive Language-Image Pre-training Model (CLIP) has shown promising performance for segmentation and detection tasks. However, although SAM excels in fine-grained segmentation, it faces major challenges when applying it to semantic-aware segmentation. While CLIP exhibits a strong semantic understanding capability via aligning the global features of language and vision, it has deficiencies in fine-grained segmentation tasks. Human parsing requires to segment human bodies into constituent parts and involves both accurate fine-grained segmentation and high semantic understanding of each part. Based on traits of SAM and CLIP, we formulate high efficient modules to effectively integrate features of them to benefit human parsing. We propose a Semantic-Refinement Module to integrate semantic features of CLIP with SAM features to benefit parsing. Moreover, we formulate a high efficient Fine-tuning Module to adjust the pretrained SAM for human parsing that needs high semantic information and simultaneously demands spatial details, which significantly reduces the training time compared with full-time training and achieves notable performance. Extensive experiments demonstrate the effectiveness of our method on LIP, PPP, and CIHP databases.</li>
</ul>

<h3>Title: Analysis of On-policy Policy Gradient Methods under the Distribution Mismatch</h3>
<ul>
<li><strong>Authors: </strong>Weizhen Wang, Jianping He, Xiaoming Duan</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22244">https://arxiv.org/abs/2503.22244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22244">https://arxiv.org/pdf/2503.22244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22244]] Analysis of On-policy Policy Gradient Methods under the Distribution Mismatch(https://arxiv.org/abs/2503.22244)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Policy gradient methods are one of the most successful methods for solving challenging reinforcement learning problems. However, despite their empirical successes, many SOTA policy gradient algorithms for discounted problems deviate from the theoretical policy gradient theorem due to the existence of a distribution mismatch. In this work, we analyze the impact of this mismatch on the policy gradient methods. Specifically, we first show that in the case of tabular parameterizations, the methods under the mismatch remain globally optimal. Then, we extend this analysis to more general parameterizations by leveraging the theory of biased stochastic gradient descent. Our findings offer new insights into the robustness of policy gradient methods as well as the gap between theoretical foundations and practical implementations.</li>
</ul>

<h3>Title: Efficient Building Roof Type Classification: A Domain-Specific Self-Supervised Approach</h3>
<ul>
<li><strong>Authors: </strong>Guneet Mutreja, Ksenia Bittner</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22251">https://arxiv.org/abs/2503.22251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22251">https://arxiv.org/pdf/2503.22251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22251]] Efficient Building Roof Type Classification: A Domain-Specific Self-Supervised Approach(https://arxiv.org/abs/2503.22251)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Accurate classification of building roof types from aerial imagery is crucial for various remote sensing applications, including urban planning, disaster management, and infrastructure monitoring. However, this task is often hindered by the limited availability of labeled data for supervised learning approaches. To address this challenge, this paper investigates the effectiveness of self supervised learning with EfficientNet architectures, known for their computational efficiency, for building roof type classification. We propose a novel framework that incorporates a Convolutional Block Attention Module (CBAM) to enhance the feature extraction capabilities of EfficientNet. Furthermore, we explore the benefits of pretraining on a domain-specific dataset, the Aerial Image Dataset (AID), compared to ImageNet pretraining. Our experimental results demonstrate the superiority of our approach. Employing Simple Framework for Contrastive Learning of Visual Representations (SimCLR) with EfficientNet-B3 and CBAM achieves a 95.5% accuracy on our validation set, matching the performance of state-of-the-art transformer-based models while utilizing significantly fewer parameters. We also provide a comprehensive evaluation on two challenging test sets, demonstrating the generalization capability of our method. Notably, our findings highlight the effectiveness of domain-specific pretraining, consistently leading to higher accuracy compared to models pretrained on the generic ImageNet dataset. Our work establishes EfficientNet based self-supervised learning as a computationally efficient and highly effective approach for building roof type classification, particularly beneficial in scenarios with limited labeled data.</li>
</ul>

<h3>Title: DynaGraph: Interpretable Multi-Label Prediction from EHRs via Dynamic Graph Learning and Contrastive Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Munib Mesinovic, Soheila Molaei, Peter Watkinson, Tingting Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22257">https://arxiv.org/abs/2503.22257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22257">https://arxiv.org/pdf/2503.22257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22257]] DynaGraph: Interpretable Multi-Label Prediction from EHRs via Dynamic Graph Learning and Contrastive Augmentation(https://arxiv.org/abs/2503.22257)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Learning from longitudinal electronic health records is limited if it does not capture the temporal trajectories of the patient's state in a clinical setting. Graph models allow us to capture the hidden dependencies of the multivariate time-series when the graphs are constructed in a similar dynamic manner. Previous dynamic graph models require a pre-defined and/or static graph structure, which is unknown in most cases, or they only capture the spatial relations between the features. Furthermore in healthcare, the interpretability of the model is an essential requirement to build trust with clinicians. In addition to previously proposed attention mechanisms, there has not been an interpretable dynamic graph framework for data from multivariate electronic health records (EHRs). Here, we propose DynaGraph, an end-to-end interpretable contrastive graph model that learns the dynamics of multivariate time-series EHRs as part of optimisation. We validate our model in four real-world clinical datasets, ranging from primary care to secondary care settings with broad demographics, in challenging settings where tasks are imbalanced and multi-labelled. Compared to state-of-the-art models, DynaGraph achieves significant improvements in balanced accuracy and sensitivity over the nearest complex competitors in time-series or dynamic graph modelling across three ICU and one primary care datasets. Through a pseudo-attention approach to graph construction, our model also indicates the importance of clinical covariates over time, providing means for clinical validation.</li>
</ul>

<h3>Title: Mono2Stereo: A Benchmark and Empirical Study for Stereo Conversion</h3>
<ul>
<li><strong>Authors: </strong>Songsong Yu, Yuxin Chen, Zhongang Qi, Zeke Xie, Yifan Wang, Lijun Wang, Ying Shan, Huchuan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22262">https://arxiv.org/abs/2503.22262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22262">https://arxiv.org/pdf/2503.22262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22262]] Mono2Stereo: A Benchmark and Empirical Study for Stereo Conversion(https://arxiv.org/abs/2503.22262)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the rapid proliferation of 3D devices and the shortage of 3D content, stereo conversion is attracting increasing attention. Recent works introduce pretrained Diffusion Models (DMs) into this task. However, due to the scarcity of large-scale training data and comprehensive benchmarks, the optimal methodologies for employing DMs in stereo conversion and the accurate evaluation of stereo effects remain largely unexplored. In this work, we introduce the Mono2Stereo dataset, providing high-quality training data and benchmark to support in-depth exploration of stereo conversion. With this dataset, we conduct an empirical study that yields two primary findings. 1) The differences between the left and right views are subtle, yet existing metrics consider overall pixels, failing to concentrate on regions critical to stereo effects. 2) Mainstream methods adopt either one-stage left-to-right generation or warp-and-inpaint pipeline, facing challenges of degraded stereo effect and image distortion respectively. Based on these findings, we introduce a new evaluation metric, Stereo Intersection-over-Union, which prioritizes disparity and achieves a high correlation with human judgments on stereo effect. Moreover, we propose a strong baseline model, harmonizing the stereo effect and image quality simultaneously, and notably surpassing current mainstream methods. Our code and data will be open-sourced to promote further research in stereo conversion. Our models are available at this http URL.</li>
</ul>

<h3>Title: FLIP: Towards Comprehensive and Reliable Evaluation of Federated Prompt Learning</h3>
<ul>
<li><strong>Authors: </strong>Dongping Liao, Xitong Gao, Yabo Xu, Chengzhong Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22263">https://arxiv.org/abs/2503.22263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22263">https://arxiv.org/pdf/2503.22263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22263]] FLIP: Towards Comprehensive and Reliable Evaluation of Federated Prompt Learning(https://arxiv.org/abs/2503.22263)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, federate</a></li>
<li><strong>Abstract: </strong>The increasing emphasis on privacy and data security has driven the adoption of federated learning, a decentralized approach to train machine learning models without sharing raw data. Prompt learning, which fine-tunes prompt embeddings of pretrained models, offers significant advantages in federated settings by reducing computational costs and communication overheads while leveraging the strong performance and generalization capabilities of vision-language models such as CLIP. This paper addresses the intersection of federated learning and prompt learning, particularly for vision-language models. In this work, we introduce a comprehensive framework, named FLIP, to evaluate federated prompt learning algorithms. FLIP assesses the performance of 8 state-of-the-art federated prompt learning methods across 4 federated learning protocols and 12 open datasets, considering 6 distinct evaluation scenarios. Our findings demonstrate that prompt learning maintains strong generalization performance in both in-distribution and out-of-distribution settings with minimal resource consumption. This work highlights the effectiveness of federated prompt learning in environments characterized by data scarcity, unseen classes, and cross-domain distributional shifts. We open-source the code for all implemented algorithms in FLIP to facilitate further research in this domain.</li>
</ul>

<h3>Title: Segment Any Motion in Videos</h3>
<ul>
<li><strong>Authors: </strong>Nan Huang, Wenzhao Zheng, Chenfeng Xu, Kurt Keutzer, Shanghang Zhang, Angjoo Kanazawa, Qianqian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22268">https://arxiv.org/abs/2503.22268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22268">https://arxiv.org/pdf/2503.22268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22268]] Segment Any Motion in Videos(https://arxiv.org/abs/2503.22268)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Moving object segmentation is a crucial task for achieving a high-level understanding of visual scenes and has numerous downstream applications. Humans can effortlessly segment moving objects in videos. Previous work has largely relied on optical flow to provide motion cues; however, this approach often results in imperfect predictions due to challenges such as partial motion, complex deformations, motion blur and background distractions. We propose a novel approach for moving object segmentation that combines long-range trajectory motion cues with DINO-based semantic features and leverages SAM2 for pixel-level mask densification through an iterative prompting strategy. Our model employs Spatio-Temporal Trajectory Attention and Motion-Semantic Decoupled Embedding to prioritize motion while integrating semantic support. Extensive testing on diverse datasets demonstrates state-of-the-art performance, excelling in challenging scenarios and fine-grained segmentation of multiple objects. Our code is available at this https URL.</li>
</ul>

<h3>Title: Machine Learning Models for Soil Parameter Prediction Based on Satellite, Weather, Clay and Yield Data</h3>
<ul>
<li><strong>Authors: </strong>Calvin Kammerlander, Viola Kolb, Marinus Luegmair, Lou Scheermann, Maximilian Schmailzl, Marco Seufert, Jiayun Zhang, Denis Dalic, Torsten Schn</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22276">https://arxiv.org/abs/2503.22276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22276">https://arxiv.org/pdf/2503.22276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22276]] Machine Learning Models for Soil Parameter Prediction Based on Satellite, Weather, Clay and Yield Data(https://arxiv.org/abs/2503.22276)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Efficient nutrient management and precise fertilization are essential for advancing modern agriculture, particularly in regions striving to optimize crop yields sustainably. The AgroLens project endeavors to address this challenge by develop ing Machine Learning (ML)-based methodologies to predict soil nutrient levels without reliance on laboratory tests. By leveraging state of the art techniques, the project lays a foundation for acionable insights to improve agricultural productivity in resource-constrained areas, such as Africa. The approach begins with the development of a robust European model using the LUCAS Soil dataset and Sentinel-2 satellite imagery to estimate key soil properties, including phosphorus, potassium, nitrogen, and pH levels. This model is then enhanced by integrating supplementary features, such as weather data, harvest rates, and Clay AI-generated embeddings. This report details the methodological framework, data preprocessing strategies, and ML pipelines employed in this project. Advanced algorithms, including Random Forests, Extreme Gradient Boosting (XGBoost), and Fully Connected Neural Networks (FCNN), were implemented and finetuned for precise nutrient prediction. Results showcase robust model performance, with root mean square error values meeting stringent accuracy thresholds. By establishing a reproducible and scalable pipeline for soil nutrient prediction, this research paves the way for transformative agricultural applications, including precision fertilization and improved resource allocation in underresourced regions like Africa.</li>
</ul>

<h3>Title: MultiClaimNet: A Massively Multilingual Dataset of Fact-Checked Claim Clusters</h3>
<ul>
<li><strong>Authors: </strong>Rrubaa Panchendrarajan, Rubn Mguez, Arkaitz Zubiaga</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22280">https://arxiv.org/abs/2503.22280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22280">https://arxiv.org/pdf/2503.22280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22280]] MultiClaimNet: A Massively Multilingual Dataset of Fact-Checked Claim Clusters(https://arxiv.org/abs/2503.22280)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the context of fact-checking, claims are often repeated across various platforms and in different languages, which can benefit from a process that reduces this redundancy. While retrieving previously fact-checked claims has been investigated as a solution, the growing number of unverified claims and expanding size of fact-checked databases calls for alternative, more efficient solutions. A promising solution is to group claims that discuss the same underlying facts into clusters to improve claim retrieval and validation. However, research on claim clustering is hindered by the lack of suitable datasets. To bridge this gap, we introduce \textit{MultiClaimNet}, a collection of three multilingual claim cluster datasets containing claims in 86 languages across diverse topics. Claim clusters are formed automatically from claim-matching pairs with limited manual intervention. We leverage two existing claim-matching datasets to form the smaller datasets within \textit{MultiClaimNet}. To build the larger dataset, we propose and validate an approach involving retrieval of approximate nearest neighbors to form candidate claim pairs and an automated annotation of claim similarity using large language models. This larger dataset contains 85.3K fact-checked claims written in 78 languages. We further conduct extensive experiments using various clustering techniques and sentence embedding models to establish baseline performance. Our datasets and findings provide a strong foundation for scalable claim clustering, contributing to efficient fact-checking pipelines.</li>
</ul>

<h3>Title: A Dataset for Semantic Segmentation in the Presence of Unknowns</h3>
<ul>
<li><strong>Authors: </strong>Zakaria Laskar, Tomas Vojir, Matej Grcic, Iaroslav Melekhov, Shankar Gangisettye, Juho Kannala, Jiri Matas, Giorgos Tolias, C.V. Jawahar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22309">https://arxiv.org/abs/2503.22309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22309">https://arxiv.org/pdf/2503.22309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22309]] A Dataset for Semantic Segmentation in the Presence of Unknowns(https://arxiv.org/abs/2503.22309)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Before deployment in the real-world deep neural networks require thorough evaluation of how they handle both knowns, inputs represented in the training data, and unknowns (anomalies). This is especially important for scene understanding tasks with safety critical applications, such as in autonomous driving. Existing datasets allow evaluation of only knowns or unknowns - but not both, which is required to establish "in the wild" suitability of deep neural network models. To bridge this gap, we propose a novel anomaly segmentation dataset, ISSU, that features a diverse set of anomaly inputs from cluttered real-world environments. The dataset is twice larger than existing anomaly segmentation datasets, and provides a training, validation and test set for controlled in-domain evaluation. The test set consists of a static and temporal part, with the latter comprised of videos. The dataset provides annotations for both closed-set (knowns) and anomalies, enabling closed-set and open-set evaluation. The dataset covers diverse conditions, such as domain and cross-sensor shift, illumination variation and allows ablation of anomaly detection methods with respect to these variations. Evaluation results of current state-of-the-art methods confirm the need for improvements especially in domain-generalization, small and large object segmentation.</li>
</ul>

<h3>Title: A Refined Analysis of Massive Activations in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Louis Owen, Nilabhra Roy Chowdhury, Abhay Kumar, Fabian Gra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22329">https://arxiv.org/abs/2503.22329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22329">https://arxiv.org/pdf/2503.22329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22329]] A Refined Analysis of Massive Activations in LLMs(https://arxiv.org/abs/2503.22329)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Motivated in part by their relevance for low-precision training and quantization, massive activations in large language models (LLMs) have recently emerged as a topic of interest. However, existing analyses are limited in scope, and generalizability across architectures is unclear. This paper helps address some of these gaps by conducting an analysis of massive activations across a broad range of LLMs, including both GLU-based and non-GLU-based architectures. Our findings challenge several prior assumptions, most importantly: (1) not all massive activations are detrimental, i.e. suppressing them does not lead to an explosion of perplexity or a collapse in downstream task performance; (2) proposed mitigation strategies such as Attention KV bias are model-specific and ineffective in certain cases. We consequently investigate novel hybrid mitigation strategies; in particular pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT) successfully balances the mitigation of massive activations with preserved downstream model performance in the scenarios we investigated. Our code is available at: this https URL.</li>
</ul>

<h3>Title: Imperceptible but Forgeable: Practical Invisible Watermark Forgery via Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ziping Dong, Chao Shuai, Zhongjie Ba, Peng Cheng, Zhan Qin, Qinglong Wang, Kui Ren</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22330">https://arxiv.org/abs/2503.22330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22330">https://arxiv.org/pdf/2503.22330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22330]] Imperceptible but Forgeable: Practical Invisible Watermark Forgery via Diffusion Models(https://arxiv.org/abs/2503.22330)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, watermark, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Invisible watermarking is critical for content provenance and accountability in Generative AI. Although commercial companies have increasingly committed to using watermarks, the robustness of existing watermarking schemes against forgery attacks is understudied. This paper proposes DiffForge, the first watermark forgery framework capable of forging imperceptible watermarks under a no-box setting. We estimate the watermark distribution using an unconditional diffusion model and introduce shallow inversion to inject the watermark into a non-watermarked image seamlessly. This approach facilitates watermark injection while preserving image quality by adaptively selecting the depth of inversion steps, leveraging our key insight that watermarks degrade with added noise during the early diffusion phases. Comprehensive evaluations show that DiffForge deceives open-source watermark detectors with a 96.38% success rate and misleads a commercial watermark system with over 97% success rate, achieving high confidence.1 This work reveals fundamental security limitations in current watermarking paradigms.</li>
</ul>

<h3>Title: SKDU at De-Factify 4.0: Natural Language Features for AI-Generated Text-Detection</h3>
<ul>
<li><strong>Authors: </strong>Shrikant Malviya, Pablo Arnau-Gonzlez, Miguel Arevalillo-Herrez, Stamos Katsigiannis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22338">https://arxiv.org/abs/2503.22338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22338">https://arxiv.org/pdf/2503.22338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22338]] SKDU at De-Factify 4.0: Natural Language Features for AI-Generated Text-Detection(https://arxiv.org/abs/2503.22338)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has introduced new challenges in distinguishing human-written text from AI-generated content. In this work, we explored a pipelined approach for AI-generated text detection that includes a feature extraction step (i.e. prompt-based rewriting features inspired by RAIDAR and content-based features derived from the NELA toolkit) followed by a classification module. Comprehensive experiments were conducted on the Defactify4.0 dataset, evaluating two tasks: binary classification to differentiate human-written and AI-generated text, and multi-class classification to identify the specific generative model used to generate the input text. Our findings reveal that NELA features significantly outperform RAIDAR features in both tasks, demonstrating their ability to capture nuanced linguistic, stylistic, and content-based differences. Combining RAIDAR and NELA features provided minimal improvement, highlighting the redundancy introduced by less discriminative features. Among the classifiers tested, XGBoost emerged as the most effective, leveraging the rich feature sets to achieve high accuracy and generalisation.</li>
</ul>

<h3>Title: Semantix: An Energy Guided Sampler for Semantic Style Transfer</h3>
<ul>
<li><strong>Authors: </strong>Huiang He, Minghui Hu, Chuanxia Zheng, Chaoyue Wang, Tat-Jen Cham</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22344">https://arxiv.org/abs/2503.22344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22344">https://arxiv.org/pdf/2503.22344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22344]] Semantix: An Energy Guided Sampler for Semantic Style Transfer(https://arxiv.org/abs/2503.22344)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in style and appearance transfer are impressive, but most methods isolate global style and local appearance transfer, neglecting semantic correspondence. Additionally, image and video tasks are typically handled in isolation, with little focus on integrating them for video transfer. To address these limitations, we introduce a novel task, Semantic Style Transfer, which involves transferring style and appearance features from a reference image to a target visual content based on semantic correspondence. We subsequently propose a training-free method, Semantix an energy-guided sampler designed for Semantic Style Transfer that simultaneously guides both style and appearance transfer based on semantic understanding capacity of pre-trained diffusion models. Additionally, as a sampler, Semantix be seamlessly applied to both image and video models, enabling semantic style transfer to be generic across various visual media. Specifically, once inverting both reference and context images or videos to noise space by SDEs, Semantix utilizes a meticulously crafted energy function to guide the sampling process, including three key components: Style Feature Guidance, Spatial Feature Guidance and Semantic Distance as a regularisation term. Experimental results demonstrate that Semantix not only effectively accomplishes the task of semantic style transfer across images and videos, but also surpasses existing state-of-the-art solutions in both fields. The project website is available at this https URL</li>
</ul>

<h3>Title: ArchCAD-400K: An Open Large-Scale Architectural CAD Dataset and New Baseline for Panoptic Symbol Spotting</h3>
<ul>
<li><strong>Authors: </strong>Ruifeng Luo, Zhengjie Liu, Tianxiao Cheng, Jie Wang, Tongjie Wang, Xingguang Wei, Haomin Wang, YanPeng Li, Fu Chai, Fei Cheng, Shenglong Ye, Wenhai Wang, Yanting Zhang, Yu Qiao, Hongjie Zhang, Xianzhong Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22346">https://arxiv.org/abs/2503.22346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22346">https://arxiv.org/pdf/2503.22346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22346]] ArchCAD-400K: An Open Large-Scale Architectural CAD Dataset and New Baseline for Panoptic Symbol Spotting(https://arxiv.org/abs/2503.22346)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recognizing symbols in architectural CAD drawings is critical for various advanced engineering applications. In this paper, we propose a novel CAD data annotation engine that leverages intrinsic attributes from systematically archived CAD drawings to automatically generate high-quality annotations, thus significantly reducing manual labeling efforts. Utilizing this engine, we construct ArchCAD-400K, a large-scale CAD dataset consisting of 413,062 chunks from 5538 highly standardized drawings, making it over 26 times larger than the largest existing CAD dataset. ArchCAD-400K boasts an extended drawing diversity and broader categories, offering line-grained annotations. Furthermore, we present a new baseline model for panoptic symbol spotting, termed Dual-Pathway Symbol Spotter (DPSS). It incorporates an adaptive fusion module to enhance primitive features with complementary image features, achieving state-of-the-art performance and enhanced robustness. Extensive experiments validate the effectiveness of DPSS, demonstrating the value of ArchCAD-400K and its potential to drive innovation in architectural design and construction.</li>
</ul>

<h3>Title: GCRayDiffusion: Pose-Free Surface Reconstruction via Geometric Consistent Ray Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Li-Heng Chen, Zi-Xin Zou, Chang Liu, Tianjiao Jing, Yan-Pei Cao, Shi-Sheng Huang, Hongbo Fu, Hua Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22349">https://arxiv.org/abs/2503.22349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22349">https://arxiv.org/pdf/2503.22349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22349]] GCRayDiffusion: Pose-Free Surface Reconstruction via Geometric Consistent Ray Diffusion(https://arxiv.org/abs/2503.22349)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Accurate surface reconstruction from unposed images is crucial for efficient 3D object or scene creation. However, it remains challenging, particularly for the joint camera pose estimation. Previous approaches have achieved impressive pose-free surface reconstruction results in dense-view settings, but could easily fail for sparse-view scenarios without sufficient visual overlap. In this paper, we propose a new technique for pose-free surface reconstruction, which follows triplane-based signed distance field (SDF) learning but regularizes the learning by explicit points sampled from ray-based diffusion of camera pose estimation. Our key contribution is a novel Geometric Consistent Ray Diffusion model (GCRayDiffusion), where we represent camera poses as neural bundle rays and regress the distribution of noisy rays via a diffusion model. More importantly, we further condition the denoising process of RGRayDiffusion using the triplane-based SDF of the entire scene, which provides effective 3D consistent regularization to achieve multi-view consistent camera pose estimation. Finally, we incorporate RGRayDiffusion into the triplane-based SDF learning by introducing on-surface geometric regularization from the sampling points of the neural bundle rays, which leads to highly accurate pose-free surface reconstruction results even for sparse-view inputs. Extensive evaluations on public datasets show that our GCRayDiffusion achieves more accurate camera pose estimation than previous approaches, with geometrically more consistent surface reconstruction results, especially given sparse-view inputs.</li>
</ul>

<h3>Title: Meta-LoRA: Meta-Learning LoRA Components for Domain-Aware ID Personalization</h3>
<ul>
<li><strong>Authors: </strong>Bar Batuhan Topal, Umut zyurt, Zafer Doan Budak, Ramazan Gokberk Cinbis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22352">https://arxiv.org/abs/2503.22352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22352">https://arxiv.org/pdf/2503.22352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22352]] Meta-LoRA: Meta-Learning LoRA Components for Domain-Aware ID Personalization(https://arxiv.org/abs/2503.22352)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image generative models, particularly latent diffusion models (LDMs), have demonstrated remarkable capabilities in synthesizing high-quality images from textual prompts. However, achieving identity personalization-ensuring that a model consistently generates subject-specific outputs from limited reference images-remains a fundamental challenge. To address this, we introduce Meta-Low-Rank Adaptation (Meta-LoRA), a novel framework that leverages meta-learning to encode domain-specific priors into LoRA-based identity personalization. Our method introduces a structured three-layer LoRA architecture that separates identity-agnostic knowledge from identity-specific adaptation. In the first stage, the LoRA Meta-Down layers are meta-trained across multiple subjects, learning a shared manifold that captures general identity-related features. In the second stage, only the LoRA-Mid and LoRA-Up layers are optimized to specialize on a given subject, significantly reducing adaptation time while improving identity fidelity. To evaluate our approach, we introduce Meta-PHD, a new benchmark dataset for identity personalization, and compare Meta-LoRA against state-of-the-art methods. Our results demonstrate that Meta-LoRA achieves superior identity retention, computational efficiency, and adaptability across diverse identity conditions. The code, model weights, and dataset will be released publicly upon acceptance.</li>
</ul>

<h3>Title: Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions</h3>
<ul>
<li><strong>Authors: </strong>Yubo Li, Yidi Miao, Xueying Ding, Ramayya Krishnan, Rema Padman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22353">https://arxiv.org/abs/2503.22353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22353">https://arxiv.org/pdf/2503.22353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22353]] Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions(https://arxiv.org/abs/2503.22353)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable capabilities across various tasks, but their deployment in high-stake domains requires consistent performance across multiple interaction rounds. This paper introduces a comprehensive framework for evaluating and improving LLM response consistency, making three key contributions. First, we propose a novel Position-Weighted Consistency (PWC) score that captures both the importance of early-stage stability and recovery patterns in multi-turn interactions. Second, we present a carefully curated benchmark dataset spanning diverse domains and difficulty levels, specifically designed to evaluate LLM consistency under various challenging follow-up scenarios. Third, we introduce Confidence-Aware Response Generation (CARG), a framework that significantly improves response stability by incorporating model confidence signals into the generation process. Empirical results demonstrate that CARG significantly improves response stability without sacrificing accuracy, underscoring its potential for reliable LLM deployment in critical applications.</li>
</ul>

<h3>Title: EchoFlow: A Foundation Model for Cardiac Ultrasound Image and Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Hadrien Reynaud, Alberto Gomez, Paul Leeson, Qingjie Meng, Bernhard Kainz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22357">https://arxiv.org/abs/2503.22357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22357">https://arxiv.org/pdf/2503.22357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22357]] EchoFlow: A Foundation Model for Cardiac Ultrasound Image and Video Generation(https://arxiv.org/abs/2503.22357)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Advances in deep learning have significantly enhanced medical image analysis, yet the availability of large-scale medical datasets remains constrained by patient privacy concerns. We present EchoFlow, a novel framework designed to generate high-quality, privacy-preserving synthetic echocardiogram images and videos. EchoFlow comprises four key components: an adversarial variational autoencoder for defining an efficient latent representation of cardiac ultrasound images, a latent image flow matching model for generating accurate latent echocardiogram images, a latent re-identification model to ensure privacy by filtering images anatomically, and a latent video flow matching model for animating latent images into realistic echocardiogram videos conditioned on ejection fraction. We rigorously evaluate our synthetic datasets on the clinically relevant task of ejection fraction regression and demonstrate, for the first time, that downstream models trained exclusively on EchoFlow-generated synthetic datasets achieve performance parity with models trained on real datasets. We release our models and synthetic datasets, enabling broader, privacy-compliant research in medical ultrasound imaging at this https URL.</li>
</ul>

<h3>Title: Mitigating Knowledge Discrepancies among Multiple Datasets for Task-agnostic Unified Face Alignment</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Xia, Min Xu, Wenjian Huang, Jianguo Zhang, Haimin Zhang, Chunxia Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22359">https://arxiv.org/abs/2503.22359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22359">https://arxiv.org/pdf/2503.22359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22359]] Mitigating Knowledge Discrepancies among Multiple Datasets for Task-agnostic Unified Face Alignment(https://arxiv.org/abs/2503.22359)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite the similar structures of human faces, existing face alignment methods cannot learn unified knowledge from multiple datasets with different landmark annotations. The limited training samples in a single dataset commonly result in fragile robustness in this field. To mitigate knowledge discrepancies among different datasets and train a task-agnostic unified face alignment (TUFA) framework, this paper presents a strategy to unify knowledge from multiple datasets. Specifically, we calculate a mean face shape for each dataset. To explicitly align these mean shapes on an interpretable plane based on their semantics, each shape is then incorporated with a group of semantic alignment embeddings. The 2D coordinates of these aligned shapes can be viewed as the anchors of the plane. By encoding them into structure prompts and further regressing the corresponding facial landmarks using image features, a mapping from the plane to the target faces is finally established, which unifies the learning target of different datasets. Consequently, multiple datasets can be utilized to boost the generalization ability of the model. The successful mitigation of discrepancies also enhances the efficiency of knowledge transferring to a novel dataset, significantly boosts the performance of few-shot face alignment. Additionally, the interpretable plane endows TUFA with a task-agnostic characteristic, enabling it to locate landmarks unseen during training in a zero-shot manner. Extensive experiments are carried on seven benchmarks and the results demonstrate an impressive improvement in face alignment brought by knowledge discrepancies mitigation.</li>
</ul>

<h3>Title: Supposedly Equivalent Facts That Aren't? Entity Frequency in Pre-training Induces Asymmetry in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yuan He, Bailan He, Zifeng Ding, Alisia Lupidi, Yuqicheng Zhu, Shuo Chen, Caiqi Zhang, Jiaoyan Chen, Yunpu Ma, Volker Tresp, Ian Horrocks</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22362">https://arxiv.org/abs/2503.22362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22362">https://arxiv.org/pdf/2503.22362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22362]] Supposedly Equivalent Facts That Aren't? Entity Frequency in Pre-training Induces Asymmetry in LLMs(https://arxiv.org/abs/2503.22362)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding and mitigating hallucinations in Large Language Models (LLMs) is crucial for ensuring reliable content generation. While previous research has primarily focused on "when" LLMs hallucinate, our work explains "why" and directly links model behaviour to the pre-training data that forms their prior knowledge. Specifically, we demonstrate that an asymmetry exists in the recognition of logically equivalent facts, which can be attributed to frequency discrepancies of entities appearing as subjects versus objects. Given that most pre-training datasets are inaccessible, we leverage the fully open-source OLMo series by indexing its Dolma dataset to estimate entity frequencies. Using relational facts (represented as triples) from Wikidata5M, we construct probing datasets to isolate this effect. Our experiments reveal that facts with a high-frequency subject and a low-frequency object are better recognised than their inverse, despite their logical equivalence. The pattern reverses in low-to-high frequency settings, and no statistically significant asymmetry emerges when both entities are high-frequency. These findings highlight the influential role of pre-training data in shaping model predictions and provide insights for inferring the characteristics of pre-training data in closed or partially closed LLMs.</li>
</ul>

<h3>Title: ViSketch-GPT: Collaborative Multi-Scale Feature Extraction for Sketch Recognition and Generation</h3>
<ul>
<li><strong>Authors: </strong>Giulio Federico, Giuseppe Amato, Fabio Carrara, Claudio Gennaro, Marco Di Benedetto</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22374">https://arxiv.org/abs/2503.22374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22374">https://arxiv.org/pdf/2503.22374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22374]] ViSketch-GPT: Collaborative Multi-Scale Feature Extraction for Sketch Recognition and Generation(https://arxiv.org/abs/2503.22374)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Understanding the nature of human sketches is challenging because of the wide variation in how they are created. Recognizing complex structural patterns improves both the accuracy in recognizing sketches and the fidelity of the generated sketches. In this work, we introduce ViSketch-GPT, a novel algorithm designed to address these challenges through a multi-scale context extraction approach. The model captures intricate details at multiple scales and combines them using an ensemble-like mechanism, where the extracted features work collaboratively to enhance the recognition and generation of key details crucial for classification and generation tasks. The effectiveness of ViSketch-GPT is validated through extensive experiments on the QuickDraw dataset. Our model establishes a new benchmark, significantly outperforming existing methods in both classification and generation tasks, with substantial improvements in accuracy and the fidelity of generated sketches. The proposed algorithm offers a robust framework for understanding complex structures by extracting features that collaborate to recognize intricate details, enhancing the understanding of structures like sketches and making it a versatile tool for various applications in computer vision and machine learning.</li>
</ul>

<h3>Title: Data Quality Matters: Quantifying Image Quality Impact on Machine Learning Performance</h3>
<ul>
<li><strong>Authors: </strong>Christian Steinhauser, Philipp Reis, Hubert Padusinski, Jacob Langner, Eric Sax</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22375">https://arxiv.org/abs/2503.22375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22375">https://arxiv.org/pdf/2503.22375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22375]] Data Quality Matters: Quantifying Image Quality Impact on Machine Learning Performance(https://arxiv.org/abs/2503.22375)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Precise perception of the environment is essential in highly automated driving systems, which rely on machine learning tasks such as object detection and segmentation. Compression of sensor data is commonly used for data handling, while virtualization is used for hardware-in-the-loop validation. Both methods can alter sensor data and degrade model performance. This necessitates a systematic approach to quantifying image validity. This paper presents a four-step framework to evaluate the impact of image modifications on machine learning tasks. First, a dataset with modified images is prepared to ensure one-to-one matching image pairs, enabling measurement of deviations resulting from compression and virtualization. Second, image deviations are quantified by comparing the effects of compression and virtualization against original camera-based sensor data. Third, the performance of state-of-the-art object detection models is analyzed to determine how altered input data affects perception tasks, including bounding box accuracy and reliability. Finally, a correlation analysis is performed to identify relationships between image quality and model performance. As a result, the LPIPS metric achieves the highest correlation between image deviation and machine learning performance across all evaluated machine learning tasks.</li>
</ul>

<h3>Title: Spend Your Budget Wisely: Towards an Intelligent Distribution of the Privacy Budget in Differentially Private Text Rewriting</h3>
<ul>
<li><strong>Authors: </strong>Stephen Meisenbacher, Chaeeun Joy Lee, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22379">https://arxiv.org/abs/2503.22379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22379">https://arxiv.org/pdf/2503.22379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22379]] Spend Your Budget Wisely: Towards an Intelligent Distribution of the Privacy Budget in Differentially Private Text Rewriting(https://arxiv.org/abs/2503.22379)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The task of $\textit{Differentially Private Text Rewriting}$ is a class of text privatization techniques in which (sensitive) input textual documents are $\textit{rewritten}$ under Differential Privacy (DP) guarantees. The motivation behind such methods is to hide both explicit and implicit identifiers that could be contained in text, while still retaining the semantic meaning of the original text, thus preserving utility. Recent years have seen an uptick in research output in this field, offering a diverse array of word-, sentence-, and document-level DP rewriting methods. Common to these methods is the selection of a privacy budget (i.e., the $\varepsilon$ parameter), which governs the degree to which a text is privatized. One major limitation of previous works, stemming directly from the unique structure of language itself, is the lack of consideration of $\textit{where}$ the privacy budget should be allocated, as not all aspects of language, and therefore text, are equally sensitive or personal. In this work, we are the first to address this shortcoming, asking the question of how a given privacy budget can be intelligently and sensibly distributed amongst a target document. We construct and evaluate a toolkit of linguistics- and NLP-based methods used to allocate a privacy budget to constituent tokens in a text document. In a series of privacy and utility experiments, we empirically demonstrate that given the same privacy budget, intelligent distribution leads to higher privacy levels and more positive trade-offs than a naive distribution of $\varepsilon$. Our work highlights the intricacies of text privatization with DP, and furthermore, it calls for further work on finding more efficient ways to maximize the privatization benefits offered by DP in text rewriting.</li>
</ul>

<h3>Title: MASCOTS: Model-Agnostic Symbolic COunterfactual explanations for Time Series</h3>
<ul>
<li><strong>Authors: </strong>Dawid Pudowski, Francesco Spinnato, Piotr Wilczyski, Krzysztof Kotowski, Evridiki Vasileia Ntagiou, Riccardo Guidotti, Przemysaw Biecek</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22389">https://arxiv.org/abs/2503.22389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22389">https://arxiv.org/pdf/2503.22389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22389]] MASCOTS: Model-Agnostic Symbolic COunterfactual explanations for Time Series(https://arxiv.org/abs/2503.22389)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Counterfactual explanations provide an intuitive way to understand model decisions by identifying minimal changes required to alter an outcome. However, applying counterfactual methods to time series models remains challenging due to temporal dependencies, high dimensionality, and the lack of an intuitive human-interpretable representation. We introduce MASCOTS, a method that leverages the Bag-of-Receptive-Fields representation alongside symbolic transformations inspired by Symbolic Aggregate Approximation. By operating in a symbolic feature space, it enhances interpretability while preserving fidelity to the original data and model. Unlike existing approaches that either depend on model structure or autoencoder-based sampling, MASCOTS directly generates meaningful and diverse counterfactual observations in a model-agnostic manner, operating on both univariate and multivariate data. We evaluate MASCOTS on univariate and multivariate benchmark datasets, demonstrating comparable validity, proximity, and plausibility to state-of-the-art methods, while significantly improving interpretability and sparsity. Its symbolic nature allows for explanations that can be expressed visually, in natural language, or through semantic representations, making counterfactual reasoning more accessible and actionable.</li>
</ul>

<h3>Title: Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision</h3>
<ul>
<li><strong>Authors: </strong>Rulin Zhou, Wenlong He, An Wang, Qiqi Yao, Haijun Hu, Jiankun Wang, Xi Zhang an Hongliang Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22394">https://arxiv.org/abs/2503.22394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22394">https://arxiv.org/pdf/2503.22394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22394]] Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision(https://arxiv.org/abs/2503.22394)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate tissue point tracking in endoscopic videos is critical for robotic-assisted surgical navigation and scene understanding, but remains challenging due to complex deformations, instrument occlusion, and the scarcity of dense trajectory annotations. Existing methods struggle with long-term tracking under these conditions due to limited feature utilization and annotation dependence. We present Endo-TTAP, a novel framework addressing these challenges through: (1) A Multi-Facet Guided Attention (MFGA) module that synergizes multi-scale flow dynamics, DINOv2 semantic embeddings, and explicit motion patterns to jointly predict point positions with uncertainty and occlusion awareness; (2) A two-stage curriculum learning strategy employing an Auxiliary Curriculum Adapter (ACA) for progressive initialization and hybrid supervision. Stage I utilizes synthetic data with optical flow ground truth for uncertainty-occlusion regularization, while Stage II combines unsupervised flow consistency and semi-supervised learning with refined pseudo-labels from off-the-shelf trackers. Extensive validation on two MICCAI Challenge datasets and our collected dataset demonstrates that Endo-TTAP achieves state-of-the-art performance in tissue point tracking, particularly in scenarios characterized by complex endoscopic conditions. The source code and dataset will be available at this https URL.</li>
</ul>

<h3>Title: Negation: A Pink Elephant in the Large Language Models' Room?</h3>
<ul>
<li><strong>Authors: </strong>Tereza Vrabcov, Marek Kadlk, Petr Sojka, Michal tefnik, Michal Spiegel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22395">https://arxiv.org/abs/2503.22395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22395">https://arxiv.org/pdf/2503.22395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22395]] Negation: A Pink Elephant in the Large Language Models' Room?(https://arxiv.org/abs/2503.22395)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Negations are key to determining sentence meaning, making them essential for logical reasoning. Despite their importance, negations pose a substantial challenge for large language models (LLMs) and remain underexplored. We construct two multilingual natural language inference (NLI) datasets with \textit{paired} examples differing in negation. We investigate how model size and language impact its ability to handle negation correctly by evaluating popular LLMs. Contrary to previous work, we show that increasing the model size consistently improves the models' ability to handle negations. Furthermore, we find that both the models' reasoning accuracy and robustness to negation are language-dependent and that the length and explicitness of the premise have a greater impact on robustness than language. Our datasets can facilitate further research and improvements of language model reasoning in multilingual settings.</li>
</ul>

<h3>Title: GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain</h3>
<ul>
<li><strong>Authors: </strong>Vida Adeli, Soroush Mehraban, Majid Mirmehdi, Alan Whone, Benjamin Filtjens, Amirhossein Dadashzadeh, Alfonso Fasano, Andrea Iaboni Babak Taati</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22397">https://arxiv.org/abs/2503.22397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22397">https://arxiv.org/pdf/2503.22397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22397]] GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain(https://arxiv.org/abs/2503.22397)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Gait analysis is crucial for the diagnosis and monitoring of movement disorders like Parkinson's Disease. While computer vision models have shown potential for objectively evaluating parkinsonian gait, their effectiveness is limited by scarce clinical datasets and the challenge of collecting large and well-labelled data, impacting model accuracy and risk of bias. To address these gaps, we propose GAITGen, a novel framework that generates realistic gait sequences conditioned on specified pathology severity levels. GAITGen employs a Conditional Residual Vector Quantized Variational Autoencoder to learn disentangled representations of motion dynamics and pathology-specific factors, coupled with Mask and Residual Transformers for conditioned sequence generation. GAITGen generates realistic, diverse gait sequences across severity levels, enriching datasets and enabling large-scale model training in parkinsonian gait analysis. Experiments on our new PD-GaM (real) dataset demonstrate that GAITGen outperforms adapted state-of-the-art models in both reconstruction fidelity and generation quality, accurately capturing critical pathology-specific gait features. A clinical user study confirms the realism and clinical relevance of our generated sequences. Moreover, incorporating GAITGen-generated data into downstream tasks improves parkinsonian gait severity estimation, highlighting its potential for advancing clinical gait analysis.</li>
</ul>

<h3>Title: DF-Net: The Digital Forensics Network for Image Forgery Detection</h3>
<ul>
<li><strong>Authors: </strong>David Fischinger, Martin Boyer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22398">https://arxiv.org/abs/2503.22398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22398">https://arxiv.org/pdf/2503.22398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22398]] DF-Net: The Digital Forensics Network for Image Forgery Detection(https://arxiv.org/abs/2503.22398)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The orchestrated manipulation of public opinion, particularly through manipulated images, often spread via online social networks (OSN), has become a serious threat to society. In this paper we introduce the Digital Forensics Net (DF-Net), a deep neural network for pixel-wise image forgery detection. The released model outperforms several state-of-the-art methods on four established benchmark datasets. Most notably, DF-Net's detection is robust against lossy image operations (e.g resizing, compression) as they are automatically performed by social networks.</li>
</ul>

<h3>Title: Generative Reliability-Based Design Optimization Using In-Context Learning Capabilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhonglin Jiang, Qian Tang, Zequn Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22401">https://arxiv.org/abs/2503.22401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22401">https://arxiv.org/pdf/2503.22401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22401]] Generative Reliability-Based Design Optimization Using In-Context Learning Capabilities of Large Language Models(https://arxiv.org/abs/2503.22401)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable in-context learning capabilities, enabling flexible utilization of limited historical information to play pivotal roles in reasoning, problem-solving, and complex pattern recognition tasks. Inspired by the successful applications of LLMs in multiple domains, this paper proposes a generative design method by leveraging the in-context learning capabilities of LLMs with the iterative search mechanisms of metaheuristic algorithms for solving reliability-based design optimization problems. In detail, reliability analysis is performed by engaging the LLMs and Kriging surrogate modeling to overcome the computational burden. By dynamically providing critical information of design points to the LLMs with prompt engineering, the method enables rapid generation of high-quality design alternatives that satisfy reliability constraints while achieving performance optimization. With the Deepseek-V3 model, three case studies are used to demonstrated the performance of the proposed approach. Experimental results indicate that the proposed LLM-RBDO method successfully identifies feasible solutions that meet reliability constraints while achieving a comparable convergence rate compared to traditional genetic algorithms.</li>
</ul>

<h3>Title: Training Large Language Models for Advanced Typosquatting Detection</h3>
<ul>
<li><strong>Authors: </strong>Jackson Welch</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22406">https://arxiv.org/abs/2503.22406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22406">https://arxiv.org/pdf/2503.22406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22406]] Training Large Language Models for Advanced Typosquatting Detection(https://arxiv.org/abs/2503.22406)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Typosquatting is a long-standing cyber threat that exploits human error in typing URLs to deceive users, distribute malware, and conduct phishing attacks. With the proliferation of domain names and new Top-Level Domains (TLDs), typosquatting techniques have grown more sophisticated, posing significant risks to individuals, businesses, and national cybersecurity infrastructure. Traditional detection methods primarily focus on well-known impersonation patterns, leaving gaps in identifying more complex attacks. This study introduces a novel approach leveraging large language models (LLMs) to enhance typosquatting detection. By training an LLM on character-level transformations and pattern-based heuristics rather than domain-specific data, a more adaptable and resilient detection mechanism develops. Experimental results indicate that the Phi-4 14B model outperformed other tested models when properly fine tuned achieving a 98% accuracy rate with only a few thousand training samples. This research highlights the potential of LLMs in cybersecurity applications, specifically in mitigating domain-based deception tactics, and provides insights into optimizing machine learning strategies for threat detection.</li>
</ul>

<h3>Title: Instance-Level Data-Use Auditing of Visual ML Models</h3>
<ul>
<li><strong>Authors: </strong>Zonghao Huang, Neil Zhenqiang Gong, Michael K. Reiter</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22413">https://arxiv.org/abs/2503.22413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22413">https://arxiv.org/pdf/2503.22413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22413]] Instance-Level Data-Use Auditing of Visual ML Models(https://arxiv.org/abs/2503.22413)</code><input type="text"></li>
<li><strong>Keywords: </strong>membership infer</a></li>
<li><strong>Abstract: </strong>The growing trend of legal disputes over the unauthorized use of data in machine learning (ML) systems highlights the urgent need for reliable data-use auditing mechanisms to ensure accountability and transparency in ML. In this paper, we present the first proactive instance-level data-use auditing method designed to enable data owners to audit the use of their individual data instances in ML models, providing more fine-grained auditing results. Our approach integrates any black-box membership inference technique with a sequential hypothesis test, providing a quantifiable and tunable false-detection rate. We evaluate our method on three types of visual ML models: image classifiers, visual encoders, and Contrastive Image-Language Pretraining (CLIP) models. In additional, we apply our method to evaluate the performance of two state-of-the-art approximate unlearning methods. Our findings reveal that neither method successfully removes the influence of the unlearned data instances from image classifiers and CLIP models even if sacrificing model utility by $10.33\%$.</li>
</ul>

<h3>Title: Robustness quantification and how it allows for reliable classification, even in the presence of distribution shift and for small training sets</h3>
<ul>
<li><strong>Authors: </strong>Adrin Detavernier, Jasper De Bock</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22418">https://arxiv.org/abs/2503.22418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22418">https://arxiv.org/pdf/2503.22418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22418]] Robustness quantification and how it allows for reliable classification, even in the presence of distribution shift and for small training sets(https://arxiv.org/abs/2503.22418)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Based on existing ideas in the field of imprecise probabilities, we present a new approach for assessing the reliability of the individual predictions of a generative probabilistic classifier. We call this approach robustness quantification, compare it to uncertainty quantification, and demonstrate that it continues to work well even for classifiers that are learned from small training sets that are sampled from a shifted distribution.</li>
</ul>

<h3>Title: Unveiling the Mist over 3D Vision-Language Understanding: Object-centric Evaluation with Chain-of-Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jiangyong Huang, Baoxiong Jia, Yan Wang, Ziyu Zhu, Xiongkun Linghu, Qing Li, Song-Chun Zhu, Siyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22420">https://arxiv.org/abs/2503.22420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22420">https://arxiv.org/pdf/2503.22420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22420]] Unveiling the Mist over 3D Vision-Language Understanding: Object-centric Evaluation with Chain-of-Analysis(https://arxiv.org/abs/2503.22420)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VL models, creating a "mist" that obscures rigorous insights into model capabilities and 3D-VL tasks. This mist persists due to three key limitations. First, flawed test data, like ambiguous referential text in the grounding task, can yield incorrect and unreliable test results. Second, oversimplified metrics such as simply averaging accuracy per question answering (QA) pair, cannot reveal true model capability due to their vulnerability to language variations. Third, existing benchmarks isolate the grounding and QA tasks, disregarding the underlying coherence that QA should be based on solid grounding capabilities. To unveil the "mist", we propose Beacon3D, a benchmark for 3D-VL grounding and QA tasks, delivering a perspective shift in the evaluation of 3D-VL understanding. Beacon3D features (i) high-quality test data with precise and natural language, (ii) object-centric evaluation with multiple tests per object to ensure robustness, and (iii) a novel chain-of-analysis paradigm to address language robustness and model performance coherence across grounding and QA. Our evaluation of state-of-the-art 3D-VL models on Beacon3D reveals that (i) object-centric evaluation elicits true model performance and particularly weak generalization in QA; (ii) grounding-QA coherence remains fragile in current 3D-VL models, and (iii) incorporating large language models (LLMs) to 3D-VL models, though as a prevalent practice, hinders grounding capabilities and has yet to elevate QA capabilities. We hope Beacon3D and our comprehensive analysis could benefit the 3D-VL community towards faithful developments.</li>
</ul>

<h3>Title: MVSAnywhere: Zero-Shot Multi-View Stereo</h3>
<ul>
<li><strong>Authors: </strong>Sergio Izquierdo, Mohamed Sayed, Michael Firman, Guillermo Garcia-Hernando, Daniyar Turmukhambetov, Javier Civera, Oisin Mac Aodha, Gabriel Brostow, Jamie Watson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22430">https://arxiv.org/abs/2503.22430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22430">https://arxiv.org/pdf/2503.22430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22430]] MVSAnywhere: Zero-Shot Multi-View Stereo(https://arxiv.org/abs/2503.22430)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Computing accurate depth from multiple views is a fundamental and longstanding challenge in computer vision. However, most existing approaches do not generalize well across different domains and scene types (e.g. indoor vs. outdoor). Training a general-purpose multi-view stereo model is challenging and raises several questions, e.g. how to best make use of transformer-based architectures, how to incorporate additional metadata when there is a variable number of input views, and how to estimate the range of valid depths which can vary considerably across different scenes and is typically not known a priori? To address these issues, we introduce MVSA, a novel and versatile Multi-View Stereo architecture that aims to work Anywhere by generalizing across diverse domains and depth ranges. MVSA combines monocular and multi-view cues with an adaptive cost volume to deal with scale-related issues. We demonstrate state-of-the-art zero-shot depth estimation on the Robust Multi-View Depth Benchmark, surpassing existing multi-view stereo and monocular baselines.</li>
</ul>

<h3>Title: STADE: Standard Deviation as a Pruning Metric</h3>
<ul>
<li><strong>Authors: </strong>Diego Coello de Portugal Mecke, Haya Alyoussef, Ilia Koloiarov, Maximilian Stubbemann, Lars Schmidt-Thieme</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22451">https://arxiv.org/abs/2503.22451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22451">https://arxiv.org/pdf/2503.22451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22451]] STADE: Standard Deviation as a Pruning Metric(https://arxiv.org/abs/2503.22451)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recently, Large Language Models (LLMs) have become very widespread and are used to solve a wide variety of tasks. To successfully handle these tasks, LLMs require longer training times and larger model sizes. This makes LLMs ideal candidates for pruning methods that reduce computational demands while maintaining performance. Previous methods require a retraining phase after pruning to maintain the original model's performance. However, state-of-the-art pruning methods, such as Wanda, prune the model without retraining, making the pruning process faster and more efficient. Building upon Wanda's work, this study provides a theoretical explanation of why the method is effective and leverages these insights to enhance the pruning process. Specifically, a theoretical analysis of the pruning problem reveals a common scenario in Machine Learning where Wanda is the optimal pruning method. Furthermore, this analysis is extended to cases where Wanda is no longer optimal, leading to the development of a new method, STADE, based on the standard deviation of the input. From a theoretical standpoint, STADE demonstrates better generality across different scenarios. Finally, extensive experiments on Llama and Open Pre-trained Transformers (OPT) models validate these theoretical findings, showing that depending on the training conditions, Wanda's optimal performance varies as predicted by the theoretical framework. These insights contribute to a more robust understanding of pruning strategies and their practical implications. Code is available at: this https URL</li>
</ul>

<h3>Title: A Causal Framework to Measure and Mitigate Non-binary Treatment Discrimination</h3>
<ul>
<li><strong>Authors: </strong>Ayan Majumdar, Deborah D. Kanubala, Kavya Gupta, Isabel Valera</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22454">https://arxiv.org/abs/2503.22454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22454">https://arxiv.org/pdf/2503.22454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22454]] A Causal Framework to Measure and Mitigate Non-binary Treatment Discrimination(https://arxiv.org/abs/2503.22454)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Fairness studies of algorithmic decision-making systems often simplify complex decision processes, such as bail or loan approvals, into binary classification tasks. However, these approaches overlook that such decisions are not inherently binary (e.g., approve or not approve bail or loan); they also involve non-binary treatment decisions (e.g., bail conditions or loan terms) that can influence the downstream outcomes (e.g., loan repayment or reoffending). In this paper, we argue that non-binary treatment decisions are integral to the decision process and controlled by decision-makers and, therefore, should be central to fairness analyses in algorithmic decision-making. We propose a causal framework that extends fairness analyses and explicitly distinguishes between decision-subjects' covariates and the treatment decisions. This specification allows decision-makers to use our framework to (i) measure treatment disparity and its downstream effects in historical data and, using counterfactual reasoning, (ii) mitigate the impact of past unfair treatment decisions when automating decision-making. We use our framework to empirically analyze four widely used loan approval datasets to reveal potential disparity in non-binary treatment decisions and their discriminatory impact on outcomes, highlighting the need to incorporate treatment decisions in fairness assessments. Moreover, by intervening in treatment decisions, we show that our framework effectively mitigates treatment discrimination from historical data to ensure fair risk score estimation and (non-binary) decision-making processes that benefit all stakeholders.</li>
</ul>

<h3>Title: Entropy-guided sequence weighting for efficient exploration in RL-based LLM fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Abdullah Vanlioglu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22456">https://arxiv.org/abs/2503.22456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22456">https://arxiv.org/pdf/2503.22456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22456]] Entropy-guided sequence weighting for efficient exploration in RL-based LLM fine-tuning(https://arxiv.org/abs/2503.22456)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that enhances the exploration-exploitation tradeoff by dynamically assigning weights to generated outputs based on their advantage and entropy for Reinforcement Learning-based Large Language Model fine-tuning. EGSW integrates entropy regularization with advantage-based weighting to balance policy updates, enabling efficient exploration in high-dimensional state spaces. By employing temperature-scaled softmax weighting over sequences, EGSW prioritizing high-reward, high-uncertainty steps while maintaining training stability. Although originally developed to improve Group Relative Policy Optimization (GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to other reinforcement learning (RL) algorithms and can be implemented in both step-wise and trajectory-wise settings. Empirical evaluations demonstrate that EGSW enhances GRPO reasoning ability, yielding improvements in sample efficiency. Future work will explore the application of EGSW to advanced RL methodologies.</li>
</ul>

<h3>Title: Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, Jian-guang Lou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22458">https://arxiv.org/abs/2503.22458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22458">https://arxiv.org/pdf/2503.22458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22458]] Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey(https://arxiv.org/abs/2503.22458)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This survey examines evaluation methods for large language model (LLM)-based agents in multi-turn conversational settings. Using a PRISMA-inspired framework, we systematically reviewed nearly 250 scholarly sources, capturing the state of the art from various venues of publication, and establishing a solid foundation for our analysis. Our study offers a structured approach by developing two interrelated taxonomy systems: one that defines \emph{what to evaluate} and another that explains \emph{how to evaluate}. The first taxonomy identifies key components of LLM-based agents for multi-turn conversations and their evaluation dimensions, including task completion, response quality, user experience, memory and context retention, as well as planning and tool integration. These components ensure that the performance of conversational agents is assessed in a holistic and meaningful manner. The second taxonomy system focuses on the evaluation methodologies. It categorizes approaches into annotation-based evaluations, automated metrics, hybrid strategies that combine human assessments with quantitative measures, and self-judging methods utilizing LLMs. This framework not only captures traditional metrics derived from language understanding, such as BLEU and ROUGE scores, but also incorporates advanced techniques that reflect the dynamic, interactive nature of multi-turn dialogues.</li>
</ul>

<h3>Title: SemAlign3D: Semantic Correspondence between RGB-Images through Aligning 3D Object-Class Representations</h3>
<ul>
<li><strong>Authors: </strong>Krispin Wandel, Hesheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22462">https://arxiv.org/abs/2503.22462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22462">https://arxiv.org/pdf/2503.22462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22462]] SemAlign3D: Semantic Correspondence between RGB-Images through Aligning 3D Object-Class Representations(https://arxiv.org/abs/2503.22462)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Semantic correspondence made tremendous progress through the recent advancements of large vision models (LVM). While these LVMs have been shown to reliably capture local semantics, the same can currently not be said for capturing global geometric relationships between semantic object regions. This problem leads to unreliable performance for semantic correspondence between images with extreme view variation. In this work, we aim to leverage monocular depth estimates to capture these geometric relationships for more robust and data-efficient semantic correspondence. First, we introduce a simple but effective method to build 3D object-class representations from monocular depth estimates and LVM features using a sparsely annotated image correspondence dataset. Second, we formulate an alignment energy that can be minimized using gradient descent to obtain an alignment between the 3D object-class representation and the object-class instance in the input RGB-image. Our method achieves state-of-the-art matching accuracy in multiple categories on the challenging SPair-71k dataset, increasing the PCK@0.1 score by more than 10 points on three categories and overall by 3.3 points from 85.6% to 88.9%. Additional resources and code are available at this https URL.</li>
</ul>

<h3>Title: WorkTeam: Constructing Workflows from Natural Language with Multi-Agents</h3>
<ul>
<li><strong>Authors: </strong>Hanchao Liu, Rongjun Li, Weimin Xiong, Ziyu Zhou, Wei Peng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22473">https://arxiv.org/abs/2503.22473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22473">https://arxiv.org/pdf/2503.22473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22473]] WorkTeam: Constructing Workflows from Natural Language with Multi-Agents(https://arxiv.org/abs/2503.22473)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Workflows play a crucial role in enhancing enterprise efficiency by orchestrating complex processes with multiple tools or components. However, hand-crafted workflow construction requires expert knowledge, presenting significant technical barriers. Recent advancements in Large Language Models (LLMs) have improved the generation of workflows from natural language instructions (aka NL2Workflow), yet existing single LLM agent-based methods face performance degradation on complex tasks due to the need for specialized knowledge and the strain of task-switching. To tackle these challenges, we propose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor, orchestrator, and filler agent, each with distinct roles that collaboratively enhance the conversion process. As there are currently no publicly available NL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, which includes 3,695 real-world business samples for training and evaluation. Experimental results show that our approach significantly increases the success rate of workflow construction, providing a novel and effective solution for enterprise NL2Workflow services.</li>
</ul>

<h3>Title: DeepOFormer: Deep Operator Learning with Domain-informed Features for Fatigue Life Prediction</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Li, Tanmay Sunil Kapure, Prokash Chandra Roy, Zhengtao Gan, Bo Shen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22475">https://arxiv.org/abs/2503.22475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22475">https://arxiv.org/pdf/2503.22475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22475]] DeepOFormer: Deep Operator Learning with Domain-informed Features for Fatigue Life Prediction(https://arxiv.org/abs/2503.22475)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Fatigue life characterizes the duration a material can function before failure under specific environmental conditions, and is traditionally assessed using stress-life (S-N) curves. While machine learning and deep learning offer promising results for fatigue life prediction, they face the overfitting challenge because of the small size of fatigue experimental data in specific materials. To address this challenge, we propose, DeepOFormer, by formulating S-N curve prediction as an operator learning problem. DeepOFormer improves the deep operator learning framework with a transformer-based encoder and a mean L2 relative error loss function. We also consider Stussi, Weibull, and Pascual and Meeker (PM) features as domain-informed features. These features are motivated by empirical fatigue models. To evaluate the performance of our DeepOFormer, we compare it with different deep learning models and XGBoost on a dataset with 54 S-N curves of aluminum alloys. With seven different aluminum alloys selected for testing, our DeepOFormer achieves an R2 of 0.9515, a mean absolute error of 0.2080, and a mean relative error of 0.5077, significantly outperforming state-of-the-art deep/machine learning methods including DeepONet, TabTransformer, and XGBoost, etc. The results highlight that our Deep0Former integrating with domain-informed features substantially improves prediction accuracy and generalization capabilities for fatigue life prediction in aluminum alloys.</li>
</ul>

<h3>Title: Almost Bayesian: The Fractal Dynamics of Stochastic Gradient Descent</h3>
<ul>
<li><strong>Authors: </strong>Max Hennick, Stijn De Baerdemacker</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22478">https://arxiv.org/abs/2503.22478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22478">https://arxiv.org/pdf/2503.22478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22478]] Almost Bayesian: The Fractal Dynamics of Stochastic Gradient Descent(https://arxiv.org/abs/2503.22478)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We show that the behavior of stochastic gradient descent is related to Bayesian statistics by showing that SGD is effectively diffusion on a fractal landscape, where the fractal dimension can be accounted for in a purely Bayesian way. By doing this we show that SGD can be regarded as a modified Bayesian sampler which accounts for accessibility constraints induced by the fractal structure of the loss landscape. We verify our results experimentally by examining the diffusion of weights during training. These results offer insight into the factors which determine the learning process, and seemingly answer the question of how SGD and purely Bayesian sampling are related.</li>
</ul>

<h3>Title: Probabilistic Uncertain Reward Model: A Natural Generalization of Bradley-Terry Reward Model</h3>
<ul>
<li><strong>Authors: </strong>Wangtao Sun, Xiang Cheng, Xing Yu, Haotian Xu, Zhao Yang, Shizhu He, Jun Zhao, Kang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22480">https://arxiv.org/abs/2503.22480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22480">https://arxiv.org/pdf/2503.22480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22480]] Probabilistic Uncertain Reward Model: A Natural Generalization of Bradley-Terry Reward Model(https://arxiv.org/abs/2503.22480)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical technique for training large language models. However, reward hacking-a phenomenon where models exploit flaws in the reward model-remains a significant barrier to achieving robust and scalable intelligence through long-term training. Existing studies have proposed uncertain reward model to address reward hacking, however, they often lack systematic or theoretical foundations, failing to model the uncertainty intrinsically emerging from preference data. In this paper, we propose the Probabilistic Uncertain Reward Model (PURM), a natural generalization of the classical Bradley-Terry reward model. PURM learns reward distributions directly from preference data and quantifies per-sample uncertainty via the average overlap area between reward distributions. To mitigate reward hacking, we further introduce an uncertainty-aware penalty into Proximal Policy Optimization (PPO), which leverages the learned uncertainty to dynamically balance reward optimization and exploration. We propose a lightweight and easy-to-use implementation of PURM. Experiments demonstrate that PURM significantly delays the onset of reward hacking while improving final reward performance, outperforming baseline methods in both stability and effectiveness.</li>
</ul>

<h3>Title: SPDNet: Seasonal-Periodic Decomposition Network for Advanced Residential Demand Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Reza Nematirad, Anil Pahwa, Balasubramaniam Natarajan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22485">https://arxiv.org/abs/2503.22485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22485">https://arxiv.org/pdf/2503.22485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22485]] SPDNet: Seasonal-Periodic Decomposition Network for Advanced Residential Demand Forecasting(https://arxiv.org/abs/2503.22485)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Residential electricity demand forecasting is critical for efficient energy management and grid stability. Accurate predictions enable utility companies to optimize planning and operations. However, real-world residential electricity demand data often exhibit intricate temporal variability, including multiple seasonalities, periodicities, and abrupt fluctuations, which pose significant challenges for forecasting models. Previous models that rely on statistical methods, recurrent, convolutional neural networks, and transformers often struggle to capture these intricate temporal dynamics. To address these challenges, we propose the Seasonal-Periodic Decomposition Network (SPDNet), a novel deep learning framework consisting of two main modules. The first is the Seasonal-Trend Decomposition Module (STDM), which decomposes the input data into trend, seasonal, and residual components. The second is the Periodical Decomposition Module (PDM), which employs the Fast Fourier Transform to identify the dominant periods. For each dominant period, 1D input data is reshaped into a 2D tensor, where rows represent periods and columns correspond to frequencies. The 2D representations are then processed through three submodules: a 1D convolution to capture sharp fluctuations, a transformer-based encoder to model global patterns, and a 2D convolution to capture interactions between periods. Extensive experiments conducted on real-world residential electricity load data demonstrate that SPDNet outperforms traditional and advanced models in both forecasting accuracy and computational efficiency. The code is available in this repository: this https URL.</li>
</ul>

<h3>Title: Learnable cut flow</h3>
<ul>
<li><strong>Authors: </strong>Jing Li, Hao Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, hep-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22498">https://arxiv.org/abs/2503.22498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22498">https://arxiv.org/pdf/2503.22498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22498]] Learnable cut flow(https://arxiv.org/abs/2503.22498)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Neural networks have emerged as a powerful paradigm for tasks in high energy physics, yet their opaque training process renders them as a black box. In contrast, the traditional cut flow method offers simplicity and interpretability but demands human effort to identify optimal boundaries. To merge the strengths of both approaches, we propose the Learnable Cut Flow (LCF), a neural network that transforms the traditional cut selection into a fully differentiable, data-driven process. LCF implements two cut strategies-parallel, where observable distributions are treated independently, and sequential, where prior cuts shape subsequent ones-to flexibly determine optimal boundaries. Building on this, we introduce the Learnable Importance, a metric that quantifies feature importance and adjusts their contributions to the loss accordingly, offering model-driven insights unlike ad-hoc metrics. To ensure differentiability, a modified loss function replaces hard cuts with mask operations, preserving data shape throughout the training process. LCF is tested on six varied mock datasets and a realistic diboson vs. QCD dataset. Results demonstrate that LCF (1) accurately learns cut boundaries across typical feature distributions in both parallel and sequential strategies, (2) assigns higher importance to discriminative features with minimal overlap, (3) handles redundant or correlated features robustly, and (4) performs effectively in real-world scenarios. In diboson dataset, LCF initially underperforms boosted decision trees and multiplayer perceptrons when using all observables. However, pruning less critical features-guided by learned importance-boosts its performance to match or exceed these baselines. LCF bridges the gap between traditional cut flow method and modern black-box neural networks, delivering actionable insights into the training process and feature importance.</li>
</ul>

<h3>Title: Masked Self-Supervised Pre-Training for Text Recognition Transformers on Large-Scale Datasets</h3>
<ul>
<li><strong>Authors: </strong>Martin Ki, Michal Hradi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22513">https://arxiv.org/abs/2503.22513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22513">https://arxiv.org/pdf/2503.22513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22513]] Masked Self-Supervised Pre-Training for Text Recognition Transformers on Large-Scale Datasets(https://arxiv.org/abs/2503.22513)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Self-supervised learning has emerged as a powerful approach for leveraging large-scale unlabeled data to improve model performance in various domains. In this paper, we explore masked self-supervised pre-training for text recognition transformers. Specifically, we propose two modifications to the pre-training phase: progressively increasing the masking probability, and modifying the loss function to incorporate both masked and non-masked patches. We conduct extensive experiments using a dataset of 50M unlabeled text lines for pre-training and four differently sized annotated datasets for fine-tuning. Furthermore, we compare our pre-trained models against those trained with transfer learning, demonstrating the effectiveness of the self-supervised pre-training. In particular, pre-training consistently improves the character error rate of models, in some cases up to 30 % relatively. It is also on par with transfer learning but without relying on extra annotated text lines.</li>
</ul>

<h3>Title: Assessing Foundation Models for Sea Ice Type Segmentation in Sentinel-1 SAR Imagery</h3>
<ul>
<li><strong>Authors: </strong>Samira Alkaee Taleghan, Morteza Karimzadeh, Andrew P. Barrett, Walter N. Meier, Farnoush Banaei-Kashani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22516">https://arxiv.org/abs/2503.22516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22516">https://arxiv.org/pdf/2503.22516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22516]] Assessing Foundation Models for Sea Ice Type Segmentation in Sentinel-1 SAR Imagery(https://arxiv.org/abs/2503.22516)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of sea ice types is essential for mapping and operational forecasting of sea ice conditions for safe navigation and resource extraction in ice-covered waters, as well as for understanding polar climate processes. While deep learning methods have shown promise in automating sea ice segmentation, they often rely on extensive labeled datasets which require expert knowledge and are time-consuming to create. Recently, foundation models (FMs) have shown excellent results for segmenting remote sensing images by utilizing pre-training on large datasets using self-supervised techniques. However, their effectiveness for sea ice segmentation remains unexplored, especially given sea ice's complex structures, seasonal changes, and unique spectral signatures, as well as peculiar Synthetic Aperture Radar (SAR) imagery characteristics including banding and scalloping noise, and varying ice backscatter characteristics, which are often missing in standard remote sensing pre-training datasets. In particular, SAR images over polar regions are acquired using different modes than used to capture the images at lower latitudes by the same sensors that form training datasets for FMs. This study evaluates ten remote sensing FMs for sea ice type segmentation using Sentinel-1 SAR imagery, focusing on their seasonal and spatial generalization. Among the selected models, Prithvi-600M outperforms the baseline models, while CROMA achieves a very similar performance in F1-score. Our contributions include offering a systematic methodology for selecting FMs for sea ice data analysis, a comprehensive benchmarking study on performances of FMs for sea ice segmentation with tailored performance metrics, and insights into existing gaps and future directions for improving domain-specific models in polar applications using SAR data.</li>
</ul>

<h3>Title: Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative Abilities</h3>
<ul>
<li><strong>Authors: </strong>Raman Dutt, Harleen Hanspal, Guoxuan Xia, Petru-Daniel Tudosiu, Alexander Black, Yongxin Yang, Steven McDonagh, Sarah Parisot</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22517">https://arxiv.org/abs/2503.22517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22517">https://arxiv.org/pdf/2503.22517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22517]] Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative Abilities(https://arxiv.org/abs/2503.22517)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>In this work, we undertake the challenge of augmenting the existing generative capabilities of pre-trained text-only large language models (LLMs) with multi-modal generation capability while satisfying two core constraints: C1 preserving the preservation of original language generative capabilities with negligible performance degradation, and C2 adhering to a small parameter budget to learn the new modality, ensuring scalability and efficiency. In contrast to current approaches that add dedicated modules, thereby significantly increasing the parameter count, we propose a method that leverages the underutilized capacity inherent in deep models. Specifically, we exploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source of additional capacity for learning a new modality, enabling better parameter efficiency (C1). Moreover, we preserve the original language generation capabilities by applying low-rank adaptation exclusively to the tokens of the new modality (C2). Furthermore, we introduce a novel parameter initialization scheme based on the Gromov-Wasserstein distance to improve convergence and training stability. Through an extensive analysis of the routing mechanism, we uncover the emergence of modality-specific pathways and decreased redundancy within the experts that can efficiently unlock multi-modal generative capabilities. Overall, our method can be seamlessly applied to a wide range of contemporary LLMs, providing a new pathway for transitioning from uni-modal to multi-modal architectures.</li>
</ul>

<h3>Title: MixFunn: A Neural Network for Differential Equations with Improved Generalization and Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Tiago de Souza Farias, Gubio Gomes de Lima, Jonas Maziero, Celso Jorge Villas-Boas</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.app-ph, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22528">https://arxiv.org/abs/2503.22528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22528">https://arxiv.org/pdf/2503.22528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22528]] MixFunn: A Neural Network for Differential Equations with Improved Generalization and Interpretability(https://arxiv.org/abs/2503.22528)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability</a></li>
<li><strong>Abstract: </strong>We introduce MixFunn, a novel neural network architecture designed to solve differential equations with enhanced precision, interpretability, and generalization capability. The architecture comprises two key components: the mixed-function neuron, which integrates multiple parameterized nonlinear functions to improve representational flexibility, and the second-order neuron, which combines a linear transformation of its inputs with a quadratic term to capture cross-combinations of input variables. These features significantly enhance the expressive power of the network, enabling it to achieve comparable or superior results with drastically fewer parameters and a reduction of up to four orders of magnitude compared to conventional approaches. We applied MixFunn in a physics-informed setting to solve differential equations in classical mechanics, quantum mechanics, and fluid dynamics, demonstrating its effectiveness in achieving higher accuracy and improved generalization to regions outside the training domain relative to standard machine learning models. Furthermore, the architecture facilitates the extraction of interpretable analytical expressions, offering valuable insights into the underlying solutions.</li>
</ul>

<h3>Title: LIM: Large Interpolator Model for Dynamic Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Remy Sabathier, Niloy J. Mitra, David Novotny</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22537">https://arxiv.org/abs/2503.22537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22537">https://arxiv.org/pdf/2503.22537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22537]] LIM: Large Interpolator Model for Dynamic Reconstruction(https://arxiv.org/abs/2503.22537)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Reconstructing dynamic assets from video data is central to many in computer vision and graphics tasks. Existing 4D reconstruction approaches are limited by category-specific models or slow optimization-based methods. Inspired by the recent Large Reconstruction Model (LRM), we present the Large Interpolation Model (LIM), a transformer-based feed-forward solution, guided by a novel causal consistency loss, for interpolating implicit 3D representations across time. Given implicit 3D representations at times $t_0$ and $t_1$, LIM produces a deformed shape at any continuous time $t\in[t_0,t_1]$, delivering high-quality interpolated frames in seconds. Furthermore, LIM allows explicit mesh tracking across time, producing a consistently uv-textured mesh sequence ready for integration into existing production pipelines. We also use LIM, in conjunction with a diffusion-based multiview generator, to produce dynamic 4D reconstructions from monocular videos. We evaluate LIM on various dynamic datasets, benchmarking against image-space interpolation methods (e.g., FiLM) and direct triplane linear interpolation, and demonstrate clear advantages. In summary, LIM is the first feed-forward model capable of high-speed tracked 4D asset reconstruction across diverse categories.</li>
</ul>

<h3>Title: Efficient Verified Machine Unlearning For Distillation</h3>
<ul>
<li><strong>Authors: </strong>Yijun Quan, Zushu Li, Giovanni Montana</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22539">https://arxiv.org/abs/2503.22539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22539">https://arxiv.org/pdf/2503.22539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22539]] Efficient Verified Machine Unlearning For Distillation(https://arxiv.org/abs/2503.22539)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Growing data privacy demands, driven by regulations like GDPR and CCPA, require machine unlearning methods capable of swiftly removing the influence of specific training points. Although verified approaches like SISA, using data slicing and checkpointing, achieve efficient unlearning for single models by reverting to intermediate states, these methods struggle in teacher-student knowledge distillation settings. Unlearning in the teacher typically forces costly, complete student retraining due to pervasive information propagation during distillation. Our primary contribution is PURGE (Partitioned Unlearning with Retraining Guarantee for Ensembles), a novel framework integrating verified unlearning with distillation. We introduce constituent mapping and an incremental multi-teacher strategy that partitions the distillation process, confines each teacher constituent's impact to distinct student data subsets, and crucially maintains data isolation. The PURGE framework substantially reduces retraining overhead, requiring only partial student updates when teacher-side unlearning occurs. We provide both theoretical analysis, quantifying significant speed-ups in the unlearning process, and empirical validation on multiple datasets, demonstrating that PURGE achieves these efficiency gains while maintaining student accuracy comparable to standard baselines.</li>
</ul>

<h3>Title: Bridging the Dimensional Chasm: Uncover Layer-wise Dimensional Reduction in Transformers through Token Correlation</h3>
<ul>
<li><strong>Authors: </strong>Zhuo-Yang Song, Zeyu Li, Qing-Hong Cao, Ming-xing Luo, Hua Xing Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22547">https://arxiv.org/abs/2503.22547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22547">https://arxiv.org/pdf/2503.22547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22547]] Bridging the Dimensional Chasm: Uncover Layer-wise Dimensional Reduction in Transformers through Token Correlation(https://arxiv.org/abs/2503.22547)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>The geometric evolution of token representations in large language models (LLMs) presents a fundamental paradox: while human language inherently organizes semantic information in low-dimensional spaces ($\sim 10^1$ dimensions), modern LLMs employ high-dimensional embeddings ($\sim 10^3$ dimensions) processed through Transformer architectures. To resolve this paradox, this work bridges this conceptual gap by developing a geometric framework that tracks token dynamics across Transformers layers. Through layer-wise analysis of intrinsic dimensions across multiple architectures, we reveal an expansion-contraction pattern where tokens diffuse to a "working space" and then progressively project onto lower-dimensional submanifolds. Our finding implies a negative correlation between the working space dimension and parameter-sensitive performance of the LLMs, and indicates that effective models tend to compress tokens into approximately 10-dimensional submanifolds, closely resembling human semantic spaces. This work not only advances LLM interpretability by reframing Transformers layers as projectors that mediate between high-dimensional computation and low-dimensional semantics, but also provides practical tools for model diagnostics that do not rely on task-specific evaluations.</li>
</ul>

<h3>Title: MO-CTranS: A unified multi-organ segmentation model learning from multiple heterogeneously labelled datasets</h3>
<ul>
<li><strong>Authors: </strong>Zhendi Gong, Susan Francis, Eleanor Cox, Stamatios N. Sotiropoulos, Dorothee P. Auer, Guoping Qiu, Andrew P. French, Xin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22557">https://arxiv.org/abs/2503.22557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22557">https://arxiv.org/pdf/2503.22557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22557]] MO-CTranS: A unified multi-organ segmentation model learning from multiple heterogeneously labelled datasets(https://arxiv.org/abs/2503.22557)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Multi-organ segmentation holds paramount significance in many clinical tasks. In practice, compared to large fully annotated datasets, multiple small datasets are often more accessible and organs are not labelled consistently. Normally, an individual model is trained for each of these datasets, which is not an effective way of using data for model learning. It remains challenging to train a single model that can robustly learn from several partially labelled datasets due to label conflict and data imbalance problems. We propose MO-CTranS: a single model that can overcome such problems. MO-CTranS contains a CNN-based encoder and a Transformer-based decoder, which are connected in a multi-resolution manner. Task-specific tokens are introduced in the decoder to help differentiate label discrepancies. Our method was evaluated and compared to several baseline models and state-of-the-art (SOTA) solutions on abdominal MRI datasets that were acquired in different views (i.e. axial and coronal) and annotated for different organs (i.e. liver, kidney, spleen). Our method achieved better performance (most were statistically significant) than the compared methods. Github link: this https URL.</li>
</ul>

<h3>Title: Niyama : Breaking the Silos of LLM Inference Serving</h3>
<ul>
<li><strong>Authors: </strong>Kanishk Goel, Jayashree Mohan, Nipun Kwatra, Ravi Shreyas Anupindi, Ramachandran Ramjee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22562">https://arxiv.org/abs/2503.22562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22562">https://arxiv.org/pdf/2503.22562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22562]] Niyama : Breaking the Silos of LLM Inference Serving(https://arxiv.org/abs/2503.22562)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The widespread adoption of Large Language Models (LLMs) has enabled diverse applications with very different latency requirements. Existing LLM serving frameworks rely on siloed infrastructure with coarse-grained workload segregation -- interactive and batch -- leading to inefficient resource utilization and limited support for fine-grained Quality-of-Service (QoS) differentiation. This results in operational inefficiencies, over-provisioning and poor load management during traffic surges. We present Niyama, a novel QoS-driven inference serving system that enables efficient co-scheduling of diverse workloads on shared infrastructure. Niyama introduces fine-grained QoS classification allowing applications to specify precise latency requirements, and dynamically adapts scheduling decisions based on real-time system state. Leveraging the predictable execution characteristics of LLM inference, Niyama implements a dynamic chunking mechanism to improve overall throughput while maintaining strict QoS guarantees. Additionally, Niyama employs a hybrid prioritization policy that balances fairness and efficiency, and employs selective request relegation that enables graceful service degradation during overload conditions. Our evaluation demonstrates that Niyama increases serving capacity by 32% compared to current siloed deployments, while maintaining QoS guarantees. Notably, under extreme load, our system reduces SLO violations by an order of magnitude compared to current strategies.</li>
</ul>

<h3>Title: Benchmarking Ultra-Low-Power $$NPUs</h3>
<ul>
<li><strong>Authors: </strong>Josh Millar, Yushan Huang, Sarab Sethi, Hamed Haddadi, Anil Madhavapeddy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22567">https://arxiv.org/abs/2503.22567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22567">https://arxiv.org/pdf/2503.22567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22567]] Benchmarking Ultra-Low-Power $$NPUs(https://arxiv.org/abs/2503.22567)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Efficient on-device neural network (NN) inference has various advantages over cloud-based processing, including predictable latency, enhanced privacy, greater reliability, and reduced operating costs for vendors. This has sparked the recent rapid development of microcontroller-scale NN accelerators, often referred to as neural processing units ($\mu$NPUs), designed specifically for ultra-low-power applications. In this paper we present the first comparative evaluation of a number of commercially-available $\mu$NPUs, as well as the first independent benchmarks for several of these platforms. We develop and open-source a model compilation framework to enable consistent benchmarking of quantized models across diverse $\mu$NPU hardware. Our benchmark targets end-to-end performance and includes model inference latency, power consumption, and memory overhead, alongside other factors. The resulting analysis uncovers both expected performance trends as well as surprising disparities between hardware specifications and actual performance, including $\mu$NPUs exhibiting unexpected scaling behaviors with increasing model complexity. Our framework provides a foundation for further evaluation of $\mu$NPU platforms alongside valuable insights for both hardware designers and software developers in this rapidly evolving space.</li>
</ul>

<h3>Title: Comparing Methods for Bias Mitigation in Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Barbara Hoffmann, Ruben Mayer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22569">https://arxiv.org/abs/2503.22569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22569">https://arxiv.org/pdf/2503.22569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22569]] Comparing Methods for Bias Mitigation in Graph Neural Networks(https://arxiv.org/abs/2503.22569)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative</a></li>
<li><strong>Abstract: </strong>This paper examines the critical role of Graph Neural Networks (GNNs) in data preparation for generative artificial intelligence (GenAI) systems, with a particular focus on addressing and mitigating biases. We present a comparative analysis of three distinct methods for bias mitigation: data sparsification, feature modification, and synthetic data augmentation. Through experimental analysis using the german credit dataset, we evaluate these approaches using multiple fairness metrics, including statistical parity, equality of opportunity, and false positive rates. Our research demonstrates that while all methods improve fairness metrics compared to the original dataset, stratified sampling and synthetic data augmentation using GraphSAGE prove particularly effective in balancing demographic representation while maintaining model performance. The results provide practical insights for developing more equitable AI systems while maintaining model performance.</li>
</ul>

<h3>Title: A Framework for Cryptographic Verifiability of End-to-End AI Pipelines</h3>
<ul>
<li><strong>Authors: </strong>Kar Balan, Robert Learney, Tim Wood</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22573">https://arxiv.org/abs/2503.22573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22573">https://arxiv.org/pdf/2503.22573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22573]] A Framework for Cryptographic Verifiability of End-to-End AI Pipelines(https://arxiv.org/abs/2503.22573)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The increasing integration of Artificial Intelligence across multiple industry sectors necessitates robust mechanisms for ensuring transparency, trust, and auditability of its development and deployment. This topic is particularly important in light of recent calls in various jurisdictions to introduce regulation and legislation on AI safety. In this paper, we propose a framework for complete verifiable AI pipelines, identifying key components and analyzing existing cryptographic approaches that contribute to verifiability across different stages of the AI lifecycle, from data sourcing to training, inference, and unlearning. This framework could be used to combat misinformation by providing cryptographic proofs alongside AI-generated assets to allow downstream verification of their provenance and correctness. Our findings underscore the importance of ongoing research to develop cryptographic tools that are not only efficient for isolated AI processes, but that are efficiently `linkable' across different processes within the AI pipeline, to support the development of end-to-end verifiable AI technologies.</li>
</ul>

<h3>Title: Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization</h3>
<ul>
<li><strong>Authors: </strong>Iigo Pikabea, Iaki Lacunza, Oriol Pareras, Carlos Escolano, Aitor Gonzalez-Agirre, Javier Hernando, Marta Villegas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22577">https://arxiv.org/abs/2503.22577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22577">https://arxiv.org/pdf/2503.22577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22577]] Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization(https://arxiv.org/abs/2503.22577)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Rapid advancements in Visual Language Models (VLMs) have transformed multimodal understanding but are often constrained by generating English responses regardless of the input language. This phenomenon has been termed as Image-induced Fidelity Loss (IFL) and stems from limited multimodal multilingual training data. To address this, we propose a continuous multilingual integration strategy that injects text-only multilingual data during visual instruction tuning, preserving the language model's original multilingual capabilities. Extensive evaluations demonstrate that our approach significantly improves linguistic fidelity across languages without degradation in visual performance. We also explore model merging, which improves language fidelity but comes at the cost of visual performance. In contrast, our core method achieves robust multilingual alignment without trade-offs, offering a scalable and effective path to mitigating IFL for global VLM adoption.</li>
</ul>

<h3>Title: Beyond Vanilla Fine-Tuning: Leveraging Multistage, Multilingual, and Domain-Specific Methods for Low-Resource Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Sarubi Thillainathan, Songchen Yuan, En-Shiun Annie Lee, Sanath Jayasena, Surangika Ranathunga</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22582">https://arxiv.org/abs/2503.22582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22582">https://arxiv.org/pdf/2503.22582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22582]] Beyond Vanilla Fine-Tuning: Leveraging Multistage, Multilingual, and Domain-Specific Methods for Low-Resource Machine Translation(https://arxiv.org/abs/2503.22582)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning multilingual sequence-to-sequence large language models (msLLMs) has shown promise in developing neural machine translation (NMT) systems for low-resource languages (LRLs). However, conventional single-stage fine-tuning methods struggle in extremely low-resource NMT settings, where training data is very limited. This paper contributes to artificial intelligence by proposing two approaches for adapting msLLMs in these challenging scenarios: (1) continual pre-training (CPT), where the msLLM is further trained with domain-specific monolingual data to compensate for the under-representation of LRLs, and (2) intermediate task transfer learning (ITTL), a method that fine-tunes the msLLM with both in-domain and out-of-domain parallel data to enhance its translation capabilities across various domains and tasks. As an application in engineering, these methods are implemented in NMT systems for Sinhala, Tamil, and English (six language pairs) in domain-specific, extremely low-resource settings (datasets containing fewer than 100,000 samples). Our experiments reveal that these approaches enhance translation performance by an average of +1.47 bilingual evaluation understudy (BLEU) score compared to the standard single-stage fine-tuning baseline across all translation directions. Additionally, a multi-model ensemble further improves performance by an additional BLEU score.</li>
</ul>

<h3>Title: Historical Ink: Exploring Large Language Models for Irony Detection in 19th-Century Spanish</h3>
<ul>
<li><strong>Authors: </strong>Kevin Cohen, Laura Manrique-Gmez, Rubn Manrique</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22585">https://arxiv.org/abs/2503.22585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22585">https://arxiv.org/pdf/2503.22585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22585]] Historical Ink: Exploring Large Language Models for Irony Detection in 19th-Century Spanish(https://arxiv.org/abs/2503.22585)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study explores the use of large language models (LLMs) to enhance datasets and improve irony detection in 19th-century Latin American newspapers. Two strategies were employed to evaluate the efficacy of BERT and GPT-4o models in capturing the subtle nuances nature of irony, through both multi-class and binary classification tasks. First, we implemented dataset enhancements focused on enriching emotional and contextual cues; however, these showed limited impact on historical language analysis. The second strategy, a semi-automated annotation process, effectively addressed class imbalance and augmented the dataset with high-quality annotations. Despite the challenges posed by the complexity of irony, this work contributes to the advancement of sentiment analysis through two key contributions: introducing a new historical Spanish dataset tagged for sentiment analysis and irony detection, and proposing a semi-automated annotation methodology where human expertise is crucial for refining LLMs results, enriched by incorporating historical and cultural contexts as core features.</li>
</ul>

<h3>Title: Generative Latent Neural PDE Solver using Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Zijie Li, Anthony Zhou, Amir Barati Farimani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22600">https://arxiv.org/abs/2503.22600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22600">https://arxiv.org/pdf/2503.22600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22600]] Generative Latent Neural PDE Solver using Flow Matching(https://arxiv.org/abs/2503.22600)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Autoregressive next-step prediction models have become the de-facto standard for building data-driven neural solvers to forecast time-dependent partial differential equations (PDEs). Denoise training that is closely related to diffusion probabilistic model has been shown to enhance the temporal stability of neural solvers, while its stochastic inference mechanism enables ensemble predictions and uncertainty quantification. In principle, such training involves sampling a series of discretized diffusion timesteps during both training and inference, inevitably increasing computational overhead. In addition, most diffusion models apply isotropic Gaussian noise on structured, uniform grids, limiting their adaptability to irregular domains. We propose a latent diffusion model for PDE simulation that embeds the PDE state in a lower-dimensional latent space, which significantly reduces computational costs. Our framework uses an autoencoder to map different types of meshes onto a unified structured latent grid, capturing complex geometries. By analyzing common diffusion paths, we propose to use a coarsely sampled noise schedule from flow matching for both training and testing. Numerical experiments show that the proposed model outperforms several deterministic baselines in both accuracy and long-term stability, highlighting the potential of diffusion-based approaches for robust data-driven PDE learning.</li>
</ul>

<h3>Title: Advancing DevSecOps in SMEs: Challenges and Best Practices for Secure CI/CD Pipelines</h3>
<ul>
<li><strong>Authors: </strong>Jayaprakashreddy Cheenepalli, John D. Hastings, Khandaker Mamun Ahmed, Chad Fenner</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22612">https://arxiv.org/abs/2503.22612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22612">https://arxiv.org/pdf/2503.22612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22612]] Advancing DevSecOps in SMEs: Challenges and Best Practices for Secure CI/CD Pipelines(https://arxiv.org/abs/2503.22612)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, diffusion</a></li>
<li><strong>Abstract: </strong>This study evaluates the adoption of DevSecOps among small and medium-sized enterprises (SMEs), identifying key challenges, best practices, and future trends. Through a mixed methods approach backed by the Technology Acceptance Model (TAM) and Diffusion of Innovations (DOI) theory, we analyzed survey data from 405 SME professionals, revealing that while 68% have implemented DevSecOps, adoption is hindered by technical complexity (41%), resource constraints (35%), and cultural resistance (38%). Despite strong leadership prioritization of security (73%), automation gaps persist, with only 12% of organizations conducting security scans per commit. Our findings highlight a growing integration of security tools, particularly API security (63%) and software composition analysis (62%), although container security adoption remains low (34%). Looking ahead, SMEs anticipate artificial intelligence and machine learning to significantly influence DevSecOps, underscoring the need for proactive adoption of AI-driven security enhancements. Based on our findings, this research proposes strategic best practices to enhance CI/CD pipeline security including automation, leadership-driven security culture, and cross-team collaboration.</li>
</ul>

<h3>Title: Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Jangho Park, Taesung Kwon, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22622">https://arxiv.org/abs/2503.22622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22622">https://arxiv.org/pdf/2503.22622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22622]] Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion Model(https://arxiv.org/abs/2503.22622)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, multi-view or 4D video generation has emerged as a significant research topic. Nonetheless, recent approaches to 4D generation still struggle with fundamental limitations, as they primarily rely on harnessing multiple video diffusion models with additional training or compute-intensive training of a full 4D diffusion model with limited real-world 4D data and large computational costs. To address these challenges, here we propose the first training-free 4D video generation method that leverages the off-the-shelf video diffusion models to generate multi-view videos from a single input video. Our approach consists of two key steps: (1) By designating the edge frames in the spatio-temporal sampling grid as key frames, we first synthesize them using a video diffusion model, leveraging a depth-based warping technique for guidance. This approach ensures structural consistency across the generated frames, preserving spatial and temporal coherence. (2) We then interpolate the remaining frames using a video diffusion model, constructing a fully populated and temporally coherent sampling grid while preserving spatial and temporal consistency. Through this approach, we extend a single video into a multi-view video along novel camera trajectories while maintaining spatio-temporal consistency. Our method is training-free and fully utilizes an off-the-shelf video diffusion model, offering a practical and effective solution for multi-view video generation.</li>
</ul>

<h3>Title: Tropical Bisectors and Carlini-Wagner Attacks</h3>
<ul>
<li><strong>Authors: </strong>Gillian Grindstaff, Julia Lindberg, Daniela Schkoda, Miruna-Stefana Sorea, Ruriko Yoshida</a></li>
<li><strong>Subjects: </strong>cs.LG, math.AG, math.CO, math.MG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22653">https://arxiv.org/abs/2503.22653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22653">https://arxiv.org/pdf/2503.22653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22653]] Tropical Bisectors and Carlini-Wagner Attacks(https://arxiv.org/abs/2503.22653)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Pasque et al. showed that using a tropical symmetric metric as an activation function in the last layer can improve the robustness of convolutional neural networks (CNNs) against state-of-the-art attacks, including the Carlini-Wagner attack. This improvement occurs when the attacks are not specifically adapted to the non-differentiability of the tropical layer. Moreover, they showed that the decision boundary of a tropical CNN is defined by tropical bisectors. In this paper, we explore the combinatorics of tropical bisectors and analyze how the tropical embedding layer enhances robustness against Carlini-Wagner attacks. We prove an upper bound on the number of linear segments the decision boundary of a tropical CNN can have. We then propose a refined version of the Carlini-Wagner attack, specifically tailored for the tropical architecture. Computational experiments with MNIST and LeNet5 showcase our attacks improved success rate.</li>
</ul>

<h3>Title: TranSplat: Lighting-Consistent Cross-Scene Object Transfer with 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Boyang (Tony)Yu, Yanlin Jin, Ashok Veeraraghavan, Akshat Dave, Guha Balakrishnan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22676">https://arxiv.org/abs/2503.22676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22676">https://arxiv.org/pdf/2503.22676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22676]] TranSplat: Lighting-Consistent Cross-Scene Object Transfer with 3D Gaussian Splatting(https://arxiv.org/abs/2503.22676)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>We present TranSplat, a 3D scene rendering algorithm that enables realistic cross-scene object transfer (from a source to a target scene) based on the Gaussian Splatting framework. Our approach addresses two critical challenges: (1) precise 3D object extraction from the source scene, and (2) faithful relighting of the transferred object in the target scene without explicit material property estimation. TranSplat fits a splatting model to the source scene, using 2D object masks to drive fine-grained 3D segmentation. Following user-guided insertion of the object into the target scene, along with automatic refinement of position and orientation, TranSplat derives per-Gaussian radiance transfer functions via spherical harmonic analysis to adapt the object's appearance to match the target scene's lighting environment. This relighting strategy does not require explicitly estimating physical scene properties such as BRDFs. Evaluated on several synthetic and real-world scenes and objects, TranSplat yields excellent 3D object extractions and relighting performance compared to recent baseline methods and visually convincing cross-scene object transfers. We conclude by discussing the limitations of the approach.</li>
</ul>

<h3>Title: DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness</h3>
<ul>
<li><strong>Authors: </strong>Ruining Li, Chuanxia Zheng, Christian Rupprecht, Andrea Vedaldi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22677">https://arxiv.org/abs/2503.22677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22677">https://arxiv.org/pdf/2503.22677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22677]] DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness(https://arxiv.org/abs/2503.22677)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Most 3D object generators focus on aesthetic quality, often neglecting physical constraints necessary in applications. One such constraint is that the 3D object should be self-supporting, i.e., remains balanced under gravity. Prior approaches to generating stable 3D objects used differentiable physics simulators to optimize geometry at test-time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models to external feedback, we propose Direct Simulation Optimization (DSO), a framework to use the feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator outputs stable 3D objects directly. We construct a dataset of 3D objects labeled with a stability score obtained from the physics simulator. We can then fine-tune the 3D generator using the stability score as the alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO), a novel objective, which we introduce, to align diffusion models without requiring pairwise preferences. Our experiments show that the fine-tuned feed-forward generator, using either DPO or DRO objective, is much faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework works even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs.</li>
</ul>

<h3>Title: Q-Insight: Understanding Image Quality via Visual Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22679">https://arxiv.org/abs/2503.22679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22679">https://arxiv.org/pdf/2503.22679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22679]] Q-Insight: Understanding Image Quality via Visual Reinforcement Learning(https://arxiv.org/abs/2503.22679)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Image quality assessment (IQA) focuses on the perceptual visual quality of images, playing a crucial role in downstream tasks such as image reconstruction, compression, and generation. The rapid advancement of multi-modal large language models (MLLMs) has significantly broadened the scope of IQA, moving toward comprehensive image quality understanding that incorporates content analysis, degradation perception, and comparison reasoning beyond mere numerical scoring. Previous MLLM-based methods typically either generate numerical scores lacking interpretability or heavily rely on supervised fine-tuning (SFT) using large-scale annotated datasets to provide descriptive assessments, limiting their flexibility and applicability. In this paper, we propose Q-Insight, a reinforcement learning-based model built upon group relative policy optimization (GRPO), which demonstrates strong visual reasoning capability for image quality understanding while requiring only a limited amount of rating scores and degradation labels. By jointly optimizing score regression and degradation perception tasks with carefully designed reward functions, our approach effectively exploits their mutual benefits for enhanced performance. Extensive experiments demonstrate that Q-Insight substantially outperforms existing state-of-the-art methods in both score regression and degradation perception tasks, while exhibiting impressive zero-shot generalization to comparison reasoning tasks. Code will be available at this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
