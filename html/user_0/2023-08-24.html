<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing. (arXiv:2308.12061v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12061">http://arxiv.org/abs/2308.12061</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12061]] HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing(http://arxiv.org/abs/2308.12061)</code></li>
<li>Summary: <p>Small farms contribute to a large share of the productive land in developing
countries. In regions such as sub-Saharan Africa, where 80% of farms are small
(under 2 ha in size), the task of mapping smallholder cropland is an important
part of tracking sustainability measures such as crop productivity. However,
the visually diverse and nuanced appearance of small farms has limited the
effectiveness of traditional approaches to cropland mapping. Here we introduce
a new approach based on the detection of harvest piles characteristic of many
smallholder systems throughout the world. We present HarvestNet, a dataset for
mapping the presence of farms in the Ethiopian regions of Tigray and Amhara
during 2020-2023, collected using expert knowledge and satellite images,
totaling 7k hand-labeled images and 2k ground collected labels. We also
benchmark a set of baselines including SOTA models in remote sensing with our
best models having around 80% classification performance on hand labelled data
and 90%, 98% accuracy on ground truth data for Tigray, Amhara respectively. We
also perform a visual comparison with a widely used pre-existing coverage map
and show that our model detects an extra 56,621 hectares of cropland in Tigray.
We conclude that remote sensing of harvest piles can contribute to more timely
and accurate cropland assessments in food insecure region.
</p></li>
</ul>

<h3>Title: Empirical Analysis of Software Vulnerabilities Causing Timing Side Channels. (arXiv:2308.11862v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11862">http://arxiv.org/abs/2308.11862</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11862]] Empirical Analysis of Software Vulnerabilities Causing Timing Side Channels(http://arxiv.org/abs/2308.11862)</code></li>
<li>Summary: <p>Timing attacks are considered one of the most damaging side-channel attacks.
These attacks exploit timing fluctuations caused by certain operations to
disclose confidential information to an attacker. For instance, in asymmetric
encryption, operations such as multiplication and division can cause
time-varying execution times that can be ill-treated to obtain an encryption
key. Whilst several efforts have been devoted to exploring the various aspects
of timing attacks, particularly in cryptography, little attention has been paid
to empirically studying the timing attack-related vulnerabilities in
non-cryptographic software. By inspecting these software vulnerabilities, this
study aims to gain an evidence-based understanding of weaknesses in
non-cryptographic software that may help timing attacks succeed. We used
qualitative and quantitative research approaches to systematically study the
timing attack-related vulnerabilities reported in the National Vulnerability
Database (NVD) from March 2003 to December 2022. Our analysis was focused on
the modifications made to the code for patching the identified vulnerabilities.
We found that a majority of the timing attack-related vulnerabilities were
introduced due to not following known secure coding practices. The findings of
this study are expected to help the software security community gain
evidence-based information about the nature and causes of the vulnerabilities
related to timing attacks.
</p></li>
</ul>

<h3>Title: PARseL: Towards a Verified Root-of-Trust over seL4. (arXiv:2308.11921v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11921">http://arxiv.org/abs/2308.11921</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11921]] PARseL: Towards a Verified Root-of-Trust over seL4(http://arxiv.org/abs/2308.11921)</code></li>
<li>Summary: <p>Widespread adoption and growing popularity of embedded/IoT/CPS devices make
them attractive attack targets. On low-to-mid-range devices, security features
are typically few or none due to various constraints. Such devices are thus
subject to malware-based compromise. One popular defensive measure is Remote
Attestation (RA) which allows a trusted entity to determine the current
software integrity of an untrusted remote device.
</p>
<p>For higher-end devices, RA is achievable via secure hardware components. For
low-end (bare metal) devices, minimalistic hybrid (hardware/software) RA is
effective, which incurs some hardware modifications. That leaves certain
mid-range devices (e.g., ARM Cortex-A family) equipped with standard hardware
components, e.g., a memory management unit (MMU) and perhaps a secure boot
facility. In this space, seL4 (a verified microkernel with guaranteed process
isolation) is a promising platform for attaining RA. HYDRA made a first step
towards this, albeit without achieving any verifiability or provable
guarantees.
</p>
<p>This paper picks up where HYDRA left off by constructing a PARseL
architecture, that separates all user-dependent components from the TCB. This
leads to much stronger isolation guarantees, based on seL4 alone, and
facilitates formal verification. In PARseL, We use formal verification to
obtain several security properties for the isolated RA TCB, including: memory
safety, functional correctness, and secret independence. We implement PARseL in
F* and specify/prove expected properties using Hoare logic. Next, we
automatically translate the F* implementation to C using KaRaMeL, which
preserves verified properties of PARseL C implementation (atop seL4). Finally,
we instantiate and evaluate PARseL on a commodity platform -- a SabreLite
embedded device.
</p></li>
</ul>

<h3>Title: CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures. (arXiv:2308.12031v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12031">http://arxiv.org/abs/2308.12031</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12031]] CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures(http://arxiv.org/abs/2308.12031)</code></li>
<li>Summary: <p>The availability of large data sets is providing an impetus for driving
current artificial intelligent developments. There are, however, challenges for
developing solutions with small data sets due to practical and cost-effective
deployment and the opacity of deep learning models. The Comprehensive
Abstraction and Classification Tool for Uncovering Structures called CACTUS is
presented for improved secure analytics by effectively employing explainable
artificial intelligence. It provides additional support for categorical
attributes, preserving their original meaning, optimising memory usage, and
speeding up the computation through parallelisation. It shows to the user the
frequency of the attributes in each class and ranks them by their
discriminative power. Its performance is assessed by application to the
Wisconsin diagnostic breast cancer and Thyroid0387 data sets.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets. (arXiv:2308.11880v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11880">http://arxiv.org/abs/2308.11880</a></li>
<li>Code URL: https://github.com/csimo005/summit</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11880]] SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets(http://arxiv.org/abs/2308.11880)</code></li>
<li>Summary: <p>Scene understanding using multi-modal data is necessary in many applications,
e.g., autonomous navigation. To achieve this in a variety of situations,
existing models must be able to adapt to shifting data distributions without
arduous data annotation. Current approaches assume that the source data is
available during adaptation and that the source consists of paired multi-modal
data. Both these assumptions may be problematic for many applications. Source
data may not be available due to privacy, security, or economic concerns.
Assuming the existence of paired multi-modal data for training also entails
significant data collection costs and fails to take advantage of widely
available freely distributed pre-trained uni-modal models. In this work, we
relax both of these assumptions by addressing the problem of adapting a set of
models trained independently on uni-modal data to a target domain consisting of
unlabeled multi-modal data, without having access to the original source
dataset. Our proposed approach solves this problem through a switching
framework which automatically chooses between two complementary methods of
cross-modal pseudo-label fusion -- agreement filtering and entropy weighting --
based on the estimated domain gap. We demonstrate our work on the semantic
segmentation problem. Experiments across seven challenging adaptation scenarios
verify the efficacy of our approach, achieving results comparable to, and in
some cases outperforming, methods which assume access to source data. Our
method achieves an improvement in mIoU of up to 12% over competing baselines.
Our code is publicly available at https://github.com/csimo005/SUMMIT.
</p></li>
</ul>

<h3>Title: Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments. (arXiv:2308.12086v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12086">http://arxiv.org/abs/2308.12086</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12086]] Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments(http://arxiv.org/abs/2308.12086)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have gained widespread popularity across diverse
domains involving text generation, summarization, and various natural language
processing tasks. Despite their inherent limitations, LLM-based designs have
shown promising capabilities in planning and navigating open-world scenarios.
This paper introduces a novel application of pre-trained LLMs as agents within
cybersecurity network environments, focusing on their utility for sequential
decision-making processes.
</p>
<p>We present an approach wherein pre-trained LLMs are leveraged as attacking
agents in two reinforcement learning environments. Our proposed agents
demonstrate similar or better performance against state-of-the-art agents
trained for thousands of episodes in most scenarios and configurations. In
addition, the best LLM agents perform similarly to human testers of the
environment without any additional training process. This design highlights the
potential of LLMs to efficiently address complex decision-making tasks within
cybersecurity.
</p>
<p>Furthermore, we introduce a new network security environment named
NetSecGame. The environment is designed to eventually support complex
multi-agent scenarios within the network security domain. The proposed
environment mimics real network attacks and is designed to be highly modular
and adaptable for various scenarios.
</p></li>
</ul>

<h3>Title: Unleashing IoT Security: Assessing the Effectiveness of Best Practices in Protecting Against Threats. (arXiv:2308.12072v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12072">http://arxiv.org/abs/2308.12072</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12072]] Unleashing IoT Security: Assessing the Effectiveness of Best Practices in Protecting Against Threats(http://arxiv.org/abs/2308.12072)</code></li>
<li>Summary: <p>The Internet of Things (IoT) market is rapidly growing and is expected to
double from 2020 to 2025. The increasing use of IoT devices, particularly in
smart homes, raises crucial concerns about user privacy and security as these
devices often handle sensitive and critical information. Inadequate security
designs and implementations by IoT vendors can lead to significant
vulnerabilities.
</p>
<p>To address these IoT device vulnerabilities, institutions, and organizations
have published IoT security best practices (BPs) to guide manufacturers in
ensuring the security of their products. However, there is currently no
standardized approach for evaluating the effectiveness of individual BP
recommendations. This leads to manufacturers investing effort in implementing
less effective BPs while potentially neglecting measures with greater impact.
</p>
<p>In this paper, we propose a methodology for evaluating the security impact of
IoT BPs and ranking them based on their effectiveness in protecting against
security threats. Our approach involves translating identified BPs into
concrete test cases that can be applied to real-world IoT devices to assess
their effectiveness in mitigating vulnerabilities. We applied this methodology
to evaluate the security impact of nine commodity IoT products, discovering 18
vulnerabilities. By empirically assessing the actual impact of BPs on device
security, IoT designers and implementers can prioritize their security
investments more effectively, improving security outcomes and optimizing
limited security budgets.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Towards Privacy-Supporting Fall Detection via Deep Unsupervised RGB2Depth Adaptation. (arXiv:2308.12049v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12049">http://arxiv.org/abs/2308.12049</a></li>
<li>Code URL: https://github.com/1015206533/privacy_supporting_fall_detection</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12049]] Towards Privacy-Supporting Fall Detection via Deep Unsupervised RGB2Depth Adaptation(http://arxiv.org/abs/2308.12049)</code></li>
<li>Summary: <p>Fall detection is a vital task in health monitoring, as it allows the system
to trigger an alert and therefore enabling faster interventions when a person
experiences a fall. Although most previous approaches rely on standard RGB
video data, such detailed appearance-aware monitoring poses significant privacy
concerns. Depth sensors, on the other hand, are better at preserving privacy as
they merely capture the distance of objects from the sensor or camera, omitting
color and texture information. In this paper, we introduce a privacy-supporting
solution that makes the RGB-trained model applicable in depth domain and
utilizes depth data at test time for fall detection. To achieve cross-modal
fall detection, we present an unsupervised RGB to Depth (RGB2Depth) cross-modal
domain adaptation approach that leverages labelled RGB data and unlabelled
depth data during training. Our proposed pipeline incorporates an intermediate
domain module for feature bridging, modality adversarial loss for modality
discrimination, classification loss for pseudo-labeled depth data and labeled
source data, triplet loss that considers both source and target domains, and a
novel adaptive loss weight adjustment method for improved coordination among
various losses. Our approach achieves state-of-the-art results in the
unsupervised RGB2Depth domain adaptation task for fall detection. Code is
available at https://github.<a href="http://export.arxiv.org/abs/com/1015206">com/1015206</a>533/privacy_supporting_fall_detection.
</p></li>
</ul>

<h3>Title: Towards an On-device Agent for Text Rewriting. (arXiv:2308.11807v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11807">http://arxiv.org/abs/2308.11807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11807]] Towards an On-device Agent for Text Rewriting(http://arxiv.org/abs/2308.11807)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated impressive capabilities for
text rewriting. Nonetheless, the large sizes of these models make them
impractical for on-device inference, which would otherwise allow for enhanced
privacy and economical inference. Creating a smaller yet potent language model
for text rewriting presents a formidable challenge because it requires
balancing the need for a small size with the need to retain the emergent
capabilities of the LLM, that requires costly data collection. To address the
above challenge, we introduce a new instruction tuning approach for building a
mobile-centric text rewriting model. Our strategies enable the generation of
high quality training data without any human labeling. In addition, we propose
a heuristic reinforcement learning framework which substantially enhances
performance without requiring preference data. To further bridge the
performance gap with the larger server-side model, we propose an effective
approach that combines the mobile rewrite agent with the server model using a
cascade. To tailor the text rewriting tasks to mobile scenarios, we introduce
MessageRewriteEval, a benchmark that focuses on text rewriting for messages
through natural language instructions. Through empirical experiments, we
demonstrate that our on-device model surpasses the current state-of-the-art
LLMs in text rewriting while maintaining a significantly reduced model size.
Notably, we show that our proposed cascading approach improves model
performance.
</p></li>
</ul>

<h3>Title: Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in Private SGD. (arXiv:2308.12018v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12018">http://arxiv.org/abs/2308.12018</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12018]] Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in Private SGD(http://arxiv.org/abs/2308.12018)</code></li>
<li>Summary: <p>Differentially private SGD (DP-SGD) holds the promise of enabling the safe
and responsible application of machine learning to sensitive datasets. However,
DP-SGD only provides a biased, noisy estimate of a mini-batch gradient. This
renders optimisation steps less effective and limits model utility as a result.
With this work, we show a connection between per-sample gradient norms and the
estimation bias of the private gradient oracle used in DP-SGD. Here, we propose
Bias-Aware Minimisation (BAM) that allows for the provable reduction of private
gradient estimator bias. We show how to efficiently compute quantities needed
for BAM to scale to large neural networks and highlight similarities to closely
related methods such as Sharpness-Aware Minimisation. Finally, we provide
empirical evidence that BAM not only reduces bias but also substantially
improves privacy-utility trade-offs on the CIFAR-10, CIFAR-100, and ImageNet-32
datasets.
</p></li>
</ul>

<h3>Title: ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy. (arXiv:2308.12210v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12210">http://arxiv.org/abs/2308.12210</a></li>
<li>Code URL: https://github.com/fumiyukikato/uldp-fl</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12210]] ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy(http://arxiv.org/abs/2308.12210)</code></li>
<li>Summary: <p>Differentially Private Federated Learning (DP-FL) has garnered attention as a
collaborative machine learning approach that ensures formal privacy. Most DP-FL
approaches ensure DP at the record-level within each silo for cross-silo FL.
However, a single user's data may extend across multiple silos, and the desired
user-level DP guarantee for such a setting remains unknown. In this study, we
present ULDP-FL, a novel FL framework designed to guarantee user-level DP in
cross-silo FL where a single user's data may belong to multiple silos. Our
proposed algorithm directly ensures user-level DP through per-user weighted
clipping, departing from group-privacy approaches. We provide a theoretical
analysis of the algorithm's privacy and utility. Additionally, we enhance the
algorithm's utility and showcase its private implementation using cryptographic
building blocks. Empirical experiments on real-world datasets show substantial
improvements in our methods in privacy-utility trade-offs under user-level DP
compared to baseline methods. To the best of our knowledge, our work is the
first FL framework that effectively provides user-level DP in the general
cross-silo FL setting.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: How to Protect Copyright Data in Optimization of Large Language Models?. (arXiv:2308.12247v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12247">http://arxiv.org/abs/2308.12247</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12247]] How to Protect Copyright Data in Optimization of Large Language Models?(http://arxiv.org/abs/2308.12247)</code></li>
<li>Summary: <p>Large language models (LLMs) and generative AI have played a transformative
role in computer research and applications. Controversy has arisen as to
whether these models output copyrighted data, which can occur if the data the
models are trained on is copyrighted. LLMs are built on the transformer neural
network architecture, which in turn relies on a mathematical computation called
Attention that uses the softmax function.
</p>
<p>In this paper, we show that large language model training and optimization
can be seen as a softmax regression problem. We then establish a method of
efficiently performing softmax regression, in a way that prevents the
regression function from generating copyright data. This establishes a
theoretical method of training large language models in a way that avoids
generating copyright data.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification. (arXiv:2308.11822v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11822">http://arxiv.org/abs/2308.11822</a></li>
<li>Code URL: https://github.com/xaiveryuan/patchbackdoor</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11822]] PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification(http://arxiv.org/abs/2308.11822)</code></li>
<li>Summary: <p>Backdoor attack is a major threat to deep learning systems in safety-critical
scenarios, which aims to trigger misbehavior of neural network models under
attacker-controlled conditions. However, most backdoor attacks have to modify
the neural network models through training with poisoned data and/or direct
model editing, which leads to a common but false belief that backdoor attack
can be easily avoided by properly protecting the model. In this paper, we show
that backdoor attacks can be achieved without any model modification. Instead
of injecting backdoor logic into the training data or the model, we propose to
place a carefully-designed patch (namely backdoor patch) in front of the
camera, which is fed into the model together with the input images. The patch
can be trained to behave normally at most of the time, while producing wrong
prediction when the input image contains an attacker-controlled trigger object.
Our main techniques include an effective training method to generate the
backdoor patch and a digital-physical transformation modeling method to enhance
the feasibility of the patch in real deployments. Extensive experiments show
that PatchBackdoor can be applied to common deep learning models (VGG,
MobileNet, ResNet) with an attack success rate of 93% to 99% on classification
tasks. Moreover, we implement PatchBackdoor in real-world scenarios and show
that the attack is still threatening.
</p></li>
</ul>

<h3>Title: Does Physical Adversarial Example Really Matter to Autonomous Driving? Towards System-Level Effect of Adversarial Object Evasion Attack. (arXiv:2308.11894v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11894">http://arxiv.org/abs/2308.11894</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11894]] Does Physical Adversarial Example Really Matter to Autonomous Driving? Towards System-Level Effect of Adversarial Object Evasion Attack(http://arxiv.org/abs/2308.11894)</code></li>
<li>Summary: <p>In autonomous driving (AD), accurate perception is indispensable to achieving
safe and secure driving. Due to its safety-criticality, the security of AD
perception has been widely studied. Among different attacks on AD perception,
the physical adversarial object evasion attacks are especially severe. However,
we find that all existing literature only evaluates their attack effect at the
targeted AI component level but not at the system level, i.e., with the entire
system semantics and context such as the full AD pipeline. Thereby, this raises
a critical research question: can these existing researches effectively achieve
system-level attack effects (e.g., traffic rule violations) in the real-world
AD context? In this work, we conduct the first measurement study on whether and
how effectively the existing designs can lead to system-level effects,
especially for the STOP sign-evasion attacks due to their popularity and
severity. Our evaluation results show that all the representative prior works
cannot achieve any system-level effects. We observe two design limitations in
the prior works: 1) physical model-inconsistent object size distribution in
pixel sampling and 2) lack of vehicle plant model and AD system model
consideration. Then, we propose SysAdv, a novel system-driven attack design in
the AD context and our evaluation results show that the system-level effects
can be significantly improved, i.e., the violation rate increases by around
70%.
</p></li>
</ul>

<h3>Title: A Probabilistic Fluctuation based Membership Inference Attack for Generative Models. (arXiv:2308.12143v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12143">http://arxiv.org/abs/2308.12143</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12143]] A Probabilistic Fluctuation based Membership Inference Attack for Generative Models(http://arxiv.org/abs/2308.12143)</code></li>
<li>Summary: <p>Membership Inference Attack (MIA) identifies whether a record exists in a
machine learning model's training set by querying the model. MIAs on the
classic classification models have been well-studied, and recent works have
started to explore how to transplant MIA onto generative models. Our
investigation indicates that existing MIAs designed for generative models
mainly depend on the overfitting in target models. However, overfitting can be
avoided by employing various regularization techniques, whereas existing MIAs
demonstrate poor performance in practice. Unlike overfitting, memorization is
essential for deep learning models to attain optimal performance, making it a
more prevalent phenomenon. Memorization in generative models leads to an
increasing trend in the probability distribution of generating records around
the member record. Therefore, we propose a Probabilistic Fluctuation Assessing
Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by
detecting these trends via analyzing the overall probabilistic fluctuations
around given records. We conduct extensive experiments across multiple
generative models and datasets, which demonstrate PFAMI can improve the attack
success rate (ASR) by about 27.9% when compared with the best baseline.
</p></li>
</ul>

<h3>Title: Multi-Instance Adversarial Attack on GNN-Based Malicious Domain Detection. (arXiv:2308.11754v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11754">http://arxiv.org/abs/2308.11754</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11754]] Multi-Instance Adversarial Attack on GNN-Based Malicious Domain Detection(http://arxiv.org/abs/2308.11754)</code></li>
<li>Summary: <p>Malicious domain detection (MDD) is an open security challenge that aims to
detect if an Internet domain is associated with cyber-attacks. Among many
approaches to this problem, graph neural networks (GNNs) are deemed highly
effective. GNN-based MDD uses DNS logs to represent Internet domains as nodes
in a maliciousness graph (DMG) and trains a GNN to infer their maliciousness by
leveraging identified malicious domains. Since this method relies on accessible
DNS logs to construct DMGs, it exposes a vulnerability for adversaries to
manipulate their domain nodes' features and connections within DMGs. Existing
research mainly concentrates on threat models that manipulate individual
attacker nodes. However, adversaries commonly generate multiple domains to
achieve their goals economically and avoid detection. Their objective is to
evade discovery across as many domains as feasible. In this work, we call the
attack that manipulates several nodes in the DMG concurrently a multi-instance
evasion attack. We present theoretical and empirical evidence that the existing
single-instance evasion techniques for are inadequate to launch multi-instance
evasion attacks against GNN-based MDDs. Therefore, we introduce MintA, an
inference-time multi-instance adversarial attack on GNN-based MDDs. MintA
enhances node and neighborhood evasiveness through optimized perturbations and
operates successfully with only black-box access to the target model,
eliminating the need for knowledge about the model's specifics or non-adversary
nodes. We formulate an optimization challenge for MintA, achieving an
approximate solution. Evaluating MintA on a leading GNN-based MDD technique
with real-world data showcases an attack success rate exceeding 80%. These
findings act as a warning for security experts, underscoring GNN-based MDDs'
susceptibility to practical attacks that can undermine their effectiveness and
benefits.
</p></li>
</ul>

<h3>Title: Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings. (arXiv:2308.11804v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11804">http://arxiv.org/abs/2308.11804</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11804]] Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings(http://arxiv.org/abs/2308.11804)</code></li>
<li>Summary: <p>Multi-modal encoders map images, sounds, texts, videos, etc. into a single
embedding space, aligning representations across modalities (e.g., associate an
image of a dog with a barking sound). We show that multi-modal embeddings can
be vulnerable to an attack we call "adversarial illusions." Given an input in
any modality, an adversary can perturb it so as to make its embedding close to
that of an arbitrary, adversary-chosen input in another modality. Illusions
thus enable the adversary to align any image with any text, any text with any
sound, etc.
</p>
<p>Adversarial illusions exploit proximity in the embedding space and are thus
agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how
adversarially aligned inputs, generated without knowledge of specific
downstream tasks, mislead image generation, text generation, and zero-shot
classification.
</p></li>
</ul>

<h3>Title: SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks. (arXiv:2308.11845v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11845">http://arxiv.org/abs/2308.11845</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11845]] SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks(http://arxiv.org/abs/2308.11845)</code></li>
<li>Summary: <p>Machine Learning (ML) systems are vulnerable to adversarial examples,
particularly those from query-based black-box attacks. Despite various efforts
to detect and prevent such attacks, there is a need for a more comprehensive
approach to logging, analyzing, and sharing evidence of attacks. While classic
security benefits from well-established forensics and intelligence sharing,
Machine Learning is yet to find a way to profile its attackers and share
information about them. In response, this paper introduces SEA, a novel ML
security system to characterize black-box attacks on ML systems for forensic
purposes and to facilitate human-explainable intelligence sharing. SEA
leverages the Hidden Markov Models framework to attribute the observed query
sequence to known attacks. It thus understands the attack's progression rather
than just focusing on the final adversarial examples. Our evaluations reveal
that SEA is effective at attack attribution, even on their second occurrence,
and is robust to adaptive strategies designed to evade forensics analysis.
Interestingly, SEA's explanations of the attack behavior allow us even to
fingerprint specific minor implementation bugs in attack libraries. For
example, we discover that the SignOPT and Square attacks implementation in ART
v1.14 sends over 50% specific zero difference queries. We thoroughly evaluate
SEA on a variety of settings and demonstrate that it can recognize the same
attack's second occurrence with 90+% Top-1 and 95+% Top-3 accuracy.
</p></li>
</ul>

<h3>Title: Adversarial Training Using Feedback Loops. (arXiv:2308.11881v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11881">http://arxiv.org/abs/2308.11881</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11881]] Adversarial Training Using Feedback Loops(http://arxiv.org/abs/2308.11881)</code></li>
<li>Summary: <p>Deep neural networks (DNN) have found wide applicability in numerous fields
due to their ability to accurately learn very complex input-output relations.
Despite their accuracy and extensive use, DNNs are highly susceptible to
adversarial attacks due to limited generalizability. For future progress in the
field, it is essential to build DNNs that are robust to any kind of
perturbations to the data points. In the past, many techniques have been
proposed to robustify DNNs using first-order derivative information of the
network.
</p>
<p>This paper proposes a new robustification approach based on control theory. A
neural network architecture that incorporates feedback control, named Feedback
Neural Networks, is proposed. The controller is itself a neural network, which
is trained using regular and adversarial data such as to stabilize the system
outputs. The novel adversarial training approach based on the feedback control
architecture is called Feedback Looped Adversarial Training (FLAT). Numerical
results on standard test problems empirically show that our FLAT method is more
effective than the state-of-the-art to guard against adversarial attacks.
</p></li>
</ul>

<h3>Title: Sample Complexity of Robust Learning against Evasion Attacks. (arXiv:2308.12054v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12054">http://arxiv.org/abs/2308.12054</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12054]] Sample Complexity of Robust Learning against Evasion Attacks(http://arxiv.org/abs/2308.12054)</code></li>
<li>Summary: <p>It is becoming increasingly important to understand the vulnerability of
machine learning models to adversarial attacks. One of the fundamental problems
in adversarial machine learning is to quantify how much training data is needed
in the presence of evasion attacks, where data is corrupted at test time. In
this thesis, we work with the exact-in-the-ball notion of robustness and study
the feasibility of adversarially robust learning from the perspective of
learning theory, considering sample complexity.
</p>
<p>We first explore the setting where the learner has access to random examples
only, and show that distributional assumptions are essential. We then focus on
learning problems with distributions on the input data that satisfy a Lipschitz
condition and show that robustly learning monotone conjunctions has sample
complexity at least exponential in the adversary's budget (the maximum number
of bits it can perturb on each input). However, if the adversary is restricted
to perturbing $O(\log n)$ bits, then one can robustly learn conjunctions and
decision lists w.r.t. log-Lipschitz distributions.
</p>
<p>We then study learning models where the learner is given more power. We first
consider local membership queries, where the learner can query the label of
points near the training sample. We show that, under the uniform distribution,
the exponential dependence on the adversary's budget to robustly learn
conjunctions remains inevitable. We then introduce a local equivalence query
oracle, which returns whether the hypothesis and target concept agree in a
given region around a point in the training sample, and a counterexample if it
exists. We show that if the query radius is equal to the adversary's budget, we
can develop robust empirical risk minimization algorithms in the
distribution-free setting. We give general query complexity upper and lower
bounds, as well as for concrete concept classes.
</p></li>
</ul>

<h3>Title: On-Manifold Projected Gradient Descent. (arXiv:2308.12279v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12279">http://arxiv.org/abs/2308.12279</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12279]] On-Manifold Projected Gradient Descent(http://arxiv.org/abs/2308.12279)</code></li>
<li>Summary: <p>This work provides a computable, direct, and mathematically rigorous
approximation to the differential geometry of class manifolds for
high-dimensional data, along with nonlinear projections from input space onto
these class manifolds. The tools are applied to the setting of neural network
image classifiers, where we generate novel, on-manifold data samples, and
implement a projected gradient descent algorithm for on-manifold adversarial
training. The susceptibility of neural networks (NNs) to adversarial attack
highlights the brittle nature of NN decision boundaries in input space.
Introducing adversarial examples during training has been shown to reduce the
susceptibility of NNs to adversarial attack; however, it has also been shown to
reduce the accuracy of the classifier if the examples are not valid examples
for that class. Realistic "on-manifold" examples have been previously generated
from class manifolds in the latent of an autoencoder. Our work explores these
phenomena in a geometric and computational setting that is much closer to the
raw, high-dimensional input space than can be provided by VAE or other black
box dimensionality reductions. We employ conformally invariant diffusion maps
(CIDM) to approximate class manifolds in diffusion coordinates, and develop the
Nystr\"{o}m projection to project novel points onto class manifolds in this
setting. On top of the manifold approximation, we leverage the spectral
exterior calculus (SEC) to determine geometric quantities such as tangent
vectors of the manifold. We use these tools to obtain adversarial examples that
reside on a class manifold, yet fool a classifier. These misclassifications
then become explainable in terms of human-understandable manipulations within
the data, by expressing the on-manifold adversary in the semantic basis on the
manifold.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection. (arXiv:2308.11681v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11681">http://arxiv.org/abs/2308.11681</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11681]] VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection(http://arxiv.org/abs/2308.11681)</code></li>
<li>Summary: <p>The recent contrastive language-image pre-training (CLIP) model has shown
great success in a wide range of image-level tasks, revealing remarkable
ability for learning powerful visual representations with rich semantics. An
open and worthwhile problem is efficiently adapting such a strong model to the
video domain and designing a robust video anomaly detector. In this work, we
propose VadCLIP, a new paradigm for weakly supervised video anomaly detection
(WSVAD) by leveraging the frozen CLIP model directly without any pre-training
and fine-tuning process. Unlike current works that directly feed extracted
features into the weakly supervised classifier for frame-level binary
classification, VadCLIP makes full use of fine-grained associations between
vision and language on the strength of CLIP and involves dual branch. One
branch simply utilizes visual features for coarse-grained binary
classification, while the other fully leverages the fine-grained language-image
alignment. With the benefit of dual branch, VadCLIP achieves both
coarse-grained and fine-grained video anomaly detection by transferring
pre-trained knowledge from CLIP to WSVAD task. We conduct extensive experiments
on two commonly-used benchmarks, demonstrating that VadCLIP achieves the best
performance on both coarse-grained and fine-grained WSVAD, surpassing the
state-of-the-art methods by a large margin. Specifically, VadCLIP achieves
84.51% AP and 88.02% AUC on XD-Violence and UCF-Crime, respectively. Code and
features will be released to facilitate future VAD research.
</p></li>
</ul>

<h3>Title: WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters. (arXiv:2308.11776v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11776">http://arxiv.org/abs/2308.11776</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11776]] WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters(http://arxiv.org/abs/2308.11776)</code></li>
<li>Summary: <p>Depth estimation in surgical video plays a crucial role in many image-guided
surgery procedures. However, it is difficult and time consuming to create depth
map ground truth datasets in surgical videos due in part to inconsistent
brightness and noise in the surgical scene. Therefore, building an accurate and
robust self-supervised depth and camera ego-motion estimation system is gaining
more attention from the computer vision community. Although several
self-supervision methods alleviate the need for ground truth depth maps and
poses, they still need known camera intrinsic parameters, which are often
missing or not recorded. Moreover, the camera intrinsic prediction methods in
existing works depend heavily on the quality of datasets. In this work, we
aimed to build a self-supervised depth and ego-motion estimation system which
can predict not only accurate depth maps and camera pose, but also camera
intrinsic parameters. We proposed a cost-volume-based supervision manner to
give the system auxiliary supervision for camera parameters prediction. The
experimental results showed that the proposed method improved the accuracy of
estimated camera parameters, ego-motion, and depth estimation.
</p></li>
</ul>

<h3>Title: Semi-Supervised Learning via Weight-aware Distillation under Class Distribution Mismatch. (arXiv:2308.11874v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11874">http://arxiv.org/abs/2308.11874</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11874]] Semi-Supervised Learning via Weight-aware Distillation under Class Distribution Mismatch(http://arxiv.org/abs/2308.11874)</code></li>
<li>Summary: <p>Semi-Supervised Learning (SSL) under class distribution mismatch aims to
tackle a challenging problem wherein unlabeled data contain lots of unknown
categories unseen in the labeled ones. In such mismatch scenarios, traditional
SSL suffers severe performance damage due to the harmful invasion of the
instances with unknown categories into the target classifier. In this study, by
strict mathematical reasoning, we reveal that the SSL error under class
distribution mismatch is composed of pseudo-labeling error and invasion error,
both of which jointly bound the SSL population risk. To alleviate the SSL
error, we propose a robust SSL framework called Weight-Aware Distillation (WAD)
that, by weights, selectively transfers knowledge beneficial to the target task
from unsupervised contrastive representation to the target classifier.
Specifically, WAD captures adaptive weights and high-quality pseudo labels to
target instances by exploring point mutual information (PMI) in representation
space to maximize the role of unlabeled data and filter unknown categories.
Theoretically, we prove that WAD has a tight upper bound of population risk
under class distribution mismatch. Experimentally, extensive results
demonstrate that WAD outperforms five state-of-the-art SSL approaches and one
standard baseline on two benchmark datasets, CIFAR10 and CIFAR100, and an
artificial cross-dataset. The code is available at
https://github.com/RUC-DWBI-ML/research/tree/main/WAD-master.
</p></li>
</ul>

<h3>Title: Integrated Image and Location Analysis for Wound Classification: A Deep Learning Approach. (arXiv:2308.11877v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11877">http://arxiv.org/abs/2308.11877</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11877]] Integrated Image and Location Analysis for Wound Classification: A Deep Learning Approach(http://arxiv.org/abs/2308.11877)</code></li>
<li>Summary: <p>The global burden of acute and chronic wounds presents a compelling case for
enhancing wound classification methods, a vital step in diagnosing and
determining optimal treatments. Recognizing this need, we introduce an
innovative multi-modal network based on a deep convolutional neural network for
categorizing wounds into four categories: diabetic, pressure, surgical, and
venous ulcers. Our multi-modal network uses wound images and their
corresponding body locations for more precise classification. A unique aspect
of our methodology is incorporating a body map system that facilitates accurate
wound location tagging, improving upon traditional wound image classification
techniques. A distinctive feature of our approach is the integration of models
such as VGG16, ResNet152, and EfficientNet within a novel architecture. This
architecture includes elements like spatial and channel-wise
Squeeze-and-Excitation modules, Axial Attention, and an Adaptive Gated
Multi-Layer Perceptron, providing a robust foundation for classification. Our
multi-modal network was trained and evaluated on two distinct datasets
comprising relevant images and corresponding location information. Notably, our
proposed network outperformed traditional methods, reaching an accuracy range
of 74.79% to 100% for Region of Interest (ROI) without location
classifications, 73.98% to 100% for ROI with location classifications, and
78.10% to 100% for whole image classifications. This marks a significant
enhancement over previously reported performance metrics in the literature. Our
results indicate the potential of our multi-modal network as an effective
decision-support tool for wound image classification, paving the way for its
application in various clinical contexts.
</p></li>
</ul>

<h3>Title: Camera-Driven Representation Learning for Unsupervised Domain Adaptive Person Re-identification. (arXiv:2308.11901v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11901">http://arxiv.org/abs/2308.11901</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11901]] Camera-Driven Representation Learning for Unsupervised Domain Adaptive Person Re-identification(http://arxiv.org/abs/2308.11901)</code></li>
<li>Summary: <p>We present a novel unsupervised domain adaption method for person
re-identification (reID) that generalizes a model trained on a labeled source
domain to an unlabeled target domain. We introduce a camera-driven curriculum
learning (CaCL) framework that leverages camera labels of person images to
transfer knowledge from source to target domains progressively. To this end, we
divide target domain dataset into multiple subsets based on the camera labels,
and initially train our model with a single subset (i.e., images captured by a
single camera). We then gradually exploit more subsets for training, according
to a curriculum sequence obtained with a camera-driven scheduling rule. The
scheduler considers maximum mean discrepancies (MMD) between each subset and
the source domain dataset, such that the subset closer to the source domain is
exploited earlier within the curriculum. For each curriculum sequence, we
generate pseudo labels of person images in a target domain to train a reID
model in a supervised way. We have observed that the pseudo labels are highly
biased toward cameras, suggesting that person images obtained from the same
camera are likely to have the same pseudo labels, even for different IDs. To
address the camera bias problem, we also introduce a camera-diversity (CD) loss
encouraging person images of the same pseudo label, but captured across various
cameras, to involve more for discriminative feature learning, providing person
representations robust to inter-camera variations. Experimental results on
standard benchmarks, including real-to-real and synthetic-to-real scenarios,
demonstrate the effectiveness of our framework.
</p></li>
</ul>

<h3>Title: AMSP-UOD: When Vortex Convolution and Stochastic Perturbation Meet Underwater Object Detection. (arXiv:2308.11918v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11918">http://arxiv.org/abs/2308.11918</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11918]] AMSP-UOD: When Vortex Convolution and Stochastic Perturbation Meet Underwater Object Detection(http://arxiv.org/abs/2308.11918)</code></li>
<li>Summary: <p>In this paper, we present a novel Amplitude-Modulated Stochastic Perturbation
and Vortex Convolutional Network, AMSP-UOD, designed for underwater object
detection. AMSP-UOD specifically addresses the impact of non-ideal imaging
factors on detection accuracy in complex underwater environments. To mitigate
the influence of noise on object detection performance, we propose AMSP Vortex
Convolution (AMSP-VConv) to disrupt the noise distribution, enhance feature
extraction capabilities, effectively reduce parameters, and improve network
robustness. We design the Feature Association Decoupling Cross Stage Partial
(FAD-CSP) module, which strengthens the association of long and short-range
features, improving the network performance in complex underwater environments.
Additionally, our sophisticated post-processing method, based on non-maximum
suppression with aspect-ratio similarity thresholds, optimizes detection in
dense scenes, such as waterweed and schools of fish, improving object detection
accuracy. Extensive experiments on the URPC and RUOD datasets demonstrate that
our method outperforms existing state-of-the-art methods in terms of accuracy
and noise immunity. AMSP-UOD proposes an innovative solution with the potential
for real-world applications. Code will be made publicly available.
</p></li>
</ul>

<h3>Title: Multi-stage Factorized Spatio-Temporal Representation for RGB-D Action and Gesture Recognition. (arXiv:2308.12006v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12006">http://arxiv.org/abs/2308.12006</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12006]] Multi-stage Factorized Spatio-Temporal Representation for RGB-D Action and Gesture Recognition(http://arxiv.org/abs/2308.12006)</code></li>
<li>Summary: <p>RGB-D action and gesture recognition remain an interesting topic in
human-centered scene understanding, primarily due to the multiple granularities
and large variation in human motion. Although many RGB-D based action and
gesture recognition approaches have demonstrated remarkable results by
utilizing highly integrated spatio-temporal representations across multiple
modalities (i.e., RGB and depth data), they still encounter several challenges.
Firstly, vanilla 3D convolution makes it hard to capture fine-grained motion
differences between local clips under different modalities. Secondly, the
intricate nature of highly integrated spatio-temporal modeling can lead to
optimization difficulties. Thirdly, duplicate and unnecessary information can
add complexity and complicate entangled spatio-temporal modeling. To address
the above issues, we propose an innovative heuristic architecture called
Multi-stage Factorized Spatio-Temporal (MFST) for RGB-D action and gesture
recognition. The proposed MFST model comprises a 3D Central Difference
Convolution Stem (CDC-Stem) module and multiple factorized spatio-temporal
stages. The CDC-Stem enriches fine-grained temporal perception, and the
multiple hierarchical spatio-temporal stages construct dimension-independent
higher-order semantic primitives. Specifically, the CDC-Stem module captures
bottom-level spatio-temporal features and passes them successively to the
following spatio-temporal factored stages to capture the hierarchical spatial
and temporal features through the Multi- Scale Convolution and Transformer
(MSC-Trans) hybrid block and Weight-shared Multi-Scale Transformer (WMS-Trans)
block. The seamless integration of these innovative designs results in a robust
spatio-temporal representation that outperforms state-of-the-art approaches on
RGB-D action and gesture recognition datasets.
</p></li>
</ul>

<h3>Title: Cross-Modality Proposal-guided Feature Mining for Unregistered RGB-Thermal Pedestrian Detection. (arXiv:2308.12111v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12111">http://arxiv.org/abs/2308.12111</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12111]] Cross-Modality Proposal-guided Feature Mining for Unregistered RGB-Thermal Pedestrian Detection(http://arxiv.org/abs/2308.12111)</code></li>
<li>Summary: <p>RGB-Thermal (RGB-T) pedestrian detection aims to locate the pedestrians in
RGB-T image pairs to exploit the complementation between the two modalities for
improving detection robustness in extreme conditions. Most existing algorithms
assume that the RGB-T image pairs are well registered, while in the real world
they are not aligned ideally due to parallax or different field-of-view of the
cameras. The pedestrians in misaligned image pairs may locate at different
positions in two images, which results in two challenges: 1) how to achieve
inter-modality complementation using spatially misaligned RGB-T pedestrian
patches, and 2) how to recognize the unpaired pedestrians at the boundary. To
deal with these issues, we propose a new paradigm for unregistered RGB-T
pedestrian detection, which predicts two separate pedestrian locations in the
RGB and thermal images, respectively. Specifically, we propose a cross-modality
proposal-guided feature mining (CPFM) mechanism to extract the two precise
fusion features for representing the pedestrian in the two modalities, even if
the RGB-T image pair is unaligned. It enables us to effectively exploit the
complementation between the two modalities. With the CPFM mechanism, we build a
two-stream dense detector; it predicts the two pedestrian locations in the two
modalities based on the corresponding fusion feature mined by the CPFM
mechanism. Besides, we design a data augmentation method, named Homography, to
simulate the discrepancy in scales and views between images. We also
investigate two non-maximum suppression (NMS) methods for post-processing.
Favorable experimental results demonstrate the effectiveness and robustness of
our method in dealing with unregistered pedestrians with different shifts.
</p></li>
</ul>

<h3>Title: Masking Strategies for Background Bias Removal in Computer Vision Models. (arXiv:2308.12127v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12127">http://arxiv.org/abs/2308.12127</a></li>
<li>Code URL: https://github.com/ananthu-aniraj/masking_strategies_bias_removal</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12127]] Masking Strategies for Background Bias Removal in Computer Vision Models(http://arxiv.org/abs/2308.12127)</code></li>
<li>Summary: <p>Models for fine-grained image classification tasks, where the difference
between some classes can be extremely subtle and the number of samples per
class tends to be low, are particularly prone to picking up background-related
biases and demand robust methods to handle potential examples with
out-of-distribution (OOD) backgrounds. To gain deeper insights into this
critical problem, our research investigates the impact of background-induced
bias on fine-grained image classification, evaluating standard backbone models
such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We
explore two masking strategies to mitigate background-induced bias: Early
masking, which removes background information at the (input) image level, and
late masking, which selectively masks high-level spatial features corresponding
to the background. Extensive experiments assess the behavior of CNN and ViT
models under different masking strategies, with a focus on their generalization
to OOD backgrounds. The obtained findings demonstrate that both proposed
strategies enhance OOD performance compared to the baseline models, with early
masking consistently exhibiting the best OOD performance. Notably, a ViT
variant employing GAP-Pooled Patch token-based classification combined with
early masking achieves the highest OOD robustness.
</p></li>
</ul>

<h3>Title: A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11838">http://arxiv.org/abs/2308.11838</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11838]] A Benchmark Study on Calibration(http://arxiv.org/abs/2308.11838)</code></li>
<li>Summary: <p>Deep neural networks are increasingly utilized in various machine learning
tasks. However, as these models grow in complexity, they often face calibration
issues, despite enhanced prediction accuracy. Many studies have endeavored to
improve calibration performance through data preprocessing, the use of specific
loss functions, and training frameworks. Yet, investigations into calibration
properties have been somewhat overlooked. Our study leverages the Neural
Architecture Search (NAS) search space, offering an exhaustive model
architecture space for thorough calibration properties exploration. We
specifically create a model calibration dataset. This dataset evaluates 90
bin-based and 12 additional calibration measurements across 117,702 unique
neural networks within the widely employed NATS-Bench search space. Our
analysis aims to answer several longstanding questions in the field, using our
proposed dataset: (i) Can model calibration be generalized across different
tasks? (ii) Can robustness be used as a calibration measurement? (iii) How
reliable are calibration metrics? (iv) Does a post-hoc calibration method
affect all models uniformly? (v) How does calibration interact with accuracy?
(vi) What is the impact of bin size on calibration measurement? (vii) Which
architectural designs are beneficial for calibration? Additionally, our study
bridges an existing gap by exploring calibration within NAS. By providing this
dataset, we enable further research into NAS calibration. As far as we are
aware, our research represents the first large-scale investigation into
calibration properties and the premier study of calibration issues within NAS.
</p></li>
</ul>

<h3>Title: A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12044">http://arxiv.org/abs/2308.12044</a></li>
<li>Code URL: https://github.com/aamakor/continuation-method</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12044]] A multiobjective continuation method to compute the regularization path of deep neural networks(http://arxiv.org/abs/2308.12044)</code></li>
<li>Summary: <p>Sparsity is a highly desired feature in deep neural networks (DNNs) since it
ensures numerical efficiency, improves the interpretability of models (due to
the smaller number of relevant features), and robustness. In machine learning
approaches based on linear models, it is well known that there exists a
connecting path between the sparsest solution in terms of the $\ell^1$ norm
(i.e., zero weights) and the non-regularized solution, which is called the
regularization path. Very recently, there was a first attempt to extend the
concept of regularization paths to DNNs by means of treating the empirical loss
and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the
resulting multiobjective optimization problem. However, due to the
non-smoothness of the $\ell^1$ norm and the high number of parameters, this
approach is not very efficient from a computational perspective. To overcome
this limitation, we present an algorithm that allows for the approximation of
the entire Pareto front for the above-mentioned objectives in a very efficient
manner. We present numerical examples using both deterministic and stochastic
gradients. We furthermore demonstrate that knowledge of the regularization path
allows for a well-generalizing network parametrization.
</p></li>
</ul>

<h3>Title: Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques. (arXiv:2308.12192v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12192">http://arxiv.org/abs/2308.12192</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12192]] Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques(http://arxiv.org/abs/2308.12192)</code></li>
<li>Summary: <p>This paper presents, in a unified fashion, deterministic as well as
statistical Lagrangian-verification techniques. They formally quantify the
behavioral robustness of any time-continuous process, formulated as a
continuous-depth model. To this end, we review LRT-NG, SLR, and GoTube,
algorithms for constructing a tight reachtube, that is, an over-approximation
of the set of states reachable within a given time-horizon, and provide
guarantees for the reachtube bounds. We compare the usage of the variational
equations, associated to the system equations, the mean value theorem, and the
Lipschitz constants, in achieving deterministic and statistical guarantees. In
LRT-NG, the Lipschitz constant is used as a bloating factor of the initial
perturbation, to compute the radius of an ellipsoid in an optimal metric, which
over-approximates the set of reachable states. In SLR and GoTube, we get
statistical guarantees, by using the Lipschitz constants to compute local balls
around samples. These are needed to calculate the probability of having found
an upper bound, of the true maximum perturbation at every timestep. Our
experiments demonstrate the superior performance of Lagrangian techniques, when
compared to LRT, Flow*, and CAPD, and illustrate their use in the robustness
analysis of various continuous-depth models.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: HashReID: Dynamic Network with Binary Codes for Efficient Person Re-identification. (arXiv:2308.11900v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11900">http://arxiv.org/abs/2308.11900</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11900]] HashReID: Dynamic Network with Binary Codes for Efficient Person Re-identification(http://arxiv.org/abs/2308.11900)</code></li>
<li>Summary: <p>Biometric applications, such as person re-identification (ReID), are often
deployed on energy constrained devices. While recent ReID methods prioritize
high retrieval performance, they often come with large computational costs and
high search time, rendering them less practical in real-world settings. In this
work, we propose an input-adaptive network with multiple exit blocks, that can
terminate computation early if the retrieval is straightforward or noisy,
saving a lot of computation. To assess the complexity of the input, we
introduce a temporal-based classifier driven by a new training strategy.
Furthermore, we adopt a binary hash code generation approach instead of relying
on continuous-valued features, which significantly improves the search process
by a factor of 20. To ensure similarity preservation, we utilize a new ranking
regularizer that bridges the gap between continuous and binary features.
Extensive analysis of our proposed method is conducted on three datasets:
Market1501, MSMT17 (Multi-Scene Multi-Time), and the BGC1 (BRIAR Government
Collection). Using our approach, more than 70% of the samples with compact hash
codes exit early on the Market1501 dataset, saving 80% of the networks
computational cost and improving over other hash-based methods by 60%. These
results demonstrate a significant improvement over dynamic networks and
showcase comparable accuracy performance to conventional ReID methods. Code
will be made available.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Rotation-Invariant Completion Network. (arXiv:2308.11979v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11979">http://arxiv.org/abs/2308.11979</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11979]] Rotation-Invariant Completion Network(http://arxiv.org/abs/2308.11979)</code></li>
<li>Summary: <p>Real-world point clouds usually suffer from incompleteness and display
different poses. While current point cloud completion methods excel in
reproducing complete point clouds with consistent poses as seen in the training
set, their performance tends to be unsatisfactory when handling point clouds
with diverse poses. We propose a network named Rotation-Invariant Completion
Network (RICNet), which consists of two parts: a Dual Pipeline Completion
Network (DPCNet) and an enhancing module. Firstly, DPCNet generates a coarse
complete point cloud. The feature extraction module of DPCNet can extract
consistent features, no matter if the input point cloud has undergone rotation
or translation. Subsequently, the enhancing module refines the fine-grained
details of the final generated point cloud. RICNet achieves better rotation
invariance in feature extraction and incorporates structural relationships in
man-made objects. To assess the performance of RICNet and existing methods on
point clouds with various poses, we applied random transformations to the point
clouds in the MVP dataset and conducted experiments on them. Our experiments
demonstrate that RICNet exhibits superior completion performance compared to
existing methods.
</p></li>
</ul>

<h3>Title: Sign Language Translation with Iterative Prototype. (arXiv:2308.12191v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12191">http://arxiv.org/abs/2308.12191</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12191]] Sign Language Translation with Iterative Prototype(http://arxiv.org/abs/2308.12191)</code></li>
<li>Summary: <p>This paper presents IP-SLT, a simple yet effective framework for sign
language translation (SLT). Our IP-SLT adopts a recurrent structure and
enhances the semantic representation (prototype) of the input sign language
video via an iterative refinement manner. Our idea mimics the behavior of human
reading, where a sentence can be digested repeatedly, till reaching accurate
understanding. Technically, IP-SLT consists of feature extraction, prototype
initialization, and iterative prototype refinement. The initialization module
generates the initial prototype based on the visual feature extracted by the
feature extraction module. Then, the iterative refinement module leverages the
cross-attention mechanism to polish the previous prototype by aggregating it
with the original video feature. Through repeated refinement, the prototype
finally converges to a more stable and accurate state, leading to a fluent and
appropriate translation. In addition, to leverage the sequential dependence of
prototypes, we further propose an iterative distillation loss to compress the
knowledge of the final iteration into previous ones. As the autoregressive
decoding process is executed only once in inference, our IP-SLT is ready to
improve various SLT systems with acceptable overhead. Extensive experiments are
conducted on public benchmarks to demonstrate the effectiveness of the IP-SLT.
</p></li>
</ul>

<h3>Title: Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion. (arXiv:2308.11720v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11720">http://arxiv.org/abs/2308.11720</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11720]] Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion(http://arxiv.org/abs/2308.11720)</code></li>
<li>Summary: <p>Relation Extraction (RE) is a pivotal task in automatically extracting
structured information from unstructured text. In this paper, we present a
multi-faceted approach that integrates representative examples and through
co-set expansion. The primary goal of our method is to enhance relation
classification accuracy and mitigating confusion between contrastive classes.
</p>
<p>Our approach begins by seeding each relationship class with representative
examples. Subsequently, our co-set expansion algorithm enriches training
objectives by incorporating similarity measures between target pairs and
representative pairs from the target class. Moreover, the co-set expansion
process involves a class ranking procedure that takes into account exemplars
from contrastive classes. Contextual details encompassing relation mentions are
harnessed via context-free Hearst patterns to ascertain contextual similarity.
</p>
<p>Empirical evaluation demonstrates the efficacy of our co-set expansion
approach, resulting in a significant enhancement of relation classification
performance. Our method achieves an observed margin of at least 1 percent
improvement in accuracy in most settings, on top of existing fine-tuning
approaches. To further refine our approach, we conduct an in-depth analysis
that focuses on tuning contrastive examples. This strategic selection and
tuning effectively reduce confusion between classes sharing similarities,
leading to a more precise classification process.
</p>
<p>Experimental results underscore the effectiveness of our proposed framework
for relation extraction. The synergy between co-set expansion and context-aware
prompt tuning substantially contributes to improved classification accuracy.
Furthermore, the reduction in confusion between contrastive classes through
contrastive examples tuning validates the robustness and reliability of our
method.
</p></li>
</ul>

<h3>Title: Knowledge-injected Prompt Learning for Chinese Biomedical Entity Normalization. (arXiv:2308.12025v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12025">http://arxiv.org/abs/2308.12025</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12025]] Knowledge-injected Prompt Learning for Chinese Biomedical Entity Normalization(http://arxiv.org/abs/2308.12025)</code></li>
<li>Summary: <p>The Biomedical Entity Normalization (BEN) task aims to align raw,
unstructured medical entities to standard entities, thus promoting data
coherence and facilitating better downstream medical applications. Recently,
prompt learning methods have shown promising results in this task. However,
existing research falls short in tackling the more complex Chinese BEN task,
especially in the few-shot scenario with limited medical data, and the vast
potential of the external medical knowledge base has yet to be fully harnessed.
To address these challenges, we propose a novel Knowledge-injected Prompt
Learning (PL-Knowledge) method. Specifically, our approach consists of five
stages: candidate entity matching, knowledge extraction, knowledge encoding,
knowledge injection, and prediction output. By effectively encoding the
knowledge items contained in medical entities and incorporating them into our
tailor-made knowledge-injected templates, the additional knowledge enhances the
model's ability to capture latent relationships between medical entities, thus
achieving a better match with the standard entities. We extensively evaluate
our model on a benchmark dataset in both few-shot and full-scale scenarios. Our
method outperforms existing baselines, with an average accuracy boost of
12.96\% in few-shot and 0.94\% in full-data cases, showcasing its excellence in
the BEN task.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Understanding Hessian Alignment for Domain Generalization. (arXiv:2308.11778v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11778">http://arxiv.org/abs/2308.11778</a></li>
<li>Code URL: https://github.com/huawei-noah/federated-learning</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11778]] Understanding Hessian Alignment for Domain Generalization(http://arxiv.org/abs/2308.11778)</code></li>
<li>Summary: <p>Out-of-distribution (OOD) generalization is a critical ability for deep
learning models in many real-world scenarios including healthcare and
autonomous vehicles. Recently, different techniques have been proposed to
improve OOD generalization. Among these methods, gradient-based regularizers
have shown promising performance compared with other competitors. Despite this
success, our understanding of the role of Hessian and gradient alignment in
domain generalization is still limited. To address this shortcoming, we analyze
the role of the classifier's head Hessian matrix and gradient in domain
generalization using recent OOD theory of transferability. Theoretically, we
show that spectral norm between the classifier's head Hessian matrices across
domains is an upper bound of the transfer measure, a notion of distance between
target and source domains. Furthermore, we analyze all the attributes that get
aligned when we encourage similarity between Hessians and gradients. Our
analysis explains the success of many regularizers like CORAL, IRM, V-REx,
Fish, IGA, and Fishr as they regularize part of the classifier's head Hessian
and/or gradient. Finally, we propose two simple yet effective methods to match
the classifier's head Hessians and gradients in an efficient way, based on the
Hessian Gradient Product (HGP) and Hutchinson's method (Hutchinson), and
without directly calculating Hessians. We validate the OOD generalization
ability of proposed methods in different scenarios, including transferability,
severe correlation shift, label shift and diversity shift. Our results show
that Hessian alignment methods achieve promising performance on various OOD
benchmarks. The code is available at
\url{https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment}.
</p></li>
</ul>

<h3>Title: A Survey for Federated Learning Evaluations: Goals and Measures. (arXiv:2308.11841v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11841">http://arxiv.org/abs/2308.11841</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11841]] A Survey for Federated Learning Evaluations: Goals and Measures(http://arxiv.org/abs/2308.11841)</code></li>
<li>Summary: <p>Evaluation is a systematic approach to assessing how well a system achieves
its intended purpose. Federated learning (FL) is a novel paradigm for
privacy-preserving machine learning that allows multiple parties to
collaboratively train models without sharing sensitive data. However,
evaluating FL is challenging due to its interdisciplinary nature and diverse
goals, such as utility, efficiency, and security. In this survey, we first
review the major evaluation goals adopted in the existing studies and then
explore the evaluation metrics used for each goal. We also introduce FedEval,
an open-source platform that provides a standardized and comprehensive
evaluation framework for FL algorithms in terms of their utility, efficiency,
and security. Finally, we discuss several challenges and future research
directions for FL evaluation.
</p></li>
</ul>

<h3>Title: Unsupervised anomalies detection in IIoT edge devices networks using federated learning. (arXiv:2308.12175v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12175">http://arxiv.org/abs/2308.12175</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12175]] Unsupervised anomalies detection in IIoT edge devices networks using federated learning(http://arxiv.org/abs/2308.12175)</code></li>
<li>Summary: <p>In a connection of many IoT devices that each collect data, normally training
a machine learning model would involve transmitting the data to a central
server which requires strict privacy rules. However, some owners are reluctant
of availing their data out of the company due to data security concerns.
Federated learning(FL) as a distributed machine learning approach performs
training of a machine learning model on the device that gathered the data
itself. In this scenario, data is not share over the network for training
purpose. Fedavg as one of FL algorithms permits a model to be copied to
participating devices during a training session. The devices could be chosen at
random, and a device can be aborted. The resulting models are sent to the
coordinating server and then average models from the devices that finished
training. The process is repeated until a desired model accuracy is achieved.
By doing this, FL approach solves the privacy problem for IoT/ IIoT devices
that held sensitive data for the owners. In this paper, we leverage the
benefits of FL and implemented Fedavg algorithm on a recent dataset that
represent the modern IoT/ IIoT device networks. The results were almost the
same as the centralized machine learning approach. We also evaluated some
shortcomings of Fedavg such as unfairness that happens during the training when
struggling devices do not participate for every stage of training. This
inefficient training of local or global model could lead in a high number of
false alarms in intrusion detection systems for IoT/IIoT gadgets developed
using Fedavg. Hence, after evaluating the FedAv deep auto encoder with
centralized deep auto encoder ML, we further proposed and designed a Fair
Fedavg algorithm that will be evaluated in the future work.
</p></li>
</ul>

<h3>Title: Joint Local Relational Augmentation and Global Nash Equilibrium for Federated Learning with Non-IID Data. (arXiv:2308.11646v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11646">http://arxiv.org/abs/2308.11646</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11646]] Joint Local Relational Augmentation and Global Nash Equilibrium for Federated Learning with Non-IID Data(http://arxiv.org/abs/2308.11646)</code></li>
<li>Summary: <p>Federated learning (FL) is a distributed machine learning paradigm that needs
collaboration between a server and a series of clients with decentralized data.
To make FL effective in real-world applications, existing work devotes to
improving the modeling of decentralized data with non-independent and identical
distributions (non-IID). In non-IID settings, there are intra-client
inconsistency that comes from the imbalanced data modeling, and inter-client
inconsistency among heterogeneous client distributions, which not only hinders
sufficient representation of the minority data, but also brings discrepant
model deviations. However, previous work overlooks to tackle the above two
coupling inconsistencies together. In this work, we propose FedRANE, which
consists of two main modules, i.e., local relational augmentation (LRA) and
global Nash equilibrium (GNE), to resolve intra- and inter-client inconsistency
simultaneously. Specifically, in each client, LRA mines the similarity
relations among different data samples and enhances the minority sample
representations with their neighbors using attentive message passing. In
server, GNE reaches an agreement among inconsistent and discrepant model
deviations from clients to server, which encourages the global model to update
in the direction of global optimum without breaking down the clients
optimization toward their local optimums. We conduct extensive experiments on
four benchmark datasets to show the superiority of FedRANE in enhancing the
performance of FL with non-IID data.
</p></li>
</ul>

<h3>Title: When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation. (arXiv:2308.11953v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11953">http://arxiv.org/abs/2308.11953</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11953]] When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation(http://arxiv.org/abs/2308.11953)</code></li>
<li>Summary: <p>Federated learning (FL) enables collaborative model training across
distributed clients (e.g., edge devices) without sharing raw data. Yet, FL can
be computationally expensive as the clients need to train the entire model
multiple times. SplitFed learning (SFL) is a recent distributed approach that
alleviates computation workload at the client device by splitting the model at
a cut layer into two parts, where clients only need to train part of the model.
However, SFL still suffers from the \textit{client drift} problem when clients'
data are highly non-IID. To address this issue, we propose MiniBatch-SFL. This
algorithm incorporates MiniBatch SGD into SFL, where the clients train the
client-side model in an FL fashion while the server trains the server-side
model similar to MiniBatch SGD. We analyze the convergence of MiniBatch-SFL and
show that the bound of the expected loss can be obtained by analyzing the
expected server-side and client-side model updates, respectively. The
server-side updates do not depend on the non-IID degree of the clients'
datasets and can potentially mitigate client drift. However, the client-side
model relies on the non-IID degree and can be optimized by properly choosing
the cut layer. Perhaps counter-intuitive, our empirical result shows that a
latter position of the cut layer leads to a smaller average gradient divergence
and a better algorithm performance. Moreover, numerical results show that
MiniBatch-SFL achieves higher accuracy than conventional SFL and FL. The
accuracy improvement can be up to 24.1\% and 17.1\% with highly non-IID data,
respectively.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: (Un)fair Exposure in Deep Face Rankings at a Distance. (arXiv:2308.11732v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11732">http://arxiv.org/abs/2308.11732</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11732]] (Un)fair Exposure in Deep Face Rankings at a Distance(http://arxiv.org/abs/2308.11732)</code></li>
<li>Summary: <p>Law enforcement regularly faces the challenge of ranking suspects from their
facial images. Deep face models aid this process but frequently introduce
biases that disproportionately affect certain demographic segments. While bias
investigation is common in domains like job candidate ranking, the field of
forensic face rankings remains underexplored. In this paper, we propose a novel
experimental framework, encompassing six state-of-the-art face encoders and two
public data sets, designed to scrutinize the extent to which demographic groups
suffer from biases in exposure in the context of forensic face rankings.
Through comprehensive experiments that cover both re-identification and
identification tasks, we show that exposure biases within this domain are far
from being countered, demanding attention towards establishing ad-hoc policies
and corrective measures. The source code is available at
https://github.com/atzoriandrea/ijcb2023-unfair-face-rankings
</p></li>
</ul>

<h3>Title: Compressed Models Decompress Race Biases: What Quantized Models Forget for Fair Face Recognition. (arXiv:2308.11840v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11840">http://arxiv.org/abs/2308.11840</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11840]] Compressed Models Decompress Race Biases: What Quantized Models Forget for Fair Face Recognition(http://arxiv.org/abs/2308.11840)</code></li>
<li>Summary: <p>With the ever-growing complexity of deep learning models for face
recognition, it becomes hard to deploy these systems in real life. Researchers
have two options: 1) use smaller models; 2) compress their current models.
Since the usage of smaller models might lead to concerning biases, compression
gains relevance. However, compressing might be also responsible for an increase
in the bias of the final model. We investigate the overall performance, the
performance on each ethnicity subgroup and the racial bias of a
State-of-the-Art quantization approach when used with synthetic and real data.
This analysis provides a few more details on potential benefits of performing
quantization with synthetic data, for instance, the reduction of biases on the
majority of test scenarios. We tested five distinct architectures and three
different training datasets. The models were evaluated on a fourth dataset
which was collected to infer and compare the performance of face recognition
models on different ethnicity.
</p></li>
</ul>

<h3>Title: Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder. (arXiv:2308.11819v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11819">http://arxiv.org/abs/2308.11819</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11819]] Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder(http://arxiv.org/abs/2308.11819)</code></li>
<li>Summary: <p>The fairness issue of clinical data modeling, especially on Electronic Health
Records (EHRs), is of utmost importance due to EHR's complex latent structure
and potential selection bias. It is frequently necessary to mitigate health
disparity while keeping the model's overall accuracy in practice. However,
traditional methods often encounter the trade-off between accuracy and
fairness, as they fail to capture the underlying factors beyond observed data.
To tackle this challenge, we propose a novel model called Fair Longitudinal
Medical Deconfounder (FLMD) that aims to achieve both fairness and accuracy in
longitudinal Electronic Health Records (EHR) modeling. Drawing inspiration from
the deconfounder theory, FLMD employs a two-stage training process. In the
first stage, FLMD captures unobserved confounders for each encounter, which
effectively represents underlying medical factors beyond observed EHR, such as
patient genotypes and lifestyle habits. This unobserved confounder is crucial
for addressing the accuracy/fairness dilemma. In the second stage, FLMD
combines the learned latent representation with other relevant features to make
predictions. By incorporating appropriate fairness criteria, such as
counterfactual fairness, FLMD ensures that it maintains high prediction
accuracy while simultaneously minimizing health disparities. We conducted
comprehensive experiments on two real-world EHR datasets to demonstrate the
effectiveness of FLMD. Apart from the comparison of baseline methods and FLMD
variants in terms of fairness and accuracy, we assessed the performance of all
models on disturbed/imbalanced and synthetic datasets to showcase the
superiority of FLMD across different settings and provide valuable insights
into its capabilities.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: CoC-GAN: Employing Context Cluster for Unveiling a New Pathway in Image Generation. (arXiv:2308.11857v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11857">http://arxiv.org/abs/2308.11857</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11857]] CoC-GAN: Employing Context Cluster for Unveiling a New Pathway in Image Generation(http://arxiv.org/abs/2308.11857)</code></li>
<li>Summary: <p>Image generation tasks are traditionally undertaken using Convolutional
Neural Networks (CNN) or Transformer architectures for feature aggregating and
dispatching. Despite the frequent application of convolution and attention
structures, these structures are not fundamentally required to solve the
problem of instability and the lack of interpretability in image generation. In
this paper, we propose a unique image generation process premised on the
perspective of converting images into a set of point clouds. In other words, we
interpret an image as a set of points. As such, our methodology leverages
simple clustering methods named Context Clustering (CoC) to generate images
from unordered point sets, which defies the convention of using convolution or
attention mechanisms. Hence, we exclusively depend on this clustering
technique, combined with the multi-layer perceptron (MLP) in a generative
model. Furthermore, we implement the integration of a module termed the 'Point
Increaser' for the model. This module is just an MLP tasked with generating
additional points for clustering, which are subsequently integrated within the
paradigm of the Generative Adversarial Network (GAN). We introduce this model
with the novel structure as the Context Clustering Generative Adversarial
Network (CoC-GAN), which offers a distinctive viewpoint in the domain of
feature aggregating and dispatching. Empirical evaluations affirm that our
CoC-GAN, devoid of convolution and attention mechanisms, exhibits outstanding
performance. Its interpretability, endowed by the CoC module, also allows for
visualization in our experiments. The promising results underscore the
feasibility of our method and thus warrant future investigations of applying
Context Clustering to more novel and interpretable image generation.
</p></li>
</ul>

<h3>Title: Concept Bottleneck with Visual Concept Filtering for Explainable Medical Image Classification. (arXiv:2308.11920v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11920">http://arxiv.org/abs/2308.11920</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11920]] Concept Bottleneck with Visual Concept Filtering for Explainable Medical Image Classification(http://arxiv.org/abs/2308.11920)</code></li>
<li>Summary: <p>Interpretability is a crucial factor in building reliable models for various
medical applications. Concept Bottleneck Models (CBMs) enable interpretable
image classification by utilizing human-understandable concepts as intermediate
targets. Unlike conventional methods that require extensive human labor to
construct the concept set, recent works leveraging Large Language Models (LLMs)
for generating concepts made automatic concept generation possible. However,
those methods do not consider whether a concept is visually relevant or not,
which is an important factor in computing meaningful concept scores. Therefore,
we propose a visual activation score that measures whether the concept contains
visual cues or not, which can be easily computed with unlabeled image data.
Computed visual activation scores are then used to filter out the less visible
concepts, thus resulting in a final concept set with visually meaningful
concepts. Our experimental results show that adopting the proposed visual
activation score for concept filtering consistently boosts performance compared
to the baseline. Moreover, qualitative analyses also validate that visually
relevant concepts are successfully selected with the visual activation score.
</p></li>
</ul>

<h3>Title: Distribution-Aware Calibration for Object Detection with Noisy Bounding Boxes. (arXiv:2308.12017v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12017">http://arxiv.org/abs/2308.12017</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12017]] Distribution-Aware Calibration for Object Detection with Noisy Bounding Boxes(http://arxiv.org/abs/2308.12017)</code></li>
<li>Summary: <p>Large-scale well-annotated datasets are of great importance for training an
effective object detector. However, obtaining accurate bounding box annotations
is laborious and demanding. Unfortunately, the resultant noisy bounding boxes
could cause corrupt supervision signals and thus diminish detection
performance. Motivated by the observation that the real ground-truth is usually
situated in the aggregation region of the proposals assigned to a noisy
ground-truth, we propose DIStribution-aware CalibratiOn (DISCO) to model the
spatial distribution of proposals for calibrating supervision signals. In
DISCO, spatial distribution modeling is performed to statistically extract the
potential locations of objects. Based on the modeled distribution, three
distribution-aware techniques, i.e., distribution-aware proposal augmentation
(DA-Aug), distribution-aware box refinement (DA-Ref), and distribution-aware
confidence estimation (DA-Est), are developed to improve classification,
localization, and interpretability, respectively. Extensive experiments on
large-scale noisy image datasets (i.e., Pascal VOC and MS-COCO) demonstrate
that DISCO can achieve state-of-the-art detection performance, especially at
high noise levels.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Layer-wise Feedback Propagation. (arXiv:2308.12053v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12053">http://arxiv.org/abs/2308.12053</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12053]] Layer-wise Feedback Propagation(http://arxiv.org/abs/2308.12053)</code></li>
<li>Summary: <p>In this paper, we present Layer-wise Feedback Propagation (LFP), a novel
training approach for neural-network-like predictors that utilizes
explainability, specifically Layer-wise Relevance Propagation(LRP), to assign
rewards to individual connections based on their respective contributions to
solving a given task. This differs from traditional gradient descent, which
updates parameters towards anestimated loss minimum. LFP distributes a reward
signal throughout the model without the need for gradient computations. It then
strengthens structures that receive positive feedback while reducingthe
influence of structures that receive negative feedback. We establish the
convergence of LFP theoretically and empirically, and demonstrate its
effectiveness in achieving comparable performance to gradient descent on
various models and datasets. Notably, LFP overcomes certain limitations
associated with gradient-based methods, such as reliance on meaningful
derivatives. We further investigate how the different LRP-rules can be extended
to LFP, what their effects are on training, as well as potential applications,
such as training models with no meaningful derivatives, e.g., step-function
activated Spiking Neural Networks (SNNs), or for transfer learning, to
efficiently utilize existing knowledge.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Boosting Diffusion Models with an Adaptive Momentum Sampler. (arXiv:2308.11941v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11941">http://arxiv.org/abs/2308.11941</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11941]] Boosting Diffusion Models with an Adaptive Momentum Sampler(http://arxiv.org/abs/2308.11941)</code></li>
<li>Summary: <p>Diffusion probabilistic models (DPMs) have been shown to generate
high-quality images without the need for delicate adversarial training.
However, the current sampling process in DPMs is prone to violent shaking. In
this paper, we present a novel reverse sampler for DPMs inspired by the
widely-used Adam optimizer. Our proposed sampler can be readily applied to a
pre-trained diffusion model, utilizing momentum mechanisms and adaptive
updating to smooth the reverse sampling process and ensure stable generation,
resulting in outputs of enhanced quality. By implicitly reusing update
directions from early steps, our proposed sampler achieves a better balance
between high-level semantics and low-level details. Additionally, this sampler
is flexible and can be easily integrated into pre-trained DPMs regardless of
the sampler used during training. Our experimental results on multiple
benchmarks demonstrate that our proposed reverse sampler yields remarkable
improvements over different baselines. We will make the source code available.
</p></li>
</ul>

<h3>Title: LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model. (arXiv:2308.11945v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11945">http://arxiv.org/abs/2308.11945</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11945]] LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model(http://arxiv.org/abs/2308.11945)</code></li>
<li>Summary: <p>Dancing with music is always an essential human art form to express emotion.
Due to the high temporal-spacial complexity, long-term 3D realist dance
generation synchronized with music is challenging. Existing methods suffer from
the freezing problem when generating long-term dances due to error accumulation
and training-inference discrepancy. To address this, we design a conditional
diffusion model, LongDanceDiff, for this sequence-to-sequence long-term dance
generation, addressing the challenges of temporal coherency and spatial
constraint. LongDanceDiff contains a transformer-based diffusion model, where
the input is a concatenation of music, past motions, and noised future motions.
This partial noising strategy leverages the full-attention mechanism and learns
the dependencies among music and past motions. To enhance the diversity of
generated dance motions and mitigate the freezing problem, we introduce a
mutual information minimization objective that regularizes the dependency
between past and future motions. We also address common visual quality issues
in dance generation, such as foot sliding and unsmooth motion, by incorporating
spatial constraints through a Global-Trajectory Modulation (GTM) layer and
motion perceptual losses, thereby improving the smoothness and naturalness of
motion generation. Extensive experiments demonstrate a significant improvement
in our approach over the existing state-of-the-art methods. We plan to release
our codes and models soon.
</p></li>
</ul>

<h3>Title: Efficient Transfer Learning in Diffusion Models via Adversarial Noise. (arXiv:2308.11948v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11948">http://arxiv.org/abs/2308.11948</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11948]] Efficient Transfer Learning in Diffusion Models via Adversarial Noise(http://arxiv.org/abs/2308.11948)</code></li>
<li>Summary: <p>Diffusion Probabilistic Models (DPMs) have demonstrated substantial promise
in image generation tasks but heavily rely on the availability of large amounts
of training data. Previous works, like GANs, have tackled the limited data
problem by transferring pre-trained models learned with sufficient data.
However, those methods are hard to be utilized in DPMs since the distinct
differences between DPM-based and GAN-based methods, showing in the unique
iterative denoising process integral and the need for many timesteps with
no-targeted noise in DPMs. In this paper, we propose a novel DPMs-based
transfer learning method, TAN, to address the limited data problem. It includes
two strategies: similarity-guided training, which boosts transfer with a
classifier, and adversarial noise selection which adaptive chooses targeted
noise based on the input image. Extensive experiments in the context of
few-shot image generation tasks demonstrate that our method is not only
efficient but also excels in terms of image quality and diversity when compared
to existing GAN-based and DDPM-based methods.
</p></li>
</ul>

<h3>Title: High-quality Image Dehazing with Diffusion Model. (arXiv:2308.11949v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11949">http://arxiv.org/abs/2308.11949</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11949]] High-quality Image Dehazing with Diffusion Model(http://arxiv.org/abs/2308.11949)</code></li>
<li>Summary: <p>Image dehazing is quite challenging in dense-haze scenarios, where quite less
original information remains in the hazy image. Though previous methods have
made marvelous progress, they still suffer from information loss in content and
color in dense-haze scenarios. The recently emerged Denoising Diffusion
Probabilistic Model (DDPM) exhibits strong generation ability, showing
potential for solving this problem. However, DDPM fails to consider the physics
property of dehazing task, limiting its information completion capacity. In
this work, we propose DehazeDDPM: A DDPM-based and physics-aware image dehazing
framework that applies to complex hazy scenarios. Specifically, DehazeDDPM
works in two stages. The former stage physically models the dehazing task with
the Atmospheric Scattering Model (ASM), pulling the distribution closer to the
clear data and endowing DehazeDDPM with fog-aware ability. The latter stage
exploits the strong generation ability of DDPM to compensate for the
haze-induced huge information loss, by working in conjunction with the physical
modelling. Extensive experiments demonstrate that our method attains
state-of-the-art performance on both synthetic and real-world hazy datasets.
</p></li>
</ul>

<h3>Title: Manipulating Embeddings of Stable Diffusion Prompts. (arXiv:2308.12059v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12059">http://arxiv.org/abs/2308.12059</a></li>
<li>Code URL: https://github.com/webis-de/arxiv23-prompt-embedding-manipulation</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12059]] Manipulating Embeddings of Stable Diffusion Prompts(http://arxiv.org/abs/2308.12059)</code></li>
<li>Summary: <p>Generative text-to-image models such as Stable Diffusion allow users to
generate images based on a textual description, the prompt. Changing the prompt
is still the primary means for the user to change a generated image as desired.
However, changing the image by reformulating the prompt remains a difficult
process of trial and error, which has led to the emergence of prompt
engineering as a new field of research. We propose and analyze methods to
change the embedding of a prompt directly instead of the prompt text. It allows
for more fine-grained and targeted control that takes into account user
intentions. Our approach treats the generative text-to-image model as a
continuous function and passes gradients between the image space and the prompt
embedding space. By addressing different user interaction problems, we can
apply this idea in three scenarios: (1) Optimization of a metric defined in
image space that could measure, for example, image style. (2) Assistance of
users in creative tasks by enabling them to navigate the image space along a
selection of directions of "near" prompt embeddings. (3) Changing the embedding
of the prompt to include information that the user has seen in a particular
seed but finds difficult to describe in the prompt. Our experiments demonstrate
the feasibility of the described methods.
</p></li>
</ul>

<h3>Title: Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12219">http://arxiv.org/abs/2308.12219</a></li>
<li>Code URL: https://github.com/yegcjs/diffusionllm</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12219]] Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning(http://arxiv.org/abs/2308.12219)</code></li>
<li>Summary: <p>The recent surge of generative AI has been fueled by the generative power of
diffusion probabilistic models and the scalable capabilities of large language
models. Despite their potential, it remains elusive whether diffusion language
models can solve general language tasks comparable to their autoregressive
counterparts. This paper demonstrates that scaling diffusion models w.r.t.
data, sizes, and tasks can effectively make them strong language learners. We
build competent diffusion language models at scale by first acquiring knowledge
from massive data via masked language modeling pretraining thanks to their
intrinsic connections. We then reprogram pretrained masked language models into
diffusion language models via diffusive adaptation, wherein task-specific
finetuning and instruction finetuning are explored to unlock their versatility
in solving general language tasks. Experiments show that scaling diffusion
language models consistently improves performance across downstream language
tasks. We further discover that instruction finetuning can elicit zero-shot and
few-shot in-context learning abilities that help tackle many unseen tasks by
following natural language instructions, and show promise in advanced and
challenging abilities such as reasoning
</p></li>
</ul>

<h3>Title: Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks. (arXiv:2308.11624v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11624">http://arxiv.org/abs/2308.11624</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11624]] Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks(http://arxiv.org/abs/2308.11624)</code></li>
<li>Summary: <p>An innovative methodology that leverages artificial intelligence (AI) and
graph representation for semiconductor device encoding in TCAD device
simulation is proposed. A graph-based universal encoding scheme is presented
that not only considers material-level and device-level embeddings, but also
introduces a novel spatial relationship embedding inspired by interpolation
operations typically used in finite element meshing. Universal physical laws
from device simulations are leveraged for comprehensive data-driven modeling,
which encompasses surrogate Poisson emulation and current-voltage (IV)
prediction based on drift-diffusion model. Both are achieved using a novel
graph attention network, referred to as RelGAT. Comprehensive technical details
based on the device simulator Sentaurus TCAD are presented, empowering
researchers to adopt the proposed AI-driven Electronic Design Automation (EDA)
solution at the device level.
</p></li>
</ul>

<h3>Title: Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models. (arXiv:2308.11890v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11890">http://arxiv.org/abs/2308.11890</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11890]] Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models(http://arxiv.org/abs/2308.11890)</code></li>
<li>Summary: <p>Ligand-based drug design aims to identify novel drug candidates of similar
shapes with known active molecules. In this paper, we formulated an in silico
shape-conditioned molecule generation problem to generate 3D molecule
structures conditioned on the shape of a given molecule. To address this
problem, we developed a translation- and rotation-equivariant shape-guided
generative model ShapeMol. ShapeMol consists of an equivariant shape encoder
that maps molecular surface shapes into latent embeddings, and an equivariant
diffusion model that generates 3D molecules based on these embeddings.
Experimental results show that ShapeMol can generate novel, diverse, drug-like
molecules that retain 3D molecular shapes similar to the given shape condition.
These results demonstrate the potential of ShapeMol in designing drug
candidates of desired 3D shapes binding to protein target pockets.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Coarse-to-Fine Multi-Scene Pose Regression with Transformers. (arXiv:2308.11783v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11783">http://arxiv.org/abs/2308.11783</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11783]] Coarse-to-Fine Multi-Scene Pose Regression with Transformers(http://arxiv.org/abs/2308.11783)</code></li>
<li>Summary: <p>Absolute camera pose regressors estimate the position and orientation of a
camera given the captured image alone. Typically, a convolutional backbone with
a multi-layer perceptron (MLP) head is trained using images and pose labels to
embed a single reference scene at a time. Recently, this scheme was extended to
learn multiple scenes by replacing the MLP head with a set of fully connected
layers. In this work, we propose to learn multi-scene absolute camera pose
regression with Transformers, where encoders are used to aggregate activation
maps with self-attention and decoders transform latent features and scenes
encoding into pose predictions. This allows our model to focus on general
features that are informative for localization, while embedding multiple scenes
in parallel. We extend our previous MS-Transformer approach
\cite{shavit2021learning} by introducing a mixed classification-regression
architecture that improves the localization accuracy. Our method is evaluated
on commonly benchmark indoor and outdoor datasets and has been shown to exceed
both multi-scene and state-of-the-art single-scene absolute pose regressors.
</p></li>
</ul>

<h3>Title: Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts. (arXiv:2308.11793v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11793">http://arxiv.org/abs/2308.11793</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11793]] Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts(http://arxiv.org/abs/2308.11793)</code></li>
<li>Summary: <p>Cross-scene generalizable NeRF models, which can directly synthesize novel
views of unseen scenes, have become a new spotlight of the NeRF field. Several
existing attempts rely on increasingly end-to-end "neuralized" architectures,
i.e., replacing scene representation and/or rendering modules with performant
neural networks such as transformers, and turning novel view synthesis into a
feed-forward inference pipeline. While those feedforward "neuralized"
architectures still do not fit diverse scenes well out of the box, we propose
to bridge them with the powerful Mixture-of-Experts (MoE) idea from large
language models (LLMs), which has demonstrated superior generalization ability
by balancing between larger overall model capacity and flexible per-instance
specialization. Starting from a recent generalizable NeRF architecture called
GNT, we first demonstrate that MoE can be neatly plugged in to enhance the
model. We further customize a shared permanent expert and a geometry-aware
consistency loss to enforce cross-scene consistency and spatial smoothness
respectively, which are essential for generalizable view synthesis. Our
proposed model, dubbed GNT with Mixture-of-View-Experts (GNT-MOVE), has
experimentally shown state-of-the-art results when transferring to unseen
scenes, indicating remarkably better cross-scene generalization in both
zero-shot and few-shot settings. Our codes are available at
https://github.com/VITA-Group/GNT-MOVE.
</p></li>
</ul>

<h3>Title: A Unified Framework for 3D Point Cloud Visual Grounding. (arXiv:2308.11887v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11887">http://arxiv.org/abs/2308.11887</a></li>
<li>Code URL: https://github.com/leon1207/3dreftr</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11887]] A Unified Framework for 3D Point Cloud Visual Grounding(http://arxiv.org/abs/2308.11887)</code></li>
<li>Summary: <p>3D point cloud visual grounding plays a critical role in 3D scene
comprehension, encompassing 3D referring expression comprehension (3DREC) and
segmentation (3DRES). We argue that 3DREC and 3DRES should be unified in one
framework, which is also a natural progression in the community. To explain,
3DREC can help 3DRES locate the referent, while 3DRES can also facilitate 3DREC
via more finegrained language-visual alignment. To achieve this, this paper
takes the initiative step to integrate 3DREC and 3DRES into a unified
framework, termed 3D Referring Transformer (3DRefTR). Its key idea is to build
upon a mature 3DREC model and leverage ready query embeddings and visual tokens
from the 3DREC model to construct a dedicated mask branch. Specially, we
propose Superpoint Mask Branch, which serves a dual purpose: i) By leveraging
the heterogeneous CPU-GPU parallelism, while the GPU is occupied generating
visual tokens, the CPU concurrently produces superpoints, equivalently
accomplishing the upsampling computation; ii) By harnessing on the inherent
association between the superpoints and point cloud, it eliminates the heavy
computational overhead on the high-resolution visual features for upsampling.
This elegant design enables 3DRefTR to achieve both well-performing 3DRES and
3DREC capacities with only a 6% additional latency compared to the original
3DREC model. Empirical evaluations affirm the superiority of 3DRefTR.
Specifically, on the ScanRefer dataset, 3DRefTR surpasses the state-of-the-art
3DRES method by 12.43% in mIoU and improves upon the SOTA 3DREC method by 0.6%
Acc@0.25IoU.
</p></li>
</ul>

<h3>Title: Learning Bottleneck Transformer for Event Image-Voxel Feature Fusion based Classification. (arXiv:2308.11937v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11937">http://arxiv.org/abs/2308.11937</a></li>
<li>Code URL: https://github.com/event-ahu/efv_event_classification</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11937]] Learning Bottleneck Transformer for Event Image-Voxel Feature Fusion based Classification(http://arxiv.org/abs/2308.11937)</code></li>
<li>Summary: <p>Recognizing target objects using an event-based camera draws more and more
attention in recent years. Existing works usually represent the event streams
into point-cloud, voxel, image, etc, and learn the feature representations
using various deep neural networks. Their final results may be limited by the
following factors: monotonous modal expressions and the design of the network
structure. To address the aforementioned challenges, this paper proposes a
novel dual-stream framework for event representation, extraction, and fusion.
This framework simultaneously models two common representations: event images
and event voxels. By utilizing Transformer and Structured Graph Neural Network
(GNN) architectures, spatial information and three-dimensional stereo
information can be learned separately. Additionally, a bottleneck Transformer
is introduced to facilitate the fusion of the dual-stream information.
Extensive experiments demonstrate that our proposed framework achieves
state-of-the-art performance on two widely used event-based classification
datasets. The source code of this work is available at:
\url{https://github.com/Event-AHU/EFV_event_classification}
</p></li>
</ul>

<h3>Title: EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE. (arXiv:2308.11971v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11971">http://arxiv.org/abs/2308.11971</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11971]] EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE(http://arxiv.org/abs/2308.11971)</code></li>
<li>Summary: <p>Building scalable vision-language models to learn from diverse, multimodal
data remains an open challenge. In this paper, we introduce an Efficient
Vision-languagE foundation model, namely EVE, which is one unified multimodal
Transformer pre-trained solely by one unified pre-training task. Specifically,
EVE encodes both vision and language within a shared Transformer network
integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which
capture modality-specific information by selectively switching to different
experts. To unify pre-training tasks of vision and language, EVE performs
masked signal modeling on image-text pairs to reconstruct masked signals, i.e.,
image pixels and text tokens, given visible signals. This simple yet effective
pre-training objective accelerates training by 3.5x compared to the model
pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing
to the combination of the unified architecture and pre-training task, EVE is
easy to scale up, enabling better downstream performance with fewer resources
and faster training speed. Despite its simplicity, EVE achieves
state-of-the-art performance on various vision-language downstream tasks,
including visual question answering, visual reasoning, and image-text
retrieval.
</p></li>
</ul>

<h3>Title: Local Distortion Aware Efficient Transformer Adaptation for Image Quality Assessment. (arXiv:2308.12001v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12001">http://arxiv.org/abs/2308.12001</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12001]] Local Distortion Aware Efficient Transformer Adaptation for Image Quality Assessment(http://arxiv.org/abs/2308.12001)</code></li>
<li>Summary: <p>Image Quality Assessment (IQA) constitutes a fundamental task within the
field of computer vision, yet it remains an unresolved challenge, owing to the
intricate distortion conditions, diverse image contents, and limited
availability of data. Recently, the community has witnessed the emergence of
numerous large-scale pretrained foundation models, which greatly benefit from
dramatically increased data and parameter capacities. However, it remains an
open problem whether the scaling law in high-level tasks is also applicable to
IQA task which is closely related to low-level clues. In this paper, we
demonstrate that with proper injection of local distortion features, a larger
pretrained and fixed foundation model performs better in IQA tasks.
Specifically, for the lack of local distortion structure and inductive bias of
vision transformer (ViT), alongside the large-scale pretrained ViT, we use
another pretrained convolution neural network (CNN), which is well known for
capturing the local structure, to extract multi-scale image features. Further,
we propose a local distortion extractor to obtain local distortion features
from the pretrained CNN and a local distortion injector to inject the local
distortion features into ViT. By only training the extractor and injector, our
method can benefit from the rich knowledge in the powerful foundation models
and achieve state-of-the-art performance on popular IQA datasets, indicating
that IQA is not only a low-level problem but also benefits from stronger
high-level features drawn from large-scale pretrained models.
</p></li>
</ul>

<h3>Title: CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No. (arXiv:2308.12213v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12213">http://arxiv.org/abs/2308.12213</a></li>
<li>Code URL: https://github.com/xmed-lab/clipn</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12213]] CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No(http://arxiv.org/abs/2308.12213)</code></li>
<li>Summary: <p>Out-of-distribution (OOD) detection refers to training the model on an
in-distribution (ID) dataset to classify whether the input images come from
unknown classes. Considerable effort has been invested in designing various OOD
detection methods based on either convolutional neural networks or
transformers. However, zero-shot OOD detection methods driven by CLIP, which
only require class names for ID, have received less attention. This paper
presents a novel method, namely CLIP saying "no" (\textbf{CLIPN}), which
empowers the logic of saying "no" within CLIP. Our key motivation is to equip
CLIP with the capability of distinguishing OOD and ID samples using
positive-semantic prompts and negation-semantic prompts. Specifically, we
design a novel learnable "no" prompt and a "no" text encoder to capture
negation semantics within images. Subsequently, we introduce two loss
functions: the image-text binary-opposite loss and the text semantic-opposite
loss, which we use to teach CLIPN to associate images with "no" prompts,
thereby enabling it to identify unknown samples. Furthermore, we propose two
threshold-free inference algorithms to perform OOD detection by utilizing
negation semantics from "no" prompts and the text encoder. Experimental results
on 9 benchmark datasets (3 ID datasets and 6 OOD datasets) for the OOD
detection task demonstrate that CLIPN, based on ViT-B-16, outperforms 7
well-used algorithms by at least 2.34\% and 11.64\% in terms of AUROC and FPR95
for zero-shot OOD detection on ImageNet-1K. Our CLIPN can serve as a solid
foundation for effectively leveraging CLIP in downstream OOD tasks. The code is
available on
https://github.com/xmed-lab/CLIPN}{https://github.com/xmed-lab/CLIPN.
</p></li>
</ul>

<h3>Title: SG-Former: Self-guided Transformer with Evolving Token Reallocation. (arXiv:2308.12216v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12216">http://arxiv.org/abs/2308.12216</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12216]] SG-Former: Self-guided Transformer with Evolving Token Reallocation(http://arxiv.org/abs/2308.12216)</code></li>
<li>Summary: <p>Vision Transformer has demonstrated impressive success across various vision
tasks. However, its heavy computation cost, which grows quadratically with
respect to the token sequence length, largely limits its power in handling
large feature maps. To alleviate the computation cost, previous works rely on
either fine-grained self-attentions restricted to local small regions, or
global self-attentions but to shorten the sequence length resulting in coarse
granularity. In this paper, we propose a novel model, termed as Self-guided
Transformer~(SG-Former), towards effective global self-attention with adaptive
fine granularity. At the heart of our approach is to utilize a significance
map, which is estimated through hybrid-scale self-attention and evolves itself
during training, to reallocate tokens based on the significance of each region.
Intuitively, we assign more tokens to the salient regions for achieving
fine-grained attention, while allocating fewer tokens to the minor regions in
exchange for efficiency and global receptive fields. The proposed SG-Former
achieves performance superior to state of the art: our base size model achieves
\textbf{84.7\%} Top-1 accuracy on ImageNet-1K, \textbf{51.2mAP} bbAP on CoCo,
\textbf{52.7mIoU} on ADE20K surpassing the Swin Transformer by \textbf{+1.3\% /
+2.7 mAP/ +3 mIoU}, with lower computation costs and fewer parameters. The code
is available at
\href{https://github.com/OliverRensu/SG-Former}{https://github.com/OliverRensu/SG-Former}
</p></li>
</ul>

<h3>Title: Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test. (arXiv:2308.11827v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11827">http://arxiv.org/abs/2308.11827</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11827]] Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test(http://arxiv.org/abs/2308.11827)</code></li>
<li>Summary: <p>Large language models such as Open AI's Generative Pre-trained Transformer
(GPT) models are proficient at answering questions, but their knowledge is
confined to the information present in their training data. This limitation
renders them ineffective when confronted with questions about recent
developments or non-public documents. Our research proposes a method that
enables GPT models to answer questions by employing context from an information
source not previously included in their training data. The methodology includes
preprocessing of contextual information, the embedding of contexts and queries,
constructing prompt through the integration of context embeddings, and
generating answers using GPT models. We applied this method in a controlled
test scenario using the California Driver's Handbook as the information source.
The GPT-3 model achieved a 96% passing score on a set of 50 sample driving
knowledge test questions. In contrast, without context, the model's passing
score fell to 82%. However, the model still fails to answer some questions
correctly even with providing library of context, highlighting room for
improvement. The research also examined the impact of prompt length and context
format, on the model's performance. Overall, the study provides insights into
the limitations and potential improvements for GPT models in question-answering
tasks.
</p></li>
</ul>

<h3>Title: Cabrita: closing the gap for foreign languages. (arXiv:2308.11878v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11878">http://arxiv.org/abs/2308.11878</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11878]] Cabrita: closing the gap for foreign languages(http://arxiv.org/abs/2308.11878)</code></li>
<li>Summary: <p>The strategy of training the model from scratch in a specific language or
domain serves two essential purposes: i) enhancing performance in the
particular linguistic or domain context, and ii) ensuring effective
tokenization. The main limitation inherent to this approach lies in the
associated cost, which can reach six to seven-digit dollar values, depending on
the model size and the number of parameters involved.
</p>
<p>The main solution to overcome the cost challenge is to rely on available
pre-trained models, which, despite recent advancements such as the LLaMA and
LLaMA-2 models, still demonstrate inefficiency for certain specific domain
problems or prove ineffective in scenarios involving conversational memory
resources, given the large number of tokens required to represent text.
</p>
<p>To overcome this issue, we present a methodology named Cabrita, which, as our
research demonstrates, successfully addresses the performance and efficient
tokenization problem, all at an affordable cost. We believe that this
methodology can be applied to any transformer-like architecture model. To
validate the study, we conducted continuous pre-training exclusively using
Portuguese text on a 3-billion-parameter model known as OpenLLaMA, resulting in
a model named openCabrita 3B. The openCabrita 3B also features a new tokenizer
that results in a significant reduction in the number of tokens required to
represent the text. In our assessment, for few-shot learning tasks, we achieved
similar results with this 3B model compared to a traditional continuous
pre-training approach as well as to 7B models English pre-trained models.
</p></li>
</ul>

<h3>Title: Aligning Language Models with Offline Reinforcement Learning from Human Feedback. (arXiv:2308.12050v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12050">http://arxiv.org/abs/2308.12050</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12050]] Aligning Language Models with Offline Reinforcement Learning from Human Feedback(http://arxiv.org/abs/2308.12050)</code></li>
<li>Summary: <p>Learning from human preferences is crucial for language models (LMs) to
effectively cater to human needs and societal values. Previous research has
made notable progress by leveraging human feedback to follow instructions.
However, these approaches rely primarily on online reinforcement learning (RL)
techniques like Proximal Policy Optimization (PPO), which have been proven
unstable and challenging to tune for language models. Moreover, PPO requires
complex distributed system implementation, hindering the efficiency of
large-scale distributed training. In this study, we propose an offline
reinforcement learning from human feedback (RLHF) framework to align LMs using
pre-generated samples without interacting with RL environments. Specifically,
we explore maximum likelihood estimation (MLE) with filtering, reward-weighted
regression (RWR), and Decision Transformer (DT) to align language models to
human preferences. By employing a loss function similar to supervised
fine-tuning, our methods ensure more stable model training than PPO with a
simple machine learning system~(MLSys) and much fewer (around 12.3\%) computing
resources. Experimental results demonstrate the DT alignment outperforms other
Offline RLHF methods and is better than PPO.
</p></li>
</ul>

<h3>Title: Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting. (arXiv:2308.11946v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11946">http://arxiv.org/abs/2308.11946</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11946]] Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting(http://arxiv.org/abs/2308.11946)</code></li>
<li>Summary: <p>Multivariate Time Series (MTS) forecasting involves modeling temporal
dependencies within historical records. Transformers have demonstrated
remarkable performance in MTS forecasting due to their capability to capture
long-term dependencies. However, prior work has been confined to modeling
temporal dependencies at either a fixed scale or multiple scales that
exponentially increase (most with base 2). This limitation hinders their
effectiveness in capturing diverse seasonalities, such as hourly and daily
patterns. In this paper, we introduce a dimension invariant embedding technique
that captures short-term temporal dependencies and projects MTS data into a
higher-dimensional space, while preserving the dimensions of time steps and
variables in MTS data. Furthermore, we present a novel Multi-scale Transformer
Pyramid Network (MTPNet), specifically designed to effectively capture temporal
dependencies at multiple unconstrained scales. The predictions are inferred
from multi-scale latent representations obtained from transformers at various
scales. Extensive experiments on nine benchmark datasets demonstrate that the
proposed MTPNet outperforms recent state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference. (arXiv:2308.12066v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12066">http://arxiv.org/abs/2308.12066</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12066]] Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference(http://arxiv.org/abs/2308.12066)</code></li>
<li>Summary: <p>Large language models (LLMs) based on transformers have made significant
strides in recent years, the success of which is driven by scaling up their
model size. Despite their high algorithmic performance, the computational and
memory requirements of LLMs present unprecedented challenges. To tackle the
high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture
was introduced which is able to scale its model size without proportionally
scaling up its computational requirements. Unfortunately, MoE's high memory
demands and dynamic activation of sparse experts restrict its applicability to
real-world problems. Previous solutions that offload MoE's memory-hungry expert
parameters to CPU memory fall short because the latency to migrate activated
experts from CPU to GPU incurs high performance overhead. Our proposed
Pre-gated MoE system effectively tackles the compute and memory challenges of
conventional MoE architectures using our algorithm-system co-design. Pre-gated
MoE employs our novel pre-gating function which alleviates the dynamic nature
of sparse expert activation, allowing our proposed system to address the large
memory footprint of MoEs while also achieving high performance. We demonstrate
that Pre-gated MoE is able to improve performance, reduce GPU memory
consumption, while also maintaining the same level of model quality. These
features allow our Pre-gated MoE system to cost-effectively deploy large-scale
LLMs using just a single GPU with high performance.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Weakly Supervised Face and Whole Body Recognition in Turbulent Environments. (arXiv:2308.11757v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11757">http://arxiv.org/abs/2308.11757</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11757]] Weakly Supervised Face and Whole Body Recognition in Turbulent Environments(http://arxiv.org/abs/2308.11757)</code></li>
<li>Summary: <p>Face and person recognition have recently achieved remarkable success under
challenging scenarios, such as off-pose and cross-spectrum matching. However,
long-range recognition systems are often hindered by atmospheric turbulence,
leading to spatially and temporally varying distortions in the image. Current
solutions rely on generative models to reconstruct a turbulent-free image, but
often preserve photo-realism instead of discriminative features that are
essential for recognition. This can be attributed to the lack of large-scale
datasets of turbulent and pristine paired images, necessary for optimal
reconstruction. To address this issue, we propose a new weakly supervised
framework that employs a parameter-efficient self-attention module to generate
domain agnostic representations, aligning turbulent and pristine images into a
common subspace. Additionally, we introduce a new tilt map estimator that
predicts geometric distortions observed in turbulent images. This estimate is
used to re-rank gallery matches, resulting in up to 13.86\% improvement in
rank-1 accuracy. Our method does not require synthesizing turbulent-free images
or ground-truth paired images, and requires significantly fewer annotated
samples, enabling more practical and rapid utility of increasingly large
datasets. We analyze our framework using two datasets -- Long-Range Face
Identification Dataset (LRFID) and BRIAR Government Collection 1 (BGC1) --
achieving enhanced discriminability under varying turbulence and standoff
distance.
</p></li>
</ul>

<h3>Title: LFS-GAN: Lifelong Few-Shot Image Generation. (arXiv:2308.11917v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11917">http://arxiv.org/abs/2308.11917</a></li>
<li>Code URL: https://github.com/jjuon/lfs-gan</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11917]] LFS-GAN: Lifelong Few-Shot Image Generation(http://arxiv.org/abs/2308.11917)</code></li>
<li>Summary: <p>We address a challenging lifelong few-shot image generation task for the
first time. In this situation, a generative model learns a sequence of tasks
using only a few samples per task. Consequently, the learned model encounters
both catastrophic forgetting and overfitting problems at a time. Existing
studies on lifelong GANs have proposed modulation-based methods to prevent
catastrophic forgetting. However, they require considerable additional
parameters and cannot generate high-fidelity and diverse images from limited
data. On the other hand, the existing few-shot GANs suffer from severe
catastrophic forgetting when learning multiple tasks. To alleviate these
issues, we propose a framework called Lifelong Few-Shot GAN (LFS-GAN) that can
generate high-quality and diverse images in lifelong few-shot image generation
task. Our proposed framework learns each task using an efficient task-specific
modulator - Learnable Factorized Tensor (LeFT). LeFT is rank-constrained and
has a rich representation ability due to its unique reconstruction technique.
Furthermore, we propose a novel mode seeking loss to improve the diversity of
our model in low-data circumstances. Extensive experiments demonstrate that the
proposed LFS-GAN can generate high-fidelity and diverse images without any
forgetting and mode collapse in various domains, achieving state-of-the-art in
lifelong few-shot image generation task. Surprisingly, we find that our LFS-GAN
even outperforms the existing few-shot GANs in the few-shot image generation
task. The code is available at Github.
</p></li>
</ul>

<h3>Title: A Generative Approach for Image Registration of Visible-Thermal (VT) Cancer Faces. (arXiv:2308.12271v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12271">http://arxiv.org/abs/2308.12271</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12271]] A Generative Approach for Image Registration of Visible-Thermal (VT) Cancer Faces(http://arxiv.org/abs/2308.12271)</code></li>
<li>Summary: <p>Since thermal imagery offers a unique modality to investigate pain, the U.S.
National Institutes of Health (NIH) has collected a large and diverse set of
cancer patient facial thermograms for AI-based pain research. However,
differing angles from camera capture between thermal and visible sensors has
led to misalignment between Visible-Thermal (VT) images. We modernize the
classic computer vision task of image registration by applying and modifying a
generative alignment algorithm to register VT cancer faces, without the need
for a reference or alignment parameters. By registering VT faces, we
demonstrate that the quality of thermal images produced in the generative AI
downstream task of Visible-to-Thermal (V2T) image translation significantly
improves up to 52.5\%, than without registration. Images in this paper have
been approved by the NIH NCI for public dissemination.
</p></li>
</ul>

<h3>Title: CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images. (arXiv:2308.12288v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12288">http://arxiv.org/abs/2308.12288</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12288]] CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images(http://arxiv.org/abs/2308.12288)</code></li>
<li>Summary: <p>We present a method for teaching machines to understand and model the
underlying spatial common sense of diverse human-object interactions in 3D in a
self-supervised way. This is a challenging task, as there exist specific
manifolds of the interactions that can be considered human-like and natural,
but the human pose and the geometry of objects can vary even for similar
interactions. Such diversity makes the annotating task of 3D interactions
difficult and hard to scale, which limits the potential to reason about that in
a supervised way. One way of learning the 3D spatial relationship between
humans and objects during interaction is by showing multiple 2D images captured
from different viewpoints when humans interact with the same type of objects.
The core idea of our method is to leverage a generative model that produces
high-quality 2D images from an arbitrary text prompt input as an "unbounded"
data generator with effective controllability and view diversity. Despite its
imperfection of the image quality over real images, we demonstrate that the
synthesized images are sufficient to learn the 3D human-object spatial
relations. We present multiple strategies to leverage the synthesized images,
including (1) the first method to leverage a generative image model for 3D
human-object spatial relation learning; (2) a framework to reason about the 3D
spatial relations from inconsistent 2D cues in a self-supervised manner via 3D
occupancy reasoning with pose canonicalization; (3) semantic clustering to
disambiguate different types of interactions with the same object types; and
(4) a novel metric to assess the quality of 3D spatial learning of interaction.
Project Page: https://jellyheadandrew.github.io/projects/chorus
</p></li>
</ul>

<h3>Title: Maintaining Plasticity via Regenerative Regularization. (arXiv:2308.11958v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11958">http://arxiv.org/abs/2308.11958</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11958]] Maintaining Plasticity via Regenerative Regularization(http://arxiv.org/abs/2308.11958)</code></li>
<li>Summary: <p>In continual learning, plasticity refers to the ability of an agent to
quickly adapt to new information. Neural networks are known to lose plasticity
when processing non-stationary data streams. In this paper, we propose L2 Init,
a very simple approach for maintaining plasticity by incorporating in the loss
function L2 regularization toward initial parameters. This is very similar to
standard L2 regularization (L2), the only difference being that L2 regularizes
toward the origin. L2 Init is simple to implement and requires selecting only a
single hyper-parameter. The motivation for this method is the same as that of
methods that reset neurons or parameter values. Intuitively, when recent losses
are insensitive to particular parameters, these parameters drift toward their
initial values. This prepares parameters to adapt quickly to new tasks. On
simple problems representative of different types of nonstationarity in
continual learning, we demonstrate that L2 Init consistently mitigates
plasticity loss. We additionally find that our regularization term reduces
parameter magnitudes and maintains a high effective feature rank.
</p></li>
</ul>

<h3>Title: Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11978">http://arxiv.org/abs/2308.11978</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11978]] Will More Expressive Graph Neural Networks do Better on Generative Tasks?(http://arxiv.org/abs/2308.11978)</code></li>
<li>Summary: <p>Graph generation poses a significant challenge as it involves predicting a
complete graph with multiple nodes and edges based on simply a given label.
This task also carries fundamental importance to numerous real-world
applications, including de-novo drug and molecular design. In recent years,
several successful methods have emerged in the field of graph generation.
However, these approaches suffer from two significant shortcomings: (1) the
underlying Graph Neural Network (GNN) architectures used in these methods are
often underexplored; and (2) these methods are often evaluated on only a
limited number of metrics. To fill this gap, we investigate the expressiveness
of GNNs under the context of the molecular graph generation task, by replacing
the underlying GNNs of graph generative models with more expressive GNNs.
Specifically, we analyse the performance of six GNNs in two different
generative frameworks (GCPN and GraphAF), on six different molecular generative
objectives on the ZINC-250k dataset. Through our extensive experiments, we
demonstrate that advanced GNNs can indeed improve the performance of GCPN and
GraphAF on molecular generation tasks, but GNN expressiveness is not a
necessary condition for a good GNN-based generative model. Moreover, we show
that GCPN and GraphAF with advanced GNNs can achieve state-of-the-art results
across 17 other non-GNN-based graph generative approaches, such as variational
autoencoders and Bayesian optimisation models, on the proposed molecular
generative objectives (DRD2, Median1, Median2), which are important metrics for
de-novo molecular design.
</p></li>
</ul>

<h3>Title: How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy. (arXiv:2308.12252v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12252">http://arxiv.org/abs/2308.12252</a></li>
<li>Code URL: https://github.com/maozj6/hsai-predictor</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12252]] How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy(http://arxiv.org/abs/2308.12252)</code></li>
<li>Summary: <p>End-to-end learning has emerged as a major paradigm for developing autonomous
systems. Unfortunately, with its performance and convenience comes an even
greater challenge of safety assurance. A key factor of this challenge is the
absence of the notion of a low-dimensional and interpretable dynamical state,
around which traditional assurance methods revolve. Focusing on the online
safety prediction problem, this paper proposes a configurable family of
learning pipelines based on generative world models, which do not require
low-dimensional states. To implement these pipelines, we overcome the
challenges of learning safety-informed latent representations and missing
safety labels under prediction-induced distribution shift. These pipelines come
with statistical calibration guarantees on their safety chance predictions
based on conformal prediction. We perform an extensive evaluation of the
proposed learning pipelines on two case studies of image-controlled systems: a
racing car and a cartpole.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages. (arXiv:2308.12038v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12038">http://arxiv.org/abs/2308.12038</a></li>
<li>Code URL: https://github.com/openbmb/viscpm</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12038]] Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages(http://arxiv.org/abs/2308.12038)</code></li>
<li>Summary: <p>Recently there has been a significant surge in multimodal learning in terms
of both image-to-text and text-to-image generation. However, the success is
typically limited to English, leaving other languages largely behind. Building
a competitive counterpart in other languages is highly challenging due to the
low-resource nature of non-English multimodal data (i.e., lack of large-scale,
high-quality image-text data). In this work, we propose MPM, an effective
training paradigm for training large multimodal models in low-resource
languages. MPM demonstrates that Multilingual language models can Pivot
zero-shot Multimodal learning across languages. Specifically, based on a strong
multilingual large language model, multimodal models pretrained on English-only
image-text data can well generalize to other languages in a zero-shot manner
for both image-to-text and text-to-image generation, even surpassing models
trained on image-text data in native languages. Taking Chinese as a practice of
MPM, we build large multimodal models VisCPM in image-to-text and text-to-image
generation, which achieve state-of-the-art (open-source) performance in
Chinese. To facilitate future research, we open-source codes and model weights
at https://github.com/OpenBMB/VisCPM.git.
</p></li>
</ul>

<h3>Title: InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4. (arXiv:2308.12067v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12067">http://arxiv.org/abs/2308.12067</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12067]] InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4(http://arxiv.org/abs/2308.12067)</code></li>
<li>Summary: <p>Multimodal large language models acquire their instruction-following
capabilities through a two-stage training process: pre-training on image-text
pairs and fine-tuning on supervised vision-language instruction data. Recent
studies have shown that large language models can achieve satisfactory results
even with a limited amount of high-quality instruction-following data. In this
paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset
comprising only 200 examples, amounting to approximately 6% of the
instruction-following data used in the alignment dataset for MiniGPT-4. We
first propose several metrics to access the quality of multimodal instruction
data. Based on these metrics, we present a simple and effective data selector
to automatically identify and filter low-quality vision-language data. By
employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on
various evaluations (e.g., visual question answering, GPT-4 preference).
Overall, our findings demonstrate that less but high-quality instruction tuning
data is efficient to enable multimodal large language models to generate better
output.
</p></li>
</ul>

<h3>Title: Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11730">http://arxiv.org/abs/2308.11730</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11730]] Knowledge Graph Prompting for Multi-Document Question Answering(http://arxiv.org/abs/2308.11730)</code></li>
<li>Summary: <p>The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has
achieved remarkable success in open-domain question answering (OD-QA). However,
few works explore this paradigm in the scenario of multi-document question
answering (MD-QA), a task demanding a thorough understanding of the logical
associations among the contents and structures of different documents. To fill
this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to
formulate the right context in prompting LLMs for MD-QA, which consists of a
graph construction module and a graph traversal module. For graph construction,
we create a knowledge graph (KG) over multiple documents with nodes symbolizing
passages or document structures (e.g., pages/tables), and edges denoting the
semantic/lexical similarity between passages or intra-document structural
relations. For graph traversal, we design an LM-guided graph traverser that
navigates across nodes and gathers supporting passages assisting LLMs in MD-QA.
The constructed graph serves as the global ruler that regulates the
transitional space among passages and reduces retrieval latency. Concurrently,
the LM-guided traverser acts as a local navigator that gathers pertinent
context to progressively approach the question and guarantee retrieval quality.
Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the
potential of leveraging graphs in enhancing the prompt design for LLMs. Our
code is at https://github.com/YuWVandy/KG-LLM-MDQA.
</p></li>
</ul>

<h3>Title: KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases. (arXiv:2308.11761v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11761">http://arxiv.org/abs/2308.11761</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11761]] KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases(http://arxiv.org/abs/2308.11761)</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated impressive impact in the field
of natural language processing, but they still struggle with several issues
regarding, such as completeness, timeliness, faithfulness and adaptability.
While recent efforts have focuses on connecting LLMs with external knowledge
sources, the integration of knowledge bases (KBs) remains understudied and
faces several challenges. In this paper, we introduce KnowledGPT, a
comprehensive framework to bridge LLMs with various knowledge bases,
facilitating both the retrieval and storage of knowledge. The retrieval process
employs the program of thought prompting, which generates search language for
KBs in code format with pre-defined functions for KB operations. Besides
retrieval, KnowledGPT offers the capability to store knowledge in a
personalized KB, catering to individual user demands. With extensive
experiments, we show that by integrating LLMs with KBs, KnowledGPT properly
answers a broader range of questions requiring world knowledge compared with
vanilla LLMs, utilizing both knowledge existing in widely-known KBs and
extracted into personalized KBs.
</p></li>
</ul>

<h3>Title: Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11764">http://arxiv.org/abs/2308.11764</a></li>
<li>Code URL: https://github.com/engsalem/halo</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11764]] Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models(http://arxiv.org/abs/2308.11764)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP). Although convenient for research and practical applications, open-source
LLMs with fewer parameters often suffer from severe hallucinations compared to
their larger counterparts. This paper focuses on measuring and reducing
hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs
that are publicly available for research and commercial applications. We
introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed
to quantify the severity of hallucinations in LLMs. Additionally, we explore
techniques like knowledge injection and teacher-student approaches to alleviate
hallucinations in low-parameter LLMs. Our experiments effectively demonstrate
the reduction of hallucinations in challenging domains for these LLMs.
</p></li>
</ul>

<h3>Title: Bridging the Gap: Deciphering Tabular Data Using Large Language Model. (arXiv:2308.11891v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11891">http://arxiv.org/abs/2308.11891</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11891]] Bridging the Gap: Deciphering Tabular Data Using Large Language Model(http://arxiv.org/abs/2308.11891)</code></li>
<li>Summary: <p>In the realm of natural language processing, the understanding of tabular
data has perpetually stood as a focal point of scholarly inquiry. The emergence
of expansive language models, exemplified by the likes of ChatGPT, has ushered
in a wave of endeavors wherein researchers aim to harness these models for
tasks related to table-based question answering. Central to our investigative
pursuits is the elucidation of methodologies that amplify the aptitude of such
large language models in discerning both the structural intricacies and
inherent content of tables, ultimately facilitating their capacity to provide
informed responses to pertinent queries. To this end, we have architected a
distinctive module dedicated to the serialization of tables for seamless
integration with expansive language models. Additionally, we've instituted a
corrective mechanism within the model to rectify potential inaccuracies.
Experimental results indicate that, although our proposed method trails the
SOTA by approximately 11.7% in overall metrics, it surpasses the SOTA by about
1.2% in tests on specific datasets. This research marks the first application
of large language models to table-based question answering tasks, enhancing the
model's comprehension of both table structures and content.
</p></li>
</ul>

<h3>Title: Prompt-Based Length Controlled Generation with Reinforcement Learning. (arXiv:2308.12030v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12030">http://arxiv.org/abs/2308.12030</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12030]] Prompt-Based Length Controlled Generation with Reinforcement Learning(http://arxiv.org/abs/2308.12030)</code></li>
<li>Summary: <p>Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted
great attention given their surprising improvement and performance. Length
controlled generation of LLMs emerges as an important topic, which also enables
users to fully leverage the capability of LLMs in more real-world scenarios
like generating a proper answer or essay of a desired length. In addition, the
autoregressive generation in LLMs is extremely time-consuming, while the
ability of controlling this generated length can arbitrarily reduce the
inference cost by limiting the length, and thus satisfy different needs.
Therefore, we aim to propose a prompt-based length control method to achieve
this length controlled generation, which can also be widely applied in
GPT-style LLMs. In particular, we adopt reinforcement learning with the reward
signal given by either trainable or rule-based reward model, which further
affects the generation of LLMs via rewarding a pre-defined target length.
Experiments show that our method significantly improves the accuracy of
prompt-based length control for summarization task on popular datasets like
CNNDM and NYT. We believe this length-controllable ability can provide more
potentials towards the era of LLMs.
</p></li>
</ul>

<h3>Title: From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning. (arXiv:2308.12032v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12032">http://arxiv.org/abs/2308.12032</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12032]] From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning(http://arxiv.org/abs/2308.12032)</code></li>
<li>Summary: <p>In the realm of Large Language Models, the balance between instruction data
quality and quantity has become a focal point. Recognizing this, we introduce a
self-guided methodology for LLMs to autonomously discern and select cherry
samples from vast open-source datasets, effectively minimizing manual curation
and potential cost for instruction tuning an LLM. Our key innovation, the
Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to
identify discrepancies between a model's expected responses and its autonomous
generation prowess. Through the adept application of IFD, cherry samples are
pinpointed, leading to a marked uptick in model training efficiency. Empirical
validations on renowned datasets like Alpaca and WizardLM underpin our
findings; with a mere 10% of conventional data input, our strategy showcases
improved results. This synthesis of self-guided cherry-picking and the IFD
metric signifies a transformative leap in the optimization of LLMs, promising
both efficiency and resource-conscious advancements.
</p></li>
</ul>

<h3>Title: PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine. (arXiv:2308.12033v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12033">http://arxiv.org/abs/2308.12033</a></li>
<li>Code URL: https://github.com/zcrwind/prefer</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12033]] PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine(http://arxiv.org/abs/2308.12033)</code></li>
<li>Summary: <p>As an effective tool for eliciting the power of Large Language Models (LLMs),
prompting has recently demonstrated unprecedented abilities across a variety of
complex tasks. To further improve the performance, prompt ensemble has
attracted substantial interest for tackling the hallucination and instability
of LLMs. However, existing methods usually adopt a two-stage paradigm, which
requires a pre-prepared set of prompts with substantial manual effort, and is
unable to perform directed optimization for different weak learners. In this
paper, we propose a simple, universal, and automatic method named PREFER (Pompt
Ensemble learning via Feedback-Reflect-Refine) to address the stated
limitations. Specifically, given the fact that weak learners are supposed to
focus on hard examples during boosting, PREFER builds a feedback mechanism for
reflecting on the inadequacies of existing weak learners. Based on this, the
LLM is required to automatically synthesize new prompts for iterative
refinement. Moreover, to enhance stability of the prompt effect evaluation, we
propose a novel prompt bagging method involving forward and backward thinking,
which is superior to majority voting and is beneficial for both feedback and
weight calculation in boosting. Extensive experiments demonstrate that our
PREFER achieves state-of-the-art performance in multiple types of tasks by a
significant margin. We have made our code publicly available.
</p></li>
</ul>

<h3>Title: Instruction Position Matters in Sequence Generation with Large Language Models. (arXiv:2308.12097v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12097">http://arxiv.org/abs/2308.12097</a></li>
<li>Code URL: https://github.com/adaxry/post-instruction</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12097]] Instruction Position Matters in Sequence Generation with Large Language Models(http://arxiv.org/abs/2308.12097)</code></li>
<li>Summary: <p>Large language models (LLMs) are capable of performing conditional sequence
generation tasks, such as translation or summarization, through instruction
fine-tuning. The fine-tuning data is generally sequentially concatenated from a
specific task instruction, an input sentence, and the corresponding response.
Considering the locality modeled by the self-attention mechanism of LLMs, these
models face the risk of instruction forgetting when generating responses for
long input sentences. To mitigate this issue, we propose enhancing the
instruction-following capability of LLMs by shifting the position of task
instructions after the input sentences. Theoretical analysis suggests that our
straightforward method can alter the model's learning focus, thereby
emphasizing the training of instruction-following capabilities. Concurrently,
experimental results demonstrate that our approach consistently outperforms
traditional settings across various model scales (1B / 7B / 13B) and different
sequence generation tasks (translation and summarization), without any
additional data or annotation costs. Notably, our method significantly improves
the zero-shot performance on conditional sequence generation, e.g., up to 9.7
BLEU points on WMT zero-shot translation tasks.
</p></li>
</ul>

<h3>Title: Evaluation of Faithfulness Using the Longest Supported Subsequence. (arXiv:2308.12157v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12157">http://arxiv.org/abs/2308.12157</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12157]] Evaluation of Faithfulness Using the Longest Supported Subsequence(http://arxiv.org/abs/2308.12157)</code></li>
<li>Summary: <p>As increasingly sophisticated language models emerge, their trustworthiness
becomes a pivotal issue, especially in tasks such as summarization and
question-answering. Ensuring their responses are contextually grounded and
faithful is challenging due to the linguistic diversity and the myriad of
possible answers. In this paper, we introduce a novel approach to evaluate
faithfulness of machine-generated text by computing the longest noncontinuous
substring of the claim that is supported by the context, which we refer to as
the Longest Supported Subsequence (LSS). Using a new human-annotated dataset,
we finetune a model to generate LSS. We introduce a new method of evaluation
and demonstrate that these metrics correlate better with human ratings when LSS
is employed, as opposed to when it is not. Our proposed metric demonstrates an
18% enhancement over the prevailing state-of-the-art metric for faithfulness on
our dataset. Our metric consistently outperforms other metrics on a
summarization dataset across six different models. Finally, we compare several
popular Large Language Models (LLMs) for faithfulness using this metric. We
release the human-annotated dataset built for predicting LSS and our fine-tuned
model for evaluating faithfulness.
</p></li>
</ul>

<h3>Title: Prompt2Model: Generating Deployable Models from Natural Language Instructions. (arXiv:2308.12261v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12261">http://arxiv.org/abs/2308.12261</a></li>
<li>Code URL: https://github.com/neulab/prompt2model</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12261]] Prompt2Model: Generating Deployable Models from Natural Language Instructions(http://arxiv.org/abs/2308.12261)</code></li>
<li>Summary: <p>Large language models (LLMs) enable system builders today to create competent
NLP systems through prompting, where they only need to describe the task in
natural language and provide a few examples. However, in other ways, LLMs are a
step backward from traditional special-purpose NLP models; they require
extensive computational resources for deployment and can be gated behind APIs.
In this paper, we propose Prompt2Model, a general-purpose method that takes a
natural language task description like the prompts provided to LLMs, and uses
it to train a special-purpose model that is conducive to deployment. This is
done through a multi-step process of retrieval of existing datasets and
pretrained models, dataset generation using LLMs, and supervised fine-tuning on
these retrieved and generated datasets. Over three tasks, we demonstrate that
given the same few-shot prompt as input, Prompt2Model trains models that
outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20%
while being up to 700 times smaller. We also show that this data can be used to
obtain reliable performance estimates of model performance, enabling model
developers to assess model reliability before deployment. Prompt2Model is
available open-source at https://github.com/neulab/prompt2model.
</p></li>
</ul>

<h3>Title: D4: Improving LLM Pretraining via Document De-Duplication and Diversification. (arXiv:2308.12284v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12284">http://arxiv.org/abs/2308.12284</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12284]] D4: Improving LLM Pretraining via Document De-Duplication and Diversification(http://arxiv.org/abs/2308.12284)</code></li>
<li>Summary: <p>Over recent years, an increasing amount of compute and data has been poured
into training large language models (LLMs), usually by doing one-pass learning
on as many tokens as possible randomly selected from large-scale web corpora.
While training on ever-larger portions of the internet leads to consistent
performance improvements, the size of these improvements diminishes with scale,
and there has been little work exploring the effect of data selection on
pre-training and downstream performance beyond simple de-duplication methods
such as MinHash. Here, we show that careful data selection (on top of
de-duplicated data) via pre-trained model embeddings can speed up training (20%
efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up
to 2%) at the 6.7B model scale. Furthermore, we show that repeating data
intelligently consistently outperforms baseline training (while repeating
random data performs worse than baseline training). Our results indicate that
clever data selection can significantly improve LLM pre-training, calls into
question the common practice of training for a single epoch on as much data as
possible, and demonstrates a path to keep improving our models past the limits
of randomly sampling web data.
</p></li>
</ul>

<h3>Title: Devising and Detecting Phishing: large language models vs. Smaller Human Models. (arXiv:2308.12287v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12287">http://arxiv.org/abs/2308.12287</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12287]] Devising and Detecting Phishing: large language models vs(http://arxiv.org/abs/2308.12287)</code></li>
<li>Summary: <p>AI programs, built using large language models, make it possible to
automatically create phishing emails based on a few data points about a user.
They stand in contrast to traditional phishing emails that hackers manually
design using general rules gleaned from experience. The V-Triad is an advanced
set of rules for manually designing phishing emails to exploit our cognitive
heuristics and biases. In this study, we compare the performance of phishing
emails created automatically by GPT-4 and manually using the V-Triad. We also
combine GPT-4 with the V-Triad to assess their combined potential. A fourth
group, exposed to generic phishing emails, was our control group. We utilized a
factorial approach, sending emails to 112 randomly selected participants
recruited for the study. The control group emails received a click-through rate
between 19-28%, the GPT-generated emails 30-44%, emails generated by the
V-Triad 69-79%, and emails generated by GPT and the V-Triad 43-81%. Each
participant was asked to explain for why they pressed or did not press a link
in the email. These answers often contradict each other, highlighting the need
for personalized content. The cues that make one person avoid phishing emails
make another person fall for them. Next, we used four popular large language
models (GPT, Claude, PaLM, and LLaMA) to detect the intention of phishing
emails and compare the results to human detection. The language models
demonstrated a strong ability to detect malicious intent, even in non-obvious
phishing emails. They sometimes surpassed human detection, although often being
slightly less accurate than humans.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: SAMSNeRF: Segment Anything Model (SAM) Guides Dynamic Surgical Scene Reconstruction by Neural Radiance Field (NeRF). (arXiv:2308.11774v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11774">http://arxiv.org/abs/2308.11774</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11774]] SAMSNeRF: Segment Anything Model (SAM) Guides Dynamic Surgical Scene Reconstruction by Neural Radiance Field (NeRF)(http://arxiv.org/abs/2308.11774)</code></li>
<li>Summary: <p>The accurate reconstruction of surgical scenes from surgical videos is
critical for various applications, including intraoperative navigation and
image-guided robotic surgery automation. However, previous approaches, mainly
relying on depth estimation, have limited effectiveness in reconstructing
surgical scenes with moving surgical tools. To address this limitation and
provide accurate 3D position prediction for surgical tools in all frames, we
propose a novel approach called SAMSNeRF that combines Segment Anything Model
(SAM) and Neural Radiance Field (NeRF) techniques. Our approach generates
accurate segmentation masks of surgical tools using SAM, which guides the
refinement of the dynamic surgical scene reconstruction by NeRF. Our
experimental results on public endoscopy surgical videos demonstrate that our
approach successfully reconstructs high-fidelity dynamic surgical scenes and
accurately reflects the spatial information of surgical tools. Our proposed
approach can significantly enhance surgical navigation and automation by
providing surgeons with accurate 3D position information of surgical tools
during surgery.The source code will be released soon.
</p></li>
</ul>

<h3>Title: Time Does Tell: Self-Supervised Time-Tuning of Dense Image Representations. (arXiv:2308.11796v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11796">http://arxiv.org/abs/2308.11796</a></li>
<li>Code URL: https://github.com/smsd75/timetuning</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11796]] Time Does Tell: Self-Supervised Time-Tuning of Dense Image Representations(http://arxiv.org/abs/2308.11796)</code></li>
<li>Summary: <p>Spatially dense self-supervised learning is a rapidly growing problem domain
with promising applications for unsupervised segmentation and pretraining for
dense downstream tasks. Despite the abundance of temporal data in the form of
videos, this information-rich source has been largely overlooked. Our paper
aims to address this gap by proposing a novel approach that incorporates
temporal consistency in dense self-supervised learning. While methods designed
solely for images face difficulties in achieving even the same performance on
videos, our method improves not only the representation quality for videos-but
also images. Our approach, which we call time-tuning, starts from
image-pretrained models and fine-tunes them with a novel self-supervised
temporal-alignment clustering loss on unlabeled videos. This effectively
facilitates the transfer of high-level information from videos to image
representations. Time-tuning improves the state-of-the-art by 8-10% for
unsupervised semantic segmentation on videos and matches it for images. We
believe this method paves the way for further self-supervised scaling by
leveraging the abundant availability of videos. The implementation can be found
here : https://github.com/SMSD75/Timetuning
</p></li>
</ul>

<h3>Title: Exploring the Optimization Objective of One-Class Classification for Anomaly Detection. (arXiv:2308.11898v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11898">http://arxiv.org/abs/2308.11898</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11898]] Exploring the Optimization Objective of One-Class Classification for Anomaly Detection(http://arxiv.org/abs/2308.11898)</code></li>
<li>Summary: <p>One-class classification (OCC) is a longstanding method for anomaly
detection. With the powerful representation capability of the pre-trained
backbone, OCC methods have witnessed significant performance improvements.
Typically, most of these OCC methods employ transfer learning to enhance the
discriminative nature of the pre-trained backbone's features, thus achieving
remarkable efficacy. While most current approaches emphasize feature transfer
strategies, we argue that the optimization objective space within OCC methods
could also be an underlying critical factor influencing performance. In this
work, we conducted a thorough investigation into the optimization objective of
OCC. Through rigorous theoretical analysis and derivation, we unveil a key
insights: any space with the suitable norm can serve as an equivalent
substitute for the hypersphere center, without relying on the distribution
assumption of training samples. Further, we provide guidelines for determining
the feasible domain of norms for the OCC optimization objective. This novel
insight sparks a simple and data-agnostic deep one-class classification method.
Our method is straightforward, with a single 1x1 convolutional layer as a
trainable projector and any space with suitable norm as the optimization
objective. Extensive experiments validate the reliability and efficacy of our
findings and the corresponding methodology, resulting in state-of-the-art
performance in both one-class classification and industrial vision anomaly
detection and segmentation tasks.
</p></li>
</ul>

<h3>Title: Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation. (arXiv:2308.11903v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11903">http://arxiv.org/abs/2308.11903</a></li>
<li>Code URL: https://github.com/zhenzhao/dpms</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11903]] Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation(http://arxiv.org/abs/2308.11903)</code></li>
<li>Summary: <p>Studies on semi-supervised medical image segmentation (SSMIS) have seen fast
progress recently. Due to the limited labelled data, SSMIS methods mainly focus
on effectively leveraging unlabeled data to enhance the segmentation
performance. However, despite their promising performance, current
state-of-the-art methods often prioritize integrating complex techniques and
loss terms rather than addressing the core challenges of semi-supervised
scenarios directly. We argue that the key to SSMIS lies in generating
substantial and appropriate prediction disagreement on unlabeled data. To this
end, we emphasize the crutiality of data perturbation and model stabilization
in semi-supervised segmentation, and propose a simple yet effective approach to
boost SSMIS performance significantly, dubbed DPMS. Specifically, we first
revisit SSMIS from three distinct perspectives: the data, the model, and the
loss, and conduct a comprehensive study of corresponding strategies to examine
their effectiveness. Based on these examinations, we then propose DPMS, which
adopts a plain teacher-student framework with a standard supervised loss and
unsupervised consistency loss. To produce appropriate prediction disagreements,
DPMS perturbs the unlabeled data via strong augmentations to enlarge prediction
disagreements considerably. On the other hand, using EMA teacher when strong
augmentation is applied does not necessarily improve performance. DPMS further
utilizes a forwarding-twice and momentum updating strategies for normalization
statistics to stabilize the training on unlabeled data effectively. Despite its
simplicity, DPMS can obtain new state-of-the-art performance on the public 2D
ACDC and 3D LA datasets across various semi-supervised settings, e.g. obtaining
a remarkable 22.62% improvement against previous SOTA on ACDC with 5% labels.
</p></li>
</ul>

<h3>Title: ACLS: Adaptive and Conditional Label Smoothing for Network Calibration. (arXiv:2308.11911v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11911">http://arxiv.org/abs/2308.11911</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11911]] ACLS: Adaptive and Conditional Label Smoothing for Network Calibration(http://arxiv.org/abs/2308.11911)</code></li>
<li>Summary: <p>We address the problem of network calibration adjusting miscalibrated
confidences of deep neural networks. Many approaches to network calibration
adopt a regularization-based method that exploits a regularization term to
smooth the miscalibrated confidences. Although these approaches have shown the
effectiveness on calibrating the networks, there is still a lack of
understanding on the underlying principles of regularization in terms of
network calibration. We present in this paper an in-depth analysis of existing
regularization-based methods, providing a better understanding on how they
affect to network calibration. Specifically, we have observed that 1) the
regularization-based methods can be interpreted as variants of label smoothing,
and 2) they do not always behave desirably. Based on the analysis, we introduce
a novel loss function, dubbed ACLS, that unifies the merits of existing
regularization methods, while avoiding the limitations. We show extensive
experimental results for image classification and semantic segmentation on
standard benchmarks, including CIFAR10, Tiny-ImageNet, ImageNet, and PASCAL
VOC, demonstrating the effectiveness of our loss function.
</p></li>
</ul>

<h3>Title: Advancements in Point Cloud Data Augmentation for Deep Learning: A Survey. (arXiv:2308.12113v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12113">http://arxiv.org/abs/2308.12113</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12113]] Advancements in Point Cloud Data Augmentation for Deep Learning: A Survey(http://arxiv.org/abs/2308.12113)</code></li>
<li>Summary: <p>Point cloud has a wide range of applications in areas such as autonomous
driving, mapping, navigation, scene reconstruction, and medical imaging. Due to
its great potentials in these applications, point cloud processing has gained
great attention in the field of computer vision. Among various point cloud
processing techniques, deep learning (DL) has become one of the mainstream and
effective methods for tasks such as detection, segmentation and classification.
To reduce overfitting during training DL models and improve model performance
especially when the amount and/or diversity of training data are limited,
augmentation is often crucial. Although various point cloud data augmentation
methods have been widely used in different point cloud processing tasks, there
are currently no published systematic surveys or reviews of these methods.
Therefore, this article surveys and discusses these methods and categorizes
them into a taxonomy framework. Through the comprehensive evaluation and
comparison of the augmentation methods, this article identifies their
potentials and limitations and suggests possible future research directions.
This work helps researchers gain a holistic understanding of the current status
of point cloud data augmentation and promotes its wider application and
development.
</p></li>
</ul>

<h3>Title: The TYC Dataset for Understanding Instance-Level Semantics and Motions of Cells in Microstructures. (arXiv:2308.12116v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12116">http://arxiv.org/abs/2308.12116</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12116]] The TYC Dataset for Understanding Instance-Level Semantics and Motions of Cells in Microstructures(http://arxiv.org/abs/2308.12116)</code></li>
<li>Summary: <p>Segmenting cells and tracking their motion over time is a common task in
biomedical applications. However, predicting accurate instance-wise
segmentation and cell motions from microscopy imagery remains a challenging
task. Using microstructured environments for analyzing single cells in a
constant flow of media adds additional complexity. While large-scale labeled
microscopy datasets are available, we are not aware of any large-scale dataset,
including both cells and microstructures. In this paper, we introduce the
trapped yeast cell (TYC) dataset, a novel dataset for understanding
instance-level semantics and motions of cells in microstructures. We release
$105$ dense annotated high-resolution brightfield microscopy images, including
about $19$k instance masks. We also release $261$ curated video clips composed
of $1293$ high-resolution microscopy images to facilitate unsupervised
understanding of cell motions and morphology. TYC offers ten times more
instance annotations than the previously largest dataset, including cells and
microstructures. Our effort also exceeds previous attempts in terms of
microstructure variability, resolution, complexity, and capturing device
(microscopy) variability. We facilitate a unified comparison on our novel
dataset by introducing a standardized evaluation strategy. TYC and evaluation
code are publicly available under CC BY 4.0 license.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
