<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-17</h1>
<h3>Title: PowerGrow: Feasible Co-Growth of Structures and Dynamics for Power Grid Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Xinyu He, Chenhan Xiao, Haoran Li, Ruizhong Qiu, Zhe Xu, Yang Weng, Jingrui He, Hanghang Tong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12212">https://arxiv.org/abs/2509.12212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12212">https://arxiv.org/pdf/2509.12212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12212]] PowerGrow: Feasible Co-Growth of Structures and Dynamics for Power Grid Synthesis(https://arxiv.org/abs/2509.12212)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Modern power systems are becoming increasingly dynamic, with changing topologies and time-varying loads driven by renewable energy variability, electric vehicle adoption, and active grid reconfiguration. Despite these changes, publicly available test cases remain scarce, due to security concerns and the significant effort required to anonymize real systems. Such limitations call for generative tools that can jointly synthesize grid structure and nodal dynamics. However, modeling the joint distribution of network topology, branch attributes, bus properties, and dynamic load profiles remains a major challenge, while preserving physical feasibility and avoiding prohibitive computational costs. We present PowerGrow, a co-generative framework that significantly reduces computational overhead while maintaining operational validity. The core idea is dependence decomposition: the complex joint distribution is factorized into a chain of conditional distributions over feasible grid topologies, time-series bus loads, and other system attributes, leveraging their mutual dependencies. By constraining the generation process at each stage, we implement a hierarchical graph beta-diffusion process for structural synthesis, paired with a temporal autoencoder that embeds time-series data into a compact latent space, improving both training stability and sample fidelity. Experiments across benchmark settings show that PowerGrow not only outperforms prior diffusion models in fidelity and diversity but also achieves a 98.9\% power flow convergence rate and improved N-1 contingency resilience. This demonstrates its ability to generate operationally valid and realistic power grid scenarios.</li>
</ul>

<h3>Title: MEUV: Achieving Fine-Grained Capability Activation in Large Language Models via Mutually Exclusive Unlock Vectors</h3>
<ul>
<li><strong>Authors: </strong>Xin Tong, Zhi Lin, Jingya Wang, Meng Han, Bo Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12221">https://arxiv.org/abs/2509.12221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12221">https://arxiv.org/pdf/2509.12221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12221]] MEUV: Achieving Fine-Grained Capability Activation in Large Language Models via Mutually Exclusive Unlock Vectors(https://arxiv.org/abs/2509.12221)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) enforce safety alignment to reliably refuse malicious requests, yet the same blanket safeguards also block legitimate uses in policing, defense, and other high-stakes settings. Earlier "refusal-direction" edits can bypass those layers, but they rely on a single vector that indiscriminately unlocks all hazardous topics, offering no semantic control. We introduce Mutually Exclusive Unlock Vectors (MEUV), a lightweight framework that factorizes the monolithic refusal direction into topic-aligned, nearly orthogonal vectors, each dedicated to one sensitive capability. MEUV is learned in a single epoch with a multi-task objective that blends a differential-ablation margin, cross-topic and orthogonality penalties, and several auxiliary terms. On bilingual malicious-prompt benchmarks, MEUV achieves an attack success rate of no less than 87% on Gemma-2-2B, LLaMA-3-8B, and Qwen-7B, yet cuts cross-topic leakage by up to 90% compared with the best single-direction baseline. Vectors trained in Chinese transfer almost unchanged to English (and vice versa), suggesting a language-agnostic refusal subspace. The results show that fine-grained, topic-level capability activation is achievable with minimal utility loss, paving the way for controlled LLMs deployment in security-sensitive domains.</li>
</ul>

<h3>Title: Accelerating Privacy-Preserving Federated Learning in Large-Scale LEO Satellite Systems</h3>
<ul>
<li><strong>Authors: </strong>Binquan Guo, Junteng Cao, Marie Siew, Binbin Chen, Tony Q. S. Quek, Zhu Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12222">https://arxiv.org/abs/2509.12222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12222">https://arxiv.org/pdf/2509.12222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12222]] Accelerating Privacy-Preserving Federated Learning in Large-Scale LEO Satellite Systems(https://arxiv.org/abs/2509.12222)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Large-scale low-Earth-orbit (LEO) satellite systems are increasingly valued for their ability to enable rapid and wide-area data exchange, thereby facilitating the collaborative training of artificial intelligence (AI) models across geographically distributed regions. Due to privacy concerns and regulatory constraints, raw data collected at remote clients cannot be centrally aggregated, posing a major obstacle to traditional AI training methods. Federated learning offers a privacy-preserving alternative by training local models on distributed devices and exchanging only model parameters. However, the dynamic topology and limited bandwidth of satellite systems will hinder timely parameter aggregation and distribution, resulting in prolonged training times. To address this challenge, we investigate the problem of scheduling federated learning over satellite networks and identify key bottlenecks that impact the overall duration of each training round. We propose a discrete temporal graph-based on-demand scheduling framework that dynamically allocates communication resources to accelerate federated learning. Simulation results demonstrate that the proposed approach achieves significant performance gains over traditional statistical multiplexing-based model exchange strategies, reducing overall round times by 14.20% to 41.48%. Moreover, the acceleration effect becomes more pronounced for larger models and higher numbers of clients, highlighting the scalability of the proposed approach.</li>
</ul>

<h3>Title: TripOptimizer: Generative 3D Shape Optimization and Drag Prediction using Triplane VAE Networks</h3>
<ul>
<li><strong>Authors: </strong>Parsa Vatani, Mohamed Elrefaie, Farhad Nazarpour, Faez Ahmed</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12224">https://arxiv.org/abs/2509.12224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12224">https://arxiv.org/pdf/2509.12224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12224]] TripOptimizer: Generative 3D Shape Optimization and Drag Prediction using Triplane VAE Networks(https://arxiv.org/abs/2509.12224)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>The computational cost of traditional Computational Fluid Dynamics-based Aerodynamic Shape Optimization severely restricts design space exploration. This paper introduces TripOptimizer, a fully differentiable deep learning framework for rapid aerodynamic analysis and shape optimization directly from vehicle point cloud data. TripOptimizer employs a Variational Autoencoder featuring a triplane-based implicit neural representation for high-fidelity 3D geometry reconstruction and a drag coefficient prediction head. Trained on DrivAerNet++, a large-scale dataset of 8,000 unique vehicle geometries with corresponding drag coefficients computed via Reynolds-Averaged Navier-Stokes simulations, the model learns a latent representation that encodes aerodynamically salient geometric features. We propose an optimization strategy that modifies a subset of the encoder parameters to steer an initial geometry towards a target drag value, and demonstrate its efficacy in case studies where optimized designs achieved drag coefficient reductions up to 11.8\%. These results were subsequently validated by using independent, high-fidelity Computational Fluid Dynamics simulations with more than 150 million cells. A key advantage of the implicit representation is its inherent robustness to geometric imperfections, enabling optimization of non-watertight meshes, a significant challenge for traditional adjoint-based methods. The framework enables a more agile Aerodynamic Shape Optimization workflow, reducing reliance on computationally intensive CFD simulations, especially during early design stages.</li>
</ul>

<h3>Title: A Physics-Informed Neural Networks-Based Model Predictive Control Framework for $SIR$ Epidemics</h3>
<ul>
<li><strong>Authors: </strong>Aiping Zhong, Baike She, Philip E. Par√©</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, q-bio.PE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12226">https://arxiv.org/abs/2509.12226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12226">https://arxiv.org/pdf/2509.12226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12226]] A Physics-Informed Neural Networks-Based Model Predictive Control Framework for $SIR$ Epidemics(https://arxiv.org/abs/2509.12226)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This work introduces a physics-informed neural networks (PINNs)-based model predictive control (MPC) framework for susceptible-infected-recovered ($SIR$) spreading models. Existing studies in MPC design for epidemic control often assume either 1) measurable states of the dynamics, where the parameters are learned, or 2) known parameters of the model, where the states are learned. In this work, we address the joint real-time estimation of states and parameters within the MPC framework using only noisy infected states, under the assumption that 1) only the recovery rate is known, or 2) only the basic reproduction number is known. Under the first assumption, we propose MPC-PINNs and two novel PINNs algorithms, all of which are integrated into the MPC framework. First, we introduce MPC-PINNs, which are designed for $SIR$ models with control. We then propose log-scaled PINNs (MPC-LS-PINNs), which incorporate a log-scaled loss function to improve robustness against noise. Next, we present split-integral PINNs (MPC-SI-PINNs), which leverage integral operators and state coupling in the neural network training process to effectively reconstruct the complete epidemic state information. Building upon these methods, we further extend our framework for the second assumption. We establish the necessary conditions and extend our PINNs algorithms, where MPC-SI-PINNs are simplified as split-PINNs (MPC-S-PINNs). By incorporating these algorithms into the MPC framework, we simultaneously estimate the epidemic states and parameters while generating optimal control strategies. Experiment results demonstrate the effectiveness of the proposed methods under different settings.</li>
</ul>

<h3>Title: Profiling LoRA/QLoRA Fine-Tuning Efficiency on Consumer GPUs: An RTX 4060 Case Study</h3>
<ul>
<li><strong>Authors: </strong>MSR Avinash</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12229">https://arxiv.org/abs/2509.12229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12229">https://arxiv.org/pdf/2509.12229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12229]] Profiling LoRA/QLoRA Fine-Tuning Efficiency on Consumer GPUs: An RTX 4060 Case Study(https://arxiv.org/abs/2509.12229)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) with parameter-efficient techniques such as LoRA and QLoRA has enabled adaptation of foundation models on modest hardware. Yet the efficiency of such training on consumer-grade GPUs, especially under strict 8 GB VRAM limits, remains underexplored. We present a controlled profiling study of LoRA/QLoRA fine-tuning using the Qwen2.5-1.5B-Instruct model on a single NVIDIA RTX 4060. Across three representative configurations, we systematically vary batch size, sequence length, optimizer choice (AdamW vs. PagedAdamW), and precision (fp16 vs. bf16). We report throughput (tokens/s), time per 10k tokens, and VRAM footprint, alongside energy estimates derived from GPU board power limits. Our results show that paged optimizers improve throughput by up to 25% (628 tok/s vs. 500 tok/s baseline), while bf16 degrades efficiency relative to fp16. Despite 8 GB constraints, sequence lengths up to 2048 tokens were feasible using parameter-efficient strategies. To our knowledge, this is the first systematic case study of LLM fine- tuning efficiency on consumer GPUs, providing reproducible benchmarks and practical guidelines for resource-constrained researchers and practitioners.</li>
</ul>

<h3>Title: Towards Trustworthy Agentic IoEV: AI Agents for Explainable Cyberthreat Mitigation and State Analytics</h3>
<ul>
<li><strong>Authors: </strong>Meryem Malak Dif, Mouhamed Amine Bouchiha, Abdelaziz Amara Korba, Yacine Ghamri-Doudane</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.ET, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12233">https://arxiv.org/abs/2509.12233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12233">https://arxiv.org/pdf/2509.12233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12233]] Towards Trustworthy Agentic IoEV: AI Agents for Explainable Cyberthreat Mitigation and State Analytics(https://arxiv.org/abs/2509.12233)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>The Internet of Electric Vehicles (IoEV) envisions a tightly coupled ecosystem of electric vehicles (EVs), charging infrastructure, and grid services, yet it remains vulnerable to cyberattacks, unreliable battery-state predictions, and opaque decision processes that erode trust and performance. To address these challenges, we introduce a novel Agentic Artificial Intelligence (AAI) framework tailored for IoEV, where specialized agents collaborate to deliver autonomous threat mitigation, robust analytics, and interpretable decision support. Specifically, we design an AAI architecture comprising dedicated agents for cyber-threat detection and response at charging stations, real-time State of Charge (SoC) estimation, and State of Health (SoH) anomaly detection, all coordinated through a shared, explainable reasoning layer; develop interpretable threat-mitigation mechanisms that proactively identify and neutralize attacks on both physical charging points and learning components; propose resilient SoC and SoH models that leverage continuous and adversarial-aware learning to produce accurate, uncertainty-aware forecasts with human-readable explanations; and implement a three-agent pipeline, where each agent uses LLM-driven reasoning and dynamic tool invocation to interpret intent, contextualize tasks, and execute formal optimizations for user-centric assistance. Finally, we validate our framework through comprehensive experiments across diverse IoEV scenarios, demonstrating significant improvements in security and prediction accuracy. All datasets, models, and code will be released publicly.</li>
</ul>

<h3>Title: Flexible Multimodal Neuroimaging Fusion for Alzheimer's Disease Progression Prediction</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Burns, Yuan Xue, Douglas W. Scharre, Xia Ning</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12234">https://arxiv.org/abs/2509.12234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12234">https://arxiv.org/pdf/2509.12234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12234]] Flexible Multimodal Neuroimaging Fusion for Alzheimer's Disease Progression Prediction(https://arxiv.org/abs/2509.12234)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's disease (AD) is a progressive neurodegenerative disease with high inter-patient variance in rate of cognitive decline. AD progression prediction aims to forecast patient cognitive decline and benefits from incorporating multiple neuroimaging modalities. However, existing multimodal models fail to make accurate predictions when many modalities are missing during inference, as is often the case in clinical settings. To increase multimodal model flexibility under high modality missingness, we introduce PerM-MoE, a novel sparse mixture-of-experts method that uses independent routers for each modality in place of the conventional, single router. Using T1-weighted MRI, FLAIR, amyloid beta PET, and tau PET neuroimaging data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), we evaluate PerM-MoE, state-of-the-art Flex-MoE, and unimodal neuroimaging models on predicting two-year change in Clinical Dementia Rating-Sum of Boxes (CDR-SB) scores under varying levels of modality missingness. PerM-MoE outperforms the state of the art in most variations of modality missingness and demonstrates more effective utility of experts than Flex-MoE.</li>
</ul>

<h3>Title: RL Fine-Tuning Heals OOD Forgetting in SFT</h3>
<ul>
<li><strong>Authors: </strong>Hangzhan Jin, Sitao Luan, Sicheng Lyu, Guillaume Rabusseau, Reihaneh Rabbany, Doina Precup, Mohammad Hamdaqa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12235">https://arxiv.org/abs/2509.12235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12235">https://arxiv.org/pdf/2509.12235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12235]] RL Fine-Tuning Heals OOD Forgetting in SFT(https://arxiv.org/abs/2509.12235)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The two-stage fine-tuning paradigm of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has empirically shown better reasoning performance than one-stage SFT for the post-training of Large Language Models (LLMs). However, the evolution and mechanism behind the synergy of SFT and RL are still under-explored and inconclusive. In our study, we find the well-known claim "SFT memorizes, RL generalizes" is over-simplified, and discover that: (1) OOD performance peaks at the early stage of SFT and then declines (OOD forgetting), the best SFT checkpoint cannot be captured by training/test loss; (2) the subsequent RL stage does not generate fundamentally better OOD capability, instead it plays an \textbf{OOD restoration} role, recovering the lost reasoning ability during SFT; (3) The recovery ability has boundaries, \ie{} \textbf{if SFT trains for too short or too long, RL cannot recover the lost OOD ability;} (4) To uncover the underlying mechanisms behind the forgetting and restoration process, we employ SVD analysis on parameter matrices, manually edit them, and observe their impacts on model performance. Unlike the common belief that the shift of model capacity mainly results from the changes of singular values, we find that they are actually quite stable throughout fine-tuning. Instead, the OOD behavior strongly correlates with the \textbf{rotation of singular vectors}. Our findings re-identify the roles of SFT and RL in the two-stage fine-tuning and discover the rotation of singular vectors as the key mechanism. %reversing the rotations induced by SFT, which shows recovery from forgetting, whereas imposing the SFT parameter directions onto a RL-tuned model results in performance degradation. Code is available at this https URL</li>
</ul>

<h3>Title: InJecteD: Analyzing Trajectories and Drift Dynamics in Denoising Diffusion Probabilistic Models for 2D Point Cloud Generation</h3>
<ul>
<li><strong>Authors: </strong>Sanyam Jain, Khuram Naveed, Illia Oleksiienko, Alexandros Iosifidis, Ruben Pauwels</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12239">https://arxiv.org/abs/2509.12239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12239">https://arxiv.org/pdf/2509.12239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12239]] InJecteD: Analyzing Trajectories and Drift Dynamics in Denoising Diffusion Probabilistic Models for 2D Point Cloud Generation(https://arxiv.org/abs/2509.12239)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This work introduces InJecteD, a framework for interpreting Denoising Diffusion Probabilistic Models (DDPMs) by analyzing sample trajectories during the denoising process of 2D point cloud generation. We apply this framework to three datasets from the Datasaurus Dozen bullseye, dino, and circle using a simplified DDPM architecture with customizable input and time embeddings. Our approach quantifies trajectory properties, including displacement, velocity, clustering, and drift field dynamics, using statistical metrics such as Wasserstein distance and cosine similarity. By enhancing model transparency, InJecteD supports human AI collaboration by enabling practitioners to debug and refine generative models. Experiments reveal distinct denoising phases: initial noise exploration, rapid shape formation, and final refinement, with dataset-specific behaviors example, bullseyes concentric convergence vs. dinos complex contour formation. We evaluate four model configurations, varying embeddings and noise schedules, demonstrating that Fourier based embeddings improve trajectory stability and reconstruction quality</li>
</ul>

<h3>Title: Artificial Intelligence in Breast Cancer Care: Transforming Preoperative Planning and Patient Education with 3D Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Mustafa Khanbhai, Giulia Di Nardo, Jun Ma, Vivienne Freitas, Caterina Masino, Ali Dolatabadi, Zhaoxun "Lorenz" Liu, Wey Leong, Wagner H. Souza, Amin Madani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12242">https://arxiv.org/abs/2509.12242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12242">https://arxiv.org/pdf/2509.12242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12242]] Artificial Intelligence in Breast Cancer Care: Transforming Preoperative Planning and Patient Education with 3D Reconstruction(https://arxiv.org/abs/2509.12242)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Effective preoperative planning requires accurate algorithms for segmenting anatomical structures across diverse datasets, but traditional models struggle with generalization. This study presents a novel machine learning methodology to improve algorithm generalization for 3D anatomical reconstruction beyond breast cancer applications. We processed 120 retrospective breast MRIs (January 2018-June 2023) through three phases: anonymization and manual segmentation of T1-weighted and dynamic contrast-enhanced sequences; co-registration and segmentation of whole breast, fibroglandular tissue, and tumors; and 3D visualization using ITK-SNAP. A human-in-the-loop approach refined segmentations using U-Mamba, designed to generalize across imaging scenarios. Dice similarity coefficient assessed overlap between automated segmentation and ground truth. Clinical relevance was evaluated through clinician and patient interviews. U-Mamba showed strong performance with DSC values of 0.97 ($\pm$0.013) for whole organs, 0.96 ($\pm$0.024) for fibroglandular tissue, and 0.82 ($\pm$0.12) for tumors on T1-weighted images. The model generated accurate 3D reconstructions enabling visualization of complex anatomical features. Clinician interviews indicated improved planning, intraoperative navigation, and decision support. Integration of 3D visualization enhanced patient education, communication, and understanding. This human-in-the-loop machine learning approach successfully generalizes algorithms for 3D reconstruction and anatomical segmentation across patient datasets, offering enhanced visualization for clinicians, improved preoperative planning, and more effective patient education, facilitating shared decision-making and empowering informed patient choices across medical applications.</li>
</ul>

<h3>Title: RU-Net for Automatic Characterization of TRISO Fuel Cross Sections</h3>
<ul>
<li><strong>Authors: </strong>Lu Cai, Fei Xu, Min Xian, Yalei Tang, Shoukun Sun, John Stempien</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12244">https://arxiv.org/abs/2509.12244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12244">https://arxiv.org/pdf/2509.12244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12244]] RU-Net for Automatic Characterization of TRISO Fuel Cross Sections(https://arxiv.org/abs/2509.12244)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>During irradiation, phenomena such as kernel swelling and buffer densification may impact the performance of tristructural isotropic (TRISO) particle fuel. Post-irradiation microscopy is often used to identify these irradiation-induced morphologic changes. However, each fuel compact generally contains thousands of TRISO particles. Manually performing the work to get statistical information on these phenomena is cumbersome and subjective. To reduce the subjectivity inherent in that process and to accelerate data analysis, we used convolutional neural networks (CNNs) to automatically segment cross-sectional images of microscopic TRISO layers. CNNs are a class of machine-learning algorithms specifically designed for processing structured grid data. They have gained popularity in recent years due to their remarkable performance in various computer vision tasks, including image classification, object detection, and image segmentation. In this research, we generated a large irradiated TRISO layer dataset with more than 2,000 microscopic images of cross-sectional TRISO particles and the corresponding annotated images. Based on these annotated images, we used different CNNs to automatically segment different TRISO layers. These CNNs include RU-Net (developed in this study), as well as three existing architectures: U-Net, Residual Network (ResNet), and Attention U-Net. The preliminary results show that the model based on RU-Net performs best in terms of Intersection over Union (IoU). Using CNN models, we can expedite the analysis of TRISO particle cross sections, significantly reducing the manual labor involved and improving the objectivity of the segmentation results.</li>
</ul>

<h3>Title: Modular, On-Site Solutions with Lightweight Anomaly Detection for Sustainable Nutrient Management in Agriculture</h3>
<ul>
<li><strong>Authors: </strong>Abigail R. Cohen, Yuming Sun, Zhihao Qin, Harsh S. Muriki, Zihao Xiao, Yeonju Lee, Matthew Housley, Andrew F. Sharkey, Rhuanito S. Ferrarezi, Jing Li, Lu Gan, Yongsheng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12247">https://arxiv.org/abs/2509.12247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12247">https://arxiv.org/pdf/2509.12247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12247]] Modular, On-Site Solutions with Lightweight Anomaly Detection for Sustainable Nutrient Management in Agriculture(https://arxiv.org/abs/2509.12247)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Efficient nutrient management is critical for crop growth and sustainable resource consumption (e.g., nitrogen, energy). Current approaches require lengthy analyses, preventing real-time optimization; similarly, imaging facilitates rapid phenotyping but can be computationally intensive, preventing deployment under resource constraints. This study proposes a flexible, tiered pipeline for anomaly detection and status estimation (fresh weight, dry mass, and tissue nutrients), including a comprehensive energy analysis of approaches that span the efficiency-accuracy spectrum. Using a nutrient depletion experiment with three treatments (T1-100%, T2-50%, and T3-25% fertilizer strength) and multispectral imaging (MSI), we developed a hierarchical pipeline using an autoencoder (AE) for early warning. Further, we compared two status estimation modules of different complexity for more detailed analysis: vegetation index (VI) features with machine learning (Random Forest, RF) and raw whole-image deep learning (Vision Transformer, ViT). Results demonstrated high-efficiency anomaly detection (73% net detection of T3 samples 9 days after transplanting) at substantially lower energy than embodied energy in wasted nitrogen. The state estimation modules show trade-offs, with ViT outperforming RF on phosphorus and calcium estimation (R2 0.61 vs. 0.58, 0.48 vs. 0.35) at higher energy cost. With our modular pipeline, this work opens opportunities for edge diagnostics and practical opportunities for agricultural sustainability.</li>
</ul>

<h3>Title: Representation Learning on Large Non-Bipartite Transaction Networks using GraphSAGE</h3>
<ul>
<li><strong>Authors: </strong>Mihir Tare, Clemens Rattasits, Yiming Wu, Euan Wielewski</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12255">https://arxiv.org/abs/2509.12255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12255">https://arxiv.org/pdf/2509.12255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12255]] Representation Learning on Large Non-Bipartite Transaction Networks using GraphSAGE(https://arxiv.org/abs/2509.12255)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Financial institutions increasingly require scalable tools to analyse complex transactional networks, yet traditional graph embedding methods struggle with dynamic, real-world banking data. This paper demonstrates the practical application of GraphSAGE, an inductive Graph Neural Network framework, to non-bipartite heterogeneous transaction networks within a banking context. Unlike transductive approaches, GraphSAGE scales well to large networks and can generalise to unseen nodes which is critical for institutions working with temporally evolving transactional data. We construct a transaction network using anonymised customer and merchant transactions and train a GraphSAGE model to generate node embeddings. Our exploratory work on the embeddings reveals interpretable clusters aligned with geographic and demographic attributes. Additionally, we illustrate their utility in downstream classification tasks by applying them to a money mule detection model where using these embeddings improves the prioritisation of high-risk accounts. Beyond fraud detection, our work highlights the adaptability of this framework to banking-scale networks, emphasising its inductive capability, scalability, and interpretability. This study provides a blueprint for financial organisations to harness graph machine learning for actionable insights in transactional ecosystems.</li>
</ul>

<h3>Title: EfficientNet-Based Multi-Class Detection of Real, Deepfake, and Plastic Surgery Faces</h3>
<ul>
<li><strong>Authors: </strong>Li Kun, Milena Radenkovic</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12258">https://arxiv.org/abs/2509.12258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12258">https://arxiv.org/pdf/2509.12258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12258]] EfficientNet-Based Multi-Class Detection of Real, Deepfake, and Plastic Surgery Faces(https://arxiv.org/abs/2509.12258)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Currently, deep learning has been utilised to tackle several difficulties in our everyday lives. It not only exhibits progress in computer vision but also constitutes the foundation for several revolutionary technologies. Nonetheless, similar to all phenomena, the use of deep learning in diverse domains has produced a multifaceted interaction of advantages and disadvantages for human society. Deepfake technology has advanced, significantly impacting social life. However, developments in this technology can affect privacy, the reputations of prominent personalities, and national security via software development. It can produce indistinguishable counterfeit photographs and films, potentially impairing the functionality of facial recognition systems, so presenting a significant risk. The improper application of deepfake technology produces several detrimental effects on society. Face-swapping programs mislead users by altering persons' appearances or expressions to fulfil particular aims or to appropriate personal information. Deepfake technology permeates daily life through such techniques. Certain individuals endeavour to sabotage election campaigns or subvert prominent political figures by creating deceptive pictures to influence public perception, causing significant harm to a nation's political and economic structure.</li>
</ul>

<h3>Title: Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) for Diabetes Risk Prediction</h3>
<ul>
<li><strong>Authors: </strong>Kenneth G. Young II</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12259">https://arxiv.org/abs/2509.12259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12259">https://arxiv.org/pdf/2509.12259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12259]] Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) for Diabetes Risk Prediction(https://arxiv.org/abs/2509.12259)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>The Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) is an innovative machine learning framework that harnesses quantum-inspired techniques to predict diabetes risk with exceptional accuracy and efficiency. Utilizing the PIMA Indians Diabetes dataset augmented with 2,000 synthetic samples to mitigate class imbalance (total: 2,768 samples, 1,949 positives), QISICGM integrates a self-improving concept graph with a stacked ensemble comprising Random Forests (RF), Extra Trees (ET), transformers, convolutional neural networks (CNNs), and feed-forward neural networks (FFNNs). This approach achieves an out-of-fold (OOF) F1 score of 0.8933 and an AUC of 0.8699, outperforming traditional methods. Quantum inspired elements, such as phase feature mapping and neighborhood sequence modeling, enrich feature representations, enabling CPU-efficient inference at 8.5 rows per second. This paper presents a detailed architecture, theoretical foundations, code insights, and performance evaluations, including visualizations from the outputs subfolder. The open-source implementation (v1.0.0) is available at this https URL, positioning QISICGM as a potential benchmark for AI-assisted clinical triage in diabetes and beyond. Ultimately, this work emphasizes trustworthy AI through calibration, interpretability, and open-source reproducibility.</li>
</ul>

<h3>Title: A Modern Look at Simplicity Bias in Image Classification Tasks</h3>
<ul>
<li><strong>Authors: </strong>Xiaoguang Chang, Teng Wang, Changyin Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12265">https://arxiv.org/abs/2509.12265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12265">https://arxiv.org/pdf/2509.12265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12265]] A Modern Look at Simplicity Bias in Image Classification Tasks(https://arxiv.org/abs/2509.12265)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The simplicity Bias (SB) of neural networks, i.e.\ their tendency to represent simple functions, is a key factor in their generalization capabilities. Recent studies show that an excessive SB may harm performance on complex tasks, and the need for this bias varies across tasks. Many of these studies focus on simple models or synthetic tasks. It remains challenging to measure the SB in large models and little is known about the relevance of the SB to various image classification tasks. In this paper, we investigate the relationship between the SB in CLIP models and their performance across image classification tasks. First, we theoretically analyze the potential limitation of existing measures of complexity that have been used to characterize small models. To address this, we propose a frequency-aware measure capturing finer-grained SB differences. We validate this measure on CLIP models subjected to two recent SB-modulation methods, demonstrating that it is more informative and consistent than previous measures. Second, we examine the relation between the SB of those models and their performance across a range of image classification tasks, including zero-shot and fine-tuning settings. These experiments reveal a range of behaviors. For example, a stronger SB correlates with a better performance on OOD generalization than on adversarial robustness. These results highlight the benefits of aligning a model's inductive biases with the characteristics of the target task.</li>
</ul>

<h3>Title: Research on Short-Video Platform User Decision-Making via Multimodal Temporal Modeling and Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jinmeiyang Wang, Jing Dong, Li Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12269">https://arxiv.org/abs/2509.12269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12269">https://arxiv.org/pdf/2509.12269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12269]] Research on Short-Video Platform User Decision-Making via Multimodal Temporal Modeling and Reinforcement Learning(https://arxiv.org/abs/2509.12269)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper proposes the MT-DQN model, which integrates a Transformer, Temporal Graph Neural Network (TGNN), and Deep Q-Network (DQN) to address the challenges of predicting user behavior and optimizing recommendation strategies in short-video environments. Experiments demonstrated that MT-DQN consistently outperforms traditional concatenated models, such as Concat-Modal, achieving an average F1-score improvement of 10.97% and an average NDCG@5 improvement of 8.3%. Compared to the classic reinforcement learning model Vanilla-DQN, MT-DQN reduces MSE by 34.8% and MAE by 26.5%. Nonetheless, we also recognize challenges in deploying MT-DQN in real-world scenarios, such as its computational cost and latency sensitivity during online inference, which will be addressed through future architectural optimization.</li>
</ul>

<h3>Title: GraphDerm: Fusing Imaging, Physical Scale, and Metadata in a Population-Graph Classifier for Dermoscopic Lesions</h3>
<ul>
<li><strong>Authors: </strong>Mehdi Yousefzadeh, Parsa Esfahanian, Sara Rashidifar, Hossein Salahshoor Gavalan, Negar Sadat Rafiee Tabatabaee, Saeid Gorgin, Dara Rahmati, Maryam Daneshpazhooh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12277">https://arxiv.org/abs/2509.12277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12277">https://arxiv.org/pdf/2509.12277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12277]] GraphDerm: Fusing Imaging, Physical Scale, and Metadata in a Population-Graph Classifier for Dermoscopic Lesions(https://arxiv.org/abs/2509.12277)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Introduction. Dermoscopy aids melanoma triage, yet image-only AI often ignores patient metadata (age, sex, site) and the physical scale needed for geometric analysis. We present GraphDerm, a population-graph framework that fuses imaging, millimeter-scale calibration, and metadata for multiclass dermoscopic classification, to the best of our knowledge the first ISIC-scale application of GNNs to dermoscopy. Methods. We curate ISIC 2018/2019, synthesize ruler-embedded images with exact masks, and train U-Nets (SE-ResNet-18) for lesion and ruler segmentation. Pixels-per-millimeter are regressed from the ruler-mask two-point correlation via a lightweight 1D-CNN. From lesion masks we compute real-scale descriptors (area, perimeter, radius of gyration). Node features use EfficientNet-B3; edges encode metadata/geometry similarity (fully weighted or thresholded). A spectral GNN performs semi-supervised node classification; an image-only ANN is the baseline. Results. Ruler and lesion segmentation reach Dice 0.904 and 0.908; scale regression attains MAE 1.5 px (RMSE 6.6). The graph attains AUC 0.9812, with a thresholded variant using about 25% of edges preserving AUC 0.9788 (vs. 0.9440 for the image-only baseline); per-class AUCs typically fall in the 0.97-0.99 range. Conclusion. Unifying calibrated scale, lesion geometry, and metadata in a population graph yields substantial gains over image-only pipelines on ISIC-2019. Sparser graphs retain near-optimal accuracy, suggesting efficient deployment. Scale-aware, graph-based AI is a promising direction for dermoscopic decision support; future work will refine learned edge semantics and evaluate on broader curated benchmarks.</li>
</ul>

<h3>Title: PATIMT-Bench: A Multi-Scenario Benchmark for Position-Aware Text Image Machine Translation in Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wanru Zhuang, Wenbo Li, Zhibin Lan, Xu Han, Peng Li, Jinsong Su</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12278">https://arxiv.org/abs/2509.12278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12278">https://arxiv.org/pdf/2509.12278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12278]] PATIMT-Bench: A Multi-Scenario Benchmark for Position-Aware Text Image Machine Translation in Large Vision-Language Models(https://arxiv.org/abs/2509.12278)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Text Image Machine Translation (TIMT) aims to translate texts embedded within an image into another language. Current TIMT studies primarily focus on providing translations for all the text within an image, while neglecting to provide bounding boxes and covering limited scenarios. In this work, we extend traditional TIMT into position-aware TIMT (PATIMT), aiming to support fine-grained and layoutpreserving translation, which holds great practical value but remains largely unexplored. This task comprises two key sub-tasks: regionspecific translation and full-image translation with grounding. To support existing models on PATIMT and conduct fair evaluation, we construct the PATIMT benchmark (PATIMTBench), which consists of 10 diverse real-world scenarios. Specifically, we introduce an Adaptive Image OCR Refinement Pipeline, which adaptively selects appropriate OCR tools based on scenario and refines the results of text-rich images. To ensure evaluation reliability, we further construct a test set, which contains 1,200 high-quality instances manually annotated and reviewed by human experts. After fine-tuning on our data, compact Large Vision-Language Models (LVLMs) achieve state-of-the-art performance on both sub-tasks. Experimental results also highlight the scalability and generalizability of our training data</li>
</ul>

<h3>Title: Domain Adaptive SAR Wake Detection: Leveraging Similarity Filtering and Memory Guidance</h3>
<ul>
<li><strong>Authors: </strong>He Gao, Baoxiang Huang, Milena Radenkovic, Borui Li, Ge Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12279">https://arxiv.org/abs/2509.12279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12279">https://arxiv.org/pdf/2509.12279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12279]] Domain Adaptive SAR Wake Detection: Leveraging Similarity Filtering and Memory Guidance(https://arxiv.org/abs/2509.12279)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Synthetic Aperture Radar (SAR), with its all- weather and wide-area observation capabilities, serves as a crucial tool for wake detection. However, due to its complex imaging mechanism, wake features in SAR images often appear abstract and noisy, posing challenges for accurate annotation. In contrast, optical images provide more distinct visual cues, but models trained on optical data suffer from performance degradation when applied to SAR images due to domain shift. To address this cross-modal domain adaptation challenge, we propose a Similarity-Guided and Memory-Guided Domain Adap- tation (termed SimMemDA) framework for unsupervised domain adaptive ship wake detection via instance-level feature similarity filtering and feature memory guidance. Specifically, to alleviate the visual discrepancy between optical and SAR images, we first utilize WakeGAN to perform style transfer on optical images, generating pseudo-images close to the SAR style. Then, instance-level feature similarity filtering mechanism is designed to identify and prioritize source samples with target-like dis- tributions, minimizing negative transfer. Meanwhile, a Feature- Confidence Memory Bank combined with a K-nearest neighbor confidence-weighted fusion strategy is introduced to dynamically calibrate pseudo-labels in the target domain, improving the reliability and stability of pseudo-labels. Finally, the framework further enhances generalization through region-mixed training, strategically combining source annotations with calibrated tar- get pseudo-labels. Experimental results demonstrate that the proposed SimMemDA method can improve the accuracy and robustness of cross-modal ship wake detection tasks, validating the effectiveness and feasibility of the proposed method.</li>
</ul>

<h3>Title: Deriving the Scaled-Dot-Function via Maximum Likelihood Estimation and Maximum Entropy Approach</h3>
<ul>
<li><strong>Authors: </strong>Jiyong Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12285">https://arxiv.org/abs/2509.12285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12285">https://arxiv.org/pdf/2509.12285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12285]] Deriving the Scaled-Dot-Function via Maximum Likelihood Estimation and Maximum Entropy Approach(https://arxiv.org/abs/2509.12285)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we present a maximum likelihood estimation approach to determine the value vector in transformer models. We model the sequence of value vectors, key vectors, and the query vector as a sequence of Gaussian distributions. The variance in each Gaussian distribution depends on the time step, the corresponding key vector, and the query vector. The mean value in each Gaussian distribution depends on the time step, and the corresponding value vector. This analysis may offer a new explanation of the scaled-dot-product function or softmax function used in transformer architectures [1]. Another explanation, inspired by [4], is based on the maximum entropy approach in natural language processing [5]. In this approach, a query vector and key vectors are used to derive the feature functions for the maximum entropy model.</li>
</ul>

<h3>Title: Prediction of Stocks Index Price using Quantum GANs</h3>
<ul>
<li><strong>Authors: </strong>Sangram Deshpande, Gopal Ramesh Dahale, Sai Nandan Morapakula, Uday Wad</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12286">https://arxiv.org/abs/2509.12286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12286">https://arxiv.org/pdf/2509.12286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12286]] Prediction of Stocks Index Price using Quantum GANs(https://arxiv.org/abs/2509.12286)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper investigates the application of Quantum Generative Adversarial Networks (QGANs) for stock price prediction. Financial markets are inherently complex, marked by high volatility and intricate patterns that traditional models often fail to capture. QGANs, leveraging the power of quantum computing, offer a novel approach by combining the strengths of generative models with quantum machine learning techniques. We implement a QGAN model tailored for stock price prediction and evaluate its performance using historical stock market data. Our results demonstrate that QGANs can generate synthetic data closely resembling actual market behavior, leading to enhanced prediction accuracy. The experiment was conducted using the Stocks index price data and the AWS Braket SV1 simulator for training the QGAN circuits. The quantum-enhanced model outperforms classical Long Short-Term Memory (LSTM) and GAN models in terms of convergence speed and prediction accuracy. This research represents a key step toward integrating quantum computing in financial forecasting, offering potential advantages in speed and precision over traditional methods. The findings suggest important implications for traders, financial analysts, and researchers seeking advanced tools for market analysis.</li>
</ul>

<h3>Title: Secure Human Oversight of AI: Exploring the Attack Surface of Human Oversight</h3>
<ul>
<li><strong>Authors: </strong>Jonas C. Ditz, Veronika Lazar, Elmar Lichtme√ü, Carola Plesch, Matthias Heck, Kevin Baum, Markus Langer</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12290">https://arxiv.org/abs/2509.12290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12290">https://arxiv.org/pdf/2509.12290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12290]] Secure Human Oversight of AI: Exploring the Attack Surface of Human Oversight(https://arxiv.org/abs/2509.12290)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Human oversight of AI is promoted as a safeguard against risks such as inaccurate outputs, system malfunctions, or violations of fundamental rights, and is mandated in regulation like the European AI Act. Yet debates on human oversight have largely focused on its effectiveness, while overlooking a critical dimension: the security of human oversight. We argue that human oversight creates a new attack surface within the safety, security, and accountability architecture of AI operations. Drawing on cybersecurity perspectives, we analyze attack vectors that threaten the requirements of effective human oversight, thereby undermining the safety of AI operations. Such attacks may target the AI system, its communication with oversight personnel, or the personnel themselves. We then outline hardening strategies to mitigate these risks. Our contributions are: (1) introducing a security perspective on human oversight, and (2) providing an overview of attack vectors and hardening strategies to enable secure human oversight of AI.</li>
</ul>

<h3>Title: Collaborative P4-SDN DDoS Detection and Mitigation with Early-Exit Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Ouassim Karrakchou, Alaa Zniber, Anass Sebbar, Mounir Ghogho</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12291">https://arxiv.org/abs/2509.12291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12291">https://arxiv.org/pdf/2509.12291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12291]] Collaborative P4-SDN DDoS Detection and Mitigation with Early-Exit Neural Networks(https://arxiv.org/abs/2509.12291)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Distributed Denial of Service (DDoS) attacks pose a persistent threat to network security, requiring timely and scalable mitigation strategies. In this paper, we propose a novel collaborative architecture that integrates a P4-programmable data plane with an SDN control plane to enable real-time DDoS detection and response. At the core of our approach is a split early-exit neural network that performs partial inference in the data plane using a quantized Convolutional Neural Network (CNN), while deferring uncertain cases to a Gated Recurrent Unit (GRU) module in the control plane. This design enables high-speed classification at line rate with the ability to escalate more complex flows for deeper analysis. Experimental evaluation using real-world DDoS datasets demonstrates that our approach achieves high detection accuracy with significantly reduced inference latency and control plane overhead. These results highlight the potential of tightly coupled ML-P4-SDN systems for efficient, adaptive, and low-latency DDoS defense.</li>
</ul>

<h3>Title: Uncertainty-Aware Hourly Air Temperature Mapping at 2 km Resolution via Physics-Guided Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Shengjie Kris Liu, Siqin Wang, Lu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12329">https://arxiv.org/abs/2509.12329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12329">https://arxiv.org/pdf/2509.12329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12329]] Uncertainty-Aware Hourly Air Temperature Mapping at 2 km Resolution via Physics-Guided Deep Learning(https://arxiv.org/abs/2509.12329)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Near-surface air temperature is a key physical property of the Earth's surface. Although weather stations offer continuous monitoring and satellites provide broad spatial coverage, no single data source offers seamless data in a spatiotemporal fashion. Here, we propose a data-driven, physics-guided deep learning approach to generate hourly air temperature data at 2 km resolution over the contiguous United States. The approach, called Amplifier Air-Transformer, first reconstructs GOES-16 surface temperature data obscured by clouds. It does so through a neural network encoded with the annual temperature cycle, incorporating a linear term to amplify ERA5 temperature values at finer scales and convolutional layers to capture spatiotemporal variations. Then, another neural network transforms the reconstructed surface temperature into air temperature by leveraging its latent relationship with key Earth surface properties. The approach is further enhanced with predictive uncertainty estimation through deep ensemble learning to improve reliability. The proposed approach is built and tested on 77.7 billion surface temperature pixels and 155 million air temperature records from weather stations across the contiguous United States (2018-2024), achieving hourly air temperature mapping accuracy of 1.93 C in station-based validation. The proposed approach streamlines surface temperature reconstruction and air temperature prediction, and it can be extended to other satellite sources for seamless air temperature monitoring at high spatiotemporal resolution. The generated data of this study can be downloaded at this https URL, and the project webpage can be found at this https URL.</li>
</ul>

<h3>Title: Integrating Attention-Enhanced LSTM and Particle Swarm Optimization for Dynamic Pricing and Replenishment Strategies in Fresh Food Supermarkets</h3>
<ul>
<li><strong>Authors: </strong>Xianchen Liu (1), Tianhui Zhang (2), Xinyu Zhang (3), Lingmin Hou (3), Zhen Guo (4), Yuanhao Tian (5), Yang Liu (6) ((1) Department of Electrical and Computer Engineering, Florida International University, Miami, FL, 33199 USA (2) College of Engineering, Northeastern University, Boston, MA, 02169 USA (3) Department of Computer Science, Rochester Institute of Technology, Rochester, USA (4) Department of Mechanical and Materials Engineering, Florida International University, Miami, FL, 33199 USA (5) Department of Politics &amp; International Relations, Florida International University, Miami, FL, 33199 USA (6) College of Arts &amp; Sciences, University of Miami, Miami, FL 33124, USA)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12339">https://arxiv.org/abs/2509.12339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12339">https://arxiv.org/pdf/2509.12339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12339]] Integrating Attention-Enhanced LSTM and Particle Swarm Optimization for Dynamic Pricing and Replenishment Strategies in Fresh Food Supermarkets(https://arxiv.org/abs/2509.12339)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This paper presents a novel approach to optimizing pricing and replenishment strategies in fresh food supermarkets by combining Long Short-Term Memory (LSTM) networks with Particle Swarm Optimization (PSO). The LSTM model, enhanced with an attention mechanism, is used to predict sales volumes, pricing trends, and spoilage rates over a seven-day period. The predictions generated by the LSTM model serve as inputs for the PSO algorithm, which iteratively optimizes pricing and replenishment strategies to maximize profitability while adhering to inventory constraints. The integration of cost-plus pricing allows for dynamic adjustments based on fixed and variable costs, ensuring real-time adaptability to market fluctuations. The framework not only maximizes profits but also reduces food waste, contributing to more sustainable supermarket operations. The attention mechanism enhances the interpretability of the LSTM model by identifying key time points and factors influencing sales, improving decision-making accuracy. This methodology bridges the gap between predictive modeling and optimization, offering a scalable solution for dynamic pricing and inventory management in fresh food retail and other industries dealing with perishable goods.</li>
</ul>

<h3>Title: MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch</h3>
<ul>
<li><strong>Authors: </strong>Nikolay Banar, Ehsan Lotfi, Jens Van Nooten, Cristina Arhiliuc, Marija Kliocaite, Walter Daelemans</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12340">https://arxiv.org/abs/2509.12340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12340">https://arxiv.org/pdf/2509.12340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12340]] MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch(https://arxiv.org/abs/2509.12340)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, embedding resources, including models, benchmarks, and datasets, have been widely released to support a variety of languages. However, the Dutch language remains underrepresented, typically comprising only a small fraction of the published multilingual resources. To address this gap and encourage the further development of Dutch embeddings, we introduce new resources for their evaluation and generation. First, we introduce the Massive Text Embedding Benchmark for Dutch (MTEB-NL), which includes both existing Dutch datasets and newly created ones, covering a wide range of tasks. Second, we provide a training dataset compiled from available Dutch retrieval datasets, complemented with synthetic data generated by large language models to expand task coverage beyond retrieval. Finally, we release a series of E5-NL models compact yet efficient embedding models that demonstrate strong performance across multiple tasks. We make our resources publicly available through the Hugging Face Hub and the MTEB package.</li>
</ul>

<h3>Title: FEDONet : Fourier-Embedded DeepONet for Spectrally Accurate Operator Learning</h3>
<ul>
<li><strong>Authors: </strong>Arth Sojitra, Mrigank Dhingra, Omer San</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12344">https://arxiv.org/abs/2509.12344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12344">https://arxiv.org/pdf/2509.12344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12344]] FEDONet : Fourier-Embedded DeepONet for Spectrally Accurate Operator Learning(https://arxiv.org/abs/2509.12344)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep Operator Networks (DeepONets) have recently emerged as powerful data-driven frameworks for learning nonlinear operators, particularly suited for approximating solutions to partial differential equations (PDEs). Despite their promising capabilities, the standard implementation of DeepONets, which typically employs fully connected linear layers in the trunk network, can encounter limitations in capturing complex spatial structures inherent to various PDEs. To address this, we introduce Fourier-embedded trunk networks within the DeepONet architecture, leveraging random Fourier feature mappings to enrich spatial representation capabilities. Our proposed Fourier-embedded DeepONet, FEDONet demonstrates superior performance compared to the traditional DeepONet across a comprehensive suite of PDE-driven datasets, including the two-dimensional Poisson equation, Burgers' equation, the Lorenz-63 chaotic system, Eikonal equation, Allen-Cahn equation, Kuramoto-Sivashinsky equation, and the Lorenz-96 system. Empirical evaluations of FEDONet consistently show significant improvements in solution reconstruction accuracy, with average relative L2 performance gains ranging between 2-3x compared to the DeepONet baseline. This study highlights the effectiveness of Fourier embeddings in enhancing neural operator learning, offering a robust and broadly applicable methodology for PDE surrogate modeling.</li>
</ul>

<h3>Title: DS@GT AnimalCLEF: Triplet Learning over ViT Manifolds with Nearest Neighbor Classification for Animal Re-identification</h3>
<ul>
<li><strong>Authors: </strong>Anthony Miyaguchi, Chandrasekaran Maruthaiyannan, Charles R. Clark</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12353">https://arxiv.org/abs/2509.12353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12353">https://arxiv.org/pdf/2509.12353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12353]] DS@GT AnimalCLEF: Triplet Learning over ViT Manifolds with Nearest Neighbor Classification for Animal Re-identification(https://arxiv.org/abs/2509.12353)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper details the DS@GT team's entry for the AnimalCLEF 2025 re-identification challenge. Our key finding is that the effectiveness of post-hoc metric learning is highly contingent on the initial quality and domain-specificity of the backbone embeddings. We compare a general-purpose model (DINOv2) with a domain-specific model (MegaDescriptor) as a backbone. A K-Nearest Neighbor classifier with robust thresholding then identifies known individuals or flags new ones. While a triplet-learning projection head improved the performance of the specialized MegaDescriptor model by 0.13 points, it yielded minimal gains (0.03) for the general-purpose DINOv2 on averaged BAKS and BAUS. We demonstrate that the general-purpose manifold is more difficult to reshape for fine-grained tasks, as evidenced by stagnant validation loss and qualitative visualizations. This work highlights the critical limitations of refining general-purpose features for specialized, limited-data re-ID tasks and underscores the importance of domain-specific pre-training. The implementation for this work is publicly available at this http URL.</li>
</ul>

<h3>Title: Enhancing Smart Farming Through Federated Learning: A Secure, Scalable, and Efficient Approach for AI-Driven Agriculture</h3>
<ul>
<li><strong>Authors: </strong>Ritesh Janga, Rushit Dave</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12363">https://arxiv.org/abs/2509.12363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12363">https://arxiv.org/pdf/2509.12363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12363]] Enhancing Smart Farming Through Federated Learning: A Secure, Scalable, and Efficient Approach for AI-Driven Agriculture(https://arxiv.org/abs/2509.12363)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate</a></li>
<li><strong>Abstract: </strong>The agricultural sector is undergoing a transformation with the integration of advanced technologies, particularly in data-driven decision-making. This work proposes a federated learning framework for smart farming, aiming to develop a scalable, efficient, and secure solution for crop disease detection tailored to the environmental and operational conditions of Minnesota farms. By maintaining sensitive farm data locally and enabling collaborative model updates, our proposed framework seeks to achieve high accuracy in crop disease classification without compromising data privacy. We outline a methodology involving data collection from Minnesota farms, application of local deep learning algorithms, transfer learning, and a central aggregation server for model refinement, aiming to achieve improved accuracy in disease detection, good generalization across agricultural scenarios, lower costs in communication and training time, and earlier identification and intervention against diseases in future implementations. We outline a methodology and anticipated outcomes, setting the stage for empirical validation in subsequent studies. This work comes in a context where more and more demand for data-driven interpretations in agriculture has to be weighed with concerns about privacy from farms that are hesitant to share their operational data. This will be important to provide a secure and efficient disease detection method that can finally revolutionize smart farming systems and solve local agricultural problems with data confidentiality. In doing so, this paper bridges the gap between advanced machine learning techniques and the practical, privacy-sensitive needs of farmers in Minnesota and beyond, leveraging the benefits of federated learning.</li>
</ul>

<h3>Title: MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables</h3>
<ul>
<li><strong>Authors: </strong>Matteo Marcuzzo, Alessandro Zangari, Andrea Albarelli, Jose Camacho-Collados, Mohammad Taher Pilehvar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12371">https://arxiv.org/abs/2509.12371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12371">https://arxiv.org/pdf/2509.12371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12371]] MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables(https://arxiv.org/abs/2509.12371)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As LLMs excel on standard reading comprehension benchmarks, attention is shifting toward evaluating their capacity for complex abstract reasoning and inference. Literature-based benchmarks, with their rich narrative and moral depth, provide a compelling framework for evaluating such deeper comprehension skills. Here, we present MORABLES, a human-verified benchmark built from fables and short stories drawn from historical literature. The main task is structured as multiple-choice questions targeting moral inference, with carefully crafted distractors that challenge models to go beyond shallow, extractive question answering. To further stress-test model robustness, we introduce adversarial variants designed to surface LLM vulnerabilities and shortcuts due to issues such as data contamination. Our findings show that, while larger models outperform smaller ones, they remain susceptible to adversarial manipulation and often rely on superficial patterns rather than true moral reasoning. This brittleness results in significant self-contradiction, with the best models refuting their own answers in roughly 20% of cases depending on the framing of the moral choice. Interestingly, reasoning-enhanced models fail to bridge this gap, suggesting that scale - not reasoning ability - is the primary driver of performance.</li>
</ul>

<h3>Title: Explainable Unsupervised Multi-Anomaly Detection and Temporal Localization in Nuclear Times Series Data with a Dual Attention-Based Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos Vasili, Zachery T. Dahm, Stylianos Chatzidakis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12372">https://arxiv.org/abs/2509.12372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12372">https://arxiv.org/pdf/2509.12372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12372]] Explainable Unsupervised Multi-Anomaly Detection and Temporal Localization in Nuclear Times Series Data with a Dual Attention-Based Autoencoder(https://arxiv.org/abs/2509.12372)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>The nuclear industry is advancing toward more new reactor designs, with next-generation reactors expected to be smaller in scale and power output. These systems have the potential to produce large volumes of information in the form of multivariate time-series data, which could be used for enhanced real-time monitoring and control. In this context, the development of remote autonomous or semi-autonomous control systems for reactor operation has gained significant interest. A critical first step toward such systems is an accurate diagnostics module capable of detecting and localizing anomalies within the reactor system. Recent studies have proposed various ML and DL approaches for anomaly detection in the nuclear domain. Despite promising results, key challenges remain, including limited to no explainability, lack of access to real-world data, and scarcity of abnormal events, which impedes benchmarking and characterization. Most existing studies treat these methods as black boxes, while recent work highlights the need for greater interpretability of ML/DL outputs in safety-critical domains. Here, we propose an unsupervised methodology based on an LSTM autoencoder with a dual attention mechanism for characterization of abnormal events in a real-world reactor radiation area monitoring system. The framework includes not only detection but also localization of the event and was evaluated using real-world datasets of increasing complexity from the PUR-1 research reactor. The attention mechanisms operate in both the feature and temporal dimensions, where the feature attention assigns weights to radiation sensors exhibiting abnormal patterns, while time attention highlights the specific timesteps where irregularities occur, thus enabling localization. By combining the results, the framework can identify both the affected sensors and the duration of each anomaly within a single unified network.</li>
</ul>

<h3>Title: Diffusion-Based Generation and Imputation of Driving Scenarios from Limited Vehicle CAN Data</h3>
<ul>
<li><strong>Authors: </strong>Julian Ripper, Ousama Esbel, Rafael Fietzek, Max M√ºhlh√§user, Thomas Kreutz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12375">https://arxiv.org/abs/2509.12375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12375">https://arxiv.org/pdf/2509.12375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12375]] Diffusion-Based Generation and Imputation of Driving Scenarios from Limited Vehicle CAN Data(https://arxiv.org/abs/2509.12375)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Training deep learning methods on small time series datasets that also include corrupted samples is challenging. Diffusion models have shown to be effective to generate realistic and synthetic data, and correct corrupted samples through imputation. In this context, this paper focuses on generating synthetic yet realistic samples of automotive time series data. We show that denoising diffusion probabilistic models (DDPMs) can effectively solve this task by applying them to a challenging vehicle CAN-dataset with long-term data and a limited number of samples. Therefore, we propose a hybrid generative approach that combines autoregressive and non-autoregressive techniques. We evaluate our approach with two recently proposed DDPM architectures for time series generation, for which we propose several improvements. To evaluate the generated samples, we propose three metrics that quantify physical correctness and test track adherence. Our best model is able to outperform even the training data in terms of physical correctness, while showing plausible driving behavior. Finally, we use our best model to successfully impute physically implausible regions in the training data, thereby improving the data quality.</li>
</ul>

<h3>Title: LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Anu Pradhan, Alexandra Ortan, Apurv Verma, Madhavan Seshadri</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12382">https://arxiv.org/abs/2509.12382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12382">https://arxiv.org/pdf/2509.12382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12382]] LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation(https://arxiv.org/abs/2509.12382)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>The evaluation bottleneck in recommendation systems has become particularly acute with the rise of Generative AI, where traditional metrics fall short of capturing nuanced quality dimensions that matter in specialized domains like legal research. Can we trust Large Language Models to serve as reliable judges of their own kind? This paper investigates LLM-as-a-Judge as a principled approach to evaluating Retrieval-Augmented Generation systems in legal contexts, where the stakes of recommendation quality are exceptionally high. We tackle two fundamental questions that determine practical viability: which inter-rater reliability metrics best capture the alignment between LLM and human assessments, and how do we conduct statistically sound comparisons between competing systems? Through systematic experimentation, we discover that traditional agreement metrics like Krippendorff's alpha can be misleading in the skewed distributions typical of AI system evaluations. Instead, Gwet's AC2 and rank correlation coefficients emerge as more robust indicators for judge selection, while the Wilcoxon Signed-Rank Test with Benjamini-Hochberg corrections provides the statistical rigor needed for reliable system comparisons. Our findings suggest a path toward scalable, cost-effective evaluation that maintains the precision demanded by legal applications, transforming what was once a human-intensive bottleneck into an automated, yet statistically principled, evaluation framework.</li>
</ul>

<h3>Title: SENTRA: Selected-Next-Token Transformer for LLM Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Mitchell Plyler, Yilun Zhang, Alexander Tuzhilin, Saoud Khalifah, Sen Tian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12385">https://arxiv.org/abs/2509.12385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12385">https://arxiv.org/pdf/2509.12385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12385]] SENTRA: Selected-Next-Token Transformer for LLM Text Detection(https://arxiv.org/abs/2509.12385)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>LLMs are becoming increasingly capable and widespread. Consequently, the potential and reality of their misuse is also growing. In this work, we address the problem of detecting LLM-generated text that is not explicitly declared as such. We present a novel, general-purpose, and supervised LLM text detector, SElected-Next-Token tRAnsformer (SENTRA). SENTRA is a Transformer-based encoder leveraging selected-next-token-probability sequences and utilizing contrastive pre-training on large amounts of unlabeled data. Our experiments on three popular public datasets across 24 domains of text demonstrate SENTRA is a general-purpose classifier that significantly outperforms popular baselines in the out-of-domain setting.</li>
</ul>

<h3>Title: Amulet: a Python Library for Assessing Interactions Among ML Defenses and Risks</h3>
<ul>
<li><strong>Authors: </strong>Asim Waheed, Vasisht Duddu, Rui Zhang, Sebastian Szyller, N. Asokan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12386">https://arxiv.org/abs/2509.12386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12386">https://arxiv.org/pdf/2509.12386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12386]] Amulet: a Python Library for Assessing Interactions Among ML Defenses and Risks(https://arxiv.org/abs/2509.12386)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, defense, attack, fair</a></li>
<li><strong>Abstract: </strong>ML models are susceptible to risks to security, privacy, and fairness. Several defenses are designed to protect against their intended risks, but can inadvertently affect susceptibility to other unrelated risks, known as unintended interactions. Several jurisdictions are preparing ML regulatory frameworks that require ML practitioners to assess the susceptibility of ML models to different risks. A library for valuating unintended interactions that can be used by (a) practitioners to evaluate unintended interactions at scale prior to model deployment and (b) researchers to design defenses which do not suffer from an unintended increase in unrelated risks. Ideally, such a library should be i) comprehensive by including representative attacks, defenses and metrics for different risks, ii) extensible to new modules due to its modular design, iii) consistent with a user-friendly API template for inputs and outputs, iv) applicable to evaluate previously unexplored unintended interactions. We present AMULET, a Python library that covers risks to security, privacy, and fairness, which satisfies all these requirements. AMULET can be used to evaluate unexplored unintended interactions, compare effectiveness between defenses or attacks, and include new attacks and defenses.</li>
</ul>

<h3>Title: Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for Few-Shot Generalization</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Zayaan S</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12387">https://arxiv.org/abs/2509.12387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12387">https://arxiv.org/pdf/2509.12387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12387]] Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for Few-Shot Generalization(https://arxiv.org/abs/2509.12387)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Modern deep learning models excel at pattern recognition but remain fundamentally limited by their reliance on spurious correlations, leading to poor generalization and a demand for massive datasets. We argue that a key ingredient for human-like intelligence-robust, sample-efficient learning-stems from an understanding of causal mechanisms. In this work, we introduce Causal-Symbolic Meta-Learning (CSML), a novel framework that learns to infer the latent causal structure of a task distribution. CSML comprises three key modules: a perception module that maps raw inputs to disentangled symbolic representations; a differentiable causal induction module that discovers the underlying causal graph governing these symbols and a graph-based reasoning module that leverages this graph to make predictions. By meta-learning a shared causal world model across a distribution of tasks, CSML can rapidly adapt to novel tasks, including those requiring reasoning about interventions and counterfactuals, from only a handful of examples. We introduce CausalWorld, a new physics-based benchmark designed to test these capabilities. Our experiments show that CSML dramatically outperforms state-of-the-art meta-learning and neuro-symbolic baselines, particularly on tasks demanding true causal inference.</li>
</ul>

<h3>Title: From Orthomosaics to Raw UAV Imagery: Enhancing Palm Detection and Crown-Center Localization</h3>
<ul>
<li><strong>Authors: </strong>Rongkun Zhu, Kangning Cui, Wei Tang, Rui-Feng Wang, Sarra Alqahtani, David Lutz, Fan Yang, Paul Fine, Jordan Karubian, Robert Plemmons, Jean-Michel Morel, Victor Pauca, Miles Silman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12400">https://arxiv.org/abs/2509.12400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12400">https://arxiv.org/pdf/2509.12400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12400]] From Orthomosaics to Raw UAV Imagery: Enhancing Palm Detection and Crown-Center Localization(https://arxiv.org/abs/2509.12400)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate mapping of individual trees is essential for ecological monitoring and forest management. Orthomosaic imagery from unmanned aerial vehicles (UAVs) is widely used, but stitching artifacts and heavy preprocessing limit its suitability for field deployment. This study explores the use of raw UAV imagery for palm detection and crown-center localization in tropical forests. Two research questions are addressed: (1) how detection performance varies across orthomosaic and raw imagery, including within-domain and cross-domain transfer, and (2) to what extent crown-center annotations improve localization accuracy beyond bounding-box centroids. Using state-of-the-art detectors and keypoint models, we show that raw imagery yields superior performance in deployment-relevant scenarios, while orthomosaics retain value for robust cross-domain generalization. Incorporating crown-center annotations in training further improves localization and provides precise tree positions for downstream ecological analyses. These findings offer practical guidance for UAV-based biodiversity and conservation monitoring.</li>
</ul>

<h3>Title: MORQA: Benchmarking Evaluation Metrics for Medical Open-Ended Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Wen-wai Yim, Asma Ben Abacha, Zixuan Yu, Robert Doerning, Fei Xia, Meliha Yetisgen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12405">https://arxiv.org/abs/2509.12405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12405">https://arxiv.org/pdf/2509.12405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12405]] MORQA: Benchmarking Evaluation Metrics for Medical Open-Ended Question Answering(https://arxiv.org/abs/2509.12405)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Evaluating natural language generation (NLG) systems in the medical domain presents unique challenges due to the critical demands for accuracy, relevance, and domain-specific expertise. Traditional automatic evaluation metrics, such as BLEU, ROUGE, and BERTScore, often fall short in distinguishing between high-quality outputs, especially given the open-ended nature of medical question answering (QA) tasks where multiple valid responses may exist. In this work, we introduce MORQA (Medical Open-Response QA), a new multilingual benchmark designed to assess the effectiveness of NLG evaluation metrics across three medical visual and text-based QA datasets in English and Chinese. Unlike prior resources, our datasets feature 2-4+ gold-standard answers authored by medical professionals, along with expert human ratings for three English and Chinese subsets. We benchmark both traditional metrics and large language model (LLM)-based evaluators, such as GPT-4 and Gemini, finding that LLM-based approaches significantly outperform traditional metrics in correlating with expert judgments. We further analyze factors driving this improvement, including LLMs' sensitivity to semantic nuances and robustness to variability among reference answers. Our results provide the first comprehensive, multilingual qualitative study of NLG evaluation in the medical domain, highlighting the need for human-aligned evaluation methods. All datasets and annotations will be publicly released to support future research.</li>
</ul>

<h3>Title: Bayesian Parametric Matrix Models: Principled Uncertainty Quantification for Spectral Learning</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Nooraiepour</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12406">https://arxiv.org/abs/2509.12406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12406">https://arxiv.org/pdf/2509.12406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12406]] Bayesian Parametric Matrix Models: Principled Uncertainty Quantification for Spectral Learning(https://arxiv.org/abs/2509.12406)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Scientific machine learning increasingly uses spectral methods to understand physical systems. Current spectral learning approaches provide only point estimates without uncertainty quantification, limiting their use in safety-critical applications where prediction confidence is essential. Parametric matrix models have emerged as powerful tools for scientific machine learning, achieving exceptional performance by learning governing equations. However, their deterministic nature limits deployment in uncertainty quantification applications. We introduce Bayesian parametric matrix models (B-PMMs), a principled framework that extends PMMs to provide uncertainty estimates while preserving their spectral structure and computational efficiency. B-PMM addresses the fundamental challenge of quantifying uncertainty in matrix eigenvalue problems where standard Bayesian methods fail due to the geometric constraints of spectral decomposition. The theoretical contributions include: (i) adaptive spectral decomposition with regularized matrix perturbation bounds that characterize eigenvalue uncertainty propagation, (ii) structured variational inference algorithms using manifold-aware matrix-variate Gaussian posteriors that respect Hermitian constraints, and (iii) finite-sample calibration guarantees with explicit dependence on spectral gaps and problem conditioning. Experimental validation across matrix dimensions from 5x5 to 500x500 with perfect convergence rates demonstrates that B-PMMs achieve exceptional uncertainty calibration (ECE < 0.05) while maintaining favorable scaling. The framework exhibits graceful degradation under spectral ill-conditioning and provides reliable uncertainty estimates even in near-degenerate regimes. The proposed framework supports robust spectral learning in uncertainty-critical domains and lays the groundwork for broader Bayesian spectral machine learning.</li>
</ul>

<h3>Title: MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts</h3>
<ul>
<li><strong>Authors: </strong>Jiayi He, Yangmin Huang, Qianyun Du, Xiangying Zhou, Zhiyang He, Jiaxue Hu, Xiaodong Tao, Lixian Lai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12440">https://arxiv.org/abs/2509.12440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12440">https://arxiv.org/pdf/2509.12440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12440]] MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts(https://arxiv.org/abs/2509.12440)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The increasing deployment of Large Language Models (LLMs) in healthcare necessitates a rigorous evaluation of their factual reliability. However, existing benchmarks are often limited by narrow domains of data, failing to capture the complexity of real-world medical information. To address this critical gap, we introduce MedFact, a new and challenging benchmark for Chinese medical fact-checking. MedFact comprises 2,116 expert-annotated instances curated from diverse real-world texts, spanning 13 medical specialties, 8 fine-grained error types, 4 writing styles, and multiple difficulty levels. Its construction employs a hybrid AI-human framework where iterative expert feedback refines an AI-driven, multi-criteria filtering process, ensuring both high data quality and difficulty. We conduct a comprehensive evaluation of 20 leading LLMs, benchmarking their performance on veracity classification and error localization against a human expert baseline. Our results reveal that while models can often determine if a text contains an error, precisely localizing it remains a substantial challenge, with even top-performing models falling short of human performance. Furthermore, our analysis uncovers a frequent ``over-criticism'' phenomenon, a tendency for models to misidentify correct information as erroneous, which is exacerbated by advanced reasoning techniques such as multi-agent collaboration and inference-time scaling. By highlighting these critical challenges for deploying LLMs in medical applications, MedFact provides a robust resource to drive the development of more factually reliable and medically aware models.</li>
</ul>

<h3>Title: Cott-ADNet: Lightweight Real-Time Cotton Boll and Flower Detection Under Field Conditions</h3>
<ul>
<li><strong>Authors: </strong>Rui-Feng Wang, Mingrui Xu, Matthew C Bauer, Iago Beffart Schardong, Xiaowen Ma, Kangning Cui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12442">https://arxiv.org/abs/2509.12442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12442">https://arxiv.org/pdf/2509.12442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12442]] Cott-ADNet: Lightweight Real-Time Cotton Boll and Flower Detection Under Field Conditions(https://arxiv.org/abs/2509.12442)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Cotton is one of the most important natural fiber crops worldwide, yet harvesting remains limited by labor-intensive manual picking, low efficiency, and yield losses from missing the optimal harvest window. Accurate recognition of cotton bolls and their maturity is therefore essential for automation, yield estimation, and breeding research. We propose Cott-ADNet, a lightweight real-time detector tailored to cotton boll and flower recognition under complex field conditions. Building on YOLOv11n, Cott-ADNet enhances spatial representation and robustness through improved convolutional designs, while introducing two new modules: a NeLU-enhanced Global Attention Mechanism to better capture weak and low-contrast features, and a Dilated Receptive Field SPPF to expand receptive fields for more effective multi-scale context modeling at low computational cost. We curate a labeled dataset of 4,966 images, and release an external validation set of 1,216 field images to support future research. Experiments show that Cott-ADNet achieves 91.5% Precision, 89.8% Recall, 93.3% mAP50, 71.3% mAP, and 90.6% F1-Score with only 7.5 GFLOPs, maintaining stable performance under multi-scale and rotational variations. These results demonstrate Cott-ADNet as an accurate and efficient solution for in-field deployment, and thus provide a reliable basis for automated cotton harvesting and high-throughput phenotypic analysis. Code and dataset is available at this https URL.</li>
</ul>

<h3>Title: Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications</h3>
<ul>
<li><strong>Authors: </strong>Zhenxin Zhang, Zhihua Xu, Yuwei Cao, Ningli Xu, Shuye Wang, Shen'ao Cui, Zhen Li, Rongjun Qin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12452">https://arxiv.org/abs/2509.12452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12452">https://arxiv.org/pdf/2509.12452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12452]] Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications(https://arxiv.org/abs/2509.12452)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Point cloud processing as a fundamental task in the field of geomatics and computer vision, has been supporting tasks and applications at different scales from air to ground, including mapping, environmental monitoring, urban/tree structure modeling, automated driving, robotics, disaster responses etc. Due to the rapid development of deep learning, point cloud processing algorithms have nowadays been almost explicitly dominated by learning-based approaches, most of which are yet transitioned into real-world practices. Existing surveys primarily focus on the ever-updating network architecture to accommodate unordered point clouds, largely ignoring their practical values in typical point cloud processing applications, in which extra-large volume of data, diverse scene contents, varying point density, data modality need to be considered. In this paper, we provide a meta review on deep learning approaches and datasets that cover a selection of critical tasks of point cloud processing in use such as scene completion, registration, semantic segmentation, and modeling. By reviewing a broad range of urban and environmental applications these tasks can support, we identify gaps to be closed as these methods transformed into applications and draw concluding remarks in both the algorithmic and practical aspects of the surveyed methods.</li>
</ul>

<h3>Title: Two-Stage Decoupling Framework for Variable-Length Glaucoma Prognosis</h3>
<ul>
<li><strong>Authors: </strong>Yiran Song, Yikai Zhang, Silvia Orengo-Nania, Nian Wang, Fenglong Ma, Rui Zhang, Yifan Peng, Mingquan Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12453">https://arxiv.org/abs/2509.12453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12453">https://arxiv.org/pdf/2509.12453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12453]] Two-Stage Decoupling Framework for Variable-Length Glaucoma Prognosis(https://arxiv.org/abs/2509.12453)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Glaucoma is one of the leading causes of irreversible blindness worldwide. Glaucoma prognosis is essential for identifying at-risk patients and enabling timely intervention to prevent blindness. Many existing approaches rely on historical sequential data but are constrained by fixed-length inputs, limiting their flexibility. Additionally, traditional glaucoma prognosis methods often employ end-to-end models, which struggle with the limited size of glaucoma datasets. To address these challenges, we propose a Two-Stage Decoupling Framework (TSDF) for variable-length glaucoma prognosis. In the first stage, we employ a feature representation module that leverages self-supervised learning to aggregate multiple glaucoma datasets for training, disregarding differences in their supervisory information. This approach enables datasets of varying sizes to learn better feature representations. In the second stage, we introduce a temporal aggregation module that incorporates an attention-based mechanism to process sequential inputs of varying lengths, ensuring flexible and efficient utilization of all available data. This design significantly enhances model performance while maintaining a compact parameter size. Extensive experiments on two benchmark glaucoma datasets:the Ocular Hypertension Treatment Study (OHTS) and the Glaucoma Real-world Appraisal Progression Ensemble (GRAPE),which differ significantly in scale and clinical settings,demonstrate the effectiveness and robustness of our approach.</li>
</ul>

<h3>Title: On the Regularity and Fairness of Combinatorial Multi-Armed Bandit</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyi Wu, Bin Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12457">https://arxiv.org/abs/2509.12457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12457">https://arxiv.org/pdf/2509.12457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12457]] On the Regularity and Fairness of Combinatorial Multi-Armed Bandit(https://arxiv.org/abs/2509.12457)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The combinatorial multi-armed bandit model is designed to maximize cumulative rewards in the presence of uncertainty by activating a subset of arms in each round. This paper is inspired by two critical applications in wireless networks, where it's not only essential to maximize cumulative rewards but also to guarantee fairness among arms (i.e., the minimum average reward required by each arm) and ensure reward regularity (i.e., how often each arm receives the reward). In this paper, we propose a parameterized regular and fair learning algorithm to achieve these three objectives. In particular, the proposed algorithm linearly combines virtual queue-lengths (tracking the fairness violations), Time-Since-Last-Reward (TSLR) metrics, and Upper Confidence Bound (UCB) estimates in its weight measure. Here, TSLR is similar to age-of-information and measures the elapsed number of rounds since the last time an arm received a reward, capturing the reward regularity performance, and UCB estimates are utilized to balance the tradeoff between exploration and exploitation in online learning. By exploring a key relationship between virtual queue-lengths and TSLR metrics and utilizing several non-trivial Lyapunov functions, we analytically characterize zero cumulative fairness violation, reward regularity, and cumulative regret performance under our proposed algorithm. These theoretical outcomes are verified by simulations based on two real-world datasets.</li>
</ul>

<h3>Title: Does Language Model Understand Language?</h3>
<ul>
<li><strong>Authors: </strong>Suvojit Acharjee, Utathya Aich, Asfak Ali</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12459">https://arxiv.org/abs/2509.12459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12459">https://arxiv.org/pdf/2509.12459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12459]] Does Language Model Understand Language?(https://arxiv.org/abs/2509.12459)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite advances in natural language generation and understanding, LM still struggle with fine grained linguistic phenomena such as tense, negation, voice, and modality which are the elements central to effective human communication. In the context of the United Nations SDG 4, where linguistic clarity is critical, the deployment of LMs in educational technologies demands careful scrutiny. As LMs are increasingly powering applications like tutoring systems, automated grading, and translation, their alignment with human linguistic interpretation becomes essential for effective learning. In this study, we conduct a evaluation of SOTA language models across these challenging contexts in both English and Bengali. To ensure a structured assessment, we introduce a new Route for Evaluation of Cognitive Inference in Systematic Environments guidelines. Our proposed LUCID dataset, composed of carefully crafted sentence pairs in English and Bengali, specifically challenges these models on critical aspects of language comprehension, including negation, tense, voice variations. We assess the performance of SOTA models including MISTRAL-SABA-24B, LLaMA-4-Scout-17B, LLaMA-3.3-70B, Gemma2-9B, and Compound-Beta using standard metrics like Pearson correlation, Spearman correlation, and Mean Absolute Error, as well as novel, linguistically inspired metric the HCE accuracy. The HCE accuracy measures how often model predictions fall within one standard deviation of the mean human rating, thus capturing human like tolerance for variability in language interpretation. Our findings highlight Compound-Beta as the most balanced model, consistently achieving high correlations and low MAEs across diverse language conditions. It records the highest Pearson correlation in English and demonstrates robust performance on mixed-language data, indicating a strong alignment with human judgments in cross lingual scenarios.</li>
</ul>

<h3>Title: Redefining Website Fingerprinting Attacks With Multiagent LLMs</h3>
<ul>
<li><strong>Authors: </strong>Chuxu Song, Dheekshith Dev Manohar Mekala, Hao Wang, Richard Martin</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12462">https://arxiv.org/abs/2509.12462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12462">https://arxiv.org/pdf/2509.12462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12462]] Redefining Website Fingerprinting Attacks With Multiagent LLMs(https://arxiv.org/abs/2509.12462)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Website Fingerprinting (WFP) uses deep learning models to classify encrypted network traffic to infer visited websites. While historically effective, prior methods fail to generalize to modern web environments. Single-page applications (SPAs) eliminate the paradigm of websites as sets of discrete pages, undermining page-based classification, and traffic from scripted browsers lacks the behavioral richness seen in real user sessions. Our study reveals that users exhibit highly diverse behaviors even on the same website, producing traffic patterns that vary significantly across individuals. This behavioral entropy makes WFP a harder problem than previously assumed and highlights the need for larger, more diverse, and representative datasets to achieve robust performance. To address this, we propose a new paradigm: we drop session-boundaries in favor of contiguous traffic segments and develop a scalable data generation pipeline using large language models (LLM) agents. These multi-agent systems coordinate decision-making and browser interaction to simulate realistic, persona-driven browsing behavior at 3--5x lower cost than human collection. We evaluate nine state-of-the-art WFP models on traffic from 20 modern websites browsed by 30 real users, and compare training performance across human, scripted, and LLM-generated datasets. All models achieve under 10\% accuracy when trained on scripted traffic and tested on human data. In contrast, LLM-generated traffic boosts accuracy into the 80\% range, demonstrating strong generalization to real-world traces. Our findings indicate that for modern WFP, model performance is increasingly bottlenecked by data quality, and that scalable, semantically grounded synthetic traffic is essential for capturing the complexity of real user behavior.</li>
</ul>

<h3>Title: Image Tokenizer Needs Post-Training</h3>
<ul>
<li><strong>Authors: </strong>Kai Qiu, Xiang Li, Hao Chen, Jason Kuen, Xiaohao Xu, Jiuxiang Gu, Yinyi Luo, Bhiksha Raj, Zhe Lin, Marios Savvides</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12474">https://arxiv.org/abs/2509.12474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12474">https://arxiv.org/pdf/2509.12474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12474]] Image Tokenizer Needs Post-Training(https://arxiv.org/abs/2509.12474)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent image generative models typically capture the image distribution in a pre-constructed latent space, relying on a frozen image tokenizer. However, there exists a significant discrepancy between the reconstruction and generation distribution, where current tokenizers only prioritize the reconstruction task that happens before generative training without considering the generation errors during sampling. In this paper, we comprehensively analyze the reason for this discrepancy in a discrete latent space, and, from which, we propose a novel tokenizer training scheme including both main-training and post-training, focusing on improving latent space construction and decoding respectively. During the main training, a latent perturbation strategy is proposed to simulate sampling noises, \ie, the unexpected tokens generated in generative inference. Specifically, we propose a plug-and-play tokenizer training scheme, which significantly enhances the robustness of tokenizer, thus boosting the generation quality and convergence speed, and a novel tokenizer evaluation metric, \ie, pFID, which successfully correlates the tokenizer performance to generation quality. During post-training, we further optimize the tokenizer decoder regarding a well-trained generative model to mitigate the distribution difference between generated and reconstructed tokens. With a $\sim$400M generator, a discrete tokenizer trained with our proposed main training achieves a notable 1.60 gFID and further obtains 1.36 gFID with the additional post-training. Further experiments are conducted to broadly validate the effectiveness of our post-training strategy on off-the-shelf discrete and continuous tokenizers, coupled with autoregressive and diffusion-based generators.</li>
</ul>

<h3>Title: QKD Oracles for Authenticated Key Exchange</h3>
<ul>
<li><strong>Authors: </strong>Kathrin H√∂velmanns, Daan Planken, Christian Schaffner, Sebastian R. Verschoor</a></li>
<li><strong>Subjects: </strong>cs.CR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12478">https://arxiv.org/abs/2509.12478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12478">https://arxiv.org/pdf/2509.12478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12478]] QKD Oracles for Authenticated Key Exchange(https://arxiv.org/abs/2509.12478)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Authenticated Key Exchange (AKE) establishes shared ('symmetric') cryptographic keys which are essential for secure online communication. AKE protocols can be constructed from public-key cryptography like Key Encapsulation Mechanisms (KEMs). Another approach is to use Quantum Key Distribution (QKD) to establish a symmetric key, which uses quantum communication. Combining post-quantum AKE and QKD appropriately may provide security against quantum attacks even if only one of the two approaches turns out to be secure. We provide an extensive review of existing security analyses for combined AKE and their formal security models, and identify some gaps in their treatment of QKD key IDs. In particular, improper handling of QKD key IDs leads to Dependent-Key attacks on AKE. As our main conceptual contribution, we model QKD as an oracle that closely resembles the standard ETSI 014 QKD interface. We demonstrate the usability of our QKD oracle for cryptographic security analyses by integrating it into a prominent security model for AKE, called CK+ model, thereby obtaining a security model for combined AKE that catches Dependent-Key attacks. In this model, we formally prove security of a new protocol that combines QKD with a triple-KEM handshake. This is the first provably secure hybrid protocol that maintains information-theoretic security of QKD.</li>
</ul>

<h3>Title: Towards Foundational Models for Single-Chip Radar</h3>
<ul>
<li><strong>Authors: </strong>Tianshu Huang, Akarsh Prabhakara, Chuhan Chen, Jay Karhade, Deva Ramanan, Matthew O'Toole, Anthony Rowe</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12482">https://arxiv.org/abs/2509.12482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12482">https://arxiv.org/pdf/2509.12482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12482]] Towards Foundational Models for Single-Chip Radar(https://arxiv.org/abs/2509.12482)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>mmWave radars are compact, inexpensive, and durable sensors that are robust to occlusions and work regardless of environmental conditions, such as weather and darkness. However, this comes at the cost of poor angular resolution, especially for inexpensive single-chip radars, which are typically used in automotive and indoor sensing applications. Although many have proposed learning-based methods to mitigate this weakness, no standardized foundational models or large datasets for the mmWave radar have emerged, and practitioners have largely trained task-specific models from scratch using relatively small datasets. In this paper, we collect (to our knowledge) the largest available raw radar dataset with 1M samples (29 hours) and train a foundational model for 4D single-chip radar, which can predict 3D occupancy and semantic segmentation with quality that is typically only possible with much higher resolution sensors. We demonstrate that our Generalizable Radar Transformer (GRT) generalizes across diverse settings, can be fine-tuned for different tasks, and shows logarithmic data scaling of 20\% per $10\times$ data. We also run extensive ablations on common design decisions, and find that using raw radar data significantly outperforms widely-used lossy representations, equivalent to a $10\times$ increase in training data. Finally, we roughly estimate that $\approx$100M samples (3000 hours) of data are required to fully exploit the potential of GRT.</li>
</ul>

<h3>Title: Finite-Agent Stochastic Differential Games on Large Graphs: II. Graph-Based Architectures</h3>
<ul>
<li><strong>Authors: </strong>Ruimeng Hu, Jihao Long, Haosheng Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12484">https://arxiv.org/abs/2509.12484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12484">https://arxiv.org/pdf/2509.12484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12484]] Finite-Agent Stochastic Differential Games on Large Graphs: II. Graph-Based Architectures(https://arxiv.org/abs/2509.12484)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>We propose a novel neural network architecture, called Non-Trainable Modification (NTM), for computing Nash equilibria in stochastic differential games (SDGs) on graphs. These games model a broad class of graph-structured multi-agent systems arising in finance, robotics, energy, and social dynamics, where agents interact locally under uncertainty. The NTM architecture imposes a graph-guided sparsification on feedforward neural networks, embedding fixed, non-trainable components aligned with the underlying graph topology. This design enhances interpretability and stability, while significantly reducing the number of trainable parameters in large-scale, sparse settings. We theoretically establish a universal approximation property for NTM in static games on graphs and numerically validate its expressivity and robustness through supervised learning tasks. Building on this foundation, we incorporate NTM into two state-of-the-art game solvers, Direct Parameterization and Deep BSDE, yielding their sparse variants (NTM-DP and NTM-DBSDE). Numerical experiments on three SDGs across various graph structures demonstrate that NTM-based methods achieve performance comparable to their fully trainable counterparts, while offering improved computational efficiency.</li>
</ul>

<h3>Title: Evaluating Robustness of Vision-Language Models Under Noisy Conditions</h3>
<ul>
<li><strong>Authors: </strong>Purushoth, Alireza</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12492">https://arxiv.org/abs/2509.12492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12492">https://arxiv.org/pdf/2509.12492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12492]] Evaluating Robustness of Vision-Language Models Under Noisy Conditions(https://arxiv.org/abs/2509.12492)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have attained exceptional success across multimodal tasks such as image captioning and visual question answering. However, their robustness under noisy conditions remains unfamiliar. In this study, we present a comprehensive evaluation framework to evaluate the performance of several state-of-the-art VLMs under controlled perturbations, including lighting variation, motion blur, and compression artifacts. We used both lexical-based metrics (BLEU, METEOR, ROUGE, CIDEr) and neural-based similarity measures using sentence embeddings to quantify semantic alignment. Our experiments span diverse datasets, revealing key insights: (1) descriptiveness of ground-truth captions significantly influences model performance; (2) larger models like LLaVA excel in semantic understanding but do not universally outperform smaller models; and (3) certain noise types, such as JPEG compression and motion blur, dramatically degrade performance across models. Our findings highlight the nuanced trade-offs between model size, dataset characteristics, and noise resilience, offering a standardized benchmark for future robust multimodal learning.</li>
</ul>

<h3>Title: Instance-Guided Class Activation Mapping for Weakly Supervised Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ali Torabi, Sanjog Gaihre, MD Mahbubur Rahman, Yaqoob Majeed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12496">https://arxiv.org/abs/2509.12496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12496">https://arxiv.org/pdf/2509.12496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12496]] Instance-Guided Class Activation Mapping for Weakly Supervised Semantic Segmentation(https://arxiv.org/abs/2509.12496)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Weakly Supervised Semantic Segmentation (WSSS) addresses the challenge of training segmentation models using only image-level annotations, eliminating the need for expensive pixel-level labeling. While existing methods struggle with precise object boundary localization and often focus only on the most discriminative regions, we propose IG-CAM (Instance-Guided Class Activation Mapping), a novel approach that leverages instance-level cues and influence functions to generate high-quality, boundary-aware localization maps. Our method introduces three key innovations: (1) Instance-Guided Refinement that uses ground truth segmentation masks to guide CAM generation, ensuring complete object coverage rather than just discriminative parts; (2) Influence Function Integration that captures the relationship between training samples and model predictions, leading to more robust feature representations; and (3) Multi-Scale Boundary Enhancement that employs progressive refinement strategies to achieve sharp, precise object boundaries. IG-CAM achieves state-of-the-art performance on the PASCAL VOC 2012 dataset with an mIoU of 82.3% before post-processing, which further improves to 86.6% after applying Conditional Random Field (CRF) refinement, significantly outperforming previous WSSS methods. Our approach demonstrates superior localization accuracy, with complete object coverage and precise boundary delineation, while maintaining computational efficiency. Extensive ablation studies validate the contribution of each component, and qualitative comparisons across 600 diverse images showcase the method's robustness and generalization capability. The results establish IG-CAM as a new benchmark for weakly supervised semantic segmentation, offering a practical solution for scenarios where pixel-level annotations are unavailable or prohibitively expensive.</li>
</ul>

<h3>Title: Artist-Created Mesh Generation from Raw Observation</h3>
<ul>
<li><strong>Authors: </strong>Yao He, Youngjoong Kwon, Wenxiao Cai, Ehsan Adeli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12501">https://arxiv.org/abs/2509.12501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12501">https://arxiv.org/pdf/2509.12501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12501]] Artist-Created Mesh Generation from Raw Observation(https://arxiv.org/abs/2509.12501)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present an end-to-end framework for generating artist-style meshes from noisy or incomplete point clouds, such as those captured by real-world sensors like LiDAR or mobile RGB-D cameras. Artist-created meshes are crucial for commercial graphics pipelines due to their compatibility with animation and texturing tools and their efficiency in rendering. However, existing approaches often assume clean, complete inputs or rely on complex multi-stage pipelines, limiting their applicability in real-world scenarios. To address this, we propose an end-to-end method that refines the input point cloud and directly produces high-quality, artist-style meshes. At the core of our approach is a novel reformulation of 3D point cloud refinement as a 2D inpainting task, enabling the use of powerful generative models. Preliminary results on the ShapeNet dataset demonstrate the promise of our framework in producing clean, complete meshes.</li>
</ul>

<h3>Title: FunAudio-ASR Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Keyu An, Yanni Chen, Chong Deng, Changfeng Gao, Zhifu Gao, Bo Gong, Xiangang Li, Yabin Li, Xiang Lv, Yunjie Ji, Yiheng Jiang, Bin Ma, Haoneng Luo, Chongjia Ni, Zexu Pan, Yiping Peng, Zhendong Peng, Peiyao Wang, Hao Wang, Wen Wang, Wupeng Wang, Biao Tian, Zhentao Tan, Nan Yang, Bin Yuan, Jieping Ye, Jixing Yu, Qinglin Zhang, Kun Zou, Han Zhao, Shengkui Zhao, Jingren Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12508">https://arxiv.org/abs/2509.12508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12508">https://arxiv.org/pdf/2509.12508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12508]] FunAudio-ASR Technical Report(https://arxiv.org/abs/2509.12508)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present FunAudio-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, FunAudio-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, FunAudio-ASR achieves SOTA performance on real application datasets, demonstrating its effectiveness and robustness in practical settings.</li>
</ul>

<h3>Title: Axis-Aligned 3D Stalk Diameter Estimation from RGB-D Imagery</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Vail, Rahul Harsha Cheppally, Ajay Sharda, Sidharth Rai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12511">https://arxiv.org/abs/2509.12511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12511">https://arxiv.org/pdf/2509.12511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12511]] Axis-Aligned 3D Stalk Diameter Estimation from RGB-D Imagery(https://arxiv.org/abs/2509.12511)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate, high-throughput phenotyping is a critical component of modern crop breeding programs, especially for improving traits such as mechanical stability, biomass production, and disease resistance. Stalk diameter is a key structural trait, but traditional measurement methods are labor-intensive, error-prone, and unsuitable for scalable phenotyping. In this paper, we present a geometry-aware computer vision pipeline for estimating stalk diameter from RGB-D imagery. Our method integrates deep learning-based instance segmentation, 3D point cloud reconstruction, and axis-aligned slicing via Principal Component Analysis (PCA) to perform robust diameter estimation. By mitigating the effects of curvature, occlusion, and image noise, this approach offers a scalable and reliable solution to support high-throughput phenotyping in breeding and agronomic research.</li>
</ul>

<h3>Title: A comparison of pipelines for the translation of a low resource language based on transformers</h3>
<ul>
<li><strong>Authors: </strong>Chiara Bonfanti, Michele Colombino, Giulia Coucourde, Faeze Memari, Stefano Pinardi, Rosa Meo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CE, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12514">https://arxiv.org/abs/2509.12514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12514">https://arxiv.org/pdf/2509.12514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12514]] A comparison of pipelines for the translation of a low resource language based on transformers(https://arxiv.org/abs/2509.12514)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This work compares three pipelines for training transformer-based neural networks to produce machine translators for Bambara, a Mand√® language spoken in Africa by about 14,188,850 people. The first pipeline trains a simple transformer to translate sentences from French into Bambara. The second fine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures for French-to-Bambara translation. Models from the first two pipelines were trained with different hyperparameter combinations to improve BLEU and chrF scores, evaluated on both test sentences and official Bambara benchmarks. The third pipeline uses language distillation with a student-teacher dual neural network to integrate Bambara into a pre-trained LaBSE model, which provides language-agnostic embeddings. A BERT extension is then applied to LaBSE to generate translations. All pipelines were tested on Dokotoro (medical) and Bayelemagaba (mixed domains). Results show that the first pipeline, although simpler, achieves the best translation accuracy (10% BLEU, 21% chrF on Bayelemagaba), consistent with low-resource translation results. On the Yiri dataset, created for this work, it achieves 33.81% BLEU and 41% chrF. Instructor-based models perform better on single datasets than on aggregated collections, suggesting they capture dataset-specific patterns more effectively.</li>
</ul>

<h3>Title: Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time</h3>
<ul>
<li><strong>Authors: </strong>Yifan Lan, Yuanpu Cao, Weitong Zhang, Lu Lin, Jinghui Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12521">https://arxiv.org/abs/2509.12521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12521">https://arxiv.org/pdf/2509.12521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12521]] Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time(https://arxiv.org/abs/2509.12521)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Recently, Multimodal Large Language Models (MLLMs) have gained significant attention across various domains. However, their widespread adoption has also raised serious safety concerns. In this paper, we uncover a new safety risk of MLLMs: the output preference of MLLMs can be arbitrarily manipulated by carefully optimized images. Such attacks often generate contextually relevant yet biased responses that are neither overtly harmful nor unethical, making them difficult to detect. Specifically, we introduce a novel method, Preference Hijacking (Phi), for manipulating the MLLM response preferences using a preference hijacked image. Our method works at inference time and requires no model modifications. Additionally, we introduce a universal hijacking perturbation -- a transferable component that can be embedded into different images to hijack MLLM responses toward any attacker-specified preferences. Experimental results across various tasks demonstrate the effectiveness of our approach. The code for Phi is accessible at this https URL.</li>
</ul>

<h3>Title: Selective Risk Certification for LLM Outputs via Information-Lift Statistics: PAC-Bayes, Robustness, and Skeleton Design</h3>
<ul>
<li><strong>Authors: </strong>Sanjeda Akter, Ibne Farabi Shihab, Anuj Sharma</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12527">https://arxiv.org/abs/2509.12527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12527">https://arxiv.org/pdf/2509.12527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12527]] Selective Risk Certification for LLM Outputs via Information-Lift Statistics: PAC-Bayes, Robustness, and Skeleton Design(https://arxiv.org/abs/2509.12527)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models often produce plausible but incorrect outputs. Existing heuristics such as HallBayes lack formal guarantees. We develop the first comprehensive theory of \emph{information-lift certificates} under selective classification. Our contributions are: (i) a PAC-Bayes \emph{sub-gamma} analysis extending beyond standard Bernstein bounds; (ii) explicit skeleton sensitivity theorems quantifying robustness to misspecification; (iii) failure-mode guarantees under assumption violations; and (iv) a principled variational method for skeleton construction. Across six datasets and multiple model families, we validate assumptions empirically, reduce abstention by 12--15\% at the same risk, and maintain runtime overhead below 20\% (further reduced via batching).</li>
</ul>

<h3>Title: Exploiting Timing Side-Channels in Quantum Circuits Simulation Via ML-Based Methods</h3>
<ul>
<li><strong>Authors: </strong>Ben Dong, Hui Feng, Qian Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12535">https://arxiv.org/abs/2509.12535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12535">https://arxiv.org/pdf/2509.12535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12535]] Exploiting Timing Side-Channels in Quantum Circuits Simulation Via ML-Based Methods(https://arxiv.org/abs/2509.12535)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>As quantum computing advances, quantum circuit simulators serve as critical tools to bridge the current gap caused by limited quantum hardware availability. These simulators are typically deployed on cloud platforms, where users submit proprietary circuit designs for simulation. In this work, we demonstrate a novel timing side-channel attack targeting cloud- based quantum simulators. A co-located malicious process can observe fine-grained execution timing patterns to extract sensitive information about concurrently running quantum circuits. We systematically analyze simulator behavior using the QASMBench benchmark suite, profiling timing and memory characteristics across various circuit executions. Our experimental results show that timing profiles exhibit circuit-dependent patterns that can be effectively classified using pattern recognition techniques, enabling the adversary to infer circuit identities and compromise user confidentiality. We were able to achieve 88% to 99.9% identification rate of quantum circuits based on different datasets. This work highlights previously unexplored security risks in quantum simulation environments and calls for stronger isolation mechanisms to protect user workloads</li>
</ul>

<h3>Title: Cross-Modal Deep Metric Learning for Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Wei Li, Zheze Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12540">https://arxiv.org/abs/2509.12540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12540">https://arxiv.org/pdf/2509.12540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12540]] Cross-Modal Deep Metric Learning for Time Series Anomaly Detection(https://arxiv.org/abs/2509.12540)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>To effectively address the issues of low sensitivity and high time consumption in time series anomaly detection, we propose an anomaly detection method based on cross-modal deep metric learning. A cross-modal deep metric learning feature clustering model is constructed, composed of an input layer, a triplet selection layer, and a loss function computation layer. The squared Euclidean distances between cluster centers are calculated, and a stochastic gradient descent strategy is employed to optimize the model and classify different time series features. The inner product of principal component direction vectors is used as a metric for anomaly measurement. The von Mises-Fisher (vMF) distribution is applied to describe the directional characteristics of time series data, and historical data is used to train and obtain evaluation parameters. By comparing the principal component direction vector of actual time series data with the threshold, anomaly detection is performed. Experimental results demonstrate that the proposed method accurately classifies time series data with different attributes, exhibits high sensitivity to anomalies, and achieves high detection accuracy, fast detection speed, and strong robustness.</li>
</ul>

<h3>Title: Neural Collapse-Inspired Multi-Label Federated Learning under Label-Distribution Skew</h3>
<ul>
<li><strong>Authors: </strong>Can Peng, Yuyuan Liu, Yingyu Yang, Pramit Saha, Qianye Yang, J. Alison Noble</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12544">https://arxiv.org/abs/2509.12544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12544">https://arxiv.org/pdf/2509.12544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12544]] Neural Collapse-Inspired Multi-Label Federated Learning under Label-Distribution Skew(https://arxiv.org/abs/2509.12544)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy. However, the performance of deep learning often deteriorates in FL due to decentralized and heterogeneous data. This challenge is further amplified in multi-label scenarios, where data exhibit complex characteristics such as label co-occurrence, inter-label dependency, and discrepancies between local and global label relationships. While most existing FL research primarily focuses on single-label classification, many real-world applications, particularly in domains such as medical imaging, often involve multi-label settings. In this paper, we address this important yet underexplored scenario in FL, where clients hold multi-label data with skewed label distributions. Neural Collapse (NC) describes a geometric structure in the latent feature space where features of each class collapse to their class mean with vanishing intra-class variance, and the class means form a maximally separated configuration. Motivated by this theory, we propose a method to align feature distributions across clients and to learn high-quality, well-clustered representations. To make the NC-structure applicable to multi-label settings, where image-level features may contain multiple semantic concepts, we introduce a feature disentanglement module that extracts semantically specific features. The clustering of these disentangled class-wise features is guided by a predefined shared NC structure, which mitigates potential conflicts between client models due to diverse local data distributions. In addition, we design regularisation losses to encourage compact clustering in the latent feature space. Experiments conducted on four benchmark datasets across eight diverse settings demonstrate that our approach outperforms existing methods, validating its effectiveness in this challenging FL scenario.</li>
</ul>

<h3>Title: iCD: A Implicit Clustering Distillation Mathod for Structural Information Mining</h3>
<ul>
<li><strong>Authors: </strong>Xiang Xue, Yatu Ji, Qing-dao-er-ji Ren, Bao Shi, Min Lu, Nier Wu, Xufei Zhuang, Haiteng Xu, Gan-qi-qi-ge Cha</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12553">https://arxiv.org/abs/2509.12553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12553">https://arxiv.org/pdf/2509.12553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12553]] iCD: A Implicit Clustering Distillation Mathod for Structural Information Mining(https://arxiv.org/abs/2509.12553)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Logit Knowledge Distillation has gained substantial research interest in recent years due to its simplicity and lack of requirement for intermediate feature alignment; however, it suffers from limited interpretability in its decision-making process. To address this, we propose implicit Clustering Distillation (iCD): a simple and effective method that mines and transfers interpretable structural knowledge from logits, without requiring ground-truth labels or feature-space alignment. iCD leverages Gram matrices over decoupled local logit representations to enable student models to learn latent semantic structural patterns. Extensive experiments on benchmark datasets demonstrate the effectiveness of iCD across diverse teacher-student architectures, with particularly strong performance in fine-grained classification tasks -- achieving a peak improvement of +5.08% over the baseline. The code is available at: this https URL.</li>
</ul>

<h3>Title: Explicit Multimodal Graph Modeling for Human-Object Interaction Detection</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Ji, Haichao Shi, Xiao-Yu zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12554">https://arxiv.org/abs/2509.12554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12554">https://arxiv.org/pdf/2509.12554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12554]] Explicit Multimodal Graph Modeling for Human-Object Interaction Detection(https://arxiv.org/abs/2509.12554)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based methods have recently become the prevailing approach for Human-Object Interaction (HOI) detection. However, the Transformer architecture does not explicitly model the relational structures inherent in HOI detection, which impedes the recognition of interactions. In contrast, Graph Neural Networks (GNNs) are inherently better suited for this task, as they explicitly model the relationships between human-object pairs. Therefore, in this paper, we propose \textbf{M}ultimodal \textbf{G}raph \textbf{N}etwork \textbf{M}odeling (MGNM) that leverages GNN-based relational structures to enhance HOI detection. Specifically, we design a multimodal graph network framework that explicitly models the HOI task in a four-stage graph structure. Furthermore, we introduce a multi-level feature interaction mechanism within our graph network. This mechanism leverages multi-level vision and language features to enhance information propagation across human-object pairs. Consequently, our proposed MGNM achieves state-of-the-art performance on two widely used benchmarks: HICO-DET and V-COCO. Moreover, when integrated with a more advanced object detector, our method demonstrates a significant performance gain and maintains an effective balance between rare and non-rare classes.</li>
</ul>

<h3>Title: VQT-Light:Lightweight HDR Illumination Map Prediction with Richer Texture.pdf</h3>
<ul>
<li><strong>Authors: </strong>Kunliang Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12556">https://arxiv.org/abs/2509.12556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12556">https://arxiv.org/pdf/2509.12556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12556]] VQT-Light:Lightweight HDR Illumination Map Prediction with Richer Texture.pdf(https://arxiv.org/abs/2509.12556)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Accurate lighting estimation is a significant yet challenging task in computer vision and graphics. However, existing methods either struggle to restore detailed textures of illumination map, or face challenges in run-ning speed and texture fidelity. To tackle this problem, we propose a novel framework (VQT-Light) based on VQVAE and ViT architecture. VQT-Light includes two modules: feature extraction and lighting estima-tion. First, we take advantages of VQVAE to extract discrete features of illumination map rather than con-tinuous features to avoid "posterior collapse". Second, we capture global context and dependencies of in-put image through ViT rather than CNNs to improve the prediction of illumination outside the field of view. Combining the above two modules, we formulate the lighting estimation as a multiclass classification task, which plays a key role in our pipeline. As a result, our model predicts light map with richer texture and better fidelity while keeping lightweight and fast. VQT-Light achieves an inference speed of 40FPS and im-proves multiple evaluation metrics. Qualitative and quantitative experiments demonstrate that the proposed method realizes superior results compared to existing state-of-the-art methods.</li>
</ul>

<h3>Title: Adaptive Sampling Scheduler</h3>
<ul>
<li><strong>Authors: </strong>Qi Wang, Shuliang Zhu, Jinjia Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12569">https://arxiv.org/abs/2509.12569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12569">https://arxiv.org/pdf/2509.12569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12569]] Adaptive Sampling Scheduler(https://arxiv.org/abs/2509.12569)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Consistent distillation methods have evolved into effective techniques that significantly accelerate the sampling process of diffusion models. Although existing methods have achieved remarkable results, the selection of target timesteps during distillation mainly relies on deterministic or stochastic strategies, which often require sampling schedulers to be designed specifically for different distillation processes. Moreover, this pattern severely limits flexibility, thereby restricting the full sampling potential of diffusion models in practical applications. To overcome these limitations, this paper proposes an adaptive sampling scheduler that is applicable to various consistency distillation frameworks. The scheduler introduces three innovative strategies: (i) dynamic target timestep selection, which adapts to different consistency distillation frameworks by selecting timesteps based on their computed importance; (ii) Optimized alternating sampling along the solution trajectory by guiding forward denoising and backward noise addition based on the proposed time step importance, enabling more effective exploration of the solution space to enhance generation performance; and (iii) Utilization of smoothing clipping and color balancing techniques to achieve stable and high-quality generation results at high guidance scales, thereby expanding the applicability of consistency distillation models in complex generation scenarios. We validated the effectiveness and flexibility of the adaptive sampling scheduler across various consistency distillation methods through comprehensive experimental evaluations. Experimental results consistently demonstrated significant improvements in generative performance, highlighting the strong adaptability achieved by our method.</li>
</ul>

<h3>Title: No Need for "Learning" to Defer? A Training Free Deferral Framework to Multiple Experts through Conformal Prediction</h3>
<ul>
<li><strong>Authors: </strong>Tim Bary, Beno√Æt Macq, Louis Petit</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12573">https://arxiv.org/abs/2509.12573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12573">https://arxiv.org/pdf/2509.12573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12573]] No Need for "Learning" to Defer? A Training Free Deferral Framework to Multiple Experts through Conformal Prediction(https://arxiv.org/abs/2509.12573)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>AI systems often fail to deliver reliable predictions across all inputs, prompting the need for hybrid human-AI decision-making. Existing Learning to Defer (L2D) approaches address this by training deferral models, but these are sensitive to changes in expert composition and require significant retraining if experts change. We propose a training-free, model- and expert-agnostic framework for expert deferral based on conformal prediction. Our method uses the prediction set generated by a conformal predictor to identify label-specific uncertainty and selects the most discriminative expert using a segregativity criterion, measuring how well an expert distinguishes between the remaining plausible labels. Experiments on CIFAR10-H and ImageNet16-H show that our method consistently outperforms both the standalone model and the strongest expert, with accuracies attaining $99.57\pm0.10\%$ and $99.40\pm0.52\%$, while reducing expert workload by up to a factor of $11$. The method remains robust under degraded expert performance and shows a gradual performance drop in low-information settings. These results suggest a scalable, retraining-free alternative to L2D for real-world human-AI collaboration.</li>
</ul>

<h3>Title: Yet Another Watermark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Bao, Ying Shi, Zhiguang Yang, Hanzhou Wu, Xinpeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12574">https://arxiv.org/abs/2509.12574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12574">https://arxiv.org/pdf/2509.12574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12574]] Yet Another Watermark for Large Language Models(https://arxiv.org/abs/2509.12574)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Existing watermarking methods for large language models (LLMs) mainly embed watermark by adjusting the token sampling prediction or post-processing, lacking intrinsic coupling with LLMs, which may significantly reduce the semantic quality of the generated marked texts. Traditional watermarking methods based on training or fine-tuning may be extendable to LLMs. However, most of them are limited to the white-box scenario, or very time-consuming due to the massive parameters of LLMs. In this paper, we present a new watermarking framework for LLMs, where the watermark is embedded into the LLM by manipulating the internal parameters of the LLM, and can be extracted from the generated text without accessing the LLM. Comparing with related methods, the proposed method entangles the watermark with the intrinsic parameters of the LLM, which better balances the robustness and imperceptibility of the watermark. Moreover, the proposed method enables us to extract the watermark under the black-box scenario, which is computationally efficient for use. Experimental results have also verified the feasibility, superiority and practicality. This work provides a new perspective different from mainstream works, which may shed light on future research.</li>
</ul>

<h3>Title: Secure and Efficient Out-of-band Call Metadata Transmission</h3>
<ul>
<li><strong>Authors: </strong>David Adei, Varun Madathil, Nithin Shyam S., Bradley Reaves</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12582">https://arxiv.org/abs/2509.12582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12582">https://arxiv.org/pdf/2509.12582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12582]] Secure and Efficient Out-of-band Call Metadata Transmission(https://arxiv.org/abs/2509.12582)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect</a></li>
<li><strong>Abstract: </strong>The STIR/SHAKEN (S/S) attestation Framework mandated by the United States, Canada, and France to combat pervasive telephone abuse has not achieved its goals, partly because legacy non-VoIP infrastructure could not participate. The industry solution to extend S/S broadcasts sensitive metadata of every non-VoIP call in plaintext to every third party required to facilitate the system. It has no mechanism to determine whether a provider's request for call data is appropriate, nor can it ensure that every copy of that call data is unavailable after its specified expiration. It threatens subscriber privacy and provider confidentiality. In this paper, we present Sidecar, a distributed, privacy-preserving system with tunable decentralization that securely extends S/S across all telephone network technologies. We introduce the notion of secure out-of-band signaling for telephony and formalize its system and security requirements. We then design novel, scalable protocols that realize these requirements and prove their security within the Universal Composability framework. Finally, we demonstrate Sidecar's efficiency with our open-sourced reference implementation. Compared to the current solution, Sidecar 1) protects the confidentiality of subscriber identity and provider trade secrets, 2) guarantees record expiration as long as a single node handling a record is honest, 3) reduces resource requirements while providing virtually identical call-setup times and equivalent or better uptimes, and 4) enables secure pay-per-use billing and integrates mechanisms to mitigate and detect misbehavior. Moreover, Sidecar can be extended to provide the same security guarantees for arbitrary call metadata. Not only is Sidecar a superior approach, it is also a transformative tool to retrofit fragmented global telephony and enable future improvements, such as stronger call authentication and Branded Calling.</li>
</ul>

<h3>Title: MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models</h3>
<ul>
<li><strong>Authors: </strong>Vijay Govindarajan, Pratik Patel, Sahil Tripathi, Md Azizul Hoque, Gautam Siddharth Kashyap</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12591">https://arxiv.org/abs/2509.12591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12591">https://arxiv.org/pdf/2509.12591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12591]] MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models(https://arxiv.org/abs/2509.12591)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automated Audio Captioning (AAC) generates captions for audio clips but faces challenges due to limited datasets compared to image captioning. To overcome this, we propose the zero-shot AAC system that leverages pre-trained models, eliminating the need for extensive training. Our approach uses a pre-trained audio CLIP model to extract auditory features and generate a structured prompt, which guides a Large Language Model (LLM) in caption generation. Unlike traditional greedy decoding, our method refines token selection through the audio CLIP model, ensuring alignment with the audio content. Experimental results demonstrate a 35% improvement in NLG mean score (from 4.7 to 7.3) using MAGIC search with the WavCaps model. The performance is heavily influenced by the audio-text matching model and keyword selection, with optimal results achieved using a single keyword prompt, and a 50% performance drop when no keyword list is used.</li>
</ul>

<h3>Title: DisorientLiDAR: Physical Attacks on LiDAR-based Localization</h3>
<ul>
<li><strong>Authors: </strong>Yizhen Lao, Yu Zhang, Ziting Wang, Chengbo Wang, Yifei Xue, Wanpeng Shao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12595">https://arxiv.org/abs/2509.12595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12595">https://arxiv.org/pdf/2509.12595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12595]] DisorientLiDAR: Physical Attacks on LiDAR-based Localization(https://arxiv.org/abs/2509.12595)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Deep learning models have been shown to be susceptible to adversarial attacks with visually imperceptible perturbations. Even this poses a serious security challenge for the localization of self-driving cars, there has been very little exploration of attack on it, as most of adversarial attacks have been applied to 3D perception. In this work, we propose a novel adversarial attack framework called DisorientLiDAR targeting LiDAR-based localization. By reverse-engineering localization models (e.g., feature extraction networks), adversaries can identify critical keypoints and strategically remove them, thereby disrupting LiDAR-based localization. Our proposal is first evaluated on three state-of-the-art point-cloud registration models (HRegNet, D3Feat, and GeoTransformer) using the KITTI dataset. Experimental results demonstrate that removing regions containing Top-K keypoints significantly degrades their registration accuracy. We further validate the attack's impact on the Autoware autonomous driving platform, where hiding merely a few critical regions induces noticeable localization drift. Finally, we extended our attacks to the physical world by hiding critical regions with near-infrared absorptive materials, thereby successfully replicate the attack effects observed in KITTI data. This step has been closer toward the realistic physical-world attack that demonstrate the veracity and generality of our proposal.</li>
</ul>

<h3>Title: EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving</h3>
<ul>
<li><strong>Authors: </strong>Mukai Li, Linfeng Song, Zhenwen Liang, Jiahao Xu, Shansan Gong, Qi Liu, Haitao Mi, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12603">https://arxiv.org/abs/2509.12603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12603">https://arxiv.org/pdf/2509.12603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12603]] EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving(https://arxiv.org/abs/2509.12603)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently advanced the field of Automated Theorem Proving (ATP), attaining substantial performance gains through widely adopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT) reasoning and increased sampling passes. However, they both introduce significant computational overhead for inference. Moreover, existing cost analyses typically regulate only the number of sampling passes, while neglecting the substantial disparities in sampling costs introduced by different scaling strategies. In this paper, we systematically compare the efficiency of different test-time scaling strategies for ATP models and demonstrate the inefficiency of the current state-of-the-art (SOTA) open-source approaches. We then investigate approaches to significantly reduce token usage and sample passes while maintaining the original performance. Specifically, we propose two complementary methods that can be integrated into a unified EconRL pipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching mechanism designed to mitigate unnecessary token consumption, and (2) Diverse parallel-scaled reinforcement learning (RL) with trainable prefixes to enhance pass rates under constrained sampling passes. Experiments on miniF2F and ProofNet demonstrate that our EconProver achieves comparable performance to baseline methods with only 12% of the computational cost. This work provides actionable insights for deploying lightweight ATP models without sacrificing performance.</li>
</ul>

<h3>Title: Exploring Spectral Characteristics for Single Image Reflection Removal</h3>
<ul>
<li><strong>Authors: </strong>Pengbo Guo, Chengxu Liu, Guoshuai Zhao, Xingsong Hou, Jialie Shen, Xueming Qian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12627">https://arxiv.org/abs/2509.12627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12627">https://arxiv.org/pdf/2509.12627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12627]] Exploring Spectral Characteristics for Single Image Reflection Removal(https://arxiv.org/abs/2509.12627)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Eliminating reflections caused by incident light interacting with reflective medium remains an ill-posed problem in the image restoration area. The primary challenge arises from the overlapping of reflection and transmission components in the captured images, which complicates the task of accurately distinguishing and recovering the clean background. Existing approaches typically address reflection removal solely in the image domain, ignoring the spectral property variations of reflected light, which hinders their ability to effectively discern reflections. In this paper, we start with a new perspective on spectral learning, and propose the Spectral Codebook to reconstruct the optical spectrum of the reflection image. The reflections can be effectively distinguished by perceiving the wavelength differences between different light sources in the spectrum. To leverage the reconstructed spectrum, we design two spectral prior refinement modules to re-distribute pixels in the spatial dimension and adaptively enhance the spectral differences along the wavelength dimension. Furthermore, we present the Spectrum-Aware Transformer to jointly recover the transmitted content in spectral and pixel domains. Experimental results on three different reflection benchmarks demonstrate the superiority and generalization ability of our method compared to state-of-the-art models.</li>
</ul>

<h3>Title: High-Energy Concentration for Federated Learning in Frequency Domain</h3>
<ul>
<li><strong>Authors: </strong>Haozhi Shi, Weiying Xie, Hangyu Ye, Daixun Li, Jitao Ma, Leyuan Fang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12630">https://arxiv.org/abs/2509.12630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12630">https://arxiv.org/pdf/2509.12630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12630]] High-Energy Concentration for Federated Learning in Frequency Domain(https://arxiv.org/abs/2509.12630)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) presents significant potential for collaborative optimization without data sharing. Since synthetic data is sent to the server, leveraging the popular concept of dataset distillation, this FL framework protects real data privacy while alleviating data heterogeneity. However, such methods are still challenged by the redundant information and noise in entire spatial-domain designs, which inevitably increases the communication burden. In this paper, we propose a novel Frequency-Domain aware FL method with high-energy concentration (FedFD) to address this problem. Our FedFD is inspired by the discovery that the discrete cosine transform predominantly distributes energy to specific regions, referred to as high-energy concentration. The principle behind FedFD is that low-energy like high-frequency components usually contain redundant information and noise, thus filtering them helps reduce communication costs and optimize performance. Our FedFD is mathematically formulated to preserve the low-frequency components using a binary mask, facilitating an optimal solution through frequency-domain distribution alignment. In particular, real data-driven synthetic classification is imposed into the loss to enhance the quality of the low-frequency components. On five image and speech datasets, FedFD achieves superior performance than state-of-the-art methods while reducing communication costs. For example, on the CIFAR-10 dataset with Dirichlet coefficient $\alpha = 0.01$, FedFD achieves a minimum reduction of 37.78\% in the communication cost, while attaining a 10.88\% performance gain.</li>
</ul>

<h3>Title: CIARD: Cyclic Iterative Adversarial Robustness Distillation</h3>
<ul>
<li><strong>Authors: </strong>Liming Lu, Shuchao Pang, Xu Zheng, Xiang Gu, Anan Du, Yunhuai Liu, Yongbin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12633">https://arxiv.org/abs/2509.12633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12633">https://arxiv.org/pdf/2509.12633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12633]] CIARD: Cyclic Iterative Adversarial Robustness Distillation(https://arxiv.org/abs/2509.12633)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Adversarial robustness distillation (ARD) aims to transfer both performance and robustness from teacher model to lightweight student model, enabling resilient performance on resource-constrained scenarios. Though existing ARD approaches enhance student model's robustness, the inevitable by-product leads to the degraded performance on clean examples. We summarize the causes of this problem inherent in existing methods with dual-teacher framework as: 1. The divergent optimization objectives of dual-teacher models, i.e., the clean and robust teachers, impede effective knowledge transfer to the student model, and 2. The iteratively generated adversarial examples during training lead to performance deterioration of the robust teacher model. To address these challenges, we propose a novel Cyclic Iterative ARD (CIARD) method with two key innovations: a. A multi-teacher framework with contrastive push-loss alignment to resolve conflicts in dual-teacher optimization objectives, and b. Continuous adversarial retraining to maintain dynamic teacher robustness against performance degradation from the varying adversarial examples. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CIARD achieves remarkable performance with an average 3.53 improvement in adversarial defense rates across various attack scenarios and a 5.87 increase in clean sample accuracy, establishing a new benchmark for balancing model robustness and generalization. Our code is available at this https URL</li>
</ul>

<h3>Title: PAC: Pronunciation-Aware Contextualized Large Language Model-based Automatic Speech Recognition</h3>
<ul>
<li><strong>Authors: </strong>Li Fu, Yu Xin, Sunlu Zeng, Lu Fan, Youzheng Wu, Xiaodong He</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12647">https://arxiv.org/abs/2509.12647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12647">https://arxiv.org/pdf/2509.12647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12647]] PAC: Pronunciation-Aware Contextualized Large Language Model-based Automatic Speech Recognition(https://arxiv.org/abs/2509.12647)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a Pronunciation-Aware Contextualized (PAC) framework to address two key challenges in Large Language Model (LLM)-based Automatic Speech Recognition (ASR) systems: effective pronunciation modeling and robust homophone discrimination. Both are essential for raw or long-tail word recognition. The proposed approach adopts a two-stage learning paradigm. First, we introduce a pronunciation-guided context learning method. It employs an interleaved grapheme-phoneme context modeling strategy that incorporates grapheme-only distractors, encouraging the model to leverage phonemic cues for accurate recognition. Then, we propose a pronunciation-discriminative reinforcement learning method with perturbed label sampling to further enhance the model≈õ ability to distinguish contextualized homophones. Experimental results on the public English Librispeech and Mandarin AISHELL-1 datasets indicate that PAC: (1) reduces relative Word Error Rate (WER) by 30.2% and 53.8% compared to pre-trained LLM-based ASR models, and (2) achieves 31.8% and 60.5% relative reductions in biased WER for long-tail words compared to strong baselines, respectively.</li>
</ul>

<h3>Title: A Systematic Evaluation of Parameter-Efficient Fine-Tuning Methods for the Security of Code LLMs</h3>
<ul>
<li><strong>Authors: </strong>Kiho Lee, Jungkon Kim, Doowon Kim, Hyoungshick Kim</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12649">https://arxiv.org/abs/2509.12649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12649">https://arxiv.org/pdf/2509.12649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12649]] A Systematic Evaluation of Parameter-Efficient Fine-Tuning Methods for the Security of Code LLMs(https://arxiv.org/abs/2509.12649)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Code-generating Large Language Models (LLMs) significantly accelerate software development. However, their frequent generation of insecure code presents serious risks. We present a comprehensive evaluation of seven parameter-efficient fine-tuning (PEFT) techniques, demonstrating substantial gains in secure code generation without compromising functionality. Our research identifies prompt-tuning as the most effective PEFT method, achieving an 80.86% Overall-Secure-Rate on CodeGen2 16B, a 13.5-point improvement over the 67.28% baseline. Optimizing decoding strategies through sampling temperature further elevated security to 87.65%. This equates to a reduction of approximately 203,700 vulnerable code snippets per million generated. Moreover, prompt and prefix tuning increase robustness against poisoning attacks in our TrojanPuzzle evaluation, with strong performance against CWE-79 and CWE-502 attack vectors. Our findings generalize across Python and Java, confirming prompt-tuning's consistent effectiveness. This study provides essential insights and practical guidance for building more resilient software systems with LLMs.</li>
</ul>

<h3>Title: Don't Change My View: Ideological Bias Auditing in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Paul Kr√∂ger, Emilio Barkett</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12652">https://arxiv.org/abs/2509.12652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12652">https://arxiv.org/pdf/2509.12652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12652]] Don't Change My View: Ideological Bias Auditing in Large Language Models(https://arxiv.org/abs/2509.12652)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly embedded in products used by millions, their outputs may influence individual beliefs and, cumulatively, shape public opinion. If the behavior of LLMs can be intentionally steered toward specific ideological positions, such as political or religious views, then those who control these systems could gain disproportionate influence over public discourse. Although it remains an open question whether LLMs can reliably be guided toward coherent ideological stances and whether such steering can be effectively prevented, a crucial first step is to develop methods for detecting when such steering attempts occur. In this work, we adapt a previously proposed statistical method to the new context of ideological bias auditing. Our approach carries over the model-agnostic design of the original framework, which does not require access to the internals of the language model. Instead, it identifies potential ideological steering by analyzing distributional shifts in model outputs across prompts that are thematically related to a chosen topic. This design makes the method particularly suitable for auditing proprietary black-box systems. We validate our approach through a series of experiments, demonstrating its practical applicability and its potential to support independent post hoc audits of LLM behavior.</li>
</ul>

<h3>Title: Beyond Artificial Misalignment: Detecting and Grounding Semantic-Coordinated Multimodal Manipulations</h3>
<ul>
<li><strong>Authors: </strong>Jinjie Shen, Yaxiong Wang, Lechao Cheng, Nan Pu, Zhun Zhong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12653">https://arxiv.org/abs/2509.12653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12653">https://arxiv.org/pdf/2509.12653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12653]] Beyond Artificial Misalignment: Detecting and Grounding Semantic-Coordinated Multimodal Manipulations(https://arxiv.org/abs/2509.12653)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>The detection and grounding of manipulated content in multimodal data has emerged as a critical challenge in media forensics. While existing benchmarks demonstrate technical progress, they suffer from misalignment artifacts that poorly reflect real-world manipulation patterns: practical attacks typically maintain semantic consistency across modalities, whereas current datasets artificially disrupt cross-modal alignment, creating easily detectable anomalies. To bridge this gap, we pioneer the detection of semantically-coordinated manipulations where visual edits are systematically paired with semantically consistent textual descriptions. Our approach begins with constructing the first Semantic-Aligned Multimodal Manipulation (SAMM) dataset, generated through a two-stage pipeline: 1) applying state-of-the-art image manipulations, followed by 2) generation of contextually-plausible textual narratives that reinforce the visual deception. Building on this foundation, we propose a Retrieval-Augmented Manipulation Detection and Grounding (RamDG) framework. RamDG commences by harnessing external knowledge repositories to retrieve contextual evidence, which serves as the auxiliary texts and encoded together with the inputs through our image forgery grounding and deep manipulation detection modules to trace all manipulations. Extensive experiments demonstrate our framework significantly outperforms existing methods, achieving 2.06\% higher detection accuracy on SAMM compared to state-of-the-art approaches. The dataset and code are publicly available at this https URL.</li>
</ul>

<h3>Title: Mitigating Strategy Preference Bias in Emotional Support Conversation via Uncertainty Estimations</h3>
<ul>
<li><strong>Authors: </strong>Yougen Zhou, Qin Chen, Ningning Zhou, Jie Zhou, Xingjiao Wu, Liang He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12661">https://arxiv.org/abs/2509.12661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12661">https://arxiv.org/pdf/2509.12661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12661]] Mitigating Strategy Preference Bias in Emotional Support Conversation via Uncertainty Estimations(https://arxiv.org/abs/2509.12661)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Emotional support conversation (ESC) aims to alleviate distress through empathetic dialogue, yet large language models (LLMs) face persistent challenges in delivering effective ESC due to low accuracy in strategy planning. Moreover, there is a considerable preference bias towards specific strategies. Prior methods using fine-tuned strategy planners have shown potential in reducing such bias, while the underlying causes of the preference bias in LLMs have not well been studied. To address these issues, we first reveal the fundamental causes of the bias by identifying the knowledge boundaries of LLMs in strategy planning. Then, we propose an approach to mitigate the bias by reinforcement learning with a dual reward function, which optimizes strategy planning via both accuracy and entropy-based confidence for each region according to the knowledge boundaries. Experiments on the ESCov and ExTES datasets with multiple LLM backbones show that our approach outperforms the baselines, confirming the effectiveness of our approach.</li>
</ul>

<h3>Title: Chat-Driven Text Generation and Interaction for Person Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Zequn Xie, Chuxin Wang, Sihang Cai, Yeqiang Wang, Shulei Wang, Tao Jin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12662">https://arxiv.org/abs/2509.12662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12662">https://arxiv.org/pdf/2509.12662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12662]] Chat-Driven Text Generation and Interaction for Person Retrieval(https://arxiv.org/abs/2509.12662)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Text-based person search (TBPS) enables the retrieval of person images from large-scale databases using natural language descriptions, offering critical value in surveillance applications. However, a major challenge lies in the labor-intensive process of obtaining high-quality textual annotations, which limits scalability and practical deployment. To address this, we introduce two complementary modules: Multi-Turn Text Generation (MTG) and Multi-Turn Text Interaction (MTI). MTG generates rich pseudo-labels through simulated dialogues with MLLMs, producing fine-grained and diverse visual descriptions without manual supervision. MTI refines user queries at inference time through dynamic, dialogue-based reasoning, enabling the system to interpret and resolve vague, incomplete, or ambiguous descriptions - characteristics often seen in real-world search scenarios. Together, MTG and MTI form a unified and annotation-free framework that significantly improves retrieval accuracy, robustness, and usability. Extensive evaluations demonstrate that our method achieves competitive or superior results while eliminating the need for manual captions, paving the way for scalable and practical deployment of TBPS systems.</li>
</ul>

<h3>Title: Towards Inclusive Toxic Content Moderation: Addressing Vulnerabilities to Adversarial Attacks in Toxicity Classifiers Tackling LLM-generated Content</h3>
<ul>
<li><strong>Authors: </strong>Shaz Furniturewala, Arkaitz Zubiaga</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12672">https://arxiv.org/abs/2509.12672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12672">https://arxiv.org/pdf/2509.12672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12672]] Towards Inclusive Toxic Content Moderation: Addressing Vulnerabilities to Adversarial Attacks in Toxicity Classifiers Tackling LLM-generated Content(https://arxiv.org/abs/2509.12672)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, fair, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The volume of machine-generated content online has grown dramatically due to the widespread use of Large Language Models (LLMs), leading to new challenges for content moderation systems. Conventional content moderation classifiers, which are usually trained on text produced by humans, suffer from misclassifications due to LLM-generated text deviating from their training data and adversarial attacks that aim to avoid detection. Present-day defence tactics are reactive rather than proactive, since they rely on adversarial training or external detection models to identify attacks. In this work, we aim to identify the vulnerable components of toxicity classifiers that contribute to misclassification, proposing a novel strategy based on mechanistic interpretability techniques. Our study focuses on fine-tuned BERT and RoBERTa classifiers, testing on diverse datasets spanning a variety of minority groups. We use adversarial attacking techniques to identify vulnerable circuits. Finally, we suppress these vulnerable circuits, improving performance against adversarial attacks. We also provide demographic-level insights into these vulnerable circuits, exposing fairness and robustness gaps in model training. We find that models have distinct heads that are either crucial for performance or vulnerable to attack and suppressing the vulnerable heads improves performance on adversarial input. We also find that different heads are responsible for vulnerability across different demographic groups, which can inform more inclusive development of toxicity detection models.</li>
</ul>

<h3>Title: MFAF: An EVA02-Based Multi-scale Frequency Attention Fusion Method for Cross-View Geo-Localization</h3>
<ul>
<li><strong>Authors: </strong>YiTong Liu, TianZhu Liu, YanFeng GU</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12673">https://arxiv.org/abs/2509.12673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12673">https://arxiv.org/pdf/2509.12673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12673]] MFAF: An EVA02-Based Multi-scale Frequency Attention Fusion Method for Cross-View Geo-Localization(https://arxiv.org/abs/2509.12673)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Cross-view geo-localization aims to determine the geographical location of a query image by matching it against a gallery of images. This task is challenging due to the significant appearance variations of objects observed from variable views, along with the difficulty in extracting discriminative features. Existing approaches often rely on extracting features through feature map segmentation while neglecting spatial and semantic information. To address these issues, we propose the EVA02-based Multi-scale Frequency Attention Fusion (MFAF) method. The MFAF method consists of Multi-Frequency Branch-wise Block (MFB) and the Frequency-aware Spatial Attention (FSA) module. The MFB block effectively captures both low-frequency structural features and high-frequency edge details across multiple scales, improving the consistency and robustness of feature representations across various viewpoints. Meanwhile, the FSA module adaptively focuses on the key regions of frequency features, significantly mitigating the interference caused by background noise and viewpoint variability. Extensive experiments on widely recognized benchmarks, including University-1652, SUES-200, and Dense-UAV, demonstrate that the MFAF method achieves competitive performance in both drone localization and drone navigation tasks.</li>
</ul>

<h3>Title: Case-Based Decision-Theoretic Decoding with Quality Memories</h3>
<ul>
<li><strong>Authors: </strong>Hiroyuki Deguchi, Masaaki Nagata</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12677">https://arxiv.org/abs/2509.12677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12677">https://arxiv.org/pdf/2509.12677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12677]] Case-Based Decision-Theoretic Decoding with Quality Memories(https://arxiv.org/abs/2509.12677)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Minimum Bayes risk (MBR) decoding is a decision rule of text generation, which selects the hypothesis that maximizes the expected utility and robustly generates higher-quality texts than maximum a posteriori (MAP) decoding. However, it depends on sample texts drawn from the text generation model; thus, it is difficult to find a hypothesis that correctly captures the knowledge or information of out-of-domain. To tackle this issue, we propose case-based decision-theoretic (CBDT) decoding, another method to estimate the expected utility using examples of domain data. CBDT decoding not only generates higher-quality texts than MAP decoding, but also the combination of MBR and CBDT decoding outperformed MBR decoding in seven domain De--En and Ja$\leftrightarrow$En translation tasks and image captioning tasks on MSCOCO and nocaps datasets.</li>
</ul>

<h3>Title: Instance-level Randomization: Toward More Stable LLM Evaluations</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Li, Yonghuang Wu, Ying Luo, Liangtai Sun, Zishu Qin, Lin Qiu, Xuezhi Cao, Xunliang Cai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12678">https://arxiv.org/abs/2509.12678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12678">https://arxiv.org/pdf/2509.12678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12678]] Instance-level Randomization: Toward More Stable LLM Evaluations(https://arxiv.org/abs/2509.12678)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Evaluations of large language models (LLMs) suffer from instability, where small changes of random factors such as few-shot examples can lead to drastic fluctuations of scores and even model rankings. Moreover, different LLMs can have different preferences for a certain setting of random factors. As a result, using a fixed setting of random factors, which is often adopted as the paradigm of current evaluations, can lead to potential unfair comparisons between LLMs. To mitigate the volatility of evaluations, we first theoretically analyze the sources of variance induced by changes in random factors. Targeting these specific sources, we then propose the instance-level randomization (ILR) method to reduce variance and enhance fairness in model comparisons. Instead of using a fixed setting across the whole benchmark in a single experiment, we randomize all factors that affect evaluation scores for every single instance, run multiple experiments and report the averaged score. Theoretical analyses and empirical results demonstrate that ILR can reduce the variance and unfair comparisons caused by random factors, as well as achieve similar robustness level with less than half computational cost compared with previous methods.</li>
</ul>

<h3>Title: Large Language Model Scaling Laws for Neural Quantum States in Quantum Chemistry</h3>
<ul>
<li><strong>Authors: </strong>Oliver Knitter, Dan Zhao, Stefan Leichenauer, Shravan Veerapaneni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12679">https://arxiv.org/abs/2509.12679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12679">https://arxiv.org/pdf/2509.12679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12679]] Large Language Model Scaling Laws for Neural Quantum States in Quantum Chemistry(https://arxiv.org/abs/2509.12679)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Scaling laws have been used to describe how large language model (LLM) performance scales with model size, training data size, or amount of computational resources. Motivated by the fact that neural quantum states (NQS) has increasingly adopted LLM-based components, we seek to understand NQS scaling laws, thereby shedding light on the scalability and optimal performance--resource trade-offs of NQS ansatze. In particular, we identify scaling laws that predict the performance, as measured by absolute error and V-score, for transformer-based NQS as a function of problem size in second-quantized quantum chemistry applications. By performing analogous compute-constrained optimization of the obtained parametric curves, we find that the relationship between model size and training time is highly dependent on loss metric and ansatz, and does not follow the approximately linear relationship found for language models.</li>
</ul>

<h3>Title: StereoCarla: A High-Fidelity Driving Dataset for Generalizable Stereo</h3>
<ul>
<li><strong>Authors: </strong>Xianda Guo, Chenming Zhang, Ruilin Wang, Youmin Zhang, Wenzhao Zheng, Matteo Poggi, Hao Zhao, Qin Zou, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12683">https://arxiv.org/abs/2509.12683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12683">https://arxiv.org/pdf/2509.12683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12683]] StereoCarla: A High-Fidelity Driving Dataset for Generalizable Stereo(https://arxiv.org/abs/2509.12683)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Stereo matching plays a crucial role in enabling depth perception for autonomous driving and robotics. While recent years have witnessed remarkable progress in stereo matching algorithms, largely driven by learning-based methods and synthetic datasets, the generalization performance of these models remains constrained by the limited diversity of existing training data. To address these challenges, we present StereoCarla, a high-fidelity synthetic stereo dataset specifically designed for autonomous driving scenarios. Built on the CARLA simulator, StereoCarla incorporates a wide range of camera configurations, including diverse baselines, viewpoints, and sensor placements as well as varied environmental conditions such as lighting changes, weather effects, and road geometries. We conduct comprehensive cross-domain experiments across four standard evaluation datasets (KITTI2012, KITTI2015, Middlebury, ETH3D) and demonstrate that models trained on StereoCarla outperform those trained on 11 existing stereo datasets in terms of generalization accuracy across multiple benchmarks. Furthermore, when integrated into multi-dataset training, StereoCarla contributes substantial improvements to generalization accuracy, highlighting its compatibility and scalability. This dataset provides a valuable benchmark for developing and evaluating stereo algorithms under realistic, diverse, and controllable settings, facilitating more robust depth perception systems for autonomous vehicles. Code can be available at this https URL, and data can be available at this https URL.</li>
</ul>

<h3>Title: ZTree: A Subgroup Identification Based Decision Tree Learning Framework</h3>
<ul>
<li><strong>Authors: </strong>Eric Cheng, Jie Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12688">https://arxiv.org/abs/2509.12688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12688">https://arxiv.org/pdf/2509.12688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12688]] ZTree: A Subgroup Identification Based Decision Tree Learning Framework(https://arxiv.org/abs/2509.12688)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Decision trees are a commonly used class of machine learning models valued for their interpretability and versatility, capable of both classification and regression. We propose ZTree, a novel decision tree learning framework that replaces CART's traditional purity based splitting with statistically principled subgroup identification. At each node, ZTree applies hypothesis testing (e.g., z-tests, t-tests, Mann-Whitney U, log-rank) to assess whether a candidate subgroup differs meaningfully from the complement. To adjust for the complication of multiple testing, we employ a cross-validation-based approach to determine if further node splitting is needed. This robust stopping criterion eliminates the need for post-pruning and makes the test threshold (z-threshold) the only parameter for controlling tree complexity. Because of the simplicity of the tree growing procedure, once a detailed tree is learned using the most lenient z-threshold, all simpler trees can be derived by simply removing nodes that do not meet the larger z-thresholds. This makes parameter tuning intuitive and efficient. Furthermore, this z-threshold is essentially a p-value, allowing users to easily plug in appropriate statistical tests into our framework without adjusting the range of parameter search. Empirical evaluation on five large-scale UCI datasets demonstrates that ZTree consistently delivers strong performance, especially at low data regimes. Compared to CART, ZTree also tends to grow simpler trees without sacrificing performance. ZTree introduces a statistically grounded alternative to traditional decision tree splitting by leveraging hypothesis testing and a cross-validation approach to multiple testing correction, resulting in an efficient and flexible framework.</li>
</ul>

<h3>Title: Soft Graph Transformer for MIMO Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiadong Hong, Lei Liu, Xinyu Bian, Wenjie Wang, Zhaoyang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12694">https://arxiv.org/abs/2509.12694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12694">https://arxiv.org/pdf/2509.12694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12694]] Soft Graph Transformer for MIMO Detection(https://arxiv.org/abs/2509.12694)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We propose the Soft Graph Transformer (SGT), a Soft-Input-Soft-Output neural architecture tailored for MIMO detection. While Maximum Likelihood (ML) detection achieves optimal accuracy, its prohibitive exponential complexity renders it impractical for real-world systems. Conventional message passing algorithms offer tractable alternatives but rely on large-system asymptotics and random matrix assumptions, both of which break down under practical implementations. Prior Transformer-based detectors, on the other hand, fail to incorporate the MIMO factor graph structure and cannot utilize decoder-side soft information, limiting their standalone performance and their applicability in iterative detection-decoding (IDD). To overcome these limitations, SGT integrates message passing directly into a graph-aware attention mechanism and supports decoder-informed updates through soft-input embeddings. This design enables effective soft-output generation while preserving computational efficiency. As a standalone detector, SGT closely approaches ML performance and surpasses prior Transformer-based approaches.</li>
</ul>

<h3>Title: Bi-level Personalization for Federated Foundation Models: A Task-vector Aggregation Approach</h3>
<ul>
<li><strong>Authors: </strong>Yiyuan Yang, Guodong Long, Qinghua Lu, Liming Zhu, Jing Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12697">https://arxiv.org/abs/2509.12697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12697">https://arxiv.org/pdf/2509.12697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12697]] Bi-level Personalization for Federated Foundation Models: A Task-vector Aggregation Approach(https://arxiv.org/abs/2509.12697)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated foundation models represent a new paradigm to jointly fine-tune pre-trained foundation models across clients. It is still a challenge to fine-tune foundation models for a small group of new users or specialized scenarios, which typically involve limited data compared to the large-scale data used in pre-training. In this context, the trade-off between personalization and federation becomes more sensitive. To tackle these, we proposed a bi-level personalization framework for federated fine-tuning on foundation models. Specifically, we conduct personalized fine-tuning on the client-level using its private data, and then conduct a personalized aggregation on the server-level using similar users measured by client-specific task vectors. Given the personalization information gained from client-level fine-tuning, the server-level personalized aggregation can gain group-wise personalization information while mitigating the disturbance of irrelevant or interest-conflict clients with non-IID data. The effectiveness of the proposed algorithm has been demonstrated by extensive experimental analysis in benchmark datasets.</li>
</ul>

<h3>Title: SmokeBench: A Real-World Dataset for Surveillance Image Desmoking in Early-Stage Fire Scenes</h3>
<ul>
<li><strong>Authors: </strong>Wenzhuo Jin, Qianfeng Yang, Xianhao Wu, Hongming Chen, Pengpeng Li, Xiang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12701">https://arxiv.org/abs/2509.12701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12701">https://arxiv.org/pdf/2509.12701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12701]] SmokeBench: A Real-World Dataset for Surveillance Image Desmoking in Early-Stage Fire Scenes(https://arxiv.org/abs/2509.12701)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Early-stage fire scenes (0-15 minutes after ignition) represent a crucial temporal window for emergency interventions. During this stage, the smoke produced by combustion significantly reduces the visibility of surveillance systems, severely impairing situational awareness and hindering effective emergency response and rescue operations. Consequently, there is an urgent need to remove smoke from images to obtain clear scene information. However, the development of smoke removal algorithms remains limited due to the lack of large-scale, real-world datasets comprising paired smoke-free and smoke-degraded images. To address these limitations, we present a real-world surveillance image desmoking benchmark dataset named SmokeBench, which contains image pairs captured under diverse scenes setup and smoke concentration. The curated dataset provides precisely aligned degraded and clean images, enabling supervised learning and rigorous evaluation. We conduct comprehensive experiments by benchmarking a variety of desmoking methods on our dataset. Our dataset provides a valuable foundation for advancing robust and practical image desmoking in real-world fire scenes. This dataset has been released to the public and can be downloaded from this https URL.</li>
</ul>

<h3>Title: Spatio-temporal DeepKriging in PyTorch: A Supplementary Application to Precipitation Data for Interpolation and Probabilistic Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Pratik Nag</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12708">https://arxiv.org/abs/2509.12708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12708">https://arxiv.org/pdf/2509.12708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12708]] Spatio-temporal DeepKriging in PyTorch: A Supplementary Application to Precipitation Data for Interpolation and Probabilistic Forecasting(https://arxiv.org/abs/2509.12708)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A detailed analysis of precipitation data over Europe is presented, with a focus on interpolation and forecasting applications. A Spatio-temporal DeepKriging (STDK) framework has been implemented using the PyTorch platform to achieve these objectives. The proposed model is capable of handling spatio-temporal irregularities while generating high-resolution interpolations and multi-step forecasts. Reproducible code modules have been developed as standalone PyTorch implementations for the interpolation\footnote[2]{Interpolation - this https URL} and forecasting\footnote[3]{Forecasting - this https URL}, facilitating broader application to similar climate datasets. The effectiveness of this approach is demonstrated through extensive evaluation on daily precipitation measurements, highlighting predictive performance and robustness.</li>
</ul>

<h3>Title: RIS-FUSION: Rethinking Text-Driven Infrared and Visible Image Fusion from the Perspective of Referring Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Siju Ma, Changsiyu Gong, Xiaofeng Fan, Yong Ma, Chengjie Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12710">https://arxiv.org/abs/2509.12710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12710">https://arxiv.org/pdf/2509.12710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12710]] RIS-FUSION: Rethinking Text-Driven Infrared and Visible Image Fusion from the Perspective of Referring Image Segmentation(https://arxiv.org/abs/2509.12710)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Text-driven infrared and visible image fusion has gained attention for enabling natural language to guide the fusion process. However, existing methods lack a goal-aligned task to supervise and evaluate how effectively the input text contributes to the fusion outcome. We observe that referring image segmentation (RIS) and text-driven fusion share a common objective: highlighting the object referred to by the text. Motivated by this, we propose RIS-FUSION, a cascaded framework that unifies fusion and RIS through joint optimization. At its core is the LangGatedFusion module, which injects textual features into the fusion backbone to enhance semantic alignment. To support multimodal referring image segmentation task, we introduce MM-RIS, a large-scale benchmark with 12.5k training and 3.5k testing triplets, each consisting of an infrared-visible image pair, a segmentation mask, and a referring expression. Extensive experiments show that RIS-FUSION achieves state-of-the-art performance, outperforming existing methods by over 11% in mIoU. Code and dataset will be released at this https URL.</li>
</ul>

<h3>Title: HistoryBankQA: Multilingual Temporal Question Answering on Historical Events</h3>
<ul>
<li><strong>Authors: </strong>Biswadip Mandal, Anant Khandelwal, Manish Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12720">https://arxiv.org/abs/2509.12720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12720">https://arxiv.org/pdf/2509.12720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12720]] HistoryBankQA: Multilingual Temporal Question Answering on Historical Events(https://arxiv.org/abs/2509.12720)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Temporal reasoning about historical events is a critical skill for NLP tasks like event extraction, historical entity linking, temporal question answering, timeline summarization, temporal event clustering and temporal natural language inference. Yet efforts on benchmarking temporal reasoning capabilities of large language models (LLMs) are rather limited. Existing temporal reasoning datasets are limited in scale, lack multilingual coverage and focus more on contemporary events. To address these limitations, we present HistoryBank, a multilingual database of 10M+ historical events extracted from Wikipedia timeline pages and article infoboxes. Our database provides unprecedented coverage in both historical depth and linguistic breadth with 10 languages. Additionally, we construct a comprehensive question answering benchmark for temporal reasoning across all languages. This benchmark covers a diverse set of 6 temporal QA reasoning tasks, and we evaluate a suite of popular language models (LLaMA-3-8B, Mistral-7B, Gemma-2-9b, Qwen3-8B, GPT4o) to assess their performance on these tasks. As expected GPT4o performs best across all answer types and languages; Gemma-2 outperforms the other small language models. Our work aims to provide a comprehensive resource for advancing multilingual and temporally-aware natural language understanding of historical events. To facilitate further research, we will make our code and datasets publicly available upon acceptance of this paper.</li>
</ul>

<h3>Title: SPGen: Spherical Projection as Consistent and Flexible Representation for Single Image 3D Shape Generation</h3>
<ul>
<li><strong>Authors: </strong>Jingdong Zhang, Weikai Chen, Yuan Liu, Jionghao Wang, Zhengming Yu, Zhuowen Shen, Bo Yang, Wenping Wang, Xin Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12721">https://arxiv.org/abs/2509.12721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12721">https://arxiv.org/pdf/2509.12721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12721]] SPGen: Spherical Projection as Consistent and Flexible Representation for Single Image 3D Shape Generation(https://arxiv.org/abs/2509.12721)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Existing single-view 3D generative models typically adopt multiview diffusion priors to reconstruct object surfaces, yet they remain prone to inter-view inconsistencies and are unable to faithfully represent complex internal structure or nontrivial topologies. In particular, we encode geometry information by projecting it onto a bounding sphere and unwrapping it into a compact and structural multi-layer 2D Spherical Projection (SP) representation. Operating solely in the image domain, SPGen offers three key advantages simultaneously: (1) Consistency. The injective SP mapping encodes surface geometry with a single viewpoint which naturally eliminates view inconsistency and ambiguity; (2) Flexibility. Multi-layer SP maps represent nested internal structures and support direct lifting to watertight or open 3D surfaces; (3) Efficiency. The image-domain formulation allows the direct inheritance of powerful 2D diffusion priors and enables efficient finetuning with limited computational resources. Extensive experiments demonstrate that SPGen significantly outperforms existing baselines in geometric quality and computational efficiency.</li>
</ul>

<h3>Title: Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yunhan Zhao, Xiang Zheng, Xingjun Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12724">https://arxiv.org/abs/2509.12724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12724">https://arxiv.org/pdf/2509.12724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12724]] Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models(https://arxiv.org/abs/2509.12724)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Despite their superb capabilities, Vision-Language Models (VLMs) have been shown to be vulnerable to jailbreak attacks. While recent jailbreaks have achieved notable progress, their effectiveness and efficiency can still be improved. In this work, we reveal an interesting phenomenon: incorporating weak defense into the attack pipeline can significantly enhance both the effectiveness and the efficiency of jailbreaks on VLMs. Building on this insight, we propose Defense2Attack, a novel jailbreak method that bypasses the safety guardrails of VLMs by leveraging defensive patterns to guide jailbreak prompt design. Specifically, Defense2Attack consists of three key components: (1) a visual optimizer that embeds universal adversarial perturbations with affirmative and encouraging semantics; (2) a textual optimizer that refines the input using a defense-styled prompt; and (3) a red-team suffix generator that enhances the jailbreak through reinforcement fine-tuning. We empirically evaluate our method on four VLMs and four safety benchmarks. The results demonstrate that Defense2Attack achieves superior jailbreak performance in a single attempt, outperforming state-of-the-art attack methods that often require multiple tries. Our work offers a new perspective on jailbreaking VLMs.</li>
</ul>

<h3>Title: A Novel Recurrent Neural Network Framework for Prediction and Treatment of Oncogenic Mutation Progression</h3>
<ul>
<li><strong>Authors: </strong>Rishab Parthasarathy, Achintya Bhowmik</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12732">https://arxiv.org/abs/2509.12732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12732">https://arxiv.org/pdf/2509.12732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12732]] A Novel Recurrent Neural Network Framework for Prediction and Treatment of Oncogenic Mutation Progression(https://arxiv.org/abs/2509.12732)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite significant medical advancements, cancer remains the second leading cause of death, with over 600,000 deaths per year in the US. One emerging field, pathway analysis, is promising but still relies on manually derived wet lab data, which is time-consuming to acquire. This work proposes an efficient, effective end-to-end framework for Artificial Intelligence (AI) based pathway analysis that predicts both cancer severity and mutation progression, thus recommending possible treatments. The proposed technique involves a novel combination of time-series machine learning models and pathway analysis. First, mutation sequences were isolated from The Cancer Genome Atlas (TCGA) Database. Then, a novel preprocessing algorithm was used to filter key mutations by mutation frequency. This data was fed into a Recurrent Neural Network (RNN) that predicted cancer severity. Then, the model probabilistically used the RNN predictions, information from the preprocessing algorithm, and multiple drug-target databases to predict future mutations and recommend possible treatments. This framework achieved robust results and Receiver Operating Characteristic (ROC) curves (a key statistical metric) with accuracies greater than 60%, similar to existing cancer diagnostics. In addition, preprocessing played an instrumental role in isolating important mutations, demonstrating that each cancer stage studied may contain on the order of a few-hundred key driver mutations, consistent with current research. Heatmaps based on predicted gene frequency were also generated, highlighting key mutations in each cancer. Overall, this work is the first to propose an efficient, cost-effective end-to-end framework for projecting cancer progression and providing possible treatments without relying on expensive, time-consuming wet lab work.</li>
</ul>

<h3>Title: What Makes a Good Generated Image? Investigating Human and Multimodal LLM Image Preference Alignment</h3>
<ul>
<li><strong>Authors: </strong>Rishab Parthasarathy, Jasmine Collins, Cory Stephenson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12750">https://arxiv.org/abs/2509.12750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12750">https://arxiv.org/pdf/2509.12750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12750]] What Makes a Good Generated Image? Investigating Human and Multimodal LLM Image Preference Alignment(https://arxiv.org/abs/2509.12750)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Automated evaluation of generative text-to-image models remains a challenging problem. Recent works have proposed using multimodal LLMs to judge the quality of images, but these works offer little insight into how multimodal LLMs make use of concepts relevant to humans, such as image style or composition, to generate their overall assessment. In this work, we study what attributes of an image--specifically aesthetics, lack of artifacts, anatomical accuracy, compositional correctness, object adherence, and style--are important for both LLMs and humans to make judgments on image quality. We first curate a dataset of human preferences using synthetically generated image pairs. We use inter-task correlation between each pair of image quality attributes to understand which attributes are related in making human judgments. Repeating the same analysis with LLMs, we find that the relationships between image quality attributes are much weaker. Finally, we study individual image quality attributes by generating synthetic datasets with a high degree of control for each axis. Humans are able to easily judge the quality of an image with respect to all of the specific image quality attributes (e.g. high vs. low aesthetic image), however we find that some attributes, such as anatomical accuracy, are much more difficult for multimodal LLMs to learn to judge. Taken together, these findings reveal interesting differences between how humans and multimodal LLMs perceive images.</li>
</ul>

<h3>Title: Recurrent Cross-View Object Geo-Localization</h3>
<ul>
<li><strong>Authors: </strong>Xiaohan Zhang, Si-Yuan Cao, Xiaokai Bai, Yiming Li, Zhangkai Shen, Zhe Wu, Xiaoxi Hu, Hui-liang Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12757">https://arxiv.org/abs/2509.12757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12757">https://arxiv.org/pdf/2509.12757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12757]] Recurrent Cross-View Object Geo-Localization(https://arxiv.org/abs/2509.12757)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Cross-view object geo-localization (CVOGL) aims to determine the location of a specific object in high-resolution satellite imagery given a query image with a point prompt. Existing approaches treat CVOGL as a one-shot detection task, directly regressing object locations from cross-view information aggregation, but they are vulnerable to feature noise and lack mechanisms for error correction. In this paper, we propose ReCOT, a Recurrent Cross-view Object geo-localization Transformer, which reformulates CVOGL as a recurrent localization task. ReCOT introduces a set of learnable tokens that encode task-specific intent from the query image and prompt embeddings, and iteratively attend to the reference features to refine the predicted location. To enhance this recurrent process, we incorporate two complementary modules: (1) a SAM-based knowledge distillation strategy that transfers segmentation priors from the Segment Anything Model (SAM) to provide clearer semantic guidance without additional inference cost, and (2) a Reference Feature Enhancement Module (RFEM) that introduces a hierarchical attention to emphasize object-relevant regions in the reference features. Extensive experiments on standard CVOGL benchmarks demonstrate that ReCOT achieves state-of-the-art (SOTA) performance while reducing parameters by 60% compared to previous SOTA approaches.</li>
</ul>

<h3>Title: Similarity-Distance-Magnitude Activations</h3>
<ul>
<li><strong>Authors: </strong>Allen Schmaltz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12760">https://arxiv.org/abs/2509.12760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12760">https://arxiv.org/pdf/2509.12760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12760]] Similarity-Distance-Magnitude Activations(https://arxiv.org/abs/2509.12760)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>We introduce a more robust and interpretable formulation of the standard softmax activation function commonly used with neural networks by adding Similarity (i.e., correctly predicted depth-matches into training) awareness and Distance-to-training-distribution awareness to the existing output Magnitude (i.e., decision-boundary) awareness. When used as the final-layer activation with language models, the resulting Similarity-Distance-Magnitude (SDM) activation function is more robust than the softmax function to co-variate shifts and out-of-distribution inputs in high-probability regions, and provides interpretability-by-exemplar via dense matching. Complementing the prediction-conditional estimates, the SDM activation enables a partitioning of the class-wise empirical CDFs to guard against low class-wise recall among selective classifications. These properties make it preferable for selective classification, even when considering post-hoc calibration methods over the softmax.</li>
</ul>

<h3>Title: DyGLNet: Hybrid Global-Local Feature Fusion with Dynamic Upsampling for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yican Zhao, Ce Wang, You Hao, Lei Li, Tianli Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12763">https://arxiv.org/abs/2509.12763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12763">https://arxiv.org/pdf/2509.12763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12763]] DyGLNet: Hybrid Global-Local Feature Fusion with Dynamic Upsampling for Medical Image Segmentation(https://arxiv.org/abs/2509.12763)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation grapples with challenges including multi-scale lesion variability, ill-defined tissue boundaries, and computationally intensive processing demands. This paper proposes the DyGLNet, which achieves efficient and accurate segmentation by fusing global and local features with a dynamic upsampling mechanism. The model innovatively designs a hybrid feature extraction module (SHDCBlock), combining single-head self-attention and multi-scale dilated convolutions to model local details and global context collaboratively. We further introduce a dynamic adaptive upsampling module (DyFusionUp) to realize high-fidelity reconstruction of feature maps based on learnable offsets. Then, a lightweight design is adopted to reduce computational overhead. Experiments on seven public datasets demonstrate that DyGLNet outperforms existing methods, particularly excelling in boundary accuracy and small-object segmentation. Meanwhile, it exhibits lower computation complexity, enabling an efficient and reliable solution for clinical medical image analysis. The code will be made available soon.</li>
</ul>

<h3>Title: BATR-FST: Bi-Level Adaptive Token Refinement for Few-Shot Transformers</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Al-Habib, Zuping Zhang, Abdulrahman Noman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12768">https://arxiv.org/abs/2509.12768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12768">https://arxiv.org/pdf/2509.12768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12768]] BATR-FST: Bi-Level Adaptive Token Refinement for Few-Shot Transformers(https://arxiv.org/abs/2509.12768)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs) have shown significant promise in computer vision applications. However, their performance in few-shot learning is limited by challenges in refining token-level interactions, struggling with limited training data, and developing a strong inductive bias. Existing methods often depend on inflexible token matching or basic similarity measures, which limit the effective incorporation of global context and localized feature refinement. To address these challenges, we propose Bi-Level Adaptive Token Refinement for Few-Shot Transformers (BATR-FST), a two-stage approach that progressively improves token representations and maintains a robust inductive bias for few-shot classification. During the pre-training phase, Masked Image Modeling (MIM) provides Vision Transformers (ViTs) with transferable patch-level representations by recreating masked image regions, providing a robust basis for subsequent adaptation. In the meta-fine-tuning phase, BATR-FST incorporates a Bi-Level Adaptive Token Refinement module that utilizes Token Clustering to capture localized interactions, Uncertainty-Aware Token Weighting to prioritize dependable features, and a Bi-Level Attention mechanism to balance intra-cluster and inter-cluster relationships, thereby facilitating thorough token refinement. Furthermore, Graph Token Propagation ensures semantic consistency between support and query instances, while a Class Separation Penalty preserves different class borders, enhancing discriminative capability. Extensive experiments on three benchmark few-shot datasets demonstrate that BATR-FST achieves superior results in both 1-shot and 5-shot scenarios and improves the few-shot classification via transformers.</li>
</ul>

<h3>Title: Double Helix Diffusion for Cross-Domain Anomaly Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Linchun Wu, Qin Zou, Xianbiao Qi, Bo Du, Zhongyuan Wang, Qingquan Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12787">https://arxiv.org/abs/2509.12787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12787">https://arxiv.org/pdf/2509.12787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12787]] Double Helix Diffusion for Cross-Domain Anomaly Image Generation(https://arxiv.org/abs/2509.12787)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Visual anomaly inspection is critical in manufacturing, yet hampered by the scarcity of real anomaly samples for training robust detectors. Synthetic data generation presents a viable strategy for data augmentation; however, current methods remain constrained by two principal limitations: 1) the generation of anomalies that are structurally inconsistent with the normal background, and 2) the presence of undesirable feature entanglement between synthesized images and their corresponding annotation masks, which undermines the perceptual realism of the output. This paper introduces Double Helix Diffusion (DH-Diff), a novel cross-domain generative framework designed to simultaneously synthesize high-fidelity anomaly images and their pixel-level annotation masks, explicitly addressing these challenges. DH-Diff employs a unique architecture inspired by a double helix, cycling through distinct modules for feature separation, connection, and merging. Specifically, a domain-decoupled attention mechanism mitigates feature entanglement by enhancing image and annotation features independently, and meanwhile a semantic score map alignment module ensures structural authenticity by coherently integrating anomaly foregrounds. DH-Diff offers flexible control via text prompts and optional graphical guidance. Extensive experiments demonstrate that DH-Diff significantly outperforms state-of-the-art methods in diversity and authenticity, leading to significant improvements in downstream anomaly detection performance.</li>
</ul>

<h3>Title: Superpixel Anything: A general object-based framework for accurate yet regular superpixel segmentation</h3>
<ul>
<li><strong>Authors: </strong>Julien Walther, R√©mi Giraud, Micha√´l Cl√©ment</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12791">https://arxiv.org/abs/2509.12791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12791">https://arxiv.org/pdf/2509.12791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12791]] Superpixel Anything: A general object-based framework for accurate yet regular superpixel segmentation(https://arxiv.org/abs/2509.12791)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Superpixels are widely used in computer vision to simplify image representation and reduce computational complexity. While traditional methods rely on low-level features, deep learning-based approaches leverage high-level features but also tend to sacrifice regularity of superpixels to capture complex objects, leading to accurate but less interpretable segmentations. In this work, we introduce SPAM (SuperPixel Anything Model), a versatile framework for segmenting images into accurate yet regular superpixels. We train a model to extract image features for superpixel generation, and at inference, we leverage a large-scale pretrained model for semantic-agnostic segmentation to ensure that superpixels align with object masks. SPAM can handle any prior high-level segmentation, resolving uncertainty regions, and is able to interactively focus on specific objects. Comprehensive experiments demonstrate that SPAM qualitatively and quantitatively outperforms state-of-the-art methods on segmentation tasks, making it a valuable and robust tool for various applications. Code and pre-trained models are available here: this https URL.</li>
</ul>

<h3>Title: ConvergeWriter: Data-Driven Bottom-Up Article Construction</h3>
<ul>
<li><strong>Authors: </strong>Binquan Ji, Jiaqi Wang, Ruiting Li, Xingchen Han, Yiyang Qi, Shichao Wang, Yifei Lu, Yuantao Han, Feiliang Ren</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12811">https://arxiv.org/abs/2509.12811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12811">https://arxiv.org/pdf/2509.12811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12811]] ConvergeWriter: Data-Driven Bottom-Up Article Construction(https://arxiv.org/abs/2509.12811)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable prowess in text generation, yet producing long-form, factual documents grounded in extensive external knowledge bases remains a significant challenge. Existing "top-down" methods, which first generate a hypothesis or outline and then retrieve evidence, often suffer from a disconnect between the model's plan and the available knowledge, leading to content fragmentation and factual inaccuracies. To address these limitations, we propose a novel "bottom-up," data-driven framework that inverts the conventional generation pipeline. Our approach is predicated on a "Retrieval-First for Knowledge, Clustering for Structure" strategy, which first establishes the "knowledge boundaries" of the source corpus before any generative planning occurs. Specifically, we perform exhaustive iterative retrieval from the knowledge base and then employ an unsupervised clustering algorithm to organize the retrieved documents into distinct "knowledge clusters." These clusters form an objective, data-driven foundation that directly guides the subsequent generation of a hierarchical outline and the final document content. This bottom-up process ensures that the generated text is strictly constrained by and fully traceable to the source material, proactively adapting to the finite scope of the knowledge base and fundamentally mitigating the risk of hallucination. Experimental results on both 14B and 32B parameter models demonstrate that our method achieves performance comparable to or exceeding state-of-the-art baselines, and is expected to demonstrate unique advantages in knowledge-constrained scenarios that demand high fidelity and structural coherence. Our work presents an effective paradigm for generating reliable, structured, long-form documents, paving the way for more robust LLM applications in high-stakes, knowledge-intensive domains.</li>
</ul>

<h3>Title: Energy-Efficient Quantized Federated Learning for Resource-constrained IoT devices</h3>
<ul>
<li><strong>Authors: </strong>Wilfrid Sougrinoma Compaor√©, Yaya Etiabi, El Mehdi Amhoud, Mohamad Assaad</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12814">https://arxiv.org/abs/2509.12814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12814">https://arxiv.org/pdf/2509.12814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12814]] Energy-Efficient Quantized Federated Learning for Resource-constrained IoT devices(https://arxiv.org/abs/2509.12814)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has emerged as a promising paradigm for enabling collaborative machine learning while preserving data privacy, making it particularly suitable for Internet of Things (IoT) environments. However, resource-constrained IoT devices face significant challenges due to limited energy,unreliable communication channels, and the impracticality of assuming infinite blocklength transmission. This paper proposes a federated learning framework for IoT networks that integrates finite blocklength transmission, model quantization, and an error-aware aggregation mechanism to enhance energy efficiency and communication reliability. The framework also optimizes uplink transmission power to balance energy savings and model performance. Simulation results demonstrate that the proposed approach significantly reduces energy consumption by up to 75\% compared to a standard FL model, while maintaining robust model accuracy, making it a viable solution for FL in real-world IoT scenarios with constrained resources. This work paves the way for efficient and reliable FL implementations in practical IoT deployments. Index Terms: Federated learning, IoT, finite blocklength, quantization, energy efficiency.</li>
</ul>

<h3>Title: SAGA: Selective Adaptive Gating for Efficient and Expressive Linear Attention</h3>
<ul>
<li><strong>Authors: </strong>Yuan Cao, Dong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12817">https://arxiv.org/abs/2509.12817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12817">https://arxiv.org/pdf/2509.12817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12817]] SAGA: Selective Adaptive Gating for Efficient and Expressive Linear Attention(https://arxiv.org/abs/2509.12817)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While Transformer architecture excel at modeling long-range dependencies contributing to its widespread adoption in vision tasks the quadratic complexity of softmax-based attention mechanisms imposes a major bottleneck, particularly when processing high-resolution images. Linear attention presents a promising alternative by reformulating the attention computation from $(QK)V$ to $Q(KV)$, thereby reducing the complexity from $\mathcal{O}(N^2)$ to $\mathcal{O}(N)$ while preserving the global receptive field. However, most existing methods compress historical key-value (KV) information uniformly, which can lead to feature redundancy and the loss of directional alignment with the query (Q). This uniform compression results in low-rank $KV$ feature maps, contributing to a performance gap compared to softmax attention. To mitigate this limitation, we propose \textbf{S}elective \textbf{A}daptive \textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which introduces input-adaptive learnable gates to selectively modulate information aggregation into the $KV$ feature map. These gates enhance semantic diversity and alleviate the low-rank constraint inherent in conventional linear attention. Additionally, we propose an efficient Hadamard-product decomposition method for gate computation, which introduces no additional memory overhead. Experiments demonstrate that SAGA achieves a 1.76$\times$ improvement in throughput and a 2.69$\times$ reduction in peak GPU memory compared to PVT-T at a resolution of $1280 \times 1280$. Moreover, it improves top-1 accuracy by up to 4.4\% on the ImageNet dataset, demonstrating both computational efficiency and model effectiveness.</li>
</ul>

<h3>Title: Data Scaling Laws for Radiology Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Ilse, Harshita Sharma, Anton Schwaighofer, Sam Bond-Taylor, Fernando P√©rez-Garc√≠a, Olesya Melnichenko, Anne-Marie G. Sykes, Kelly K. Horst, Ashish Khandelwal, Maxwell Reynolds, Maria T. Wetscherek, Noel C. F. Codella, Javier Alvarez-Valle, Korfiatis Panagiotis, Valentina Salvatelli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12818">https://arxiv.org/abs/2509.12818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12818">https://arxiv.org/pdf/2509.12818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12818]] Data Scaling Laws for Radiology Foundation Models(https://arxiv.org/abs/2509.12818)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Foundation vision encoders such as CLIP and DINOv2, trained on web-scale data, exhibit strong transfer performance across tasks and datasets. However, medical imaging foundation models remain constrained by smaller datasets, limiting our understanding of how data scale and pretraining paradigms affect performance in this setting. In this work, we systematically study continual pretraining of two vision encoders, MedImageInsight (MI2) and RAD-DINO representing the two major encoder paradigms CLIP and DINOv2, on up to 3.5M chest x-rays from a single institution, holding compute and evaluation protocols constant. We evaluate on classification (radiology findings, lines and tubes), segmentation (lines and tubes), and radiology report generation. While prior work has primarily focused on tasks related to radiology findings, we include lines and tubes tasks to counterbalance this bias and evaluate a model's ability to extract features that preserve continuity along elongated structures. Our experiments show that MI2 scales more effectively for finding-related tasks, while RAD-DINO is stronger on tube-related tasks. Surprisingly, continually pretraining MI2 with both reports and structured labels using UniCL improves performance, underscoring the value of structured supervision at scale. We further show that for some tasks, as few as 30k in-domain samples are sufficient to surpass open-weights foundation models. These results highlight the utility of center-specific continual pretraining, enabling medical institutions to derive significant performance gains by utilizing in-domain data.</li>
</ul>

<h3>Title: Exploring Metric Fusion for Evaluation of NeRFs</h3>
<ul>
<li><strong>Authors: </strong>Shreyas Shivakumara, Gabriel Eilertsen, Karljohan Lundin Palmerius</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12836">https://arxiv.org/abs/2509.12836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12836">https://arxiv.org/pdf/2509.12836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12836]] Exploring Metric Fusion for Evaluation of NeRFs(https://arxiv.org/abs/2509.12836)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural Radiance Fields (NeRFs) have demonstrated significant potential in synthesizing novel viewpoints. Evaluating the NeRF-generated outputs, however, remains a challenge due to the unique artifacts they exhibit, and no individual metric performs well across all datasets. We hypothesize that combining two successful metrics, Deep Image Structure and Texture Similarity (DISTS) and Video Multi-Method Assessment Fusion (VMAF), based on different perceptual methods, can overcome the limitations of individual metrics and achieve improved correlation with subjective quality scores. We experiment with two normalization strategies for the individual metrics and two fusion strategies to evaluate their impact on the resulting correlation with the subjective scores. The proposed pipeline is tested on two distinct datasets, Synthetic and Outdoor, and its performance is evaluated across three different configurations. We present a detailed analysis comparing the correlation coefficients of fusion methods and individual scores with subjective scores to demonstrate the robustness and generalizability of the fusion metrics.</li>
</ul>

<h3>Title: Leveraging Large Language Models to Effectively Generate Visual Data for Canine Musculoskeletal Diagnoses</h3>
<ul>
<li><strong>Authors: </strong>Martin Thi√üen, Thi Ngoc Diep Tran, Barbara Esteve Ratsch, Ben Joel Sch√∂nbein, Ute Trapp, Beate Egner, Romana Piat, Elke Hergenr√∂ther</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12866">https://arxiv.org/abs/2509.12866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12866">https://arxiv.org/pdf/2509.12866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12866]] Leveraging Large Language Models to Effectively Generate Visual Data for Canine Musculoskeletal Diagnoses(https://arxiv.org/abs/2509.12866)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>It is well-established that more data generally improves AI model performance. However, data collection can be challenging for certain tasks due to the rarity of occurrences or high costs. These challenges are evident in our use case, where we apply AI models to a novel approach for visually documenting the musculoskeletal condition of dogs. Here, abnormalities are marked as colored strokes on a body map of a dog. Since these strokes correspond to distinct muscles or joints, they can be mapped to the textual domain in which large language models (LLMs) operate. LLMs have demonstrated impressive capabilities across a wide range of tasks, including medical applications, offering promising potential for generating synthetic training data. In this work, we investigate whether LLMs can effectively generate synthetic visual training data for canine musculoskeletal diagnoses. For this, we developed a mapping that segments visual documentations into over 200 labeled regions representing muscles or joints. Using techniques like guided decoding, chain-of-thought reasoning, and few-shot prompting, we generated 1,000 synthetic visual documentations for patellar luxation (kneecap dislocation) diagnosis, the diagnosis for which we have the most real-world data. Our analysis shows that the generated documentations are sensitive to location and severity of the diagnosis while remaining independent of the dog's sex. We further generated 1,000 visual documentations for various other diagnoses to create a binary classification dataset. A model trained solely on this synthetic data achieved an F1 score of 88% on 70 real-world documentations. These results demonstrate the potential of LLM-generated synthetic data, which is particularly valuable for addressing data scarcity in rare diseases. While our methodology is tailored to the medical domain, the insights and techniques can be adapted to other fields.</li>
</ul>

<h3>Title: Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use</h3>
<ul>
<li><strong>Authors: </strong>Yabo Zhang, Yihan Zeng, Qingyun Li, Zhen Hu, Kavin Han, Wangmeng Zuo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12867">https://arxiv.org/abs/2509.12867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12867">https://arxiv.org/pdf/2509.12867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12867]] Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use(https://arxiv.org/abs/2509.12867)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated strong capabilities in language understanding and reasoning, yet they remain limited when tackling real-world tasks that require up-to-date knowledge, precise operations, or specialized tool use. To address this, we propose Tool-R1, a reinforcement learning framework that enables LLMs to perform general, compositional, and multi-step tool use by generating executable Python code. Tool-R1 supports integration of user-defined tools and standard libraries, with variable sharing across steps to construct coherent workflows. An outcome-based reward function, combining LLM-based answer judgment and code execution success, guides policy optimization. To improve training efficiency, we maintain a dynamic sample queue to cache and reuse high-quality trajectories, reducing the overhead of costly online sampling. Experiments on the GAIA benchmark show that Tool-R1 substantially improves both accuracy and robustness, achieving about 10\% gain over strong baselines, with larger improvements on complex multi-step tasks. These results highlight the potential of Tool-R1 for enabling reliable and efficient tool-augmented reasoning in real-world applications. Our code will be available at this https URL.</li>
</ul>

<h3>Title: Cumulative Consensus Score: Label-Free and Model-Agnostic Evaluation of Object Detectors in Deployment</h3>
<ul>
<li><strong>Authors: </strong>Avinaash Manoharan, Xiangyu Yin, Domenik Helm, Chih-Hong Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12871">https://arxiv.org/abs/2509.12871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12871">https://arxiv.org/pdf/2509.12871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12871]] Cumulative Consensus Score: Label-Free and Model-Agnostic Evaluation of Object Detectors in Deployment(https://arxiv.org/abs/2509.12871)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Evaluating object detection models in deployment is challenging because ground-truth annotations are rarely available. We introduce the Cumulative Consensus Score (CCS), a label-free metric that enables continuous monitoring and comparison of detectors in real-world settings. CCS applies test-time data augmentation to each image, collects predicted bounding boxes across augmented views, and computes overlaps using Intersection over Union. Maximum overlaps are normalized and averaged across augmentation pairs, yielding a measure of spatial consistency that serves as a proxy for reliability without annotations. In controlled experiments on Open Images and KITTI, CCS achieved over 90% congruence with F1-score, Probabilistic Detection Quality, and Optimal Correction Cost. The method is model-agnostic, working across single-stage and two-stage detectors, and operates at the case level to highlight under-performing scenarios. Altogether, CCS provides a robust foundation for DevOps-style monitoring of object detectors.</li>
</ul>

<h3>Title: Benchmarking and Improving LVLMs on Event Extraction from Multimedia Documents</h3>
<ul>
<li><strong>Authors: </strong>Fuyu Xing, Zimu Wang, Wei Wang, Haiyang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12876">https://arxiv.org/abs/2509.12876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12876">https://arxiv.org/pdf/2509.12876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12876]] Benchmarking and Improving LVLMs on Event Extraction from Multimedia Documents(https://arxiv.org/abs/2509.12876)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The proliferation of multimedia content necessitates the development of effective Multimedia Event Extraction (M2E2) systems. Though Large Vision-Language Models (LVLMs) have shown strong cross-modal capabilities, their utility in the M2E2 task remains underexplored. In this paper, we present the first systematic evaluation of representative LVLMs, including DeepSeek-VL2 and the Qwen-VL series, on the M2E2 dataset. Our evaluations cover text-only, image-only, and cross-media subtasks, assessed under both few-shot prompting and fine-tuning settings. Our key findings highlight the following valuable insights: (1) Few-shot LVLMs perform notably better on visual tasks but struggle significantly with textual tasks; (2) Fine-tuning LVLMs with LoRA substantially enhances model performance; and (3) LVLMs exhibit strong synergy when combining modalities, achieving superior performance in cross-modal settings. We further provide a detailed error analysis to reveal persistent challenges in areas such as semantic precision, localization, and cross-modal grounding, which remain critical obstacles for advancing M2E2 capabilities.</li>
</ul>

<h3>Title: Hardened CTIDH: Dummy-Free and Deterministic CTIDH</h3>
<ul>
<li><strong>Authors: </strong>Gustavo Banegas (LIX, GRACE), Andreas Hellenbrand, Matheus Saldanha (UFSC)</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12877">https://arxiv.org/abs/2509.12877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12877">https://arxiv.org/pdf/2509.12877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12877]] Hardened CTIDH: Dummy-Free and Deterministic CTIDH(https://arxiv.org/abs/2509.12877)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Isogeny-based cryptography has emerged as a promising postquantum alternative, with CSIDH and its constant-time variants CTIDH and dCTIDH offering efficient group-action protocols. However, CTIDH and dCTIDH rely on dummy operations in differential addition chains (DACs) and Matryoshka, which can be exploitable by fault-injection attacks. In this work, we present the first dummy-free implementation of dCTIDH. Our approach combines two recent ideas: DACsHUND, which enforces equal-length DACs within each batch without padding, and a reformulated Matryoshka structure that removes dummy multiplications and validates all intermediate points. Our analysis shows that small primes such as 3, 5, and 7 severely restrict feasible DACsHUND configurations, motivating new parameter sets that exclude them. We implement dummy-free dCTIDH-2048-194 and dCTIDH-2048-205, achieving group action costs of roughly 357,000-362,000 Fp-multiplications, with median evaluation times of 1.59-1.60 (Gcyc). These results do not surpass dC-TIDH, but they outperform CTIDH by roughly 5% while eliminating dummy operations entirely. Compared to dCSIDH, our construction is more than 4x faster. To the best of our knowledge, this is the first efficient implementation of a CSIDH-like protocol that is simultaneously deterministic, constant-time, and fully dummy-free.</li>
</ul>

<h3>Title: Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Qianguang Zhao, Dongli Wang, Yan Zhou, Jianxun Li, Richard Irampa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12878">https://arxiv.org/abs/2509.12878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12878">https://arxiv.org/pdf/2509.12878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12878]] Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation(https://arxiv.org/abs/2509.12878)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Few-shot 3D point cloud semantic segmentation aims to segment novel categories using a minimal number of annotated support samples. While existing prototype-based methods have shown promise, they are constrained by two critical challenges: (1) Intra-class Diversity, where a prototype's limited representational capacity fails to cover a class's full variations, and (2) Inter-set Inconsistency, where prototypes derived from the support set are misaligned with the query feature space. Motivated by the powerful generative capability of diffusion model, we re-purpose its pre-trained conditional encoder to provide a novel source of generalizable features for expanding the prototype's representational range. Under this setup, we introduce the Prototype Expansion Network (PENet), a framework that constructs big-capacity prototypes from two complementary feature sources. PENet employs a dual-stream learner architecture: it retains a conventional fully supervised Intrinsic Learner (IL) to distill representative features, while introducing a novel Diffusion Learner (DL) to provide rich generalizable features. The resulting dual prototypes are then processed by a Prototype Assimilation Module (PAM), which adopts a novel push-pull cross-guidance attention block to iteratively align the prototypes with the query space. Furthermore, a Prototype Calibration Mechanism (PCM) regularizes the final big capacity prototype to prevent semantic drift. Extensive experiments on the S3DIS and ScanNet datasets demonstrate that PENet significantly outperforms state-of-the-art methods across various few-shot settings.</li>
</ul>

<h3>Title: A Fault Analysis on SNOVA</h3>
<ul>
<li><strong>Authors: </strong>Gustavo Banegas (GRACE), Ricardo Villanueva-Polanco (TII)</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12879">https://arxiv.org/abs/2509.12879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12879">https://arxiv.org/pdf/2509.12879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12879]] A Fault Analysis on SNOVA(https://arxiv.org/abs/2509.12879)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>SNOVA is a post-quantum cryptographic signature scheme known for its efficiency and compact key sizes, making it a second-round candidate in the NIST post-quantum cryptography standardization process. This paper presents a comprehensive fault analysis of SNOVA, focusing on both permanent and transient faults during signature generation. We introduce several fault injection strategies that exploit SNOVA's structure to recover partial or complete secret keys with limited faulty signatures. Our analysis reveals that as few as 22 to 68 faulty signatures, depending on the security level, can suffice for key recovery. We propose a novel fault-assisted reconciliation attack, demonstrating its effectiveness in extracting the secret key space via solving a quadratic polynomial system. Simulations show transient faults in key signature generation steps can significantly compromise SNOVA's security. To address these vulnerabilities, we propose a lightweight countermeasure to reduce the success of fault attacks without adding significant overhead. Our results highlight the importance of fault-resistant mechanisms in post-quantum cryptographic schemes like SNOVA to ensure robustness.</li>
</ul>

<h3>Title: Lego-Edit: A General Image Editing Framework with Model-Level Bricks and MLLM Builder</h3>
<ul>
<li><strong>Authors: </strong>Qifei Jia, Yu Liu, Yajie Chai, Xintong Yao, Qiming Lu, Yasen Zhang, Runyu Shi, Ying Huang, Guoquan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12883">https://arxiv.org/abs/2509.12883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12883">https://arxiv.org/pdf/2509.12883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12883]] Lego-Edit: A General Image Editing Framework with Model-Level Bricks and MLLM Builder(https://arxiv.org/abs/2509.12883)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Instruction-based image editing has garnered significant attention due to its direct interaction with users. However, real-world user instructions are immensely diverse, and existing methods often fail to generalize effectively to instructions outside their training domain, limiting their practical application. To address this, we propose Lego-Edit, which leverages the generalization capability of Multi-modal Large Language Model (MLLM) to organize a suite of model-level editing tools to tackle this challenge. Lego-Edit incorporates two key designs: (1) a model-level toolkit comprising diverse models efficiently trained on limited data and several image manipulation functions, enabling fine-grained composition of editing actions by the MLLM; and (2) a three-stage progressive reinforcement learning approach that uses feedback on unannotated, open-domain instructions to train the MLLM, equipping it with generalized reasoning capabilities for handling real-world instructions. Experiments demonstrate that Lego-Edit achieves state-of-the-art performance on GEdit-Bench and ImgBench. It exhibits robust reasoning capabilities for open-domain instructions and can utilize newly introduced editing tools without additional fine-tuning. Code is available: this https URL.</li>
</ul>

<h3>Title: The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations</h3>
<ul>
<li><strong>Authors: </strong>Yubo Zhu, Dongrui Liu, Zecheng Lin, Wei Tong, Sheng Zhong, Jing Shao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12886">https://arxiv.org/abs/2509.12886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12886">https://arxiv.org/pdf/2509.12886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12886]] The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations(https://arxiv.org/abs/2509.12886)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Estimating the difficulty of input questions as perceived by large language models (LLMs) is essential for accurate performance evaluation and adaptive inference. Existing methods typically rely on repeated response sampling, auxiliary models, or fine-tuning the target model itself, which may incur substantial computational costs or compromise generality. In this paper, we propose a novel approach for difficulty estimation that leverages only the hidden representations produced by the target LLM. We model the token-level generation process as a Markov chain and define a value function to estimate the expected output quality given any hidden state. This allows for efficient and accurate difficulty estimation based solely on the initial hidden state, without generating any output tokens. Extensive experiments across both textual and multimodal tasks demonstrate that our method consistently outperforms existing baselines in difficulty estimation. Moreover, we apply our difficulty estimates to guide adaptive reasoning strategies, including Self-Consistency, Best-of-N, and Self-Refine, achieving higher inference efficiency with fewer generated tokens.</li>
</ul>

<h3>Title: Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing</h3>
<ul>
<li><strong>Authors: </strong>Weiming Chen, Zhihan Zhu, Yijia Wang, Zhihai He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12888">https://arxiv.org/abs/2509.12888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12888">https://arxiv.org/pdf/2509.12888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12888]] Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing(https://arxiv.org/abs/2509.12888)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Rectified flow (RF) models have recently demonstrated superior generative performance compared to DDIM-based diffusion models. However, in real-world applications, they suffer from two major challenges: (1) low inversion accuracy that hinders the consistency with the source image, and (2) entangled multimodal attention in diffusion transformers, which hinders precise attention control. To address the first challenge, we propose an efficient high-order inversion method for rectified flow models based on the Runge-Kutta solver of differential equations. To tackle the second challenge, we introduce Decoupled Diffusion Transformer Attention (DDTA), a novel mechanism that disentangles text and image attention inside the multimodal diffusion transformers, enabling more precise semantic control. Extensive experiments on image reconstruction and text-guided editing tasks demonstrate that our method achieves state-of-the-art performance in terms of fidelity and editability. Code is available at this https URL.</li>
</ul>

<h3>Title: Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Shiyu Li, Yang Tang, Ruijie Liu, Shi-Zhe Chen, Xi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12892">https://arxiv.org/abs/2509.12892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12892">https://arxiv.org/pdf/2509.12892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12892]] Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings(https://arxiv.org/abs/2509.12892)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently demonstrated excellent performance in text embedding tasks. Previous work usually use LoRA to fine-tune existing LLMs, which are limited by the data and training gap between LLMs and embedding models. In this work, we introduce Conan-embedding-v2, a new 1.4B-parameter LLM trained from scratch and fine-tuned as a text embedder. First, we add news data and multilingual pairs for LLM pretraining to bridge the data gap. Based on this, we propose a cross-lingual retrieval dataset that enables the LLM to better integrate embeddings across different languages. Second, whereas LLMs use a causal mask with token-level loss, embedding models use a bidirectional mask with sentence-level loss. This training gap makes full fine-tuning less effective than LoRA. We introduce a soft-masking mechanism to gradually transition between these two types of masks, enabling the model to learn more comprehensive representations. Based on this, we propose a dynamic hard negative mining method that exposes the model to more difficult negative examples throughout the training process. Being intuitive and effective, with only approximately 1.4B parameters, Conan-embedding-v2 achieves SOTA performance on both the Massive Text Embedding Benchmark (MTEB) and Chinese MTEB (May 19, 2025).</li>
</ul>

<h3>Title: MEJO: MLLM-Engaged Surgical Triplet Recognition via Inter- and Intra-Task Joint Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yiyi Zhang, Yuchen Yuan, Ying Zheng, Jialun Pei, Jinpeng Li, Zheng Li, Pheng-Ann Heng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12893">https://arxiv.org/abs/2509.12893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12893">https://arxiv.org/pdf/2509.12893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12893]] MEJO: MLLM-Engaged Surgical Triplet Recognition via Inter- and Intra-Task Joint Optimization(https://arxiv.org/abs/2509.12893)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Surgical triplet recognition, which involves identifying instrument, verb, target, and their combinations, is a complex surgical scene understanding challenge plagued by long-tailed data distribution. The mainstream multi-task learning paradigm benefiting from cross-task collaborative promotion has shown promising performance in identifying triples, but two key challenges remain: 1) inter-task optimization conflicts caused by entangling task-generic and task-specific representations; 2) intra-task optimization conflicts due to class-imbalanced training data. To overcome these difficulties, we propose the MLLM-Engaged Joint Optimization (MEJO) framework that empowers both inter- and intra-task optimization for surgical triplet recognition. For inter-task optimization, we introduce the Shared-Specific-Disentangled (S$^2$D) learning scheme that decomposes representations into task-shared and task-specific components. To enhance task-shared representations, we construct a Multimodal Large Language Model (MLLM) powered probabilistic prompt pool to dynamically augment visual features with expert-level semantic cues. Additionally, comprehensive task-specific cues are modeled via distinct task prompts covering the temporal-spatial dimensions, effectively mitigating inter-task ambiguities. To tackle intra-task optimization conflicts, we develop a Coordinated Gradient Learning (CGL) strategy, which dissects and rebalances the positive-negative gradients originating from head and tail classes for more coordinated learning behaviors. Extensive experiments on the CholecT45 and CholecT50 datasets demonstrate the superiority of our proposed framework, validating its effectiveness in handling optimization conflicts.</li>
</ul>

<h3>Title: TimeCluster with PCA is Equivalent to Subspace Identification of Linear Dynamical Systems</h3>
<ul>
<li><strong>Authors: </strong>Christian L. Hines, Samuel Spillard, Daniel P. Martin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12895">https://arxiv.org/abs/2509.12895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12895">https://arxiv.org/pdf/2509.12895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12895]] TimeCluster with PCA is Equivalent to Subspace Identification of Linear Dynamical Systems(https://arxiv.org/abs/2509.12895)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>TimeCluster is a visual analytics technique for discovering structure in long multivariate time series by projecting overlapping windows of data into a low-dimensional space. We show that, when Principal Component Analysis (PCA) is chosen as the dimensionality reduction technique, this procedure is mathematically equivalent to classical linear subspace identification (block-Hankel matrix plus Singular Vector Decomposition (SVD)). In both approaches, the same low-dimensional linear subspace is extracted from the time series data. We first review the TimeCluster method and the theory of subspace system identification. Then we show that forming the sliding-window matrix of a time series yields a Hankel matrix, so applying PCA (via SVD) to this matrix recovers the same principal directions as subspace identification. Thus the cluster coordinates from TimeCluster coincide with the subspace identification methods. We present experiments on synthetic and real dynamical signals confirming that the two embeddings coincide. Finally, we explore and discuss future opportunities enabled by this equivalence, including forecasting from the identified state space, streaming/online extensions, incorporating and visualising external inputs and robust techniques for displaying underlying trends in corrupted data.</li>
</ul>

<h3>Title: EByFTVeS: Efficient Byzantine Fault Tolerant-based Verifiable Secret-sharing in Distributed Privacy-preserving Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhen Li, Zijian Zhang, Wenjin Yang, Pengbo Wang, Zhaoqi Wang, Meng Li, Yan Wu, Xuyang Liu, Jing Sun, Liehuang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12899">https://arxiv.org/abs/2509.12899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12899">https://arxiv.org/pdf/2509.12899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12899]] EByFTVeS: Efficient Byzantine Fault Tolerant-based Verifiable Secret-sharing in Distributed Privacy-preserving Machine Learning(https://arxiv.org/abs/2509.12899)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack</a></li>
<li><strong>Abstract: </strong>Verifiable Secret Sharing (VSS) has been widespread in Distributed Privacy-preserving Machine Learning (DPML), because invalid shares from malicious dealers or participants can be recognized by verifying the commitment of the received shares for honest participants. However, the consistency and the computation and communitation burden of the VSS-based DPML schemes are still two serious challenges. Although Byzantine Fault Tolerance (BFT) system has been brought to guarantee the consistency and improve the efficiency of the existing VSS-based DPML schemes recently, we explore an Adaptive Share Delay Provision (ASDP) strategy, and launch an ASDP-based Customized Model Poisoning Attack (ACuMPA) for certain participants in this paper. We theoretically analyzed why the ASDP strategy and the ACuMPA algorithm works to the existing schemes. Next, we propose an [E]fficient [By]zantine [F]ault [T]olerant-based [Ve]rifiable [S]ecret-sharing (EByFTVeS) scheme. Finally, the validity, liveness, consistency and privacy of the EByFTVeS scheme are theoretically analyzed, while the efficiency of the EByFTVeS scheme outperforms that of the-state-of-art VSS scheme according to comparative experiment results.</li>
</ul>

<h3>Title: MSGFusion: Multimodal Scene Graph-Guided Infrared and Visible Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Guihui Li, Bowei Dong, Kaizhi Dong, Jiayi Li, Haiyong Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12901">https://arxiv.org/abs/2509.12901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12901">https://arxiv.org/pdf/2509.12901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12901]] MSGFusion: Multimodal Scene Graph-Guided Infrared and Visible Image Fusion(https://arxiv.org/abs/2509.12901)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Infrared and visible image fusion has garnered considerable attention owing to the strong complementarity of these two modalities in complex, harsh environments. While deep learning-based fusion methods have made remarkable advances in feature extraction, alignment, fusion, and reconstruction, they still depend largely on low-level visual cues, such as texture and contrast, and struggle to capture the high-level semantic information embedded in images. Recent attempts to incorporate text as a source of semantic guidance have relied on unstructured descriptions that neither explicitly model entities, attributes, and relationships nor provide spatial localization, thereby limiting fine-grained fusion performance. To overcome these challenges, we introduce MSGFusion, a multimodal scene graph-guided fusion framework for infrared and visible imagery. By deeply coupling structured scene graphs derived from text and vision, MSGFusion explicitly represents entities, attributes, and spatial relations, and then synchronously refines high-level semantics and low-level details through successive modules for scene graph representation, hierarchical aggregation, and graph-driven fusion. Extensive experiments on multiple public benchmarks show that MSGFusion significantly outperforms state-of-the-art approaches, particularly in detail preservation and structural clarity, and delivers superior semantic consistency and generalizability in downstream tasks such as low-light object detection, semantic segmentation, and medical image fusion.</li>
</ul>

<h3>Title: AREPAS: Anomaly Detection in Fine-Grained Anatomy with Reconstruction-Based Semantic Patch-Scoring</h3>
<ul>
<li><strong>Authors: </strong>Branko Mitic, Philipp Seeb√∂ck, Helmut Prosch, Georg Langs</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12905">https://arxiv.org/abs/2509.12905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12905">https://arxiv.org/pdf/2509.12905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12905]] AREPAS: Anomaly Detection in Fine-Grained Anatomy with Reconstruction-Based Semantic Patch-Scoring(https://arxiv.org/abs/2509.12905)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Early detection of newly emerging diseases, lesion severity assessment, differentiation of medical conditions and automated screening are examples for the wide applicability and importance of anomaly detection (AD) and unsupervised segmentation in medicine. Normal fine-grained tissue variability such as present in pulmonary anatomy is a major challenge for existing generative AD methods. Here, we propose a novel generative AD approach addressing this issue. It consists of an image-to-image translation for anomaly-free reconstruction and a subsequent patch similarity scoring between observed and generated image-pairs for precise anomaly localization. We validate the new method on chest computed tomography (CT) scans for the detection and segmentation of infectious disease lesions. To assess generalizability, we evaluate the method on an ischemic stroke lesion segmentation task in T1-weighted brain MRI. Results show improved pixel-level anomaly segmentation in both chest CTs and brain MRIs, with relative DICE score improvements of +1.9% and +4.4%, respectively, compared to other state-of-the-art reconstruction-based methods.</li>
</ul>

<h3>Title: All Roads Lead to Rome: Graph-Based Confidence Estimation for Large Language Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Caiqi Zhang, Chang Shu, Ehsan Shareghi, Nigel Collier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12908">https://arxiv.org/abs/2509.12908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12908">https://arxiv.org/pdf/2509.12908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12908]] All Roads Lead to Rome: Graph-Based Confidence Estimation for Large Language Model Reasoning(https://arxiv.org/abs/2509.12908)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Confidence estimation is essential for the reliable deployment of large language models (LLMs). Existing methods are primarily designed for factual QA tasks and often fail to generalize to reasoning tasks. To address this gap, we propose a set of training-free, graph-based confidence estimation methods tailored to reasoning tasks. Our approach models reasoning paths as directed graphs and estimates confidence by exploiting graph properties such as centrality, path convergence, and path weighting. Experiments with two LLMs on three reasoning datasets demonstrate improved confidence estimation and enhanced performance on two downstream tasks.</li>
</ul>

<h3>Title: T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking</h3>
<ul>
<li><strong>Authors: </strong>Hojat Ardi (1), Amir Jahanshahi (1), Ali Diba (2) ((1) Department of Electrical Engineering, Amirkabir University of Technology (AUT), Tehran, Iran (2) Qatar Computing Research Institute, Hamad Bin Khalifa University, Doha, Qatar)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12913">https://arxiv.org/abs/2509.12913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12913">https://arxiv.org/pdf/2509.12913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12913]] T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking(https://arxiv.org/abs/2509.12913)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Aerial object tracking remains a challenging task due to scale variations, dynamic backgrounds, clutter, and frequent occlusions. While most existing trackers emphasize spatial cues, they often overlook temporal dependencies, resulting in limited robustness in long-term tracking and under occlusion. Furthermore, correlation-based Siamese trackers are inherently constrained by the linear nature of correlation operations, making them ineffective against complex, non-linear appearance changes. To address these limitations, we introduce T-SiamTPN, a temporal-aware Siamese tracking framework that extends the SiamTPN architecture with explicit temporal modeling. Our approach incorporates temporal feature fusion and attention-based interactions, strengthening temporal consistency and enabling richer feature representations. These enhancements yield significant improvements over the baseline and achieve performance competitive with state-of-the-art trackers. Crucially, despite the added temporal modules, T-SiamTPN preserves computational efficiency. Deployed on the resource-constrained Jetson Nano, the tracker runs in real time at 7.1 FPS, demonstrating its suitability for real-world embedded applications without notable runtime overhead. Experimental results highlight substantial gains: compared to the baseline, T-SiamTPN improves success rate by 13.7% and precision by 14.7%. These findings underscore the importance of temporal modeling in Siamese tracking frameworks and establish T-SiamTPN as a strong and efficient solution for aerial object tracking. Code is available at: this https URL</li>
</ul>

<h3>Title: A Graph-Based Approach to Alert Contextualisation in Security Operations Centres</h3>
<ul>
<li><strong>Authors: </strong>Magnus Wiik Eckhoff, Peter Marius Flydal, Siem Peters, Martin Eian, Jonas Halvorsen, Vasileios Mavroeidis, Gudmund Grov</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12923">https://arxiv.org/abs/2509.12923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12923">https://arxiv.org/pdf/2509.12923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12923]] A Graph-Based Approach to Alert Contextualisation in Security Operations Centres(https://arxiv.org/abs/2509.12923)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Interpreting the massive volume of security alerts is a significant challenge in Security Operations Centres (SOCs). Effective contextualisation is important, enabling quick distinction between genuine threats and benign activity to prioritise what needs further this http URL paper proposes a graph-based approach to enhance alert contextualisation in a SOC by aggregating alerts into graph-based alert groups, where nodes represent alerts and edges denote relationships within defined time-windows. By grouping related alerts, we enable analysis at a higher abstraction level, capturing attack steps more effectively than individual alerts. Furthermore, to show that our format is well suited for downstream machine learning methods, we employ Graph Matching Networks (GMNs) to correlate incoming alert groups with historical incidents, providing analysts with additional insights.</li>
</ul>

<h3>Title: MATTER: Multiscale Attention for Registration Error Regression</h3>
<ul>
<li><strong>Authors: </strong>Shipeng Liu, Ziliang Xiong, Khac-Hoang Ngo, Per-Erik Forss√©n</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12924">https://arxiv.org/abs/2509.12924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12924">https://arxiv.org/pdf/2509.12924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12924]] MATTER: Multiscale Attention for Registration Error Regression(https://arxiv.org/abs/2509.12924)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Point cloud registration (PCR) is crucial for many downstream tasks, such as simultaneous localization and mapping (SLAM) and object tracking. This makes detecting and quantifying registration misalignment, i.e.,~{\it PCR quality validation}, an important task. All existing methods treat validation as a classification task, aiming to assign the PCR quality to a few classes. In this work, we instead use regression for PCR validation, allowing for a more fine-grained quantification of the registration quality. We also extend previously used misalignment-related features by using multiscale extraction and attention-based aggregation. This leads to accurate and robust registration error estimation on diverse datasets, especially for point clouds with heterogeneous spatial densities. Furthermore, when used to guide a mapping downstream task, our method significantly improves the mapping quality for a given amount of re-registered frames, compared to the state-of-the-art classification-based method.</li>
</ul>

<h3>Title: Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety</h3>
<ul>
<li><strong>Authors: </strong>Denis Janiak, Julia Moska, Dawid Motyka, Karolina Seweryn, Pawe≈Ç Walkowiak, Bartosz ≈ªuk, Arkadiusz Janz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12936">https://arxiv.org/abs/2509.12936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12936">https://arxiv.org/pdf/2509.12936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12936]] Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety(https://arxiv.org/abs/2509.12936)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) require careful alignment to balance competing objectives - factuality, safety, conciseness, proactivity, and diversity. Existing studies focus on individual techniques or specific dimensions, lacking a holistic assessment of the inherent trade-offs. We propose a unified evaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO) across these five axes, using both in-distribution and out-of-distribution datasets. Leveraging a specialized LLM-as-Judge prompt, validated through human studies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead in safety, and PPO best balances conciseness with proactivity. Our findings provide insights into trade-offs of common alignment methods, guiding the development of more balanced and reliable LLMs.</li>
</ul>

<h3>Title: Jailbreaking Large Language Models Through Content Concretization</h3>
<ul>
<li><strong>Authors: </strong>Johan Wahr√©us, Ahmed Hussain, Panos Papadimitratos</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12937">https://arxiv.org/abs/2509.12937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12937">https://arxiv.org/pdf/2509.12937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12937]] Jailbreaking Large Language Models Through Content Concretization(https://arxiv.org/abs/2509.12937)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly deployed for task automation and content generation, yet their safety mechanisms remain vulnerable to circumvention through different jailbreaking techniques. In this paper, we introduce \textit{Content Concretization} (CC), a novel jailbreaking technique that iteratively transforms abstract malicious requests into concrete, executable implementations. CC is a two-stage process: first, generating initial LLM responses using lower-tier, less constrained safety filters models, then refining them through higher-tier models that process both the preliminary output and original prompt. We evaluate our technique using 350 cybersecurity-specific prompts, demonstrating substantial improvements in jailbreak Success Rates (SRs), increasing from 7\% (no refinements) to 62\% after three refinement iterations, while maintaining a cost of 7.5\textcent~per prompt. Comparative A/B testing across nine different LLM evaluators confirms that outputs from additional refinement steps are consistently rated as more malicious and technically superior. Moreover, manual code analysis reveals that generated outputs execute with minimal modification, although optimal deployment typically requires target-specific fine-tuning. With eventual improved harmful code generation, these results highlight critical vulnerabilities in current LLM safety frameworks.</li>
</ul>

<h3>Title: Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian Splatting and Bag of Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Abdalla Arafa, Didier Stricker</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12938">https://arxiv.org/abs/2509.12938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12938">https://arxiv.org/pdf/2509.12938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12938]] Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian Splatting and Bag of Embeddings(https://arxiv.org/abs/2509.12938)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Novel view synthesis has seen significant advancements with 3D Gaussian Splatting (3DGS), enabling real-time photorealistic rendering. However, the inherent fuzziness of Gaussian Splatting presents challenges for 3D scene understanding, restricting its broader applications in AR/VR and robotics. While recent works attempt to learn semantics via 2D foundation model distillation, they inherit fundamental limitations: alpha blending averages semantics across objects, making 3D-level understanding impossible. We propose a paradigm-shifting alternative that bypasses differentiable rendering for semantics entirely. Our key insight is to leverage predecomposed object-level Gaussians and represent each object through multiview CLIP feature aggregation, creating comprehensive "bags of embeddings" that holistically describe objects. This allows: (1) accurate open-vocabulary object retrieval by comparing text queries to object-level (not Gaussian-level) embeddings, and (2) seamless task adaptation: propagating object IDs to pixels for 2D segmentation or to Gaussians for 3D extraction. Experiments demonstrate that our method effectively overcomes the challenges of 3D open-vocabulary object extraction while remaining comparable to state-of-the-art performance in 2D open-vocabulary segmentation, ensuring minimal compromise.</li>
</ul>

<h3>Title: Sy-FAR: Symmetry-based Fair Adversarial Robustness</h3>
<ul>
<li><strong>Authors: </strong>Haneen Najjar, Eyal Ronen, Mahmood Sharif</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12939">https://arxiv.org/abs/2509.12939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12939">https://arxiv.org/pdf/2509.12939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12939]] Sy-FAR: Symmetry-based Fair Adversarial Robustness(https://arxiv.org/abs/2509.12939)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, fair</a></li>
<li><strong>Abstract: </strong>Security-critical machine-learning (ML) systems, such as face-recognition systems, are susceptible to adversarial examples, including real-world physically realizable attacks. Various means to boost ML's adversarial robustness have been proposed; however, they typically induce unfair robustness: It is often easier to attack from certain classes or groups than from others. Several techniques have been developed to improve adversarial robustness while seeking perfect fairness between classes. Yet, prior work has focused on settings where security and fairness are less critical. Our insight is that achieving perfect parity in realistic fairness-critical tasks, such as face recognition, is often infeasible -- some classes may be highly similar, leading to more misclassifications between them. Instead, we suggest that seeking symmetry -- i.e., attacks from class $i$ to $j$ would be as successful as from $j$ to $i$ -- is more tractable. Intuitively, symmetry is a desirable because class resemblance is a symmetric relation in most domains. Additionally, as we prove theoretically, symmetry between individuals induces symmetry between any set of sub-groups, in contrast to other fairness notions where group-fairness is often elusive. We develop Sy-FAR, a technique to encourage symmetry while also optimizing adversarial robustness and extensively evaluate it using five datasets, with three model architectures, including against targeted and untargeted realistic attacks. The results show Sy-FAR significantly improves fair adversarial robustness compared to state-of-the-art methods. Moreover, we find that Sy-FAR is faster and more consistent across runs. Notably, Sy-FAR also ameliorates another type of unfairness we discover in this work -- target classes that adversarial examples are likely to be classified into become significantly less vulnerable after inducing symmetry.</li>
</ul>

<h3>Title: Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for Cultural Adaptations of Wine Reviews</h3>
<ul>
<li><strong>Authors: </strong>Chenye Zou, Xingyue Wen, Tianyi Hu, Qian Janice Wang, Daniel Hershcovich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12961">https://arxiv.org/abs/2509.12961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12961">https://arxiv.org/pdf/2509.12961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12961]] Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for Cultural Adaptations of Wine Reviews(https://arxiv.org/abs/2509.12961)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have opened the door to culture-aware language tasks. We introduce the novel problem of adapting wine reviews across Chinese and English, which goes beyond literal translation by incorporating regional taste preferences and culture-specific flavor descriptors. In a case study on cross-cultural wine review adaptation, we compile the first parallel corpus of professional reviews, containing 8k Chinese and 16k Anglophone reviews. We benchmark both neural-machine-translation baselines and state-of-the-art LLMs with automatic metrics and human evaluation. For the latter, we propose three culture-oriented criteria -- Cultural Proximity, Cultural Neutrality, and Cultural Genuineness -- to assess how naturally a translated review resonates with target-culture readers. Our analysis shows that current models struggle to capture cultural nuances, especially in translating wine descriptions across different cultures. This highlights the challenges and limitations of translation models in handling cultural content.</li>
</ul>

<h3>Title: MMMS: Multi-Modal Multi-Surface Interactive Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Robin Sch√∂n, Julian Lorenz, Katja Ludwig, Daniel Kienzle, Rainer Lienhart</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12963">https://arxiv.org/abs/2509.12963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12963">https://arxiv.org/pdf/2509.12963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12963]] MMMS: Multi-Modal Multi-Surface Interactive Segmentation(https://arxiv.org/abs/2509.12963)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we present a method to interactively create segmentation masks on the basis of user clicks. We pay particular attention to the segmentation of multiple surfaces that are simultaneously present in the same image. Since these surfaces may be heavily entangled and adjacent, we also present a novel extended evaluation metric that accounts for the challenges of this scenario. Additionally, the presented method is able to use multi-modal inputs to facilitate the segmentation task. At the center of this method is a network architecture which takes as input an RGB image, a number of non-RGB modalities, an erroneous mask, and encoded clicks. Based on this input, the network predicts an improved segmentation mask. We design our architecture such that it adheres to two conditions: (1) The RGB backbone is only available as a black-box. (2) To reduce the response time, we want our model to integrate the interaction-specific information after the image feature extraction and the multi-modal fusion. We refer to the overall task as Multi-Modal Multi-Surface interactive segmentation (MMMS). We are able to show the effectiveness of our multi-modal fusion strategy. Using additional modalities, our system reduces the NoC@90 by up to 1.28 clicks per surface on average on DeLiVER and up to 1.19 on MFNet. On top of this, we are able to show that our RGB-only baseline achieves competitive, and in some cases even superior performance when tested in a classical, single-mask interactive segmentation scenario.</li>
</ul>

<h3>Title: BAPFL: Exploring Backdoor Attacks Against Prototype-based Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Honghong Zeng, Jiong Lou, Zhe Wang, Hefeng Zhou, Chentao Wu, Wei Zhao, Jie Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12964">https://arxiv.org/abs/2509.12964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12964">https://arxiv.org/pdf/2509.12964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12964]] BAPFL: Exploring Backdoor Attacks Against Prototype-based Federated Learning(https://arxiv.org/abs/2509.12964)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, steal, federate</a></li>
<li><strong>Abstract: </strong>Prototype-based federated learning (PFL) has emerged as a promising paradigm to address data heterogeneity problems in federated learning, as it leverages mean feature vectors as prototypes to enhance model generalization. However, its robustness against backdoor attacks remains largely unexplored. In this paper, we identify that PFL is inherently resistant to existing backdoor attacks due to its unique prototype learning mechanism and local data heterogeneity. To further explore the security of PFL, we propose BAPFL, the first backdoor attack method specifically designed for PFL frameworks. BAPFL integrates a prototype poisoning strategy with a trigger optimization mechanism. The prototype poisoning strategy manipulates the trajectories of global prototypes to mislead the prototype training of benign clients, pushing their local prototypes of clean samples away from the prototypes of trigger-embedded samples. Meanwhile, the trigger optimization mechanism learns a unique and stealthy trigger for each potential target label, and guides the prototypes of trigger-embedded samples to align closely with the global prototype of the target label. Experimental results across multiple datasets and PFL variants demonstrate that BAPFL achieves a 35\%-75\% improvement in attack success rate compared to traditional backdoor attacks, while preserving main task accuracy. These results highlight the effectiveness, stealthiness, and adaptability of BAPFL in PFL.</li>
</ul>

<h3>Title: ICDAR 2025 Competition on FEw-Shot Text line segmentation of ancient handwritten documents (FEST)</h3>
<ul>
<li><strong>Authors: </strong>Silvia Zottin, Axel De Nardin, Giuseppe Branca, Claudio Piciarelli, Gian Luca Foresti</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12965">https://arxiv.org/abs/2509.12965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12965">https://arxiv.org/pdf/2509.12965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12965]] ICDAR 2025 Competition on FEw-Shot Text line segmentation of ancient handwritten documents (FEST)(https://arxiv.org/abs/2509.12965)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Text line segmentation is a critical step in handwritten document image analysis. Segmenting text lines in historical handwritten documents, however, presents unique challenges due to irregular handwriting, faded ink, and complex layouts with overlapping lines and non-linear text flow. Furthermore, the scarcity of large annotated datasets renders fully supervised learning approaches impractical for such materials. To address these challenges, we introduce the Few-Shot Text Line Segmentation of Ancient Handwritten Documents (FEST) Competition. Participants are tasked with developing systems capable of segmenting text lines in U-DIADS-TL dataset, using only three annotated images per manuscript for training. The competition dataset features a diverse collection of ancient manuscripts exhibiting a wide range of layouts, degradation levels, and non-standard formatting, closely reflecting real-world conditions. By emphasizing few-shot learning, FEST competition aims to promote the development of robust and adaptable methods that can be employed by humanities scholars with minimal manual annotation effort, thus fostering broader adoption of automated document analysis tools in historical research.</li>
</ul>

<h3>Title: Universal share based quantum multi secret image sharing scheme</h3>
<ul>
<li><strong>Authors: </strong>Dipak K. Rabari, Yogesh K. Meghrajani, Laxmi S. Desai</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12979">https://arxiv.org/abs/2509.12979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12979">https://arxiv.org/pdf/2509.12979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12979]] Universal share based quantum multi secret image sharing scheme(https://arxiv.org/abs/2509.12979)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust</a></li>
<li><strong>Abstract: </strong>Image security for information has become increasingly critical as internet become more prevalent due to hacking and unauthorized access. To ensure the security of confidential image data, image encryption using visual cryptography plays a crucial role. To share multiple images using visual cryptography, the company organizer utilizes the concept of a universal or common share. Likewise, quantum computing is an emerging technology that facilitates secure communication. The ability of quantum computers to solve certain mathematical problems efficiently threatens the security of many current encryption algorithms. Hence, to leverage the strengths of quantum computing and visual cryptography, this research introduces a novel universal share-based quantum multi-secret sharing technique for secure image communication. Quantum computing enables the scheme to exhibit high resilience to different eavesdropping threats. Consequently, the proposed method offers robust security solution for sharing confidential images across a range of applications, including enterprise data access and military communications.</li>
</ul>

<h3>Title: PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</h3>
<ul>
<li><strong>Authors: </strong>Xu Zheng, Chenfei Liao, Ziqiao Weng, Kaiyu Lei, Zihao Dongfang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Lu Qi, Li Chen, Danda Pani Paudel, Kailun Yang, Linfeng Zhang, Luc Van Gool, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12989">https://arxiv.org/abs/2509.12989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12989">https://arxiv.org/pdf/2509.12989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12989]] PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era(https://arxiv.org/abs/2509.12989)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Omnidirectional vision, using 360-degree vision to understand the environment, has become increasingly critical across domains like robotics, industrial inspection, and environmental monitoring. Compared to traditional pinhole vision, omnidirectional vision provides holistic environmental awareness, significantly enhancing the completeness of scene perception and the reliability of decision-making. However, foundational research in this area has historically lagged behind traditional pinhole vision. This talk presents an emerging trend in the embodied AI era: the rapid development of omnidirectional vision, driven by growing industrial demand and academic interest. We highlight recent breakthroughs in omnidirectional generation, omnidirectional perception, omnidirectional understanding, and related datasets. Drawing on insights from both academia and industry, we propose an ideal panoramic system architecture in the embodied AI era, PANORAMA, which consists of four key subsystems. Moreover, we offer in-depth opinions related to emerging trends and cross-community impacts at the intersection of panoramic vision and embodied AI, along with the future roadmap and open challenges. This overview synthesizes state-of-the-art advancements and outlines challenges and opportunities for future research in building robust, general-purpose omnidirectional AI systems in the embodied AI era.</li>
</ul>

<h3>Title: SitLLM: Large Language Models for Sitting Posture Health Understanding via Pressure Sensor Data</h3>
<ul>
<li><strong>Authors: </strong>Jian Gao, Fufangchen Zhao, Yiyang Zhang, Danfeng Yan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12994">https://arxiv.org/abs/2509.12994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12994">https://arxiv.org/pdf/2509.12994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12994]] SitLLM: Large Language Models for Sitting Posture Health Understanding via Pressure Sensor Data(https://arxiv.org/abs/2509.12994)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Poor sitting posture is a critical yet often overlooked factor contributing to long-term musculoskeletal disorders and physiological dysfunctions. Existing sitting posture monitoring systems, although leveraging visual, IMU, or pressure-based modalities, often suffer from coarse-grained recognition and lack the semantic expressiveness necessary for personalized feedback. In this paper, we propose \textbf{SitLLM}, a lightweight multimodal framework that integrates flexible pressure sensing with large language models (LLMs) to enable fine-grained posture understanding and personalized health-oriented response generation. SitLLM comprises three key components: (1) a \textit{Gaussian-Robust Sensor Embedding Module} that partitions pressure maps into spatial patches and injects local noise perturbations for robust feature extraction; (2) a \textit{Prompt-Driven Cross-Modal Alignment Module} that reprograms sensor embeddings into the LLM's semantic space via multi-head cross-attention using the pre-trained vocabulary embeddings; and (3) a \textit{Multi-Context Prompt Module} that fuses feature-level, structure-level, statistical-level, and semantic-level contextual information to guide instruction comprehension.</li>
</ul>

<h3>Title: ReTrack: Data Unlearning in Diffusion Models through Redirecting the Denoising Trajectory</h3>
<ul>
<li><strong>Authors: </strong>Qitan Shi, Cheng Jin, Jiawei Zhang, Yuantao Gu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13007">https://arxiv.org/abs/2509.13007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13007">https://arxiv.org/pdf/2509.13007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13007]] ReTrack: Data Unlearning in Diffusion Models through Redirecting the Denoising Trajectory(https://arxiv.org/abs/2509.13007)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models excel at generating high-quality, diverse images but suffer from training data memorization, raising critical privacy and safety concerns. Data unlearning has emerged to mitigate this issue by removing the influence of specific data without retraining from scratch. We propose ReTrack, a fast and effective data unlearning method for diffusion models. ReTrack employs importance sampling to construct a more efficient fine-tuning loss, which we approximate by retaining only dominant terms. This yields an interpretable objective that redirects denoising trajectories toward the $k$-nearest neighbors, enabling efficient unlearning while preserving generative quality. Experiments on MNIST T-Shirt, CelebA-HQ, CIFAR-10, and Stable Diffusion show that ReTrack achieves state-of-the-art performance, striking the best trade-off between unlearning strength and generation quality preservation.</li>
</ul>

<h3>Title: Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Gaofeng Liu, Hengsen Li, Ruoyu Gao, Xuetong Li, Zhiyuan Ma, Tao Fang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13013">https://arxiv.org/abs/2509.13013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13013">https://arxiv.org/pdf/2509.13013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13013]] Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single Image(https://arxiv.org/abs/2509.13013)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of 3D representation techniques and generative models, substantial progress has been made in reconstructing full-body 3D avatars from a single image. However, this task remains fundamentally ill-posedness due to the limited information available from monocular input, making it difficult to control the geometry and texture of occluded regions during generation. To address these challenges, we redesign the reconstruction pipeline and propose Dream3DAvatar, an efficient and text-controllable two-stage framework for 3D avatar generation. In the first stage, we develop a lightweight, adapter-enhanced multi-view generation model. Specifically, we introduce the Pose-Adapter to inject SMPL-X renderings and skeletal information into SDXL, enforcing geometric and pose consistency across views. To preserve facial identity, we incorporate ID-Adapter-G, which injects high-resolution facial features into the generation process. Additionally, we leverage BLIP2 to generate high-quality textual descriptions of the multi-view images, enhancing text-driven controllability in occluded regions. In the second stage, we design a feedforward Transformer model equipped with a multi-view feature fusion module to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS) from the generated images. Furthermore, we introduce ID-Adapter-R, which utilizes a gating mechanism to effectively fuse facial features into the reconstruction process, improving high-frequency detail recovery. Extensive experiments demonstrate that our method can generate realistic, animation-ready 3D avatars without any post-processing and consistently outperforms existing baselines across multiple evaluation metrics.</li>
</ul>

<h3>Title: Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual Reasoning in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yan Chen, Long Li, Teng Xi, Long Zeng, Jingdong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13031">https://arxiv.org/abs/2509.13031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13031">https://arxiv.org/pdf/2509.13031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13031]] Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual Reasoning in Vision-Language Models(https://arxiv.org/abs/2509.13031)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has proven highly effective in eliciting the reasoning capabilities of large language models (LLMs). Inspired by this success, recent studies have explored applying similar techniques to vision-language models (VLMs), aiming to enhance their reasoning performance. However, directly transplanting RL methods from LLMs to VLMs is suboptimal, as the tasks faced by VLMs are inherently more complex. Specifically, VLMs must first accurately perceive and understand visual inputs before reasoning can be effectively performed. To address this challenge, we propose a two-stage reinforcement learning framework designed to jointly enhance both the perceptual and reasoning capabilities of VLMs. To mitigate the vanishing advantage issue commonly observed in RL training, we first perform dataset-level sampling to selectively strengthen specific capabilities using distinct data sources. During training, the first stage focuses on improving the model's visual perception through coarse- and fine-grained visual understanding, while the second stage targets the enhancement of reasoning abilities. After the proposed two-stage reinforcement learning process, we obtain PeBR-R1, a vision-language model with significantly enhanced perceptual and reasoning capabilities. Experimental results on seven benchmark datasets demonstrate the effectiveness of our approach and validate the superior performance of PeBR-R1 across diverse visual reasoning tasks.</li>
</ul>

<h3>Title: Bridging Threat Models and Detections: Formal Verification via CADP</h3>
<ul>
<li><strong>Authors: </strong>Dumitru-Bogdan Prelipcean (Bitdefender, Ia≈üi, Romania, Alexandru Ioan Cuza University, Iasi, Romania, LACL, Universite Paris-Est Creteil, France), CƒÉtƒÉlin Dima (LACL, Universit√© Paris-Est Cr√©t√©il, France)</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13035">https://arxiv.org/abs/2509.13035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13035">https://arxiv.org/pdf/2509.13035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13035]] Bridging Threat Models and Detections: Formal Verification via CADP(https://arxiv.org/abs/2509.13035)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Threat detection systems rely on rule-based logic to identify adversarial behaviors, yet the conformance of these rules to high-level threat models is rarely verified formally. We present a formal verification framework that models both detection logic and attack trees as labeled transition systems (LTSs), enabling automated conformance checking via bisimulation and weak trace inclusion. Detection rules specified in the Generic Threat Detection Language (GTDL, a general-purpose detection language we formalize in this work) are assigned a compositional operational semantics, and threat models expressed as attack trees are interpreted as LTSs through a structural trace semantics. Both representations are translated to LNT, a modeling language supported by the CADP toolbox. This common semantic domain enables systematic and automated verification of detection coverage. We evaluate our approach on real-world malware scenarios such as LokiBot and Emotet and provide scalability analysis through parametric synthetic models. Results confirm that our methodology identifies semantic mismatches between threat models and detection rules, supports iterative refinement, and scales to realistic threat landscapes.</li>
</ul>

<h3>Title: MIA-EPT: Membership Inference Attack via Error Prediction for Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Eyal German, Daniel Samira, Yuval Elovici, Asaf Shabtai</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13046">https://arxiv.org/abs/2509.13046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13046">https://arxiv.org/pdf/2509.13046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13046]] MIA-EPT: Membership Inference Attack via Error Prediction for Tabular Data(https://arxiv.org/abs/2509.13046)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Synthetic data generation plays an important role in enabling data sharing, particularly in sensitive domains like healthcare and finance. Recent advances in diffusion models have made it possible to generate realistic, high-quality tabular data, but they may also memorize training records and leak sensitive information. Membership inference attacks (MIAs) exploit this vulnerability by determining whether a record was used in training. While MIAs have been studied in images and text, their use against tabular diffusion models remains underexplored despite the unique risks of structured attributes and limited record diversity. In this paper, we introduce MIAEPT, Membership Inference Attack via Error Prediction for Tabular Data, a novel black-box attack specifically designed to target tabular diffusion models. MIA-EPT constructs errorbased feature vectors by masking and reconstructing attributes of target records, disclosing membership signals based on how well these attributes are predicted. MIA-EPT operates without access to the internal components of the generative model, relying only on its synthetic data output, and was shown to generalize across multiple state-of-the-art diffusion models. We validate MIA-EPT on three diffusion-based synthesizers, achieving AUC-ROC scores of up to 0.599 and TPR@10% FPR values of 22.0% in our internal tests. Under the MIDST 2025 competition conditions, MIA-EPT achieved second place in the Black-box Multi-Table track (TPR@10% FPR = 20.0%). These results demonstrate that our method can uncover substantial membership leakage in synthetic tabular data, challenging the assumption that synthetic data is inherently privacy-preserving. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Multi-Model Synthetic Training for Mission-Critical Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nolan Platt, Pragyansmita Nayak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13047">https://arxiv.org/abs/2509.13047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13047">https://arxiv.org/pdf/2509.13047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13047]] Multi-Model Synthetic Training for Mission-Critical Small Language Models(https://arxiv.org/abs/2509.13047)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities across many domains, yet their appli- cation to specialized fields remains constrained by the scarcity and complexity of domain-specific training data. We present a novel approach that achieves a 261x cost reduction for maritime intelligence by using LLMs as one-time teachers rather than using them directly for inference. Our method transforms 3.2 billion Automatic Identification System (AIS) vessel tracking records into 21,543 synthetic question and answer pairs through multi-model generation (GPT-4o and o3-mini), preventing over- fitting and ensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves 75% accuracy on maritime tasks, while being substantially cheaper than using a larger model for inference. We show that smaller, cheaper models - when fine tuned properly - can provide similar accuracy compared to larger models that are prohibitively expensive. Our work contributes to the growing field of synthetic dataset generation for specialized AI applications and presents a highly reproducible framework for domains where manual annotation is infeasible. Beyond expand- ing research in the growing field of specialized small language models, our approach has immediate applications in maritime safety, security operations, and vessel traffic management systems in various industries.</li>
</ul>

<h3>Title: SLasH-DSA: Breaking SLH-DSA Using an Extensible End-To-End Rowhammer Framework</h3>
<ul>
<li><strong>Authors: </strong>Jeremy Boy, Antoon Purnal, Anna P√§tschke, Luca Wilke, Thomas Eisenbarth</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13048">https://arxiv.org/abs/2509.13048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13048">https://arxiv.org/pdf/2509.13048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13048]] SLasH-DSA: Breaking SLH-DSA Using an Extensible End-To-End Rowhammer Framework(https://arxiv.org/abs/2509.13048)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>As quantum computing advances, PQC schemes are adopted to replace classical algorithms. Among them is the SLH-DSA that was recently standardized by NIST and is favored for its conservative security foundations. In this work, we present the first software-only universal forgery attack on SLH-DSA, leveraging Rowhammer-induced bit flips to corrupt the internal state and forge signatures. While prior work targeted embedded systems and required physical access, our attack is software-only, targeting commodity desktop and server hardware, significantly broadening the threat model. We demonstrate a full end-to-end attack against all security levels of SLH-DSA in OpenSSL 3.5.1, achieving universal forgery for the highest security level after eight hours of hammering and 36 seconds of post-processing. Our post-processing is informed by a novel complexity analysis that, given a concrete set of faulty signatures, identifies the most promising computational path to pursue. To enable the attack, we introduce Swage, a modular and extensible framework for implementing end-to-end Rowhammer-based fault attacks. Swage abstracts and automates key components of practical Rowhammer attacks. Unlike prior tooling, Swage is untangled from the attacked code, making it reusable and suitable for frictionless analysis of different targets. Our findings highlight that even theoretically sound PQC schemes can fail under real-world conditions, underscoring the need for additional implementation hardening or hardware defenses against Rowhammer.</li>
</ul>

<h3>Title: TFANet: Three-Stage Image-Text Feature Alignment Network for Robust Referring Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Qianqi Lu, Yuxiang Xie, Jing Zhang, Shiwei Zou, Yan Chen, Xidao Luan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13070">https://arxiv.org/abs/2509.13070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13070">https://arxiv.org/pdf/2509.13070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13070]] TFANet: Three-Stage Image-Text Feature Alignment Network for Robust Referring Image Segmentation(https://arxiv.org/abs/2509.13070)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Referring Image Segmentation (RIS) is a task that segments image regions based on language expressions, requiring fine-grained alignment between two modalities. However, existing methods often struggle with multimodal misalignment and language semantic loss, especially in complex scenes containing multiple visually similar objects, where uniquely described targets are frequently mislocalized or incompletely segmented. To tackle these challenges, this paper proposes TFANet, a Three-stage Image-Text Feature Alignment Network that systematically enhances multimodal alignment through a hierarchical framework comprising three stages: Knowledge Plus Stage (KPS), Knowledge Fusion Stage (KFS), and Knowledge Intensification Stage (KIS). In the first stage, we design the Multiscale Linear Cross-Attention Module (MLAM), which facilitates bidirectional semantic exchange between visual features and textual representations across multiple scales. This establishes rich and efficient alignment between image regions and different granularities of linguistic descriptions. Subsequently, the KFS further strengthens feature alignment through the Cross-modal Feature Scanning Module (CFSM), which applies multimodal selective scanning to capture long-range dependencies and construct a unified multimodal representation. This is essential for modeling long-range cross-modal dependencies and enhancing alignment accuracy in complex scenes. Finally, in the KIS, we propose the Word-level Linguistic Feature-guided Semantic Deepening Module (WFDM) to compensate for semantic degradation introduced in earlier stages.</li>
</ul>

<h3>Title: Digital Sovereignty Control Framework for Military AI-based Cyber Security</h3>
<ul>
<li><strong>Authors: </strong>Clara Maathuis, Kasper Cools</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13072">https://arxiv.org/abs/2509.13072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13072">https://arxiv.org/pdf/2509.13072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13072]] Digital Sovereignty Control Framework for Military AI-based Cyber Security(https://arxiv.org/abs/2509.13072)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>In today's evolving threat landscape, ensuring digital sovereignty has become mandatory for military organizations, especially given their increased development and investment in AI-driven cyber security solutions. To this end, a multi-angled framework is proposed in this article in order to define and assess digital sovereign control of data and AI-based models for military cyber security. This framework focuses on aspects such as context, autonomy, stakeholder involvement, and mitigation of risks in this domain. Grounded on the concepts of digital sovereignty and data sovereignty, the framework aims to protect sensitive defence assets against threats such as unauthorized access, ransomware, and supply-chain attacks. This approach reflects the multifaceted nature of digital sovereignty by preserving operational autonomy, assuring security and safety, securing privacy, and fostering ethical compliance of both military systems and decision-makers. At the same time, the framework addresses interoperability challenges among allied forces, strategic and legal considerations, and the integration of emerging technologies by considering a multidisciplinary approach that enhances the resilience and preservation of control over (critical) digital assets. This is done by adopting a design oriented research where systematic literature review is merged with critical thinking and analysis of field incidents in order to assure the effectivity and realism of the framework proposed.</li>
</ul>

<h3>Title: When Inverse Data Outperforms: Exploring the Pitfalls of Mixed Data in Multi-Stage Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Mengyi Deng, Xin Li, Tingyu Zhu, Zhicheng Yang, Zhijiang Guo, Wei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13079">https://arxiv.org/abs/2509.13079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13079">https://arxiv.org/pdf/2509.13079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13079]] When Inverse Data Outperforms: Exploring the Pitfalls of Mixed Data in Multi-Stage Fine-Tuning(https://arxiv.org/abs/2509.13079)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing work has shown that o1-level performance can be achieved with limited data distillation, but most existing methods focus on unidirectional supervised fine-tuning (SFT), overlooking the intricate interplay between diverse reasoning patterns. In this paper, we construct r1k, a high-quality reverse reasoning dataset derived by inverting 1,000 forward examples from s1k, and examine how SFT and Direct Preference Optimization (DPO) affect alignment under bidirectional reasoning objectives. SFT on r1k yields a 1.6%--6.8% accuracy improvement over s1k across evaluated benchmarks. However, naively mixing forward and reverse data during SFT weakens the directional distinction. Although DPO can partially recover this distinction, it also suppresses less preferred reasoning paths by shifting the probability mass toward irrelevant outputs. These findings suggest that mixed reasoning data introduce conflicting supervision signals, underscoring the need for robust and direction-aware alignment strategies.</li>
</ul>

<h3>Title: Shaping Explanations: Semantic Reward Modeling with Encoder-Only Transformers for GRPO</h3>
<ul>
<li><strong>Authors: </strong>Francesco Pappone, Ruggero Marino Lazzaroni, Federico Califano, Niccol√≤ Gentile, Roberto Marras</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13081">https://arxiv.org/abs/2509.13081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13081">https://arxiv.org/pdf/2509.13081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13081]] Shaping Explanations: Semantic Reward Modeling with Encoder-Only Transformers for GRPO(https://arxiv.org/abs/2509.13081)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) excel at generating human-like text, aligning their outputs with complex, qualitative goals like pedagogical soundness remains a significant challenge. Standard reinforcement learning techniques often rely on slow and expensive LLM-as-a-judge evaluations or on brittle, keyword-based metrics like ROUGE, which fail to capture the semantic essence of a high-quality explanation. In this work, we introduce a novel approach to reward shaping within the Group Relative Policy Optimisation (GRPO) framework. Our central contribution is the use of a small, efficient encoder-only transformer as a semantic reward model. This model provides a dense, semantically rich reward signal based on the cosine similarity between a generated explanation and a ground-truth reference, guiding the policy towards explanations that are not just factually correct but also structurally and conceptually aligned with expert reasoning. We apply this method to the task of training a model for the Italian medical-school entrance examinations, following standard domain-adaptive continued pre-training (CPT) and supervised fine-tuning (SFT). Our results demonstrate that GRPO with our proposed semantic reward significantly improves explanation faithfulness and clarity over a strong SFT baseline, showcasing the power of using lightweight encoder models for nuanced reward shaping in complex generation tasks</li>
</ul>

<h3>Title: Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Yan Xingyang, Huang Xiaohong, Zhang Zhao, You Tian, Xu Ziheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13083">https://arxiv.org/abs/2509.13083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13083">https://arxiv.org/pdf/2509.13083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13083]] Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement(https://arxiv.org/abs/2509.13083)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the Fourier domain, luminance information is primarily encoded in the amplitude spectrum, while spatial structures are captured in the phase components. The traditional Fourier Frequency information fitting employs pixel-wise loss functions, which tend to focus excessively on local information and may lead to global information loss. In this paper, we present LLFDisc, a U-shaped deep enhancement network that integrates cross-attention and gating mechanisms tailored for frequency-aware enhancement. We propose a novel distribution-aware loss that directly fits the Fourier-domain information and minimizes their divergence using a closed-form KL-Divergence objective. This enables the model to align Fourier-domain information more robustly than with conventional MSE-based losses. Furthermore, we enhance the perceptual loss based on VGG by embedding KL-Divergence on extracted deep features, enabling better structural fidelity. Extensive experiments across multiple benchmarks demonstrate that LLFDisc achieves state-of-the-art performance in both qualitative and quantitative evaluations. Our code will be released at: this https URL</li>
</ul>

<h3>Title: Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation with Uncertainty-Guided Pseudo-Labeling</h3>
<ul>
<li><strong>Authors: </strong>Yunyao Lu, Yihang Wu, Ahmad Chaddad, Tareef Daqqaq, Reem Kateb</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13084">https://arxiv.org/abs/2509.13084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13084">https://arxiv.org/pdf/2509.13084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13084]] Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation with Uncertainty-Guided Pseudo-Labeling(https://arxiv.org/abs/2509.13084)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Despite the remarkable performance of supervised medical image segmentation models, relying on a large amount of labeled data is impractical in real-world situations. Semi-supervised learning approaches aim to alleviate this challenge using unlabeled data through pseudo-label generation. Yet, existing semi-supervised segmentation methods still suffer from noisy pseudo-labels and insufficient supervision within the feature space. To solve these challenges, this paper proposes a novel semi-supervised 3D medical image segmentation framework based on a dual-network architecture. Specifically, we investigate a Cross Consistency Enhancement module using both cross pseudo and entropy-filtered supervision to reduce the noisy pseudo-labels, while we design a dynamic weighting strategy to adjust the contributions of pseudo-labels using an uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). In addition, we use a self-supervised contrastive learning mechanism to align uncertain voxel features with reliable class prototypes by effectively differentiating between trustworthy and uncertain predictions, thus reducing prediction uncertainty. Extensive experiments are conducted on three 3D segmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposed approach consistently exhibits superior performance across various settings (e.g., 89.95\% Dice score on left Atrial with 10\% labeled data) compared to the state-of-the-art methods. Furthermore, the usefulness of the proposed modules is further validated via ablation experiments.</li>
</ul>

<h3>Title: Hierarchical Deep Fusion Framework for Multi-dimensional Facial Forgery Detection - The 2024 Global Deepfake Image Detection Challenge</h3>
<ul>
<li><strong>Authors: </strong>Kohou Wang, Huan Hu, Xiang Liu, Zezhou Chen, Ping Chen, Zhaoxiang Liu, Shiguo Lian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13107">https://arxiv.org/abs/2509.13107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13107">https://arxiv.org/pdf/2509.13107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13107]] Hierarchical Deep Fusion Framework for Multi-dimensional Facial Forgery Detection - The 2024 Global Deepfake Image Detection Challenge(https://arxiv.org/abs/2509.13107)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>The proliferation of sophisticated deepfake technology poses significant challenges to digital security and authenticity. Detecting these forgeries, especially across a wide spectrum of manipulation techniques, requires robust and generalized models. This paper introduces the Hierarchical Deep Fusion Framework (HDFF), an ensemble-based deep learning architecture designed for high-performance facial forgery detection. Our framework integrates four diverse pre-trained sub-models, Swin-MLP, CoAtNet, EfficientNetV2, and DaViT, which are meticulously fine-tuned through a multi-stage process on the MultiFFDI dataset. By concatenating the feature representations from these specialized models and training a final classifier layer, HDFF effectively leverages their collective strengths. This approach achieved a final score of 0.96852 on the competition's private leaderboard, securing the 20th position out of 184 teams, demonstrating the efficacy of hierarchical fusion for complex image classification tasks.</li>
</ul>

<h3>Title: Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Ruibo Li, Hanyu Shi, Zhe Wang, Guosheng Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13116">https://arxiv.org/abs/2509.13116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13116">https://arxiv.org/pdf/2509.13116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13116]] Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving(https://arxiv.org/abs/2509.13116)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Understanding motion in dynamic environments is critical for autonomous driving, thereby motivating research on class-agnostic motion prediction. In this work, we investigate weakly and self-supervised class-agnostic motion prediction from LiDAR point clouds. Outdoor scenes typically consist of mobile foregrounds and static backgrounds, allowing motion understanding to be associated with scene parsing. Based on this observation, we propose a novel weakly supervised paradigm that replaces motion annotations with fully or partially annotated (1%, 0.1%) foreground/background masks for supervision. To this end, we develop a weakly supervised approach utilizing foreground/background cues to guide the self-supervised learning of motion prediction models. Since foreground motion generally occurs in non-ground regions, non-ground/ground masks can serve as an alternative to foreground/background masks, further reducing annotation effort. Leveraging non-ground/ground cues, we propose two additional approaches: a weakly supervised method requiring fewer (0.01%) foreground/background annotations, and a self-supervised method without annotations. Furthermore, we design a Robust Consistency-aware Chamfer Distance loss that incorporates multi-frame information and robust penalty functions to suppress outliers in self-supervised learning. Experiments show that our weakly and self-supervised models outperform existing self-supervised counterparts, and our weakly supervised models even rival some supervised ones. This demonstrates that our approaches effectively balance annotation effort and performance.</li>
</ul>

<h3>Title: Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning</h3>
<ul>
<li><strong>Authors: </strong>Sijia Cui, Shuai Xu, Aiyao He, Yanna Wang, Bo Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13127">https://arxiv.org/abs/2509.13127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13127">https://arxiv.org/pdf/2509.13127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13127]] Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning(https://arxiv.org/abs/2509.13127)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models(LLMs) have led to the development of LLM-based AI agents. A key challenge is the creation of agents that can effectively ground themselves in complex, adversarial long-horizon environments. Existing methods mainly focus on (1) using LLMs as policies to interact with the environment through generating low-level feasible actions, and (2) utilizing LLMs to generate high-level tasks or language guides to stimulate action generation. However, the former struggles to generate reliable actions, while the latter relies heavily on expert experience to translate high-level tasks into specific action sequences. To address these challenges, we introduce the Plan with Language, Act with Parameter (PLAP) planning framework that facilitates the grounding of LLM-based agents in long-horizon environments. The PLAP method comprises three key components: (1) a skill library containing environment-specific parameterized skills, (2) a skill planner powered by LLMs, and (3) a skill executor converting the parameterized skills into executable action sequences. We implement PLAP in MicroRTS, a long-horizon real-time strategy game that provides an unfamiliar and challenging environment for LLMs. The experimental results demonstrate the effectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting outperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully crafted few-shot examples, surpasses the top-tier scripted agent, CoacAI. Additionally, we design comprehensive evaluation metrics and test 6 closed-source and 2 open-source LLMs within the PLAP framework, ultimately releasing an LLM leaderboard ranking long-horizon skill planning ability. Our code is available at this https URL.</li>
</ul>

<h3>Title: Discovering Mathematical Equations with Diffusion Language Model</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxu Han, Chengzhen Ning, Jinghui Zhong, Fubiao Yang, Yu Wang, Xin Mu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13136">https://arxiv.org/abs/2509.13136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13136">https://arxiv.org/pdf/2509.13136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13136]] Discovering Mathematical Equations with Diffusion Language Model(https://arxiv.org/abs/2509.13136)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discovering valid and meaningful mathematical equations from observed data plays a crucial role in scientific discovery. While this task, symbolic regression, remains challenging due to the vast search space and the trade-off between accuracy and complexity. In this paper, we introduce DiffuSR, a pre-training framework for symbolic regression built upon a continuous-state diffusion language model. DiffuSR employs a trainable embedding layer within the diffusion process to map discrete mathematical symbols into a continuous latent space, modeling equation distributions effectively. Through iterative denoising, DiffuSR converts an initial noisy sequence into a symbolic equation, guided by numerical data injected via a cross-attention mechanism. We also design an effective inference strategy to enhance the accuracy of the diffusion-based equation generator, which injects logit priors into genetic programming. Experimental results on standard symbolic regression benchmarks demonstrate that DiffuSR achieves competitive performance with state-of-the-art autoregressive methods and generates more interpretable and diverse mathematical expressions.</li>
</ul>

<h3>Title: MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation</h3>
<ul>
<li><strong>Authors: </strong>Minqing Huang, Shouyi Lu, Boyuan Zheng, Ziyao Li, Xiao Tang, Guirong Zhuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13149">https://arxiv.org/abs/2509.13149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13149">https://arxiv.org/pdf/2509.13149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13149]] MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation(https://arxiv.org/abs/2509.13149)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>4D radar super-resolution, which aims to reconstruct sparse and noisy point clouds into dense and geometrically consistent representations, is a foundational problem in autonomous perception. However, existing methods often suffer from high training cost or rely on complex diffusion-based sampling, resulting in high inference latency and poor generalization, making it difficult to balance accuracy and efficiency. To address these limitations, we propose MSDNet, a multi-stage distillation framework that efficiently transfers dense LiDAR priors to 4D radar features to achieve both high reconstruction quality and computational efficiency. The first stage performs reconstruction-guided feature distillation, aligning and densifying the student's features through feature reconstruction. In the second stage, we propose diffusion-guided feature distillation, which treats the stage-one distilled features as a noisy version of the teacher's representations and refines them via a lightweight diffusion network. Furthermore, we introduce a noise adapter that adaptively aligns the noise level of the feature with a predefined diffusion timestep, enabling a more precise denoising. Extensive experiments on the VoD and in-house datasets demonstrate that MSDNet achieves both high-fidelity reconstruction and low-latency inference in the task of 4D radar point cloud super-resolution, and consistently improves performance on downstream tasks. The code will be publicly available upon publication.</li>
</ul>

<h3>Title: TexTAR : Textual Attribute Recognition in Multi-domain and Multi-lingual Document Images</h3>
<ul>
<li><strong>Authors: </strong>Rohan Kumar, Jyothi Swaroopa Jinka, Ravi Kiran Sarvadevabhatla</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13151">https://arxiv.org/abs/2509.13151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13151">https://arxiv.org/pdf/2509.13151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13151]] TexTAR : Textual Attribute Recognition in Multi-domain and Multi-lingual Document Images(https://arxiv.org/abs/2509.13151)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recognizing textual attributes such as bold, italic, underline and strikeout is essential for understanding text semantics, structure, and visual presentation. These attributes highlight key information, making them crucial for document analysis. Existing methods struggle with computational efficiency or adaptability in noisy, multilingual settings. To address this, we introduce TexTAR, a multi-task, context-aware Transformer for Textual Attribute Recognition (TAR). Our novel data selection pipeline enhances context awareness, and our architecture employs a 2D RoPE (Rotary Positional Embedding)-style mechanism to incorporate input context for more accurate attribute predictions. We also introduce MMTAD, a diverse, multilingual, multi-domain dataset annotated with text attributes across real-world documents such as legal records, notices, and textbooks. Extensive evaluations show TexTAR outperforms existing methods, demonstrating that contextual awareness contributes to state-of-the-art TAR performance.</li>
</ul>

<h3>Title: LLM Hallucination Detection: A Fast Fourier Transform Method Based on Hidden Layer Temporal Signals</h3>
<ul>
<li><strong>Authors: </strong>Jinxin Li, Gang Tu, ShengYu Cheng, Junjie Hu, Jinting Wang, Rui Chen, Zhilong Zhou, Dongbo Shan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13154">https://arxiv.org/abs/2509.13154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13154">https://arxiv.org/pdf/2509.13154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13154]] LLM Hallucination Detection: A Fast Fourier Transform Method Based on Hidden Layer Temporal Signals(https://arxiv.org/abs/2509.13154)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Hallucination remains a critical barrier for deploying large language models (LLMs) in reliability-sensitive applications. Existing detection methods largely fall into two categories: factuality checking, which is fundamentally constrained by external knowledge coverage, and static hidden-state analysis, that fails to capture deviations in reasoning dynamics. As a result, their effectiveness and robustness remain limited. We propose HSAD (Hidden Signal Analysis-based Detection), a novel hallucination detection framework that models the temporal dynamics of hidden representations during autoregressive generation. HSAD constructs hidden-layer signals by sampling activations across layers, applies Fast Fourier Transform (FFT) to obtain frequency-domain representations, and extracts the strongest non-DC frequency component as spectral features. Furthermore, by leveraging the autoregressive nature of LLMs, HSAD identifies optimal observation points for effective and reliable detection. Across multiple benchmarks, including TruthfulQA, HSAD achieves over 10 percentage points improvement compared to prior state-of-the-art methods. By integrating reasoning-process modeling with frequency-domain analysis, HSAD establishes a new paradigm for robust hallucination detection in LLMs.</li>
</ul>

<h3>Title: Enhancing Video Large Language Models with Structured Multi-Video Collaborative Reasoning (early version)</h3>
<ul>
<li><strong>Authors: </strong>Zhihao He, Tianyao He, Tieyuan Chen, Yun Xu, Huabin Liu, Chaofan Gan, Gui Zou, Weiyao Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13161">https://arxiv.org/abs/2509.13161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13161">https://arxiv.org/pdf/2509.13161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13161]] Enhancing Video Large Language Models with Structured Multi-Video Collaborative Reasoning (early version)(https://arxiv.org/abs/2509.13161)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the prosperity of the video language model, the current pursuit of comprehensive video reasoning is thwarted by the inherent spatio-temporal incompleteness within individual videos, resulting in hallucinations and inaccuracies. A promising solution is to augment the reasoning performance with multiple related videos. However, video tokens are numerous and contain redundant information, so directly feeding the relevant video data into a large language model to enhance responses could be counterproductive. To address this challenge, we propose a multi-video collaborative framework for video language models. For efficient and flexible video representation, we establish a Video Structuring Module to represent the video's knowledge as a spatio-temporal graph. Based on the structured video representation, we design the Graph Fusion Module to fuse the structured knowledge and valuable information from related videos into the augmented graph node tokens. Finally, we construct an elaborate multi-video structured prompt to integrate the graph, visual, and textual tokens as the input to the large language model. Extensive experiments substantiate the effectiveness of our framework, showcasing its potential as a promising avenue for advancing video language models.</li>
</ul>

<h3>Title: On the Correlation between Individual Fairness and Predictive Accuracy in Probabilistic Models</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Antonucci, Eric Rossetto, Ivan Duvnjak</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13165">https://arxiv.org/abs/2509.13165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13165">https://arxiv.org/pdf/2509.13165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13165]] On the Correlation between Individual Fairness and Predictive Accuracy in Probabilistic Models(https://arxiv.org/abs/2509.13165)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, generative</a></li>
<li><strong>Abstract: </strong>We investigate individual fairness in generative probabilistic classifiers by analysing the robustness of posterior inferences to perturbations in private features. Building on established results in robustness analysis, we hypothesise a correlation between robustness and predictive accuracy, specifically, instances exhibiting greater robustness are more likely to be classified accurately. We empirically assess this hypothesis using a benchmark of fourteen datasets with fairness concerns, employing Bayesian networks as the underlying generative models. To address the computational complexity associated with robustness analysis over multiple private features with Bayesian networks, we reformulate the problem as a most probable explanation task in an auxiliary Markov random field. Our experiments confirm the hypothesis about the correlation, suggesting novel directions to mitigate the traditional trade-off between fairness and accuracy.</li>
</ul>

<h3>Title: WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory</h3>
<ul>
<li><strong>Authors: </strong>Ruifei Ding, Zhe Chen, Wen Fan, Chen Long, Huijuan Xiao, Yelu Zeng, Zhen Dong, Bisheng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13172">https://arxiv.org/abs/2509.13172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13172">https://arxiv.org/pdf/2509.13172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13172]] WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory(https://arxiv.org/abs/2509.13172)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Street trees are vital to urban livability, providing ecological and social benefits. Establishing a detailed, accurate, and dynamically updated street tree inventory has become essential for optimizing these multifunctional assets within space-constrained urban environments. Given that traditional field surveys are time-consuming and labor-intensive, automated surveys utilizing Mobile Mapping Systems (MMS) offer a more efficient solution. However, existing MMS-acquired tree datasets are limited by small-scale scene, limited annotation, or single modality, restricting their utility for comprehensive analysis. To address these limitations, we introduce WHU-STree, a cross-city, richly annotated, and multi-modal urban street tree dataset. Collected across two distinct cities, WHU-STree integrates synchronized point clouds and high-resolution images, encompassing 21,007 annotated tree instances across 50 species and 2 morphological parameters. Leveraging the unique characteristics, WHU-STree concurrently supports over 10 tasks related to street tree inventory. We benchmark representative baselines for two key tasks--tree species classification and individual tree segmentation. Extensive experiments and in-depth analysis demonstrate the significant potential of multi-modal data fusion and underscore cross-domain applicability as a critical prerequisite for practical algorithm deployment. In particular, we identify key challenges and outline potential future works for fully exploiting WHU-STree, encompassing multi-modal fusion, multi-task collaboration, cross-domain generalization, spatial pattern learning, and Multi-modal Large Language Model for street tree asset management. The WHU-STree dataset is accessible at: this https URL.</li>
</ul>

<h3>Title: More performant and scalable: Rethinking contrastive vision-language pre-training of radiology in the LLM era</h3>
<ul>
<li><strong>Authors: </strong>Yingtai Li, Haoran Lai, Xiaoqian Zhou, Shuai Ming, Wenxin Ma, Wei Wei, Shaohua Kevin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13175">https://arxiv.org/abs/2509.13175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13175">https://arxiv.org/pdf/2509.13175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13175]] More performant and scalable: Rethinking contrastive vision-language pre-training of radiology in the LLM era(https://arxiv.org/abs/2509.13175)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The emergence of Large Language Models (LLMs) presents unprecedented opportunities to revolutionize medical contrastive vision-language pre-training. In this paper, we show how LLMs can facilitate large-scale supervised pre-training, thereby advancing vision-language alignment. We begin by demonstrate that modern LLMs can automatically extract diagnostic labels from radiology reports with remarkable precision (>96\% AUC in our experiments) without complex prompt engineering, enabling the creation of large-scale "silver-standard" datasets at a minimal cost (~\$3 for 50k CT image-report pairs). Further, we find that vision encoder trained on this "silver-standard" dataset achieves performance comparable to those trained on labels extracted by specialized BERT-based models, thereby democratizing the access to large-scale supervised pre-training. Building on this foundation, we proceed to reveal that supervised pre-training fundamentally improves contrastive vision-language alignment. Our approach achieves state-of-the-art performance using only a 3D ResNet-18 with vanilla CLIP training, including 83.8\% AUC for zero-shot diagnosis on CT-RATE, 77.3\% AUC on RAD-ChestCT, and substantial improvements in cross-modal retrieval (MAP@50=53.7\% for image-image, Recall@100=52.2\% for report-image). These results demonstrate the potential of utilizing LLMs to facilitate {\bf more performant and scalable} medical AI systems. Our code is avaiable at this https URL.</li>
</ul>

<h3>Title: CoVariance Filters and Neural Networks over Hilbert Spaces</h3>
<ul>
<li><strong>Authors: </strong>Claudio Battiloro, Andrea Cavallo, Elvin Isufi</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13178">https://arxiv.org/abs/2509.13178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13178">https://arxiv.org/pdf/2509.13178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13178]] CoVariance Filters and Neural Networks over Hilbert Spaces(https://arxiv.org/abs/2509.13178)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>CoVariance Neural Networks (VNNs) perform graph convolutions on the empirical covariance matrix of signals defined over finite-dimensional Hilbert spaces, motivated by robustness and transferability properties. Yet, little is known about how these arguments extend to infinite-dimensional Hilbert spaces. In this work, we take a first step by introducing a novel convolutional learning framework for signals defined over infinite-dimensional Hilbert spaces, centered on the (empirical) covariance operator. We constructively define Hilbert coVariance Filters (HVFs) and design Hilbert coVariance Networks (HVNs) as stacks of HVF filterbanks with nonlinear activations. We propose a principled discretization procedure, and we prove that empirical HVFs can recover the Functional PCA (FPCA) of the filtered signals. We then describe the versatility of our framework with examples ranging from multivariate real-valued functions to reproducing kernel Hilbert spaces. Finally, we validate HVNs on both synthetic and real-world time-series classification tasks, showing robust performance compared to MLP and FPCA-based classifiers.</li>
</ul>

<h3>Title: Road Obstacle Video Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Shyam Nandan Rai, Shyamgopal Karthik, Mariana-Iuliana Georgescu, Barbara Caputo, Carlo Masone, Zeynep Akata</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13181">https://arxiv.org/abs/2509.13181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13181">https://arxiv.org/pdf/2509.13181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13181]] Road Obstacle Video Segmentation(https://arxiv.org/abs/2509.13181)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>With the growing deployment of autonomous driving agents, the detection and segmentation of road obstacles have become critical to ensure safe autonomous navigation. However, existing road-obstacle segmentation methods are applied on individual frames, overlooking the temporal nature of the problem, leading to inconsistent prediction maps between consecutive frames. In this work, we demonstrate that the road-obstacle segmentation task is inherently temporal, since the segmentation maps for consecutive frames are strongly correlated. To address this, we curate and adapt four evaluation benchmarks for road-obstacle video segmentation and evaluate 11 state-of-the-art image- and video-based segmentation methods on these benchmarks. Moreover, we introduce two strong baseline methods based on vision foundation models. Our approach establishes a new state-of-the-art in road-obstacle video segmentation for long-range video sequences, providing valuable insights and direction for future research.</li>
</ul>

<h3>Title: Is Meta-Learning Out? Rethinking Unsupervised Few-Shot Classification with Limited Entropy</h3>
<ul>
<li><strong>Authors: </strong>Yunchuan Guan, Yu Liu, Ke Zhou, Zhiqi Shen, Jenq-Neng Hwang, Serge Belongie, Lei Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13185">https://arxiv.org/abs/2509.13185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13185">https://arxiv.org/pdf/2509.13185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13185]] Is Meta-Learning Out? Rethinking Unsupervised Few-Shot Classification with Limited Entropy(https://arxiv.org/abs/2509.13185)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Meta-learning is a powerful paradigm for tackling few-shot tasks. However, recent studies indicate that models trained with the whole-class training strategy can achieve comparable performance to those trained with meta-learning in few-shot classification tasks. To demonstrate the value of meta-learning, we establish an entropy-limited supervised setting for fair comparisons. Through both theoretical analysis and experimental validation, we establish that meta-learning has a tighter generalization bound compared to whole-class training. We unravel that meta-learning is more efficient with limited entropy and is more robust to label noise and heterogeneous tasks, making it well-suited for unsupervised tasks. Based on these insights, We propose MINO, a meta-learning framework designed to enhance unsupervised performance. MINO utilizes the adaptive clustering algorithm DBSCAN with a dynamic head for unsupervised task construction and a stability-based meta-scaler for robustness against label noise. Extensive experiments confirm its effectiveness in multiple unsupervised few-shot and zero-shot tasks.</li>
</ul>

<h3>Title: The Few-shot Dilemma: Over-prompting Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yongjian Tang, Doruk Tuncel, Christian Koerner, Thomas Runkler</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13196">https://arxiv.org/abs/2509.13196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13196">https://arxiv.org/pdf/2509.13196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13196]] The Few-shot Dilemma: Over-prompting Large Language Models(https://arxiv.org/abs/2509.13196)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Over-prompting, a phenomenon where excessive examples in prompts lead to diminished performance in Large Language Models (LLMs), challenges the conventional wisdom about in-context few-shot learning. To investigate this few-shot dilemma, we outline a prompting framework that leverages three standard few-shot selection methods - random sampling, semantic embedding, and TF-IDF vectors - and evaluate these methods across multiple LLMs, including GPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral. Our experimental results reveal that incorporating excessive domain-specific examples into prompts can paradoxically degrade performance in certain LLMs, which contradicts the prior empirical conclusion that more relevant few-shot examples universally benefit LLMs. Given the trend of LLM-assisted software engineering and requirement analysis, we experiment with two real-world software requirement classification datasets. By gradually increasing the number of TF-IDF-selected and stratified few-shot examples, we identify their optimal quantity for each LLM. This combined approach achieves superior performance with fewer examples, avoiding the over-prompting problem, thus surpassing the state-of-the-art by 1% in classifying functional and non-functional requirements.</li>
</ul>

<h3>Title: B-TGAT: A Bi-directional Temporal Graph Attention Transformer for Clustering Multivariate Spatiotemporal Data</h3>
<ul>
<li><strong>Authors: </strong>Francis Ndikum Nji, Vandana Janaja, Jianwu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13202">https://arxiv.org/abs/2509.13202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13202">https://arxiv.org/pdf/2509.13202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13202]] B-TGAT: A Bi-directional Temporal Graph Attention Transformer for Clustering Multivariate Spatiotemporal Data(https://arxiv.org/abs/2509.13202)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Clustering high-dimensional multivariate spatiotemporal climate data is challenging due to complex temporal dependencies, evolving spatial interactions, and non-stationary dynamics. Conventional clustering methods, including recurrent and convolutional models, often struggle to capture both local and global temporal relationships while preserving spatial context. We present a time-distributed hybrid U-Net autoencoder that integrates a Bi-directional Temporal Graph Attention Transformer (B-TGAT) to guide efficient temporal clustering of multidimensional spatiotemporal climate datasets. The encoder and decoder are equipped with ConvLSTM2D modules that extract joint spatial--temporal features by modeling localized dynamics and spatial correlations over time, and skip connections that preserve multiscale spatial details during feature compression and reconstruction. At the bottleneck, B-TGAT integrates graph-based spatial modeling with attention-driven temporal encoding, enabling adaptive weighting of temporal neighbors and capturing both short and long-range dependencies across regions. This architecture produces discriminative latent embeddings optimized for clustering. Experiments on three distinct spatiotemporal climate datasets demonstrate superior cluster separability, temporal stability, and alignment with known climate transitions compared to state-of-the-art baselines. The integration of ConvLSTM2D, U-Net skip connections, and B-TGAT enhances temporal clustering performance while providing interpretable insights into complex spatiotemporal variability, advancing both methodological development and climate science applications.</li>
</ul>

<h3>Title: End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting Detection</h3>
<ul>
<li><strong>Authors: </strong>Fei Wang, Xuecheng Wu, Zheng Zhang, Danlei Huang, Yuheng Huang, BoWang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13214">https://arxiv.org/abs/2509.13214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13214">https://arxiv.org/pdf/2509.13214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13214]] End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting Detection(https://arxiv.org/abs/2509.13214)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The powerful generative capabilities of diffusion models have significantly advanced the field of image synthesis, enhancing both full image generation and inpainting-based image editing. Despite their remarkable advancements, diffusion models also raise concerns about potential misuse for malicious purposes. However, existing approaches struggle to identify images generated by diffusion-based inpainting models, even when similar inpainted images are included in their training data. To address this challenge, we propose a novel detection method based on End-to-end denoising diffusion (End4). Specifically, End4 designs a denoising reconstruction model to improve the alignment degree between the latent spaces of the reconstruction and detection processes, thus reconstructing features that are more conducive to detection. Meanwhile, it leverages a Scale-aware Pyramid-like Fusion Module (SPFM) that refines local image features under the guidance of attention pyramid layers at different scales, enhancing feature discriminability. Additionally, to evaluate detection performance on inpainted images, we establish a comprehensive benchmark comprising images generated from five distinct masked regions. Extensive experiments demonstrate that our End4 effectively generalizes to unseen masking patterns and remains robust under various perturbations. Our code and dataset will be released soon.</li>
</ul>

<h3>Title: Trustworthy and Confidential SBOM Exchange</h3>
<ul>
<li><strong>Authors: </strong>Eman Abu Ishgair, Chinenye Okafor, Marcela S. Melara, Santiago Torres-Arias</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13217">https://arxiv.org/abs/2509.13217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13217">https://arxiv.org/pdf/2509.13217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13217]] Trustworthy and Confidential SBOM Exchange(https://arxiv.org/abs/2509.13217)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Software Bills of Materials (SBOMs) have become a regulatory requirement for improving software supply chain security and trust by means of transparency regarding components that make up software artifacts. However, enterprise and regulated software vendors commonly wish to restrict who can view confidential software metadata recorded in their SBOMs due to intellectual property or security vulnerability information. To address this tension between transparency and confidentiality, we propose Petra, an SBOM exchange system that empowers software vendors to interoperably compose and distribute redacted SBOM data using selective encryption. Petra enables software consumers to search redacted SBOMs for answers to specific security questions without revealing information they are not authorized to access. Petra leverages a format-agnostic, tamper-evident SBOM representation to generate efficient and confidentiality-preserving integrity proofs, allowing interested parties to cryptographically audit and establish trust in redacted SBOMs. Exchanging redacted SBOMs in our Petra prototype requires less than 1 extra KB per SBOM, and SBOM decryption account for at most 1% of the performance overhead during an SBOM query.</li>
</ul>

<h3>Title: FOSSIL: Regret-minimizing weighting for robust learning under imbalance and small data</h3>
<ul>
<li><strong>Authors: </strong>J. Cha (Gwinnett Technical College), J. Lee (Intel Corporation), J. Cho (Prairie View A&amp;M University), J. Shin (Ohio State University)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13218">https://arxiv.org/abs/2509.13218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13218">https://arxiv.org/pdf/2509.13218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13218]] FOSSIL: Regret-minimizing weighting for robust learning under imbalance and small data(https://arxiv.org/abs/2509.13218)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Imbalanced and small data regimes are pervasive in domains such as rare disease imaging, genomics, and disaster response, where labeled samples are scarce and naive augmentation often introduces artifacts. Existing solutions such as oversampling, focal loss, or meta-weighting address isolated aspects of this challenge but remain fragile or complex. We introduce FOSSIL (Flexible Optimization via Sample Sensitive Importance Learning), a unified weighting framework that seamlessly integrates class imbalance correction, difficulty-aware curricula, augmentation penalties, and warmup dynamics into a single interpretable formula. Unlike prior heuristics, the proposed framework provides regret-based theoretical guarantees and achieves consistent empirical gains over ERM, curriculum, and meta-weighting baselines on synthetic and real-world datasets, while requiring no architectural changes.</li>
</ul>

<h3>Title: On the Out-of-Distribution Backdoor Attack for Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Xu, Zikai Zhang, Rui Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13219">https://arxiv.org/abs/2509.13219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13219">https://arxiv.org/pdf/2509.13219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13219]] On the Out-of-Distribution Backdoor Attack for Federated Learning(https://arxiv.org/abs/2509.13219)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, steal, federate</a></li>
<li><strong>Abstract: </strong>Traditional backdoor attacks in federated learning (FL) operate within constrained attack scenarios, as they depend on visible triggers and require physical modifications to the target object, which limits their practicality. To address this limitation, we introduce a novel backdoor attack prototype for FL called the out-of-distribution (OOD) backdoor attack ($\mathtt{OBA}$), which uses OOD data as both poisoned samples and triggers simultaneously. Our approach significantly broadens the scope of backdoor attack scenarios in FL. To improve the stealthiness of $\mathtt{OBA}$, we propose $\mathtt{SoDa}$, which regularizes both the magnitude and direction of malicious local models during local training, aligning them closely with their benign versions to evade detection. Empirical results demonstrate that $\mathtt{OBA}$ effectively circumvents state-of-the-art defenses while maintaining high accuracy on the main task. To address this security vulnerability in the FL system, we introduce $\mathtt{BNGuard}$, a new server-side defense method tailored against $\mathtt{SoDa}$. $\mathtt{BNGuard}$ leverages the observation that OOD data causes significant deviations in the running statistics of batch normalization layers. This allows $\mathtt{BNGuard}$ to identify malicious model updates and exclude them from aggregation, thereby enhancing the backdoor robustness of FL. Extensive experiments across various settings show the effectiveness of $\mathtt{BNGuard}$ on defending against $\mathtt{SoDa}$. The code is available at this https URL.</li>
</ul>

<h3>Title: Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hugo Carlesso, Josiane Mothe, Radu Tudor Ionescu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13229">https://arxiv.org/abs/2509.13229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13229">https://arxiv.org/pdf/2509.13229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13229]] Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation(https://arxiv.org/abs/2509.13229)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Hyperspectral imaging (HSI) captures detailed spectral signatures across hundreds of contiguous bands per pixel, being indispensable for remote sensing applications such as land-cover classification, change detection, and environmental monitoring. Due to the high dimensionality of HSI data and the slow rate of data transfer in satellite-based systems, compact and efficient models are required to support onboard processing and minimize the transmission of redundant or low-value data, e.g. cloud-covered areas. To this end, we introduce a novel curriculum multi-task self-supervised learning (CMTSSL) framework designed for lightweight architectures for HSI analysis. CMTSSL integrates masked image modeling with decoupled spatial and spectral jigsaw puzzle solving, guided by a curriculum learning strategy that progressively increases data complexity during self-supervision. This enables the encoder to jointly capture fine-grained spectral continuity, spatial structure, and global semantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously addresses spatial and spectral reasoning within a unified and computationally efficient design, being particularly suitable for training lightweight models for onboard satellite deployment. We validate our approach on four public benchmark datasets, demonstrating consistent gains in downstream segmentation tasks, using architectures that are over 16,000x lighter than some state-of-the-art models. These results highlight the potential of CMTSSL in generalizable representation learning with lightweight architectures for real-world HSI applications. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Single-stream Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zhongwen Xu, Zihan Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13232">https://arxiv.org/abs/2509.13232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13232">https://arxiv.org/pdf/2509.13232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13232]] Single-stream Policy Optimization(https://arxiv.org/abs/2509.13232)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We revisit policy-gradient optimization for Large Language Models (LLMs) from a single-stream perspective. Prevailing group-based methods like GRPO reduce variance with on-the-fly baselines but suffer from critical flaws: frequent degenerate groups erase learning signals, and synchronization barriers hinder scalability. We introduce Single-stream Policy Optimization (SPO), which eliminates these issues by design. SPO replaces per-group baselines with a persistent, KL-adaptive value tracker and normalizes advantages globally across the batch, providing a stable, low-variance learning signal for every sample. Being group-free, SPO enables higher throughput and scales effectively in long-horizon or tool-integrated settings where generation times vary. Furthermore, the persistent value tracker naturally enables an adaptive curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO converges more smoothly and attains higher accuracy than GRPO, while eliminating computation wasted on degenerate groups. Ablation studies confirm that SPO's gains stem from its principled approach to baseline estimation and advantage normalization, offering a more robust and efficient path for LLM reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25, +4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain in pass@$k$ across the evaluated $k$ values. SPO's success challenges the prevailing trend of adding incidental complexity to RL algorithms, highlighting a path where fundamental principles, not architectural workarounds, drive the next wave of progress in LLM reasoning.</li>
</ul>

<h3>Title: Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise Behaviors</h3>
<ul>
<li><strong>Authors: </strong>Aniket Didolkar, Nicolas Ballas, Sanjeev Arora, Anirudh Goyal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13237">https://arxiv.org/abs/2509.13237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13237">https://arxiv.org/pdf/2509.13237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13237]] Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise Behaviors(https://arxiv.org/abs/2509.13237)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) now solve multi-step problems by emitting extended chains of thought. During the process, they often re-derive the same intermediate steps across problems, inflating token usage and latency. This saturation of the context window leaves less capacity for exploration. We study a simple mechanism that converts recurring reasoning fragments into concise, reusable "behaviors" (name + instruction) via the model's own metacognitive analysis of prior traces. These behaviors are stored in a "behavior handbook" which supplies them to the model in-context at inference or distills them into parameters via supervised fine-tuning. This approach achieves improved test-time reasoning across three different settings - 1) Behavior-conditioned inference: Providing the LLM relevant behaviors in-context during reasoning reduces number of reasoning tokens by up to 46% while matching or improving baseline accuracy; 2) Behavior-guided self-improvement: Without any parameter updates, the model improves its own future reasoning by leveraging behaviors from its own past problem solving attempts. This yields up to 10% higher accuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned SFT: SFT on behavior-conditioned reasoning traces is more effective at converting non-reasoning models into reasoning models as compared to vanilla SFT. Together, these results indicate that turning slow derivations into fast procedural hints enables LLMs to remember how to reason, not just what to conclude.</li>
</ul>

<h3>Title: Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Bo Yin, Xingyi Yang, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13240">https://arxiv.org/abs/2509.13240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13240">https://arxiv.org/pdf/2509.13240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13240]] Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning(https://arxiv.org/abs/2509.13240)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Existing parameter-efficient fine-tuning (PEFT) methods primarily adapt weight matrices while keeping activation functions fixed. We introduce \textbf{NoRA}, the first PEFT framework that directly adapts nonlinear activation functions in pretrained transformer-based models. NoRA replaces fixed activations with learnable rational functions and applies structured low-rank updates to numerator and denominator coefficients, with a group-wise design that localizes adaptation and improves stability at minimal cost. On vision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceeds full fine-tuning while updating only 0.4\% of parameters (0.02M), achieving accuracy gains of +0.17\% and +0.27\%. When combined with LoRA (\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgets by adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++ consistently improves generation quality, yielding average MMLU gains of +0.3\%--0.8\%, including +1.6\% on STEM (Alpaca) and +1.3\% on OpenOrca. We further show that NoRA constrains adaptation to a low-dimensional functional subspace, implicitly regularizing update magnitude and direction. These results establish activation-space tuning as a complementary and highly parameter-efficient alternative to weight-based PEFT, positioning activation functions as first-class objects for model adaptation.</li>
</ul>

<h3>Title: Evaluating LLM Alignment on Personality Inference from Real-World Interview Data</h3>
<ul>
<li><strong>Authors: </strong>Jianfeng Zhu, Julina Maharjan, Xinyu Li, Karin G. Coifman, Ruoming Jin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13244">https://arxiv.org/abs/2509.13244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13244">https://arxiv.org/pdf/2509.13244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13244]] Evaluating LLM Alignment on Personality Inference from Real-World Interview Data(https://arxiv.org/abs/2509.13244)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly deployed in roles requiring nuanced psychological understanding, such as emotional support agents, counselors, and decision-making assistants. However, their ability to interpret human personality traits, a critical aspect of such applications, remains unexplored, particularly in ecologically valid conversational settings. While prior work has simulated LLM "personas" using discrete Big Five labels on social media data, the alignment of LLMs with continuous, ground-truth personality assessments derived from natural interactions is largely unexamined. To address this gap, we introduce a novel benchmark comprising semi-structured interview transcripts paired with validated continuous Big Five trait scores. Using this dataset, we systematically evaluate LLM performance across three paradigms: (1) zero-shot and chain-of-thought prompting with GPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA architectures, and (3) regression using static embeddings from pretrained BERT and OpenAI's text-embedding-3-small. Our results reveal that all Pearson correlations between model predictions and ground-truth personality traits remain below 0.26, highlighting the limited alignment of current LLMs with validated psychological constructs. Chain-of-thought prompting offers minimal gains over zero-shot, suggesting that personality inference relies more on latent semantic representation than explicit reasoning. These findings underscore the challenges of aligning LLMs with complex human attributes and motivate future work on trait-specific prompting, context-aware modeling, and alignment-oriented fine-tuning.</li>
</ul>

<h3>Title: ResidualViT for Efficient Temporally Dense Video Encoding</h3>
<ul>
<li><strong>Authors: </strong>Mattia Soldan, Fabian Caba Heilbron, Bernard Ghanem, Josef Sivic, Bryan Russell</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13255">https://arxiv.org/abs/2509.13255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13255">https://arxiv.org/pdf/2509.13255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13255]] ResidualViT for Efficient Temporally Dense Video Encoding(https://arxiv.org/abs/2509.13255)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Several video understanding tasks, such as natural language temporal video grounding, temporal activity localization, and audio description generation, require "temporally dense" reasoning over frames sampled at high temporal resolution. However, computing frame-level features for these tasks is computationally expensive given the temporal resolution requirements. In this paper, we make three contributions to reduce the cost of computing features for temporally dense tasks. First, we introduce a vision transformer (ViT) architecture, dubbed ResidualViT, that leverages the large temporal redundancy in videos to efficiently compute temporally dense frame-level features. Our architecture incorporates (i) learnable residual connections that ensure temporal consistency across consecutive frames and (ii) a token reduction module that enhances processing speed by selectively discarding temporally redundant information while reusing weights of a pretrained foundation model. Second, we propose a lightweight distillation strategy to approximate the frame-level features of the original foundation model. Finally, we evaluate our approach across four tasks and five datasets, in both zero-shot and fully supervised settings, demonstrating significant reductions in computational cost (up to 60%) and improvements in inference speed (up to 2.5x faster), all while closely approximating the accuracy of the original foundation model.</li>
</ul>

<h3>Title: JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Zhang, Xiaobing Pei, Zhaokun Zhong, Wenqiang Hao, Zhenghao Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13266">https://arxiv.org/abs/2509.13266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13266">https://arxiv.org/pdf/2509.13266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13266]] JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks(https://arxiv.org/abs/2509.13266)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, generative</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have demonstrated remarkable performance across various applications, yet they are vulnerable to sophisticated adversarial attacks, particularly node injection attacks. The success of such attacks heavily relies on their stealthiness, the ability to blend in with the original graph and evade detection. However, existing methods often achieve stealthiness by relying on indirect proxy metrics, lacking consideration for the fundamental characteristics of the injected content, or focusing only on imitating local structures, which leads to the problem of local myopia. To overcome these limitations, we propose a dual-constraint stealthy node injection framework, called Joint Alignment of Nodal and Universal Structures (JANUS). At the local level, we introduce a local feature manifold alignment strategy to achieve geometric consistency in the feature space. At the global level, we incorporate structured latent variables and maximize the mutual information with the generated structures, ensuring the injected structures are consistent with the semantic patterns of the original graph. We model the injection attack as a sequential decision process, which is optimized by a reinforcement learning agent. Experiments on multiple standard datasets demonstrate that the JANUS framework significantly outperforms existing methods in terms of both attack effectiveness and stealthiness.</li>
</ul>

<h3>Title: LLMs for energy and macronutrients estimation using only text data from 24-hour dietary recalls: a parameter-efficient fine-tuning experiment using a 10-shot prompt</h3>
<ul>
<li><strong>Authors: </strong>Rodrigo M Carrillo-Larco</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13268">https://arxiv.org/abs/2509.13268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13268">https://arxiv.org/pdf/2509.13268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13268]] LLMs for energy and macronutrients estimation using only text data from 24-hour dietary recalls: a parameter-efficient fine-tuning experiment using a 10-shot prompt(https://arxiv.org/abs/2509.13268)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>BACKGROUND: Most artificial intelligence tools used to estimate nutritional content rely on image input. However, whether large language models (LLMs) can accurately predict nutritional values based solely on text descriptions of foods consumed remains unknown. If effective, this approach could enable simpler dietary monitoring without the need for photographs. METHODS: We used 24-hour dietary recalls from adolescents aged 12-19 years in the National Health and Nutrition Examination Survey (NHANES). An open-source quantized LLM was prompted using a 10-shot, chain-of-thought approach to estimate energy and five macronutrients based solely on text strings listing foods and their quantities. We then applied parameter-efficient fine-tuning (PEFT) to evaluate whether predictive accuracy improved. NHANES-calculated values served as the ground truth for energy, proteins, carbohydrates, total sugar, dietary fiber and total fat. RESULTS: In a pooled dataset of 11,281 adolescents (49.9% male, mean age 15.4 years), the vanilla LLM yielded poor predictions. The mean absolute error (MAE) was 652.08 for energy and the Lin's CCC <0.46 across endpoints. In contrast, the fine-tuned model performed substantially better, with energy MAEs ranging from 171.34 to 190.90 across subsets, and Lin's CCC exceeding 0.89 for all outcomes. CONCLUSIONS: When prompted using a chain-of-thought approach and fine-tuned with PEFT, open-source LLMs exposed solely to text input can accurately predict energy and macronutrient values from 24-hour dietary recalls. This approach holds promise for low-burden, text-based dietary monitoring tools.</li>
</ul>

<h3>Title: ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement</h3>
<ul>
<li><strong>Authors: </strong>Ali Salamatian, Amirhossein Abaskohi, Wan-Cyuan Fan, Mir Rayat Imtiaz Hossain, Leonid Sigal, Giuseppe Carenini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13282">https://arxiv.org/abs/2509.13282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13282">https://arxiv.org/pdf/2509.13282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13282]] ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement(https://arxiv.org/abs/2509.13282)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Charts are a crucial visual medium for communicating and representing information. While Large Vision-Language Models (LVLMs) have made progress on chart question answering (CQA), the task remains challenging, particularly when models attend to irrelevant regions of the chart. In this work, we present ChartGaze, a new eye-tracking dataset that captures human gaze patterns during chart reasoning tasks. Through a systematic comparison of human and model attention, we find that LVLMs often diverge from human gaze, leading to reduced interpretability and accuracy. To address this, we propose a gaze-guided attention refinement that aligns image-text attention with human fixations. Our approach improves both answer accuracy and attention alignment, yielding gains of up to 2.56 percentage points across multiple models. These results demonstrate the promise of incorporating human gaze to enhance both the reasoning quality and interpretability of chart-focused LVLMs.</li>
</ul>

<h3>Title: Image Realness Assessment and Localization with Multimodal Features</h3>
<ul>
<li><strong>Authors: </strong>Lovish Kaushik, Agnij Biswas, Somdyuti Paul</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13289">https://arxiv.org/abs/2509.13289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13289">https://arxiv.org/pdf/2509.13289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13289]] Image Realness Assessment and Localization with Multimodal Features(https://arxiv.org/abs/2509.13289)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A reliable method of quantifying the perceptual realness of AI-generated images and identifying visually inconsistent regions is crucial for practical use of AI-generated images and for improving photorealism of generative AI via realness feedback during training. This paper introduces a framework that accomplishes both overall objective realness assessment and local inconsistency identification of AI-generated images using textual descriptions of visual inconsistencies generated by vision-language models trained on large datasets that serve as reliable substitutes for human annotations. Our results demonstrate that the proposed multimodal approach improves objective realness prediction performance and produces dense realness maps that effectively distinguish between realistic and unrealistic spatial regions.</li>
</ul>

<h3>Title: Scaling Agents via Continual Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, Chenxiong Qian, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13310">https://arxiv.org/abs/2509.13310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13310">https://arxiv.org/pdf/2509.13310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13310]] Scaling Agents via Continual Pre-training(https://arxiv.org/abs/2509.13310)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have evolved into agentic systems capable of autonomous tool use and multi-step reasoning for complex problem-solving. However, post-training approaches building upon general-purpose foundation models consistently underperform in agentic tasks, particularly in open-source implementations. We identify the root cause: the absence of robust agentic foundation models forces models during post-training to simultaneously learn diverse agentic behaviors while aligning them to expert demonstrations, thereby creating fundamental optimization tensions. To this end, we are the first to propose incorporating Agentic Continual Pre-training (Agentic CPT) into the deep research agents training pipeline to build powerful agentic foundational models. Based on this approach, we develop a deep research agent model named AgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve state-of-the-art performance while retains strong tool-use ability, notably 39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.</li>
</ul>

<h3>Title: Towards General Agentic Intelligence via Environment Scaling</h3>
<ul>
<li><strong>Authors: </strong>Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, Shibin Wu, Zhengwei Tao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13311">https://arxiv.org/abs/2509.13311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13311">https://arxiv.org/pdf/2509.13311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13311]] Towards General Agentic Intelligence via Environment Scaling(https://arxiv.org/abs/2509.13311)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Advanced agentic intelligence is a prerequisite for deploying Large Language Models in practical, real-world applications. Diverse real-world APIs demand precise, robust function-calling intelligence, which needs agents to develop these capabilities through interaction in varied environments. The breadth of function-calling competence is closely tied to the diversity of environments in which agents are trained. In this work, we scale up environments as a step towards advancing general agentic intelligence. This gives rise to two central challenges: (i) how to scale environments in a principled manner, and (ii) how to effectively train agentic capabilities from experiences derived through interactions with these environments. To address these, we design a scalable framework that automatically constructs heterogeneous environments that are fully simulated, systematically broadening the space of function-calling scenarios. We further adapt a two-phase agent fine-tuning strategy: first endowing agents with fundamental agentic capabilities, then specializing them for domain-specific contexts. Extensive experiments on agentic benchmarks, tau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model, AgentScaler, significantly enhances the function-calling capability of models.</li>
</ul>

<h3>Title: ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization</h3>
<ul>
<li><strong>Authors: </strong>Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, Huifeng Yin, Zhongwang Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Minhao Cheng, Shuai Wang, Hong Cheng, Jingren Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13313">https://arxiv.org/abs/2509.13313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13313">https://arxiv.org/pdf/2509.13313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13313]] ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization(https://arxiv.org/abs/2509.13313)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM)-based web agents demonstrate strong performance on knowledge-intensive tasks but are hindered by context window limitations in paradigms like ReAct. Complex queries involving multiple entities, intertwined relationships, and high uncertainty demand extensive search cycles that rapidly exhaust context budgets before reaching complete solutions. To overcome this challenge, we introduce ReSum, a novel paradigm that enables indefinite exploration through periodic context summarization. ReSum converts growing interaction histories into compact reasoning states, maintaining awareness of prior discoveries while bypassing context constraints. For paradigm adaptation, we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and advantage broadcasting to familiarize agents with summary-conditioned reasoning. Extensive experiments on web agents of varying scales across three benchmarks demonstrate that ReSum delivers an average absolute improvement of 4.5\% over ReAct, with further gains of up to 8.2\% following ReSum-GRPO training. Notably, with only 1K training samples, our WebResummer-30B (a ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\% Pass@1 on BrowseComp-zh and 18.3\% on BrowseComp-en, surpassing existing open-source web agents.</li>
</ul>

<h3>Title: Do Natural Language Descriptions of Model Activations Convey Privileged Information?</h3>
<ul>
<li><strong>Authors: </strong>Millicent Li, Alberto Mario Ceballos Arroyo, Giordano Rogers, Naomi Saphra, Byron C. Wallace</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13316">https://arxiv.org/abs/2509.13316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13316">https://arxiv.org/pdf/2509.13316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13316]] Do Natural Language Descriptions of Model Activations Convey Privileged Information?(https://arxiv.org/abs/2509.13316)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Recent interpretability methods have proposed to translate LLM internal representations into natural language descriptions using a second verbalizer LLM. This is intended to illuminate how the target model represents and operates on inputs. But do such activation verbalization approaches actually provide privileged knowledge about the internal workings of the target model, or do they merely convey information about its inputs? We critically evaluate popular verbalization methods across datasets used in prior work and find that they succeed at benchmarks without any access to target model internals, suggesting that these datasets are not ideal for evaluating verbalization methods. We then run controlled experiments which reveal that verbalizations often reflect the parametric knowledge of the verbalizer LLM which generated them, rather than the activations of the target LLM being decoded. Taken together, our results indicate a need for targeted benchmarks and experimental controls to rigorously assess whether verbalization methods provide meaningful insights into the operations of LLMs.</li>
</ul>

<h3>Title: 3D Aware Region Prompted Vision Language Model</h3>
<ul>
<li><strong>Authors: </strong>An-Chieh Cheng, Yang Fu, Yukang Chen, Zhijian Liu, Xiaolong Li, Subhashree Radhakrishnan, Song Han, Yao Lu, Jan Kautz, Pavlo Molchanov, Hongxu Yin, Xiaolong Wang, Sifei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13317">https://arxiv.org/abs/2509.13317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13317">https://arxiv.org/pdf/2509.13317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13317]] 3D Aware Region Prompted Vision Language Model(https://arxiv.org/abs/2509.13317)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present Spatial Region 3D (SR-3D) aware vision-language model that connects single-view 2D images and multi-view 3D data through a shared visual token space. SR-3D supports flexible region prompting, allowing users to annotate regions with bounding boxes, segmentation masks on any frame, or directly in 3D, without the need for exhaustive multi-frame labeling. We achieve this by enriching 2D visual features with 3D positional embeddings, which allows the 3D model to draw upon strong 2D priors for more accurate spatial reasoning across frames, even when objects of interest do not co-occur within the same view. Extensive experiments on both general 2D vision language and specialized 3D spatial benchmarks demonstrate that SR-3D achieves state-of-the-art performance, underscoring its effectiveness for unifying 2D and 3D representation space on scene understanding. Moreover, we observe applicability to in-the-wild videos without sensory 3D inputs or ground-truth 3D annotations, where SR-3D accurately infers spatial relationships and metric measurements.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
