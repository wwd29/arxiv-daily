<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: UFed-GAN: A Secure Federated Learning Framework with Constrained Computation and Unlabeled Data. (arXiv:2308.05870v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05870">http://arxiv.org/abs/2308.05870</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05870]] UFed-GAN: A Secure Federated Learning Framework with Constrained Computation and Unlabeled Data(http://arxiv.org/abs/2308.05870)</code></li>
<li>Summary: <p>To satisfy the broad applications and insatiable hunger for deploying low
latency multimedia data classification and data privacy in a cloud-based
setting, federated learning (FL) has emerged as an important learning paradigm.
For the practical cases involving limited computational power and only
unlabeled data in many wireless communications applications, this work
investigates FL paradigm in a resource-constrained and label-missing
environment. Specifically, we propose a novel framework of UFed-GAN:
Unsupervised Federated Generative Adversarial Network, which can capture
user-side data distribution without local classification training. We also
analyze the convergence and privacy of the proposed UFed-GAN. Our experimental
results demonstrate the strong potential of UFed-GAN in addressing limited
computational resources and unlabeled data while preserving privacy.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Aphid Cluster Recognition and Detection in the Wild Using Deep Learning Models. (arXiv:2308.05881v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05881">http://arxiv.org/abs/2308.05881</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05881]] Aphid Cluster Recognition and Detection in the Wild Using Deep Learning Models(http://arxiv.org/abs/2308.05881)</code></li>
<li>Summary: <p>Aphid infestation poses a significant threat to crop production, rural
communities, and global food security. While chemical pest control is crucial
for maximizing yields, applying chemicals across entire fields is both
environmentally unsustainable and costly. Hence, precise localization and
management of aphids are essential for targeted pesticide application. The
paper primarily focuses on using deep learning models for detecting aphid
clusters. We propose a novel approach for estimating infection levels by
detecting aphid clusters. To facilitate this research, we have captured a
large-scale dataset from sorghum fields, manually selected 5,447 images
containing aphids, and annotated each individual aphid cluster within these
images. To facilitate the use of machine learning models, we further process
the images by cropping them into patches, resulting in a labeled dataset
comprising 151,380 image patches. Then, we implemented and compared the
performance of four state-of-the-art object detection models (VFNet, GFLV2,
PAA, and ATSS) on the aphid dataset. Extensive experimental results show that
all models yield stable similar performance in terms of average precision and
recall. We then propose to merge close neighboring clusters and remove tiny
clusters caused by cropping, and the performance is further boosted by around
17%. The study demonstrates the feasibility of automatically detecting and
managing insects using machine learning models. The labeled dataset will be
made openly available to the research community.
</p></li>
</ul>

<h3>Title: Continual Face Forgery Detection via Historical Distribution Preserving. (arXiv:2308.06217v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06217">http://arxiv.org/abs/2308.06217</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06217]] Continual Face Forgery Detection via Historical Distribution Preserving(http://arxiv.org/abs/2308.06217)</code></li>
<li>Summary: <p>Face forgery techniques have advanced rapidly and pose serious security
threats. Existing face forgery detection methods try to learn generalizable
features, but they still fall short of practical application. Additionally,
finetuning these methods on historical training data is resource-intensive in
terms of time and storage. In this paper, we focus on a novel and challenging
problem: Continual Face Forgery Detection (CFFD), which aims to efficiently
learn from new forgery attacks without forgetting previous ones. Specifically,
we propose a Historical Distribution Preserving (HDP) framework that reserves
and preserves the distributions of historical faces. To achieve this, we use
universal adversarial perturbation (UAP) to simulate historical forgery
distribution, and knowledge distillation to maintain the distribution variation
of real faces across different models. We also construct a new benchmark for
CFFD with three evaluation protocols. Our extensive experiments on the
benchmarks show that our method outperforms the state-of-the-art competitors.
</p></li>
</ul>

<h3>Title: Security of XCB and HCTR. (arXiv:2308.06082v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06082">http://arxiv.org/abs/2308.06082</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06082]] Security of XCB and HCTR(http://arxiv.org/abs/2308.06082)</code></li>
<li>Summary: <p>Tweakable Enciphering Scheme (TES) is a length preserving scheme which
provides confidentiality and admissible integrity. XCB (Extended Code Book) is
a TES which was introduced in 2004. In 2007, it was modified and security bound
was provided. Later, these two versions were referred to as XCBv1 and XCBv2
respectively. XCBv2 was proposed as the IEEE-std 1619.2 2010 for encryption of
sector oriented storage media. In 2013, first time Security bound of XCBv1 was
given and XCBv2's security bound was enhanced. A constant of $2^{22}$ appears
in the security bounds of the XCBv1 and XCBv2.
</p>
<p>We showed that this constant of $2^{22}$ can be reduced to $2^{5}$. Further,
we modified the XCB (MXCB) scheme such that it gives better security bound
compared to the present XCB scheme. We also analyzed some weak keys attack on
XCB and a type of TES known as HCTR (proposed in 2005). We performed
distinguishing attack and the hash key recovery attack on HCTR. Next, we
analyzed the dependency of the two different keys in HCTR.
</p></li>
</ul>

<h3>Title: A Uniform Representation of Classical and Quantum Source Code for Static Code Analysis. (arXiv:2308.06113v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06113">http://arxiv.org/abs/2308.06113</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06113]] A Uniform Representation of Classical and Quantum Source Code for Static Code Analysis(http://arxiv.org/abs/2308.06113)</code></li>
<li>Summary: <p>The emergence of quantum computing raises the question of how to identify
(security-relevant) programming errors during development. However, current
static code analysis tools fail to model information specific to quantum
computing. In this paper, we identify this information and propose to extend
classical code analysis tools accordingly. Among such tools, we identify the
Code Property Graph to be very well suited for this task as it can be easily
extended with quantum computing specific information. For our proof of concept,
we implemented a tool which includes information from the quantum world in the
graph and demonstrate its ability to analyze source code written in Qiskit and
OpenQASM. Our tool brings together the information from the classical and
quantum world, enabling analysis across both domains. By combining all relevant
information into a single detailed analysis, this powerful tool can facilitate
tackling future quantum source code analysis challenges.
</p></li>
</ul>

<h3>Title: SALSy: Security-Aware Layout Synthesis. (arXiv:2308.06201v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06201">http://arxiv.org/abs/2308.06201</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06201]] SALSy: Security-Aware Layout Synthesis(http://arxiv.org/abs/2308.06201)</code></li>
<li>Summary: <p>Integrated Circuits (ICs) are the target of diverse attacks during their
lifetime. Fabrication-time attacks, such as the insertion of Hardware Trojans,
can give an adversary access to privileged data and/or the means to corrupt the
IC's internal computation. Post-fabrication attacks, where the end-user takes a
malicious role, also attempt to obtain privileged information through means
such as fault injection and probing. Taking these threats into account and at
the same time, this paper proposes a methodology for Security-Aware Layout
Synthesis (SALSy), such that ICs can be designed with security in mind in the
same manner as power-performance-area (PPA) metrics are considered today, a
concept known as security closure. Furthermore, the trade-offs between PPA and
security are considered and a chip is fabricated in a 65nm CMOS commercial
technology for validation purposes - a feature not seen in previous research on
security closure. Measurements on the fabricated ICs indicate that SALSy
promotes a modest increase in power in order to achieve significantly improved
security metrics.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Private Distribution Learning with Public Data: The View from Sample Compression. (arXiv:2308.06239v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06239">http://arxiv.org/abs/2308.06239</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06239]] Private Distribution Learning with Public Data: The View from Sample Compression(http://arxiv.org/abs/2308.06239)</code></li>
<li>Summary: <p>We study the problem of private distribution learning with access to public
data. In this setup, which we refer to as public-private learning, the learner
is given public and private samples drawn from an unknown distribution $p$
belonging to a class $\mathcal Q$, with the goal of outputting an estimate of
$p$ while adhering to privacy constraints (here, pure differential privacy)
only with respect to the private samples.
</p>
<p>We show that the public-private learnability of a class $\mathcal Q$ is
connected to the existence of a sample compression scheme for $\mathcal Q$, as
well as to an intermediate notion we refer to as list learning. Leveraging this
connection: (1) approximately recovers previous results on Gaussians over
$\mathbb R^d$; and (2) leads to new ones, including sample complexity upper
bounds for arbitrary $k$-mixtures of Gaussians over $\mathbb R^d$, results for
agnostic and distribution-shift resistant learners, as well as closure
properties for public-private learnability under taking mixtures and products
of distributions. Finally, via the connection to list learning, we show that
for Gaussians in $\mathbb R^d$, at least $d$ public samples are necessary for
private learnability, which is close to the known upper bound of $d+1$ public
samples.
</p></li>
</ul>

<h3>Title: Cost-effective On-device Continual Learning over Memory Hierarchy with Miro. (arXiv:2308.06053v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06053">http://arxiv.org/abs/2308.06053</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06053]] Cost-effective On-device Continual Learning over Memory Hierarchy with Miro(http://arxiv.org/abs/2308.06053)</code></li>
<li>Summary: <p>Continual learning (CL) trains NN models incrementally from a continuous
stream of tasks. To remember previously learned knowledge, prior studies store
old samples over a memory hierarchy and replay them when new tasks arrive. Edge
devices that adopt CL to preserve data privacy are typically energy-sensitive
and thus require high model accuracy while not compromising energy efficiency,
i.e., cost-effectiveness. Our work is the first to explore the design space of
hierarchical memory replay-based CL to gain insights into achieving
cost-effectiveness on edge devices. We present Miro, a novel system runtime
that carefully integrates our insights into the CL framework by enabling it to
dynamically configure the CL system based on resource states for the best
cost-effectiveness. To reach this goal, Miro also performs online profiling on
parameters with clear accuracy-energy trade-offs and adapts to optimal values
with low overhead. Extensive evaluations show that Miro significantly
outperforms baseline systems we build for comparison, consistently achieving
higher cost-effectiveness.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Test-Time Adaptation for Backdoor Defense. (arXiv:2308.06107v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06107">http://arxiv.org/abs/2308.06107</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06107]] Test-Time Adaptation for Backdoor Defense(http://arxiv.org/abs/2308.06107)</code></li>
<li>Summary: <p>Deep neural networks have played a crucial part in many critical domains,
such as autonomous driving, face recognition, and medical diagnosis. However,
deep neural networks are facing security threats from backdoor attacks and can
be manipulated into attacker-decided behaviors by the backdoor attacker. To
defend the backdoor, prior research has focused on using clean data to remove
backdoor attacks before model deployment. In this paper, we investigate the
possibility of defending against backdoor attacks at test time by utilizing
partially poisoned data to remove the backdoor from the model. To address the
problem, a two-stage method Test-Time Backdoor Defense (TTBD) is proposed. In
the first stage, we propose two backdoor sample detection methods, namely DDP
and TeCo, to identify poisoned samples from a batch of mixed, partially
poisoned samples. Once the poisoned samples are detected, we employ Shapley
estimation to calculate the contribution of each neuron's significance in the
network, locate the poisoned neurons, and prune them to remove backdoor in the
models. Our experiments demonstrate that TTBD removes the backdoor successfully
with only a batch of partially poisoned data across different model
architectures and datasets against different types of backdoor attacks.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Face Encryption via Frequency-Restricted Identity-Agnostic Attacks. (arXiv:2308.05983v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05983">http://arxiv.org/abs/2308.05983</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05983]] Face Encryption via Frequency-Restricted Identity-Agnostic Attacks(http://arxiv.org/abs/2308.05983)</code></li>
<li>Summary: <p>Billions of people are sharing their daily live images on social media
everyday. However, malicious collectors use deep face recognition systems to
easily steal their biometric information (e.g., faces) from these images. Some
studies are being conducted to generate encrypted face photos using adversarial
attacks by introducing imperceptible perturbations to reduce face information
leakage. However, existing studies need stronger black-box scenario feasibility
and more natural visual appearances, which challenge the feasibility of privacy
protection. To address these problems, we propose a frequency-restricted
identity-agnostic (FRIA) framework to encrypt face images from unauthorized
face recognition without access to personal information. As for the weak
black-box scenario feasibility, we obverse that representations of the average
feature in multiple face recognition models are similar, thus we propose to
utilize the average feature via the crawled dataset from the Internet as the
target to guide the generation, which is also agnostic to identities of unknown
face recognition systems; in nature, the low-frequency perturbations are more
visually perceptible by the human vision system. Inspired by this, we restrict
the perturbation in the low-frequency facial regions by discrete cosine
transform to achieve the visual naturalness guarantee. Extensive experiments on
several face recognition models demonstrate that our FRIA outperforms other
state-of-the-art methods in generating more natural encrypted faces while
attaining high black-box attack success rates of 96%. In addition, we validate
the efficacy of FRIA using real-world black-box commercial API, which reveals
the potential of FRIA in practice. Our codes can be found in
https://github.com/XinDong10/FRIA.
</p></li>
</ul>

<h3>Title: Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook. (arXiv:2308.06173v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06173">http://arxiv.org/abs/2308.06173</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06173]] Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook(http://arxiv.org/abs/2308.06173)</code></li>
<li>Summary: <p>In this paper, we present a comprehensive survey of the current trends
focusing specifically on physical adversarial attacks. We aim to provide a
thorough understanding of the concept of physical adversarial attacks,
analyzing their key characteristics and distinguishing features. Furthermore,
we explore the specific requirements and challenges associated with executing
attacks in the physical world. Our article delves into various physical
adversarial attack methods, categorized according to their target tasks in
different applications, including classification, detection, face recognition,
semantic segmentation and depth estimation. We assess the performance of these
attack methods in terms of their effectiveness, stealthiness, and robustness.
We examine how each technique strives to ensure the successful manipulation of
DNNs while mitigating the risk of detection and withstanding real-world
distortions. Lastly, we discuss the current challenges and outline potential
future research directions in the field of physical adversarial attacks. We
highlight the need for enhanced defense mechanisms, the exploration of novel
attack strategies, the evaluation of attacks in different application domains,
and the establishment of standardized benchmarks and evaluation criteria for
physical adversarial attacks. Through this comprehensive survey, we aim to
provide a valuable resource for researchers, practitioners, and policymakers to
gain a holistic understanding of physical adversarial attacks in computer
vision and facilitate the development of robust and secure DNN-based systems.
</p></li>
</ul>

<h3>Title: FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks. (arXiv:2308.05832v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05832">http://arxiv.org/abs/2308.05832</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05832]] FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks(http://arxiv.org/abs/2308.05832)</code></li>
<li>Summary: <p>Federated learning (FL) is revolutionizing how we learn from data. With its
growing popularity, it is now being used in many safety-critical domains such
as autonomous vehicles and healthcare. Since thousands of participants can
contribute in this collaborative setting, it is, however, challenging to ensure
security and reliability of such systems. This highlights the need to design FL
systems that are secure and robust against malicious participants' actions
while also ensuring high utility, privacy of local data, and efficiency. In
this paper, we propose a novel FL framework dubbed as FLShield that utilizes
benign data from FL participants to validate the local models before taking
them into account for generating the global model. This is in stark contrast
with existing defenses relying on server's access to clean datasets -- an
assumption often impractical in real-life scenarios and conflicting with the
fundamentals of FL. We conduct extensive experiments to evaluate our FLShield
framework in different settings and demonstrate its effectiveness in thwarting
various types of poisoning and backdoor attacks including a defense-aware one.
FLShield also preserves privacy of local data against gradient inversion
attacks.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Efficient Large-scale AUV-based Visual Seafloor Mapping. (arXiv:2308.06147v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06147">http://arxiv.org/abs/2308.06147</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06147]] Efficient Large-scale AUV-based Visual Seafloor Mapping(http://arxiv.org/abs/2308.06147)</code></li>
<li>Summary: <p>Driven by the increasing number of marine data science applications, there is
a growing interest in surveying and exploring the vast, uncharted terrain of
the deep sea with robotic platforms. Despite impressive results achieved by
many on-land visual mapping algorithms in the past decades, transferring these
methods from land to the deep sea remains a challenge due to harsh
environmental conditions. Typically, deep-sea exploration involves the use of
autonomous underwater vehicles (AUVs) equipped with high-resolution cameras and
artificial illumination systems. However, images obtained in this manner often
suffer from heterogeneous illumination and quality degradation due to
attenuation and scattering, on top of refraction of light rays. All of this
together often lets on-land SLAM approaches fail underwater or makes
Structure-from-Motion approaches drift or omit difficult images, resulting in
gaps, jumps or weakly registered areas. In this work, we present a system that
incorporates recent developments in underwater imaging and visual mapping to
facilitate automated robotic 3D reconstruction of hectares of seafloor. Our
approach is efficient in that it detects and reconsiders difficult, weakly
registered areas, to avoid omitting images and to make better use of limited
dive time; on the other hand it is computationally efficient; leveraging a
hybrid approach combining benefits from SLAM and Structure-from-Motion that
runs much faster than incremental reconstructions while achieving at least
on-par performance. The proposed system has been extensively tested and
evaluated during several research cruises, demonstrating its robustness and
practicality in real-world conditions.
</p></li>
</ul>

<h3>Title: Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction. (arXiv:2308.06058v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06058">http://arxiv.org/abs/2308.06058</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06058]] Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction(http://arxiv.org/abs/2308.06058)</code></li>
<li>Summary: <p>The recently proposed stochastic Polyak stepsize (SPS) and stochastic
line-search (SLS) for SGD have shown remarkable effectiveness when training
over-parameterized models. However, in non-interpolation settings, both
algorithms only guarantee convergence to a neighborhood of a solution which may
result in a worse output than the initial guess. While artificially decreasing
the adaptive stepsize has been proposed to address this issue (Orvieto et al.
[2022]), this approach results in slower convergence rates for convex and
over-parameterized models. In this work, we make two contributions: Firstly, we
propose two new variants of SPS and SLS, called AdaSPS and AdaSLS, which
guarantee convergence in non-interpolation settings and maintain sub-linear and
linear convergence rates for convex and strongly convex functions when training
over-parameterized models. AdaSLS requires no knowledge of problem-dependent
parameters, and AdaSPS requires only a lower bound of the optimal function
value as input. Secondly, we equip AdaSPS and AdaSLS with a novel variance
reduction technique and obtain algorithms that require
$\smash{\widetilde{\mathcal{O}}}(n+1/\epsilon)$ gradient evaluations to achieve
an $\mathcal{O}(\epsilon)$-suboptimality for convex functions, which improves
upon the slower $\mathcal{O}(1/\epsilon^2)$ rates of AdaSPS and AdaSLS without
variance reduction in the non-interpolation regimes. Moreover, our result
matches the fast rates of AdaSVRG but removes the inner-outer-loop structure,
which is easier to implement and analyze. Finally, numerical experiments on
synthetic and real datasets validate our theory and demonstrate the
effectiveness and robustness of our algorithms.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Scale-Preserving Automatic Concept Extraction (SPACE). (arXiv:2308.06022v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06022">http://arxiv.org/abs/2308.06022</a></li>
<li>Code URL: https://github.com/data-science-in-mechanical-engineering/space</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06022]] Scale-Preserving Automatic Concept Extraction (SPACE)(http://arxiv.org/abs/2308.06022)</code></li>
<li>Summary: <p>Convolutional Neural Networks (CNN) have become a common choice for
industrial quality control, as well as other critical applications in the
Industry 4.0. When these CNNs behave in ways unexpected to human users or
developers, severe consequences can arise, such as economic losses or an
increased risk to human life. Concept extraction techniques can be applied to
increase the reliability and transparency of CNNs through generating global
explanations for trained neural network models. The decisive features of image
datasets in quality control often depend on the feature's scale; for example,
the size of a hole or an edge. However, existing concept extraction methods do
not correctly represent scale, which leads to problems interpreting these
models as we show herein. To address this issue, we introduce the
Scale-Preserving Automatic Concept Extraction (SPACE) algorithm, as a
state-of-the-art alternative concept extraction technique for CNNs, focused on
industrial applications. SPACE is specifically designed to overcome the
aforementioned problems by avoiding scale changes throughout the concept
extraction process. SPACE proposes an approach based on square slices of input
images, which are selected and then tiled before being clustered into concepts.
Our method provides explanations of the models' decision-making process in the
form of human-understandable concepts. We evaluate SPACE on three image
classification datasets in the context of industrial quality control. Through
experimental results, we illustrate how SPACE outperforms other methods and
provides actionable insights on the decision mechanisms of CNNs. Finally, code
for the implementation of SPACE is provided.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Towards Instance-adaptive Inference for Federated Learning. (arXiv:2308.06051v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06051">http://arxiv.org/abs/2308.06051</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06051]] Towards Instance-adaptive Inference for Federated Learning(http://arxiv.org/abs/2308.06051)</code></li>
<li>Summary: <p>Federated learning (FL) is a distributed learning paradigm that enables
multiple clients to learn a powerful global model by aggregating local
training. However, the performance of the global model is often hampered by
non-i.i.d. distribution among the clients, requiring extensive efforts to
mitigate inter-client data heterogeneity. Going beyond inter-client data
heterogeneity, we note that intra-client heterogeneity can also be observed on
complex real-world data and seriously deteriorate FL performance. In this
paper, we present a novel FL algorithm, i.e., FedIns, to handle intra-client
data heterogeneity by enabling instance-adaptive inference in the FL framework.
Instead of huge instance-adaptive models, we resort to a parameter-efficient
fine-tuning method, i.e., scale and shift deep features (SSF), upon a
pre-trained model. Specifically, we first train an SSF pool for each client,
and aggregate these SSF pools on the server side, thus still maintaining a low
communication cost. To enable instance-adaptive inference, for a given
instance, we dynamically find the best-matched SSF subsets from the pool and
aggregate them to generate an adaptive SSF specified for the instance, thereby
reducing the intra-client as well as the inter-client heterogeneity. Extensive
experiments show that our FedIns outperforms state-of-the-art FL algorithms,
e.g., a 6.64\% improvement against the top-performing method with less than
15\% communication cost on Tiny-ImageNet. Our code and models will be publicly
released.
</p></li>
</ul>

<h3>Title: CyberForce: A Federated Reinforcement Learning Framework for Malware Mitigation. (arXiv:2308.05978v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05978">http://arxiv.org/abs/2308.05978</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05978]] CyberForce: A Federated Reinforcement Learning Framework for Malware Mitigation(http://arxiv.org/abs/2308.05978)</code></li>
<li>Summary: <p>The expansion of the Internet-of-Things (IoT) paradigm is inevitable, but
vulnerabilities of IoT devices to malware incidents have become an increasing
concern. Recent research has shown that the integration of Reinforcement
Learning with Moving Target Defense (MTD) mechanisms can enhance cybersecurity
in IoT devices. Nevertheless, the numerous new malware attacks and the time
that agents take to learn and select effective MTD techniques make this
approach impractical for real-world IoT scenarios. To tackle this issue, this
work presents CyberForce, a framework that employs Federated Reinforcement
Learning (FRL) to collectively and privately determine suitable MTD techniques
for mitigating diverse zero-day attacks. CyberForce integrates device
fingerprinting and anomaly detection to reward or penalize MTD mechanisms
chosen by an FRL-based agent. The framework has been evaluated in a federation
consisting of ten devices of a real IoT platform. A pool of experiments with
six malware samples affecting the devices has demonstrated that CyberForce can
precisely learn optimum MTD mitigation strategies. When all clients are
affected by all attacks, the FRL agent exhibits high accuracy and reduced
training time when compared to a centralized RL agent. In cases where different
clients experience distinct attacks, the CyberForce clients gain benefits
through the transfer of knowledge from other clients and similar attack
behavior. Additionally, CyberForce showcases notable robustness against data
poisoning attacks.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: LittleMu: Deploying an Online Virtual Teaching Assistant via Heterogeneous Sources Integration and Chain of Teach Prompts. (arXiv:2308.05935v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05935">http://arxiv.org/abs/2308.05935</a></li>
<li>Code URL: https://github.com/thu-keg/vta</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05935]] LittleMu: Deploying an Online Virtual Teaching Assistant via Heterogeneous Sources Integration and Chain of Teach Prompts(http://arxiv.org/abs/2308.05935)</code></li>
<li>Summary: <p>Teaching assistants have played essential roles in the long history of
education. However, few MOOC platforms are providing human or virtual teaching
assistants to support learning for massive online students due to the
complexity of real-world online education scenarios and the lack of training
data. In this paper, we present a virtual MOOC teaching assistant, LittleMu
with minimum labeled training data, to provide question answering and chit-chat
services. Consisting of two interactive modules of heterogeneous retrieval and
language model prompting, LittleMu first integrates structural, semi- and
unstructured knowledge sources to support accurate answers for a wide range of
questions. Then, we design delicate demonstrations named "Chain of Teach"
prompts to exploit the large-scale pre-trained model to handle complex
uncollected questions. Except for question answering, we develop other
educational services such as knowledge-grounded chit-chat. We test the system's
performance via both offline evaluation and online deployment. Since May 2020,
our LittleMu system has served over 80,000 users with over 300,000 queries from
over 500 courses on XuetangX MOOC platform, which continuously contributes to a
more convenient and fair education. Our code, services, and dataset will be
available at https://github.com/THU-KEG/VTA.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Uncertainty Quantification for Image-based Traffic Prediction across Cities. (arXiv:2308.06129v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06129">http://arxiv.org/abs/2308.06129</a></li>
<li>Code URL: https://github.com/alextimans/traffic4cast-uncertainty</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06129]] Uncertainty Quantification for Image-based Traffic Prediction across Cities(http://arxiv.org/abs/2308.06129)</code></li>
<li>Summary: <p>Despite the strong predictive performance of deep learning models for traffic
prediction, their widespread deployment in real-world intelligent
transportation systems has been restrained by a lack of interpretability.
Uncertainty quantification (UQ) methods provide an approach to induce
probabilistic reasoning, improve decision-making and enhance model deployment
potential. To gain a comprehensive picture of the usefulness of existing UQ
methods for traffic prediction and the relation between obtained uncertainties
and city-wide traffic dynamics, we investigate their application to a
large-scale image-based traffic dataset spanning multiple cities and time
periods. We compare two epistemic and two aleatoric UQ methods on both temporal
and spatio-temporal transfer tasks, and find that meaningful uncertainty
estimates can be recovered. We further demonstrate how uncertainty estimates
can be employed for unsupervised outlier detection on changes in city traffic
dynamics. We find that our approach can capture both temporal and spatial
effects on traffic behaviour in a representative case study for the city of
Moscow. Our work presents a further step towards boosting uncertainty awareness
in traffic prediction tasks, and aims to highlight the value contribution of UQ
methods to a better understanding of city traffic dynamics.
</p></li>
</ul>

<h3>Title: MaxFloodCast: Ensemble Machine Learning Model for Predicting Peak Inundation Depth And Decoding Influencing Features. (arXiv:2308.06228v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06228">http://arxiv.org/abs/2308.06228</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06228]] MaxFloodCast: Ensemble Machine Learning Model for Predicting Peak Inundation Depth And Decoding Influencing Features(http://arxiv.org/abs/2308.06228)</code></li>
<li>Summary: <p>Timely, accurate, and reliable information is essential for decision-makers,
emergency managers, and infrastructure operators during flood events. This
study demonstrates a proposed machine learning model, MaxFloodCast, trained on
physics-based hydrodynamic simulations in Harris County, offers efficient and
interpretable flood inundation depth predictions. Achieving an average
R-squared of 0.949 and a Root Mean Square Error of 0.61 ft on unseen data, it
proves reliable in forecasting peak flood inundation depths. Validated against
Hurricane Harvey and Storm Imelda, MaxFloodCast shows the potential in
supporting near-time floodplain management and emergency operations. The
model's interpretability aids decision-makers in offering critical information
to inform flood mitigation strategies, to prioritize areas with critical
facilities and to examine how rainfall in other watersheds influences flood
exposure in one area. The MaxFloodCast model enables accurate and interpretable
inundation depth predictions while significantly reducing computational time,
thereby supporting emergency response efforts and flood risk management more
effectively.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Revisiting N-CNN for Clinical Practice. (arXiv:2308.05877v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05877">http://arxiv.org/abs/2308.05877</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05877]] Revisiting N-CNN for Clinical Practice(http://arxiv.org/abs/2308.05877)</code></li>
<li>Summary: <p>This paper revisits the Neonatal Convolutional Neural Network (N-CNN) by
optimizing its hyperparameters and evaluating how they affect its
classification metrics, explainability and reliability, discussing their
potential impact in clinical practice. We have chosen hyperparameters that do
not modify the original N-CNN architecture, but mainly modify its learning rate
and training regularization. The optimization was done by evaluating the
improvement in F1 Score for each hyperparameter individually, and the best
hyperparameters were chosen to create a Tuned N-CNN. We also applied soft
labels derived from the Neonatal Facial Coding System, proposing a novel
approach for training facial expression classification models for neonatal pain
assessment. Interestingly, while the Tuned N-CNN results point towards
improvements in classification metrics and explainability, these improvements
did not directly translate to calibration performance. We believe that such
insights might have the potential to contribute to the development of more
reliable pain evaluation tools for newborns, aiding healthcare professionals in
delivering appropriate interventions and improving patient outcomes.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: YOLOrtho -- A Unified Framework for Teeth Enumeration and Dental Disease Detection. (arXiv:2308.05967v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05967">http://arxiv.org/abs/2308.05967</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05967]] YOLOrtho -- A Unified Framework for Teeth Enumeration and Dental Disease Detection(http://arxiv.org/abs/2308.05967)</code></li>
<li>Summary: <p>Detecting dental diseases through panoramic X-rays images is a standard
procedure for dentists. Normally, a dentist need to identify diseases and find
the infected teeth. While numerous machine learning models adopting this
two-step procedure have been developed, there has not been an end-to-end model
that can identify teeth and their associated diseases at the same time. To fill
the gap, we develop YOLOrtho, a unified framework for teeth enumeration and
dental disease detection. We develop our model on Dentex Challenge 2023 data,
which consists of three distinct types of annotated data. The first part is
labeled with quadrant, and the second part is labeled with quadrant and
enumeration and the third part is labeled with quadrant, enumeration and
disease. To further improve detection, we make use of Tufts Dental public
dataset. To fully utilize the data and learn both teeth detection and disease
identification simultaneously, we formulate diseases as attributes attached to
their corresponding teeth. Due to the nature of position relation in teeth
enumeration, We replace convolution layer with CoordConv in our model to
provide more position information for the model. We also adjust the model
architecture and insert one more upsampling layer in FPN in favor of large
object detection. Finally, we propose a post-process strategy for teeth layout
that corrects teeth enumeration based on linear sum assignment. Results from
experiments show that our model exceeds large Diffusion-based model.
</p></li>
</ul>

<h3>Title: Zero-shot Text-driven Physically Interpretable Face Editing. (arXiv:2308.05976v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05976">http://arxiv.org/abs/2308.05976</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05976]] Zero-shot Text-driven Physically Interpretable Face Editing(http://arxiv.org/abs/2308.05976)</code></li>
<li>Summary: <p>This paper proposes a novel and physically interpretable method for face
editing based on arbitrary text prompts. Different from previous
GAN-inversion-based face editing methods that manipulate the latent space of
GANs, or diffusion-based methods that model image manipulation as a reverse
diffusion process, we regard the face editing process as imposing vector flow
fields on face images, representing the offset of spatial coordinates and color
for each image pixel. Under the above-proposed paradigm, we represent the
vector flow field in two ways: 1) explicitly represent the flow vectors with
rasterized tensors, and 2) implicitly parameterize the flow vectors as
continuous, smooth, and resolution-agnostic neural fields, by leveraging the
recent advances of implicit neural representations. The flow vectors are
iteratively optimized under the guidance of the pre-trained Contrastive
Language-Image Pretraining~(CLIP) model by maximizing the correlation between
the edited image and the text prompt. We also propose a learning-based one-shot
face editing framework, which is fast and adaptable to any text prompt input.
Our method can also be flexibly extended to real-time video face editing.
Compared with state-of-the-art text-driven face editing methods, our method can
generate physically interpretable face editing results with high identity
consistency and image quality. Our code will be made publicly available.
</p></li>
</ul>

<h3>Title: Masked-Attention Diffusion Guidance for Spatially Controlling Text-to-Image Generation. (arXiv:2308.06027v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06027">http://arxiv.org/abs/2308.06027</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06027]] Masked-Attention Diffusion Guidance for Spatially Controlling Text-to-Image Generation(http://arxiv.org/abs/2308.06027)</code></li>
<li>Summary: <p>Text-to-image synthesis has achieved high-quality results with recent
advances in diffusion models. However, text input alone has high spatial
ambiguity and limited user controllability. Most existing methods allow spatial
control through additional visual guidance (e.g, sketches and semantic masks)
but require additional training with annotated images. In this paper, we
propose a method for spatially controlling text-to-image generation without
further training of diffusion models. Our method is based on the insight that
the cross-attention maps reflect the positional relationship between words and
pixels. Our aim is to control the attention maps according to given semantic
masks and text prompts. To this end, we first explore a simple approach of
directly swapping the cross-attention maps with constant maps computed from the
semantic regions. Moreover, we propose masked-attention guidance, which can
generate images more faithful to semantic masks than the first approach.
Masked-attention guidance indirectly controls attention to each word and pixel
according to the semantic regions by manipulating noise images fed to diffusion
models. Experiments show that our method enables more accurate spatial control
than baselines qualitatively and quantitatively.
</p></li>
</ul>

<h3>Title: Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning. (arXiv:2308.06038v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06038">http://arxiv.org/abs/2308.06038</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06038]] Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning(http://arxiv.org/abs/2308.06038)</code></li>
<li>Summary: <p>Benefiting from prompt tuning, recent years have witnessed the promising
performance of pre-trained vision-language models, e.g., CLIP, on versatile
downstream tasks. In this paper, we focus on a particular setting of learning
adaptive prompts on the fly for each test sample from an unseen new domain,
which is known as test-time prompt tuning (TPT). Existing TPT methods typically
rely on data augmentation and confidence selection. However, conventional data
augmentation techniques, e.g., random resized crops, suffers from the lack of
data diversity, while entropy-based confidence selection alone is not
sufficient to guarantee prediction fidelity. To address these issues, we
propose a novel TPT method, named DiffTPT, which leverages pre-trained
diffusion models to generate diverse and informative new data. Specifically, we
incorporate augmented data by both conventional method and pre-trained stable
diffusion to exploit their respective merits, improving the models ability to
adapt to unknown new test data. Moreover, to ensure the prediction fidelity of
generated data, we introduce a cosine similarity-based filtration technique to
select the generated data with higher similarity to the single test sample. Our
experiments on test datasets with distribution shifts and unseen categories
demonstrate that DiffTPT improves the zero-shot accuracy by an average of
5.13\% compared to the state-of-the-art TPT method. Our code and models will be
publicly released.
</p></li>
</ul>

<h3>Title: Head Rotation in Denoising Diffusion Models. (arXiv:2308.06057v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06057">http://arxiv.org/abs/2308.06057</a></li>
<li>Code URL: https://github.com/asperti/head-rotation</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06057]] Head Rotation in Denoising Diffusion Models(http://arxiv.org/abs/2308.06057)</code></li>
<li>Summary: <p>Denoising Diffusion Models (DDM) are emerging as the cutting-edge technology
in the realm of deep generative modeling, challenging the dominance of
Generative Adversarial Networks. However, effectively exploring the latent
space's semantics and identifying compelling trajectories for manipulating and
editing important attributes of the generated samples remains challenging,
primarily due to the high-dimensional nature of the latent space. In this
study, we specifically concentrate on face rotation, which is known to be one
of the most intricate editing operations. By leveraging a recent embedding
technique for Denoising Diffusion Implicit Models (DDIM), we achieve, in many
cases, noteworthy manipulations encompassing a wide rotation angle of $\pm
30^o$, preserving the distinct characteristics of the individual. Our
methodology exploits the computation of trajectories approximating clouds of
latent representations of dataset samples with different yaw rotations through
linear regression. Specific trajectories are obtained by restricting the
analysis to subsets of data sharing significant attributes with the source
image. One of these attributes is the light provenance: a byproduct of our
research is a labeling of CelebA, categorizing images into three major groups
based on the illumination direction: left, center, and right.
</p></li>
</ul>

<h3>Title: Diffusion-based Visual Counterfactual Explanations -- Towards Systematic Quantitative Evaluation. (arXiv:2308.06100v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06100">http://arxiv.org/abs/2308.06100</a></li>
<li>Code URL: https://github.com/cairo-thws/dbvce_eval</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06100]] Diffusion-based Visual Counterfactual Explanations -- Towards Systematic Quantitative Evaluation(http://arxiv.org/abs/2308.06100)</code></li>
<li>Summary: <p>Latest methods for visual counterfactual explanations (VCE) harness the power
of deep generative models to synthesize new examples of high-dimensional images
of impressive quality. However, it is currently difficult to compare the
performance of these VCE methods as the evaluation procedures largely vary and
often boil down to visual inspection of individual examples and small scale
user studies. In this work, we propose a framework for systematic, quantitative
evaluation of the VCE methods and a minimal set of metrics to be used. We use
this framework to explore the effects of certain crucial design choices in the
latest diffusion-based generative models for VCEs of natural image
classification (ImageNet). We conduct a battery of ablation-like experiments,
generating thousands of VCEs for a suite of classifiers of various complexity,
accuracy and robustness. Our findings suggest multiple directions for future
advancements and improvements of VCE methods. By sharing our methodology and
our approach to tackle the computational challenges of such a study on a
limited hardware setup (including the complete code base), we offer a valuable
guidance for researchers in the field fostering consistency and transparency in
the assessment of counterfactual explanations.
</p></li>
</ul>

<h3>Title: Taming the Power of Diffusion Models for High-Quality Virtual Try-On with Appearance Flow. (arXiv:2308.06101v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06101">http://arxiv.org/abs/2308.06101</a></li>
<li>Code URL: https://github.com/bcmi/DCI-VTON-Virtual-Try-On</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06101]] Taming the Power of Diffusion Models for High-Quality Virtual Try-On with Appearance Flow(http://arxiv.org/abs/2308.06101)</code></li>
<li>Summary: <p>Virtual try-on is a critical image synthesis task that aims to transfer
clothes from one image to another while preserving the details of both humans
and clothes. While many existing methods rely on Generative Adversarial
Networks (GANs) to achieve this, flaws can still occur, particularly at high
resolutions. Recently, the diffusion model has emerged as a promising
alternative for generating high-quality images in various applications.
However, simply using clothes as a condition for guiding the diffusion model to
inpaint is insufficient to maintain the details of the clothes. To overcome
this challenge, we propose an exemplar-based inpainting approach that leverages
a warping module to guide the diffusion model's generation effectively. The
warping module performs initial processing on the clothes, which helps to
preserve the local details of the clothes. We then combine the warped clothes
with clothes-agnostic person image and add noise as the input of diffusion
model. Additionally, the warped clothes is used as local conditions for each
denoising process to ensure that the resulting output retains as much detail as
possible. Our approach, namely Diffusion-based Conditional Inpainting for
Virtual Try-ON (DCI-VTON), effectively utilizes the power of the diffusion
model, and the incorporation of the warping module helps to produce
high-quality and realistic virtual try-on results. Experimental results on
VITON-HD demonstrate the effectiveness and superiority of our method.
</p></li>
</ul>

<h3>Title: DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models. (arXiv:2308.06160v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06160">http://arxiv.org/abs/2308.06160</a></li>
<li>Code URL: https://github.com/showlab/datasetdm</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06160]] DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models(http://arxiv.org/abs/2308.06160)</code></li>
<li>Summary: <p>Current deep networks are very data-hungry and benefit from training on
largescale datasets, which are often time-consuming to collect and annotate. By
contrast, synthetic data can be generated infinitely using generative models
such as DALL-E and diffusion models, with minimal effort and cost. In this
paper, we present DatasetDM, a generic dataset generation model that can
produce diverse synthetic images and the corresponding high-quality perception
annotations (e.g., segmentation masks, and depth). Our method builds upon the
pre-trained diffusion model and extends text-guided image synthesis to
perception data generation. We show that the rich latent code of the diffusion
model can be effectively decoded as accurate perception annotations using a
decoder module. Training the decoder only needs less than 1% (around 100
images) manually labeled images, enabling the generation of an infinitely large
annotated dataset. Then these synthetic data can be used for training various
perception models for downstream tasks. To showcase the power of the proposed
approach, we generate datasets with rich dense pixel-wise labels for a wide
range of downstream tasks, including semantic segmentation, instance
segmentation, and depth estimation. Notably, it achieves 1) state-of-the-art
results on semantic segmentation and instance segmentation; 2) significantly
more robust on domain generalization than using the real data alone; and
state-of-the-art results in zero-shot segmentation setting; and 3) flexibility
for efficient application and novel task composition (e.g., image editing). The
project website and code can be found at
https://weijiawu.github.io/DatasetDM_page/ and
https://github.com/showlab/DatasetDM, respectively
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Temporally-Adaptive Models for Efficient Video Understanding. (arXiv:2308.05787v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05787">http://arxiv.org/abs/2308.05787</a></li>
<li>Code URL: https://github.com/alibaba-mmai-research/TAdaConv</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05787]] Temporally-Adaptive Models for Efficient Video Understanding(http://arxiv.org/abs/2308.05787)</code></li>
<li>Summary: <p>Spatial convolutions are extensively used in numerous deep video models. It
fundamentally assumes spatio-temporal invariance, i.e., using shared weights
for every location in different frames. This work presents Temporally-Adaptive
Convolutions (TAdaConv) for video understanding, which shows that adaptive
weight calibration along the temporal dimension is an efficient way to
facilitate modeling complex temporal dynamics in videos. Specifically, TAdaConv
empowers spatial convolutions with temporal modeling abilities by calibrating
the convolution weights for each frame according to its local and global
temporal context. Compared to existing operations for temporal modeling,
TAdaConv is more efficient as it operates over the convolution kernels instead
of the features, whose dimension is an order of magnitude smaller than the
spatial resolutions. Further, kernel calibration brings an increased model
capacity. Based on this readily plug-in operation TAdaConv as well as its
extension, i.e., TAdaConvV2, we construct TAdaBlocks to empower ConvNeXt and
Vision Transformer to have strong temporal modeling capabilities. Empirical
results show TAdaConvNeXtV2 and TAdaFormer perform competitively against
state-of-the-art convolutional and Transformer-based models in various video
understanding benchmarks. Our codes and models are released at:
https://github.com/alibaba-mmai-research/TAdaConv.
</p></li>
</ul>

<h3>Title: Vision Backbone Enhancement via Multi-Stage Cross-Scale Attention. (arXiv:2308.05872v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05872">http://arxiv.org/abs/2308.05872</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05872]] Vision Backbone Enhancement via Multi-Stage Cross-Scale Attention(http://arxiv.org/abs/2308.05872)</code></li>
<li>Summary: <p>Convolutional neural networks (CNNs) and vision transformers (ViTs) have
achieved remarkable success in various vision tasks. However, many
architectures do not consider interactions between feature maps from different
stages and scales, which may limit their performance. In this work, we propose
a simple add-on attention module to overcome these limitations via multi-stage
and cross-scale interactions. Specifically, the proposed Multi-Stage
Cross-Scale Attention (\meth) module takes feature maps from different stages
to enable multi-stage interactions and achieves cross-scale interactions by
computing self-attention at different scales based on the multi-stage feature
maps. Our experiments on several downstream tasks show that \meth~provides a
significant performance boost with modest additional FLOPs and runtime.
</p></li>
</ul>

<h3>Title: Compositional Learning in Transformer-Based Human-Object Interaction Detection. (arXiv:2308.05961v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05961">http://arxiv.org/abs/2308.05961</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05961]] Compositional Learning in Transformer-Based Human-Object Interaction Detection(http://arxiv.org/abs/2308.05961)</code></li>
<li>Summary: <p>Human-object interaction (HOI) detection is an important part of
understanding human activities and visual scenes. The long-tailed distribution
of labeled instances is a primary challenge in HOI detection, promoting
research in few-shot and zero-shot learning. Inspired by the combinatorial
nature of HOI triplets, some existing approaches adopt the idea of
compositional learning, in which object and action features are learned
individually and re-composed as new training samples. However, these methods
follow the CNN-based two-stage paradigm with limited feature extraction
ability, and often rely on auxiliary information for better performance.
Without introducing any additional information, we creatively propose a
transformer-based framework for compositional HOI learning. Human-object pair
representations and interaction representations are re-composed across
different HOI instances, which involves richer contextual information and
promotes the generalization of knowledge. Experiments show our simple but
effective method achieves state-of-the-art performance, especially on rare HOI
classes.
</p></li>
</ul>

<h3>Title: ViGT: Proposal-free Video Grounding with Learnable Token in Transformer. (arXiv:2308.06009v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06009">http://arxiv.org/abs/2308.06009</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06009]] ViGT: Proposal-free Video Grounding with Learnable Token in Transformer(http://arxiv.org/abs/2308.06009)</code></li>
<li>Summary: <p>The video grounding (VG) task aims to locate the queried action or event in
an untrimmed video based on rich linguistic descriptions. Existing
proposal-free methods are trapped in complex interaction between video and
query, overemphasizing cross-modal feature fusion and feature correlation for
VG. In this paper, we propose a novel boundary regression paradigm that
performs regression token learning in a transformer. Particularly, we present a
simple but effective proposal-free framework, namely Video Grounding
Transformer (ViGT), which predicts the temporal boundary using a learnable
regression token rather than multi-modal or cross-modal features. In ViGT, the
benefits of a learnable token are manifested as follows. (1) The token is
unrelated to the video or the query and avoids data bias toward the original
video and query. (2) The token simultaneously performs global context
aggregation from video and query features. First, we employed a sharing feature
encoder to project both video and query into a joint feature space before
performing cross-modal co-attention (i.e., video-to-query attention and
query-to-video attention) to highlight discriminative features in each
modality. Furthermore, we concatenated a learnable regression token [REG] with
the video and query features as the input of a vision-language transformer.
Finally, we utilized the token [REG] to predict the target moment and visual
features to constrain the foreground and background probabilities at each
timestamp. The proposed ViGT performed well on three public datasets: ANet
Captions, TACoS and YouCookII. Extensive ablation studies and qualitative
analysis further validated the interpretability of ViGT.
</p></li>
</ul>

<h3>Title: Experts Weights Averaging: A New General Training Scheme for Vision Transformers. (arXiv:2308.06093v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06093">http://arxiv.org/abs/2308.06093</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06093]] Experts Weights Averaging: A New General Training Scheme for Vision Transformers(http://arxiv.org/abs/2308.06093)</code></li>
<li>Summary: <p>Structural re-parameterization is a general training scheme for Convolutional
Neural Networks (CNNs), which achieves performance improvement without
increasing inference cost. As Vision Transformers (ViTs) are gradually
surpassing CNNs in various visual tasks, one may question: if a training scheme
specifically for ViTs exists that can also achieve performance improvement
without increasing inference cost? Recently, Mixture-of-Experts (MoE) has
attracted increasing attention, as it can efficiently scale up the capacity of
Transformers at a fixed cost through sparsely activated experts. Considering
that MoE can also be viewed as a multi-branch structure, can we utilize MoE to
implement a ViT training scheme similar to structural re-parameterization? In
this paper, we affirmatively answer these questions, with a new general
training strategy for ViTs. Specifically, we decouple the training and
inference phases of ViTs. During training, we replace some Feed-Forward
Networks (FFNs) of the ViT with specially designed, more efficient MoEs that
assign tokens to experts by random uniform partition, and perform Experts
Weights Averaging (EWA) on these MoEs at the end of each iteration. After
training, we convert each MoE into an FFN by averaging the experts,
transforming the model back into original ViT for inference. We further provide
a theoretical analysis to show why and how it works. Comprehensive experiments
across various 2D and 3D visual tasks, ViT architectures, and datasets validate
the effectiveness and generalizability of the proposed training scheme.
Besides, our training scheme can also be applied to improve performance when
fine-tuning ViTs. Lastly, but equally important, the proposed EWA technique can
significantly improve the effectiveness of naive MoE in various 2D visual small
datasets and 3D visual tasks.
</p></li>
</ul>

<h3>Title: Exploring Predicate Visual Context in Detecting of Human-Object Interactions. (arXiv:2308.06202v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06202">http://arxiv.org/abs/2308.06202</a></li>
<li>Code URL: https://github.com/fredzzhang/pvic</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06202]] Exploring Predicate Visual Context in Detecting of Human-Object Interactions(http://arxiv.org/abs/2308.06202)</code></li>
<li>Summary: <p>Recently, the DETR framework has emerged as the dominant approach for
human--object interaction (HOI) research. In particular, two-stage
transformer-based HOI detectors are amongst the most performant and
training-efficient approaches. However, these often condition HOI
classification on object features that lack fine-grained contextual
information, eschewing pose and orientation information in favour of visual
cues about object identity and box extremities. This naturally hinders the
recognition of complex or ambiguous interactions. In this work, we study these
issues through visualisations and carefully designed experiments. Accordingly,
we investigate how best to re-introduce image features via cross-attention.
With an improved query design, extensive exploration of keys and values, and
box pair positional embeddings as spatial guidance, our model with enhanced
predicate visual context (PViC) outperforms state-of-the-art methods on the
HICO-DET and V-COCO benchmarks, while maintaining low training cost.
</p></li>
</ul>

<h3>Title: Optimizing transformer-based machine translation model for single GPU training: a hyperparameter ablation study. (arXiv:2308.06017v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06017">http://arxiv.org/abs/2308.06017</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06017]] Optimizing transformer-based machine translation model for single GPU training: a hyperparameter ablation study(http://arxiv.org/abs/2308.06017)</code></li>
<li>Summary: <p>In machine translation tasks, the relationship between model complexity and
performance is often presumed to be linear, driving an increase in the number
of parameters and consequent demands for computational resources like multiple
GPUs. To explore this assumption, this study systematically investigates the
effects of hyperparameters through ablation on a sequence-to-sequence machine
translation pipeline, utilizing a single NVIDIA A100 GPU. Contrary to
expectations, our experiments reveal that combinations with the most parameters
were not necessarily the most effective. This unexpected insight prompted a
careful reduction in parameter sizes, uncovering "sweet spots" that enable
training sophisticated models on a single GPU without compromising translation
quality. The findings demonstrate an intricate relationship between
hyperparameter selection, model size, and computational resource needs. The
insights from this study contribute to the ongoing efforts to make machine
translation more accessible and cost-effective, emphasizing the importance of
precise hyperparameter tuning over mere scaling.
</p></li>
</ul>

<h3>Title: Task Conditioned BERT for Joint Intent Detection and Slot-filling. (arXiv:2308.06165v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06165">http://arxiv.org/abs/2308.06165</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06165]] Task Conditioned BERT for Joint Intent Detection and Slot-filling(http://arxiv.org/abs/2308.06165)</code></li>
<li>Summary: <p>Dialogue systems need to deal with the unpredictability of user intents to
track dialogue state and the heterogeneity of slots to understand user
preferences. In this paper we investigate the hypothesis that solving these
challenges as one unified model will allow the transfer of parameter support
data across the different tasks. The proposed principled model is based on a
Transformer encoder, trained on multiple tasks, and leveraged by a rich input
that conditions the model on the target inferences. Conditioning the
Transformer encoder on multiple target inferences over the same corpus, i.e.,
intent and multiple slot types, allows learning richer language interactions
than a single-task model would be able to. In fact, experimental results
demonstrate that conditioning the model on an increasing number of dialogue
inference tasks leads to improved results: on the MultiWOZ dataset, the joint
intent and slot detection can be improved by 3.2\% by conditioning on intent,
10.8\% by conditioning on slot and 14.4\% by conditioning on both intent and
slots. Moreover, on real conversations with Farfetch costumers, the proposed
conditioned BERT can achieve high joint-goal and intent detection performance
throughout a dialogue.
</p></li>
</ul>

<h3>Title: Composable Function-preserving Expansions for Transformer Architectures. (arXiv:2308.06103v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06103">http://arxiv.org/abs/2308.06103</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06103]] Composable Function-preserving Expansions for Transformer Architectures(http://arxiv.org/abs/2308.06103)</code></li>
<li>Summary: <p>Training state-of-the-art neural networks requires a high cost in terms of
compute and time. Model scale is recognized to be a critical factor to achieve
and improve the state-of-the-art. Increasing the scale of a neural network
normally requires restarting from scratch by randomly initializing all the
parameters of the model, as this implies a change of architecture's parameters
that does not allow for a straightforward transfer of knowledge from smaller
size models. In this work, we propose six composable transformations to
incrementally increase the size of transformer-based neural networks while
preserving functionality, allowing to expand the capacity of the model as
needed. We provide proof of exact function preservation under minimal
initialization constraints for each transformation. The proposed methods may
enable efficient training pipelines for larger and more powerful models by
progressively expanding the architecture throughout training.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: DIG In: Evaluating Disparities in Image Generations with Indicators for Geographic Diversity. (arXiv:2308.06198v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06198">http://arxiv.org/abs/2308.06198</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06198]] DIG In: Evaluating Disparities in Image Generations with Indicators for Geographic Diversity(http://arxiv.org/abs/2308.06198)</code></li>
<li>Summary: <p>The unprecedented photorealistic results achieved by recent text-to-image
generative systems and their increasing use as plug-and-play content creation
solutions make it crucial to understand their potential biases. In this work,
we introduce three indicators to evaluate the realism, diversity and
prompt-generation consistency of text-to-image generative systems when prompted
to generate objects from across the world. Our indicators complement
qualitative analysis of the broader impact of such systems by enabling
automatic and efficient benchmarking of geographic disparities, an important
step towards building responsible visual content creation systems. We use our
proposed indicators to analyze potential geographic biases in state-of-the-art
visual content creation systems and find that: (1) models have less realism and
diversity of generations when prompting for Africa and West Asia than Europe,
(2) prompting with geographic information comes at a cost to prompt-consistency
and diversity of generated images, and (3) models exhibit more region-level
disparities for some objects than others. Perhaps most interestingly, our
indicators suggest that progress in image generation quality has come at the
cost of real-world geographic representation. Our comprehensive evaluation
constitutes a crucial step towards ensuring a positive experience of visual
content creation for everyone.
</p></li>
</ul>

<h3>Title: Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling. (arXiv:2308.06077v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06077">http://arxiv.org/abs/2308.06077</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06077]] Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling(http://arxiv.org/abs/2308.06077)</code></li>
<li>Summary: <p>Generative language models (LMs) have become omnipresent across data science.
For a wide variety of tasks, inputs can be phrased as natural language prompts
for an LM, from whose output the solution can then be extracted. LM performance
has consistently been increasing with model size - but so has the monetary cost
of querying the ever larger models. Importantly, however, not all inputs are
equally hard: some require larger LMs for obtaining a satisfactory solution,
whereas for others smaller LMs suffice. Based on this fact, we design a
framework for Cost-Effective Language Model Choice (CELMOC). Given a set of
inputs and a set of candidate LMs, CELMOC judiciously assigns each input to an
LM predicted to do well on the input according to a so-called meta-model,
aiming to achieve high overall performance at low cost. The cost-performance
trade-off can be flexibly tuned by the user. Options include, among others,
maximizing total expected performance (or the number of processed inputs) while
staying within a given cost budget, or minimizing total cost while processing
all inputs. We evaluate CELMOC on 14 datasets covering five natural language
tasks, using four candidate LMs of vastly different size and cost. With CELMOC,
we match the performance of the largest available LM while achieving a cost
reduction of 63%. Via our publicly available library, researchers as well as
practitioners can thus save large amounts of money without sacrificing
performance.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Encode-Store-Retrieve: Enhancing Memory Augmentation through Language-Encoded Egocentric Perception. (arXiv:2308.05822v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05822">http://arxiv.org/abs/2308.05822</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05822]] Encode-Store-Retrieve: Enhancing Memory Augmentation through Language-Encoded Egocentric Perception(http://arxiv.org/abs/2308.05822)</code></li>
<li>Summary: <p>We depend on our own memory to encode, store, and retrieve our experiences.
However, memory lapses can occur. One promising avenue for achieving memory
augmentation is through the use of augmented reality head-mounted displays to
capture and preserve egocentric videos, a practice commonly referred to as life
logging. However, a significant challenge arises from the sheer volume of video
data generated through life logging, as the current technology lacks the
capability to encode and store such large amounts of data efficiently. Further,
retrieving specific information from extensive video archives requires
substantial computational power, further complicating the task of quickly
accessing desired content. To address these challenges, we propose a memory
augmentation system that involves leveraging natural language encoding for
video data and storing them in a vector database. This approach harnesses the
power of large vision language models to perform the language encoding process.
Additionally, we propose using large language models to facilitate natural
language querying. Our system underwent extensive evaluation using the QA-Ego4D
dataset and achieved state-of-the-art results with a BLEU score of 8.3,
outperforming conventional machine learning models that scored between 3.4 and
5.8. Additionally, in a user study, our system received a higher mean response
score of 4.13/5 compared to the human participants' score of 2.46/5 on
real-life episodic memory tasks.
</p></li>
</ul>

<h3>Title: PIPPA: A Partially Synthetic Conversational Dataset. (arXiv:2308.05884v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05884">http://arxiv.org/abs/2308.05884</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05884]] PIPPA: A Partially Synthetic Conversational Dataset(http://arxiv.org/abs/2308.05884)</code></li>
<li>Summary: <p>With the emergence of increasingly powerful large language models, there is a
burgeoning interest in leveraging these models for casual conversation and
role-play applications. However, existing conversational and role-playing
datasets often fail to capture the diverse and nuanced interactions typically
exhibited by real-world role-play participants. To address this limitation and
contribute to the rapidly growing field, we introduce a partially-synthetic
dataset named PIPPA (Personal Interaction Pairs between People and AI). PIPPA
is a result of a community-driven crowdsourcing effort involving a group of
role-play enthusiasts. The dataset comprises over 1 million utterances that are
distributed across 26,000 conversation sessions and provides a rich resource
for researchers and AI developers to explore and refine conversational AI
systems in the context of role-play scenarios.
</p></li>
</ul>

<h3>Title: Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models. (arXiv:2308.06111v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06111">http://arxiv.org/abs/2308.06111</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06111]] Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models(http://arxiv.org/abs/2308.06111)</code></li>
<li>Summary: <p>Auditing financial documents is a very tedious and time-consuming process. As
of today, it can already be simplified by employing AI-based solutions to
recommend relevant text passages from a report for each legal requirement of
rigorous accounting standards. However, these methods need to be fine-tuned
regularly, and they require abundant annotated data, which is often lacking in
industrial environments. Hence, we present ZeroShotALI, a novel recommender
system that leverages a state-of-the-art large language model (LLM) in
conjunction with a domain-specifically optimized transformer-based
text-matching solution. We find that a two-step approach of first retrieving a
number of best matching document sections per legal requirement with a custom
BERT-based model and second filtering these selections using an LLM yields
significant performance improvements over existing approaches.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: SegDA: Maximum Separable Segment Mask with Pseudo Labels for Domain Adaptive Semantic Segmentation. (arXiv:2308.05851v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05851">http://arxiv.org/abs/2308.05851</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05851]] SegDA: Maximum Separable Segment Mask with Pseudo Labels for Domain Adaptive Semantic Segmentation(http://arxiv.org/abs/2308.05851)</code></li>
<li>Summary: <p>Unsupervised Domain Adaptation (UDA) aims to solve the problem of label
scarcity of the target domain by transferring the knowledge from the label rich
source domain. Usually, the source domain consists of synthetic images for
which the annotation is easily obtained using the well known computer graphics
techniques. However, obtaining annotation for real world images (target domain)
require lot of manual annotation effort and is very time consuming because it
requires per pixel annotation. To address this problem we propose SegDA module
to enhance transfer performance of UDA methods by learning the maximum
separable segment representation. This resolves the problem of identifying
visually similar classes like pedestrian/rider, sidewalk/road etc. We leveraged
Equiangular Tight Frame (ETF) classifier inspired from Neural Collapse for
maximal separation between segment classes. This causes the source domain pixel
representation to collapse to a single vector forming a simplex vertices which
are aligned to the maximal separable ETF classifier. We use this phenomenon to
propose the novel architecture for domain adaptation of segment representation
for target domain. Additionally, we proposed to estimate the noise in labelling
the target domain images and update the decoder for noise correction which
encourages the discovery of pixels for classes not identified in pseudo labels.
We have used four UDA benchmarks simulating synthetic-to-real,
daytime-to-nighttime, clear-to-adverse weather scenarios. Our proposed approach
outperforms +2.2 mIoU on GTA -&gt; Cityscapes, +2.0 mIoU on Synthia -&gt; Cityscapes,
+5.9 mIoU on Cityscapes -&gt; DarkZurich, +2.6 mIoU on Cityscapes -&gt; ACDC.
</p></li>
</ul>

<h3>Title: Semantic-embedded Similarity Prototype for Scene Recognition. (arXiv:2308.05896v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05896">http://arxiv.org/abs/2308.05896</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05896]] Semantic-embedded Similarity Prototype for Scene Recognition(http://arxiv.org/abs/2308.05896)</code></li>
<li>Summary: <p>Due to the high inter-class similarity caused by the complex composition
within scenes and the co-existing objects across scenes, various studies have
explored object semantic knowledge within scenes to improve scene recognition.
However, a resulting issue arises as semantic segmentation or object detection
techniques demand heavy computational power, thereby burdening the network
considerably. This limitation often renders object-assisted approaches
incompatible with edge devices. In contrast, this paper proposes a
semantic-based similarity prototype that assists the scene recognition network
to achieve higher accuracy without increasing network parameters. It is simple
and can be plug-and-played into existing pipelines. More specifically, a
statistical strategy is introduced to depict semantic knowledge in scenes as
class-level semantic representations. These representations are utilized to
explore inter-class correlations, ultimately constructing a similarity
prototype. Furthermore, we propose two ways to use the similarity prototype to
support network training from the perspective of gradient label softening and
batch-level contrastive loss, respectively. Comprehensive evaluations on
multiple benchmarks show that our similarity prototype enhances the performance
of existing networks without adding any computational burden. Code and the
statistical similarity prototype will be available soon.
</p></li>
</ul>

<h3>Title: FoodSAM: Any Food Segmentation. (arXiv:2308.05938v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05938">http://arxiv.org/abs/2308.05938</a></li>
<li>Code URL: https://github.com/jamesjg/foodsam</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05938]] FoodSAM: Any Food Segmentation(http://arxiv.org/abs/2308.05938)</code></li>
<li>Summary: <p>In this paper, we explore the zero-shot capability of the Segment Anything
Model (SAM) for food image segmentation. To address the lack of class-specific
information in SAM-generated masks, we propose a novel framework, called
FoodSAM. This innovative approach integrates the coarse semantic mask with
SAM-generated masks to enhance semantic segmentation quality. Besides, we
recognize that the ingredients in food can be supposed as independent
individuals, which motivated us to perform instance segmentation on food
images. Furthermore, FoodSAM extends its zero-shot capability to encompass
panoptic segmentation by incorporating an object detector, which renders
FoodSAM to effectively capture non-food object information. Drawing inspiration
from the recent success of promptable segmentation, we also extend FoodSAM to
promptable segmentation, supporting various prompt variants. Consequently,
FoodSAM emerges as an all-encompassing solution capable of segmenting food
items at multiple levels of granularity. Remarkably, this pioneering framework
stands as the first-ever work to achieve instance, panoptic, and promptable
segmentation on food images. Extensive experiments demonstrate the feasibility
and impressing performance of FoodSAM, validating SAM's potential as a
prominent and influential tool within the domain of food image segmentation. We
release our code at https://github.com/jamesjg/FoodSAM.
</p></li>
</ul>

<h3>Title: Spatial-information Guided Adaptive Context-aware Network for Efficient RGB-D Semantic Segmentation. (arXiv:2308.06024v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06024">http://arxiv.org/abs/2308.06024</a></li>
<li>Code URL: https://github.com/mvme-hbut/sgacnet</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06024]] Spatial-information Guided Adaptive Context-aware Network for Efficient RGB-D Semantic Segmentation(http://arxiv.org/abs/2308.06024)</code></li>
<li>Summary: <p>Efficient RGB-D semantic segmentation has received considerable attention in
mobile robots, which plays a vital role in analyzing and recognizing
environmental information. According to previous studies, depth information can
provide corresponding geometric relationships for objects and scenes, but
actual depth data usually exist as noise. To avoid unfavorable effects on
segmentation accuracy and computation, it is necessary to design an efficient
framework to leverage cross-modal correlations and complementary cues. In this
paper, we propose an efficient lightweight encoder-decoder network that reduces
the computational parameters and guarantees the robustness of the algorithm.
Working with channel and spatial fusion attention modules, our network
effectively captures multi-level RGB-D features. A globally guided local
affinity context module is proposed to obtain sufficient high-level context
information. The decoder utilizes a lightweight residual unit that combines
short- and long-distance information with a few redundant computations.
Experimental results on NYUv2, SUN RGB-D, and Cityscapes datasets show that our
method achieves a better trade-off among segmentation accuracy, inference time,
and parameters than the state-of-the-art methods. The source code will be at
https://github.com/MVME-HBUT/SGACNet
</p></li>
</ul>

<h3>Title: CompTLL-UNet: Compressed Domain Text-Line Localization in Challenging Handwritten Documents using Deep Feature Learning from JPEG Coefficients. (arXiv:2308.06142v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.06142">http://arxiv.org/abs/2308.06142</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.06142]] CompTLL-UNet: Compressed Domain Text-Line Localization in Challenging Handwritten Documents using Deep Feature Learning from JPEG Coefficients(http://arxiv.org/abs/2308.06142)</code></li>
<li>Summary: <p>Automatic localization of text-lines in handwritten documents is still an
open and challenging research problem. Various writing issues such as uneven
spacing between the lines, oscillating and touching text, and the presence of
skew become much more challenging when the case of complex handwritten document
images are considered for segmentation directly in their respective compressed
representation. This is because, the conventional way of processing compressed
documents is through decompression, but here in this paper, we propose an idea
that employs deep feature learning directly from the JPEG compressed
coefficients without full decompression to accomplish text-line localization in
the JPEG compressed domain. A modified U-Net architecture known as Compressed
Text-Line Localization Network (CompTLL-UNet) is designed to accomplish it. The
model is trained and tested with JPEG compressed version of benchmark datasets
including ICDAR2017 (cBAD) and ICDAR2019 (cBAD), reporting the state-of-the-art
performance with reduced storage and computational costs in the JPEG compressed
domain.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
