<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Explainable Artificial Intelligence and Causal Inference based ATM Fraud Detection. (arXiv:2211.10595v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10595">http://arxiv.org/abs/2211.10595</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10595] Explainable Artificial Intelligence and Causal Inference based ATM Fraud Detection](http://arxiv.org/abs/2211.10595)</code></li>
<li>Summary: <p>Gaining the trust of customers and providing them empathy are very critical
in the financial domain. Frequent occurrence of fraudulent activities affects
these two factors. Hence, financial organizations and banks must take utmost
care to mitigate them. Among them, ATM fraudulent transaction is a common
problem faced by banks. There following are the critical challenges involved in
fraud datasets: the dataset is highly imbalanced, the fraud pattern is
changing, etc. Owing to the rarity of fraudulent activities, Fraud detection
can be formulated as either a binary classification problem or One class
classification (OCC). In this study, we handled these techniques on an ATM
transactions dataset collected from India. In binary classification, we
investigated the effectiveness of various over-sampling techniques, such as the
Synthetic Minority Oversampling Technique (SMOTE) and its variants, Generative
Adversarial Networks (GAN), to achieve oversampling. Further, we employed
various machine learning techniques viz., Naive Bayes (NB), Logistic Regression
(LR), Support Vector Machine (SVM), Decision Tree (DT), Random Forest (RF),
Gradient Boosting Tree (GBT), Multi-layer perceptron (MLP). GBT outperformed
the rest of the models by achieving 0.963 AUC, and DT stands second with 0.958
AUC. DT is the winner if the complexity and interpretability aspects are
considered. Among all the oversampling approaches, SMOTE and its variants were
observed to perform better. In OCC, IForest attained 0.959 CR, and OCSVM
secured second place with 0.947 CR. Further, we incorporated explainable
artificial intelligence (XAI) and causal inference (CI) in the fraud detection
framework and studied it through various analyses.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: PIDray: A Large-scale X-ray Benchmark for Real-World Prohibited Item Detection. (arXiv:2211.10763v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10763">http://arxiv.org/abs/2211.10763</a></li>
<li>Code URL: <a href="https://github.com/lutao2021/pidray">https://github.com/lutao2021/pidray</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10763] PIDray: A Large-scale X-ray Benchmark for Real-World Prohibited Item Detection](http://arxiv.org/abs/2211.10763)</code></li>
<li>Summary: <p>Automatic security inspection relying on computer vision technology is a
challenging task in real-world scenarios due to many factors, such as
intra-class variance, class imbalance, and occlusion. Most previous methods
rarely touch the cases where the prohibited items are deliberately hidden in
messy objects because of the scarcity of large-scale datasets, hindering their
applications. To address this issue and facilitate related research, we present
a large-scale dataset, named PIDray, which covers various cases in real-world
scenarios for prohibited item detection, especially for deliberately hidden
items. In specific, PIDray collects 124,486 X-ray images for $12$ categories of
prohibited items, and each image is manually annotated with careful inspection,
which makes it, to our best knowledge, to largest prohibited items detection
dataset to date. Meanwhile, we propose a general divide-and-conquer pipeline to
develop baseline algorithms on PIDray. Specifically, we adopt the tree-like
structure to suppress the influence of the long-tailed issue in the PIDray
dataset, where the first course-grained node is tasked with the binary
classification to alleviate the influence of head category, while the
subsequent fine-grained node is dedicated to the specific tasks of the tail
categories. Based on this simple yet effective scheme, we offer strong
task-specific baselines across object detection, instance segmentation, and
multi-label classification tasks and verify the generalization ability on
common datasets (e.g., COCO and PASCAL VOC). Extensive experiments on PIDray
demonstrate that the proposed method performs favorably against current
state-of-the-art methods, especially for deliberately hidden items. Our
benchmark and codes will be released at https://github.com/lutao2021/PIDray.
</p></li>
</ul>

<h3>Title: Investigating the Security of EV Charging Mobile Applications As an Attack Surface. (arXiv:2211.10603v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10603">http://arxiv.org/abs/2211.10603</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10603] Investigating the Security of EV Charging Mobile Applications As an Attack Surface](http://arxiv.org/abs/2211.10603)</code></li>
<li>Summary: <p>The adoption rate of EVs has witnessed a significant increase in recent years
driven by multiple factors, chief among which is the increased flexibility and
ease of access to charging infrastructure. To improve user experience, increase
system flexibility and commercialize the charging process, mobile applications
have been incorporated into the EV charging ecosystem. EV charging mobile
applications allow consumers to remotely trigger actions on charging stations
and use functionalities such as start/stop charging sessions, pay for usage,
and locate charging stations, to name a few. In this paper, we study the
security posture of the EV charging ecosystem against remote attacks, which
exploit the insecurity of the EV charging mobile applications as an attack
surface. We leverage a combination of static and dynamic analysis techniques to
analyze the security of widely used EV charging mobile applications. Our
analysis of 31 widely used mobile applications and their interactions with
various components such as the cloud management systems indicate the lack of
user/vehicle verification and improper authorization for critical functions,
which lead to remote (dis)charging session hijacking and Denial of Service
(DoS) attacks against the EV charging station. Indeed, we discuss specific
remote attack scenarios and their impact on the EV users. More importantly, our
analysis results demonstrate the feasibility of leveraging existing
vulnerabilities across various EV charging mobile applications to perform
wide-scale coordinated remote charging/discharging attacks against the
connected critical infrastructure (e.g., power grid), with significant
undesired economical and operational implications. Finally, we propose counter
measures to secure the infrastructure and impede adversaries from performing
reconnaissance and launching remote attacks using compromised accounts.
</p></li>
</ul>

<h3>Title: AiCEF: An AI-assisted Cyber Exercise Content Generation Framework Using Named Entity Recognition. (arXiv:2211.10806v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10806">http://arxiv.org/abs/2211.10806</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10806] AiCEF: An AI-assisted Cyber Exercise Content Generation Framework Using Named Entity Recognition](http://arxiv.org/abs/2211.10806)</code></li>
<li>Summary: <p>Content generation that is both relevant and up to date with the current
threats of the target audience is a critical element in the success of any
Cyber Security Exercise (CSE). Through this work, we explore the results of
applying machine learning techniques to unstructured information sources to
generate structured CSE content. The corpus of our work is a large dataset of
publicly available cyber security articles that have been used to predict
future threats and to form the skeleton for new exercise scenarios. Machine
learning techniques, like named entity recognition (NER) and topic extraction,
have been utilised to structure the information based on a novel ontology we
developed, named Cyber Exercise Scenario Ontology (CESO). Moreover, we used
clustering with outliers to classify the generated extracted data into objects
of our ontology. Graph comparison methodologies were used to match generated
scenario fragments to known threat actors' tactics and help enrich the proposed
scenario accordingly with the help of synthetic text generators. CESO has also
been chosen as the prominent way to express both fragments and the final
proposed scenario content by our AI-assisted Cyber Exercise Framework (AiCEF).
Our methodology was put to test by providing a set of generated scenarios for
evaluation to a group of experts to be used as part of a real-world awareness
tabletop exercise.
</p></li>
</ul>

<h3>Title: Mask Off: Analytic-based Malware Detection By Transfer Learning and Model Personalization. (arXiv:2211.10843v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10843">http://arxiv.org/abs/2211.10843</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10843] Mask Off: Analytic-based Malware Detection By Transfer Learning and Model Personalization](http://arxiv.org/abs/2211.10843)</code></li>
<li>Summary: <p>The vulnerability of smartphones to cyberattacks has been a severe concern to
users arising from the integrity of installed applications (\textit{apps}).
Although applications are to provide legitimate and diversified on-the-go
services, harmful and dangerous ones have also uncovered the feasible way to
penetrate smartphones for malicious behaviors. Thorough application analysis is
key to revealing malicious intent and providing more insights into the
application behavior for security risk assessments. Such in-depth analysis
motivates employing deep neural networks (DNNs) for a set of features and
patterns extracted from applications to facilitate detecting potentially
dangerous applications independently. This paper presents an Analytic-based
deep neural network, Android Malware detection (ADAM), that employs a
fine-grained set of features to train feature-specific DNNs to have consensus
on the application labels when their ground truth is unknown. In addition, ADAM
leverages the transfer learning technique to obtain its adjustability to new
applications across smartphones for recycling the pre-trained model(s) and
making them more adaptable by model personalization and federated learning
techniques. This adjustability is also assisted by federated learning guards,
which protect ADAM against poisoning attacks through model analysis. ADAM
relies on a diverse dataset containing more than 153000 applications with over
41000 extracted features for DNNs training. The ADAM's feature-specific DNNs,
on average, achieved more than 98% accuracy, resulting in an outstanding
performance against data manipulation attacks.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Learning to Generate Image Embeddings with User-level Differential Privacy. (arXiv:2211.10844v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10844">http://arxiv.org/abs/2211.10844</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10844] Learning to Generate Image Embeddings with User-level Differential Privacy](http://arxiv.org/abs/2211.10844)</code></li>
<li>Summary: <p>Small on-device models have been successfully trained with user-level
differential privacy (DP) for next word prediction and image classification
tasks in the past. However, existing methods can fail when directly applied to
learn embedding models using supervised training data with a large class space.
To achieve user-level DP for large image-to-embedding feature extractors, we
propose DP-FedEmb, a variant of federated learning algorithms with per-user
sensitivity control and noise addition, to train from user-partitioned data
centralized in the datacenter. DP-FedEmb combines virtual clients, partial
aggregation, private local fine-tuning, and public pretraining to achieve
strong privacy utility trade-offs. We apply DP-FedEmb to train image embedding
models for faces, landmarks and natural species, and demonstrate its superior
utility under same privacy budget on benchmark datasets DigiFace, EMNIST, GLD
and iNaturalist. We further illustrate it is possible to achieve strong
user-level DP guarantees of $\epsilon<2$ while controlling the utility drop
within 5%, when millions of users can participate in training.
</p></li>
</ul>

<h3>Title: A Unified Framework for Quantifying Privacy Risk in Synthetic Data. (arXiv:2211.10459v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10459">http://arxiv.org/abs/2211.10459</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10459] A Unified Framework for Quantifying Privacy Risk in Synthetic Data](http://arxiv.org/abs/2211.10459)</code></li>
<li>Summary: <p>Synthetic data is often presented as a method for sharing sensitive
information in a privacy-preserving manner by reproducing the global
statistical properties of the original data without disclosing sensitive
information about any individual. In practice, as with other anonymization
methods, privacy risks cannot be entirely eliminated. The residual privacy
risks need instead to be ex-post assessed. We present Anonymeter, a statistical
framework to jointly quantify different types of privacy risks in synthetic
tabular datasets. We equip this framework with attack-based evaluations for the
singling out, linkability, and inference risks, the three key indicators of
factual anonymization according to the European General Data Protection
Regulation (GDPR). To the best of our knowledge, we are the first to introduce
a coherent and legally aligned evaluation of these three privacy risks for
synthetic data, and to design privacy attacks which model directly the singling
out and linkability risks. We demonstrate the effectiveness of our methods by
conducting an extensive set of experiments that measure the privacy risks of
data with deliberately inserted privacy leakages, and of synthetic data
generated with and without differential privacy. Our results highlight that the
three privacy risks reported by our framework scale linearly with the amount of
privacy leakage in the data. Furthermore, we observe that synthetic data
exhibits the lowest vulnerability against linkability, indicating one-to-one
relationships between real and synthetic data records are not preserved.
Finally, we demonstrate quantitatively that Anonymeter outperforms existing
synthetic data privacy evaluation frameworks both in terms of detecting privacy
leaks, as well as computation speed. To contribute to a privacy-conscious usage
of synthetic data, we open source Anonymeter at
https://github.com/statice/anonymeter.
</p></li>
</ul>

<h3>Title: Anonymizing Periodical Releases of SRS Data by Fusing Differential Privacy. (arXiv:2211.10648v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10648">http://arxiv.org/abs/2211.10648</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10648] Anonymizing Periodical Releases of SRS Data by Fusing Differential Privacy](http://arxiv.org/abs/2211.10648)</code></li>
<li>Summary: <p>Spontaneous reporting systems (SRS) have been developed to collect adverse
event records that contain personal demographics and sensitive information like
drug indications and adverse reactions. The release of SRS data may disclose
the privacy of the data provider. Unlike other microdata, very few
anonymyization methods have been proposed to protect individual privacy while
publishing SRS data. MS(k, {\theta}<em>)-bounding is the first privacy model for
SRS data that considers multiple individual records, mutli-valued sensitive
attributes, and rare events. PPMS(k, {\theta}</em>)-bounding then is proposed for
solving cross-release attacks caused by the follow-up cases in the periodical
SRS releasing scenario. A recent trend of microdata anonymization combines the
traditional syntactic model and differential privacy, fusing the advantages of
both models to yield a better privacy protection method. This paper proposes
the PPMS-DP(k, {\theta}<em>, {\epsilon}) framework, an enhancement of PPMS(k,
{\theta}</em>)-bounding that embraces differential privacy to improve privacy
protection of periodically released SRS data. We propose two anonymization
algorithms conforming to the PPMS-DP(k, {\theta}<em>, {\epsilon}) framework,
PPMS-DPnum and PPMS-DPall. Experimental results on the FAERS datasets show that
both PPMS-DPnum and PPMS-DPall provide significantly better privacy protection
than PPMS-(k, {\theta}</em>)-bounding without sacrificing data distortion and data
utility.
</p></li>
</ul>

<h3>Title: A Survey on Differential Privacy with Machine Learning and Future Outlook. (arXiv:2211.10708v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10708">http://arxiv.org/abs/2211.10708</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10708] A Survey on Differential Privacy with Machine Learning and Future Outlook](http://arxiv.org/abs/2211.10708)</code></li>
<li>Summary: <p>Nowadays, machine learning models and applications have become increasingly
pervasive. With this rapid increase in the development and employment of
machine learning models, a concern regarding privacy has risen. Thus, there is
a legitimate need to protect the data from leaking and from any attacks. One of
the strongest and most prevalent privacy models that can be used to protect
machine learning models from any attacks and vulnerabilities is differential
privacy (DP). DP is strict and rigid definition of privacy, where it can
guarantee that an adversary is not capable to reliably predict if a specific
participant is included in the dataset or not. It works by injecting a noise to
the data whether to the inputs, the outputs, the ground truth labels, the
objective functions, or even to the gradients to alleviate the privacy issue
and protect the data. To this end, this survey paper presents different
differentially private machine learning algorithms categorized into two main
categories (traditional machine learning models vs. deep learning models).
Moreover, future research directions for differential privacy with machine
learning algorithms are outlined.
</p></li>
</ul>

<h3>Title: A privacy-preserving data storage and service framework based on deep learning and blockchain for construction workers' wearable IoT sensors. (arXiv:2211.10713v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10713">http://arxiv.org/abs/2211.10713</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10713] A privacy-preserving data storage and service framework based on deep learning and blockchain for construction workers' wearable IoT sensors](http://arxiv.org/abs/2211.10713)</code></li>
<li>Summary: <p>Classifying brain signals collected by wearable Internet of Things (IoT)
sensors, especially brain-computer interfaces (BCIs), is one of the
fastest-growing areas of research. However, research has mostly ignored the
secure storage and privacy protection issues of collected personal
neurophysiological data. Therefore, in this article, we try to bridge this gap
and propose a secure privacy-preserving protocol for implementing BCI
applications. We first transformed brain signals into images and used
generative adversarial network to generate synthetic signals to protect data
privacy. Subsequently, we applied the paradigm of transfer learning for signal
classification. The proposed method was evaluated by a case study and results
indicate that real electroencephalogram data augmented with artificially
generated samples provide superior classification performance. In addition, we
proposed a blockchain-based scheme and developed a prototype on Ethereum, which
aims to make storing, querying and sharing personal neurophysiological data and
analysis reports secure and privacy-aware. The rights of three main transaction
bodies - construction workers, BCI service providers and project managers - are
described and the advantages of the proposed system are discussed. We believe
this paper provides a well-rounded solution to safeguard private data against
cyber-attacks, level the playing field for BCI application developers, and to
the end improve professional well-being in the industry.
</p></li>
</ul>

<h3>Title: DYNAFED: Tackling Client Data Heterogeneity with Global Dynamics. (arXiv:2211.10878v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10878">http://arxiv.org/abs/2211.10878</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10878] DYNAFED: Tackling Client Data Heterogeneity with Global Dynamics](http://arxiv.org/abs/2211.10878)</code></li>
<li>Summary: <p>The Federated Learning (FL) paradigm is known to face challenges under
heterogeneous client data. Local training on non-iid distributed data results
in deflected local optimum, which causes the client models drift further away
from each other and degrades the aggregated global model's performance. A
natural solution is to gather all client data onto the server, such that the
server has a global view of the entire data distribution. Unfortunately, this
reduces to regular training, which compromises clients' privacy and conflicts
with the purpose of FL. In this paper, we put forth an idea to collect and
leverage global knowledge on the server without hindering data privacy. We
unearth such knowledge from the dynamics of the global model's trajectory.
Specifically, we first reserve a short trajectory of global model snapshots on
the server. Then, we synthesize a small pseudo dataset such that the model
trained on it mimics the dynamics of the reserved global model trajectory.
Afterward, the synthesized data is used to help aggregate the deflected clients
into the global model. We name our method Dynafed, which enjoys the following
advantages: 1) we do not rely on any external on-server dataset, which requires
no additional cost for data collection; 2) the pseudo data can be synthesized
in early communication rounds, which enables Dynafed to take effect early for
boosting the convergence and stabilizing training; 3) the pseudo data only
needs to be synthesized once and can be directly utilized on the server to help
aggregation in subsequent rounds. Experiments across extensive benchmarks are
conducted to showcase the effectiveness of Dynafed. We also provide insights
and understanding of the underlying mechanism of our method.
</p></li>
</ul>

<h3>Title: Scalable Collaborative Learning via Representation Sharing. (arXiv:2211.10943v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10943">http://arxiv.org/abs/2211.10943</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10943] Scalable Collaborative Learning via Representation Sharing](http://arxiv.org/abs/2211.10943)</code></li>
<li>Summary: <p>Privacy-preserving machine learning has become a key conundrum for
multi-party artificial intelligence. Federated learning (FL) and Split Learning
(SL) are two frameworks that enable collaborative learning while keeping the
data private (on device). In FL, each data holder trains a model locally and
releases it to a central server for aggregation. In SL, the clients must
release individual cut-layer activations (smashed data) to the server and wait
for its response (during both inference and back propagation). While relevant
in several settings, both of these schemes have a high communication cost, rely
on server-level computation algorithms and do not allow for tunable levels of
collaboration. In this work, we present a novel approach for privacy-preserving
machine learning, where the clients collaborate via online knowledge
distillation using a contrastive loss (contrastive w.r.t. the labels). The goal
is to ensure that the participants learn similar features on similar classes
without sharing their input data. To do so, each client releases averaged last
hidden layer activations of similar labels to a central server that only acts
as a relay (i.e., is not involved in the training or aggregation of the
models). Then, the clients download these last layer activations (feature
representations) of the ensemble of users and distill their knowledge in their
personal model using a contrastive objective. For cross-device applications
(i.e., small local datasets and limited computational capacity), this approach
increases the utility of the models compared to independent learning and other
federated knowledge distillation (FD) schemes, is communication efficient and
is scalable with the number of clients. We prove theoretically that our
framework is well-posed, and we benchmark its performance against standard FD
and FL on various datasets using different model architectures.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Provable Defense against Backdoor Policies in Reinforcement Learning. (arXiv:2211.10530v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10530">http://arxiv.org/abs/2211.10530</a></li>
<li>Code URL: <a href="https://github.com/skbharti/provable-defense-in-rl">https://github.com/skbharti/provable-defense-in-rl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10530] Provable Defense against Backdoor Policies in Reinforcement Learning](http://arxiv.org/abs/2211.10530)</code></li>
<li>Summary: <p>We propose a provable defense mechanism against backdoor policies in
reinforcement learning under subspace trigger assumption. A backdoor policy is
a security threat where an adversary publishes a seemingly well-behaved policy
which in fact allows hidden triggers. During deployment, the adversary can
modify observed states in a particular way to trigger unexpected actions and
harm the agent. We assume the agent does not have the resources to re-train a
good policy. Instead, our defense mechanism sanitizes the backdoor policy by
projecting observed states to a 'safe subspace', estimated from a small number
of interactions with a clean (non-triggered) environment. Our sanitized policy
achieves $\epsilon$ approximate optimality in the presence of triggers,
provided the number of clean interactions is $O\left(\frac{D}{(1-\gamma)^4
\epsilon^2}\right)$ where $\gamma$ is the discounting factor and $D$ is the
dimension of state space. Empirically, we show that our sanitization defense
performs well on two Atari game environments.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: ESTAS: Effective and Stable Trojan Attacks in Self-supervised Encoders with One Target Unlabelled Sample. (arXiv:2211.10908v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10908">http://arxiv.org/abs/2211.10908</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10908] ESTAS: Effective and Stable Trojan Attacks in Self-supervised Encoders with One Target Unlabelled Sample](http://arxiv.org/abs/2211.10908)</code></li>
<li>Summary: <p>Emerging self-supervised learning (SSL) has become a popular image
representation encoding method to obviate the reliance on labeled data and
learn rich representations from large-scale, ubiquitous unlabelled data. Then
one can train a downstream classifier on top of the pre-trained SSL image
encoder with few or no labeled downstream data. Although extensive works show
that SSL has achieved remarkable and competitive performance on different
downstream tasks, its security concerns, e.g, Trojan attacks in SSL encoders,
are still not well-studied. In this work, we present a novel Trojan Attack
method, denoted by ESTAS, that can enable an effective and stable attack in SSL
encoders with only one target unlabeled sample. In particular, we propose
consistent trigger poisoning and cascade optimization in ESTAS to improve
attack efficacy and model accuracy, and eliminate the expensive target-class
data sample extraction from large-scale disordered unlabelled data. Our
substantial experiments on multiple datasets show that ESTAS stably achieves >
99% attacks success rate (ASR) with one target-class sample. Compared to prior
works, ESTAS attains > 30% ASR increase and > 8.3% accuracy improvement on
average.
</p></li>
</ul>

<h3>Title: Invisible Backdoor Attack with Dynamic Triggers against Person Re-identification. (arXiv:2211.10933v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10933">http://arxiv.org/abs/2211.10933</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10933] Invisible Backdoor Attack with Dynamic Triggers against Person Re-identification](http://arxiv.org/abs/2211.10933)</code></li>
<li>Summary: <p>In recent years, person Re-identification (ReID) has rapidly progressed with
wide real-world applications, but also poses significant risks of adversarial
attacks. In this paper, we focus on the backdoor attack on deep ReID models.
Existing backdoor attack methods follow an all-to-one/all attack scenario,
where all the target classes in the test set have already been seen in the
training set. However, ReID is a much more complex fine-grained open-set
recognition problem, where the identities in the test set are not contained in
the training set. Thus, previous backdoor attack methods for classification are
not applicable for ReID. To ameliorate this issue, we propose a novel backdoor
attack on deep ReID under a new all-to-unknown scenario, called Dynamic
Triggers Invisible Backdoor Attack (DT-IBA). Instead of learning fixed triggers
for the target classes from the training set, DT-IBA can dynamically generate
new triggers for any unknown identities. Specifically, an identity hashing
network is proposed to first extract target identity information from a
reference image, which is then injected into the benign images by image
steganography. We extensively validate the effectiveness and stealthiness of
the proposed attack on benchmark datasets, and evaluate the effectiveness of
several defense methods against our attack.
</p></li>
</ul>

<h3>Title: Let Graph be the Go Board: Gradient-free Node Injection Attack for Graph Neural Networks via Reinforcement Learning. (arXiv:2211.10782v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10782">http://arxiv.org/abs/2211.10782</a></li>
<li>Code URL: <a href="https://github.com/jumxglhf/g2a2c">https://github.com/jumxglhf/g2a2c</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10782] Let Graph be the Go Board: Gradient-free Node Injection Attack for Graph Neural Networks via Reinforcement Learning](http://arxiv.org/abs/2211.10782)</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) have drawn significant attentions over the years
and been broadly applied to essential applications requiring solid robustness
or vigorous security standards, such as product recommendation and user
behavior modeling. Under these scenarios, exploiting GNN's vulnerabilities and
further downgrading its performance become extremely incentive for adversaries.
Previous attackers mainly focus on structural perturbations or node injections
to the existing graphs, guided by gradients from the surrogate models. Although
they deliver promising results, several limitations still exist. For the
structural perturbation attack, to launch a proposed attack, adversaries need
to manipulate the existing graph topology, which is impractical in most
circumstances. Whereas for the node injection attack, though being more
practical, current approaches require training surrogate models to simulate a
white-box setting, which results in significant performance downgrade when the
surrogate architecture diverges from the actual victim model. To bridge these
gaps, in this paper, we study the problem of black-box node injection attack,
without training a potentially misleading surrogate model. Specifically, we
model the node injection attack as a Markov decision process and propose
Gradient-free Graph Advantage Actor Critic, namely G2A2C, a reinforcement
learning framework in the fashion of advantage actor critic. By directly
querying the victim model, G2A2C learns to inject highly malicious nodes with
extremely limited attacking budgets, while maintaining a similar node feature
distribution. Through our comprehensive experiments over eight acknowledged
benchmark datasets with different characteristics, we demonstrate the superior
performance of our proposed G2A2C over the existing state-of-the-art attackers.
Source code is publicly available at: https://github.com/jumxglhf/G2A2C}.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: A Practical Stereo Depth System for Smart Glasses. (arXiv:2211.10551v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10551">http://arxiv.org/abs/2211.10551</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10551] A Practical Stereo Depth System for Smart Glasses](http://arxiv.org/abs/2211.10551)</code></li>
<li>Summary: <p>We present the design of a productionized end-to-end stereo depth sensing
system that does pre-processing, online stereo rectification, and stereo depth
estimation with a fallback to monocular depth estimation when rectification is
unreliable. The output of our depth sensing system is then used in a novel view
generation pipeline to create 3D computational photography effect using
point-of-view images captured by smart glasses. All these steps are executed
on-device on the stringent compute budget of a mobile phone, and because we
expect the users can use a wide range of smartphones, our design needs to be
general and cannot be dependent on a particular hardware or ML accelerator such
as a smartphone GPU. Although each of these steps is well-studied, a
description of a practical system is still lacking. For such a system, each of
these steps need to work in tandem with one another and fallback gracefully on
failures within the system or less than ideal input data. We show how we handle
unforeseen changes to calibration, e.g. due to heat, robustly support depth
estimation in the wild, and still abide by the memory and latency constraints
required for a smooth user experience. We show that our trained models are
fast, that run in less than 1s on a six-year-old Samsung Galaxy S8 phone's CPU.
Our models generalize well to unseen data and achieve good results on
Middlebury and in-the-wild images captured from the smart glasses.
</p></li>
</ul>

<h3>Title: Semantic-aware Texture-Structure Feature Collaboration for Underwater Image Enhancement. (arXiv:2211.10608v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10608">http://arxiv.org/abs/2211.10608</a></li>
<li>Code URL: <a href="https://github.com/wdhudiekou/stsc">https://github.com/wdhudiekou/stsc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10608] Semantic-aware Texture-Structure Feature Collaboration for Underwater Image Enhancement](http://arxiv.org/abs/2211.10608)</code></li>
<li>Summary: <p>Underwater image enhancement has become an attractive topic as a significant
technology in marine engineering and aquatic robotics. However, the limited
number of datasets and imperfect hand-crafted ground truth weaken its
robustness to unseen scenarios, and hamper the application to high-level vision
tasks. To address the above limitations, we develop an efficient and compact
enhancement network in collaboration with a high-level semantic-aware
pretrained model, aiming to exploit its hierarchical feature representation as
an auxiliary for the low-level underwater image enhancement. Specifically, we
tend to characterize the shallow layer features as textures while the deep
layer features as structures in the semantic-aware model, and propose a
multi-path Contextual Feature Refinement Module (CFRM) to refine features in
multiple scales and model the correlation between different features. In
addition, a feature dominative network is devised to perform channel-wise
modulation on the aggregated texture and structure features for the adaptation
to different feature patterns of the enhancement network. Extensive experiments
on benchmarks demonstrate that the proposed algorithm achieves more appealing
results and outperforms state-of-the-art methods by large margins. We also
apply the proposed algorithm to the underwater salient object detection task to
reveal the favorable semantic-aware ability for high-level vision tasks. The
code is available at STSC.
</p></li>
</ul>

<h3>Title: Rethinking Batch Sample Relationships for Data Representation: A Batch-Graph Transformer based Approach. (arXiv:2211.10622v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10622">http://arxiv.org/abs/2211.10622</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10622] Rethinking Batch Sample Relationships for Data Representation: A Batch-Graph Transformer based Approach](http://arxiv.org/abs/2211.10622)</code></li>
<li>Summary: <p>Exploring sample relationships within each mini-batch has shown great
potential for learning image representations. Existing works generally adopt
the regular Transformer to model the visual content relationships, ignoring the
cues of semantic/label correlations between samples. Also, they generally adopt
the "full" self-attention mechanism which are obviously redundant and also
sensitive to the noisy samples. To overcome these issues, in this paper, we
design a simple yet flexible Batch-Graph Transformer (BGFormer) for mini-batch
sample representations by deeply capturing the relationships of image samples
from both visual and semantic perspectives. BGFormer has three main aspects.
(1) It employs a flexible graph model, termed Batch Graph to jointly encode the
visual and semantic relationships of samples within each mini-batch. (2) It
explores the neighborhood relationships of samples by borrowing the idea of
sparse graph representation which thus performs robustly, w.r.t., noisy
samples. (3) It devises a novel Transformer architecture that mainly adopts
dual structure-constrained self-attention (SSA), together with graph
normalization, FFN, etc, to carefully exploit the batch graph information for
sample tokens (nodes) representations. As an application, we apply BGFormer to
the metric learning tasks. Extensive experiments on four popular datasets
demonstrate the effectiveness of the proposed model.
</p></li>
</ul>

<h3>Title: Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning. (arXiv:2211.10681v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10681">http://arxiv.org/abs/2211.10681</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10681] Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning](http://arxiv.org/abs/2211.10681)</code></li>
<li>Summary: <p>Compositional Zero-Shot Learning (CZSL) aims to recognize novel concepts
formed by known states and objects during training. Existing methods either
learn the combined state-object representation, challenging the generalization
of unseen compositions, or design two classifiers to identify state and object
separately from image features, ignoring the intrinsic relationship between
them. To jointly eliminate the above issues and construct a more robust CZSL
system, we propose a novel framework termed Decomposed Fusion with Soft Prompt
(DFSP)1, by involving vision-language models (VLMs) for unseen composition
recognition. Specifically, DFSP constructs a vector combination of learnable
soft prompts with state and object to establish the joint representation of
them. In addition, a cross-modal decomposed fusion module is designed between
the language and image branches, which decomposes state and object among
language features instead of image features. Notably, being fused with the
decomposed features, the image features can be more expressive for learning the
relationship with states and objects, respectively, to improve the response of
unseen compositions in the pair space, hence narrowing the domain gap between
seen and unseen sets. Experimental results on three challenging benchmarks
demonstrate that our approach significantly outperforms other state-of-the-art
methods by large margins.
</p></li>
</ul>

<h3>Title: Passive Micron-scale Time-of-Flight with Sunlight Interferometry. (arXiv:2211.10732v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10732">http://arxiv.org/abs/2211.10732</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10732] Passive Micron-scale Time-of-Flight with Sunlight Interferometry](http://arxiv.org/abs/2211.10732)</code></li>
<li>Summary: <p>We introduce an interferometric technique for passive time-of-flight imaging
and depth sensing at micrometer axial resolutions. Our technique uses a
full-field Michelson interferometer, modified to use sunlight as the only light
source. The large spectral bandwidth of sunlight makes it possible to acquire
micrometer-resolution time-resolved scene responses, through a simple axial
scanning operation. Additionally, the angular bandwidth of sunlight makes it
possible to capture time-of-flight measurements insensitive to indirect
illumination effects, such as interreflections and subsurface scattering. We
build an experimental prototype that we operate outdoors, under direct
sunlight, and in adverse environmental conditions such as mechanical vibrations
and vehicle traffic. We use this prototype to demonstrate, for the first time,
passive imaging capabilities such as micrometer-scale depth sensing robust to
indirect illumination, direct-only imaging, and imaging through diffusers.
</p></li>
</ul>

<h3>Title: Towards Robust Dataset Learning. (arXiv:2211.10752v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10752">http://arxiv.org/abs/2211.10752</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10752] Towards Robust Dataset Learning](http://arxiv.org/abs/2211.10752)</code></li>
<li>Summary: <p>Adversarial training has been actively studied in recent computer vision
research to improve the robustness of models. However, due to the huge
computational cost of generating adversarial samples, adversarial training
methods are often slow. In this paper, we study the problem of learning a
robust dataset such that any classifier naturally trained on the dataset is
adversarially robust. Such a dataset benefits the downstream tasks as natural
training is much faster than adversarial training, and demonstrates that the
desired property of robustness is transferable between models and data. In this
work, we propose a principled, tri-level optimization to formulate the robust
dataset learning problem. We show that, under an abstraction model that
characterizes robust vs. non-robust features, the proposed method provably
learns a robust dataset. Extensive experiments on MNIST, CIFAR10, and
TinyImageNet demostrate the effectiveness of our algorithm with different
network initializations and architectures.
</p></li>
</ul>

<h3>Title: On Multi-head Ensemble of Smoothed Classifiers for Certified Robustness. (arXiv:2211.10882v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10882">http://arxiv.org/abs/2211.10882</a></li>
<li>Code URL: <a href="https://github.com/fanghenshaometeor/circular-teaching">https://github.com/fanghenshaometeor/circular-teaching</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10882] On Multi-head Ensemble of Smoothed Classifiers for Certified Robustness](http://arxiv.org/abs/2211.10882)</code></li>
<li>Summary: <p>Randomized Smoothing (RS) is a promising technique for certified robustness,
and recently in RS the ensemble of multiple deep neural networks (DNNs) has
shown state-of-the-art performances. However, such an ensemble brings heavy
computation burdens in both training and certification, and yet under-exploits
individual DNNs and their mutual effects, as the communication between these
classifiers is commonly ignored in optimization. In this work, starting from a
single DNN, we augment the network with multiple heads, each of which pertains
a classifier for the ensemble. A novel training strategy, namely Self-PAced
Circular-TEaching (SPACTE), is proposed accordingly. SPACTE enables a circular
communication flow among those augmented heads, i.e., each head teaches its
neighbor with the self-paced learning using smoothed losses, which are
specifically designed in relation to certified robustness. The deployed
multi-head structure and the circular-teaching scheme of SPACTE jointly
contribute to diversify and enhance the classifiers in augmented heads for
ensemble, leading to even stronger certified robustness than ensembling
multiple DNNs (effectiveness) at the cost of much less computational expenses
(efficiency), verified by extensive experiments and discussions.
</p></li>
</ul>

<h3>Title: Adaptive Edge-to-Edge Interaction Learning for Point Cloud Analysis. (arXiv:2211.10888v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10888">http://arxiv.org/abs/2211.10888</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10888] Adaptive Edge-to-Edge Interaction Learning for Point Cloud Analysis](http://arxiv.org/abs/2211.10888)</code></li>
<li>Summary: <p>Recent years have witnessed the great success of deep learning on various
point cloud analysis tasks, e.g., classification and semantic segmentation.
Since point cloud data is sparse and irregularly distributed, one key issue for
point cloud data processing is extracting useful information from local
regions. To achieve this, previous works mainly extract the points' features
from local regions by learning the relation between each pair of adjacent
points. However, these works ignore the relation between edges in local
regions, which encodes the local shape information. Associating the
neighbouring edges could potentially make the point-to-point relation more
aware of the local structure and more robust. To explore the role of the
relation between edges, this paper proposes a novel Adaptive Edge-to-Edge
Interaction Learning module, which aims to enhance the point-to-point relation
through modelling the edge-to-edge interaction in the local region adaptively.
We further extend the module to a symmetric version to capture the local
structure more thoroughly. Taking advantage of the proposed modules, we develop
two networks for segmentation and shape classification tasks, respectively.
Various experiments on several public point cloud datasets demonstrate the
effectiveness of our method for point cloud analysis.
</p></li>
</ul>

<h3>Title: Traceable and Authenticable Image Tagging for Fake News Detection. (arXiv:2211.10923v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10923">http://arxiv.org/abs/2211.10923</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10923] Traceable and Authenticable Image Tagging for Fake News Detection](http://arxiv.org/abs/2211.10923)</code></li>
<li>Summary: <p>To prevent fake news images from misleading the public, it is desirable not
only to verify the authenticity of news images but also to trace the source of
fake news, so as to provide a complete forensic chain for reliable fake news
detection. To simultaneously achieve the goals of authenticity verification and
source tracing, we propose a traceable and authenticable image tagging approach
that is based on a design of Decoupled Invertible Neural Network (DINN). The
designed DINN can simultaneously embed the dual-tags, \textit{i.e.},
authenticable tag and traceable tag, into each news image before publishing,
and then separately extract them for authenticity verification and source
tracing. Moreover, to improve the accuracy of dual-tags extraction, we design a
parallel Feature Aware Projection Model (FAPM) to help the DINN preserve
essential tag information. In addition, we define a Distance Metric-Guided
Module (DMGM) that learns asymmetric one-class representations to enable the
dual-tags to achieve different robustness performances under malicious
manipulations. Extensive experiments, on diverse datasets and unseen
manipulations, demonstrate that the proposed tagging approach achieves
excellent performance in the aspects of both authenticity verification and
source tracing for reliable fake news detection and outperforms the prior
works.
</p></li>
</ul>

<h3>Title: GLT-T: Global-Local Transformer Voting for 3D Single Object Tracking in Point Clouds. (arXiv:2211.10927v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10927">http://arxiv.org/abs/2211.10927</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10927] GLT-T: Global-Local Transformer Voting for 3D Single Object Tracking in Point Clouds](http://arxiv.org/abs/2211.10927)</code></li>
<li>Summary: <p>Current 3D single object tracking methods are typically based on VoteNet, a
3D region proposal network. Despite the success, using a single seed point
feature as the cue for offset learning in VoteNet prevents high-quality 3D
proposals from being generated. Moreover, seed points with different importance
are treated equally in the voting process, aggravating this defect. To address
these issues, we propose a novel global-local transformer voting scheme to
provide more informative cues and guide the model pay more attention on
potential seed points, promoting the generation of high-quality 3D proposals.
Technically, a global-local transformer (GLT) module is employed to integrate
object- and patch-aware prior into seed point features to effectively form
strong feature representation for geometric positions of the seed points, thus
providing more robust and accurate cues for offset learning. Subsequently, a
simple yet effective training strategy is designed to train the GLT module. We
develop an importance prediction branch to learn the potential importance of
the seed points and treat the output weights vector as a training constraint
term. By incorporating the above components together, we exhibit a superior
tracking method GLT-T. Extensive experiments on challenging KITTI and NuScenes
benchmarks demonstrate that GLT-T achieves state-of-the-art performance in the
3D single object tracking task. Besides, further ablation studies show the
advantages of the proposed global-local transformer voting scheme over the
original VoteNet. Code and models will be available at
https://github.com/haooozi/GLT-T.
</p></li>
</ul>

<h3>Title: Feature Weaken: Vicinal Data Augmentation for Classification. (arXiv:2211.10944v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10944">http://arxiv.org/abs/2211.10944</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10944] Feature Weaken: Vicinal Data Augmentation for Classification](http://arxiv.org/abs/2211.10944)</code></li>
<li>Summary: <p>Deep learning usually relies on training large-scale data samples to achieve
better performance. However, over-fitting based on training data always remains
a problem. Scholars have proposed various strategies, such as feature dropping
and feature mixing, to improve the generalization continuously. For the same
purpose, we subversively propose a novel training method, Feature Weaken, which
can be regarded as a data augmentation method. Feature Weaken constructs the
vicinal data distribution with the same cosine similarity for model training by
weakening features of the original samples. In especially, Feature Weaken
changes the spatial distribution of samples, adjusts sample boundaries, and
reduces the gradient optimization value of back-propagation. This work can not
only improve the classification performance and generalization of the model,
but also stabilize the model training and accelerate the model convergence. We
conduct extensive experiments on classical deep convolution neural models with
five common image classification datasets and the Bert model with four common
text classification datasets. Compared with the classical models or the
generalization improvement methods, such as Dropout, Mixup, Cutout, and CutMix,
Feature Weaken shows good compatibility and performance. We also use
adversarial samples to perform the robustness experiments, and the results show
that Feature Weaken is effective in improving the robustness of the model.
</p></li>
</ul>

<h3>Title: Artificial Interrogation for Attributing Language Models. (arXiv:2211.10877v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10877">http://arxiv.org/abs/2211.10877</a></li>
<li>Code URL: <a href="https://github.com/farhandhanani/mlmac">https://github.com/farhandhanani/mlmac</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10877] Artificial Interrogation for Attributing Language Models](http://arxiv.org/abs/2211.10877)</code></li>
<li>Summary: <p>This paper presents solutions to the Machine Learning Model Attribution
challenge (MLMAC) collectively organized by MITRE, Microsoft, Schmidt-Futures,
Robust-Intelligence, Lincoln-Network, and Huggingface community. The challenge
provides twelve open-sourced base versions of popular language models developed
by well-known organizations and twelve fine-tuned language models for text
generation. The names and architecture details of fine-tuned models were kept
hidden, and participants can access these models only through the rest APIs
developed by the organizers. Given these constraints, the goal of the contest
is to identify which fine-tuned models originated from which base model. To
solve this challenge, we have assumed that fine-tuned models and their
corresponding base versions must share a similar vocabulary set with a matching
syntactical writing style that resonates in their generated outputs. Our
strategy is to develop a set of queries to interrogate base and fine-tuned
models. And then perform one-to-many pairing between them based on similarities
in their generated responses, where more than one fine-tuned model can pair
with a base model but not vice-versa. We have employed four distinct approaches
for measuring the resemblance between the responses generated from the models
of both sets. The first approach uses evaluation metrics of the machine
translation, and the second uses a vector space model. The third approach uses
state-of-the-art multi-class text classification, Transformer models. Lastly,
the fourth approach uses a set of Transformer based binary text classifiers,
one for each provided base model, to perform multi-class text classification in
a one-vs-all fashion. This paper reports implementation details, comparison,
and experimental studies, of these approaches along with the final obtained
results.
</p></li>
</ul>

<h3>Title: Towards Adversarial Robustness of Deep Vision Algorithms. (arXiv:2211.10670v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10670">http://arxiv.org/abs/2211.10670</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10670] Towards Adversarial Robustness of Deep Vision Algorithms](http://arxiv.org/abs/2211.10670)</code></li>
<li>Summary: <p>Deep learning methods have achieved great success in solving computer vision
tasks, and they have been widely utilized in artificially intelligent systems
for image processing, analysis, and understanding. However, deep neural
networks have been shown to be vulnerable to adversarial perturbations in input
data. The security issues of deep neural networks have thus come to the fore.
It is imperative to study the adversarial robustness of deep vision algorithms
comprehensively. This talk focuses on the adversarial robustness of image
classification models and image denoisers. We will discuss the robustness of
deep vision algorithms from three perspectives: 1) robustness evaluation (we
propose the ObsAtk to evaluate the robustness of denoisers), 2) robustness
improvement (HAT, TisODE, and CIFS are developed to robustify vision models),
and 3) the connection between adversarial robustness and generalization
capability to new domains (we find that adversarially robust denoisers can deal
with unseen types of real-world noise).
</p></li>
</ul>

<h3>Title: Spectral Adversarial Training for Robust Graph Neural Network. (arXiv:2211.10896v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10896">http://arxiv.org/abs/2211.10896</a></li>
<li>Code URL: <a href="https://github.com/edisonleeeee/sat">https://github.com/edisonleeeee/sat</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10896] Spectral Adversarial Training for Robust Graph Neural Network](http://arxiv.org/abs/2211.10896)</code></li>
<li>Summary: <p>Recent studies demonstrate that Graph Neural Networks (GNNs) are vulnerable
to slight but adversarially designed perturbations, known as adversarial
examples. To address this issue, robust training methods against adversarial
examples have received considerable attention in the literature.
\emph{Adversarial Training (AT)} is a successful approach to learning a robust
model using adversarially perturbed training samples. Existing AT methods on
GNNs typically construct adversarial perturbations in terms of graph structures
or node features. However, they are less effective and fraught with challenges
on graph data due to the discreteness of graph structure and the relationships
between connected examples. In this work, we seek to address these challenges
and propose Spectral Adversarial Training (SAT), a simple yet effective
adversarial training approach for GNNs. SAT first adopts a low-rank
approximation of the graph structure based on spectral decomposition, and then
constructs adversarial perturbations in the spectral domain rather than
directly manipulating the original graph structure. To investigate its
effectiveness, we employ SAT on three widely used GNNs. Experimental results on
four public graph datasets demonstrate that SAT significantly improves the
robustness of GNNs against adversarial attacks without sacrificing
classification accuracy and training efficiency.
</p></li>
</ul>

<h3>Title: Learning from Long-Tailed Noisy Data with Sample Selection and Balanced Loss. (arXiv:2211.10906v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10906">http://arxiv.org/abs/2211.10906</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10906] Learning from Long-Tailed Noisy Data with Sample Selection and Balanced Loss](http://arxiv.org/abs/2211.10906)</code></li>
<li>Summary: <p>The success of deep learning depends on large-scale and well-curated training
data, while data in real-world applications are commonly long-tailed and noisy.
Many methods have been proposed to deal with long-tailed data or noisy data,
while a few methods are developed to tackle long-tailed noisy data. To solve
this, we propose a robust method for learning from long-tailed noisy data with
sample selection and balanced loss. Specifically, we separate the noisy
training data into clean labeled set and unlabeled set with sample selection,
and train the deep neural network in a semi-supervised manner with a novel
balanced loss based on model bias. Experiments on benchmarks demonstrate that
our method outperforms existing state-of-the-art methods.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception. (arXiv:2211.10593v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10593">http://arxiv.org/abs/2211.10593</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10593] MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception](http://arxiv.org/abs/2211.10593)</code></li>
<li>Summary: <p>This paper proposes an efficient multi-camera to Bird's-Eye-View (BEV) view
transformation method for 3D perception, dubbed MatrixVT. Existing view
transformers either suffer from poor transformation efficiency or rely on
device-specific operators, hindering the broad application of BEV models. In
contrast, our method generates BEV features efficiently with only convolutions
and matrix multiplications (MatMul). Specifically, we propose describing the
BEV feature as the MatMul of image feature and a sparse Feature Transporting
Matrix (FTM). A Prime Extraction module is then introduced to compress the
dimension of image features and reduce FTM's sparsity. Moreover, we propose the
Ring \&amp; Ray Decomposition to replace the FTM with two matrices and reformulate
our pipeline to reduce calculation further. Compared to existing methods,
MatrixVT enjoys a faster speed and less memory footprint while remaining
deploy-friendly. Extensive experiments on the nuScenes benchmark demonstrate
that our method is highly efficient but obtains results on par with the SOTA
method in object detection and map segmentation tasks
</p></li>
</ul>

<h3>Title: Knowledge Graph Generation From Text. (arXiv:2211.10511v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10511">http://arxiv.org/abs/2211.10511</a></li>
<li>Code URL: <a href="https://github.com/ibm/grapher">https://github.com/ibm/grapher</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10511] Knowledge Graph Generation From Text](http://arxiv.org/abs/2211.10511)</code></li>
<li>Summary: <p>In this work we propose a novel end-to-end multi-stage Knowledge Graph (KG)
generation system from textual inputs, separating the overall process into two
stages. The graph nodes are generated first using pretrained language model,
followed by a simple edge construction head, enabling efficient KG extraction
from the text. For each stage we consider several architectural choices that
can be used depending on the available training resources. We evaluated the
model on a recent WebNLG 2020 Challenge dataset, matching the state-of-the-art
performance on text-to-RDF generation task, as well as on New York Times (NYT)
and a large-scale TekGen datasets, showing strong overall performance,
outperforming the existing baselines. We believe that the proposed system can
serve as a viable KG construction alternative to the existing linearization or
sampling-based graph generation approaches. Our code can be found at
https://github.com/IBM/Grapher
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Personalized Federated Learning with Hidden Information on Personalized Prior. (arXiv:2211.10684v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10684">http://arxiv.org/abs/2211.10684</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10684] Personalized Federated Learning with Hidden Information on Personalized Prior](http://arxiv.org/abs/2211.10684)</code></li>
<li>Summary: <p>Federated learning (FL for simplification) is a distributed machine learning
technique that utilizes global servers and collaborative clients to achieve
privacy-preserving global model training without direct data sharing. However,
heterogeneous data problem, as one of FL's main problems, makes it difficult
for the global model to perform effectively on each client's local data. Thus,
personalized federated learning (PFL for simplification) aims to improve the
performance of the model on local data as much as possible. Bayesian learning,
where the parameters of the model are seen as random variables with a prior
assumption, is a feasible solution to the heterogeneous data problem due to the
tendency that the more local data the model use, the more it focuses on the
local data, otherwise focuses on the prior. When Bayesian learning is applied
to PFL, the global model provides global knowledge as a prior to the local
training process. In this paper, we employ Bayesian learning to model PFL by
assuming a prior in the scaled exponential family, and therefore propose
pFedBreD, a framework to solve the problem we model using Bregman divergence
regularization. Empirically, our experiments show that, under the prior
assumption of the spherical Gaussian and the first order strategy of mean
selection, our proposal significantly outcompetes other PFL algorithms on
multiple public benchmarks.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Deepfake Detection: A Comprehensive Study from the Reliability Perspective. (arXiv:2211.10881v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10881">http://arxiv.org/abs/2211.10881</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10881] Deepfake Detection: A Comprehensive Study from the Reliability Perspective](http://arxiv.org/abs/2211.10881)</code></li>
<li>Summary: <p>The mushroomed Deepfake synthetic materials circulated on the internet have
raised serious social impact to politicians, celebrities, and every human being
on earth. In this paper, we provide a thorough review of the existing models
following the development history of the Deepfake detection studies and define
the research challenges of Deepfake detection in three aspects, namely,
transferability, interpretability, and reliability. While the transferability
and interpretability challenges have both been frequently discussed and
attempted to solve with quantitative evaluations, the reliability issue has
been barely considered, leading to the lack of reliable evidence in real-life
usages and even for prosecutions on Deepfake related cases in court. We
therefore conduct a model reliability study scheme using statistical random
sampling knowledge and the publicly available benchmark datasets to
qualitatively validate the detection performance of the existing models on
arbitrary Deepfake candidate suspects. A barely remarked systematic data
pre-processing procedure is demonstrated along with the fair training and
testing experiments on the existing detection models. Case studies are further
executed to justify the real-life Deepfake cases including different groups of
victims with the help of reliably qualified detection models. The model
reliability study provides a workflow for the detection models to act as or
assist evidence for Deepfake forensic investigation in court once approved by
authentication experts or institutions.
</p></li>
</ul>

<h3>Title: LibSignal: An Open Library for Traffic Signal Control. (arXiv:2211.10649v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10649">http://arxiv.org/abs/2211.10649</a></li>
<li>Code URL: <a href="https://github.com/DaRL-LibSignal/LibSignal">https://github.com/DaRL-LibSignal/LibSignal</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10649] LibSignal: An Open Library for Traffic Signal Control](http://arxiv.org/abs/2211.10649)</code></li>
<li>Summary: <p>This paper introduces a library for cross-simulator comparison of
reinforcement learning models in traffic signal control tasks. This library is
developed to implement recent state-of-the-art reinforcement learning models
with extensible interfaces and unified cross-simulator evaluation metrics. It
supports commonly-used simulators in traffic signal control tasks, including
Simulation of Urban MObility(SUMO) and CityFlow, and multiple benchmark
datasets for fair comparisons. We conducted experiments to validate our
implementation of the models and to calibrate the simulators so that the
experiments from one simulator could be referential to the other. Based on the
validated models and calibrated environments, this paper compares and reports
the performance of current state-of-the-art RL algorithms across different
datasets and simulators. This is the first time that these methods have been
compared fairly under the same datasets with different simulators.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Concept-based Explanations using Non-negative Concept Activation Vectors and Decision Tree for CNN Models. (arXiv:2211.10807v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10807">http://arxiv.org/abs/2211.10807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10807] Concept-based Explanations using Non-negative Concept Activation Vectors and Decision Tree for CNN Models](http://arxiv.org/abs/2211.10807)</code></li>
<li>Summary: <p>This paper evaluates whether training a decision tree based on concepts
extracted from a concept-based explainer can increase interpretability for
Convolutional Neural Networks (CNNs) models and boost the fidelity and
performance of the used explainer. CNNs for computer vision have shown
exceptional performance in critical industries. However, it is a significant
barrier when deploying CNNs due to their complexity and lack of
interpretability. Recent studies to explain computer vision models have shifted
from extracting low-level features (pixel-based explanations) to mid-or
high-level features (concept-based explanations). The current research
direction tends to use extracted features in developing approximation
algorithms such as linear or decision tree models to interpret an original
model. In this work, we modify one of the state-of-the-art concept-based
explanations and propose an alternative framework named TreeICE. We design a
systematic evaluation based on the requirements of fidelity (approximate models
to original model's labels), performance (approximate models to ground-truth
labels), and interpretability (meaningful of approximate models to humans). We
conduct computational evaluation (for fidelity and performance) and human
subject experiments (for interpretability) We find that Tree-ICE outperforms
the baseline in interpretability and generates more human readable explanations
in the form of a semantic tree structure. This work features how important to
have more understandable explanations when interpretability is crucial.
</p></li>
</ul>

<h3>Title: An interpretable imbalanced semi-supervised deep learning framework for improving differential diagnosis of skin diseases. (arXiv:2211.10858v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10858">http://arxiv.org/abs/2211.10858</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10858] An interpretable imbalanced semi-supervised deep learning framework for improving differential diagnosis of skin diseases](http://arxiv.org/abs/2211.10858)</code></li>
<li>Summary: <p>Dermatological diseases are among the most common disorders worldwide. This
paper presents the first study of the interpretability and imbalanced
semi-supervised learning of the multiclass intelligent skin diagnosis framework
(ISDL) using 58,457 skin images with 10,857 unlabeled samples. Pseudo-labelled
samples from minority classes have a higher probability at each iteration of
class-rebalancing self-training, thereby promoting the utilization of unlabeled
samples to solve the class imbalance problem. Our ISDL achieved a promising
performance with an accuracy of 0.979, sensitivity of 0.975, specificity of
0.973, macro-F1 score of 0.974 and area under the receiver operating
characteristic curve (AUC) of 0.999 for multi-label skin disease
classification. The Shapley Additive explanation (SHAP) method is combined with
our ISDL to explain how the deep learning model makes predictions. This finding
is consistent with the clinical diagnosis. We also proposed a sampling
distribution optimisation strategy to select pseudo-labelled samples in a more
effective manner using ISDLplus. Furthermore, it has the potential to relieve
the pressure placed on professional doctors, as well as help with practical
issues associated with a shortage of such doctors in rural areas.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Solving 3D Inverse Problems using Pre-trained 2D Diffusion Models. (arXiv:2211.10655v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10655">http://arxiv.org/abs/2211.10655</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10655] Solving 3D Inverse Problems using Pre-trained 2D Diffusion Models](http://arxiv.org/abs/2211.10655)</code></li>
<li>Summary: <p>Diffusion models have emerged as the new state-of-the-art generative model
with high quality samples, with intriguing properties such as mode coverage and
high flexibility. They have also been shown to be effective inverse problem
solvers, acting as the prior of the distribution, while the information of the
forward model can be granted at the sampling stage. Nonetheless, as the
generative process remains in the same high dimensional (i.e. identical to data
dimension) space, the models have not been extended to 3D inverse problems due
to the extremely high memory and computational cost. In this paper, we combine
the ideas from the conventional model-based iterative reconstruction with the
modern diffusion models, which leads to a highly effective method for solving
3D medical image reconstruction tasks such as sparse-view tomography, limited
angle tomography, compressed sensing MRI from pre-trained 2D diffusion models.
In essence, we propose to augment the 2D diffusion prior with a model-based
prior in the remaining direction at test time, such that one can achieve
coherent reconstructions across all dimensions. Our method can be run in a
single commodity GPU, and establishes the new state-of-the-art, showing that
the proposed method can perform reconstructions of high fidelity and accuracy
even in the most extreme cases (e.g. 2-view 3D tomography). We further reveal
that the generalization capacity of the proposed method is surprisingly high,
and can be used to reconstruct volumes that are entirely different from the
training dataset.
</p></li>
</ul>

<h3>Title: Parallel Diffusion Models of Operator and Image for Blind Inverse Problems. (arXiv:2211.10656v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10656">http://arxiv.org/abs/2211.10656</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10656] Parallel Diffusion Models of Operator and Image for Blind Inverse Problems](http://arxiv.org/abs/2211.10656)</code></li>
<li>Summary: <p>Diffusion model-based inverse problem solvers have demonstrated
state-of-the-art performance in cases where the forward operator is known (i.e.
non-blind). However, the applicability of the method to blind inverse problems
has yet to be explored. In this work, we show that we can indeed solve a family
of blind inverse problems by constructing another diffusion prior for the
forward operator. Specifically, parallel reverse diffusion guided by gradients
from the intermediate stages enables joint optimization of both the forward
operator parameters as well as the image, such that both are jointly estimated
at the end of the parallel reverse diffusion procedure. We show the efficacy of
our method on two representative tasks -- blind deblurring, and imaging through
turbulence -- and show that our method yields state-of-the-art performance,
while also being flexible to be applicable to general blind inverse problems
when we know the functional forms.
</p></li>
</ul>

<h3>Title: DiffStyler: Controllable Dual Diffusion for Text-Driven Image Stylization. (arXiv:2211.10682v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10682">http://arxiv.org/abs/2211.10682</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10682] DiffStyler: Controllable Dual Diffusion for Text-Driven Image Stylization](http://arxiv.org/abs/2211.10682)</code></li>
<li>Summary: <p>Despite the impressive results of arbitrary image-guided style transfer
methods, text-driven image stylization has recently been proposed for
transferring a natural image into the stylized one according to textual
descriptions of the target style provided by the user. Unlike previous
image-to-image transfer approaches, text-guided stylization progress provides
users with a more precise and intuitive way to express the desired style.
However, the huge discrepancy between cross-modal inputs/outputs makes it
challenging to conduct text-driven image stylization in a typical feed-forward
CNN pipeline. In this paper, we present DiffStyler on the basis of diffusion
models. The cross-modal style information can be easily integrated as guidance
during the diffusion progress step-by-step. In particular, we use a dual
diffusion processing architecture to control the balance between the content
and style of the diffused results. Furthermore, we propose a content
image-based learnable noise on which the reverse denoising process is based,
enabling the stylization results to better preserve the structure information
of the content image. We validate the proposed DiffStyler beyond the baseline
methods through extensive qualitative and quantitative experiments.
</p></li>
</ul>

<h3>Title: IC3D: Image-Conditioned 3D Diffusion for Shape Generation. (arXiv:2211.10865v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10865">http://arxiv.org/abs/2211.10865</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10865] IC3D: Image-Conditioned 3D Diffusion for Shape Generation](http://arxiv.org/abs/2211.10865)</code></li>
<li>Summary: <p>In the last years, Denoising Diffusion Probabilistic Models (DDPMs) obtained
state-of-the-art results in many generative tasks, outperforming GANs and other
classes of generative models. In particular, they reached impressive results in
various image generation sub-tasks, among which conditional generation tasks
such as text-guided image synthesis. Given the success of DDPMs in 2D
generation, they have more recently been applied to 3D shape generation,
outperforming previous approaches and reaching state-of-the-art results.
However, 3D data pose additional challenges, such as the choice of the 3D
representation, which impacts design choices and model efficiency. While
reaching state-of-the-art results in generation quality, existing 3D DDPM works
make little or no use of guidance, mainly being unconditional or
class-conditional. In this paper, we present IC3D, the first Image-Conditioned
3D Diffusion model that generates 3D shapes by image guidance. It is also the
first 3D DDPM model that adopts voxels as a 3D representation. To guide our
DDPM, we present and leverage CISP (Contrastive Image-Shape Pre-training), a
model jointly embedding images and shapes by contrastive pre-training, inspired
by text-to-image DDPM works. Our generative diffusion model outperforms the
state-of-the-art in 3D generation quality and diversity. Furthermore, we show
that our generated shapes are preferred by human evaluators to a SoTA
single-view 3D reconstruction model in terms of quality and coherence to the
query image by running a side-by-side human evaluation.
</p></li>
</ul>

<h3>Title: Attention-based Class Activation Diffusion for Weakly-Supervised Semantic Segmentation. (arXiv:2211.10931v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10931">http://arxiv.org/abs/2211.10931</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10931] Attention-based Class Activation Diffusion for Weakly-Supervised Semantic Segmentation](http://arxiv.org/abs/2211.10931)</code></li>
<li>Summary: <p>Extracting class activation maps (CAM) is a key step for weakly-supervised
semantic segmentation (WSSS). The CAM of convolution neural networks fails to
capture long-range feature dependency on the image and result in the coverage
on only foreground object parts, i.e., a lot of false negatives. An intuitive
solution is <code>coupling'' the CAM with the long-range attention matrix of visual
transformers (ViT) We find that the direct</code>coupling'', e.g., pixel-wise
multiplication of attention and activation, achieves a more global coverage (on
the foreground), but unfortunately goes with a great increase of false
positives, i.e., background pixels are mistakenly included. This paper aims to
tackle this issue. It proposes a new method to couple CAM and Attention matrix
in a probabilistic Diffusion way, and dub it AD-CAM. Intuitively, it integrates
ViT attention and CAM activation in a conservative and convincing way.
Conservative is achieved by refining the attention between a pair of pixels
based on their respective attentions to common neighbors, where the intuition
is two pixels having very different neighborhoods are rarely dependent, i.e.,
their attention should be reduced. Convincing is achieved by diffusing a
pixel's activation to its neighbors (on the CAM) in proportion to the
corresponding attentions (on the AM). In experiments, our results on two
challenging WSSS benchmarks PASCAL VOC and MS~COCO show that AD-CAM as pseudo
labels can yield stronger WSSS models than the state-of-the-art variants of
CAM.
</p></li>
</ul>

<h3>Title: NVDiff: Graph Generation through the Diffusion of Node Vectors. (arXiv:2211.10794v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.10794">http://arxiv.org/abs/2211.10794</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.10794] NVDiff: Graph Generation through the Diffusion of Node Vectors](http://arxiv.org/abs/2211.10794)</code></li>
<li>Summary: <p>Learning to generate graphs is challenging as a graph is a set of pairwise
connected, unordered nodes encoding complex combinatorial structures. Recently,
several works have proposed graph generative models based on normalizing flows
or score-based diffusion models. However, these models need to generate nodes
and edges in parallel from the same process, whose dimensionality is
unnecessarily high. We propose NVDiff, which takes the VGAE structure and uses
a score-based generative model (SGM) as a flexible prior to sample node
vectors. By modeling only node vectors in the latent space, NVDiff
significantly reduces the dimension of the diffusion process and thus improves
sampling speed. Built on the NVDiff framework, we introduce an attention-based
score network capable of capturing both local and global contexts of graphs.
Experiments indicate that NVDiff significantly reduces computations and can
model much larger graphs than competing methods. At the same time, it achieves
superior or competitive performances over various datasets compared to previous
methods.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
