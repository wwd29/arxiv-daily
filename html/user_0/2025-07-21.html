<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-21</h1>
<h3>Title: Physical models realizing the transformer architecture of large language models</h3>
<ul>
<li><strong>Authors: </strong>Zeqian Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, math-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13354">https://arxiv.org/abs/2507.13354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13354">https://arxiv.org/pdf/2507.13354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13354]] Physical models realizing the transformer architecture of large language models(https://arxiv.org/abs/2507.13354)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The introduction of the transformer architecture in 2017 (cf.\cite{VSP2017}) marked the most striking advancement in natural language processing. The transformer is a model architecture relying entirely on an attention mechanism to draw global dependencies between input and output. However, we believe there is a gap in our theoretical understanding of what the transformer is, and why it works physically. In this paper, from a physical perspective on modern chips, we construct physical models in the Fock space over the Hilbert space of tokens realizing large language models based on a transformer architecture as open quantum systems. Our physical models underlie the transformer architecture for large language models.</li>
</ul>

<h3>Title: Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Atharva Bhargude, Ishan Gonehal, Chandler Haney, Dave Yoon, Kevin Zhu, Aaron Sandoval, Sean O'Brien, Kaustubh Vinnakota</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13357">https://arxiv.org/abs/2507.13357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13357">https://arxiv.org/pdf/2507.13357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13357]] Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models(https://arxiv.org/abs/2507.13357)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Phishing attacks represent a significant cybersecurity threat, necessitating adaptive detection techniques. This study explores few-shot Adaptive Linguistic Prompting (ALP) in detecting phishing webpages through the multimodal capabilities of state-of-the-art large language models (LLMs) such as GPT-4o and Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides LLMs to analyze textual deception by breaking down linguistic patterns, detecting urgency cues, and identifying manipulative diction commonly found in phishing content. By integrating textual, visual, and URL-based analysis, we propose a unified model capable of identifying sophisticated phishing attempts. Our experiments demonstrate that ALP significantly enhances phishing detection accuracy by guiding LLMs through structured reasoning and contextual analysis. The findings highlight the potential of ALP-integrated multimodal LLMs to advance phishing detection frameworks, achieving an F1-score of 0.93, surpassing traditional approaches. These results establish a foundation for more robust, interpretable, and adaptive linguistic-based phishing detection systems using LLMs.</li>
</ul>

<h3>Title: Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance</h3>
<ul>
<li><strong>Authors: </strong>Le-Anh Tran, Chung Nguyen Tran, Ngoc-Luu Nguyen, Nhan Cach Dang, Jordi Carrabina, David Castells-Rufas, Minh Son Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13360">https://arxiv.org/abs/2507.13360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13360">https://arxiv.org/pdf/2507.13360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13360]] Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance(https://arxiv.org/abs/2507.13360)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel deep learning framework for low-light image enhancement, named the Encoder-Decoder Network with Illumination Guidance (EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination map, derived from Bright Channel Prior (BCP), as a guidance input. This illumination guidance helps the network focus on underexposed regions, effectively steering the enhancement process. To further improve the model's representational power, a Spatial Pyramid Pooling (SPP) module is incorporated to extract multi-scale contextual features, enabling better handling of diverse lighting conditions. Additionally, the Swish activation function is employed to ensure smoother gradient propagation during training. EDNIG is optimized within a Generative Adversarial Network (GAN) framework using a composite loss function that combines adversarial loss, pixel-wise mean squared error (MSE), and perceptual loss. Experimental results show that EDNIG achieves competitive performance compared to state-of-the-art methods in quantitative metrics and visual quality, while maintaining lower model complexity, demonstrating its suitability for real-world applications. The source code for this work is available at this https URL.</li>
</ul>

<h3>Title: Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Binbin Ji, Siddharth Agrawal, Qiance Tang, Yvonne Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13362">https://arxiv.org/abs/2507.13362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13362">https://arxiv.org/pdf/2507.13362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13362]] Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning(https://arxiv.org/abs/2507.13362)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This study investigates the spatial reasoning capabilities of vision-language models (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement learning. We begin by evaluating the impact of different prompting strategies and find that simple CoT formats, where the model generates a reasoning step before the answer, not only fail to help, but can even harm the model's original performance. In contrast, structured multi-stage prompting based on scene graphs (SceneGraph CoT) significantly improves spatial reasoning accuracy. Furthermore, to improve spatial reasoning ability, we fine-tune models using Group Relative Policy Optimization (GRPO) on the SAT dataset and evaluate their performance on CVBench. Compared to supervised fine-tuning (SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates superior robustness under out-of-distribution (OOD) conditions. In particular, we find that SFT overfits to surface-level linguistic patterns and may degrade performance when test-time phrasing changes (e.g., from "closer to" to "farther from"). GRPO, on the other hand, generalizes more reliably and maintains stable performance under such shifts. Our findings provide insights into how reinforcement learning and structured prompting improve the spatial reasoning capabilities and generalization behavior of modern VLMs. All code is open source at: this https URL</li>
</ul>

<h3>Title: OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning</h3>
<ul>
<li><strong>Authors: </strong>Siddharth Srivastava, Gaurav Sharma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13364">https://arxiv.org/abs/2507.13364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13364">https://arxiv.org/pdf/2507.13364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13364]] OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning(https://arxiv.org/abs/2507.13364)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present a novel multimodal multitask network and associated training algorithm. The method is capable of ingesting data from approximately 12 different modalities namely image, video, audio, text, depth, point cloud, time series, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed approach utilizes modality specialized tokenizers, a shared transformer architecture, and cross-attention mechanisms to project the data from different modalities into a unified embedding space. It addresses multimodal and multitask scenarios by incorporating modality-specific task heads for different tasks in respective modalities. We propose a novel pretraining strategy with iterative modality switching to initialize the network, and a training algorithm which trades off fully joint training over all modalities, with training on pairs of modalities at a time. We provide comprehensive evaluation across 25 datasets from 12 modalities and show state of the art performances, demonstrating the effectiveness of the proposed architecture, pretraining strategy and adapted multitask training.</li>
</ul>

<h3>Title: A Novel APVD Steganography Technique Incorporating Pseudorandom Pixel Selection for Robust Image Security</h3>
<ul>
<li><strong>Authors: </strong>Mehrab Hosain, Rajiv Kapoor</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.MM, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13367">https://arxiv.org/abs/2507.13367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13367">https://arxiv.org/pdf/2507.13367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13367]] A Novel APVD Steganography Technique Incorporating Pseudorandom Pixel Selection for Robust Image Security(https://arxiv.org/abs/2507.13367)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust</a></li>
<li><strong>Abstract: </strong>Steganography is the process of embedding secret information discreetly within a carrier, ensuring secure exchange of confidential data. The Adaptive Pixel Value Differencing (APVD) steganography method, while effective, encounters certain challenges like the "unused blocks" issue. This problem can cause a decrease in security, compromise the embedding capacity, and lead to lower visual quality. This research presents a novel steganographic strategy that integrates APVD with pseudorandom pixel selection to effectively mitigate these issues. The results indicate that the new method outperforms existing techniques in aspects of security, data hiding capacity, and the preservation of image quality. Empirical results reveal that the combination of APVD with pseudorandom pixel selection significantly enhances key image quality metrics such as Peak Signal-to-Noise Ratio (PSNR), Universal Image Quality Index (UIQ), and Structural Similarity Index (SSIM), surpassing other contemporary methods in performance. The newly proposed method is versatile, able to handle a variety of cover and secret images in both color and grayscale, thereby ensuring secure data transmission without compromising the aesthetic quality of the image.</li>
</ul>

<h3>Title: Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation</h3>
<ul>
<li><strong>Authors: </strong>Yeming Cai, Yang Wang, Zhenglin Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13371">https://arxiv.org/abs/2507.13371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13371">https://arxiv.org/pdf/2507.13371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13371]] Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation(https://arxiv.org/abs/2507.13371)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>This paper proposes an end-to-end deep learning framework integrating optical motion capture with a Transformer-based model to enhance medical rehabilitation. It tackles data noise and missing data caused by occlusion and environmental factors, while detecting abnormal movements in real time to ensure patient safety. Utilizing temporal sequence modeling, our framework denoises and completes motion capture data, improving robustness. Evaluations on stroke and orthopedic rehabilitation datasets show superior performance in data reconstruction and anomaly detection, providing a scalable, cost-effective solution for remote rehabilitation with reduced on-site supervision.</li>
</ul>

<h3>Title: Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Yeming Cai, Zhenglin Li, Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13372">https://arxiv.org/abs/2507.13372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13372">https://arxiv.org/pdf/2507.13372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13372]] Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks(https://arxiv.org/abs/2507.13372)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Breast cancer is a leading cause of death among women globally, and early detection is critical for improving survival rates. This paper introduces an innovative framework that integrates Vision Transformers (ViT) and Graph Neural Networks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset. Our framework leverages ViT's ability to capture global image features and GNN's strength in modeling structural relationships, achieving an accuracy of 84.2%, outperforming traditional methods. Additionally, interpretable attention heatmaps provide insights into the model's decision-making process, aiding radiologists in clinical settings.</li>
</ul>

<h3>Title: Butter: Frequency Consistency and Hierarchical Fusion for Autonomous Driving Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Xiaojian Lin, Wenxin Zhang, Yuchu Jiang, Wangyu Wu, Yiran Guo, Kangxu Wang, Zongzheng Zhang, Guijin Wang, Lei Jin, Hao Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13373">https://arxiv.org/abs/2507.13373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13373">https://arxiv.org/pdf/2507.13373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13373]] Butter: Frequency Consistency and Hierarchical Fusion for Autonomous Driving Object Detection(https://arxiv.org/abs/2507.13373)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Hierarchical feature representations play a pivotal role in computer vision, particularly in object detection for autonomous driving. Multi-level semantic understanding is crucial for accurately identifying pedestrians, vehicles, and traffic signs in dynamic environments. However, existing architectures, such as YOLO and DETR, struggle to maintain feature consistency across different scales while balancing detection precision and computational efficiency. To address these challenges, we propose Butter, a novel object detection framework designed to enhance hierarchical feature representations for improving detection robustness. Specifically, Butter introduces two key innovations: Frequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which refines multi-scale feature consistency by leveraging adaptive frequency filtering to enhance structural and boundary precision, and Progressive Hierarchical Feature Fusion Network (PHFFNet) Module, which progressively integrates multi-level features to mitigate semantic gaps and strengthen hierarchical feature learning. Through extensive experiments on BDD100K, KITTI, and Cityscapes, Butter demonstrates superior feature representation capabilities, leading to notable improvements in detection accuracy while reducing model complexity. By focusing on hierarchical feature refinement and integration, Butter provides an advanced approach to object detection that achieves a balance between accuracy, deployability, and computational efficiency in real-time autonomous driving scenarios. Our model and implementation are publicly available at this https URL, facilitating further research and validation within the autonomous driving community.</li>
</ul>

<h3>Title: Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Keito Inoshita, Rushia Harada</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13380">https://arxiv.org/abs/2507.13380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13380">https://arxiv.org/pdf/2507.13380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13380]] Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition(https://arxiv.org/abs/2507.13380)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In the field of emotion recognition, the development of high-performance models remains a challenge due to the scarcity of high-quality, diverse emotional datasets. Emotional expressions are inherently subjective, shaped by individual personality traits, socio-cultural backgrounds, and contextual factors, making large-scale, generalizable data collection both ethically and practically difficult. To address this issue, we introduce PersonaGen, a novel framework for generating emotionally rich text using a Large Language Model (LLM) through multi-stage persona-based conditioning. PersonaGen constructs layered virtual personas by combining demographic attributes, socio-cultural backgrounds, and detailed situational contexts, which are then used to guide emotion expression generation. We conduct comprehensive evaluations of the generated synthetic data, assessing semantic diversity through clustering and distributional metrics, human-likeness via LLM-based quality scoring, realism through comparison with real-world emotion corpora, and practical utility in downstream emotion classification tasks. Experimental results show that PersonaGen significantly outperforms baseline methods in generating diverse, coherent, and discriminative emotion expressions, demonstrating its potential as a robust alternative for augmenting or replacing real-world emotional datasets.</li>
</ul>

<h3>Title: SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Rafiq Kamel, Filippo Guerranti, Simon Geisler, Stephan Günnemann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13381">https://arxiv.org/abs/2507.13381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13381">https://arxiv.org/pdf/2507.13381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13381]] SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation(https://arxiv.org/abs/2507.13381)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly applied to tasks involving structured inputs such as graphs. Abstract Meaning Representations (AMRs), which encode rich semantics as directed graphs, offer a rigorous testbed for evaluating LLMs on text generation from such structures. Yet, current methods often arbitrarily linearize AMRs, discarding key structural cues, or rely on architectures incompatible with standard LLMs. We introduce SAFT, a structure-aware fine-tuning approach that injects graph topology into pretrained LLMs without architectural changes. We compute direction-sensitive positional encodings from the magnetic Laplacian of transformed AMRs and project them into the embedding space of the LLM. While possibly applicable to any graph-structured inputs, we focus on AMR-to-text generation as a representative and challenging benchmark. SAFT sets a new state-of-the-art on AMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph complexity, highlighting the value of structure-aware representations in enhancing LLM performance. SAFT offers a general and effective pathway for bridging structured data and language models.</li>
</ul>

<h3>Title: Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery</h3>
<ul>
<li><strong>Authors: </strong>Arjun Rao, Esther Rolf</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13385">https://arxiv.org/abs/2507.13385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13385">https://arxiv.org/pdf/2507.13385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13385]] Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery(https://arxiv.org/abs/2507.13385)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>A large variety of geospatial data layers is available around the world ranging from remotely-sensed raster data like satellite imagery, digital elevation models, predicted land cover maps, and human-annotated data, to data derived from environmental sensors such as air temperature or wind speed data. A large majority of machine learning models trained on satellite imagery (SatML), however, are designed primarily for optical input modalities such as multi-spectral satellite imagery. To better understand the value of using other input modalities alongside optical imagery in supervised learning settings, we generate augmented versions of SatML benchmark tasks by appending additional geographic data layers to datasets spanning classification, regression, and segmentation. Using these augmented datasets, we find that fusing additional geographic inputs with optical imagery can significantly improve SatML model performance. Benefits are largest in settings where labeled data are limited and in geographic out-of-sample settings, suggesting that multi-modal inputs may be especially valuable for data-efficiency and out-of-sample performance of SatML models. Surprisingly, we find that hard-coded fusion strategies outperform learned variants, with interesting implications for future work.</li>
</ul>

<h3>Title: Minimalist Concept Erasure in Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhang, Er Jin, Yanfei Dong, Yixuan Wu, Philip Torr, Ashkan Khakzar, Johannes Stegmaier, Kenji Kawaguchi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13386">https://arxiv.org/abs/2507.13386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13386">https://arxiv.org/pdf/2507.13386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13386]] Minimalist Concept Erasure in Generative Models(https://arxiv.org/abs/2507.13386)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative models have demonstrated remarkable capabilities in producing high-quality images, but their reliance on large-scale unlabeled data has raised significant safety and copyright concerns. Efforts to address these issues by erasing unwanted concepts have shown promise. However, many existing erasure methods involve excessive modifications that compromise the overall utility of the model. In this work, we address these issues by formulating a novel minimalist concept erasure objective based \emph{only} on the distributional distance of final generation outputs. Building on our formulation, we derive a tractable loss for differentiable optimization that leverages backpropagation through all generation steps in an end-to-end manner. We also conduct extensive analysis to show theoretical connections with other models and methods. To improve the robustness of the erasure, we incorporate neuron masking as an alternative to model fine-tuning. Empirical evaluations on state-of-the-art flow-matching models demonstrate that our method robustly erases concepts without degrading overall model performance, paving the way for safer and more responsible generative models.</li>
</ul>

<h3>Title: PARAM-1 BharatGen 2.9B Model</h3>
<ul>
<li><strong>Authors: </strong>Kundeshwar Pundalik, Piyush Sawarkar, Nihar Sahoo, Abhishek Shinde, Prateek Chanda, Vedant Goswami, Ajay Nagpal, Atul Singh, Viraj Thakur, Vijay Dewane, Aamod Thakur, Bhargav Patel, Smita Gautam, Bhagwan Panditi, Shyam Pawar, Madhav Kotcha, Suraj Racha, Saral Sureka, Pankaj Singh, Rishi Bal, Rohit Saluja, Ganesh Ramakrishnan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13390">https://arxiv.org/abs/2507.13390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13390">https://arxiv.org/pdf/2507.13390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13390]] PARAM-1 BharatGen 2.9B Model(https://arxiv.org/abs/2507.13390)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as powerful general-purpose reasoning systems, yet their development remains dominated by English-centric data, architectures, and optimization paradigms. This exclusionary design results in structural under-representation of linguistically diverse regions such as India, where over 20 official languages and 100+ dialects coexist alongside phenomena like code-switching and diglossia. We introduce PARAM-1, a 2.9B parameter decoder-only, text-only language model trained from scratch with an explicit architectural and linguistic focus on Indian diversity. PARAM-1 is trained on a bilingual dataset consisting of only Hindi and English, constructed with a strong focus on fact-rich, high-quality content. It is guided by three core principles: equitable representation of Indic languages through a 25% corpus allocation; tokenization fairness via a SentencePiece tokenizer adapted to Indian morphological structures; and culturally aligned evaluation benchmarks across IndicQA, code-mixed reasoning, and socio-linguistic robustness tasks. By embedding diversity at the pretraining level-rather than deferring it to post-hoc alignment-PARAM-1 offers a design-first blueprint for equitable foundation modeling. Our results demonstrate that it serves as both a competent general-purpose model and a robust baseline for India-centric applications.</li>
</ul>

<h3>Title: TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction</h3>
<ul>
<li><strong>Authors: </strong>Emil Häglund, Johanna Björklund</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13392">https://arxiv.org/abs/2507.13392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13392">https://arxiv.org/pdf/2507.13392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13392]] TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction(https://arxiv.org/abs/2507.13392)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>We improve the extraction of insights from customer reviews by restructuring the topic modelling pipeline to operate on opinion units - distinct statements that include relevant text excerpts and associated sentiment scores. Prior work has demonstrated that such units can be reliably extracted using large language models. The result is a heightened performance of the subsequent topic modeling, leading to coherent and interpretable topics while also capturing the sentiment associated with each topic. By correlating the topics and sentiments with business metrics, such as star ratings, we can gain insights on how specific customer concerns impact business outcomes. We present our system's implementation, use cases, and advantages over other topic modeling and classification solutions. We also evaluate its effectiveness in creating coherent topics and assess methods for integrating topic and sentiment modalities for accurate star-rating prediction.</li>
</ul>

<h3>Title: Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only</h3>
<ul>
<li><strong>Authors: </strong>Xuanqi Gao, Weipeng Jiang, Juan Zhai, Shiqing Ma, Siyi Xie, Xinyang Yin, Chao Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13395">https://arxiv.org/abs/2507.13395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13395">https://arxiv.org/pdf/2507.13395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13395]] Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only(https://arxiv.org/abs/2507.13395)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The advent of neural machine translation (NMT) has revolutionized cross-lingual communication, yet preserving stylistic nuances remains a significant challenge. While existing approaches often require parallel corpora for style preservation, we introduce Babel, a novel framework that enhances stylistic fidelity in NMT using only monolingual corpora. Babel employs two key components: (1) a style detector based on contextual embeddings that identifies stylistic disparities between source and target texts, and (2) a diffusion-based style applicator that rectifies stylistic inconsistencies while maintaining semantic integrity. Our framework integrates with existing NMT systems as a post-processing module, enabling style-aware translation without requiring architectural modifications or parallel stylistic data. Extensive experiments on five diverse domains (law, literature, scientific writing, medicine, and educational content) demonstrate Babel's effectiveness: it identifies stylistic inconsistencies with 88.21% precision and improves stylistic preservation by 150% while maintaining a high semantic similarity score of 0.92. Human evaluation confirms that translations refined by Babel better preserve source text style while maintaining fluency and adequacy.</li>
</ul>

<h3>Title: InSyn: Modeling Complex Interactions for Pedestrian Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Kaiyuan Zhai, Juan Chen, Chao Wang, Zeyi Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13397">https://arxiv.org/abs/2507.13397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13397">https://arxiv.org/pdf/2507.13397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13397]] InSyn: Modeling Complex Interactions for Pedestrian Trajectory Prediction(https://arxiv.org/abs/2507.13397)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate pedestrian trajectory prediction is crucial for intelligent applications, yet it remains highly challenging due to the complexity of interactions among pedestrians. Previous methods have primarily relied on relative positions to model pedestrian interactions; however, they tend to overlook specific interaction patterns such as paired walking or conflicting behaviors, limiting the prediction accuracy in crowded scenarios. To address this issue, we propose InSyn (Interaction-Synchronization Network), a novel Transformer-based model that explicitly captures diverse interaction patterns (e.g., walking in sync or conflicting) while effectively modeling direction-sensitive social behaviors. Additionally, we introduce a training strategy termed Seq-Start of Seq (SSOS), designed to alleviate the common issue of initial-step divergence in numerical time-series prediction. Experiments on the ETH and UCY datasets demonstrate that our model outperforms recent baselines significantly, especially in high-density scenarios. Furthermore, the SSOS strategy proves effective in improving sequential prediction performance, reducing the initial-step prediction error by approximately 6.58%.</li>
</ul>

<h3>Title: Selective Embedding for Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Mert Sehri, Zehui Hua, Francisco de Assis Boldt, Patrick Dumond</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13399">https://arxiv.org/abs/2507.13399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13399">https://arxiv.org/pdf/2507.13399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13399]] Selective Embedding for Deep Learning(https://arxiv.org/abs/2507.13399)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning has revolutionized many industries by enabling models to automatically learn complex patterns from raw data, reducing dependence on manual feature engineering. However, deep learning algorithms are sensitive to input data, and performance often deteriorates under nonstationary conditions and across dissimilar domains, especially when using time-domain data. Conventional single-channel or parallel multi-source data loading strategies either limit generalization or increase computational costs. This study introduces selective embedding, a novel data loading strategy, which alternates short segments of data from multiple sources within a single input channel. Drawing inspiration from cognitive psychology, selective embedding mimics human-like information processing to reduce model overfitting, enhance generalization, and improve computational efficiency. Validation is conducted using six time-domain datasets, demonstrating that the proposed method consistently achieves high classification accuracy across various deep learning architectures while significantly reducing training times. The approach proves particularly effective for complex systems with multiple data sources, offering a scalable and resource-efficient solution for real-world applications in healthcare, heavy machinery, marine, railway, and agriculture, where robustness and adaptability are critical.</li>
</ul>

<h3>Title: MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing</h3>
<ul>
<li><strong>Authors: </strong>Shreya Kadambi, Risheek Garrepalli, Shubhankar Borse, Munawar Hyatt, Fatih Porikli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13401">https://arxiv.org/abs/2507.13401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13401">https://arxiv.org/pdf/2507.13401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13401]] MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing(https://arxiv.org/abs/2507.13401)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of diffusion models in text-to-image generation, their effectiveness in grounded visual editing and compositional control remains challenging. Motivated by advances in self-supervised learning and in-context generative modeling, we propose a series of simple yet powerful design choices that significantly enhance diffusion model capacity for structured, controllable generation and editing. We introduce Masking-Augmented Diffusion with Inference-Time Scaling (MADI), a framework that improves the editability, compositionality and controllability of diffusion models through two core innovations. First, we introduce Masking-Augmented gaussian Diffusion (MAgD), a novel training strategy with dual corruption process which combines standard denoising score matching and masked reconstruction by masking noisy input from forward process. MAgD encourages the model to learn discriminative and compositional visual representations, thus enabling localized and structure-aware editing. Second, we introduce an inference-time capacity scaling mechanism based on Pause Tokens, which act as special placeholders inserted into the prompt for increasing computational capacity at inference time. Our findings show that adopting expressive and dense prompts during training further enhances performance, particularly for MAgD. Together, these contributions in MADI substantially enhance the editability of diffusion models, paving the way toward their integration into more general-purpose, in-context generative diffusion architectures.</li>
</ul>

<h3>Title: UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data</h3>
<ul>
<li><strong>Authors: </strong>Morteza Bodaghi, Majid Hosseini, Raju Gottumukkala, Ravi Teja Bhupatiraju, Iftikhar Ahmad, Moncef Gabbouj</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13403">https://arxiv.org/abs/2507.13403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13403">https://arxiv.org/pdf/2507.13403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13403]] UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data(https://arxiv.org/abs/2507.13403)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>In this study, we present a comprehensive public dataset for driver drowsiness detection, integrating multimodal signals of facial, behavioral, and biometric indicators. Our dataset includes 3D facial video using a depth camera, IR camera footage, posterior videos, and biometric signals such as heart rate, electrodermal activity, blood oxygen saturation, skin temperature, and accelerometer data. This data set provides grip sensor data from the steering wheel and telemetry data from the American truck simulator game to provide more information about drivers' behavior while they are alert and drowsy. Drowsiness levels were self-reported every four minutes using the Karolinska Sleepiness Scale (KSS). The simulation environment consists of three monitor setups, and the driving condition is completely like a car. Data were collected from 19 subjects (15 M, 4 F) in two conditions: when they were fully alert and when they exhibited signs of sleepiness. Unlike other datasets, our multimodal dataset has a continuous duration of 40 minutes for each data collection session per subject, contributing to a total length of 1,400 minutes, and we recorded gradual changes in the driver state rather than discrete alert/drowsy labels. This study aims to create a comprehensive multimodal dataset of driver drowsiness that captures a wider range of physiological, behavioral, and driving-related signals. The dataset will be available upon request to the corresponding author.</li>
</ul>

<h3>Title: AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation</h3>
<ul>
<li><strong>Authors: </strong>Delin An, Pan Du, Jian-Xun Wang, Chaoli Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13404">https://arxiv.org/abs/2507.13404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13404">https://arxiv.org/pdf/2507.13404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13404]] AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation(https://arxiv.org/abs/2507.13404)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Accurate 3D aortic construction is crucial for clinical diagnosis, preoperative planning, and computational fluid dynamics (CFD) simulations, as it enables the estimation of critical hemodynamic parameters such as blood flow velocity, pressure distribution, and wall shear stress. Existing construction methods often rely on large annotated training datasets and extensive manual intervention. While the resulting meshes can serve for visualization purposes, they struggle to produce geometrically consistent, well-constructed surfaces suitable for downstream CFD analysis. To address these challenges, we introduce AortaDiff, a diffusion-based framework that generates smooth aortic surfaces directly from CT/MRI volumes. AortaDiff first employs a volume-guided conditional diffusion model (CDM) to iteratively generate aortic centerlines conditioned on volumetric medical images. Each centerline point is then automatically used as a prompt to extract the corresponding vessel contour, ensuring accurate boundary delineation. Finally, the extracted contours are fitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh representation. AortaDiff offers distinct advantages over existing methods, including an end-to-end workflow, minimal dependency on large labeled datasets, and the ability to generate CFD-compatible aorta meshes with high geometric fidelity. Experimental results demonstrate that AortaDiff performs effectively even with limited training data, successfully constructing both normal and pathologically altered aorta meshes, including cases with aneurysms or coarctation. This capability enables the generation of high-quality visualizations and positions AortaDiff as a practical solution for cardiovascular research.</li>
</ul>

<h3>Title: IConMark: Robust Interpretable Concept-Based Watermark For AI Images</h3>
<ul>
<li><strong>Authors: </strong>Vinu Sankar Sadasivan, Mehrdad Saberi, Soheil Feizi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13407">https://arxiv.org/abs/2507.13407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13407">https://arxiv.org/pdf/2507.13407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13407]] IConMark: Robust Interpretable Concept-Based Watermark For AI Images(https://arxiv.org/abs/2507.13407)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark, generative</a></li>
<li><strong>Abstract: </strong>With the rapid rise of generative AI and synthetic media, distinguishing AI-generated images from real ones has become crucial in safeguarding against misinformation and ensuring digital authenticity. Traditional watermarking techniques have shown vulnerabilities to adversarial attacks, undermining their effectiveness in the presence of attackers. We propose IConMark, a novel in-generation robust semantic watermarking method that embeds interpretable concepts into AI-generated images, as a first step toward interpretable watermarking. Unlike traditional methods, which rely on adding noise or perturbations to AI-generated images, IConMark incorporates meaningful semantic attributes, making it interpretable to humans and hence, resilient to adversarial manipulation. This method is not only robust against various image augmentations but also human-readable, enabling manual verification of watermarks. We demonstrate a detailed evaluation of IConMark's effectiveness, demonstrating its superiority in terms of detection accuracy and maintaining image quality. Moreover, IConMark can be combined with existing watermarking techniques to further enhance and complement its robustness. We introduce IConMark+SS and IConMark+TM, hybrid approaches combining IConMark with StegaStamp and TrustMark, respectively, to further bolster robustness against multiple types of image manipulations. Our base watermarking technique (IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9% higher mean area under the receiver operating characteristic curve (AUROC) scores for watermark detection, respectively, compared to the best baseline on various datasets.</li>
</ul>

<h3>Title: Causal Language Control in Multilingual Transformers via Sparse Feature Steering</h3>
<ul>
<li><strong>Authors: </strong>Cheng-Ting Chou, George Liu, Jessica Sun, Cole Blondin, Kevin Zhu, Vasu Sharma, Sean O'Brien</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13410">https://arxiv.org/abs/2507.13410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13410">https://arxiv.org/pdf/2507.13410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13410]] Causal Language Control in Multilingual Transformers via Sparse Feature Steering(https://arxiv.org/abs/2507.13410)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Deterministically controlling the target generation language of large multilingual language models (LLMs) remains a fundamental challenge, particularly in zero-shot settings where neither explicit language prompts nor fine-tuning are available. In this work, we investigate whether sparse autoencoder (SAE) features, previously shown to correlate with interpretable model behaviors, can be leveraged to steer the generated language of LLMs during inference. Leveraging pretrained SAEs on the residual streams of Gemma-2B and Gemma-9B, we identify features whose activations differ most significantly between English and four target languages: Chinese, Japanese, Spanish, and French. By modifying just a single SAE feature at one transformer layer, we achieve controlled language shifts with up to 90\% success, as measured by FastText language classification, while preserving semantic fidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding) similarity. Our analysis reveals that language steering is most effective in mid-to-late transformer layers and is amplified by specific attention heads disproportionately associated with language-sensitive SAE features. These results demonstrate the promise of sparse feature steering as a lightweight and interpretable mechanism for controllable multilingual generation.</li>
</ul>

<h3>Title: Aligning Knowledge Graphs and Language Models for Factual Accuracy</h3>
<ul>
<li><strong>Authors: </strong>Nur A Zarin Nishat, Andrea Coletta, Luigi Bellomarini, Kossi Amouzouvi, Jens Lehmann, Sahar Vahdati</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13411">https://arxiv.org/abs/2507.13411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13411">https://arxiv.org/pdf/2507.13411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13411]] Aligning Knowledge Graphs and Language Models for Factual Accuracy(https://arxiv.org/abs/2507.13411)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models like GPT-4, Gemini, and Claude have transformed natural language processing (NLP) tasks such as question answering, dialogue generation, summarization, and so forth; yet their susceptibility to hallucination stands as one of the major challenges. Among numerous approaches to overcome this challenge, integration of Knowledge Graphs (KGs) into language models has emerged as a promising solution as it provides structured, reliable, domain-specific, and up-to-date external information to the language models. In this paper, we introduce ALIGNed-LLM, a simple yet effective approach to improve language models' factuality via a lean strategy to infuse KGs into the latent space of language models inspired by LLaVA where visual and textual information is infused. We use embeddings from a pre-trained Knowledge Graph Embedding (KGE) model, such as TransE, and a trainable projection layer to align entity and text embeddings. This alignment enables the language model to distinguish between similar entities improving factual grounding and reducing hallucination. We tested our approach on three popular questions-answering benchmark datasets alongside language models of varying sizes, showing significant improvement. Furthermore, we applied our approach to a real-world financial use case from a large central bank in Europe, which demands high accuracy and precision, demonstrating a substantial improvement of the LLM answers.</li>
</ul>

<h3>Title: LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Aleksey Lapin, Igor Hromov, Stanislav Chumakov, Mile Mitrovic, Dmitry Simakov, Nikolay O. Nikitin, Andrey V. Savchenko</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13413">https://arxiv.org/abs/2507.13413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13413">https://arxiv.org/pdf/2507.13413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13413]] LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data(https://arxiv.org/abs/2507.13413)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>AutoML has advanced in handling complex tasks using the integration of LLMs, yet its efficiency remains limited by dependence on specific underlying tools. In this paper, we introduce LightAutoDS-Tab, a multi-AutoML agentic system for tasks with tabular data, which combines an LLM-based code generation with several AutoML tools. Our approach improves the flexibility and robustness of pipeline design, outperforming state-of-the-art open-source solutions on several data science tasks from Kaggle. The code of LightAutoDS-Tab is available in the open repository this https URL</li>
</ul>

<h3>Title: Gauge Flow Models</h3>
<ul>
<li><strong>Authors: </strong>Alexander Strunk, Roland Assam</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.DG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13414">https://arxiv.org/abs/2507.13414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13414">https://arxiv.org/pdf/2507.13414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13414]] Gauge Flow Models(https://arxiv.org/abs/2507.13414)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces Gauge Flow Models, a novel class of Generative Flow Models. These models incorporate a learnable Gauge Field within the Flow Ordinary Differential Equation (ODE). A comprehensive mathematical framework for these models, detailing their construction and properties, is provided. Experiments using Flow Matching on Gaussian Mixture Models demonstrate that Gauge Flow Models yields significantly better performance than traditional Flow Models of comparable or even larger size. Additionally, unpublished research indicates a potential for enhanced performance across a broader range of generative tasks.</li>
</ul>

<h3>Title: AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Pistola, Valentina Orru', Nicolo' Marchetti, Marco Roccetti</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13420">https://arxiv.org/abs/2507.13420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13420">https://arxiv.org/pdf/2507.13420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13420]] AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery(https://arxiv.org/abs/2507.13420)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>By upgrading an existing deep learning model with the knowledge provided by one of the oldest sets of grayscale satellite imagery, known as CORONA, we improved the AI model attitude towards the automatic identification of archaeological sites in an environment which has been completely transformed in the last five decades, including the complete destruction of many of those same sites. The initial Bing based convolutional network model was retrained using CORONA satellite imagery for the district of Abu Ghraib, west of Baghdad, central Mesopotamian floodplain. The results were twofold and surprising. First, the detection precision obtained on the area of interest increased sensibly: in particular, the Intersection over Union (IoU) values, at the image segmentation level, surpassed 85 percent, while the general accuracy in detecting archeological sites reached 90 percent. Second, our retrained model allowed the identification of four new sites of archaeological interest (confirmed through field verification), previously not identified by archaeologists with traditional techniques. This has confirmed the efficacy of using AI techniques and the CORONA imagery from the 1960 to discover archaeological sites currently no longer visible, a concrete breakthrough with significant consequences for the study of landscapes with vanishing archaeological evidence induced by anthropization</li>
</ul>

<h3>Title: CaSTFormer: Causal Spatio-Temporal Transformer for Driving Intention Prediction</h3>
<ul>
<li><strong>Authors: </strong>Sirui Wang, Zhou Guan, Bingxi Zhao, Tongjia Gu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13425">https://arxiv.org/abs/2507.13425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13425">https://arxiv.org/pdf/2507.13425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13425]] CaSTFormer: Causal Spatio-Temporal Transformer for Driving Intention Prediction(https://arxiv.org/abs/2507.13425)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Accurate prediction of driving intention is key to enhancing the safety and interactive efficiency of human-machine co-driving systems. It serves as a cornerstone for achieving high-level autonomous driving. However, current approaches remain inadequate for accurately modeling the complex spatio-temporal interdependencies and the unpredictable variability of human driving behavior. To address these challenges, we propose CaSTFormer, a Causal Spatio-Temporal Transformer to explicitly model causal interactions between driver behavior and environmental context for robust intention prediction. Specifically, CaSTFormer introduces a novel Reciprocal Shift Fusion (RSF) mechanism for precise temporal alignment of internal and external feature streams, a Causal Pattern Extraction (CPE) module that systematically eliminates spurious correlations to reveal authentic causal dependencies, and an innovative Feature Synthesis Network (FSN) that adaptively synthesizes these purified representations into coherent spatio-temporal inferences. We evaluate the proposed CaSTFormer on the public Brain4Cars dataset, and it achieves state-of-the-art performance. It effectively captures complex causal spatio-temporal dependencies and enhances both the accuracy and transparency of driving intention prediction.</li>
</ul>

<h3>Title: Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers</h3>
<ul>
<li><strong>Authors: </strong>Liang Lin, Zhihao Xu, Xuehai Tang, Shi Liu, Biyu Zhou, Fuqing Zhu, Jizhong Han, Songlin Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13474">https://arxiv.org/abs/2507.13474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13474">https://arxiv.org/pdf/2507.13474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13474]] Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers(https://arxiv.org/abs/2507.13474)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>The safety of large language models (LLMs) has garnered significant research attention. In this paper, we argue that previous empirical studies demonstrate LLMs exhibit a propensity to trust information from authoritative sources, such as academic papers, implying new possible vulnerabilities. To verify this possibility, a preliminary analysis is designed to illustrate our two findings. Based on this insight, a novel jailbreaking method, Paper Summary Attack (\llmname{PSA}), is proposed. It systematically synthesizes content from either attack-focused or defense-focused LLM safety paper to construct an adversarial prompt template, while strategically infilling harmful query as adversarial payloads within predefined subsections. Extensive experiments show significant vulnerabilities not only in base LLMs, but also in state-of-the-art reasoning model like Deepseek-R1. PSA achieves a 97\% attack success rate (ASR) on well-aligned models like Claude3.5-Sonnet and an even higher 98\% ASR on Deepseek-R1. More intriguingly, our work has further revealed diametrically opposed vulnerability bias across different base models, and even between different versions of the same model, when exposed to either attack-focused or defense-focused papers. This phenomenon potentially indicates future research clues for both adversarial methodologies and safety this http URL is available at this https URL</li>
</ul>

<h3>Title: Uncertainty Quantification Framework for Aerial and UAV Photogrammetry through Error Propagation</h3>
<ul>
<li><strong>Authors: </strong>Debao Huang, Rongjun Qin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13486">https://arxiv.org/abs/2507.13486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13486">https://arxiv.org/pdf/2507.13486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13486]] Uncertainty Quantification Framework for Aerial and UAV Photogrammetry through Error Propagation(https://arxiv.org/abs/2507.13486)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Uncertainty quantification of the photogrammetry process is essential for providing per-point accuracy credentials of the point clouds. Unlike airborne LiDAR, which typically delivers consistent accuracy across various scenes, the accuracy of photogrammetric point clouds is highly scene-dependent, since it relies on algorithm-generated measurements (i.e., stereo or multi-view stereo). Generally, errors of the photogrammetric point clouds propagate through a two-step process: Structure-from-Motion (SfM) with Bundle adjustment (BA), followed by Multi-view Stereo (MVS). While uncertainty estimation in the SfM stage has been well studied using the first-order statistics of the reprojection error function, that in the MVS stage remains largely unsolved and non-standardized, primarily due to its non-differentiable and multi-modal nature (i.e., from pixel values to geometry). In this paper, we present an uncertainty quantification framework closing this gap by associating an error covariance matrix per point accounting for this two-step photogrammetry process. Specifically, to estimate the uncertainty in the MVS stage, we propose a novel, self-calibrating method by taking reliable n-view points (n>=6) per-view to regress the disparity uncertainty using highly relevant cues (such as matching cost values) from the MVS stage. Compared to existing approaches, our method uses self-contained, reliable 3D points extracted directly from the MVS process, with the benefit of being self-supervised and naturally adhering to error propagation path of the photogrammetry process, thereby providing a robust and certifiable uncertainty quantification across diverse scenes. We evaluate the framework using a variety of publicly available airborne and UAV imagery datasets. Results demonstrate that our method outperforms existing approaches by achieving high bounding rates without overestimating uncertainty.</li>
</ul>

<h3>Title: Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?</h3>
<ul>
<li><strong>Authors: </strong>Siqi Shen, Mehar Singh, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Rada Mihalcea</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13490">https://arxiv.org/abs/2507.13490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13490">https://arxiv.org/pdf/2507.13490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13490]] Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?(https://arxiv.org/abs/2507.13490)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>There has been extensive research on assessing the value orientation of Large Language Models (LLMs) as it can shape user experiences across demographic groups. However, several challenges remain. First, while the Multiple Choice Question (MCQ) setting has been shown to be vulnerable to perturbations, there is no systematic comparison of probing methods for value probing. Second, it is unclear to what extent the probed values capture in-context information and reflect models' preferences for real-world actions. In this paper, we evaluate the robustness and expressiveness of value representations across three widely used probing strategies. We use variations in prompts and options, showing that all methods exhibit large variances under input perturbations. We also introduce two tasks studying whether the values are responsive to demographic context, and how well they align with the models' behaviors in value-related scenarios. We show that the demographic context has little effect on the free-text generation, and the models' values only weakly correlate with their preference for value-based actions. Our work highlights the need for a more careful examination of LLM value probing and awareness of its limitations.</li>
</ul>

<h3>Title: Model-free Reinforcement Learning for Model-based Control: Towards Safe, Interpretable and Sample-efficient Agents</h3>
<ul>
<li><strong>Authors: </strong>Thomas Banker, Ali Mesbah</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13491">https://arxiv.org/abs/2507.13491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13491">https://arxiv.org/pdf/2507.13491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13491]] Model-free Reinforcement Learning for Model-based Control: Towards Safe, Interpretable and Sample-efficient Agents(https://arxiv.org/abs/2507.13491)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Training sophisticated agents for optimal decision-making under uncertainty has been key to the rapid development of modern autonomous systems across fields. Notably, model-free reinforcement learning (RL) has enabled decision-making agents to improve their performance directly through system interactions, with minimal prior knowledge about the system. Yet, model-free RL has generally relied on agents equipped with deep neural network function approximators, appealing to the networks' expressivity to capture the agent's policy and value function for complex systems. However, neural networks amplify the issues of sample inefficiency, unsafe learning, and limited interpretability in model-free RL. To this end, this work introduces model-based agents as a compelling alternative for control policy approximation, leveraging adaptable models of system dynamics, cost, and constraints for safe policy learning. These models can encode prior system knowledge to inform, constrain, and aid in explaining the agent's decisions, while deficiencies due to model mismatch can be remedied with model-free RL. We outline the benefits and challenges of learning model-based agents -- exemplified by model predictive control -- and detail the primary learning approaches: Bayesian optimization, policy search RL, and offline strategies, along with their respective strengths. While model-free RL has long been established, its interplay with model-based agents remains largely unexplored, motivating our perspective on their combined potentials for sample-efficient learning of safe and interpretable decision-making agents.</li>
</ul>

<h3>Title: PHASE: Passive Human Activity Simulation Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Steven Lamp, Jason D. Hiser, Anh Nguyen-Tuong, Jack W. Davidson</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13505">https://arxiv.org/abs/2507.13505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13505">https://arxiv.org/pdf/2507.13505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13505]] PHASE: Passive Human Activity Simulation Evaluation(https://arxiv.org/abs/2507.13505)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Cybersecurity simulation environments, such as cyber ranges, honeypots, and sandboxes, require realistic human behavior to be effective, yet no quantitative method exists to assess the behavioral fidelity of synthetic user personas. This paper presents PHASE (Passive Human Activity Simulation Evaluation), a machine learning framework that analyzes Zeek connection logs and distinguishes human from non-human activity with over 90\% accuracy. PHASE operates entirely passively, relying on standard network monitoring without any user-side instrumentation or visible signs of surveillance. All network activity used for machine learning is collected via a Zeek network appliance to avoid introducing unnecessary network traffic or artifacts that could disrupt the fidelity of the simulation environment. The paper also proposes a novel labeling approach that utilizes local DNS records to classify network traffic, thereby enabling machine learning analysis. Furthermore, we apply SHAP (SHapley Additive exPlanations) analysis to uncover temporal and behavioral signatures indicative of genuine human users. In a case study, we evaluate a synthetic user persona and identify distinct non-human patterns that undermine behavioral realism. Based on these insights, we develop a revised behavioral configuration that significantly improves the human-likeness of synthetic activity yielding a more realistic and effective synthetic user persona.</li>
</ul>

<h3>Title: Fake or Real: The Impostor Hunt in Texts for Space Operations</h3>
<ul>
<li><strong>Authors: </strong>Agata Kaczmarek (1), Dawid Płudowski (1), Piotr Wilczyński (1), Przemysław Biecek (1), Krzysztof Kotowski (2), Ramez Shendy (2), Jakub Nalepa (2 and 3), Artur Janicki (1), Evridiki Ntagiou (4) ((1) Warsaw University of Technology, (2) KP Labs, (3) Silesian University of Technology, (4) European Space Agency, European Space Operations Center)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13508">https://arxiv.org/abs/2507.13508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13508">https://arxiv.org/pdf/2507.13508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13508]] Fake or Real: The Impostor Hunt in Texts for Space Operations(https://arxiv.org/abs/2507.13508)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>The "Fake or Real" competition hosted on Kaggle (\href{this https URL}{this https URL}) is the second part of a series of follow-up competitions and hackathons related to the "Assurance for Space Domain AI Applications" project funded by the European Space Agency (\href{this https URL}{this https URL}). The competition idea is based on two real-life AI security threats identified within the project -- data poisoning and overreliance in Large Language Models. The task is to distinguish between the proper output from LLM and the output generated under malicious modification of the LLM. As this problem was not extensively researched, participants are required to develop new techniques to address this issue or adjust already existing ones to this problem's statement.</li>
</ul>

<h3>Title: SparseC-AFM: a deep learning method for fast and accurate characterization of MoS$_2$ with C-AFM</h3>
<ul>
<li><strong>Authors: </strong>Levi Harris, Md Jayed Hossain, Mufan Qiu, Ruichen Zhang, Pingchuan Ma, Tianlong Chen, Jiaqi Gu, Seth Ariel Tongay, Umberto Celano</a></li>
<li><strong>Subjects: </strong>cs.CV, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13527">https://arxiv.org/abs/2507.13527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13527">https://arxiv.org/pdf/2507.13527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13527]] SparseC-AFM: a deep learning method for fast and accurate characterization of MoS$_2$ with C-AFM(https://arxiv.org/abs/2507.13527)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>The increasing use of two-dimensional (2D) materials in nanoelectronics demands robust metrology techniques for electrical characterization, especially for large-scale production. While atomic force microscopy (AFM) techniques like conductive AFM (C-AFM) offer high accuracy, they suffer from slow data acquisition speeds due to the raster scanning process. To address this, we introduce SparseC-AFM, a deep learning model that rapidly and accurately reconstructs conductivity maps of 2D materials like MoS$_2$ from sparse C-AFM scans. Our approach is robust across various scanning modes, substrates, and experimental conditions. We report a comparison between (a) classic flow implementation, where a high pixel density C-AFM image (e.g., 15 minutes to collect) is manually parsed to extract relevant material parameters, and (b) our SparseC-AFM method, which achieves the same operation using data that requires substantially less acquisition time (e.g., under 5 minutes). SparseC-AFM enables efficient extraction of critical material parameters in MoS$_2$, including film coverage, defect density, and identification of crystalline island boundaries, edges, and cracks. We achieve over 11x reduction in acquisition time compared to manual extraction from a full-resolution C-AFM image. Moreover, we demonstrate that our model-predicted samples exhibit remarkably similar electrical properties to full-resolution data gathered using classic-flow scanning. This work represents a significant step toward translating AI-assisted 2D material characterization from laboratory research to industrial fabrication. Code and model weights are available at this http URL.</li>
</ul>

<h3>Title: Provable Low-Frequency Bias of In-Context Learning of Representations</h3>
<ul>
<li><strong>Authors: </strong>Yongyi Yang, Hidenori Tanaka, Wei Hu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13540">https://arxiv.org/abs/2507.13540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13540">https://arxiv.org/pdf/2507.13540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13540]] Provable Low-Frequency Bias of In-Context Learning of Representations(https://arxiv.org/abs/2507.13540)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) enables large language models (LLMs) to acquire new behaviors from the input sequence alone without any parameter updates. Recent studies have shown that ICL can surpass the original meaning learned in pretraining stage through internalizing the structure the data-generating process (DGP) of the prompt into the hidden representations. However, the mechanisms by which LLMs achieve this ability is left open. In this paper, we present the first rigorous explanation of such phenomena by introducing a unified framework of double convergence, where hidden representations converge both over context and across layers. This double convergence process leads to an implicit bias towards smooth (low-frequency) representations, which we prove analytically and verify empirically. Our theory explains several open empirical observations, including why learned representations exhibit globally structured but locally distorted geometry, and why their total energy decays without vanishing. Moreover, our theory predicts that ICL has an intrinsic robustness towards high-frequency noise, which we empirically confirm. These results provide new insights into the underlying mechanisms of ICL, and a theoretical foundation to study it that hopefully extends to more general data distributions and settings.</li>
</ul>

<h3>Title: Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk Stratification Using Echocardiography</h3>
<ul>
<li><strong>Authors: </strong>Beka Begiashvili, Carlos J. Fernandez-Candel, Matías Pérez Paredes</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13542">https://arxiv.org/abs/2507.13542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13542">https://arxiv.org/pdf/2507.13542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13542]] Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk Stratification Using Echocardiography(https://arxiv.org/abs/2507.13542)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Traditional echocardiographic parameters such as ejection fraction (EF) and global longitudinal strain (GLS) have limitations in the early detection of cardiac dysfunction. EF often remains normal despite underlying pathology, and GLS is influenced by load conditions and vendor variability. There is a growing need for reproducible, interpretable, and operator-independent parameters that capture subtle and global cardiac functional alterations. We introduce the Acoustic Index, a novel AI-derived echocardiographic parameter designed to quantify cardiac dysfunction from standard ultrasound views. The model combines Extended Dynamic Mode Decomposition (EDMD) based on Koopman operator theory with a hybrid neural network that incorporates clinical metadata. Spatiotemporal dynamics are extracted from echocardiographic sequences to identify coherent motion patterns. These are weighted via attention mechanisms and fused with clinical data using manifold learning, resulting in a continuous score from 0 (low risk) to 1 (high risk). In a prospective cohort of 736 patients, encompassing various cardiac pathologies and normal controls, the Acoustic Index achieved an area under the curve (AUC) of 0.89 in an independent test set. Cross-validation across five folds confirmed the robustness of the model, showing that both sensitivity and specificity exceeded 0.8 when evaluated on independent data. Threshold-based analysis demonstrated stable trade-offs between sensitivity and specificity, with optimal discrimination near this threshold. The Acoustic Index represents a physics-informed, interpretable AI biomarker for cardiac function. It shows promise as a scalable, vendor-independent tool for early detection, triage, and longitudinal monitoring. Future directions include external validation, longitudinal studies, and adaptation to disease-specific classifiers.</li>
</ul>

<h3>Title: A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Achref Ben Ammar, Mohamed Taha Bennani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13544">https://arxiv.org/abs/2507.13544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13544">https://arxiv.org/pdf/2507.13544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13544]] A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows(https://arxiv.org/abs/2507.13544)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The analysis of conversational dynamics has gained increasing importance with the rise of large language model-based systems, which interact with users across diverse contexts. In this work, we propose a novel computational framework for constructing conversational graphs that capture the flow and structure of loosely organized dialogues, referred to as quasi-patterned conversations. We introduce the Filter & Reconnect method, a novel graph simplification technique that minimizes noise while preserving semantic coherence and structural integrity of conversational graphs. Through comparative analysis, we demonstrate that the use of large language models combined with our graph simplification technique has resulted in semantic metric S increasing by a factor of 2.06 compared to previous approaches while simultaneously enforcing a tree-like structure with 0 {\delta}-hyperbolicity, ensuring optimal clarity in conversation modeling. This work provides a computational method for analyzing large-scale dialogue datasets, with practical applications related to monitoring automated systems such as chatbots, dialogue management tools, and user behavior analytics.</li>
</ul>

<h3>Title: $\nabla$NABLA: Neighborhood Adaptive Block-Level Attention</h3>
<ul>
<li><strong>Authors: </strong>Dmitrii Mikhailov, Aleksey Letunovskiy, Maria Kovaleva, Vladimir Arkhipkin, Vladimir Korviakov, Vladimir Polovnikov, Viacheslav Vasilev, Evelina Sidorova, Denis Dimitrov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13546">https://arxiv.org/abs/2507.13546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13546">https://arxiv.org/pdf/2507.13546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13546]] $\nabla$NABLA: Neighborhood Adaptive Block-Level Attention(https://arxiv.org/abs/2507.13546)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Recent progress in transformer-based architectures has demonstrated remarkable success in video generation tasks. However, the quadratic complexity of full attention mechanisms remains a critical bottleneck, particularly for high-resolution and long-duration video sequences. In this paper, we propose NABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that dynamically adapts to sparsity patterns in video diffusion transformers (DiTs). By leveraging block-wise attention with adaptive sparsity-driven threshold, NABLA reduces computational overhead while preserving generative quality. Our method does not require custom low-level operator design and can be seamlessly integrated with PyTorch's Flex Attention operator. Experiments demonstrate that NABLA achieves up to 2.7x faster training and inference compared to baseline almost without compromising quantitative metrics (CLIP score, VBench score, human evaluation score) and visual quality drop. The code and model weights are available here: this https URL</li>
</ul>

<h3>Title: Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder</h3>
<ul>
<li><strong>Authors: </strong>Feng Chen, Weizhe Xu, Changye Li, Serguei Pakhomov, Alex Cohen, Simran Bhola, Sandy Yin, Sunny X Tang, Michael Mackinley, Lena Palaniyappan, Dror Ben-Zeev, Trevor Cohen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13551">https://arxiv.org/abs/2507.13551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13551">https://arxiv.org/pdf/2507.13551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13551]] Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder(https://arxiv.org/abs/2507.13551)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Formal thought disorder (FTD), a hallmark of schizophrenia spectrum disorders, manifests as incoherent speech and poses challenges for clinical assessment. Traditional clinical rating scales, though validated, are resource-intensive and lack scalability. Automated speech analysis with automatic speech recognition (ASR) allows for objective quantification of linguistic and temporal features of speech, offering scalable alternatives. The use of utterance timestamps in ASR captures pause dynamics, which are thought to reflect the cognitive processes underlying speech production. However, the utility of integrating these ASR-derived features for assessing FTD severity requires further evaluation. This study integrates pause features with semantic coherence metrics across three datasets: naturalistic self-recorded diaries (AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream narratives (PsyCL, n = 43). We evaluated pause related features alongside established coherence measures, using support vector regression (SVR) to predict clinical FTD scores. Key findings demonstrate that pause features alone robustly predict the severity of FTD. Integrating pause features with semantic coherence metrics enhanced predictive performance compared to semantic-only models, with integration of independent models achieving correlations up to \r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best \r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance gains from semantic and pause features integration held consistently across all contexts, though the nature of pause patterns was dataset-dependent. These findings suggest that frameworks combining temporal and semantic analyses provide a roadmap for refining the assessment of disorganized speech and advance automated speech analysis in psychosis.</li>
</ul>

<h3>Title: A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Kirill Borodin, Nikita Vasiliev, Vasiliy Kudryavtsev, Maxim Maslov, Mikhail Gorodnichev, Oleg Rogov, Grach Mkrtchian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13563">https://arxiv.org/abs/2507.13563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13563">https://arxiv.org/pdf/2507.13563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13563]] A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models(https://arxiv.org/abs/2507.13563)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Russian speech synthesis presents distinctive challenges, including vowel reduction, consonant devoicing, variable stress patterns, homograph ambiguity, and unnatural intonation. This paper introduces Balalaika, a novel dataset comprising more than 2,000 hours of studio-quality Russian speech with comprehensive textual annotations, including punctuation and stress markings. Experimental results show that models trained on Balalaika significantly outperform those trained on existing datasets in both speech synthesis and enhancement tasks. We detail the dataset construction pipeline, annotation methodology, and results of comparative evaluations.</li>
</ul>

<h3>Title: LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning</h3>
<ul>
<li><strong>Authors: </strong>Kaihong Wang, Donghyun Kim, Margrit Betke</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13568">https://arxiv.org/abs/2507.13568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13568">https://arxiv.org/pdf/2507.13568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13568]] LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning(https://arxiv.org/abs/2507.13568)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Continual learning for vision-language models has achieved remarkable performance through synthetic replay, where samples are generated using Stable Diffusion to regularize during finetuning and retain knowledge. However, real-world downstream applications often exhibit domain-specific nuances and fine-grained semantics not captured by generators, causing synthetic-replay methods to produce misaligned samples that misguide finetuning and undermine retention of prior knowledge. In this work, we propose a LoRA-enhanced synthetic-replay framework that injects task-specific low-rank adapters into a frozen Stable Diffusion model, efficiently capturing each new task's unique visual and semantic patterns. Specifically, we introduce a two-stage, confidence-based sample selection: we first rank real task data by post-finetuning VLM confidence to focus LoRA finetuning on the most representative examples, then generate synthetic samples and again select them by confidence for distillation. Our approach integrates seamlessly with existing replay pipelines-simply swap in the adapted generator to boost replay fidelity. Extensive experiments on the Multi-domain Task Incremental Learning (MTIL) benchmark show that our method outperforms previous synthetic-replay techniques, achieving an optimal balance among plasticity, stability, and zero-shot capability. These results demonstrate the effectiveness of generator adaptation via LoRA for robust continual learning in VLMs.</li>
</ul>

<h3>Title: Change of Thought: Adaptive Test-Time Computation</h3>
<ul>
<li><strong>Authors: </strong>Mrinal Mathur, Mike Doan, Barak Pearlmutter, Sergey Plis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13569">https://arxiv.org/abs/2507.13569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13569">https://arxiv.org/pdf/2507.13569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13569]] Change of Thought: Adaptive Test-Time Computation(https://arxiv.org/abs/2507.13569)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers evaluated in a single, fixed-depth pass are provably limited in expressive power to the constant-depth circuit class TC0. Running a Transformer autoregressively removes that ceiling -- first in next-token prediction and, more recently, in chain-of-thought reasoning. Both regimes rely on feedback loops that decode internal states into tokens only to re-encode them in subsequent steps. While this "thinking aloud" mirrors human reasoning, biological brains iterate without externalising intermediate states as language. To boost the expressive power of encoder Transformers without resorting to token-level autoregression, we introduce the SELF-Transformer: an encoder layer that iteratively refines its own attention weights to a fixed point. Instead of producing -- in one pass -- the alignment matrix that remixes the input sequence, the SELF-Transformer iteratively updates that matrix internally, scaling test-time computation with input difficulty. This adaptivity yields up to 20\% accuracy gains on encoder-style benchmarks without increasing parameter count, demonstrating that input-adaptive alignment at test time offers substantial benefits for only a modest extra compute budget. Self-Transformers thus recover much of the expressive power of iterative reasoning while preserving the simplicity of pure encoder architectures.</li>
</ul>

<h3>Title: Apple Intelligence Foundation Language Models: Tech Report 2025</h3>
<ul>
<li><strong>Authors: </strong>Hanzhi Zhou, Erik Hornberger, Pengsheng Guo, Xiyou Zhou, Saiwen Wang, Xin Wang, Yifei He, Xuankai Chang, Rene Rauch, Louis D'hauwe, John Peebles, Alec Doane, Kohen Chia, Jenna Thibodeau, Zi-Yi Dou, Yuanyang Zhang, Ruoming Pang, Reed Li, Zhifeng Chen, Jeremy Warner, Zhaoyang Xu, Sophy Lee, David Mizrahi, Ramsey Tantawi, Chris Chaney, Kelsey Peterson, Jun Qin, Alex Dombrowski, Mira Chiang, Aiswarya Raghavan, Gerard Casamayor, Qibin Chen, Aonan Zhang, Nathalie Tran, Jianyu Wang, Hang Su, Thomas Voice, Alessandro Pappalardo, Brycen Wershing, Prasanth Yadla, Rui Li, Priyal Chhatrapati, Ismael Fernandez, Yusuf Goren, Xin Zheng, Forrest Huang, Tao Lei, Eray Yildiz, Alper Kokmen, Gokul Santhanam, Areeba Kamal, Kaan Elgin, Dian Ang Yap, Jeremy Liu, Peter Gray, Howard Xing, Kieran Liu, Matteo Ronchi, Moritz Schwarzer-Becker, Yun Zhu, Mandana Saebi, Jeremy Snow, David Griffiths, Guillaume Tartavel, Erin Feldman, Simon Lehnerer, Fernando Bermúdez-Medina, Hans Han, Joe Zhou, Xiaoyi Ren, Sujeeth Reddy, Zirui Wang, Tom Gunter, Albert Antony, Yuanzhi Li, John Dennison, Tony Sun, Yena Han, Yi Qin, Sam Davarnia, Jeffrey Bigham, Wayne Shan, Hannah Gillis Coleman, Guillaume Klein, Peng Liu, Muyang Yu, Jack Cackler, Yuan Gao, Crystal Xiao, Binazir Karimzadeh, Zhengdong Zhang, Felix Bai, Albin Madappally Jose, Feng Nan, Nazir Kamaldin, Dong Yin, Hans Hao, Yanchao Sun, Yi Hua, Charles Maalouf</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13575">https://arxiv.org/abs/2507.13575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13575">https://arxiv.org/pdf/2507.13575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13575]] Apple Intelligence Foundation Language Models: Tech Report 2025(https://arxiv.org/abs/2507.13575)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, transformer</a></li>
<li><strong>Abstract: </strong>We introduce two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services: i a 3B-parameter on-device model optimized for Apple silicon through architectural innovations such as KV-cache sharing and 2-bit quantization-aware training; and ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts PT-MoE transformer that combines track parallelism, mixture-of-experts sparse computation, and interleaved global-local attention to deliver high quality with competitive cost on Apple's Private Cloud Compute platform. Both models are trained on large-scale multilingual and multimodal datasets sourced via responsible web crawling, licensed corpora, and high-quality synthetic data, then further refined with supervised fine-tuning and reinforcement learning on a new asynchronous platform. The resulting models support several additional languages while understanding images and executing tool calls. In public benchmarks and human evaluations, both the server model and the on-device model match or surpass comparably sized open baselines. A new Swift-centric Foundation Models framework exposes guided generation, constrained tool calling, and LoRA adapter fine-tuning, allowing developers to integrate these capabilities with a few lines of code. The latest advancements in Apple Intelligence models are grounded in our Responsible AI approach with safeguards like content filtering and locale-specific evaluation, as well as our commitment to protecting our users' privacy with innovations like Private Cloud Compute.</li>
</ul>

<h3>Title: Learning Pluralistic User Preferences through Reinforcement Learning Fine-tuned Summaries</h3>
<ul>
<li><strong>Authors: </strong>Hyunji Nam, Yanming Wan, Mickel Liu, Jianxun Lian, Natasha Jaques</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13579">https://arxiv.org/abs/2507.13579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13579">https://arxiv.org/pdf/2507.13579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13579]] Learning Pluralistic User Preferences through Reinforcement Learning Fine-tuned Summaries(https://arxiv.org/abs/2507.13579)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As everyday use cases of large language model (LLM) AI assistants have expanded, it is becoming increasingly important to personalize responses to align to different users' preferences and goals. While reinforcement learning from human feedback (RLHF) is effective at improving LLMs to be generally more helpful and fluent, it does not account for variability across users, as it models the entire user population with a single reward model. We present a novel framework, Preference Learning Using Summarization (PLUS), that learns text-based summaries of each user's preferences, characteristics, and past conversations. These summaries condition the reward model, enabling it to make personalized predictions about the types of responses valued by each user. We train the user-summarization model with reinforcement learning, and update the reward model simultaneously, creating an online co-adaptation loop. We show that in contrast with prior personalized RLHF techniques or with in-context learning of user information, summaries produced by PLUS capture meaningful aspects of a user's preferences. Across different pluralistic user datasets, we show that our method is robust to new users and diverse conversation topics. Additionally, we demonstrate that the textual summaries generated about users can be transferred for zero-shot personalization of stronger, proprietary models like GPT-4. The resulting user summaries are not only concise and portable, they are easy for users to interpret and modify, allowing for more transparency and user control in LLM alignment.</li>
</ul>

<h3>Title: FuSeFL: Fully Secure and Scalable Cross-Silo Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Sahar Ghoflsaz Ghinani, Elaheh Sadredini</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13591">https://arxiv.org/abs/2507.13591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13591">https://arxiv.org/pdf/2507.13591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13591]] FuSeFL: Fully Secure and Scalable Cross-Silo Federated Learning(https://arxiv.org/abs/2507.13591)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, membership infer, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative model training without centralizing client data, making it attractive for privacy-sensitive domains. While existing approaches employ cryptographic techniques such as homomorphic encryption, differential privacy, or secure multiparty computation to mitigate inference attacks-including model inversion, membership inference, and gradient leakage-they often suffer from high computational, communication, or memory overheads. Moreover, many methods overlook the confidentiality of the global model itself, which may be proprietary and sensitive. These challenges limit the practicality of secure FL, especially in cross-silo deployments involving large datasets and strict compliance requirements. We present FuSeFL, a fully secure and scalable FL scheme designed for cross-silo settings. FuSeFL decentralizes training across client pairs using lightweight secure multiparty computation (MPC), while confining the server's role to secure aggregation. This design eliminates server bottlenecks, avoids data offloading, and preserves full confidentiality of data, model, and updates throughout training. FuSeFL defends against inference threats, achieves up to 95% lower communication latency and 50% lower server memory usage, and improves accuracy over prior secure FL solutions, demonstrating strong security and efficiency at scale.</li>
</ul>

<h3>Title: GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention</h3>
<ul>
<li><strong>Authors: </strong>Amro Abdalla, Ismail Shaheen, Dan DeGenaro, Rupayan Mallick, Bogdan Raita, Sarah Adel Bargal</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13598">https://arxiv.org/abs/2507.13598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13598">https://arxiv.org/pdf/2507.13598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13598]] GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention(https://arxiv.org/abs/2507.13598)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present \textbf{GIFT}: a \textbf{G}radient-aware \textbf{I}mmunization technique to defend diffusion models against malicious \textbf{F}ine-\textbf{T}uning while preserving their ability to generate safe content. Existing safety mechanisms like safety checkers are easily bypassed, and concept erasure methods fail under adversarial fine-tuning. GIFT addresses this by framing immunization as a bi-level optimization problem: the upper-level objective degrades the model's ability to represent harmful concepts using representation noising and maximization, while the lower-level objective preserves performance on safe data. GIFT achieves robust resistance to malicious fine-tuning while maintaining safe generative quality. Experimental results show that our method significantly impairs the model's ability to re-learn harmful concepts while maintaining performance on safe content, offering a promising direction for creating inherently safer generative models resistant to adversarial fine-tuning attacks. {\small\textbf{\textcolor{red}{Warning: This paper contains NSFW content. Reader discretion is advised.}}}</li>
</ul>

<h3>Title: Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Chengxu Liu, Lu Qi, Jinshan Pan, Xueming Qian, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13599">https://arxiv.org/abs/2507.13599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13599">https://arxiv.org/pdf/2507.13599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13599]] Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model(https://arxiv.org/abs/2507.13599)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Since acquiring large amounts of realistic blurry-sharp image pairs is difficult and expensive, learning blind image deblurring from unpaired data is a more practical and promising solution. Unfortunately, dominant approaches rely heavily on adversarial learning to bridge the gap from blurry domains to sharp domains, ignoring the complex and unpredictable nature of real-world blur patterns. In this paper, we propose a novel diffusion model (DM)-based framework, dubbed \ours, for image deblurring by learning spatially varying texture prior from unpaired data. In particular, \ours performs DM to generate the prior knowledge that aids in recovering the textures of blurry images. To implement this, we propose a Texture Prior Encoder (TPE) that introduces a memory mechanism to represent the image textures and provides supervision for DM training. To fully exploit the generated texture priors, we present the Texture Transfer Transformer layer (TTformer), in which a novel Filter-Modulated Multi-head Self-Attention (FM-MSA) efficiently removes spatially varying blurring through adaptive filtering. Furthermore, we implement a wavelet-based adversarial loss to preserve high-frequency texture details. Extensive evaluations show that \ours provides a promising unsupervised deblurring solution and outperforms SOTA methods in widely-used benchmarks.</li>
</ul>

<h3>Title: Efficient Burst Super-Resolution with One-step Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Kento Kawai, Takeru Oba, Kyotaro Tokoro, Kazutoshi Akita, Norimichi Ukita</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13607">https://arxiv.org/abs/2507.13607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13607">https://arxiv.org/pdf/2507.13607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13607]] Efficient Burst Super-Resolution with One-step Diffusion(https://arxiv.org/abs/2507.13607)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While burst Low-Resolution (LR) images are useful for improving their Super Resolution (SR) image compared to a single LR image, prior burst SR methods are trained in a deterministic manner, which produces a blurry SR image. Since such blurry images are perceptually degraded, we aim to reconstruct sharp and high-fidelity SR images by a diffusion model. Our method improves the efficiency of the diffusion model with a stochastic sampler with a high-order ODE as well as one-step diffusion using knowledge distillation. Our experimental results demonstrate that our method can reduce the runtime to 1.6 % of its baseline while maintaining the SR quality measured based on image distortion and perceptual quality.</li>
</ul>

<h3>Title: Off-Policy Evaluation and Learning for Matching Markets</h3>
<ul>
<li><strong>Authors: </strong>Yudai Hayashi, Shuhei Goda, Yuta Saito</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13608">https://arxiv.org/abs/2507.13608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13608">https://arxiv.org/pdf/2507.13608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13608]] Off-Policy Evaluation and Learning for Matching Markets(https://arxiv.org/abs/2507.13608)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Matching users based on mutual preferences is a fundamental aspect of services driven by reciprocal recommendations, such as job search and dating applications. Although A/B tests remain the gold standard for evaluating new policies in recommender systems for matching markets, it is costly and impractical for frequent policy updates. Off-Policy Evaluation (OPE) thus plays a crucial role by enabling the evaluation of recommendation policies using only offline logged data naturally collected on the platform. However, unlike conventional recommendation settings, the large scale and bidirectional nature of user interactions in matching platforms introduce variance issues and exacerbate reward sparsity, making standard OPE methods unreliable. To address these challenges and facilitate effective offline evaluation, we propose novel OPE estimators, \textit{DiPS} and \textit{DPR}, specifically designed for matching markets. Our methods combine elements of the Direct Method (DM), Inverse Propensity Score (IPS), and Doubly Robust (DR) estimators while incorporating intermediate labels, such as initial engagement signals, to achieve better bias-variance control in matching markets. Theoretically, we derive the bias and variance of the proposed estimators and demonstrate their advantages over conventional methods. Furthermore, we show that these estimators can be seamlessly extended to offline policy learning methods for improving recommendation policies for making more matches. We empirically evaluate our methods through experiments on both synthetic data and A/B testing logs from a real job-matching platform. The empirical results highlight the superiority of our approach over existing methods in off-policy evaluation and learning tasks for a variety of configurations.</li>
</ul>

<h3>Title: CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks</h3>
<ul>
<li><strong>Authors: </strong>Yanan Wang, Julio Vizcarra, Zhi Li, Hao Niu, Mori Kurokawa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13609">https://arxiv.org/abs/2507.13609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13609">https://arxiv.org/pdf/2507.13609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13609]] CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks(https://arxiv.org/abs/2507.13609)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Despite recent progress in video large language models (VideoLLMs), a key open challenge remains: how to equip models with chain-of-thought (CoT) reasoning abilities grounded in fine-grained object-level video understanding. Existing instruction-tuned models, such as the Qwen and LLaVA series, are trained on high-level video-text pairs, often lacking structured annotations necessary for compositional, step-by-step reasoning. We propose CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks, a new framework that decomposes complex video questions of existing datasets (e.g., NeXT-QA, STAR) into four entity-level foundational tasks: frame localization, entity tracking, spatial and temporal relation extraction. By embedding these intermediate CoT-style reasoning steps into the input, CoTasks enables models to explicitly perform object-centric spatiotemporal reasoning. Experiments on the NeXT-QA benchmark show that CoTasks significantly enhance inference performance: LLaVA-video-7B improves by +3.3 points in average GPT-4 evaluation score, and Qwen2.5-VL-3B gains +17.4, with large boosts in causal (+14.6), temporal (+10.9), and descriptive (+48.1) subcategories. These results demonstrate the effectiveness of CoTasks as a structured CoT-style supervision framework for improving compositional video reasoning.</li>
</ul>

<h3>Title: Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sergio E. Zanotto, Segun Aroyehun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13614">https://arxiv.org/abs/2507.13614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13614">https://arxiv.org/pdf/2507.13614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13614]] Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models(https://arxiv.org/abs/2507.13614)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancements in large language models (LLMs) have significantly improved their ability to generate natural language, making texts generated by LLMs increasingly indistinguishable from human-written texts. While recent research has primarily focused on using LLMs to classify text as either human-written and machine-generated texts, our study focus on characterizing these texts using a set of linguistic features across different linguistic levels such as morphology, syntax, and semantics. We select a dataset of human-written and machine-generated texts spanning 8 domains and produced by 11 different LLMs. We calculate different linguistic features such as dependency length and emotionality and we use them for characterizing human-written and machine-generated texts along with different sampling strategies, repetition controls and model release date. Our statistical analysis reveals that human-written texts tend to exhibit simpler syntactic structures and more diverse semantic content. Furthermore, we calculate the variability of our set of features across models and domains. Both human and machine texts show stylistic diversity across domains, with humans displaying greater variation in our features. Finally, we apply style embeddings to further test variability among human-written and machine-generated texts. Notably, newer models output text that is similarly variable, pointing to an homogenization of machine-generated texts.</li>
</ul>

<h3>Title: Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters</h3>
<ul>
<li><strong>Authors: </strong>Shanbo Cheng, Yu Bao, Qian Cao, Luyang Huang, Liyan Kang, Zhicheng Liu, Yu Lu, Wenhao Zhu, Zhichao Huang, Tao Li, Sitong Liu, Ningxin Peng, Shuaijie She, Lu Xu, Nuo Xu, Sen Yang, Runsheng Yu, Yiming Yu, Liehao Zou, Hang Li, Lu Lu, Yuxuan Wang, Yonghui Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13618">https://arxiv.org/abs/2507.13618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13618">https://arxiv.org/pdf/2507.13618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13618]] Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters(https://arxiv.org/abs/2507.13618)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multilingual translation stands as a challenging task for large language models (LLMs) to handle intricate language patterns and stilted translations that arise in automated translations. In this paper, we introduce Seed-X, a family of open-source LLMs comprising instruct and reasoning models, pushing the limits of translation capability with 7B parameter size. The base model is pre-trained on a diverse, high-quality dataset encompassing both monolingual and bilingual content across 28 languages, harnessing the full potential of multilingual data. The instruct model is then finetuned to translate by Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement learning (RL) to achieve better generalization across diverse language pairs. Seed-X achieves performance comparable to leading closed-source models, including Gemini-2.5 and GPT-4o, across 28 languages, and significantly outperforms larger open-source models in both automatic metrics and human evaluations. We share the best practices through our optimization process, and make the parameter public available for advancing translation research and applications.</li>
</ul>

<h3>Title: Tri-Learn Graph Fusion Network for Attributed Graph Clustering</h3>
<ul>
<li><strong>Authors: </strong>Binxiong Li, Yuefei Wang, Xu Xiang, Xue Li, Binyu Zhao, Heyang Gao, Qinyu Zhao, Xi Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13620">https://arxiv.org/abs/2507.13620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13620">https://arxiv.org/pdf/2507.13620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13620]] Tri-Learn Graph Fusion Network for Attributed Graph Clustering(https://arxiv.org/abs/2507.13620)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>In recent years, models based on Graph Convolutional Networks (GCN) have made significant strides in the field of graph data analysis. However, challenges such as over-smoothing and over-compression remain when handling large-scale and complex graph datasets, leading to a decline in clustering quality. Although the Graph Transformer architecture has mitigated some of these issues, its performance is still limited when processing heterogeneous graph data. To address these challenges, this study proposes a novel deep clustering framework that comprising GCN, Autoencoder (AE), and Graph Transformer, termed the Tri-Learn Graph Fusion Network (Tri-GFN). This framework enhances the differentiation and consistency of global and local information through a unique tri-learning mechanism and feature fusion enhancement strategy. The framework integrates GCN, AE, and Graph Transformer modules. These components are meticulously fused by a triple-channel enhancement module, which maximizes the use of both node attributes and topological structures, ensuring robust clustering representation. The tri-learning mechanism allows mutual learning among these modules, while the feature fusion strategy enables the model to capture complex relationships, yielding highly discriminative representations for graph clustering. It surpasses many state-of-the-art methods, achieving an accuracy improvement of approximately 0.87% on the ACM dataset, 14.14 % on the Reuters dataset, and 7.58 % on the USPS dataset. Due to its outstanding performance on the Reuters dataset, Tri-GFN can be applied to automatic news classification, topic retrieval, and related fields.</li>
</ul>

<h3>Title: FedSkipTwin: Digital-Twin-Guided Client Skipping for Communication-Efficient Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Daniel Commey, Kamel Abbad, Garth V. Crosby, Lyes Khoukhi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13624">https://arxiv.org/abs/2507.13624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13624">https://arxiv.org/pdf/2507.13624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13624]] FedSkipTwin: Digital-Twin-Guided Client Skipping for Communication-Efficient Federated Learning(https://arxiv.org/abs/2507.13624)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Communication overhead remains a primary bottleneck in federated learning (FL), particularly for applications involving mobile and IoT devices with constrained bandwidth. This work introduces FedSkipTwin, a novel client-skipping algorithm driven by lightweight, server-side digital twins. Each twin, implemented as a simple LSTM, observes a client's historical sequence of gradient norms to forecast both the magnitude and the epistemic uncertainty of its next update. The server leverages these predictions, requesting communication only when either value exceeds a predefined threshold; otherwise, it instructs the client to skip the round, thereby saving bandwidth. Experiments are conducted on the UCI-HAR and MNIST datasets with 10 clients under a non-IID data distribution. The results demonstrate that FedSkipTwin reduces total communication by 12-15.5% across 20 rounds while simultaneously improving final model accuracy by up to 0.5 percentage points compared to the standard FedAvg algorithm. These findings establish that prediction-guided skipping is a practical and effective strategy for resource-aware FL in bandwidth-constrained edge environments.</li>
</ul>

<h3>Title: Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Masahiro Ogawa, Qi An, Atsushi Yamashita</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13628">https://arxiv.org/abs/2507.13628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13628">https://arxiv.org/pdf/2507.13628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13628]] Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation(https://arxiv.org/abs/2507.13628)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Separating moving and static objects from a moving camera viewpoint is essential for 3D reconstruction, autonomous navigation, and scene understanding in robotics. Existing approaches often rely primarily on optical flow, which struggles to detect moving objects in complex, structured scenes involving camera motion. To address this limitation, we propose Focus of Expansion Likelihood and Segmentation (FoELS), a method based on the core idea of integrating both optical flow and texture information. FoELS computes the focus of expansion (FoE) from optical flow and derives an initial motion likelihood from the outliers of the FoE computation. This likelihood is then fused with a segmentation-based prior to estimate the final moving probability. The method effectively handles challenges including complex structured scenes, rotational camera motion, and parallel motion. Comprehensive evaluations on the DAVIS 2016 dataset and real-world traffic videos demonstrate its effectiveness and state-of-the-art performance.</li>
</ul>

<h3>Title: Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques</h3>
<ul>
<li><strong>Authors: </strong>Niveen O. Jaffal, Mohammed Alkhanafseh, David Mohaisen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13629">https://arxiv.org/abs/2507.13629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13629">https://arxiv.org/pdf/2507.13629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13629]] Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques(https://arxiv.org/abs/2507.13629)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are transforming cybersecurity by enabling intelligent, adaptive, and automated approaches to threat detection, vulnerability assessment, and incident response. With their advanced language understanding and contextual reasoning, LLMs surpass traditional methods in tackling challenges across domains such as IoT, blockchain, and hardware security. This survey provides a comprehensive overview of LLM applications in cybersecurity, focusing on two core areas: (1) the integration of LLMs into key cybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along with mitigation strategies. By synthesizing recent advancements and identifying key limitations, this work offers practical insights and strategic recommendations for leveraging LLMs to build secure, scalable, and future-ready cyber defense systems.</li>
</ul>

<h3>Title: A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design</h3>
<ul>
<li><strong>Authors: </strong>Nimisha Ghosh, Daniele Santoni, Debaleena Nawn, Eleonora Ottaviani, Giovanni Felici</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13646">https://arxiv.org/abs/2507.13646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13646">https://arxiv.org/pdf/2507.13646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13646]] A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design(https://arxiv.org/abs/2507.13646)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The impact of Transformer-based language models has been unprecedented in Natural Language Processing (NLP). The success of such models has also led to their adoption in other fields including bioinformatics. Taking this into account, this paper discusses recent advances in Transformer-based models for protein sequence analysis and design. In this review, we have discussed and analysed a significant number of works pertaining to such applications. These applications encompass gene ontology, functional and structural protein identification, generation of de novo proteins and binding of proteins. We attempt to shed light on the strength and weaknesses of the discussed works to provide a comprehensive insight to readers. Finally, we highlight shortcomings in existing research and explore potential avenues for future developments. We believe that this review will help researchers working in this field to have an overall idea of the state of the art in this field, and to orient their future studies.</li>
</ul>

<h3>Title: CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer</h3>
<ul>
<li><strong>Authors: </strong>Teerapong Panboonyuen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13655">https://arxiv.org/abs/2507.13655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13655">https://arxiv.org/pdf/2507.13655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13655]] CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer(https://arxiv.org/abs/2507.13655)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Integrating large language models into specialized domains like healthcare presents unique challenges, including domain adaptation and limited labeled data. We introduce CU-ICU, a method for customizing unsupervised instruction-finetuned language models for ICU datasets by leveraging the Text-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse fine-tuning approach that combines few-shot prompting with selective parameter updates, enabling efficient adaptation with minimal supervision. Our evaluation across critical ICU tasks--early sepsis detection, mortality prediction, and clinical note generation--demonstrates that CU-ICU consistently improves predictive accuracy and interpretability over standard fine-tuning methods. Notably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and a 20% enhancement in generating clinically relevant explanations while updating fewer than 1% of model parameters in its most efficient configuration. These results establish CU-ICU as a scalable, low-overhead solution for delivering accurate and interpretable clinical decision support in real-world ICU environments.</li>
</ul>

<h3>Title: When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework</h3>
<ul>
<li><strong>Authors: </strong>Xiao Wang, Qian Zhu, Shujuan Wu, Bo Jiang, Shiliang Zhang, Yaowei Wang, Yonghong Tian, Bin Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13659">https://arxiv.org/abs/2507.13659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13659">https://arxiv.org/pdf/2507.13659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13659]] When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework(https://arxiv.org/abs/2507.13659)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Recent researchers have proposed using event cameras for person re-identification (ReID) due to their promising performance and better balance in terms of privacy protection, event camera-based person ReID has attracted significant attention. Currently, mainstream event-based person ReID algorithms primarily focus on fusing visible light and event stream, as well as preserving privacy. Although significant progress has been made, these methods are typically trained and evaluated on small-scale or simulated event camera datasets, making it difficult to assess their real identification performance and generalization ability. To address the issue of data scarcity, this paper introduces a large-scale RGB-event based person ReID dataset, called EvReID. The dataset contains 118,988 image pairs and covers 1200 pedestrian identities, with data collected across multiple seasons, scenes, and lighting conditions. We also evaluate 15 state-of-the-art person ReID algorithms, laying a solid foundation for future research in terms of both data and benchmarking. Based on our newly constructed dataset, this paper further proposes a pedestrian attribute-guided contrastive learning framework to enhance feature learning for person re-identification, termed TriPro-ReID. This framework not only effectively explores the visual features from both RGB frames and event streams, but also fully utilizes pedestrian attributes as mid-level semantic features. Extensive experiments on the EvReID dataset and MARS datasets fully validated the effectiveness of our proposed RGB-Event person ReID framework. The benchmark dataset and source code will be released on this https URL</li>
</ul>

<h3>Title: Global Modeling Matters: A Fast, Lightweight and Effective Baseline for Efficient Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Jiang, Ning Gao, Hongkun Dou, Xiuhui Zhang, Xiaoqing Zhong, Yue Deng, Hongjue Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13663">https://arxiv.org/abs/2507.13663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13663">https://arxiv.org/pdf/2507.13663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13663]] Global Modeling Matters: A Fast, Lightweight and Effective Baseline for Efficient Image Restoration(https://arxiv.org/abs/2507.13663)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Natural image quality is often degraded by adverse weather conditions, significantly impairing the performance of downstream tasks. Image restoration has emerged as a core solution to this challenge and has been widely discussed in the literature. Although recent transformer-based approaches have made remarkable progress in image restoration, their increasing system complexity poses significant challenges for real-time processing, particularly in real-world deployment scenarios. To this end, most existing methods attempt to simplify the self-attention mechanism, such as by channel self-attention or state space model. However, these methods primarily focus on network architecture while neglecting the inherent characteristics of image restoration itself. In this context, we explore a pyramid Wavelet-Fourier iterative pipeline to demonstrate the potential of Wavelet-Fourier processing for image restoration. Inspired by the above findings, we propose a novel and efficient restoration baseline, named Pyramid Wavelet-Fourier Network (PW-FNet). Specifically, PW-FNet features two key design principles: 1) at the inter-block level, integrates a pyramid wavelet-based multi-input multi-output structure to achieve multi-scale and multi-frequency bands decomposition; and 2) at the intra-block level, incorporates Fourier transforms as an efficient alternative to self-attention mechanisms, effectively reducing computational complexity while preserving global modeling capability. Extensive experiments on tasks such as image deraining, raindrop removal, image super-resolution, motion deblurring, image dehazing, image desnowing and underwater/low-light enhancement demonstrate that PW-FNet not only surpasses state-of-the-art methods in restoration quality but also achieves superior efficiency, with significantly reduced parameter size, computational cost and inference time.</li>
</ul>

<h3>Title: KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Woo-Chan Kim, Ji-Hoon Park, Seong-Whan Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13666">https://arxiv.org/abs/2507.13666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13666">https://arxiv.org/pdf/2507.13666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13666]] KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs(https://arxiv.org/abs/2507.13666)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated state-of-the-art performance across a wide range of natural language processing tasks. However, high-performing models are typically accessible only via APIs, incurring substantial inference costs. Cascade methods address this by initially employing a cheaper model and escalating to a stronger one only when necessary. Nevertheless, existing cascade approaches struggle to select a reliable representative response and assess the overall reliability of free-form outputs, as they rely on exact text matching. To overcome these limitations, we propose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient free-form text generation. KiC identifies the most representative answer among multiple outputs from a weaker model and evaluates the semantic alignment of other responses with it. Based on the degree of alignment, KiC determines whether to accept the weaker model's output or escalate to a stronger model. Experiments on three free-form text generation benchmarks show that KiC achieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81 percent on average, and even outperforms GPT-4 in a specific benchmark.</li>
</ul>

<h3>Title: MaskHOI: Robust 3D Hand-Object Interaction Estimation via Masked Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Yuechen Xie, Haobo Jiang, Jian Yang, Yigong Zhang, Jin Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13673">https://arxiv.org/abs/2507.13673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13673">https://arxiv.org/pdf/2507.13673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13673]] MaskHOI: Robust 3D Hand-Object Interaction Estimation via Masked Pre-training(https://arxiv.org/abs/2507.13673)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In 3D hand-object interaction (HOI) tasks, estimating precise joint poses of hands and objects from monocular RGB input remains highly challenging due to the inherent geometric ambiguity of RGB images and the severe mutual occlusions that occur during this http URL address these challenges, we propose MaskHOI, a novel Masked Autoencoder (MAE)-driven pretraining framework for enhanced HOI pose estimation. Our core idea is to leverage the masking-then-reconstruction strategy of MAE to encourage the feature encoder to infer missing spatial and structural information, thereby facilitating geometric-aware and occlusion-robust representation learning. Specifically, based on our observation that human hands exhibit far greater geometric complexity than rigid objects, conventional uniform masking fails to effectively guide the reconstruction of fine-grained hand structures. To overcome this limitation, we introduce a Region-specific Mask Ratio Allocation, primarily comprising the region-specific masking assignment and the skeleton-driven hand masking guidance. The former adaptively assigns lower masking ratios to hand regions than to rigid objects, balancing their feature learning difficulty, while the latter prioritizes masking critical hand parts (e.g., fingertips or entire fingers) to realistically simulate occlusion patterns in real-world interactions. Furthermore, to enhance the geometric awareness of the pretrained encoder, we introduce a novel Masked Signed Distance Field (SDF)-driven multimodal learning mechanism. Through the self-masking 3D SDF prediction, the learned encoder is able to perceive the global geometric structure of hands and objects beyond the 2D image plane, overcoming the inherent limitations of monocular input and alleviating self-occlusion issues. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art approaches.</li>
</ul>

<h3>Title: HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors</h3>
<ul>
<li><strong>Authors: </strong>Chuheng Wei, Ziye Qin, Walter Zimmer, Guoyuan Wu, Matthew J. Barth</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13677">https://arxiv.org/abs/2507.13677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13677">https://arxiv.org/pdf/2507.13677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13677]] HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors(https://arxiv.org/abs/2507.13677)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Real-world Vehicle-to-Everything (V2X) cooperative perception systems often operate under heterogeneous sensor configurations due to cost constraints and deployment variability across vehicles and infrastructure. This heterogeneity poses significant challenges for feature fusion and perception reliability. To address these issues, we propose HeCoFuse, a unified framework designed for cooperative perception across mixed sensor setups where nodes may carry Cameras (C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that adaptively weights features through a combination of channel-wise and spatial attention, HeCoFuse can tackle critical challenges such as cross-modality feature misalignment and imbalanced representation quality. In addition, an adaptive spatial resolution adjustment module is employed to balance computational cost and fusion effectiveness. To enhance robustness across different configurations, we further implement a cooperative learning strategy that dynamically adjusts fusion type based on available modalities. Experiments on the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22% 3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D baseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC scenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine heterogeneous sensor configurations. These results, validated by our first-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the current state-of-the-art on TUM-Traf V2X dataset while demonstrating robust performance across diverse sensor deployments.</li>
</ul>

<h3>Title: LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Haoyang Li, Zhanchao Xu, Yiming Li, Xuejia Chen, Darian Li, Anxin Tian, Qingfa Xiao, Cheng Deng, Jun Wang, Qing Li, Lei Chen, Mingxuan Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13681">https://arxiv.org/abs/2507.13681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13681">https://arxiv.org/pdf/2507.13681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13681]] LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues(https://arxiv.org/abs/2507.13681)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-turn dialogues are essential in many real-world applications of large language models, such as chatbots and virtual assistants. As conversation histories become longer, existing large language models face increasing computational and memory challenges, which hinder their ability to provide efficient and responsive interactions. Most current acceleration methods either compress the context or optimize key value caching, but they often rely on fixed or position-based heuristics that do not adapt well to the dynamic and unpredictable patterns found in actual multi-turn conversations. In this paper, we present LoopServe, an adaptive dual-phase inference acceleration framework for large language models in multi-turn dialogues. LoopServe introduces two main innovations. First, it performs online sparsification during the prefilling phase by dynamically selecting the most important parts of the attention matrix for each new input. Second, it uses progressive key value compression during decoding by adaptively maintaining a relevant and efficient cache based on the most recently generated output tokens. We also propose a \href{this https URL}{new benchmark} with eleven multi-turn datasets that reflect realistic query positions and conversational dependencies. Extensive experiments demonstrate that LoopServe consistently achieves superior effectiveness compared to existing baselines and significantly accelerates LLM inference across a wide range of long-context dialogue tasks.</li>
</ul>

<h3>Title: Kolmogorov-Arnold Networks-based GRU and LSTM for Loan Default Early Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yue Yang, Zihan Su, Ying Zhang, Chang Chuan Goh, Yuxiang Lin, Anthony Graham Bellotti, Boon Giin Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13685">https://arxiv.org/abs/2507.13685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13685">https://arxiv.org/pdf/2507.13685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13685]] Kolmogorov-Arnold Networks-based GRU and LSTM for Loan Default Early Prediction(https://arxiv.org/abs/2507.13685)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This study addresses a critical challenge in time series anomaly detection: enhancing the predictive capability of loan default models more than three months in advance to enable early identification of default events, helping financial institutions implement preventive measures before risk events materialize. Existing methods have significant drawbacks, such as their lack of accuracy in early predictions and their dependence on training and testing within the same year and specific time frames. These issues limit their practical use, particularly with out-of-time data. To address these, the study introduces two innovative architectures, GRU-KAN and LSTM-KAN, which merge Kolmogorov-Arnold Networks (KAN) with Gated Recurrent Units (GRU) and Long Short-Term Memory (LSTM) networks. The proposed models were evaluated against the baseline models (LSTM, GRU, LSTM-Attention, and LSTM-Transformer) in terms of accuracy, precision, recall, F1 and AUC in different lengths of feature window, sample sizes, and early prediction intervals. The results demonstrate that the proposed model achieves a prediction accuracy of over 92% three months in advance and over 88% eight months in advance, significantly outperforming existing baselines.</li>
</ul>

<h3>Title: TopicAttack: An Indirect Prompt Injection Attack via Topic Transition</h3>
<ul>
<li><strong>Authors: </strong>Yulin Chen, Haoran Li, Yuexin Li, Yue Liu, Yangqiu Song, Bryan Hooi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13686">https://arxiv.org/abs/2507.13686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13686">https://arxiv.org/pdf/2507.13686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13686]] TopicAttack: An Indirect Prompt Injection Attack via Topic Transition(https://arxiv.org/abs/2507.13686)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable performance across a range of NLP tasks. However, their strong instruction-following capabilities and inability to distinguish instructions from data content make them vulnerable to indirect prompt injection attacks. In such attacks, instructions with malicious purposes are injected into external data sources, such as web documents. When LLMs retrieve this injected data through tools, such as a search engine and execute the injected instructions, they provide misled responses. Recent attack methods have demonstrated potential, but their abrupt instruction injection often undermines their effectiveness. Motivated by the limitations of existing attack methods, we propose TopicAttack, which prompts the LLM to generate a fabricated conversational transition prompt that gradually shifts the topic toward the injected instruction, making the injection smoother and enhancing the plausibility and success of the attack. Through comprehensive experiments, TopicAttack achieves state-of-the-art performance, with an attack success rate (ASR) over 90\% in most cases, even when various defense methods are applied. We further analyze its effectiveness by examining attention scores. We find that a higher injected-to-original attention ratio leads to a greater success probability, and our method achieves a much higher ratio than the baseline methods.</li>
</ul>

<h3>Title: Gaussian kernel-based motion measurement</h3>
<ul>
<li><strong>Authors: </strong>Hongyi Liu, Haifeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13693">https://arxiv.org/abs/2507.13693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13693">https://arxiv.org/pdf/2507.13693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13693]] Gaussian kernel-based motion measurement(https://arxiv.org/abs/2507.13693)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The growing demand for structural health monitoring has driven increasing interest in high-precision motion measurement, as structural information derived from extracted motions can effectively reflect the current condition of the structure. Among various motion measurement techniques, vision-based methods stand out due to their low cost, easy installation, and large-scale measurement. However, when it comes to sub-pixel-level motion measurement, current vision-based methods either lack sufficient accuracy or require extensive manual parameter tuning (e.g., pyramid layers, target pixels, and filter parameters) to reach good precision. To address this issue, we developed a novel Gaussian kernel-based motion measurement method, which can extract the motion between different frames via tracking the location of Gaussian kernels. The motion consistency, which fits practical structural conditions, and a super-resolution constraint, are introduced to increase accuracy and robustness of our method. Numerical and experimental validations show that it can consistently reach high accuracy without customized parameter setup for different test samples.</li>
</ul>

<h3>Title: Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations</h3>
<ul>
<li><strong>Authors: </strong>Cedric Waterschoot, Nava Tintarev, Francesco Barile</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13705">https://arxiv.org/abs/2507.13705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13705">https://arxiv.org/pdf/2507.13705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13705]] Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations(https://arxiv.org/abs/2507.13705)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly being implemented as joint decision-makers and explanation generators for Group Recommender Systems (GRS). In this paper, we evaluate these recommendations and explanations by comparing them to social choice-based aggregation strategies. Our results indicate that LLM-generated recommendations often resembled those produced by Additive Utilitarian (ADD) aggregation. However, the explanations typically referred to averaging ratings (resembling but not identical to ADD aggregation). Group structure, uniform or divergent, did not impact the recommendations. Furthermore, LLMs regularly claimed additional criteria such as user or item similarity, diversity, or used undefined popularity metrics or thresholds. Our findings have important implications for LLMs in the GRS pipeline as well as standard aggregation strategies. Additional criteria in explanations were dependent on the number of ratings in the group scenario, indicating potential inefficiency of standard aggregation methods at larger item set sizes. Additionally, inconsistent and ambiguous explanations undermine transparency and explainability, which are key motivations behind the use of LLMs for GRS.</li>
</ul>

<h3>Title: PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement</h3>
<ul>
<li><strong>Authors: </strong>Sofia Jamil, Bollampalli Areen Reddy, Raghvendra Kumar, Sriparna Saha, Koustava Goswami, K.J. Joseph</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13708">https://arxiv.org/abs/2507.13708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13708">https://arxiv.org/pdf/2507.13708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13708]] PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement(https://arxiv.org/abs/2507.13708)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image diffusion models have achieved remarkable success in generating realistic and diverse visual content. A critical factor in this process is the model's ability to accurately interpret textual prompts. However, these models often struggle with creative expressions, particularly those involving complex, abstract, or highly descriptive language. In this work, we introduce a novel training-free approach tailored to improve image generation for a unique form of creative language: poetic verse, which frequently features layered, abstract, and dual meanings. Our proposed PoemTale Diffusion approach aims to minimise the information that is lost during poetic text-to-image conversion by integrating a multi stage prompt refinement loop into Language Models to enhance the interpretability of poetic texts. To support this, we adapt existing state-of-the-art diffusion models by modifying their self-attention mechanisms with a consistent self-attention technique to generate multiple consistent images, which are then collectively used to convey the poem's meaning. Moreover, to encourage research in the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting of 1111 poems sourced from multiple online and offline resources. We engaged a panel of poetry experts for qualitative assessments. The results from both human and quantitative evaluations validate the efficacy of our method and contribute a novel perspective to poem-to-image generation with enhanced information capture in the generated images.</li>
</ul>

<h3>Title: Benchmarking of EEG Analysis Techniques for Parkinson's Disease Diagnosis: A Comparison between Traditional ML Methods and Foundation DL Methods</h3>
<ul>
<li><strong>Authors: </strong>Danilo Avola, Andrea Bernardini, Giancarlo Crocetti, Andrea Ladogana, Mario Lezoche, Maurizio Mancini, Daniele Pannone, Amedeo Ranaldi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13716">https://arxiv.org/abs/2507.13716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13716">https://arxiv.org/pdf/2507.13716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13716]] Benchmarking of EEG Analysis Techniques for Parkinson's Disease Diagnosis: A Comparison between Traditional ML Methods and Foundation DL Methods(https://arxiv.org/abs/2507.13716)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Parkinson's Disease PD is a progressive neurodegenerative disorder that affects motor and cognitive functions with early diagnosis being critical for effective clinical intervention Electroencephalography EEG offers a noninvasive and costeffective means of detecting PDrelated neural alterations yet the development of reliable automated diagnostic models remains a challenge In this study we conduct a systematic benchmark of traditional machine learning ML and deep learning DL models for classifying PD using a publicly available oddball task dataset Our aim is to lay the groundwork for developing an effective learning system and to determine which approach produces the best results We implement a unified sevenstep preprocessing pipeline and apply consistent subjectwise crossvalidation and evaluation criteria to ensure comparability across models Our results demonstrate that while baseline deep learning architectures particularly CNNLSTM models achieve the best performance compared to other deep learning architectures underlining the importance of capturing longrange temporal dependencies several traditional classifiers such as XGBoost also offer strong predictive accuracy and calibrated decision boundaries By rigorously comparing these baselines our work provides a solid reference framework for future studies aiming to develop and evaluate more complex or specialized architectures Establishing a reliable set of baseline results is essential to contextualize improvements introduced by novel methods ensuring scientific rigor and reproducibility in the evolving field of EEGbased neurodiagnostics</li>
</ul>

<h3>Title: Bi-GRU Based Deception Detection using EEG Signals</h3>
<ul>
<li><strong>Authors: </strong>Danilo Avola, Muhammad Yasir Bilal, Emad Emam, Cristina Lakasz, Daniele Pannone, Amedeo Ranaldi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13718">https://arxiv.org/abs/2507.13718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13718">https://arxiv.org/pdf/2507.13718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13718]] Bi-GRU Based Deception Detection using EEG Signals(https://arxiv.org/abs/2507.13718)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Deception detection is a significant challenge in fields such as security, psychology, and forensics. This study presents a deep learning approach for classifying deceptive and truthful behavior using ElectroEncephaloGram (EEG) signals from the Bag-of-Lies dataset, a multimodal corpus designed for naturalistic, casual deception scenarios. A Bidirectional Gated Recurrent Unit (Bi-GRU) neural network was trained to perform binary classification of EEG samples. The model achieved a test accuracy of 97\%, along with high precision, recall, and F1-scores across both classes. These results demonstrate the effectiveness of using bidirectional temporal modeling for EEG-based deception detection and suggest potential for real-time applications and future exploration of advanced neural architectures.</li>
</ul>

<h3>Title: Augmented Reality in Cultural Heritage: A Dual-Model Pipeline for 3D Artwork Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Daniele Pannone, Alessia Castronovo, Maurizio Mancini, Gian Luca Foresti, Claudio Piciarelli, Rossana Gabrieli, Muhammad Yasir Bilal, Danilo Avola</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13719">https://arxiv.org/abs/2507.13719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13719">https://arxiv.org/pdf/2507.13719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13719]] Augmented Reality in Cultural Heritage: A Dual-Model Pipeline for 3D Artwork Reconstruction(https://arxiv.org/abs/2507.13719)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents an innovative augmented reality pipeline tailored for museum environments, aimed at recognizing artworks and generating accurate 3D models from single images. By integrating two complementary pre-trained depth estimation models, i.e., GLPN for capturing global scene structure and Depth-Anything for detailed local reconstruction, the proposed approach produces optimized depth maps that effectively represent complex artistic features. These maps are then converted into high-quality point clouds and meshes, enabling the creation of immersive AR experiences. The methodology leverages state-of-the-art neural network architectures and advanced computer vision techniques to overcome challenges posed by irregular contours and variable textures in artworks. Experimental results demonstrate significant improvements in reconstruction accuracy and visual realism, making the system a highly robust tool for museums seeking to enhance visitor engagement through interactive digital content.</li>
</ul>

<h3>Title: Quantum Blockchain Survey: Foundations, Trends, and Gaps</h3>
<ul>
<li><strong>Authors: </strong>Saurav Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, cs.ET, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13720">https://arxiv.org/abs/2507.13720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13720">https://arxiv.org/pdf/2507.13720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13720]] Quantum Blockchain Survey: Foundations, Trends, and Gaps(https://arxiv.org/abs/2507.13720)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Quantum computing poses fundamental risks to classical blockchain systems by undermining widely used cryptographic primitives. In response, two major research directions have emerged: post-quantum blockchains, which integrate quantum-resistant algorithms, and quantum blockchains, which leverage quantum properties such as entanglement and quantum key distribution. This survey reviews key developments in both areas, analyzing their cryptographic foundations, architectural designs, and implementation challenges. This work provides a comparative overview of technical proposals, highlight trade-offs in security, scalability, and deployment, and identify open research problems across hardware, consensus, and network design. The goal is to offer a structured and comprehensive reference for advancing secure blockchain systems in the quantum era.</li>
</ul>

<h3>Title: Tackling fake images in cybersecurity -- Interpretation of a StyleGAN and lifting its black-box</h3>
<ul>
<li><strong>Authors: </strong>Julia Laubmann, Johannes Reschke</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13722">https://arxiv.org/abs/2507.13722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13722">https://arxiv.org/pdf/2507.13722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13722]] Tackling fake images in cybersecurity -- Interpretation of a StyleGAN and lifting its black-box(https://arxiv.org/abs/2507.13722)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, generative</a></li>
<li><strong>Abstract: </strong>In today's digital age, concerns about the dangers of AI-generated images are increasingly common. One powerful tool in this domain is StyleGAN (style-based generative adversarial networks), a generative adversarial network capable of producing highly realistic synthetic faces. To gain a deeper understanding of how such a model operates, this work focuses on analyzing the inner workings of StyleGAN's generator component. Key architectural elements and techniques, such as the Equalized Learning Rate, are explored in detail to shed light on the model's behavior. A StyleGAN model is trained using the PyTorch framework, enabling direct inspection of its learned weights. Through pruning, it is revealed that a significant number of these weights can be removed without drastically affecting the output, leading to reduced computational requirements. Moreover, the role of the latent vector -- which heavily influences the appearance of the generated faces -- is closely examined. Global alterations to this vector primarily affect aspects like color tones, while targeted changes to individual dimensions allow for precise manipulation of specific facial features. This ability to finetune visual traits is not only of academic interest but also highlights a serious ethical concern: the potential misuse of such technology. Malicious actors could exploit this capability to fabricate convincing fake identities, posing significant risks in the context of digital deception and cybercrime.</li>
</ul>

<h3>Title: Adversarial Training Improves Generalization Under Distribution Shifts in Bioacoustics</h3>
<ul>
<li><strong>Authors: </strong>René Heinrich, Lukas Rauch, Bernhard Sick, Christoph Scholz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13727">https://arxiv.org/abs/2507.13727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13727">https://arxiv.org/pdf/2507.13727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13727]] Adversarial Training Improves Generalization Under Distribution Shifts in Bioacoustics(https://arxiv.org/abs/2507.13727)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Adversarial training is a promising strategy for enhancing model robustness against adversarial attacks. However, its impact on generalization under substantial data distribution shifts in audio classification remains largely unexplored. To address this gap, this work investigates how different adversarial training strategies improve generalization performance and adversarial robustness in audio classification. The study focuses on two model architectures: a conventional convolutional neural network (ConvNeXt) and an inherently interpretable prototype-based model (AudioProtoPNet). The approach is evaluated using a challenging bird sound classification benchmark. This benchmark is characterized by pronounced distribution shifts between training and test data due to varying environmental conditions and recording methods, a common real-world challenge. The investigation explores two adversarial training strategies: one based on output-space attacks that maximize the classification loss function, and another based on embedding-space attacks designed to maximize embedding dissimilarity. These attack types are also used for robustness evaluation. Additionally, for AudioProtoPNet, the study assesses the stability of its learned prototypes under targeted embedding-space attacks. Results show that adversarial training, particularly using output-space attacks, improves clean test data performance by an average of 10.5% relative and simultaneously strengthens the adversarial robustness of the models. These findings, although derived from the bird sound domain, suggest that adversarial training holds potential to enhance robustness against both strong distribution shifts and adversarial attacks in challenging audio classification settings.</li>
</ul>

<h3>Title: The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction</h3>
<ul>
<li><strong>Authors: </strong>Guillaume Zambrano</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13732">https://arxiv.org/abs/2507.13732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13732">https://arxiv.org/pdf/2507.13732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13732]] The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction(https://arxiv.org/abs/2507.13732)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction, large language model</a></li>
<li><strong>Abstract: </strong>This study examines the role of human judges in legal decision-making by using machine learning to predict child physical custody outcomes in French appellate courts. Building on the legal realism-formalism debate, we test whether individual judges' decision-making patterns significantly influence case outcomes, challenging the assumption that judges are neutral variables that apply the law uniformly. To ensure compliance with French privacy laws, we implement a strict pseudonymization process. Our analysis uses 18,937 living arrangements rulings extracted from 10,306 cases. We compare models trained on individual judges' past rulings (specialist models) with a judge-agnostic model trained on aggregated data (generalist models). The prediction pipeline is a hybrid approach combining large language models (LLMs) for structured feature extraction and ML models for outcome prediction (RF, XGB and SVC). Our results show that specialist models consistently achieve higher predictive accuracy than the general model, with top-performing models reaching F1 scores as high as 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x more samples. Specialist models capture stable individual patterns that are not transferable to other judges. In-Domain and Cross-Domain validity tests provide empirical support for legal realism, demonstrating that judicial identity plays a measurable role in legal outcomes. All data and code used will be made available.</li>
</ul>

<h3>Title: An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC</h3>
<ul>
<li><strong>Authors: </strong>Matthias Jobst, Tim Langer, Chen Liu, Mehmet Alici, Hector A. Gonzalez, Christian Mayr</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13736">https://arxiv.org/abs/2507.13736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13736">https://arxiv.org/pdf/2507.13736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13736]] An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC(https://arxiv.org/abs/2507.13736)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This work presents a multi-layer DNN scheduling framework as an extension of OctopuScheduler, providing an end-to-end flow from PyTorch models to inference on a single SpiNNaker2 chip. Together with a front-end comprised of quantization and lowering steps, the proposed framework enables the edge-based execution of large and complex DNNs up to transformer scale using the neuromorphic platform SpiNNaker2.</li>
</ul>

<h3>Title: Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Junsu Kim, Yunhoe Ku, Seungryul Baek</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13739">https://arxiv.org/abs/2507.13739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13739">https://arxiv.org/pdf/2507.13739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13739]] Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning(https://arxiv.org/abs/2507.13739)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Few-shot class-incremental learning (FSCIL) is challenging due to extremely limited training data; while aiming to reduce catastrophic forgetting and learn new information. We propose Diffusion-FSCIL, a novel approach that employs a text-to-image diffusion model as a frozen backbone. Our conjecture is that FSCIL can be tackled using a large generative model's capabilities benefiting from 1) generation ability via large-scale pre-training; 2) multi-scale representation; 3) representational flexibility through the text encoder. To maximize the representation capability, we propose to extract multiple complementary diffusion features to play roles as latent replay with slight support from feature distillation for preventing generative biases. Our framework realizes efficiency through 1) using a frozen backbone; 2) minimal trainable components; 3) batch processing of multiple feature extractions. Extensive experiments on CUB-200, \emph{mini}ImageNet, and CIFAR-100 show that Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on previously learned classes and adapting effectively to new ones.</li>
</ul>

<h3>Title: Search-Optimized Quantization in Biomedical Ontology Alignment</h3>
<ul>
<li><strong>Authors: </strong>Oussama Bouaggad, Natalia Grabar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13742">https://arxiv.org/abs/2507.13742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13742">https://arxiv.org/pdf/2507.13742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13742]] Search-Optimized Quantization in Biomedical Ontology Alignment(https://arxiv.org/abs/2507.13742)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In the fast-moving world of AI, as organizations and researchers develop more advanced models, they face challenges due to their sheer size and computational demands. Deploying such models on edge devices or in resource-constrained environments adds further challenges related to energy consumption, memory usage and latency. To address these challenges, emerging trends are shaping the future of efficient model optimization techniques. From this premise, by employing supervised state-of-the-art transformer-based models, this research introduces a systematic method for ontology alignment, grounded in cosine-based semantic similarity between a biomedical layman vocabulary and the Unified Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to search for target optimizations among different Execution Providers (EPs) using the ONNX Runtime backend, followed by an assembled process of dynamic quantization employing Intel Neural Compressor and IPEX (Intel Extension for PyTorch). Through our optimization process, we conduct extensive assessments on the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new state-of-the-art in both. We retain performance metrics intact, while attaining an average inference speed-up of 20x and reducing memory usage by approximately 70%.</li>
</ul>

<h3>Title: PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Maluna Menke, Thilo Hagendorff</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13743">https://arxiv.org/abs/2507.13743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13743">https://arxiv.org/pdf/2507.13743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13743]] PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs(https://arxiv.org/abs/2507.13743)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) frequently reproduce the gender- and sexual-identity prejudices embedded in their training corpora, leading to outputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of great importance. To achieve this, we evaluate two parameter-efficient fine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt tuning - as lightweight alternatives to full-model fine-tuning for mitigating such biases. Using the WinoQueer benchmark, we quantify bias in three open-source LLMs and observe baseline bias scores reaching up to 98 (out of 100) across a range of queer identities defined by gender and/or sexual orientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1% additional parameters) on a curated QueerNews corpus reduces those scores by up to 50 points and raises neutrality from virtually 0% to as much as 36%. Soft-prompt tuning (10 virtual tokens) delivers only marginal improvements. These findings show that LoRA can deliver meaningful fairness gains with minimal computation. We advocate broader adoption of community-informed PEFT, the creation of larger queer-authored corpora, and richer evaluation suites beyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive.</li>
</ul>

<h3>Title: Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Tongtong Su, Chengyu Wang, Bingyan Liu, Jun Huang, Dongming Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13753">https://arxiv.org/abs/2507.13753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13753">https://arxiv.org/pdf/2507.13753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13753]] Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis(https://arxiv.org/abs/2507.13753)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In recent years, large text-to-video (T2V) synthesis models have garnered considerable attention for their abilities to generate videos from textual descriptions. However, achieving both high imaging quality and effective motion representation remains a significant challenge for these T2V models. Existing approaches often adapt pre-trained text-to-image (T2I) models to refine video frames, leading to issues such as flickering and artifacts due to inconsistencies across frames. In this paper, we introduce EVS, a training-free Encapsulated Video Synthesizer that composes T2I and T2V models to enhance both visual fidelity and motion smoothness of generated videos. Our approach utilizes a well-trained diffusion-based T2I model to refine low-quality video frames by treating them as out-of-distribution samples, effectively optimizing them with noising and denoising steps. Meanwhile, we employ T2V backbones to ensure consistent motion dynamics. By encapsulating the T2V temporal-only prior into the T2I generation process, EVS successfully leverages the strengths of both types of models, resulting in videos of improved imaging and motion quality. Experimental results validate the effectiveness of our approach compared to previous approaches. Our composition process also leads to a significant improvement of 1.6x-4.5x speedup in inference time. Source codes: this https URL.</li>
</ul>

<h3>Title: MolPIF: A Parameter Interpolation Flow Model for Molecule Generation</h3>
<ul>
<li><strong>Authors: </strong>Yaowei Jin, Junjie Wang, Wenkai Xiang, Duanhua Cao, Dan Teng, Zhehuan Fan, Jiacheng Xiong, Xia Sheng, Chuanlong Zeng, Mingyue Zheng, Qian Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13762">https://arxiv.org/abs/2507.13762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13762">https://arxiv.org/pdf/2507.13762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13762]] MolPIF: A Parameter Interpolation Flow Model for Molecule Generation(https://arxiv.org/abs/2507.13762)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Advances in deep learning for molecular generation show promise in accelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown impressive performance across diverse chemical tasks, with their success often ascribed to the paradigm of modeling in a low-variance parameter space. However, the Bayesian inference-based strategy imposes limitations on designing more flexible distribution transformation pathways, making it challenging to adapt to diverse data distributions and varied task requirements. Furthermore, the potential for simpler, more efficient parameter-space-based models is unexplored. To address this, we propose a novel Parameter Interpolation Flow model (named PIF) with detailed theoretical foundation, training, and inference procedures. We then develop MolPIF for structure-based drug design, demonstrating its superior performance across diverse metrics compared to baselines. This work validates the effectiveness of parameter-space-based generative modeling paradigm for molecules and offers new perspectives for model design.</li>
</ul>

<h3>Title: Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Mingyang Yu, Zhijian Wu, Dingjiang Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13769">https://arxiv.org/abs/2507.13769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13769">https://arxiv.org/pdf/2507.13769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13769]] Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction(https://arxiv.org/abs/2507.13769)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Hyperspectral image (HSI) reconstruction aims to recover 3D HSI from its degraded 2D measurements. Recently great progress has been made in deep learning-based methods, however, these methods often struggle to accurately capture high-frequency details of the HSI. To address this issue, this paper proposes a Spectral Diffusion Prior (SDP) that is implicitly learned from hyperspectral images using a diffusion model. Leveraging the powerful ability of the diffusion model to reconstruct details, this learned prior can significantly improve the performance when injected into the HSI model. To further improve the effectiveness of the learned prior, we also propose the Spectral Prior Injector Module (SPIM) to dynamically guide the model to recover the HSI details. We evaluate our method on two representative HSI methods: MST and BISRNet. Experimental results show that our method outperforms existing networks by about 0.5 dB, effectively improving the performance of HSI reconstruction.</li>
</ul>

<h3>Title: Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Abhijit Sen, Giridas Maiti, Bikram K. Parida, Bhanu P. Mishra, Mahima Arya, Denys I. Bondar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13772">https://arxiv.org/abs/2507.13772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13772">https://arxiv.org/pdf/2507.13772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13772]] Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification(https://arxiv.org/abs/2507.13772)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Feature engineering continues to play a critical role in image classification, particularly when interpretability and computational efficiency are prioritized over deep learning models with millions of parameters. In this study, we revisit classical machine learning based image classification through a novel approach centered on Permutation Entropy (PE), a robust and computationally lightweight measure traditionally used in time series analysis but rarely applied to image data. We extend PE to two-dimensional images and propose a multiscale, multi-orientation entropy-based feature extraction approach that characterizes spatial order and complexity along rows, columns, diagonals, anti-diagonals, and local patches of the image. To enhance the discriminatory power of the entropy features, we integrate two classic image descriptors: the Histogram of Oriented Gradients (HOG) to capture shape and edge structure, and Local Binary Patterns (LBP) to encode micro-texture of an image. The resulting hand-crafted feature set, comprising of 780 dimensions, is used to train Support Vector Machine (SVM) classifiers optimized through grid search. The proposed approach is evaluated on multiple benchmark datasets, including Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10, where it delivers competitive classification performance without relying on deep architectures. Our results demonstrate that the fusion of PE with HOG and LBP provides a compact, interpretable, and effective alternative to computationally expensive and limited interpretable deep learning models. This shows a potential of entropy-based descriptors in image classification and contributes a lightweight and generalizable solution to interpretable machine learning in image classification and computer vision.</li>
</ul>

<h3>Title: Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI</h3>
<ul>
<li><strong>Authors: </strong>Kyriakos Flouris, Moritz Halter, Yolanne Y. R. Lee, Samuel Castonguay, Luuk Jacobs, Pietro Dirix, Jonathan Nestmann, Sebastian Kozerke, Ender Konukoglu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13789">https://arxiv.org/abs/2507.13789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13789">https://arxiv.org/pdf/2507.13789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13789]] Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI(https://arxiv.org/abs/2507.13789)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Hemodynamic analysis is essential for predicting aneurysm rupture and guiding treatment. While magnetic resonance flow imaging enables time-resolved volumetric blood velocity measurements, its low spatiotemporal resolution and signal-to-noise ratio limit its diagnostic utility. To address this, we propose the Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that enhances both spatial and temporal resolution with the ability to predict wall shear stress (WSS) directly from clinical imaging data. LoFNO integrates Laplacian eigenvectors as geometric priors for improved structural awareness on irregular, unseen geometries and employs an Enhanced Deep Super-Resolution Network (EDSR) layer for robust upsampling. By combining geometric priors with neural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow data, achieving superior velocity and WSS predictions compared to interpolation and alternative deep learning methods, enabling more precise cerebrovascular diagnostics.</li>
</ul>

<h3>Title: DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance</h3>
<ul>
<li><strong>Authors: </strong>Huu-Phu Do, Yu-Wei Chen, Yi-Cheng Liao, Chi-Wei Hsiao, Han-Yang Wang, Wei-Chen Chiu, Ching-Chun Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13797">https://arxiv.org/abs/2507.13797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13797">https://arxiv.org/pdf/2507.13797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13797]] DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance(https://arxiv.org/abs/2507.13797)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Blind Face Restoration aims to recover high-fidelity, detail-rich facial images from unknown degraded inputs, presenting significant challenges in preserving both identity and detail. Pre-trained diffusion models have been increasingly used as image priors to generate fine details. Still, existing methods often use fixed diffusion sampling timesteps and a global guidance scale, assuming uniform degradation. This limitation and potentially imperfect degradation kernel estimation frequently lead to under- or over-diffusion, resulting in an imbalance between fidelity and quality. We propose DynFaceRestore, a novel blind face restoration approach that learns to map any blindly degraded input to Gaussian blurry images. By leveraging these blurry images and their respective Gaussian kernels, we dynamically select the starting timesteps for each blurry image and apply closed-form guidance during the diffusion sampling process to maintain fidelity. Additionally, we introduce a dynamic guidance scaling adjuster that modulates the guidance strength across local regions, enhancing detail generation in complex areas while preserving structural fidelity in contours. This strategy effectively balances the trade-off between fidelity and quality. DynFaceRestore achieves state-of-the-art performance in both quantitative and qualitative evaluations, demonstrating robustness and effectiveness in blind face restoration.</li>
</ul>

<h3>Title: One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion</h3>
<ul>
<li><strong>Authors: </strong>Haoang Lu, Yuanqi Su, Xiaoning Zhang, Hao Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13801">https://arxiv.org/abs/2507.13801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13801">https://arxiv.org/pdf/2507.13801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13801]] One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion(https://arxiv.org/abs/2507.13801)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a critical perception task for autonomous driving due to its ability to infer complete 3D scene layouts and semantics from single 2D images. However, in real-world traffic scenarios, a significant portion of the scene remains occluded or outside the camera's field of view -- a fundamental challenge that existing monocular SSC methods fail to address adequately. To overcome these limitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC framework that leverages pseudo-future frame prediction to expand the model's effective perceptual range. Our approach combines poses and depths to establish accurate 3D correspondences, enabling geometrically-consistent fusion of past, present, and predicted future frames in 3D space. Unlike conventional methods that rely on simple feature stacking, our 3D-aware architecture achieves more robust scene completion by explicitly modeling spatial-temporal relationships. Comprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks demonstrate state-of-the-art performance, validating the effectiveness of our approach, highlighting our method's ability to improve occlusion reasoning and 3D scene completion accuracy.</li>
</ul>

<h3>Title: GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with Adaptive Low-Rank Compensation</h3>
<ul>
<li><strong>Authors: </strong>Weiqi Yang, Xu Zhou, Jingfu Guan, Hao Du, Tianyu Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13803">https://arxiv.org/abs/2507.13803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13803">https://arxiv.org/pdf/2507.13803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13803]] GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with Adaptive Low-Rank Compensation(https://arxiv.org/abs/2507.13803)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-modal fusion is crucial for Internet of Things (IoT) perception, widely deployed in smart homes, intelligent transport, industrial automation, and healthcare. However, existing systems often face challenges: high model complexity hinders deployment in resource-constrained environments, unidirectional modal alignment neglects inter-modal relationships, and robustness suffers when sensor data is missing. These issues impede efficient and robust multimodal perception in real-world IoT settings. To overcome these limitations, we propose GRAM-MAMBA. This framework utilizes the linear-complexity Mamba model for efficient sensor time-series processing, combined with an optimized GRAM matrix strategy for pairwise alignment among modalities, addressing the shortcomings of traditional single-modality alignment. Inspired by Low-Rank Adaptation (LoRA), we introduce an adaptive low-rank layer compensation strategy to handle missing modalities post-training. This strategy freezes the pre-trained model core and irrelevant adaptive layers, fine-tuning only those related to available modalities and the fusion process. Extensive experiments validate GRAM-MAMBA's effectiveness. On the SPAWC2021 indoor positioning dataset, the pre-trained model shows lower error than baselines; adapting to missing modalities yields a 24.5% performance boost by training less than 0.2% of parameters. On the USC-HAD human activity recognition dataset, it achieves 93.55% F1 and 93.81% Overall Accuracy (OA), outperforming prior work; the update strategy increases F1 by 23% while training less than 0.3% of parameters. These results highlight GRAM-MAMBA's potential for achieving efficient and robust multimodal perception in resource-constrained environments.</li>
</ul>

<h3>Title: SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Yingying Zhang, Lixiang Ru, Kang Wu, Lei Yu, Lei Liang, Yansheng Li, Jingdong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13812">https://arxiv.org/abs/2507.13812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13812">https://arxiv.org/pdf/2507.13812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13812]] SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing(https://arxiv.org/abs/2507.13812)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The multi-modal remote sensing foundation model (MM-RSFM) has significantly advanced various Earth observation tasks, such as urban planning, environmental monitoring, and natural disaster management. However, most existing approaches generally require the training of separate backbone networks for each data modality, leading to redundancy and inefficient parameter utilization. Moreover, prevalent pre-training methods typically apply self-supervised learning (SSL) techniques from natural images without adequately accommodating the characteristics of remote sensing (RS) images, such as the complicated semantic distribution within a single RS image. In this work, we present SkySense V2, a unified MM-RSFM that employs a single transformer backbone to handle multiple modalities. This backbone is pre-trained with a novel SSL strategy tailored to the distinct traits of RS data. In particular, SkySense V2 incorporates an innovative adaptive patch merging module and learnable modality prompt tokens to address challenges related to varying resolutions and limited feature diversity across modalities. In additional, we incorporate the mixture of experts (MoE) module to further enhance the performance of the foundation model. SkySense V2 demonstrates impressive generalization abilities through an extensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense by an average of 1.8 points.</li>
</ul>

<h3>Title: Team of One: Cracking Complex Video QA with Model Synergy</h3>
<ul>
<li><strong>Authors: </strong>Jun Xie, Zhaoran Zhao, Xiongjun Guan, Yingjian Zhu, Hongzhu Yi, Xinming Wang, Feng Chen, Zhepeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13820">https://arxiv.org/abs/2507.13820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13820">https://arxiv.org/pdf/2507.13820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13820]] Team of One: Cracking Complex Video QA with Model Synergy(https://arxiv.org/abs/2507.13820)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We propose a novel framework for open-ended video question answering that enhances reasoning depth and robustness in complex real-world scenarios, as benchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models (Video-LMMs) often exhibit limited contextual understanding, weak temporal modeling, and poor generalization to ambiguous or compositional queries. To address these challenges, we introduce a prompting-and-response integration mechanism that coordinates multiple heterogeneous Video-Language Models (VLMs) via structured chains of thought, each tailored to distinct reasoning pathways. An external Large Language Model (LLM) serves as an evaluator and integrator, selecting and fusing the most reliable responses. Extensive experiments demonstrate that our method significantly outperforms existing baselines across all evaluation metrics, showcasing superior generalization and robustness. Our approach offers a lightweight, extensible strategy for advancing multimodal reasoning without requiring model retraining, setting a strong foundation for future Video-LMM development.</li>
</ul>

<h3>Title: Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hosein Azarbonyad, Zi Long Zhu, Georgios Cheirmpos, Zubair Afzal, Vikrant Yadav, Georgios Tsatsaronis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13827">https://arxiv.org/abs/2507.13827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13827">https://arxiv.org/pdf/2507.13827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13827]] Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models(https://arxiv.org/abs/2507.13827)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>When deciding to read an article or incorporate it into their research, scholars often seek to quickly identify and understand its main ideas. In this paper, we aim to extract these key concepts and contributions from scientific articles in the form of Question and Answer (QA) pairs. We propose two distinct approaches for generating QAs. The first approach involves selecting salient paragraphs, using a Large Language Model (LLM) to generate questions, ranking these questions by the likelihood of obtaining meaningful answers, and subsequently generating answers. This method relies exclusively on the content of the articles. However, assessing an article's novelty typically requires comparison with the existing literature. Therefore, our second approach leverages a Knowledge Graph (KG) for QA generation. We construct a KG by fine-tuning an Entity Relationship (ER) extraction model on scientific articles and using it to build the graph. We then employ a salient triplet extraction method to select the most pertinent ERs per article, utilizing metrics such as the centrality of entities based on a triplet TF-IDF-like measure. This measure assesses the saliency of a triplet based on its importance within the article compared to its prevalence in the literature. For evaluation, we generate QAs using both approaches and have them assessed by Subject Matter Experts (SMEs) through a set of predefined metrics to evaluate the quality of both questions and answers. Our evaluations demonstrate that the KG-based approach effectively captures the main ideas discussed in the articles. Furthermore, our findings indicate that fine-tuning the ER extraction model on our scientific corpus is crucial for extracting high-quality triplets from such documents.</li>
</ul>

<h3>Title: Modeling Fair Play in Detective Stories with Language Models</h3>
<ul>
<li><strong>Authors: </strong>Eitan Wagner, Renana Keydar, Omri Abend</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13841">https://arxiv.org/abs/2507.13841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13841">https://arxiv.org/pdf/2507.13841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13841]] Modeling Fair Play in Detective Stories with Language Models(https://arxiv.org/abs/2507.13841)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Effective storytelling relies on a delicate balance between meeting the reader's prior expectations and introducing unexpected developments. In the domain of detective fiction, this tension is known as fair play, which includes the implicit agreement between the writer and the reader as to the range of possible resolutions the mystery story may have. In this work, we present a probabilistic framework for detective fiction that allows us to define desired qualities. Using this framework, we formally define fair play and design appropriate metrics for it. Stemming from these definitions is an inherent tension between the coherence of the story, which measures how much it ``makes sense'', and the surprise it induces. We validate the framework by applying it to LLM-generated detective stories. This domain is appealing since we have an abundance of data, we can sample from the distribution generating the story, and the story-writing capabilities of LLMs are interesting in their own right. Results show that while LLM-generated stories may be unpredictable, they generally fail to balance the trade-off between surprise and fair play, which greatly contributes to their poor quality.</li>
</ul>

<h3>Title: A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data</h3>
<ul>
<li><strong>Authors: </strong>Luigi Russo, Francesco Mauro, Babak Memar, Alessandro Sebastianelli, Silvia Liberata Ullo, Paolo Gamba</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13852">https://arxiv.org/abs/2507.13852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13852">https://arxiv.org/pdf/2507.13852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13852]] A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data(https://arxiv.org/abs/2507.13852)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Building segmentation in urban areas is essential in fields such as urban planning, disaster response, and population mapping. Yet accurately segmenting buildings in dense urban regions presents challenges due to the large size and high resolution of satellite images. This study investigates the use of a Quanvolutional pre-processing to enhance the capability of the Attention U-Net model in the building segmentation. Specifically, this paper focuses on the urban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR) imagery. In this work, Quanvolution was used to extract more informative feature maps that capture essential structural details in radar imagery, proving beneficial for accurate building segmentation. Preliminary results indicate that proposed methodology achieves comparable test accuracy to the standard Attention U-Net model while significantly reducing network parameters. This result aligns with findings from previous works, confirming that Quanvolution not only maintains model accuracy but also increases computational efficiency. These promising outcomes highlight the potential of quantum-assisted Deep Learning frameworks for large-scale building segmentation in urban environments.</li>
</ul>

<h3>Title: InTraVisTo: Inside Transformer Visualisation Tool</h3>
<ul>
<li><strong>Authors: </strong>Nicolò Brunello, Davide Rigamonti, Andrea Sassella, Vincenzo Scotti, Mark James Carman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13858">https://arxiv.org/abs/2507.13858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13858">https://arxiv.org/pdf/2507.13858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13858]] InTraVisTo: Inside Transformer Visualisation Tool(https://arxiv.org/abs/2507.13858)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The reasoning capabilities of Large Language Models (LLMs) have increased greatly over the last few years, as have their size and complexity. Nonetheless, the use of LLMs in production remains challenging due to their unpredictable nature and discrepancies that can exist between their desired behavior and their actual model output. In this paper, we introduce a new tool, InTraVisTo (Inside Transformer Visualisation Tool), designed to enable researchers to investigate and trace the computational process that generates each token in a Transformer-based LLM. InTraVisTo provides a visualization of both the internal state of the Transformer model (by decoding token embeddings at each layer of the model) and the information flow between the various components across the different layers of the model (using a Sankey diagram). With InTraVisTo, we aim to help researchers and practitioners better understand the computations being performed within the Transformer model and thus to shed some light on internal patterns and reasoning processes employed by LLMs.</li>
</ul>

<h3>Title: Label Unification for Cross-Dataset Generalization in Cybersecurity NER</h3>
<ul>
<li><strong>Authors: </strong>Maciej Jalocha, Johan Hausted Schmidt, William Michelseen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13870">https://arxiv.org/abs/2507.13870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13870">https://arxiv.org/pdf/2507.13870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13870]] Label Unification for Cross-Dataset Generalization in Cybersecurity NER(https://arxiv.org/abs/2507.13870)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The field of cybersecurity NER lacks standardized labels, making it challenging to combine datasets. We investigate label unification across four cybersecurity datasets to increase data resource usability. We perform a coarse-grained label unification and conduct pairwise cross-dataset evaluations using BiLSTM models. Qualitative analysis of predictions reveals errors, limitations, and dataset differences. To address unification limitations, we propose alternative architectures including a multihead model and a graph-based transfer model. Results show that models trained on unified datasets generalize poorly across datasets. The multihead model with weight sharing provides only marginal improvements over unified training, while our graph-based transfer model built on BERT-base-NER shows no significant performance gains compared BERT-base-NER.</li>
</ul>

<h3>Title: Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision</h3>
<ul>
<li><strong>Authors: </strong>Marten Kreis, Benjamin Kiefer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13880">https://arxiv.org/abs/2507.13880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13880">https://arxiv.org/pdf/2507.13880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13880]] Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision(https://arxiv.org/abs/2507.13880)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>This paper presents a novel approach to enhancing marine vision by fusing real-time visual data with chart information. Our system overlays nautical chart data onto live video feeds by accurately matching detected navigational aids, such as buoys, with their corresponding representations in chart data. To achieve robust association, we introduce a transformer-based end-to-end neural network that predicts bounding boxes and confidence scores for buoy queries, enabling the direct matching of image-domain detections with world-space chart markers. The proposed method is compared against baseline approaches, including a ray-casting model that estimates buoy positions via camera projection and a YOLOv7-based network extended with a distance estimation module. Experimental results on a dataset of real-world maritime scenes demonstrate that our approach significantly improves object localization and association accuracy in dynamic and challenging environments.</li>
</ul>

<h3>Title: Using LLMs to identify features of personal and professional skills in an open-response situational judgment test</h3>
<ul>
<li><strong>Authors: </strong>Cole Walsh, Rodica Ivan, Muhammad Zafar Iqbal, Colleen Robb</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13881">https://arxiv.org/abs/2507.13881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13881">https://arxiv.org/pdf/2507.13881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13881]] Using LLMs to identify features of personal and professional skills in an open-response situational judgment test(https://arxiv.org/abs/2507.13881)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Academic programs are increasingly recognizing the importance of personal and professional skills and their critical role alongside technical expertise in preparing students for future success in diverse career paths. With this growing demand comes the need for scalable systems to measure, evaluate, and develop these skills. Situational Judgment Tests (SJTs) offer one potential avenue for measuring these skills in a standardized and reliable way, but open-response SJTs have traditionally relied on trained human raters for evaluation, presenting operational challenges to delivering SJTs at scale. Past attempts at developing NLP-based scoring systems for SJTs have fallen short due to issues with construct validity of these systems. In this article, we explore a novel approach to extracting construct-relevant features from SJT responses using large language models (LLMs). We use the Casper SJT to demonstrate the efficacy of this approach. This study sets the foundation for future developments in automated scoring for personal and professional skills.</li>
</ul>

<h3>Title: PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations</h3>
<ul>
<li><strong>Authors: </strong>Yu Wei, Jiahui Zhang, Xiaoqin Zhang, Ling Shao, Shijian Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13891">https://arxiv.org/abs/2507.13891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13891">https://arxiv.org/pdf/2507.13891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13891]] PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations(https://arxiv.org/abs/2507.13891)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing attention due to its remarkable performance in reconstructing high-quality 3D scenes from unposed images or videos. However, it often struggles to handle scenes with complex camera trajectories as featured by drastic rotation and translation across adjacent camera views, leading to degraded estimation of camera poses and further local minima in joint optimization of camera poses and 3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that achieves superior 3D scene modeling and camera pose estimation via camera pose co-regularization. PCR-GS achieves regularization from two perspectives. The first is feature reprojection regularization which extracts view-robust DINO features from adjacent camera views and aligns their semantic information for camera pose regularization. The second is wavelet-based frequency regularization which exploits discrepancy in high-frequency details to further optimize the rotation matrix in camera poses. Extensive experiments over multiple real-world scenes show that the proposed PCR-GS achieves superior pose-free 3D-GS scene modeling under dramatic changes of camera trajectories.</li>
</ul>

<h3>Title: Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Yujian Mo, Yan Wu, Junqiao Zhao, Jijun Wang, Yinghao Hu, Jun Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13899">https://arxiv.org/abs/2507.13899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13899">https://arxiv.org/pdf/2507.13899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13899]] Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection(https://arxiv.org/abs/2507.13899)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Recent advances in foundation models have opened up new possibilities for enhancing 3D perception. In particular, DepthAnything offers dense and reliable geometric priors from monocular RGB images, which can complement sparse LiDAR data in autonomous driving scenarios. However, such priors remain underutilized in LiDAR-based 3D object detection. In this paper, we address the limited expressiveness of raw LiDAR point features, especially the weak discriminative capability of the reflectance attribute, by introducing depth priors predicted by DepthAnything. These priors are fused with the original LiDAR attributes to enrich each point's representation. To leverage the enhanced point features, we propose a point-wise feature extraction module. Then, a Dual-Path RoI feature extraction framework is employed, comprising a voxel-based branch for global semantic context and a point-based branch for fine-grained structural details. To effectively integrate the complementary RoI features, we introduce a bidirectional gated RoI feature fusion module that balances global and local cues. Extensive experiments on the KITTI benchmark show that our method consistently improves detection accuracy, demonstrating the value of incorporating visual foundation model priors into LiDAR-based 3D object detection.</li>
</ul>

<h3>Title: Political Leaning and Politicalness Classification of Texts</h3>
<ul>
<li><strong>Authors: </strong>Matous Volf (1), Jakub Simko (2) ((1) DELTA High school of computer science and economics, Pardubice, Czechia, (2) Kempelen Institute of Intelligent Technologies, Bratislava, Slovakia)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13913">https://arxiv.org/abs/2507.13913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13913">https://arxiv.org/pdf/2507.13913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13913]] Political Leaning and Politicalness Classification of Texts(https://arxiv.org/abs/2507.13913)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper addresses the challenge of automatically classifying text according to political leaning and politicalness using transformer models. We compose a comprehensive overview of existing datasets and models for these tasks, finding that current approaches create siloed solutions that perform poorly on out-of-distribution texts. To address this limitation, we compile a diverse dataset by combining 12 datasets for political leaning classification and creating a new dataset for politicalness by extending 18 existing datasets with the appropriate label. Through extensive benchmarking with leave-one-in and leave-one-out methodologies, we evaluate the performance of existing models and train new ones with enhanced generalization capabilities.</li>
</ul>

<h3>Title: Reframing attention as a reinforcement learning problem for causal discovery</h3>
<ul>
<li><strong>Authors: </strong>Turan Orujlu, Christian Gumbsch, Martin V. Butz, Charley M Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13920">https://arxiv.org/abs/2507.13920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13920">https://arxiv.org/pdf/2507.13920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13920]] Reframing attention as a reinforcement learning problem for causal discovery(https://arxiv.org/abs/2507.13920)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Formal frameworks of causality have operated largely parallel to modern trends in deep reinforcement learning (RL). However, there has been a revival of interest in formally grounding the representations learned by neural networks in causal concepts. Yet, most attempts at neural models of causality assume static causal graphs and ignore the dynamic nature of causal interactions. In this work, we introduce Causal Process framework as a novel theory for representing dynamic hypotheses about causal structure. Furthermore, we present Causal Process Model as an implementation of this framework. This allows us to reformulate the attention mechanism popularized by Transformer networks within an RL setting with the goal to infer interpretable causal processes from visual observations. Here, causal inference corresponds to constructing a causal graph hypothesis which itself becomes an RL task nested within the original RL problem. To create an instance of such hypothesis, we employ RL agents. These agents establish links between units similar to the original Transformer attention mechanism. We demonstrate the effectiveness of our approach in an RL environment where we outperform current alternatives in causal representation learning and agent performance, and uniquely recover graphs of dynamic causal processes.</li>
</ul>

<h3>Title: Developers Insight On Manifest v3 Privacy and Security Webextensions</h3>
<ul>
<li><strong>Authors: </strong>Libor Polčák, Giorgio Maone, Michael McMahon, Martin Bednář</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13926">https://arxiv.org/abs/2507.13926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13926">https://arxiv.org/pdf/2507.13926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13926]] Developers Insight On Manifest v3 Privacy and Security Webextensions(https://arxiv.org/abs/2507.13926)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Webextensions can improve web browser privacy, security, and user experience. The APIs offered by the browser to webextensions affect possible functionality. Currently, Chrome transitions to a modified set of APIs called Manifest v3. This paper studies the challenges and opportunities of Manifest v3 with an in-depth structured qualitative research. Even though some projects observed positive effects, a majority expresses concerns over limited benefits to users, removal of crucial APIs, or the need to find workarounds. Our findings indicate that the transition affects different types of webextensions differently; some can migrate without losing functionality, while other projects remove functionality or decline to update. The respondents identified several critical missing APIs, including reliable APIs to inject content scripts, APIs for storing confidential content, and others.</li>
</ul>

<h3>Title: Chain Table: Protecting Table-Level Data Integrity by Digital Ledger Technology</h3>
<ul>
<li><strong>Authors: </strong>Feng Yu, Ryan Laird</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13932">https://arxiv.org/abs/2507.13932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13932">https://arxiv.org/pdf/2507.13932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13932]] Chain Table: Protecting Table-Level Data Integrity by Digital Ledger Technology(https://arxiv.org/abs/2507.13932)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>The rise of blockchain and Digital Ledger Technology (DLT) has gained wide traction. Instead of relying on a traditional centralized data authority, a blockchain system consists of digitally entangled block data shared across a distributed network. The specially designed chain data structure and its consensus mechanism protect blockchain data from being tampered by unauthorized adversaries. However, implementing a full-fledged blockchain system to protect a database can be technically cumbersome. In this work, we introduce an in-database design, named chain table, to protect data integrity without the need for a blockchain system. It features a succinct design without significant technology barriers or storage overhead. To realize rigorous data security, we also propose a set of data writing principles for the chain table. We prove that the chain table, together with the data writing principles, will guarantee flexible data integrity, named table-level data integrity (TDI).</li>
</ul>

<h3>Title: DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization</h3>
<ul>
<li><strong>Authors: </strong>Marzieh Gheisari, Auguste Genovesio</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13934">https://arxiv.org/abs/2507.13934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13934">https://arxiv.org/pdf/2507.13934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13934]] DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization(https://arxiv.org/abs/2507.13934)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Unsupervised disentanglement of static appearance and dynamic motion in video remains a fundamental challenge, often hindered by information leakage and blurry reconstructions in existing VAE- and GAN-based approaches. We introduce DiViD, the first end-to-end video diffusion framework for explicit static-dynamic factorization. DiViD's sequence encoder extracts a global static token from the first frame and per-frame dynamic tokens, explicitly removing static content from the motion code. Its conditional DDPM decoder incorporates three key inductive biases: a shared-noise schedule for temporal consistency, a time-varying KL-based bottleneck that tightens at early timesteps (compressing static information) and relaxes later (enriching dynamics), and cross-attention that routes the global static token to all frames while keeping dynamic tokens frame-specific. An orthogonality regularizer further prevents residual static-dynamic leakage. We evaluate DiViD on real-world benchmarks using swap-based accuracy and cross-leakage metrics. DiViD outperforms state-of-the-art sequential disentanglement methods: it achieves the highest swap-based joint accuracy, preserves static fidelity while improving dynamic transfer, and reduces average cross-leakage.</li>
</ul>

<h3>Title: Generalist Forecasting with Frozen Video Models via Latent Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Jacob C Walker, Pedro Vélez, Luisa Polania Cabrera, Guangyao Zhou, Rishabh Kabra, Carl Doersch, Maks Ovsjanikov, João Carreira, Shiry Ginosar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13942">https://arxiv.org/abs/2507.13942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13942">https://arxiv.org/pdf/2507.13942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13942]] Generalist Forecasting with Frozen Video Models via Latent Diffusion(https://arxiv.org/abs/2507.13942)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Forecasting what will happen next is a critical skill for general-purpose systems that plan or act in the world at different levels of abstraction. In this paper, we identify a strong correlation between a vision model's perceptual ability and its generalist forecasting performance over short time horizons. This trend holds across a diverse set of pretrained models-including those trained generatively-and across multiple levels of abstraction, from raw pixels to depth, point tracks, and object motion. The result is made possible by a novel generalist forecasting framework that operates on any frozen vision backbone: we train latent diffusion models to forecast future features in the frozen representation space, which are then decoded via lightweight, task-specific readouts. To enable consistent evaluation across tasks, we introduce distributional metrics that compare distributional properties directly in the space of downstream tasks and apply this framework to nine models and four tasks. Our results highlight the value of bridging representation learning and generative modeling for temporally grounded video understanding.</li>
</ul>

<h3>Title: Exploiting Primacy Effect To Improve Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bianca Raimondi, Maurizio Gabbrielli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13949">https://arxiv.org/abs/2507.13949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13949">https://arxiv.org/pdf/2507.13949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13949]] Exploiting Primacy Effect To Improve Large Language Models(https://arxiv.org/abs/2507.13949)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become essential in many Natural Language Processing (NLP) tasks, leveraging extensive pre-training and fine-tuning to achieve high accuracy. However, like humans, LLMs exhibit biases, particularly positional biases such as primacy and recency effects, which can influence the accuracy of the answers. The primacy effect-where items presented first are more likely to be remembered or selected-plays a key role in Multiple Choice Question Answering (MCQA), where the order of answer options can affect prediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We first show that fine-tuning amplifies this bias, probably due to exposure to human-like patterns. Hence, we strategically leverage this effect by reordering response options based on semantic similarity to the query, without requiring knowledge of the correct answer. Our experimental results show that this approach significantly improves performance in MCQA. More generally, our findings underscore the dual nature of biases as both challenges and opportunities, offering insights for bias-aware model design and NLP applications.</li>
</ul>

<h3>Title: MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space</h3>
<ul>
<li><strong>Authors: </strong>Jingbo Liang, Bruna Jacobson</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.bio-ph, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13950">https://arxiv.org/abs/2507.13950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13950">https://arxiv.org/pdf/2507.13950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13950]] MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space(https://arxiv.org/abs/2507.13950)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Extensively exploring protein conformational landscapes remains a major challenge in computational biology due to the high computational cost involved in dynamic physics-based simulations. In this work, we propose a novel pipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and generative adversarial networks (GANs) to explore protein conformational spaces. MoDyGAN contains a generator that maps Gaussian distributions into MD-derived protein trajectories, and a refinement module that combines ensemble learning with a dual-discriminator to further improve the plausibility of generated conformations. Central to our approach is an innovative representation technique that reversibly transforms 3D protein structures into 2D matrices, enabling the use of advanced image-based GAN architectures. We use three rigid proteins to demonstrate that MoDyGAN can generate plausible new conformations. We also use deca-alanine as a case study to show that interpolations within the latent space closely align with trajectories obtained from steered molecular dynamics (SMD) simulations. Our results suggest that representing proteins as image-like data unlocks new possibilities for applying advanced deep learning techniques to biomolecular simulation, leading to an efficient sampling of conformational states. Additionally, the proposed framework holds strong potential for extension to other complex 3D structures.</li>
</ul>

<h3>Title: Robust Anomaly Detection with Graph Neural Networks using Controllability</h3>
<ul>
<li><strong>Authors: </strong>Yifan Wei, Anwar Said, Waseem Abbas, Xenofon Koutsoukos</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13954">https://arxiv.org/abs/2507.13954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13954">https://arxiv.org/pdf/2507.13954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13954]] Robust Anomaly Detection with Graph Neural Networks using Controllability(https://arxiv.org/abs/2507.13954)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Anomaly detection in complex domains poses significant challenges due to the need for extensive labeled data and the inherently imbalanced nature of anomalous versus benign samples. Graph-based machine learning models have emerged as a promising solution that combines attribute and relational data to uncover intricate patterns. However, the scarcity of anomalous data exacerbates the challenge, which requires innovative strategies to enhance model learning with limited information. In this paper, we hypothesize that the incorporation of the influence of the nodes, quantified through average controllability, can significantly improve the performance of anomaly detection. We propose two novel approaches to integrate average controllability into graph-based frameworks: (1) using average controllability as an edge weight and (2) encoding it as a one-hot edge attribute vector. Through rigorous evaluation on real-world and synthetic networks with six state-of-the-art baselines, our proposed methods demonstrate improved performance in identifying anomalies, highlighting the critical role of controllability measures in enhancing the performance of graph machine learning models. This work underscores the potential of integrating average controllability as additional metrics to address the challenges of anomaly detection in sparse and imbalanced datasets.</li>
</ul>

<h3>Title: Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset</h3>
<ul>
<li><strong>Authors: </strong>Sara Abdulaziz, Giacomo D'Amicantonio, Egor Bondarev</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13981">https://arxiv.org/abs/2507.13981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13981">https://arxiv.org/pdf/2507.13981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13981]] Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset(https://arxiv.org/abs/2507.13981)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, biometric</a></li>
<li><strong>Abstract: </strong>Recent advances in AI-powered surveillance have intensified concerns over the collection and processing of sensitive personal data. In response, research has increasingly focused on privacy-by-design solutions, raising the need for objective techniques to evaluate privacy protection. This paper presents a comprehensive framework for evaluating visual privacy-protection methods across three dimensions: privacy, utility, and practicality. In addition, it introduces HR-VISPR, a publicly available human-centric dataset with biometric, soft-biometric, and non-biometric labels to train an interpretable privacy metric. We evaluate 11 privacy protection methods, ranging from conventional techniques to advanced deep-learning methods, through the proposed framework. The framework differentiates privacy levels in alignment with human visual perception, while highlighting trade-offs between privacy, utility, and practicality. This study, along with the HR-VISPR dataset, serves as an insightful tool and offers a structured evaluation framework applicable across diverse contexts.</li>
</ul>

<h3>Title: CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models</h3>
<ul>
<li><strong>Authors: </strong>Quang-Binh Nguyen, Minh Luu, Quang Nguyen, Anh Tran, Khoi Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13984">https://arxiv.org/abs/2507.13984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13984">https://arxiv.org/pdf/2507.13984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13984]] CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models(https://arxiv.org/abs/2507.13984)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Disentangling content and style from a single image, known as content-style decomposition (CSD), enables recontextualization of extracted content and stylization of extracted styles, offering greater creative flexibility in visual synthesis. While recent personalization methods have explored the decomposition of explicit content style, they remain tailored for diffusion models. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a promising alternative with a next-scale prediction paradigm, achieving performance comparable to that of diffusion models. In this paper, we explore VAR as a generative framework for CSD, leveraging its scale-wise generation process for improved disentanglement. To this end, we propose CSD-VAR, a novel method that introduces three key innovations: (1) a scale-aware alternating optimization strategy that aligns content and style representation with their respective scales to enhance separation, (2) an SVD-based rectification method to mitigate content leakage into style representations, and (3) an Augmented Key-Value (K-V) memory enhancing content identity preservation. To benchmark this task, we introduce CSD-100, a dataset specifically designed for content-style decomposition, featuring diverse subjects rendered in various artistic styles. Experiments demonstrate that CSD-VAR outperforms prior approaches, achieving superior content preservation and stylization fidelity.</li>
</ul>

<h3>Title: ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies</h3>
<ul>
<li><strong>Authors: </strong>Itay Katav, Aryeh Kontorovich</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13998">https://arxiv.org/abs/2507.13998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13998">https://arxiv.org/pdf/2507.13998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13998]] ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies(https://arxiv.org/abs/2507.13998)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Modern multivariate time series forecasting primarily relies on two architectures: the Transformer with attention mechanism and Mamba. In natural language processing, an approach has been used that combines local window attention for capturing short-term dependencies and Mamba for capturing long-term dependencies, with their outputs averaged to assign equal weight to both. We find that for time-series forecasting tasks, assigning equal weight to long-term and short-term dependencies is not optimal. To mitigate this, we propose a dynamic weighting mechanism, ParallelTime Weighter, which calculates interdependent weights for long-term and short-term dependencies for each token based on the input and the model's knowledge. Furthermore, we introduce the ParallelTime architecture, which incorporates the ParallelTime Weighter mechanism to deliver state-of-the-art performance across diverse benchmarks. Our architecture demonstrates robustness, achieves lower FLOPs, requires fewer parameters, scales effectively to longer prediction horizons, and significantly outperforms existing methods. These advances highlight a promising path for future developments of parallel Attention-Mamba in time series forecasting. The implementation is readily available at: \href{this https URL}{ParallelTime GitHub</li>
</ul>

<h3>Title: The CryptoNeo Threat Modelling Framework (CNTMF): Securing Neobanks and Fintech in Integrated Blockchain Ecosystems</h3>
<ul>
<li><strong>Authors: </strong>Serhan W. Bahar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14007">https://arxiv.org/abs/2507.14007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14007">https://arxiv.org/pdf/2507.14007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14007]] The CryptoNeo Threat Modelling Framework (CNTMF): Securing Neobanks and Fintech in Integrated Blockchain Ecosystems(https://arxiv.org/abs/2507.14007)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The rapid integration of blockchain, cryptocurrency, and Web3 technologies into digital banks and fintech operations has created an integrated environment blending traditional financial systems with decentralised elements. This paper introduces the CryptoNeo Threat Modelling Framework (CNTMF), a proposed framework designed to address the risks in these ecosystems, such as oracle manipulation and cross-chain exploits. CNTMF represents a proposed extension of established methodologies like STRIDE, OWASP Top 10, NIST frameworks, LINDDUN, and PASTA, while incorporating tailored components including Hybrid Layer Analysis, the CRYPTOQ mnemonic for cryptocurrency-specific risks, and an AI-Augmented Feedback Loop. Drawing on real-world data from 2025 incidents, CNTMF supports data-driven mitigation to reduce losses, which totalled approximately $2.47 billion in the first half of 2025 across 344 security events (CertiK via GlobeNewswire, 2025; Infosecurity Magazine, 2025). Its phases guide asset mapping, risk profiling, prioritisation, mitigation, and iterative feedback. This supports security against evolving risks like state-sponsored attacks.</li>
</ul>

<h3>Title: Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations</h3>
<ul>
<li><strong>Authors: </strong>Yong Feng, Xiaolei Zhang, Shijin Feng, Yong Zhao, Yihan Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14010">https://arxiv.org/abs/2507.14010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14010">https://arxiv.org/pdf/2507.14010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14010]] Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations(https://arxiv.org/abs/2507.14010)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming to classify and segment tunnel cracks with enhanced accuracy and efficiency, this study proposes a two-step deep learning-based method. An automatic tunnel image classification model is developed using the DenseNet-169 in the first step. The proposed crack segmentation model in the second step is based on the DeepLabV3+, whose internal logic is evaluated via a score-weighted visual explanation technique. Proposed method combines tunnel image classification and segmentation together, so that the selected images containing cracks from the first step are segmented in the second step to improve the detection accuracy and efficiency. The superior performances of the two-step method are validated by experiments. The results show that the accuracy and frames per second (FPS) of the tunnel crack classification model are 92.23% and 39.80, respectively, which are higher than other convolutional neural networks (CNN) based and Transformer based models. Also, the intersection over union (IoU) and F1 score of the tunnel crack segmentation model are 57.01% and 67.44%, respectively, outperforming other state-of-the-art models. Moreover, the provided visual explanations in this study are conducive to understanding the "black box" of deep learning-based models. The developed two-stage deep learning-based method integrating visual explanations provides a basis for fast and accurate quantitative assessment of tunnel health status.</li>
</ul>

<h3>Title: Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model</h3>
<ul>
<li><strong>Authors: </strong>Ji-Yan Wu, Zheng Yong Poh, Anoop C. Patil, Bongsoo Park, Giovanni Volpe, Daisuke Urano</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14013">https://arxiv.org/abs/2507.14013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14013">https://arxiv.org/pdf/2507.14013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14013]] Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model(https://arxiv.org/abs/2507.14013)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate detection of nutrient deficiency in plant leaves is essential for precision agriculture, enabling early intervention in fertilization, disease, and stress management. This study presents a deep learning framework for leaf anomaly segmentation using multispectral imaging and an enhanced YOLOv5 model with a transformer-based attention head. The model is tailored for processing nine-channel multispectral input and uses self-attention mechanisms to better capture subtle, spatially-distributed symptoms. The plants in the experiments were grown under controlled nutrient stress conditions for evaluation. We carry out extensive experiments to benchmark the proposed model against the baseline YOLOv5. Extensive experiments show that the proposed model significantly outperforms the baseline YOLOv5, with an average Dice score and IoU (Intersection over Union) improvement of about 12%. In particular, this model is effective in detecting challenging symptoms like chlorosis and pigment accumulation. These results highlight the promise of combining multi-spectral imaging with spectral-spatial feature learning for advancing plant phenotyping and precision agriculture.</li>
</ul>

<h3>Title: Efficient Temporal Tokenization for Mobility Prediction with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoyu He, Haozheng Luo, Yan Chen, Qi R. Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14017">https://arxiv.org/abs/2507.14017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14017">https://arxiv.org/pdf/2507.14017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14017]] Efficient Temporal Tokenization for Mobility Prediction with Large Language Models(https://arxiv.org/abs/2507.14017)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), a framework that leverages large language models (LLMs) as spatio-temporal predictors and trajectory reasoners. RHYTHM partitions trajectories into daily segments encoded as discrete tokens with hierarchical attention, capturing both daily and weekly dependencies while substantially reducing the sequence length. Token representations are enriched with pre-computed prompt embeddings via a frozen LLM, enhancing the model's ability to capture interdependencies without extensive computational overhead. By freezing the LLM backbone, RHYTHM achieves significant computational efficiency. Evaluation on three real-world datasets demonstrates a 2.4% improvement in accuracy, 5.0% increase on weekends, and 24.6% reduction in training time compared to state-of-the-art methods.</li>
</ul>

<h3>Title: Byzantine-resilient federated online learning for Gaussian process regression</h3>
<ul>
<li><strong>Authors: </strong>Xu Zhang, Zhenyuan Yuan, Minghui Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14021">https://arxiv.org/abs/2507.14021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14021">https://arxiv.org/pdf/2507.14021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14021]] Byzantine-resilient federated online learning for Gaussian process regression(https://arxiv.org/abs/2507.14021)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>In this paper, we study Byzantine-resilient federated online learning for Gaussian process regression (GPR). We develop a Byzantine-resilient federated GPR algorithm that allows a cloud and a group of agents to collaboratively learn a latent function and improve the learning performances where some agents exhibit Byzantine failures, i.e., arbitrary and potentially adversarial behavior. Each agent-based local GPR sends potentially compromised local predictions to the cloud, and the cloud-based aggregated GPR computes a global model by a Byzantine-resilient product of experts aggregation rule. Then the cloud broadcasts the current global model to all the agents. Agent-based fused GPR refines local predictions by fusing the received global model with that of the agent-based local GPR. Moreover, we quantify the learning accuracy improvements of the agent-based fused GPR over the agent-based local GPR. Experiments on a toy example and two medium-scale real-world datasets are conducted to demonstrate the performances of the proposed algorithm.</li>
</ul>

<h3>Title: CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jianfei Li, Kevin Kam Fung Yuen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14022">https://arxiv.org/abs/2507.14022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14022">https://arxiv.org/pdf/2507.14022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14022]] CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis(https://arxiv.org/abs/2507.14022)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This study proposes the Cognitive Pairwise Comparison Classification Model Selection (CPC-CMS) framework for document-level sentiment analysis. The CPC, based on expert knowledge judgment, is used to calculate the weights of evaluation criteria, including accuracy, precision, recall, F1-score, specificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and efficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random Forest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long Short-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from Transformers (ALBERT) are chosen as classification baseline models. A weighted decision matrix consisting of classification evaluation scores with respect to criteria weights, is formed to select the best classification model for a classification problem. Three open datasets of social media are used to demonstrate the feasibility of the proposed CPC-CMS. Based on our simulation, for evaluation results excluding the time factor, ALBERT is the best for the three datasets; if time consumption is included, no single model always performs better than the other models. The CPC-CMS can be applied to the other classification applications in different areas.</li>
</ul>

<h3>Title: Moodifier: MLLM-Enhanced Emotion-Driven Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Jiarong Ye, Sharon X. Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14024">https://arxiv.org/abs/2507.14024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14024">https://arxiv.org/pdf/2507.14024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14024]] Moodifier: MLLM-Enhanced Emotion-Driven Image Editing(https://arxiv.org/abs/2507.14024)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Bridging emotions and visual content for emotion-driven image editing holds great potential in creative industries, yet precise manipulation remains challenging due to the abstract nature of emotions and their varied manifestations across different contexts. We tackle this challenge with an integrated approach consisting of three complementary components. First, we introduce MoodArchive, an 8M+ image dataset with detailed hierarchical emotional annotations generated by LLaVA and partially validated by human evaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned on MoodArchive to translate abstract emotions into specific visual attributes. Third, we propose Moodifier, a training-free editing model leveraging MoodifyCLIP and multimodal large language models (MLLMs) to enable precise emotional transformations while preserving content integrity. Our system works across diverse domains such as character expressions, fashion design, jewelry, and home décor, enabling creators to quickly visualize emotional variations while preserving identity and structure. Extensive experimental evaluations show that Moodifier outperforms existing methods in both emotional accuracy and content preservation, providing contextually appropriate edits. By linking abstract emotions to concrete visual changes, our solution unlocks new possibilities for emotional content creation in real-world applications. We will release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier code and demo publicly available upon acceptance.</li>
</ul>

<h3>Title: QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography</h3>
<ul>
<li><strong>Authors: </strong>Hao Fang, Sihao Teng, Hao Yu, Siyi Yuan, Huaiwu He, Zhe Liu, Yunjie Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.ET, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14031">https://arxiv.org/abs/2507.14031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14031">https://arxiv.org/pdf/2507.14031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14031]] QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography(https://arxiv.org/abs/2507.14031)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, data-free</a></li>
<li><strong>Abstract: </strong>Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside imaging modality with high temporal resolution, making it suitable for bedside monitoring. However, its inherently ill-posed inverse problem poses significant challenges for accurate image reconstruction. Deep learning (DL)-based approaches have shown promise but often rely on complex network architectures with a large number of parameters, limiting efficiency and scalability. Here, we propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework for EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network (QA-Net), combining parallel 2-qubit quantum circuits to generate expressive latent representations that serve as implicit nonlinear priors, followed by a single linear layer for conductivity reconstruction. This design drastically reduces model complexity and parameter number. Uniquely, QuantEIT operates in an unsupervised, training-data-free manner and represents the first integration of quantum circuits into EIT image reconstruction. Extensive experiments on simulated and real-world 2D and 3D EIT lung imaging data demonstrate that QuantEIT outperforms conventional methods, achieving comparable or superior reconstruction accuracy using only 0.2% of the parameters, with enhanced robustness to noise.</li>
</ul>

<h3>Title: Training-free Token Reduction for Vision Mamba</h3>
<ul>
<li><strong>Authors: </strong>Qiankun Ma, Ziyao Zhang, Chi Su, Jie Chen, Zhen Song, Hairong Zheng, Wen Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14042">https://arxiv.org/abs/2507.14042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14042">https://arxiv.org/pdf/2507.14042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14042]] Training-free Token Reduction for Vision Mamba(https://arxiv.org/abs/2507.14042)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs) due to its ability to efficiently capture long-range dependencies with linear computational complexity. While token reduction, an effective compression technique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision Mamba's efficiency is essential for enabling broader applications. However, we find that directly applying existing token reduction techniques for ViTs to Vision Mamba leads to significant performance degradation. This is primarily because Mamba is a sequence model without attention mechanisms, whereas most token reduction techniques for ViTs rely on attention mechanisms for importance measurement and overlook the order of compressed tokens. In this paper, we investigate a Mamba structure-aware importance score to evaluate token importance in a simple and effective manner. Building on this score, we further propose MTR, a training-free \textbf{M}amba \textbf{T}oken \textbf{R}eduction framework. Without the need for training or additional tuning parameters, our method can be seamlessly integrated as a plug-and-play component across various Mamba models. Extensive experiments demonstrate that our approach significantly reduces computational workload while minimizing performance impact across various tasks and multiple backbones. Notably, MTR reduces FLOPs by approximately 40\% on the Vim-B backbone, with only a 1.6\% drop in ImageNet performance without retraining.</li>
</ul>

<h3>Title: Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks</h3>
<ul>
<li><strong>Authors: </strong>Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14045">https://arxiv.org/abs/2507.14045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14045">https://arxiv.org/pdf/2507.14045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14045]] Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks(https://arxiv.org/abs/2507.14045)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive evaluation of cost-efficient Large Language Models (LLMs) for diverse biomedical tasks spanning both text and image modalities. We evaluated a range of closed-source and open-source LLMs on tasks such as biomedical text classification and generation, question answering, and multimodal image processing. Our experimental findings indicate that there is no single LLM that can consistently outperform others across all tasks. Instead, different LLMs excel in different tasks. While some closed-source LLMs demonstrate strong performance on specific tasks, their open-source counterparts achieve comparable results (sometimes even better), with additional benefits like faster inference and enhanced privacy. Our experimental results offer valuable insights for selecting models that are optimally suited for specific biomedical applications.</li>
</ul>

<h3>Title: Noradrenergic-inspired gain modulation attenuates the stability gap in joint training</h3>
<ul>
<li><strong>Authors: </strong>Alejandro Rodriguez-Garcia, Anindya Ghosh, Srikanth Ramaswamy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14056">https://arxiv.org/abs/2507.14056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14056">https://arxiv.org/pdf/2507.14056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14056]] Noradrenergic-inspired gain modulation attenuates the stability gap in joint training(https://arxiv.org/abs/2507.14056)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent studies in continual learning have identified a transient drop in performance on mastered tasks when assimilating new ones, known as the stability gap. Such dynamics contradict the objectives of continual learning, revealing a lack of robustness in mitigating forgetting, and notably, persisting even under an ideal joint-loss regime. Examining this gap within this idealized joint training context is critical to isolate it from other sources of forgetting. We argue that it reflects an imbalance between rapid adaptation and robust retention at task boundaries, underscoring the need to investigate mechanisms that reconcile plasticity and stability within continual learning frameworks. Biological brains navigate a similar dilemma by operating concurrently on multiple timescales, leveraging neuromodulatory signals to modulate synaptic plasticity. However, artificial networks lack native multitimescale dynamics, and although optimizers like momentum-SGD and Adam introduce implicit timescale regularization, they still exhibit stability gaps. Inspired by locus coeruleus mediated noradrenergic bursts, which transiently enhance neuronal gain under uncertainty to facilitate sensory assimilation, we propose uncertainty-modulated gain dynamics - an adaptive mechanism that approximates a two-timescale optimizer and dynamically balances integration of knowledge with minimal interference on previously consolidated information. We evaluate our mechanism on domain-incremental and class-incremental variants of the MNIST and CIFAR benchmarks under joint training, demonstrating that uncertainty-modulated gain dynamics effectively attenuate the stability gap. Finally, our analysis elucidates how gain modulation replicates noradrenergic functions in cortical circuits, offering mechanistic insights into reducing stability gaps and enhance performance in continual learning tasks.</li>
</ul>

<h3>Title: VLA-Mark: A cross modal watermark for large vision-language alignment model</h3>
<ul>
<li><strong>Authors: </strong>Shuliang Liu, Qi Zheng, Jesse Jiaxi Xu, Yibo Yan, He Geng, Aiwei Liu, Peijie Jiang, Jia Liu, Yik-Cheung Tam, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14067">https://arxiv.org/abs/2507.14067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14067">https://arxiv.org/pdf/2507.14067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14067]] VLA-Mark: A cross modal watermark for large vision-language alignment model(https://arxiv.org/abs/2507.14067)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, watermark</a></li>
<li><strong>Abstract: </strong>Vision-language models demand watermarking solutions that protect intellectual property without compromising multimodal coherence. Existing text watermarking methods disrupt visual-textual alignment through biased token selection and static strategies, leaving semantic-critical concepts vulnerable. We propose VLA-Mark, a vision-aligned framework that embeds detectable watermarks while preserving semantic fidelity through cross-modal coordination. Our approach integrates multiscale visual-textual alignment metrics, combining localized patch affinity, global semantic coherence, and contextual attention patterns, to guide watermark injection without model retraining. An entropy-sensitive mechanism dynamically balances watermark strength and semantic preservation, prioritizing visual grounding during low-uncertainty generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than conventional methods, with near-perfect detection (98.8% AUC). The framework demonstrates 96.1\% attack resilience against attacks such as paraphrasing and synonym substitution, while maintaining text-visual consistency, establishing new standards for quality-preserving multimodal watermarking</li>
</ul>

<h3>Title: DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits</h3>
<ul>
<li><strong>Authors: </strong>Garapati Keerthana, Manik Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14079">https://arxiv.org/abs/2507.14079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14079">https://arxiv.org/pdf/2507.14079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14079]] DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits(https://arxiv.org/abs/2507.14079)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Progress notes are among the most clinically meaningful artifacts in an Electronic Health Record (EHR), offering temporally grounded insights into a patient's evolving condition, treatments, and care decisions. Despite their importance, they are severely underrepresented in large-scale EHR datasets. For instance, in the widely used Medical Information Mart for Intensive Care III (MIMIC-III) dataset, only about $8.56\%$ of hospital visits include progress notes, leaving gaps in longitudinal patient narratives. In contrast, the dataset contains a diverse array of other note types, each capturing different aspects of care. We present DENSE (Documenting Evolving Progress Notes from Scattered Evidence), a system designed to align with clinical documentation workflows by simulating how physicians reference past encounters while drafting progress notes. The system introduces a fine-grained note categorization and a temporal alignment mechanism that organizes heterogeneous notes across visits into structured, chronological inputs. At its core, DENSE leverages a clinically informed retrieval strategy to identify temporally and semantically relevant content from both current and prior visits. This retrieved evidence is used to prompt a large language model (LLM) to generate clinically coherent and temporally aware progress notes. We evaluate DENSE on a curated cohort of patients with multiple visits and complete progress note documentation. The generated notes demonstrate strong longitudinal fidelity, achieving a temporal alignment ratio of $1.089$, surpassing the continuity observed in original notes. By restoring narrative coherence across fragmented documentation, our system supports improved downstream tasks such as summarization, predictive modeling, and clinical decision support, offering a scalable solution for LLM-driven note synthesis in real-world healthcare settings.</li>
</ul>

<h3>Title: Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Sara Abdulaziz, Egor Bondarev</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14083">https://arxiv.org/abs/2507.14083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14083">https://arxiv.org/pdf/2507.14083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14083]] Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection(https://arxiv.org/abs/2507.14083)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust</a></li>
<li><strong>Abstract: </strong>Advancements in deep learning have improved anomaly detection in surveillance videos, yet they raise urgent privacy concerns due to the collection of sensitive human data. In this paper, we present a comprehensive analysis of anomaly detection performance under four human anonymization techniques, including blurring, masking, encryption, and avatar replacement, applied to the UCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU, BN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method responds to different obfuscation techniques. Experimental results demonstrate that anomaly detection remains viable under anonymized data and is dependent on the algorithmic design and the learning strategy. For instance, under certain anonymization patterns, such as encryption and masking, some models inadvertently achieve higher AUC performance compared to raw data, due to the strong responsiveness of their algorithmic components to these noise patterns. These results highlight the algorithm-specific sensitivities to anonymization and emphasize the trade-off between preserving privacy and maintaining detection utility. Furthermore, we compare these conventional anonymization techniques with the emerging privacy-by-design solutions, highlighting an often overlooked trade-off between robust privacy protection and utility flexibility. Through comprehensive experiments and analyses, this study provides a compelling benchmark and insights into balancing human privacy with the demands of anomaly detection.</li>
</ul>

<h3>Title: DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Xiyun Li, Yining Ding, Yuhua Jiang, Yunlong Zhao, Runpeng Xie, Shuang Xu, Yuanhua Ni, Yiqin Yang, Bo Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14088">https://arxiv.org/abs/2507.14088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14088">https://arxiv.org/pdf/2507.14088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14088]] DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration(https://arxiv.org/abs/2507.14088)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Real-time human-artificial intelligence (AI) collaboration is crucial yet challenging, especially when AI agents must adapt to diverse and unseen human behaviors in dynamic scenarios. Existing large language model (LLM) agents often fail to accurately model the complex human mental characteristics such as domain intentions, especially in the absence of direct communication. To address this limitation, we propose a novel dual process multi-scale theory of mind (DPMT) framework, drawing inspiration from cognitive science dual process theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM) module to facilitate robust human partner modeling through mental characteristic reasoning. Experimental results demonstrate that DPMT significantly enhances human-AI collaboration, and ablation studies further validate the contributions of our multi-scale ToM in the slow system.</li>
</ul>

<h3>Title: C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected δ-Overlap Graphs</h3>
<ul>
<li><strong>Authors: </strong>Yung-Hong Sun, Ting-Hung Lin, Jiangang Chen, Hongrui Jiang, Yu Hen Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14095">https://arxiv.org/abs/2507.14095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14095">https://arxiv.org/pdf/2507.14095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14095]] C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected δ-Overlap Graphs(https://arxiv.org/abs/2507.14095)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-view multi-object association is a fundamental step in 3D reconstruction pipelines, enabling consistent grouping of object instances across multiple camera views. Existing methods often rely on appearance features or geometric constraints such as epipolar consistency. However, these approaches can fail when objects are visually indistinguishable or observations are corrupted by noise. We propose C-DOG, a training-free framework that serves as an intermediate module bridging object detection (or pose estimation) and 3D reconstruction, without relying on visual features. It combines connected delta-overlap graph modeling with epipolar geometry to robustly associate detections across views. Each 2D observation is represented as a graph node, with edges weighted by epipolar consistency. A delta-neighbor-overlap clustering step identifies strongly consistent groups while tolerating noise and partial connectivity. To further improve robustness, we incorporate Interquartile Range (IQR)-based filtering and a 3D back-projection error criterion to eliminate inconsistent observations. Extensive experiments on synthetic benchmarks demonstrate that C-DOG outperforms geometry-based baselines and remains robust under challenging conditions, including high object density, without visual features, and limited camera overlap, making it well-suited for scalable 3D reconstruction in real-world scenarios.</li>
</ul>

<h3>Title: Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track</h3>
<ul>
<li><strong>Authors: </strong>Brian Ondov, William Xia, Kush Attal, Ishita Unde, Jerry He, Hoa Dang, Ian Soboroff, Dina Demner-Fushman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14096">https://arxiv.org/abs/2507.14096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14096">https://arxiv.org/pdf/2507.14096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14096]] Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track(https://arxiv.org/abs/2507.14096)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Objective: Recent advances in language models have shown potential to adapt professional-facing biomedical literature to plain language, making it accessible to patients and caregivers. However, their unpredictability, combined with the high potential for harm in this domain, means rigorous evaluation is necessary. Our goals with this track were to stimulate research and to provide high-quality evaluation of the most promising systems. Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts (PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included complete, sentence-level, rewriting of abstracts (Task 1) as well as identifying and replacing difficult terms (Task 2). For automatic evaluation of Task 1, we developed a four-fold set of professionally-written references. Submissions for both Tasks 1 and 2 were provided extensive manual evaluation from biomedical experts. Results: Twelve teams spanning twelve countries participated in the track, with models from multilayer perceptrons to large pretrained transformers. In manual judgments of Task 1, top-performing models rivaled human levels of factual accuracy and completeness, but not simplicity or brevity. Automatic, reference-based metrics generally did not correlate well with manual judgments. In Task 2, systems struggled with identifying difficult terms and classifying how to replace them. When generating replacements, however, LLM-based systems did well in manually judged accuracy, completeness, and simplicity, though not in brevity. Conclusion: The PLABA track showed promise for using Large Language Models to adapt biomedical literature for the general public, while also highlighting their deficiencies and the need for improved automatic benchmarking tools.</li>
</ul>

<h3>Title: An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Cao, Bimal Adhikari, Shangqing Zhao, Jingxian Wu, Yanjun Pan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14109">https://arxiv.org/abs/2507.14109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14109">https://arxiv.org/pdf/2507.14109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14109]] An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting(https://arxiv.org/abs/2507.14109)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Radio frequency (RF) fingerprinting, which extracts unique hardware imperfections of radio devices, has emerged as a promising physical-layer device identification mechanism in zero trust architectures and beyond 5G networks. In particular, deep learning (DL) methods have demonstrated state-of-the-art performance in this domain. However, existing approaches have primarily focused on enhancing system robustness against temporal and spatial variations in wireless environments, while the security vulnerabilities of these DL-based approaches have often been overlooked. In this work, we systematically investigate the security risks of DL-based RF fingerprinting systems through an adversarial-driven experimental analysis. We observe a consistent misclassification behavior for DL models under domain shifts, where a device is frequently misclassified as another specific one. Our analysis based on extensive real-world experiments demonstrates that this behavior can be exploited as an effective backdoor to enable external attackers to intrude into the system. Furthermore, we show that training DL models on raw received signals causes the models to entangle RF fingerprints with environmental and signal-pattern features, creating additional attack vectors that cannot be mitigated solely through post-processing security methods such as confidence thresholds.</li>
</ul>

<h3>Title: NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining</h3>
<ul>
<li><strong>Authors: </strong>Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh, Georgii Fedorov, Bulat Suleimanov, Vladimir Dokholyan, Aleksandr Gordeev</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14119">https://arxiv.org/abs/2507.14119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14119">https://arxiv.org/pdf/2507.14119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14119]] NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining(https://arxiv.org/abs/2507.14119)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Recent advances in generative modeling enable image editing assistants that follow natural language instructions without additional user input. Their supervised training requires millions of triplets: original image, instruction, edited image. Yet mining pixel-accurate examples is hard. Each edit must affect only prompt-specified regions, preserve stylistic coherence, respect physical plausibility, and retain visual appeal. The lack of robust automated edit-quality metrics hinders reliable automation at scale. We present an automated, modular pipeline that mines high-fidelity triplets across domains, resolutions, instruction complexities, and styles. Built on public generative models and running without human intervention, our system uses a task-tuned Gemini validator to score instruction adherence and aesthetics directly, removing any need for segmentation or grounding models. Inversion and compositional bootstrapping enlarge the mined set by approximately 2.2x, enabling large-scale high-fidelity training data. By automating the most repetitive annotation steps, the approach allows a new scale of training without human labeling effort. To democratize research in this resource-intensive area, we release NHR-Edit: an open dataset of 358k high-quality triplets. In the largest cross-dataset evaluation, it surpasses all public alternatives. We also release Bagel-NHR-Edit, an open-source fine-tuned Bagel model, which achieves state-of-the-art metrics in our experiments.</li>
</ul>

<h3>Title: Toward Temporal Causal Representation Learning with Tensor Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Jianhong Chen, Meng Zhao, Mostafa Reisi Gahrooei, Xubo Yue</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14126">https://arxiv.org/abs/2507.14126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14126">https://arxiv.org/pdf/2507.14126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14126]] Toward Temporal Causal Representation Learning with Tensor Decomposition(https://arxiv.org/abs/2507.14126)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Temporal causal representation learning is a powerful tool for uncovering complex patterns in observational studies, which are often represented as low-dimensional time series. However, in many real-world applications, data are high-dimensional with varying input lengths and naturally take the form of irregular tensors. To analyze such data, irregular tensor decomposition is critical for extracting meaningful clusters that capture essential information. In this paper, we focus on modeling causal representation learning based on the transformed information. First, we present a novel causal formulation for a set of latent clusters. We then propose CaRTeD, a joint learning framework that integrates temporal causal representation learning with irregular tensor decomposition. Notably, our framework provides a blueprint for downstream tasks using the learned tensor factors, such as modeling latent structures and extracting causal information, and offers a more flexible regularization design to enhance tensor decomposition. Theoretically, we show that our algorithm converges to a stationary point. More importantly, our results fill the gap in theoretical guarantees for the convergence of state-of-the-art irregular tensor decomposition. Experimental results on synthetic and real-world electronic health record (EHR) datasets (MIMIC-III), with extensive benchmarks from both phenotyping and network recovery perspectives, demonstrate that our proposed method outperforms state-of-the-art techniques and enhances the explainability of causal representations.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
